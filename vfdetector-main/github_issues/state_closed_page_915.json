[{"number": 26007, "title": "tensorflow.python.keras.api._v2.keras.losses' has no attribute 'SparseCategoricalCrossentropy'   \uff1f\uff1f", "body": "it throw the error when I run autograph.ipynb, error message is following:\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-36-2c50f534479f> in <module>\r\n----> 1 compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n      2 \r\n      3 compute_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\r\n      4 \r\n      5 \r\n\r\nAttributeError: module 'tensorflow.python.keras.api._v2.keras.losses' has no attribute 'SparseCategoricalCrossentropy'", "comments": ["Could you fill issue template and provide necessary environment information? ", "Similar to https://github.com/tensorflow/tensorflow/issues/26012.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@danaugrs Would you like to fix it, Daniel?", "@facaiy Thanks! I think we can close this one. The author is probably using an old version like I was in https://github.com/tensorflow/tensorflow/issues/26012 and it should be fixed now.", "many thanks for your feedback! ", "Instead of writing complete path........ tf.keras.metrics.SparseCategoricalAccuracy() try writing soemthing like\r\nloss = 'sparse_categorical_crossentropy', \r\nmetrics=['accuracy'] \r\nfor a categorical problem. It should work"]}, {"number": 26006, "title": "Issue in model for parts of speech tagger.", "body": "**System information**\r\n- TensorFlow version (you are using):1.12\r\n- Are you willing to contribute it (Yes/No):No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI built a model which can predict parts of speech of words when given separately. Now I am building a model which can predict parts of speech of words in sentences. For this i have created two arrays. 1st Array - which has sentences tokenized as arrays[14,546,789,4556](words as sequences, There are many arrays like this representing sentences), 2nd Array- Arrays of parts of speech of each word in[14,7,3,21](respective parts of speech as sequences), .Both words and parts and speech are converted to sequences\r\n\r\n**CODE STARTS**\r\nmodel  = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Embedding(input_dim=11040,output_dim=64, input_length = 30, batch_input_shape=[128,None]))\r\nmodel.add(tf.keras.layers.LSTM(30, activation='tanh', recurrent_activation='hard_sigmoid', kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',stateful = True))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(30))\r\nmodel.summary()\r\n**CODE ENDS**\r\n\r\n**PROBLEM**\r\nIt is returning a single array where each element in the predicted array represents a whole sentences where it should give array of parts of speech for each sentence in the input. Please tell me how to change the model and why. I am a beginner trying to learn machine learning.\r\n\r\n**Will this change the current api? How?**\r\nThis wont change the API.\r\n\r\n**Who will benefit with this feature?**\r\nBeginners who are starting on textual data\r\n", "comments": []}, {"number": 26005, "title": " Model transfer Tensorflow to Keras", "body": "\r\nI want to transfer the model from Tensorflow to Keras. But I do not see this function", "comments": ["Please take a look at similar [discussion](https://stackoverflow.com/questions/44466066/how-can-i-convert-a-trained-tensorflow-model-to-keras).\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 26004, "title": "FIx CudnnCompatibleLSTM document error", "body": "Fix the CudnnCompatibleLSTM document error, it should be tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell instead of tf.contrib.cudnn_rnn.CudnnCompatibleLSTM.", "comments": []}, {"number": 26003, "title": "Issue in running the models of tensorflow", "body": "Hi,I want to know how to run various models like mobile net, image net, resnet  .Is there any particular set of steps to run these models or any official tensorflow tutorial to follow.Kindly guide me for the same.\r\nRight Now i am following these steps:\r\n1. sudo apt install python3-dev python3-pip\r\n2. sudo pip3 install -U virtualenv\r\n3. virtualenv --system-site-packages -p python3 ./venv\r\n4. source ./venv/bin/activate\r\n5. pip install --upgrade pip\r\n6. pip list\r\n7. pip install --upgrade tensorflow\r\n8. pip install -U --user pip six numpy wheel mock\r\n9. pip install -U --user keras_applications==1.0.6 --no-deps\r\n10. pip install -U --user keras_preprocessing==1.0.5 --no-deps\r\n11. git clone https://github.com/tensorflow/tensorflow.git\r\n12. git clone https://github.com/tensorflow/models.git\r\n12. cd tensorflow\r\n\r\n(Python version-3.5.2\r\nvirtual env version-16.04\r\npip version-19.0.3)\r\n\r\nPlease Help!\r\n\r\n****", "comments": ["You can refer to the instructions posted on https://github.com/tensorflow/models/tree/master/official and further the markdown files of different models in official directory."]}, {"number": 26002, "title": "Install Tensorflow GPU with CUDA 10.0 on windows", "body": "Hi guys. I am new to the tensorflow. I would like to install Tensorflow-Gpu with my RTX2080ti graphic card. I followed this instruction \r\nhttps://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows/\r\n\r\nHowever, when I do the Step 12. The error shows up.\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nBecause RTX 2080 ti only support Cuda 10.0, it is very difficult for me to install tensorflow. Anyone can help me with this error or provide another useful instruction? Thanks a lot!!\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.17\r\n- CUDA/cuDNN version: cuda 10.0 cuDNN 7.4.2\r\n- GPU model and memory: Nvidia RTX 2080ti\r\n\r\n\r\n\r\n**The Error**\r\np_packageuild --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nDEBUG: C:/users/shica/_bazel_shica/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/shica/_bazel_shica/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/shica/_bazel_shica/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1447\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1303, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 217, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 153, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(\"\\\\\", \"/\")\r\ntype 'NoneType' has no method replace(string, string)\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1447\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1303, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 217, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl\", line 153, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(\"\\\\\", \"/\")\r\ntype 'NoneType' has no method replace(string, string)\r\nINFO: Elapsed time: 1.561s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n", "comments": ["Please install TensorFlow version 1.13.0-rc2 it supports cuda 10.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 26001, "title": "MKL build failing for mkl_cpu_allocator.h header in file threadpool_device.cc", "body": "**Problem**\r\nTensorFlow with MKL isn't building successfully.\r\n\r\n**Error Log**\r\n```bash\r\n\u001b[0m\u001b[91mERROR: /opt/tensorflow/tensorflow/core/BUILD:3034:1: C++ compilation of rule '//tensorflow/core:core_cpu_impl' failed (Exit 1)\r\nIn file included from tensorflow/core/common_runtime/threadpool_device.cc:38:\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:85:8: error: 'void tensorflow::MklSmallSizeAllocator::GetStats(tensorflow::AllocatorStats*)' marked 'override', but does not override\r\n   void GetStats(AllocatorStats* stats) override {\r\n        ^~~~~~~~\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklSmallSizeAllocator::ClearStats()':\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:92:12: error: 'struct tensorflow::AllocatorStats' has no member named 'Clear'\r\n     stats_.Clear();\r\n            ^~~~~\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'void tensorflow::MklSmallSizeAllocator::IncrementStats(size_t)':\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:101:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?\r\n     stats_.max_bytes_in_use =\r\n            ^~~~~~~~~~~~~~~~\r\n            peak_bytes_in_use\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:102:25: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?\r\n         std::max(stats_.max_bytes_in_use, stats_.bytes_in_use);\r\n                         ^~~~~~~~~~~~~~~~\r\n                         peak_bytes_in_use\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:103:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?\r\n     stats_.max_alloc_size =\r\n            ^~~~~~~~~~~~~~\r\n            largest_alloc_size\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:104:57: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?\r\n         std::max(alloc_size, static_cast<size_t>(stats_.max_alloc_size));\r\n                                                         ^~~~~~~~~~~~~~\r\n                                                         largest_alloc_size\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h: At global scope:\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:247:8: error: 'void tensorflow::MklCPUAllocator::GetStats(tensorflow::AllocatorStats*)' marked 'override', but does not override\r\n   void GetStats(AllocatorStats* stats) override {\r\n        ^~~~~~~~\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'void tensorflow::MklCPUAllocator::GetStats(tensorflow::AllocatorStats*)':\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:250:45: error: no matching function for call to 'tensorflow::Allocator::GetStats(tensorflow::AllocatorStats*)'\r\n     large_size_allocator_->GetStats(&l_stats);\r\n                                             ^\r\nIn file included from ./tensorflow/core/common_runtime/device.h:35,\r\n                 from ./tensorflow/core/common_runtime/local_device.h:19,\r\n                 from ./tensorflow/core/common_runtime/threadpool_device.h:20,\r\n                 from tensorflow/core/common_runtime/threadpool_device.cc:16:\r\n./tensorflow/core/framework/allocator.h:207:42: note: candidate: 'virtual absl::optional<tensorflow::AllocatorStats> tensorflow::Allocator::GetStats()'\r\n   virtual absl::optional<AllocatorStats> GetStats() { return absl::nullopt; }\r\n                                          ^~~~~~~~\r\n./tensorflow/core/framework/allocator.h:207:42: note:   candidate expects 0 arguments, 1 provided\r\nIn file included from tensorflow/core/common_runtime/threadpool_device.cc:38:\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:255:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?\r\n     stats->max_bytes_in_use =\r\n            ^~~~~~~~~~~~~~~~\r\n            peak_bytes_in_use\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:256:17: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?\r\n         l_stats.max_bytes_in_use + s_stats.max_bytes_in_use;\r\n                 ^~~~~~~~~~~~~~~~\r\n                 peak_bytes_in_use\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:256:44: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?\r\n         l_stats.max_bytes_in_use + s_stats.max_bytes_in_use;\r\n                                            ^~~~~~~~~~~~~~~~\r\n                                            peak_bytes_in_use\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:261:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?\r\n     stats->max_alloc_size = l_stats.max_alloc_size;\r\n            ^~~~~~~~~~~~~~\r\n            largest_alloc_size\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:261:37: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?\r\n     stats->max_alloc_size = l_stats.max_alloc_size;\r\n                                     ^~~~~~~~~~~~~~\r\n                                     largest_alloc_size\r\n\u001b[0m\u001b[91mTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\n**Build call**\r\n```bash\r\n   wget https://github.com/bazelbuild/bazel/releases/download/0.19.0/bazel_0.19.0-linux-x86_64.deb && \\\r\n    sudo dpkg -i bazel_0.19.0-linux-x86_64.deb && \\\r\n    sudo apt-get install -f && \\\r\n    rm bazel_0.19.0-linux-x86_64.deb && \\\r\n    pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow && \\\r\n    cd /opt && \\\r\n    git clone --recursive https://github.com/tensorflow/tensorflow.git && \\\r\n    cd /opt/tensorflow && \\\r\n    # git checkout 0f22d6431e0d97eb264fa35253de7bf333ced14c && \\\r\n    # sed -i 's/2018\\.0\\.3\\.20180406/2019\\.0\\.1\\.20180928/g' tensorflow/contrib/cmake/external/mkl.cmake && \\\r\n    # sed -i 's/v0\\.14/v0\\.17.1/g' tensorflow/contrib/cmake/external/mkl.cmake && \\\r\n    # sed -i 's/3063b2e4c943983f6bf5f2fb9a490d4a998cd291/1687299a987d1217fe80253a3b3bddc6fcf4a487/g' tensorflow/contrib/cmake/external/mkldnn.cmake && \\\r\n    ln -s /usr/lib/libmkldnn.so /usr/lib/libiomp5.so /usr/lib/libmklml_gnu.so  /usr/lib/libmklml_intel.so $MKLROOT/lib/intel64_lin && \\\r\n    ln -s /usr/include/mkldnn* $MKLROOT/include && \\\r\n    ln -s $MKLROOT/lib/intel64_lin/* $MKLROOT/lib && \\\r\n    ln -s /usr/lib /usr/lib/lib && \\\r\n    echo \"\" > /usr/local/lib/license.txt && \\\r\n    echo \"\" > /usr/local/include/license.txt && \\\r\n    echo \"from tensorflow.contrib import cloud\" >> tensorflow/contrib/__init__.py && \\\r\n    echo \"from tensorflow.contrib import *\" >> tensorflow/contrib/__init__.py && \\\r\n    TF_MKL_ROOT=/usr/lib  \\\r\n    TF_MKL_DOWNLOAD=0 \\\r\n    USE_DEFAULT_PYTHON_LIB_PATH=1 \\\r\n    TF_NEED_MKL=1 \\\r\n    TF_NEED_JEMALLOC=1 \\\r\n    TF_NEED_GCP=0 \\\r\n    TF_NEED_HDFS=0 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NEED_MPI=0 \\\r\n    TF_NEED_GDR=0 \\\r\n    TF_NEED_S3=1 \\\r\n    TF_NEED_KAFKA=0 \\\r\n    TF_SET_ANDROID_WORKSPACE=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_MKL_ENABLED=\"true\" \\\r\n    CI_BUILD_PYTHON=/opt/conda/bin/python \\\r\n    PYTHON_BIN_PATH=/opt/conda/bin/python \\\r\n    PYTHON_LIB_PATH=/opt/conda/lib/python3.6/site-packages \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    CC_OPT_FLAGS=\" -mavx -msse2 -msse3 -msse4.2 -msse4.1 -mfpmath=sse -lmkl_gf_lp64 -Wl,--start-group -lmkldnn -lmklml_intel -lmkl_gnu_thread -lmkl_core -Wl,--end-group -dl -lpthread -lm \" \\\r\n    /bin/bash ./configure && \\\r\n    bazel build \\\r\n    --config=mkl --config=opt \\\r\n    --config=verbs \\\r\n    --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \\\r\n    --copt=-O3 --copt=-mfpmath=both \\\r\n    --copt=\"-DMKL_LP64\" \\\r\n    --copt=\"-fPIC\" \\\r\n    --linkopt=\"-lmkl_gf_lp64\" \\\r\n    --linkopt=\"-lmkl_gnu_thread\" \\\r\n    --linkopt=\"-dl\" \\\r\n    --linkopt=\"-ldl\" \\\r\n    --linkopt=\"-lpthread\" \\\r\n    --linkopt=\"-lmkl_core\" \\\r\n    --linkopt=\"-lm\" \\\r\n    --linkopt=\"-lmkl_rt\" \\\r\n    --linkopt=\"-lmkldnn\" \\\r\n    tensorflow/tools/pip_package:build_pip_package && \\\r\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\r\n    pip install --no-deps /tmp/pip/tensorflow-*.whl && \\\r\n    cd /opt && rm -rf /opt/tensorflow /tmp/* && \\\r\n    python -c \"import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))\" && \\\r\n    python -c \"import tensorboard\"\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian: Stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: Master branch\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: Source build\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n", "comments": ["@sadatnfs Thanks for submitting the issue. There is a fix that has been merged into master https://github.com/tensorflow/tensorflow/pull/25979", "@sadatnfs can you try the fix and let us know if solved for you?", "@nathan-greeneltch-intel oh shoot sorry yes it built all good! :)\r\n\r\nI had an issue which I was using VERBS in my configs, but that's really not a big deal for me so I commented that out. MKL is all good!", "fixed here - https://github.com/tensorflow/tensorflow/commit/dce9a49c19f406ba45919e8c94474e55dc5ccd54"]}, {"number": 26000, "title": "Update operator.cc", "body": "actication->activation", "comments": ["This changes are merged in to master"]}, {"number": 25999, "title": "Update xla_compilation_device.cc", "body": "aligment->alignment", "comments": []}, {"number": 25998, "title": "The difference and error in tensorflow-1.8.0", "body": "\r\nWhen I use tensorflow-1.8.0, the code is different and wrong. It is shape[axis] = -1.", "comments": ["Please provide a minimal code snippet to reproduce the issue reported here. Also provide following details: Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:", "1. - I have not written custom code\r\n2. - OS Platform is Windows10\r\n3. - Tensorflow installed by conda install --channel https://conda.anaconda.org/anaconda tensorflow-gpu=1.8.0\r\n4. - Tensorflow version is tensorflow-gpu-1.8.0\r\n5. - python version is 3.6.5\r\n6. - The code of maxout is difference in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py and the tensorflow-1.8.0 .\r\n![error](https://user-images.githubusercontent.com/33117689/53227216-9dc00400-36b8-11e9-9a02-da400e4a5266.png)\r\n\r\nmy test code is as follow:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nlayers=tf.layers\r\ndef leaky_relu(alpha):\r\n    \r\n    def op(inputs):\r\n        return tf.nn.leaky_relu(inputs,alpha=alpha)\r\n    return op\r\ndef conv_block(inputs,filters,kernel_size,strides,name,padding='valid',\r\n               activation=leaky_relu(0.1),\r\n               kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n               kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0004)):\r\n    net=layers.conv2d(inputs,filters=filters,kernel_size=kernel_size,\r\n                      activation=activation,\r\n                      padding=padding,\r\n                      kernel_initializer=kernel_initializer,\r\n                      kernel_regularizer=kernel_regularizer,\r\n                      strides=strides,name=name)\r\n    return net\r\nimages=tf.placeholder(tf.float32,[None,64,64,3])\r\nnet=conv_block(images,128,3,2,name='conv1',activation=None)\r\nnet=tf.contrib.layers.maxout(net,64,scope='maxout1')\r\n\r\nnet=conv_block(net,256,3,2,name='conv2',activation=None)\r\nnet=tf.contrib.layers.maxout(net,128,scope='maxout2')\r\nnet=layers.max_pooling2d(net,2,2,name='pool1')\r\n\r\nnet=conv_block(net,512,3,1,name='conv3',activation=None)\r\nnet=tf.contrib.layers.maxout(net,256,scope='maxout3')`\r\n\r\n```\r\n\r\nthe error is as follow:\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-22f93208db56> in <module>()\r\n     22 net=tf.contrib.layers.maxout(net,64,scope='maxout1')\r\n     23 \r\n---> 24 net=conv_block(net,256,3,2,name='conv2',activation=None)\r\n     25 net=tf.contrib.layers.maxout(net,128,scope='maxout2')\r\n     26 net=layers.max_pooling2d(net,2,2,name='pool1')\r\n\r\n<ipython-input-1-22f93208db56> in conv_block(inputs, filters, kernel_size, strides, name, padding, activation, kernel_initializer, kernel_regularizer)\r\n     16                       kernel_initializer=kernel_initializer,\r\n     17                       kernel_regularizer=kernel_regularizer,\r\n---> 18                       strides=strides,name=name)\r\n     19     return net\r\n     20 images=tf.placeholder(tf.float32,[None,64,64,3])\r\n\r\nd:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py in conv2d(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\r\n    619       _reuse=reuse,\r\n    620       _scope=name)\r\n--> 621   return layer.apply(inputs)\r\n    622 \r\n    623 \r\n\r\nd:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in apply(self, inputs, *args, **kwargs)\r\n    826       Output tensor(s).\r\n    827     \"\"\"\r\n--> 828     return self.__call__(inputs, *args, **kwargs)\r\n    829 \r\n    830   def _add_inbound_node(self,\r\n\r\nd:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    697           if all(hasattr(x, 'get_shape') for x in input_list):\r\n    698             input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)\r\n--> 699           self.build(input_shapes)\r\n    700         try:\r\n    701           # Note: not all sub-classes of Layer call Layer.__init__ (especially\r\n\r\nd:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py in build(self, input_shape)\r\n    131       channel_axis = -1\r\n    132     if input_shape[channel_axis].value is None:\r\n--> 133       raise ValueError('The channel dimension of the inputs '\r\n    134                        'should be defined. Found `None`.')\r\n    135     input_dim = input_shape[channel_axis].value\r\n\r\nValueError: The channel dimension of the inputs should be defined. Found `None`.\r\n\r\n```\r\n", "#22031 changed that code from 1.8.0 to the current master. Don't think it is a wrong change", "Hi @mihaimaruseac . I just want to say that the code in 1.8.0 can't work well . Anyway,thanks!", "That's why it was fixed in #22031"]}, {"number": 25997, "title": "Tflite convert with Bazel 0.22.0 error: Unable to load package for '//tensorflow/tools/def_file_filter:def_file_filter_configure.bzl'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n**Describe the problem**\r\nPlease help me with this error. I searched on the Internet but no one ever met. I run bazel to convert tf model to tflite model according to this tutorial [Running on mobile with TensorFlow Lite](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md)\r\nERROR: error loading package '': in F:/card-details-recognition-android-version2/tensorflow/tensorflow/workspace.bzl: Unable to load package for '//tensorflow/tools/def_file_filter:def_file_filter_configure.bzl': BUILD file not found on package path\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel run -c opt tensorflow/lite/toco:toco -- --input_file=F:/ssd_mobilenet_v1_coco_2018_01_28/lite/tflite_graph.pb --output_file=F:/ssd_mobilenet_v1_coco_2018_01_28/lite/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n", "comments": ["Are you still observing this issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 25996, "title": "Excess amount of deprecated warning in tf1.13", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6.1810\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.0rc2\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 10/7.3.1\r\n- GPU model and memory: Titan Xp\r\n\r\n**Describe the current behavior**\r\n Even a script as simple as below have thrown 3 deprecated warnings\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput = tf.keras.layers.Input(shape=(20,))\r\noutput = tf.keras.layers.Dense(2)(input)\r\nmodel = tf.keras.models.Model(inputs=input, outputs=output)\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n\r\nmodel.fit(np.random.normal(0, 1, (200, 20)), np.random.normal(0, 1, (200, 2)))\r\n```\r\n\r\n```\r\nWARNING:tensorflow:From /home/henrysky/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n```\r\n\r\n```\r\nWARNING:tensorflow:From /home/henrysky/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n```\r\n\r\n```\r\nWARNING:tensorflow:From /home/henrysky/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n```\r\n\r\n**Describe the expected behavior**\r\nLittle to no deprecated warning. I mean it is deprecated warning should be shown to user when user write their own code using low level tensorflow function. But the code as simple as above using high level function thrown 3 warnings??\r\n\r\n**Code to reproduce the issue**\r\nsee above\r\n\r\n**Other info / logs**\r\nsee above\r\n", "comments": ["Welcome to contribute. Would you like to fix those depressed warning? ", "> Welcome to contribute. Would you like to fix those depressed warning?\r\n\r\nAt least those warnings about tf.cast are fixed in master branch already. Is it possible to cherry pick them to r1.13 branch??", "I'm afraid not since r1.3 branch had been cut before. Would you like to use tf-nightly? Or just wait for next version? Sorry for the inconvenience. ", "It is okay for me, just a bit annoying to see all these deprecated warning. But it is probably not ideal for new users using tensorflow keras to see all these warnings and confuse them.\r\n\r\nI think tensorflow should make sure all these deprecated warnings are fixed in the next 1.x or 2.x release.", "Agreed. See more discussion in https://groups.google.com/a/tensorflow.org/forum/#!msg/developers/C2c6T9Hrh4I/RayJkLPbAgAJ\r\n\r\nLet's close the issue because some deprecated warnings have been fixed in the master branch as you said?", "ok", "I have the same issue in TF 1.13.1.\r\n\r\n```\r\nWARNING\tFrom C:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n```\r\n\r\n```\r\nWARNING\tFrom C:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n```\r\n\r\n```\r\nWARNING\tFrom C:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n```\r\n\r\n\r\n```\r\nWARNING\tFrom C:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\n```", "Hi, @bwolfus . I think some deprecated warnings has been clear in the master branch, and it will take effect in the next release (perhaps 1.14 or 2.0). And if not, we welcome contributions to fix it.", "Hi, @facaiy. Thank you for your answer. In that case I will wait for the new version. ", "I think this is totally wrong direction you guys took here. Its like a man from a street turns into an aerospace company and demands to switch off all the low fuel warnings in all aircrafts because they distract him from watching naughty movies..\r\n\r\nwhat you might want to do is to have a warning level users can set on tensorflow session (similar to C compiler).. like use warnings_level=0 to no warnings.", "I call it Tensorflow MAX, no worries about the warnings .. ! ", "> I think this is totally wrong direction you guys took here. Its like a man from a street turns into an aerospace company and demands to switch off all the low fuel warnings in all aircrafts because they distract him from watching naughty movies..\r\n> \r\n> what you might want to do is to have a warning level users can set on tensorflow session (similar to C compiler).. like use warnings_level=0 to no warnings.\r\n\r\nYou might misunderstand us. What we mean here is to fix those deprecated warnings (replace the deprecated op by its new candidate).", "> I think this is totally wrong direction you guys took here. Its like a man from a street turns into an aerospace company and demands to switch off all the low fuel warnings in all aircrafts because they distract him from watching naughty movies..\r\n> \r\n> what you might want to do is to have a warning level users can set on tensorflow session (similar to C compiler).. like use warnings_level=0 to no warnings.\r\n\r\nMore likely aerospace company don't use their own product with 130k stars on GH, which has blinking warning lights all the time and no fucks given because opensource.", "```\r\nimport tensorflow\r\nimport logging\r\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\r\n```", "@iperov I've tried your pasted solution but still got annoying warnings.", "@zchrissirhcz \r\n\r\n```\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # tf log errors only\r\nimport tensorflow as tf\r\nimport logging\r\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\r\n```\r\nI don't have any warnings. Tested in DeepFaceLab in tf 1.13.2 .", "`tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)`\r\nis the only one and correct method. Works on any version too.", "@evrial No, what you paste is still not working. Actually what I'm using in the very begging of my imports is:\r\n```Python\r\n# coding: utf-8\r\nimport tensorflow as tf\r\nimport os\r\n\r\nif type(tf.contrib) != type(tf): tf.contrib._warning = None\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\n# Only use CPU:\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n```\r\n\r\nAnd after re-directing the stdout of my program to a file, the console still gives warning output, e.g.\r\n```\r\nWARNING:tensorflow:From /home/zz/soft/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /home/zz/soft/miniconda3/lib/python3.7/site-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.remove_training_nodes\r\nWARNING:tensorflow:From /home/zz/soft/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n```", "@iperov\r\n\r\n> @zchrissirhcz\r\n> \r\n> ```\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # tf log errors only\r\n> import tensorflow as tf\r\n> import logging\r\n> logging.getLogger('tensorflow').setLevel(logging.ERROR)\r\n> ```\r\n> \r\n> I don't have any warnings. Tested in DeepFaceLab in tf 1.13.2 .\r\n\r\nYou don't have any warning doesn't mean other people don't get any warnings.", "> ```python\r\n> # coding: utf-8\r\n> import tensorflow as tf\r\n> import os\r\n> \r\n> if type(tf.contrib) != type(tf): tf.contrib._warning = None\r\n> tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n> \r\n> # Only use CPU:\r\n> os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\n> os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n> ```\r\n\r\nseriously you don't see what is wrong with your code?", "@iperov Please preciously describe what you mean \"wrong\". There's no error in my code, only stdout and warnings, and warnings are from TensorFlow inside. I don't thinks this is wrong, and I think people who reach here are all in situations like me: dislike the warnings throwed by TensorFlow and would like to turn off them. What you pasted **does** eliminate some common warnings, **However it is still not enough to eliminate all the warnings**.", "look at my code again. If you don't see the differences, wash your eyes and look again.", "@iperov If you mean what I should attention is:\r\n```\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n```\r\n\r\ninstead of mine:\r\n```\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n```\r\n\r\nThen, I think you don't understant what this switch mean. 3 standards for fatal message, while 2 standands for error. This different won't affect the warning output.", "![sdf](https://user-images.githubusercontent.com/8076202/84634057-517a8200-af02-11ea-87f9-6992e32339ad.jpg)\r\n", "@iperov  Actually I just tried remove my previous imports, and replace with you pasted, i.e.\r\n\r\n```pytho\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # tf log errors only\r\nimport tensorflow as tf\r\nimport logging\r\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\r\n```\r\n\r\nBut it is still warning. I think you just mis-understanding me.", "@iperov Actually I don't know DeepFaceLab until you mentioned it. It's not persuasive to prove what you paste is definitely correct for other people's program. You'd better say \"it would most possibly remove other people's warnings but not 100% sure\"."]}, {"number": 25995, "title": "Create temp", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25995) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 25994, "title": "ImportError: cannot import name cloud", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, using stock tensorflow \r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): gcc version 8.2.0 (Debian 8.2.0-20)\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rockpro64 4.4.132-1075-rockchip-ayufan-ga83beded8524 #1 SMP Thu Jul 26 08:22:22 UTC 2018 aarch64 aarch64 aarch64 GNU/Linux\r\nVERSION=\"18.04.1 LTS (Bionic Beaver)\"\r\nVERSION_ID=\"18.04\"\r\nVERSION_CODENAME=bionic\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 8.2.0-1ubuntu2~18.04) 8.2.0\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n== uname -a ImportError: cannot import name cloud=====================================================\r\nLinux rockpro64 4.4.132-1075-rockchip-ayufan-ga83beded8524 #1 SMP Thu Jul 26 08:22:22 UTC 2018 aarch64 aarch64 aarch64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                         1.16.1   \r\nprotobuf                      3.6.1    \r\ntensorflow                    1.12.0   \r\ntensorflow-aarch64            1.2      \r\ntensorflow-estimator          1.13.0rc0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nLimited tf.compat.v2.summary API due to missing TensorBoard installation\r\ntf.VERSION = 1.12.0\r\ntf.GIT_VERSION = v1.12.0-8122-g7328add9da\r\ntf.COMPILER_VERSION = v1.12.0-8122-g7328add9da\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\nrock64@rockpro64:~/Documents$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n**('v1.12.0-8122-g7328add9da', '1.12.0')**\r\n\r\n\r\nfollowing is the error. NO module name cloud.\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 31, in <module>\r\n    from tensorflow.contrib import cloud\r\n**ImportError: cannot import name cloud**\r\n", "comments": ["Please take a look at duplicate issue #23976", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25993, "title": "Enhancement to Dimension and TensorShape API documentation", "body": "This PR is intended to help beginners understand the functionality of both Dimension and TensorShape API better by providing additional notes about each function and making it self-explanatory.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25993) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "> I signed it!\r\n\r\n@SSaishruthi we did not see CLA , can you please make sure you use same username and email while signing CLA", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25993) for more info**.\n\n<!-- need_author_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25993) for more info**.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25993) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25993) for more info**.\n\n<!-- ok -->", "@rthadur CLA issue has been resolved.", "Hi @rthadur,\r\nIs this PR in review? \r\nTIA", "> Hi @rthadur,\r\n> Is this PR in review?\r\n> TIA\r\n\r\nyes, can you please resolve conflicts.", "@rthadur \r\nResolved ", "Working on the changes. I will focus more on TensorShape class changes", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 25992, "title": "Fix serialization naming for BatchNorm.", "body": "PiperOrigin-RevId: 234726512", "comments": []}, {"number": 25991, "title": "Preserve run_metadata when session run fails.", "body": "Fixes #25990 ", "comments": ["As mentioned in #25990, the issue only exists with sessions and not with tf.function in TF 2.0. Given that sessions are deprecated, I'll just close this PR."]}, {"number": 25990, "title": "run_metadata should be preserved when session run fails", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 9.0/7\r\n- GPU model and memory: Titan Xp 12GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nrun_metadata is empty when a session run fails.\r\n\r\n**Describe the expected behavior**\r\nrun_metadata should be saved even (especially?) when session run fails.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nwith tf.device(\"/GPU:0\"):\r\n    input = tf.random_uniform(dtype=tf.float32, shape=[5, 100, 100, 3], name=\"input\")\r\n    resize = tf.image.resize_bilinear(input, [99999, 99999], align_corners=True)  # OOM\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nwith tf.Session() as sess:\r\n    try:\r\n        sess.run(resize, options=run_options, run_metadata=run_metadata)\r\n    finally:\r\n        run_metadata_str = str(run_metadata)\r\n        if run_metadata_str:\r\n            print(run_metadata_str)\r\n        else:\r\n            print(\"No run_metadata\")\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@mrry Hi, Derek. What do you think? ", "It's a totally reasonable suggestion, but I don't think it will be a high priority with the [deprecation of `tf.Session` in TF 2.0](https://github.com/tensorflow/community/pull/20). If anything, it might make more sense to ensure this behavior on calls to a `tf.function`. ", "@mrry Is there any way to use RunMetadata with tf.function now? The doc you linked mentions a doc at go/tf-2.0-function-api but obviously it's not accessible externally.", "I tried the following code and it seems to be working as intended:\r\n\r\n```\r\nfrom tensorflow.python.eager import context\r\n\r\n@tf.function\r\ndef make_input():\r\n    return tf.random.uniform(dtype=tf.float32, shape=[5, 100, 100, 3], name=\"input\")\r\n\r\n@tf.function\r\ndef resize(input):\r\n    return tf.image.resize(input, [20000, 20000], align_corners=True)\r\n\r\ncontext.enable_run_metadata()\r\ntry:\r\n    with tf.device(\"/device:GPU:0\"):\r\n        output = resize(make_input())\r\nfinally:\r\n    print(context.export_run_metadata().step_stats)\r\n```\r\n\r\nI'll close the issue and if someone wants to have this behavior with sessions they can take a look at #25991 "]}, {"number": 25989, "title": "fix nightly MKL wheels build", "body": "This PR fixes that issue of installing some `python3.6` packages that have been renamed in `Ubuntu:18.04`. Currently there are no `python3.6-pip`, `python3.6-setuptools` and `python3.6-wheel` found in `Ubuntu:18.04` and they are simply renamed to `python3-pip`, `python3-setuptools` and `python3-wheel`.\r\n\r\n```\r\n$ docker run -it ubuntu:18.04 bash\r\nroot@21fb09f636d7:/# apt-get clean\r\nroot@21fb09f636d7:/# apt-get update\r\nGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\r\nGet:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\r\n...\r\nroot@21fb09f636d7:/# apt search ^python3.6-.*$\r\nSorting... Done\r\nFull Text Search... Done\r\npython3.6-dbg/bionic-updates 3.6.7-1~18.04 amd64\r\n  Debug Build of the Python Interpreter (version 3.6)\r\n\r\npython3.6-dev/bionic-updates 3.6.7-1~18.04 amd64\r\n  Header files and a static library for Python (v3.6)\r\n\r\npython3.6-doc/bionic-updates 3.6.7-1~18.04 all\r\n  Documentation for the high-level object-oriented language Python (v3.6)\r\n\r\npython3.6-examples/bionic-updates 3.6.7-1~18.04 all\r\n  Examples for the Python language (v3.6)\r\n\r\npython3.6-minimal/bionic-updates 3.6.7-1~18.04 amd64\r\n  Minimal subset of the Python language (version 3.6)\r\n\r\npython3.6-venv/bionic-updates 3.6.7-1~18.04 amd64\r\n  Interactive high-level object-oriented language (pyvenv binary, version 3.6)\r\n```\r\n\r\nAlso once `python3` is installed, user is only able to call `python3` from within shell and there is no `python` command, so in the case of `python3` we need to create the symlink, otherwise executable scripts that start with ```#!/usr/bin/env python``` would fail:\r\n\r\n```\r\nroot@21fb09f636d7:/# python\r\nbash: python: command not found\r\nroot@21fb09f636d7:/# python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```", "comments": ["@angersson  could you take a look? Also cc @claynerobison", "What nightly build does this fix?\r\n\r\nThere is a PR out right now to remove the old Dockerfile directory (https://github.com/tensorflow/tensorflow/pull/26113), which we don't use at all internally (as far as I know). You may need to pull the MKL Dockerfiles into the new directory (which @claynerobison was looking at, I think).", "No longer needed as the docker files are being reorganized."]}, {"number": 25988, "title": "[TF 2.0 API Docs] tf.keras.applications.MobileNetV2", "body": "**Existing URLs containing the issue:**\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2\r\n\r\n**Description of issue (what needs changing):**\r\n- __Correct Links__:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/python/keras/applications/__init__.py is incorrect\r\nThe correct link should be:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/__init__.py\r\n\r\n* __Clear Description__:\r\nThe description does not describe what this symbol does or when it should be \r\n\r\n* __Usage Example__:\r\nNo usage example is provided.\r\n\r\n* __Parameters Defined__:\r\nParameters are not defined.\r\n\r\n* __Returns Defined__:\r\nReturns are not defined.\r\n\r\n* __Raises Listed and Defined__:\r\nErrors are not listed or defined.\r\n\r\n* __Visuals, if Applicable__:\r\nNo visuals are included.\r\n", "comments": ["Since the actual model implementation, including parameters, returns, and errors, is in `keras_applications` (https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py), I have a few questions: \r\n* What would be the best approach to reference this in TF docs?\r\n* Is it still worth pointing the definition link to `__init__.py` in `python/keras/applications`? \r\n* Is there already a plan for replicating/sharing documentation between Keras and tf.keras?\r\n", "@JTunis Thanks so much for the interest in contributing to TF 2.0 API documentation! We appreciate it, and these are excellent questions. I believe the best approach for modifying this endpoint in TF 2.0 docs would be to modify the docstrings for the [`mobilenet_v2.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/mobilenet_v2.py) file in `tensorflow/tensorflow`. \r\n\r\nAll of the documentation for the models listed in [`tf.keras.applications`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications) (VGG16, Xception, etc.) is currently pointing at the [`__init__.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/__init__.py) file; we'll want to make similar changes for each of them, as well, or confirm that we can safely point to the [documentation you referenced](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py) in the Keras project. There is no current plan to migrate documentation from Keras endpoints to their matching `tf.keras` symbols, but we would love to have help!\r\n\r\n**Note**: these models are currently in the process of being migrated to TF 2.0: (https://github.com/tensorflow/tensorflow/issues/25341).\r\n\r\ncc: @MarkDaoust @lamberta to make sure that I'm pointing you in the right direction. \ud83d\ude42 ", "Happy to take over the documentation of all `tf.keras.applications`. I do think I'd like to play with pointing to `keras-applications`, so that changes are reflected in the TF2.0 docs, or, minimally, adding a link to the actual implementations.\r\n\r\nMaybe I'll start with migrating documentation from `keras-applications` to `tf.keras.applications` manually so that there's some useful documentation for users, then depending on the progress of #25341, work on either pointing to `tf.keras` implementations or `keras-applications` implementations.", "@JTunis would love to help as well! Need help with anything?", "@Sri-vatsa Totally! I've been struggling to figure out how to point the docs generator to the appropriate file to generate the markdown docs. Some things that I've poked:\r\n* `generate2.py` located [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/generate2.py). As it's looping through the results of the parser, it seems to be generating the doc from the markdown from the first appearance of the MobileNetV2 API that it encounters, which seems to be from `python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py`. Unfortunately, we can't change the docs there because the file is auto-generated from [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/tools/api/generator)\r\n* Maybe I'm making this harder than it has to be, but simply changing docstrings in `mobilenet_v2.py` doesn't change the generated markdown files.\r\n\r\nLet me know if you can figure out how to resolve this, then it should be pretty easy to start banging out the docs for all of `tf.keras.applications`", "@JTunis I haven't had a lot of time to look at it yet but from an initial examination, it seems like the docstrings for Keras files have a different format than what is used in Tensorflow. \r\nTake a look at the corresponding docstring [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py) for `tf.data.dataset` api [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset). \r\n\r\nFor Keras, the modules are injected from `keras.applications.MobileNetV2` [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py). So, I believe that your approach of modifying the`mobilenet_v2.py` might be the solution. Can I check what you have tried changing in `mobilenet_v2.py`?", "@JTunis Figured out how to fix the issue. It is an issue with docstrings within Python Decorators. I have tested the fix locally by checking the readme files generated through `generate2.py`. It is a simple fix by wrapping the Keras functions using the `wraps` decorator from Python's `functools` library.  \r\n\r\n1) Modify `tensorflow/tensorflow/python/keras/applications/__init__.py` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/__init__.py). Import `wraps` from `functools` by adding `from functools import wraps` to the top of the file. Add `@wraps(base_fun)` to the function `keras_modules_injection(base_fun)` \r\n\r\n2) Add docstrings to `mobilenet_v2.py` or any other models in `tf.keras.applications`.\r\n\r\n@dynamicwebpaige Regarding the documentation for `tf.keras.applications`, what is the best way to update the documentation? For now, I am thinking of copying the relevant docstrings from `keras_applications`. Would that be alright?\r\n\r\nI will clean things up locally and make a PR over this weekend.", "@Sri-vatsa Awesome, once your PR is in we can work to divvy up the rest of the the `tf.keras.application` models. \r\n\r\nAs far as the docs for each model, I was also thinking of mostly copying relevant docstrings from `keras_applications` and maybe also linking to the implementation (at least until the `tf.keras` implementations are done). There was also a good transfer learning with `tf.keras.applications` models example that I wanted to link to, but I can't seem to find it right now.\r\n\r\nAlso not sure if it's worth editing the models' arguments to match those of `keras_applications` rather than just taking `*args` and `**kwargs`."]}, {"number": 25987, "title": "TypeError: 'NoneType' object is not subscriptable", "body": "See #25985\r\n\r\nCredit: https://github.com/keras-team/keras/pull/11149", "comments": ["@fchollet Thanks for the review, I had added a unit test for reproducing this problem by throwing an exception, and also it will be passed after this PR.\r\n\r\nBTW: I had read your awesome book _Deep Learning with Python_ months ago, I love it very much and thank you very much for that great book! :)", "Close this PR because the related issue had been fixed by internal commits from Googlers."]}, {"number": 25986, "title": "Update version script for support TF 2.0", "body": "", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25985, "title": "reset_states() failure in a stateful network with initial_states set and training in batch - TypeError: 'NoneType' object is not subscriptable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190217\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GTX 1080 8G\r\n\r\n**Describe the current behavior**\r\n\r\nAs @manojrege said from https://github.com/keras-team/keras/issues/11148, when we use `initial_states` with RNN in some case, we will get an exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/manoj/Desktop/repos/yane/yane/LSTM/manytomanyLSTM.py\", line 137, in <module>\r\n    incremental_train(space)\r\n  File \"/Users/manoj/Desktop/repos/yane/yane/LSTM/manytomanyLSTM.py\", line 128, in incremental_train\r\n    model.reset_states()\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/engine/topology.py\", line 1968, in reset_states\r\n    layer.reset_states()\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 681, in reset_states\r\n    batch_size = self.input_spec[0].shape[0]        \r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\nThere's another issue talking about this problem at #25852\r\n\r\n**Describe the expected behavior**\r\n\r\nShould not throw exception.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\n# import pdb; pdb.set_trace()\r\ninputs = tf.keras.layers.Input(batch_shape=(1, 1, 1))\r\n\r\nstate_h = tf.keras.layers.Input(batch_shape=(1, 1))\r\nstate_c = tf.keras.layers.Input(batch_shape=(1, 1))\r\n\r\nstates = [state_h, state_c]\r\n\r\ndecoder_out = tf.keras.layers.LSTM(1, stateful=True)(\r\n    inputs,\r\n    initial_state=states\r\n)\r\n\r\nmodel = tf.keras.Model([inputs, state_h, state_c], decoder_out)\r\nmodel.reset_states()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI can confirm that the Pull Request https://github.com/keras-team/keras/pull/11149/files can fix this problem.\r\n", "comments": ["This should be now fixed by https://github.com/tensorflow/tensorflow/commit/83df61b4d4ad11f3b8cf05ee98d29e6fb5e25506.", "Thanks, that's awesome."]}, {"number": 25984, "title": "Tensorflow GPU installation issues", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Through PyCharm virtualenv\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10 / cuDNN 7.4.2\r\n- GPU model and memory: GeForce GTX 1050 Ti\r\n\r\nTrying to install and run tensorflow gpu version. I have installed CUDA and cuDNN and run the deviceQuery sample with what seems like good results (picture attached). I have installed tensorflow and Keras through PyCharm and when I try to run the following two lines to see if TF is installed correctly I get the following error message:\r\n\r\nimport tensorflow as tf\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Mr.Jones/Documents/Master Thesis/Offline_models/define_models_Jonas.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mr.Jones\\Documents\\Master Thesis\\Offline_models\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n![image](https://user-images.githubusercontent.com/17160539/53198308-626ff780-361c-11e9-80bb-5490b771776c.png)\r\n\r\n", "comments": ["I see that you have cuda 10 installed. Please switch to TensorFlow 1.13.0-rc2 as it supports cuda 10.", "I installed TensorFlow 1.13.0-rc2 but still got the same error", "Did you update the cuda, cudnn path as per the [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup)?", "Yee cuda, cudnn together with the correct paths are added as described in the guide.\r\n\r\nCan there be any problem due to me trying to run it all in PyCharm?", "Yes its possible that PyCharm interpreter is missing those paths. Are you able to import tensorflow using your terminal?", "I tried it today but ended up with the same error: \r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Test1.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Mr.Jones\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "Can you attach a screenshot showing environment variables to cuda path?\r\nThis issue is a duplicate of #26059 . Please take a look. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "![image](https://user-images.githubusercontent.com/17160539/54088373-cb19dc80-435d-11e9-9688-352225726db0.png)\r\nThere is a screenshot of it, I tried to re-install everything today but still no luck", "Additionally I've managed to install tensorflow cpu on a laptop and it works fine there. Neither cpu or gpu is working on my stationary though...", "You need to add sub folders: ```bin``` , ```include``` and ```lib``` to the path. I see that ```lib``` and ```include``` folder's are not added to the path. Can you please try adding them as well?", "These might have been removed during one of the re-installations. Both are added again but I wasn't sure if it was lib or lib\\x64 so both are added:\r\n\r\n![image](https://user-images.githubusercontent.com/17160539/54179199-6616d180-4498-11e9-8a26-1b3a410d800f.png)\r\n\r\nHowever it still gives me the same error"]}, {"number": 25983, "title": "Compiling model with tf.keras.optimizers.SGD optimiser in eager execution mode throws an exception", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nWhen trying to build a simple model in eager execution mode using SGD as an optimiser the following exception is thrown:\r\n```ValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.SGD'>```\r\n\r\n**Describe the expected behavior**\r\nI'd expect the SGD optimiser to be usable in eager execution mode.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(50, input_shape=(10, )))\r\nmodel.add(tf.keras.layers.Activation('relu'))\r\nmodel.add(tf.keras.layers.Dense(3))\r\nmodel.add(tf.keras.layers.Activation('sigmoid'))\r\n\r\nsgd = tf.keras.optimizers.SGD(lr=0.1, decay=0.000225, momentum=0.5)\r\n\r\nmodel.compile(optimizer=sgd,\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n```\r\n\r\n**Other info / logs**---------------------------------------------------------------------------\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-3ec239d3547d> in <module>\r\n     12 model.compile(optimizer=sgd,\r\n     13               loss='categorical_crossentropy',\r\n---> 14               metrics=['accuracy'])\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    472     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    473     try:\r\n--> 474       method(self, *args, **kwargs)\r\n    475     finally:\r\n    476       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    408       raise ValueError(\r\n    409           'optimizer must be an instance of tf.train.Optimizer, not '\r\n--> 410           'a %s' % type(optimizer))\r\n    411 \r\n    412     self.optimizer = optimizers.get(optimizer)\r\n\r\nValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.SGD'>\r\n```\r\n", "comments": ["@mjarosie With tf 1.13.0rc2 I am not able to repeat the error. I think the issue may have been resolved. Could you retry with `tensorflow==1.13.0rc2`?", "I confirm - the problem doesn't persist. I'm closing the issue then. Cheers!", "@mjarosie I have the same problem. Can you help me plz?", "@ranajkh have you tried updating Tensorflow?", "@mjarosie my Tensorflow version is '2.1.0' .\r\n"]}, {"number": 25982, "title": "Update Go test documentation to include all required library files", "body": "The README for running the Go tests includes instructions for installing `libtensorflow.so` but does not mention that `libtensorflow_framework.so` is also required. This PR adds a note about this in prose and updates the command to explicitly install this shared object file.\r\n\r\nI would note that the `cp` command that I added could be combined into a single `cp` command with a wildcard. I left them separate for now to further illustrate that both files are needed but I'm happy to update the commands if that would be desirable.", "comments": []}, {"number": 25981, "title": "Stateful version of contrib.summary.always_record_summaries", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.0-dev20190219'\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen using summaries in the V2 system, summaries are meant to be written in a context manager created by `summary.always_record_summaries()` or `record_summaries_every_n_global_steps`:\r\n\r\n```python\r\nfrom tensorflow.contrib import summary\r\n...\r\nwith summary.always_record_summaries():\r\n    summary.scalar(...)\r\n```\r\n\r\nI think it would be useful to have a non-context version of these functions as well. Something like\r\n\r\n```python\r\nsummary.do_always_record_summaries(True)\r\nsummary.scalar(...)  # This will write to the event file\r\n```\r\n\r\nThis is analogous to how summary writers have both a `as_default()` method for creating a context manager and a `set_as_default` for changing global state.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThe`contrib.summary` module (which I believe is slated to become the main `summary` module in 2.0) will have new methods added.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople who are interactively using TensorFlow (especially in eager mode) from a REPL or notebook for small-scale experiments or learning TensorFlow. It's (IMO) useful to write summaries in a quick one-off fashion for immediate visualization in tensorboard. It's tedious on the REPL to make sure all those  calls are wrapped in a wordy and easy-to-forget `with summary.always_record_summaries(): ...`\r\n\r\n\r\n\r\n**Any Other info.**\r\n", "comments": ["@malmaud Could you post this in Tensorboard issue [here](https://github.com/tensorflow/tensorboard/issues). Tensorboard group is very active so you get faster response. Thanks!", "I could, but I'm a little confused - this issue is about the TensorFlow API for writing event files, not  about consuming or visualizing them. So I'm a little unsure how it relates to tensorboard. ", "@malmaud Even EventFileWriter is also taken care by Tensorboard team. Please open it there. Thanks!", "I see, thank you! "]}, {"number": 25980, "title": "Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor", "body": "Hi,\r\nI am using python 3.6.8 and the last keras and tensorflow (1.12.0)version.\r\nWhen I build something like that, it works:\r\n\r\n```\r\nx=keras.layers.Input(shape=(100,),sparse=False)\r\nx1=keras.layers.Dropout(0.2)(x)\r\n```\r\n\r\nBut the following code would give an error:\r\n```\r\nx=keras.layers.Input(shape=(100,),sparse=True)\r\nx1=keras.layers.Dropout(0.2)(x)\r\n```\r\n\r\n\r\nThe detailed error is \r\n_TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"input_30/indices:0\", shape=(?, 2), dtype=int64), values=Tensor(\"dropout_28/cond/mul:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"input_30/shape:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type._\r\n\r\nI think it is a bug but I am not sure.\r\n\r\nThanks a lot.\r\n\r\nRegards.\r\n", "comments": ["When I tried with tf-nightly, could not reproduce. I think the issue may have been fixed. Could you retry with tf-nightly?", "@yongtang I tried with tf-nightly (version:1.13.0-dev20190221) the issue still persists.", "Thanks for your replies guys.\r\nTo be more specific, I am doing it on the mkl version of tensorflow (not the GPU one).", "I encounter a same error, so how to deal with it? please?\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.util.deprecation_wrapper.DeprecationWrapper'> to Tensor. Contents: <module 'tensorflow' from '/home/xiongyuanpeng/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py'>. Consider casting elements to a supported type.", "I have the same issue with tf 2.2.0 \r\nI tried to convert the Sparse tensors by hand what worked out fine, os they definitely can e converted. No operator error yet.", "why not use tf.sparse.to_dense()? worked for me. ", "I need sparse tensors as usual tensors are too large. is the a possibility to use to_dense() in the Keras model, i.e. just before usage?", "Just had the same issue\r\nImpossible to input sparse tensor to an input layer, it causes the conversion error", "Facing exact problem while following the guide (training ngram text classification model) at https://developers.google.com/machine-learning/guides/text-classification/step-4 Appreciate any inputs.. Thank you", "Facing the same problem in tensorflow 2.3 using tf.linalg.matmul(a,b, a_is_sparse=True,b_is_sparse=True)\r\n\r\n\r\n`File \".py\", line 147, in <module>\r\n    customer_features = tf.linalg.matmul(sparse_customer_product, sparse_product_features, a_is_sparse=True, b_is_sparse=True)\r\n  File \"--/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"--/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 3191, in matmul\r\n    a = ops.convert_to_tensor(a, name=\"a\")\r\n  File \"/--/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1499, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"--/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 338, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"--/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    allow_broadcast=True)\r\n  File \"--/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"--/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"--/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Attempt to convert a value (<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f2ce05f8860>) with an unsupported type (<class 'tensorflow.python.framework.sparse_tens\r\nor.SparseTensor'>) to a Tensor.`", "```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nprint(tf.__version__)\r\nheader_input = keras.Input(shape=(10,), name='header', sparse=True, batch_size=10)\r\nheader_features = keras.layers.Reshape((1, 10))(header_input)\r\n```\r\n\r\n```console\r\n2.3.1\r\n[...]\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"header/indices_3:0\", shape=(None, 2), dtype=int64), values=Tensor(\"header/values_3:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"header/shape_3:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.\r\n```\r\n\r\n**Workaround**\r\nI came up with a bad workaround:\r\n\r\nSince I would have ~630 GB of (dense) data simply converting my data to dense was not an option. But since my runtime isn't critical and I train my network for research purposes I wrote my own SparseToDense-Layer:\r\n\r\n```python\r\ntf.keras.layers.Lambda(tf.sparse.to_dense)(x)\r\n```\r\nor\r\n```python\r\ndef sparse_to_dense(value: Any):\r\n    if isinstance(value, tf.sparse.SparseTensor):\r\n        return tf.sparse.to_dense(value)\r\n    return value\r\n\r\ntf.keras.layers.Lambda(sparse_to_dense)(x)\r\n```\r\nor\r\n```python\r\nclass SparseToDense(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(SparseToDense, self).__init__()\r\n\r\n    def build(self, input_shape):\r\n        pass\r\n\r\n    def call(self, input: Any) -> object:\r\n        if isinstance(input, tf.sparse.SparseTensor):\r\n            return tf.sparse.to_dense(input)\r\n        return input\r\n```\r\n\r\nIt works, because it converts an input batch only on runtime and I am not running OOM (Out of Mana). This however comes with the downside of longer training and prediction times.\r\n", "I also have this problem, on a basic NLP problem.\r\nMy data is actually presented in numpy ndarrays.\r\nSo I do not even know why it assumes it is of dtype tf.sparse.\r\n\r\nAlso, I have not many Zeros in the np.ndarrays... Really strange, and no idea how to debug...", "> Facing exact problem while following the guide (training ngram text classification model) at https://developers.google.com/machine-learning/guides/text-classification/step-4 Appreciate any inputs.. Thank you\r\n\r\nI also have this problem when I followed this tutorial. Have you solved it?", "Hello, 2 years later I have the same issue, but not the same error (I am using TensorFlow 2.4.1):\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nheader_input = tf.keras.Input(shape=(10,), name='header', sparse=True, batch_size=10)\r\nx = header_input\r\nheader_features = tf.keras.layers.Dense(10)(x)\r\n```\r\n\r\nIs working fine\r\n\r\nBut when I use dropout:\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nheader_input = tf.keras.Input(shape=(10,), name='header', sparse=True, batch_size=10)\r\nx = tf.keras.layers.Dropout(0.1)(header_input)\r\nheader_features = tf.keras.layers.Dense(10)(x)\r\n```\r\n\r\nI have the following error:\r\n\r\n>   File \"/data/conda/baslad01/conda_envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\", line 1182, in build\r\n    raise ValueError('The last dimension of the inputs to `Dense` '\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n\r\n@AssassinTee Can you add more details on how to implement your solution in this case ? \r\nThanks a lot !", "> It works, because it converts an input batch only on runtime and I am not running OOM (Out of Mana). This however comes with the downside of longer training and prediction times.\r\n\r\nDoes this really slow down the runtime? I found this solution (tf.sparse.to_dense) to be the only viable solution at the moment. If I want to improve the runtime, what else can I do?", "> x=keras.layers.Input(shape=(100,),sparse=True)\r\n> x1=keras.layers.Dropout(0.2)(x)\r\n\r\nThis is fixed TF 2.4.1. See [gist](https://colab.research.google.com/gist/ymodak/36c744a25affa7c74147d5cccbf94140/git44970.ipynb) for reference. Thanks!\r\n\r\nFor other related issues it's better to create a new issue thread and tag this issue for reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25980\">No</a>\n", "It seems I encountered a similar problem when I tried the [Google Machine Learning Guide on Text Classification](https://developers.google.com/machine-learning/guides/text-classification/) \r\n\r\nAdding todense() solved it for me:\r\n\r\nx_train = vectorizer.fit_transform(train_texts).todense()\r\nx_val = vectorizer.transform(val_texts).todense()", "If you're getting this error while following the [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras) (as I was), then my problem was solved by setting the `infer_schema_shape` to `True` in the `SchemaGen` component.\r\n\r\nRelated: https://github.com/tensorflow/tfx/issues/1495#issuecomment-600603285", "> I also have this problem when I followed this tutorial. Have you solved it?\r\n\r\nThat makes three of us, as I was using TF v. 2.3.0 and following the same exact tutorial.\r\n", "Here you can find more related comments:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/47931#issue-836309758\r\n"]}, {"number": 25979, "title": "[Intel MKL]: Fixing a build issue in mkl_cpu_allocator", "body": "This PR fixes a build regression happened in mkl_cpu_allocator because of other commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/505b11b8c230e644cc4ff71f98a0b22e84514e7e#diff-b34307576896d87b6abd5a4c273d3fa6\r\n", "comments": ["Hi @penpornk, this is blocking us from building TF with MKL. Can you have a look at this fix?", "@rthadur Thank you! :)", "@penpornk @rthadur thank you so much for the quick response."]}, {"number": 25978, "title": "tf.keras.layers.conv3Dtranspose error when input size is None", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 / Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0 / 1.10.0  /  1.9.0\r\n- Python version: Python 3.6.7 \r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 8.0\r\n- GPU model and memory: Tesla M40 24GB\r\n\r\n**Describe the current behavior**\r\n\r\nConv3DTranspose layer is producing an error message when the input layer dimension size is given None.\r\n\r\n**Describe the expected behavior**\r\n\r\nLike other layers, Conv3DTranspose layer should automatically infer the dimension size as well.\r\n\r\n**Code to reproduce the issue**\r\n\r\nColab link to produce an error.\r\n[https://colab.research.google.com/drive/1ee0LncVZIr8U4mQvmTW9R14Fpv31syO4](https://colab.research.google.com/drive/1ee0LncVZIr8U4mQvmTW9R14Fpv31syO4)\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\n# Won't produce an error if we give explicitly the dimension size instead of None.\r\nx2 = tf.keras.layers.Input(shape=(None,None,None,3), name='image_input')\r\n\r\nx=tf.keras.layers.Conv3D(filters=64,kernel_size=(3,3,3),padding=\"same\",activation='relu',name='Conv1')(x2)\r\nx=tf.keras.layers.Conv3D(filters=64,kernel_size=(3,3,3),padding=\"same\",activation='relu',name='Conv2')(x)\r\n\r\nx=tf.keras.layers.Conv3DTranspose(filters=128,kernel_size=(3,3,3),padding=\"same\",activation='relu',name='Conv6')(x)\r\nx=tf.keras.layers.Conv3DTranspose(filters=64,kernel_size=(3,3,3),padding=\"same\",activation='relu',name='Conv8')(x)\r\n\r\nmod=tf.keras.models.Model(inputs=x2,outputs=x)\r\nmod.summary()\r\n\r\n```\r\n**Output**\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-3ec84051b623> in <module>()\r\n      6 x=tf.keras.layers.Conv3D(filters=4,kernel_size=(3,3,3),padding=\"same\",activation='relu',name='Conv2')(x)\r\n      7 \r\n----> 8 x=tf.keras.layers.Conv3DTranspose(filters=4,kernel_size=(3,3,3),padding=\"same\",activation='relu',name='Conv6')(x)\r\n      9 x=tf.keras.layers.Conv3DTranspose(filters=4,kernel_size=(3,3,3),padding=\"same\",activation='relu',name='Conv8')(x)\r\n     10 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    701 \r\n    702       if not in_deferred_mode:\r\n--> 703         outputs = self.call(inputs, *args, **kwargs)\r\n    704         if outputs is None:\r\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\r\n   1060       else:\r\n   1061         outputs_4d = array_ops.reshape(outputs, [\r\n-> 1062             outputs_shape[0], outputs_shape[1] * outputs_shape[2],\r\n   1063             outputs_shape[3], outputs_shape[4]\r\n   1064         ])\r\n\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\r\n\r\n\r\n```\r\n", "comments": ["I could reproduce with 1.13.0rc2 but not with tf-nightly. I think the issue has been fixed in the master branch.", "Yes, it has been fixed in #23004. Could you take tf-nightly a try, @vikashranjan26?", "@myron gets stuck in the same problem.", "Thank you @facaiy it is working as expected in tf-nightly.", "Closing as it's fixed ", "No it is still not fixed until today!", "Hi, here's my code:\r\n\r\ninput_img = keras.layers.Input( shape=( None, len(params2), len(params2), 1 ) ) # adapt this if using `channels_first` image data format\r\n\r\nconv1 = keras.layers.Conv3D(filters=32, kernel_size=(1, 3, 3), strides=(1, 1, 1), activation='selu', padding='same', data_format='channels_last', name='conv1')(input_img)\r\nconv2 = keras.layers.Conv3D(filters=64, kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv2')(conv1)\r\nconv3 = keras.layers.Conv3D(filters=128,kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv3')(conv2)\r\nconv4 = keras.layers.Conv3D(filters=256,kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv4')(conv3)\r\n\r\nconvlstm1 = keras.layers.ConvLSTM2D(filters=32, return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm1')(conv1)\r\nconvlstm2 = keras.layers.ConvLSTM2D(filters=64, return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm2')(conv2)\r\nconvlstm3 = keras.layers.ConvLSTM2D(filters=128,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm3')(conv3)\r\nconvlstm4 = keras.layers.ConvLSTM2D(filters=256,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm4')(conv4)\r\n\r\ndeconv4 = keras.layers.Conv3DTranspose(filters=128, kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv4')(convlstm4)\r\nconcat4 = keras.layers.Concatenate(axis=4, name='concat4')([convlstm3, deconv4])\r\ndeconv3 = keras.layers.Conv3DTranspose(filters=64 , kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv3')(concat4)\r\nconcat3 = keras.layers.Concatenate(axis=4, name='concat3')([convlstm2, deconv3])\r\ndeconv2 = keras.layers.Conv3DTranspose(filters=32 , kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same' , data_format='channels_last', name='deconv2')(concat3)\r\nconcat2 = keras.layers.Concatenate(axis=4, name='concat2')([convlstm1, deconv2])\r\ndeconv1 = keras.layers.Conv3DTranspose(filters=1  , kernel_size=(1, 3, 3), strides=(1, 1, 1), activation='selu', padding='same' , data_format='channels_last', name='deconv1')(concat2)\r\n\r\nautoencoder = keras.models.Model(input_img, deconv1)\r\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\r\nautoencoder.summary()\r\n\r\nand here's the error message:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-150-83622ca7a77b> in <module>\r\n     28     return autoencoder\r\n     29 \r\n---> 30 autoencoder1(1700)\r\n\r\n<ipython-input-150-83622ca7a77b> in autoencoder1(file_length)\r\n     14     convlstm4 = keras.layers.ConvLSTM2D(filters=256,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm4')(conv4)\r\n     15 \r\n---> 16     deconv4 = keras.layers.Conv3DTranspose(filters=128, kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv4')(convlstm4)\r\n     17     concat4 = keras.layers.Concatenate(axis=4, name='concat4')([convlstm3, deconv4])\r\n     18     deconv3 = keras.layers.Conv3DTranspose(filters=64 , kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv3')(concat4)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    552             # In graph mode, failure to build the layer's graph\r\n    553             # implies a user-side bug. We don't catch exceptions.\r\n--> 554             outputs = self.call(inputs, *args, **kwargs)\r\n    555           else:\r\n    556             try:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\r\n   1138       else:\r\n   1139         outputs_4d = array_ops.reshape(outputs, [\r\n-> 1140             outputs_shape[0], outputs_shape[1] * outputs_shape[2],\r\n   1141             outputs_shape[3], outputs_shape[4]\r\n   1142         ])\r\n\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'\r\n\r\nYou just said that this problem was solved. But I'm really not sure why I still have this problem today on April 2019.", "With tf-nightly 1.15.0-dev20190706, this problem is resolved", "> With tf-nightly 1.15.0-dev20190706, this problem is resolved\r\n\r\nThanks for telling us, @davidparkerhl :-)"]}]