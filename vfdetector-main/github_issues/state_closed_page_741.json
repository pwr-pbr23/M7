[{"number": 31340, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The following PR/commit breaks the --config=rocm build\r\n\r\n386a8d770269c7814b73af13521b8547b3ca481d\r\n\r\nChange StreamExecutor implementation for ROCm to cope with such interface change.", "comments": ["Fixed in commit 763049aef8bc8c725401052764e1630a0bf310ab"]}, {"number": 31339, "title": "Outer product documentation einsum", "body": "Outer product documentation in  `tf.einsum`", "comments": ["can you please merge this PR in #31338 ? , thanks for your contribution", "Done, merged into #31338 . Closing this PR"]}, {"number": 31338, "title": "outer product documentation for tensordot and einsum operations", "body": "Document the supported outer product operation in `tf.tensordot`.\r\n", "comments": ["merged in #31339 "]}, {"number": 31337, "title": "Update release notes for TensorFlow 1.14.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 1.14.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": ["Closing as the release notes have been updated once"]}, {"number": 31336, "title": "[tf.data] Cleanup of tf.data op definitions.", "body": "cl/223710135 renamed some datasets ops (adding \"Experimental\" prefix), which introduced a backwards compatibility issue. In particular, if a graph was saved using TensorFlow before cl/223710135, restoring the graph using TensorFlow built after cl/223710135 would fail if the graph contained any of the renamed ops. To address this issue, this CL reintroduces op definitions removed by cl/223710135 and uses `compat.forward_compatible` to transition away from ops that use the (misleading) \"Experimental\" prefix to the corresponding ones without the prefix.\r\n\r\nIn addition, this CL removes tf.data op definitions for ops that have no kernels.\r\n\r\nPiperOrigin-RevId: 256436076", "comments": []}, {"number": 31335, "title": "A custom operator get Segmentation Fault in tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version:\r\n3.6.8\r\n- GCC/Compiler version (if compiling from source):\r\ng++ 7.4.0\r\n\r\n**Describe the current behavior**\r\nWhen calling a custom op from a python function with tf.function, I got a segmentation fault.\r\nThe op run normally without tf.function.\r\n\r\n**Code to reproduce the issue**\r\nI implemented a custom op with [zero_out_op_kernel_1.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.cc) in tensorflow repository.\r\n\r\nI called the operator from the following python code.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n_zero_out_module = tf.load_op_library('custom_ops.so')\r\nzero_out = _zero_out_module.zero_out\r\n\r\n@tf.function\r\ndef make_zero(x):\r\n    return zero_out(x)\r\n\r\nc = tf.constant([4,2,8,9])\r\nres  = make_zero(c)\r\n```\r\n\r\n**Other info / logs**\r\nI confirmed that InferenceContext is NULL.\r\nThis bug is similar to #30494\r\n\r\n\r\n", "comments": ["@laket Can you share a standalone code to reproduce the issue? Currently the code throws the following error. So need `custom_ops.so`\r\n\r\n`NotFoundError: ./custom_ops.so: cannot open shared object file: No such file or directory`.\r\nThanks!", "I attach a C++ code and Makefile to build it.\r\n\r\nzero_out_op.cc\r\n```C++\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include <iostream>\r\nusing namespace tensorflow;  // NOLINT(build/namespaces)\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\r\n    using namespace std;\r\ncout << c << endl;\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    })\r\n    .Doc(R\"doc(\r\nZeros out all but the first value of a Tensor.\r\n\r\nzeroed: A Tensor whose first value is identical to `to_zero`, and 0\r\n  otherwise.\r\n)doc\");\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = nullptr;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output = output_tensor->template flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n    for (int i = 1; i < N; i++) {\r\n      output(i) = 0;\r\n    }\r\n\r\n    // Preserve the first input value.\r\n    if (N > 0) output(0) = input(0);\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n\r\n```\r\n\r\nMakefile\r\n```\r\nDIR := .\r\n\r\nTARGET_NAME := $(DIR)/custom_ops.so\r\n\r\nTF_CFLAGS=$(shell python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))')\r\nTF_LIB= $(shell python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))')\r\nCXX := g++\r\n\r\nSOURCES := zero_out_op.cc\r\n\r\nall: $(TARGET_NAME)\r\n\r\n$(TARGET_NAME):\r\n\t$(CXX) -std=c++11 -shared  $(SOURCES) -o $@ -fPIC ${TF_CFLAGS} ${TF_LIB}\r\n\r\nclean:\r\n\trm -rf $(TARGET_NAME)\r\n\r\nremake: clean all\r\n\r\n```", "I built tensorflow from source to investigate this bug.\r\nThe above code run normally with the tensorflow whether or not tf.function is attached. So I suspect that the difference of compile environments such as g++ version causes this problem.\r\n\r\nThe following is the detail of the tensorflow from source,\r\n\r\n- TensorFlow version (use command below):\r\nv2.0.0-beta1-0-g8e423e3d56 2.0.0-beta1\r\n- compile command\r\nbazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=v2 --config=opt  //tensorflow/tools/pip_package:build_pip_package\r\n-gcc version\r\ngcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n\r\n", "The issue here seems to be with incompatible compiler versions leading to the shape inference code being called improperly. @gunan @yifeif any idea what our recommendation is here?", "The pip package and the custom op have to be built with the exact same compiler.\r\nIf you are using our prebuilt pip packages, you should follow the instructions at github.com/tensorflow/custom-op to build your custom ops.", "I agree that using the exact same compiler is a solution for this problem. But it is not consistent with [the Adding a New Op document](https://www.tensorflow.org/guide/extend/op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation). If you encourage the solution, you should encourage readers to use the same compiler in the document.\r\n\r\nI found that this problem comes from recently changes. I confirmed that the above code runs normally with tensorflow 1.13.2 (in graph mode), and tensorflow 1.14.0 cause a segmentation fault.\r\n\r\nI think this is the same problem in #30494 . \r\n\r\n", "@lamberta @yifeif about the documentation.\r\n\r\nIt is certainly possible to see the behaviour you described. Subtle changes can cause changes in the ABI, and we certainly have seen similar issues where seemingly irrelevant changes have started causing Segmentation faults.", "The docker container provided by github.com/tensorflow/custom-op is a practical solution.\r\nSo I am concerned about documentations only. My environment is very popular (Ubuntu 18.04) and InferenceContext is a fundamental component. The solution should be mentioned as much as the warning about \"-D_GLIBCXX_USE_CXX11_ABI=0\".", "> The pip package and the custom op have to be built with the exact same compiler.\r\n\r\nIf, by \"pip package\", do you mean \"the TF binary installed via pip\"? I hope not! How would I even find what compiler was used to build it? That would make the prebuilt binaries close to useless on Linux. \r\n\r\nIf compiling TF from source is (effectively) necessary in order to write custom ops, that should be documented as such in big bright red caps :-)\r\n\r\n\r\n", "It is effectively necessary; otherwise you can use our docker images, which\nlet you reproduce exactly the build environment for our releases. This is a\ndownside of using C++ for TF, which has no stable ABI. We're working on a C\nAPI for custom ops which will not have this restriction.\n\nOn Mon, Aug 26, 2019 at 3:40 PM Steven Johnson <notifications@github.com>\nwrote:\n\n> The pip package and the custom op have to be built with the exact same\n> compiler.\n>\n> If, by \"pip package\", do you mean \"the TF binary installed via pip\"? I\n> hope not! How would I even find what compiler was used to build it? That\n> would make the prebuilt binaries close to useless on Linux.\n>\n> If compiling TF from source is (effectively) necessary in order to write\n> custom ops, that should be documented as such in big bright red caps :-)\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31335?email_source=notifications&email_token=AAABHRLSIAXTKTBMHHPJY43QGRLXRA5CNFSM4IJNAS62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5F4JHI#issuecomment-525059229>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNWPXLHZRXRS2XA74LQGRLXRANCNFSM4IJNAS6Q>\n> .\n>\n\n\n-- \n - Alex\n", "> It is effectively necessary; otherwise you can use our docker images, which let you reproduce exactly the build environment for our releases. \r\n\r\nUnderstood. May I strenuously suggest that you make this clear in the custom-op documentation? Knowing this ahead of time would have saved me several hours of frustration.\r\n\r\n", "@markdaoust @lamberta @yifeif \r\nWhile we have https://github.com/tensorflow/custom-op\r\nThe website still does not point to that guide:\r\nhttps://www.tensorflow.org/guide/extend/op#build_the_op_library", "Also, the guide as https://www.tensorflow.org/guide/extend/op also uses TF1.x syntax; it would be thoughtful to provide TF2-syntax code there as well.", "@laket\r\nIs this still an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31335\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31335\">No</a>\n"]}, {"number": 31334, "title": "Can't install a version of TF with filter_for_shard", "body": "Hey, \r\n\r\nI'm using a program which calls for filter_for_shard (tf.data.experimetnal.filter_for_shard). The API claims it should exist in TF 1.13, but I have not been able to get this to work (or find the file) with any installation of TF 1.13. Any ideas on what to install to get this working?\r\n\r\nThanks in advance!", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Hey! I solved this problem actually. My program was just looking at a different version which was 1.4 even though I had 1.3 installed elsewhere, so I just pointed it in the right direction. Thanks!"]}, {"number": 31333, "title": "Fix numpy warning with numpy 1.17.0+", "body": "This fix tries to address the issue raised in 30427 where\r\n`import tensorflow` caused the following warning with numpy 1.17.0+:\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n```\r\n\r\nThe issue was cause by changes in numpy: https://github.com/numpy/numpy/commit/ad1e0600e45b9fa71096d0a0f10c1474e003f373\r\n\r\nThis fix fixes the warning.\r\n\r\nThis fix fixes 30427.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31333) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 31332, "title": "\u52a0\u52a0\u52a0\u3001", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["sorry type wrong"]}, {"number": 31331, "title": "Error freezing saved model if it contains a tf.keras.layer.BatchNormalisation layer", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 1903\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.3\r\n- GPU model and memory: RTX 2080 Ti \r\n\r\n**Problem**\r\n\r\nTo make a frozen graph, I first create a saved model using `tf.saved_model.simple_save` and then freeze it using `tensorflow.python.tools.freeze_graph.freeze_graph`.\r\n\r\nIf a model contains some `tf.keras.layers.BatchNormalisation` layers, freezing will fail in TF 1.14.0 with:\r\n\r\n`ValueError: Tensor name 'batch_normalization/cond/ReadVariableOp/Switch:1' is invalid.`\r\n\r\nTF 1.13.1 does not give an error\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport os\r\nimport datetime\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, Activation, BatchNormalization\r\nfrom tensorflow.keras.models import Model\r\n\r\ninputs = Input(shape=(128, 128, 1))\r\nx = Conv2D(4, (3, 3))(inputs)\r\nx = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\nx = Flatten()(x)\r\nx = Dense(5, activation='softmax')(x)\r\nmodel = Model(inputs, x, name='test')\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\nK.set_learning_phase(0)\r\nsave_dir = \"./tmp_{:%Y-%m-%d_%H%M%S}\".format(datetime.datetime.now())\r\ntf.saved_model.simple_save(K.get_session(),\r\n                           save_dir,\r\n                           inputs={\"input\": model.inputs[0]},\r\n                           outputs={\"output\": model.outputs[0]})\r\n\r\nfreeze_graph.freeze_graph(None,\r\n                          None,\r\n                          None,\r\n                          None,\r\n                          model.outputs[0].op.name,\r\n                          None,\r\n                          None,\r\n                          os.path.join(save_dir, \"frozen_model.pb\"),\r\n                          False,\r\n                          \"\",\r\n                          input_saved_model_dir=save_dir)\r\n```\r\n\r\n**Update:**\r\n\r\nSeems to be a problem in `graph_util_impl.py`, in particular https://github.com/tensorflow/tensorflow/commit/0f486fc67070ba888204741c404a55a5f1a41fbc#diff-2d2827fd48cee6884e3587c901ad6952\r\n\r\n@gargn \r\n\r\nIf I change this file back to its 1.13 version there is no more error.\r\n\r\n**Update 2:**\r\n\r\nI put `K.set_learning_phase(0)` before creating the model, then it works. I don't know what effect this has though? Does it just turn off batch normalisation altogether?\r\n\r\n**Update 3**\r\n\r\nFinal remarks:\r\n- Putting `K.set_learning_phase(0)` before creating the model will let it save, however the Batch Normalisation layer doesn't seem to do anything (updating turned off?) so it is not a solution.\r\n- Changing `graph_util_impl.py` to its 1.13.1 version will let it save without error, however there will be an error `ValueError: Input 0 of node batch_normalization/cond/ReadVariableOp/Switch was passed float from batch_normalization/gamma:0 incompatible with expected resource.` when loading the frozen graph from the protobuf.\r\n- The workaround is to save the model weights, clear the session (so that tensor names are not different because of having two graphs), set learning phase to 0, recreate the model, load the weights, and then freeze (example code in my comment [below](https://github.com/tensorflow/tensorflow/issues/31331#issuecomment-518655879))\r\n\r\n", "comments": ["Same problem here with tensorflow-gpu 1.14.0, Ubuntu 16.04, Python 3.5.2.", "@wenshuangwang I found if I put `K.set_learning_phase(0)` before constructing the model, then it works. But I don't know how this affects training?! It seems the accuracy is lower afterwards.\r\n\r\nAnother way that works is to use normal keras instead of tensorflow.keras... hmmmm", "@geometrikal Thank you very much, it works for me as well. I think it's ok if training and freezing separately.\r\nI know the second way, but I must use tf.keras to use tensorflow_model_optimization.sparsity.", "@wenshuangwang Actually, I think putting `K.set_learning_phase(0)` before loading the model stops the batch normalisation layer from updating. I can tell a difference - I get worse accuracy (same as when not using batch normalisation) and the accuracy vs epochs graph looks different.", "@wenshuangwang I finally found a workaround:\r\n\r\n1. Create the model using a function (do *not* use `K.set_learning_phase(0)`):\r\n```\r\ndef create_model():\r\n    inputs = Input(...)\r\n    ...\r\n    return model\r\n\r\nmodel = create_model()\r\n```\r\n2. Train model\r\n3. Save weights: \r\n`model.save_weights(\"weights.h5\")`\r\n4. Clear session and set learning phase to 0:\r\n```\r\nK.clear_session()\r\nK.set_learning_phase(0)\r\n```\r\n5. Recreate model and load weights:\r\n```\r\nmodel = create_model()\r\nmodel.load_weights(\"weights.h5\")\r\n```\r\n6. Freeze as before\r\n\r\nI have confirmed this works with no errors for creating the frozen model protobuf. Also there are no errors when loading the model either (using `tf.import_graph_def`), and it is working in my production code (using tensorflow java 1.14)\r\n\r\nHere is full working example:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport os\r\nimport datetime\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, Activation, BatchNormalization, Lambda\r\nfrom tensorflow.keras.models import Model\r\n\r\n\r\ndef create_model():\r\n    inputs = Input(shape=(128, 128, 1))\r\n    x = Conv2D(4, (3, 3))(inputs)\r\n    x = BatchNormalization()(x)\r\n    # x = Lambda((lambda x: tf.layers.batch_normalization(x)))(x)\r\n    x = Activation('relu')(x)\r\n    x = Flatten()(x)\r\n    x = Dense(5, activation='softmax')(x)\r\n    model = Model(inputs, x, name='test')\r\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n    return model\r\n\r\n\r\nmodel = create_model()\r\n\r\n# Training goes here...\r\n\r\nmodel.save_weights(\"weights.h5\")\r\n\r\nK.clear_session()\r\nK.set_learning_phase(0)\r\n\r\nmodel = create_model()\r\nmodel.load_weights(\"weights.h5\")\r\n\r\nsave_dir = \"./tmp_{:%Y-%m-%d_%H%M%S}\".format(datetime.datetime.now())\r\ntf.saved_model.simple_save(K.get_session(),\r\n                           save_dir,\r\n                           inputs={\"input\": model.inputs[0]},\r\n                           outputs={\"output\": model.outputs[0]})\r\n\r\nfreeze_graph.freeze_graph(None,\r\n                          None,\r\n                          None,\r\n                          None,\r\n                          model.outputs[0].op.name,\r\n                          None,\r\n                          None,\r\n                          os.path.join(save_dir, \"frozen_model.pb\"),\r\n                          False,\r\n                          \"\",\r\n                          input_saved_model_dir=save_dir)\r\n```", "@geometrikal Yes, this is what I said to train and freeze the network separately. Thank you for explaining patiently.", "@geometrikal Is this resolved by the suggestion of @wenshuangwang . Can we close the issue? Thanks!", "@jvishnuvardhan I think this is a bug and should be left open.\r\n\r\nThe bug does not occur in normal keras, and it is clunky to have to create a copy of the graph and transfer the weights across just because you have a batch normalisation layer in there.\r\n\r\nIn TF 1.13.1 the models would be frozen without error, but this leads to an error when loading the frozen model: #3628", "I tested the code in the latest `tf-nightly` (`1.15.0.dev20190807`) and the model froze without issue.", "@gargn I tested as well with 1.15.0-dev20190807 to confirm. It does save without issue, but when loading the frozen graph there is a new error: `ValueError: Node 'batch_normalization/cond/ReadVariableOp/Switch' has an _output_shapes attribute inconsistent with the GraphDef for output #0: Shapes must be equal rank, but are 1 and 0`\r\n\r\nShould I open a new issue for this?\r\n\r\nCode for loading graph:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nimport tensorflow.keras.backend as K\r\n\r\nsource = \"./freeze_test/frozen_model.pb\"\r\n\r\nsession = K.get_session()\r\nwith gfile.Open(source, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    session.graph.as_default()\r\n    tf.import_graph_def(graph_def, name='')\r\n```", "@geometrikal As @gargn mentioned the original issue was resolved in `!pip install tf-nightly-gpu`. Please close the issue and open a new issue for the `loading graph` if it was not already resolved. I think `loading graph` was also resolved as I could not reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/b3a72cb75dd9b2ed5e05d260ad0247b0/tf_31331_freezing_savedmodel.ipynb). Thanks!", "@jvishnuvardhan `loading_graph` is not resolved - the code you are using in your gist is my workaround for the problem.\r\nI have opened a new issue #31668 with a bigger scope to cover the general issue of freezing and loading models with batch normalization.", "This should work on the latest nightly. Please reopen if you still have issues with running the script provided above.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31331\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31331\">No</a>\n", "import backend from tf.keras not simply keras, shown below:\r\nfrom tensorflow.keras import backend as K\r\n\r\n", "> import backend from tf.keras not simply keras, shown below:\r\n> from tensorflow.keras import backend as K\r\n\r\nawesome!\r\nit works", "I noticed if my model has conditional logic, the frozen model output is different from the keras model output. Any particular reason why that should happen?\r\nHere is a sample Custom Layer that is part of my model : \r\n\r\n```\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self, output_idx, **kwargs):\r\n        self.output_idx = output_idx\r\n        super().__init__(**kwargs)\r\n        \r\n    def build(self, input_shape):\r\n        super().build(input_shape)\r\n        \r\n    def call(self, x):\r\n        \r\n        y1 = p[0] + p[1]*x + p[2]*x*x + p[3]*x*x*x + p[4]*x*x*x*x + p[5]*x*x*x*x*x + p[6]*x*x*x*x*x*x + p[7]*x*x*x*x*x*x*x\r\n        y2 = p[8] + p[9]*x + p[10]*x*x + p[11]*x*x*x\r\n        y = tf.where(x>-5, y2, y1)\r\n        \r\n        return y\r\n\r\n\r\n    def get_config(self):    \r\n        config = super().get_config().copy()\r\n        config.update({\r\n            'output_idx' : self.output_idx\r\n        })\r\n        return config\r\n```\r\n\r\n "]}, {"number": 31330, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name:  An op that loads optimization parameters into HBM for embedding.", "body": "When I run the code I report the following problem\uff1a\r\n\r\n`from tensorflow.examples.tutorials.mnist import input_data`\r\n`dataSet = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)`\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/steven/idea/project/tensorflow/study01/testMinst.py\", line 6, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/__init__.py\", line 21, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 30, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 48, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\r\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.", "comments": []}, {"number": 31329, "title": "auto-deeplab: nas_network runs into ValueError with personal data", "body": "Hello,\r\n\r\nI've been trying to use auto deeplab on my own dataset. The only part I touched is the stem creation. Instead of feeding a image set of dimension [?, y, x, 3] I feed data of dimension [?, y, x, 13]. The first 10 features of the last dimension correspond to character embeddings, and the last 3 are unrelated floats in the [0, 1] range. There are no NANs in this dataset.\r\n\r\nTo do this, I replace the beginning of the _nas_net function in nas_network.py. I get an array with shape [?, y, x, 4] as input, the first feature is a char ID to be replaced by a 10 dimensional embedding, the 3 other features stay as they are. This leads to a [?, y, x, 13] tensor which is then used by the rest of the code.\r\n\r\nAs my input seems sensible enough, I believe this may be a bug with the implementation. Could someone investigate this further, please?\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tried on both ubuntu 18.04 and windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.1/ 7.6\r\n- GPU model and memory: Nvidia Geforce GTX 1080TI 11GB on ubuntu, CPU on windows\r\n\r\n**Describe the current behavior**\r\nprogram crashes with error pasted at the bottom of this issue\r\n\r\n**Describe the expected behavior**\r\nauto deeplab keeps going as expected\r\n\r\n**Code to reproduce the issue**\r\nreplace the content of /tensorflow/models/research/deeplab/core/nas_network.py by [this code](https://pastebin.com/jFu8EUxT) and run nas_network.py directly. The only modification I brought to this file, besides the new __main__ part at the bottom to run a simple fail case, can be found in the _nas_stem function.\r\n\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Users/lrizzello/source/repos/Company/Company.Python/deeplab/core/nas_network.py\", line 308, in <module>\r\n    hnasnet(all_features, pipeline_parameters, n_classes)\r\n  File \"C:/Users/lrizzello/source/repos/Company/Company.Python/deeplab/core/nas_network.py\", line 294, in hnasnet\r\n    final_endpoint=final_endpoint)\r\n  File \"C:/Users/lrizzello/source/repos/Company/Company.Python/deeplab/core/nas_network.py\", line 183, in _build_nas_base\r\n    cell_num=cell_num)\r\n  File \"C:\\Users\\lrizzello\\source\\repos\\Company\\Company.Python\\deeplab\\core\\nas_cell.py\", line 82, in __call__\r\n    h = h1 + h2\r\n  File \"C:\\Users\\lrizzello\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 812, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"C:\\Users\\lrizzello\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 374, in add\r\n    \"Add\", x=x, y=y, name=name)\r\n  File \"C:\\Users\\lrizzello\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\lrizzello\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\lrizzello\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\lrizzello\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1823, in __init__\r\n    control_input_ops)\r\n  File \"C:\\Users\\lrizzello\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1662, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Dimensions must be equal, but are 14 and 13 for 'hnasnet/cell_5/comb_iter_0/combine/add' (op: 'Add') with input shapes: [?,14,25,40], [?,13,25,40].\r\n```\r\n", "comments": ["@lrizzello ,\r\nAfter cloning and executing I got the following error `ModuleNotFoundError: No module named 'deeplab'` can you kindly help me resolving this?Thanks!\r\n", "@anush-o \r\nThanks for your reply, I just tried running my script directly from a clone of this repo and I can indeed see this error, even when moving the working directory to the right spot.\r\n\r\nFor now, the simplest way around this I can propose is to simply move the deeplab folder out of its parent folder. Fixed that error for me.\r\n\r\n![image](https://user-images.githubusercontent.com/53616818/62537771-8c774800-b851-11e9-8bf0-a2f929b83bf1.png)\r\n", "@lrizzello ,\r\nEven after following the same steps as per your instruction with reference to screenshot below, same issue is being faced. \r\n![issuee](https://user-images.githubusercontent.com/52397990/62770614-ffcec300-bab8-11e9-9094-6b58cecdb9d9.png)\r\n", "If you're calling it from a command line, try adding\r\n\r\n```\r\nimport sys\r\nsys.path.append(\"\")\r\n```\r\nto nas_network.py, before the deeplab imports", "@lrizzello Please post this issue in [models repo](https://github.com/tensorflow/models/issues) where the codeowners related to `deeplab` model will be able to resolve your issue. Thanks!", "ok, I'll post it there", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31329\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31329\">No</a>\n"]}, {"number": 31328, "title": "I wonder if the tflite supports GCN(Graph Convolution Network)?", "body": "", "comments": ["I'm not familiar with GCN, did you have any issues when trying to convert GCN model or does it requires a new operator?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I'm still interested in this. Since we are wondering whether to use Graph Convolutional Networks for skeleton based action recognition. But it would be useless if those networks cannot be run on mobile GPUs using tflite."]}, {"number": 31327, "title": "libtensorflow_framework.so No such file or directory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Py36\r\n- Bazel version (if compiling from source): bazel release 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI tried to include the op in the code as follows\r\n```\r\n    idtable_op_module = tf.load_op_library(\r\n    resource_loader.get_path_to_datafile('libid_table_op.so'))\r\n```\r\n\r\nonly to get the traceback infomation:\r\n```\r\nTraceback (most recent call last):\r\n  File \"wide_deep.py\", line 12, in <module>\r\n    import jarvis.tensorflow as jtf\r\n  File \"/opt/ml/job/python/jarvis/tensorflow/__init__.py\", line 4, in <module>\r\n    from jarvis.tensorflow.feature_transform import *\r\n  File \"/opt/ml/job/python/jarvis/tensorflow/feature_transform.py\", line 10, in <module>\r\n    from jarvis.tensorflow import id_column\r\n  File \"/opt/ml/job/python/jarvis/tensorflow/id_column.py\", line 44, in <module>\r\n    from jarvis.tensorflow.id_table_ops import IdHashTable\r\n  File \"/opt/ml/job/python/jarvis/tensorflow/id_table_ops.py\", line 29, in <module>\r\n    resource_loader.get_path_to_datafile('libid_table_op.so'))\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\nThe trackback shows that there's no `libtensorflow_framework.so`, I tried to find this file in tf 1.14.0 binary version(not compiled version) and only found `site-packages/tensorflow_core/libtensorflow_framework.so.1`, but still no `libtensorflow_framework.so`. So what's the correct way to include `so` file, is it a bug in tf 1.14?", "comments": ["You can use `tf.sysconfig.get_link_flags()` to compile the library correctly.\r\nThey changed the naming convention recently, see e.g. https://github.com/tensorflow/tensorflow/issues/27430", "@Spotlight0xff do you mind providing more details on how to exactly use `tf.sysconfig.get_link_flags()`?", "@Spotlight0xff Does it mean that the tf version that compiles the `so` file and the tf version that loading the compiled `so` file should be the same?", "@kasrayazdani @sjtusmartboy \r\nI think the docs cover this pretty good in the [guide to extend TF](https://www.tensorflow.org/guide/extend/op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation).\r\n\r\n`tf.sysconfig.get_link_flags()` provides the linker flags to pass to the compiler to compile the `.so` library.\r\n", "@Spotlight0xff So does it mean that the tf version that compiles the so file and the tf version that loading the compiled so file should be the same?", "I had the same problem and ended up downgrading my tf to 1.1.0 and CUDA to 9.2 and it works now.", "@sjtusmartboy Seems to be the case. After failing to compile kernels on 1.14 due to #30494, I managed to compile them for tf1.13.1 but not loading them on tf1.14. From suggestion in #31335 of using same compilers for building TF and custom op, only option to build custom ops + use AMP (#26342) seems to be to get TF from source.", "@masotrix Thanks a lot for advice, finally I also chose to downgrade to tf1.13.1 and it worked.", "As @Spotlight0xff mentioned, `tf.sysconfig.get_link_flags()` should contains the correct name for libtensorflow_framework. And you should build the custom op library against the TF pip version you want to run with. Closing this issue now. Let us know if you have more questions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31327\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31327\">No</a>\n"]}, {"number": 31326, "title": "Fix build error TensorFlow Lite for ARM64", "body": "Resolves #31320\r\n\r\nFix build error TensorFlow Lite for ARM64\r\n1.neon_tensor_utils.cc\r\nFix compile error.\r\n`error: invalid static_cast from type \u2018uint32x4_t {aka __vector(4) unsigned int}\u2019 to type \u2018int32x4_t {aka __vector(4) int}\u2019`\r\n\r\n2.tensorflow/lite/tools/make/Makefile\r\nFix quant_lstm_sup.cc path.", "comments": ["It is fixed it in both commits. (2f1d6107cc4c04d9e95f703113a68c82ed1cc0e4 and 864d2942feb2d38e85a55626c01408487cb46403)\r\nI cose pull request."]}, {"number": 31325, "title": "reduce_max on empty int32 array returns -2147483648", "body": "**System information**\r\n- Have I written custom code: **no**\r\n- OS Platform and Distribution: **Linux**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version: **1.14.0**\r\n- Python version: **2.7**\r\n\r\n**Describe the current behavior**\r\n```python\r\n# Call reduce_max on an empty Tensor.\r\nx = tf.reduce_max(tf.zeros([0], tf.int32))\r\nwith tf.Session() as sess:\r\n  print x.eval()\r\n# prints -2147483648\r\n```\r\n**Describe the expected behavior**\r\nWould be better if `reduce_max` threw an error when fed an empty Tensor.", "comments": ["I have tried on colab with TF version 1.14 and nightly versions was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1695UiZG2M9q8oKvEN0WGdwLjY1NXy6wm) here.Thanks!\r\n\r\n", "The test run into this snippet of code\r\nhttps://github.com/tensorflow/tensorflow/blob/0b180e77223a1819758945a9d09c6960c553a1f3/tensorflow/core/kernels/reduction_ops_common.h#L189-L194", "@jvishnuvardhan\r\nI believe we can close this issue now. Since the behavior is as expected.", "I am closing this issue as it is intended behavior. Thanks @Leslie-Fang . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31325\">No</a>\n"]}, {"number": 31324, "title": "Passing a Variable as learning_rate to Adam optimizer does not work as expected", "body": "tag:bug_template\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary through pip3\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: sys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.0.130_410.48 / 10.0-linux-x64-v7.4.2.24\r\n- GPU model and memory: GeForce GTX 1080 with 7598 MB memory\r\n\r\n**Describe the current behavior**\r\n\r\nIf a tf.Variable is passed as learning_rate to the Adam optimizer, and the variable is later changed that does not seem to affect the optimizer. Instead, the optimizer seems to \"cache\" the value of the variable at the time when the optimizer was constructed.\r\n\r\n**Describe the expected behavior**\r\n\r\nMy expectation was that if I pass a tf.Variable as the learning_rate argument to tf.keras.optimizers.Adam(), and later assign a new value to the variable, that would affect the optimization.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nimport sys\r\nprint(sys.version_info)\r\n\r\ntf_a = tf.Variable(1.0)\r\nprint('Variable tf_a initialized to {}.'.format(tf_a.numpy()))\r\n\r\ntf_lr = tf.Variable(0.1, trainable=False)\r\n\r\ntf_opt = tf.keras.optimizers.Adam(learning_rate=tf_lr)\r\n\r\n@tf.function\r\ndef train_step():\r\n    with tf.GradientTape() as tf_tape:\r\n        tf_loss = tf_a**2\r\n        \r\n    tf_gradients = tf_tape.gradient(tf_loss, [tf_a])\r\n\r\n    tf_opt.apply_gradients(zip(tf_gradients, [tf_a]))\r\n\r\nprint('After one step with learning rate {}... '.format(tf_lr.numpy()), end='')\r\ntrain_step()\r\nprint('Variable tf_a is {}.'.format(tf_a.numpy()))\r\n\r\ntf_lr.assign(0.0)\r\n\r\nfor _ in range(10):\r\n    print('After another step, now with learning rate {}... '.format(tf_lr.numpy()), end='')\r\n    train_step()\r\n    print('Variable tf_a is {}.'.format(tf_a.numpy()))\r\n```\r\n\r\nThe code above produces the following output on my system:\r\n\r\n```\r\nv2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\nsys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)\r\nVariable tf_a initialized to 1.0.\r\nAfter one step with learning rate 0.10000000149011612... Variable tf_a is 0.8999971747398376.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.8004083633422852.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.7015821933746338.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.6039347052574158.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.5079591274261475.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.41423195600509644.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.3234161138534546.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.23625943064689636.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.1535806804895401.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.07624538242816925.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.005127914249897003.\r\n```\r\nAs you can see tf_a keeps changing at a fast pace. My expectation was that after setting the learning rate variable to 0.0 updates would no-longer change tf_a.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I tried executing the code on Colab, was able to reproduce the issue with Tensorflow 2.0.0.beta1. Please see the gist [here](https://colab.research.google.com/drive/1jA-_Y3ZOprr6i_xIrTiXQWCmu3UyNKL0). Thanks! ", "Hi,\r\nThis is an interesting fact to point out, however I believe the proper way of setting up custom learning rates is to use an instance of a class inheriting `tf.keras.optimizers.schedules.LearningRateSchedule`, which returns a learning rate based on the ordinal of each optimization step, _e.g._ in your case:\r\n```python\r\nclass MyLrate(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n    def __init__(self, lrate=.1):\r\n        self.lrate = lrate\r\n\r\n    def __call__(self, step):\r\n        return tf.cond(tf.greater(step, 0), lambda: 0., lambda: self.lrate))\r\n```\r\n\r\nNote however that changing the `lrate` property of such an instance will not affect the optimizer's learning rate either. I think this is due to its depending on a graph node being create based on a single call to the passed object rather than on making calls at each step (_i.e._ in the formula, `self.lrate` will be fixed to the value in place when passed to the optimizer and not checked for at each step). There probably is a workaround to this, but I do not know it.", "Thanks, and yes, I filed a separate bug for the changing-lrate-has-no-effect-problem (#31323).\r\n\r\nThe workaround I've found is to assign to the learning_rate property on the optimizer object, as in:\r\n```\r\ntf_opt.learning_rate.assign(0.0)\r\n```\r\nSeems to work, but can't see that it's documented anywhere.", "Nice workaround! It would indeed be worth documenting.", "Interesting. \r\n\r\nI searched the code a little. In both the v1 and v2 optimizers, if you pass a variable as the initial learning rate it gets used as the initial value for the real learning rate variable.\r\n\r\n[Optimizer v1 here](https://github.com/tensorflow/tensorflow/blob/a062a8c1f6a89c88a908813add05a0bfd5d523b9/tensorflow/python/keras/optimizers.py#L176)\r\n\r\n[Optimizer V2 here](https://github.com/tensorflow/tensorflow/blob/a062a8c1f6a89c88a908813add05a0bfd5d523b9/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L606)\r\n\r\nThis should probably warn, fail or use the passed variable.\r\n\r\n@tanzhenyu It looks like you wrote a lot of the optimizer_v2 code. What do you think?", "I will look into it. Something weird probably happened, it should capture the variable but seem it doesn't.\r\n\r\nMeanwhile to unblock you, you can do tf_opt.lr = 0.0 and that should work", "@bjornsing I think this was resolved in `TF2.0`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/dfcdb51d1f8123e38185461f56334726/tf31324.ipynb).\r\n\r\nOutput from your code with `TF2.0` is as follows. After setting the learning rate variable to 0.0, `tf_a` was constant as expected. \r\n \r\n```\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\nsys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\r\nVariable tf_a initialized to 1.0.\r\nAfter one step with learning rate 0.10000000149011612... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.9000001549720764.\r\n```\r\n\r\nI am closing this issue as it was resolved in `TF2,0`. Please feel free to open if the issue persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31324\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31324\">No</a>\n", "Oh sorry to update this. Yes it is fixed before our official launch (yesterday)"]}, {"number": 31323, "title": "Passing a callable learning_rate to Adam optimizer does not work as documented", "body": "tag:bug_template\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary through pip3\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: sys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.0.130_410.48 / 10.0-linux-x64-v7.4.2.24\r\n- GPU model and memory: GeForce GTX 1080 with 7598 MB memory\r\n\r\n**Describe the current behavior**\r\n\r\nThe Adam optimizer does not seem to keep calling the supplied learning_rate callable. It seems like it's being called once or a few times, but then a \"cached\" value is repeatedly used in updates.\r\n\r\n**Describe the expected behavior**\r\n\r\nAccording to the [documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam) it should be possible to pass a \"callable that takes no arguments and returns the actual value to use\" as learning_rate to tf.keras.optimizers.Adam(), and this \"can be useful for changing these values across different invocations of optimizer functions\".\r\n\r\nMy expectation was that the Adam optimizer would keep calling the supplied callable at each update (i.e. from within apply_gradients()).\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nimport sys\r\nprint(sys.version_info)\r\n\r\ntf_a = tf.Variable(1.0)\r\nprint('Variable tf_a initialized to {}.'.format(tf_a.numpy()))\r\n\r\nlr = 0.1\r\ndef get_lr():\r\n    global lr\r\n    return lr\r\n\r\ntf_opt = tf.keras.optimizers.Adam(learning_rate=get_lr)\r\n\r\n@tf.function\r\ndef train_step():\r\n    with tf.GradientTape() as tf_tape:\r\n        tf_loss = tf_a**2\r\n        \r\n    tf_gradients = tf_tape.gradient(tf_loss, [tf_a])\r\n\r\n    tf_opt.apply_gradients(zip(tf_gradients, [tf_a]))\r\n\r\nprint('After one step with learning rate {}... '.format(get_lr()), end='')\r\ntrain_step()\r\nprint('Variable tf_a is {}.'.format(tf_a.numpy()))\r\n\r\nlr = 0.0\r\n\r\nfor _ in range(10):\r\n    print('After another step, now with learning rate {}... '.format(get_lr()), end='')\r\n    train_step()\r\n    print('Variable tf_a is {}.'.format(tf_a.numpy()))\r\n```\r\n\r\nThe code above produces the following output on my system:\r\n\r\n```\r\nv2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\nsys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)\r\nVariable tf_a initialized to 1.0.\r\nAfter one step with learning rate 0.1... Variable tf_a is 0.8999971747398376.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.8004083633422852.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.7015821933746338.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.6039347052574158.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.5079591274261475.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.41423195600509644.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.3234161138534546.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.23625943064689636.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.1535806804895401.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.07624538242816925.\r\nAfter another step, now with learning rate 0.0... Variable tf_a is 0.005127914249897003.\r\n```\r\nAs you can see tf_a keeps changing at a fast pace. My expectation was that after setting the learning rate to 0.0 updates would no-longer change tf_a.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Could reproduce the issue with TF Version 2.0.0 beta. Gist is attached in this file, [LR_Issue_31323.ipynb.zip](https://github.com/tensorflow/tensorflow/files/3467436/LR_Issue_31323.ipynb.zip)\r\n", "It looks like things get completely different behavior w/ and w/o tf.function", "I don't think this is particularly an optimizer problem, it seems that function wouldn't respect callables,\r\ni.e., the simplest code to reproduce that:\r\n```python\r\nlr = 0.1\r\ndef get_lr():\r\n  global lr\r\n  return lr\r\n@tf.function\r\ndef train_step():\r\n  val = get_lr()\r\n  return val\r\nprint('After one step with learning rate {}... '.format(get_lr()))\r\nprint('return value is {}'.format(train_step()))\r\n\r\nlr = 0.0\r\n\r\nfor _ in range(10):\r\n    print('After another step, now with learning rate {}... '.format(get_lr()))\r\n    print('return value is {}'.format(train_step()))\r\n```", "The output to above code is:\r\nAfter one step with learning rate 0.1... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is 0.10000000149011612", "is there a workaround for this ? I just stumbled upon the same problem..\r\nEDIT: For people with the same issue: `optimizer.learning_rate.assign(0.0)` seems to do the trick for `@tf.function`s ", "@bjornsing I think this was resolved in `tf-nightly`. I tried the above @tanzhenyu code and the output is as follows. It returns updated learning rate as 0.0.\r\n\r\nAfter one step with learning rate 0.1... \r\nreturn value is Tensor(\"PartitionedCall:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_1:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_2:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_3:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_4:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_5:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_6:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_7:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_8:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_9:0\", shape=(), dtype=float32)\r\nAfter another step, now with learning rate 0.0... \r\nreturn value is Tensor(\"PartitionedCall_10:0\", shape=(), dtype=float32)\r\n \r\nPlease close the issue if this was resolved for you. Thanks!", "Thanks @jvishnuvardhan !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31323\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31323\">No</a>\n", "> I don't think this is particularly an optimizer problem, it seems that function wouldn't respect callables,\r\n> i.e., the simplest code to reproduce that:\r\n> \r\n> ```python\r\n> lr = 0.1\r\n> def get_lr():\r\n>   global lr\r\n>   return lr\r\n> @tf.function\r\n> def train_step():\r\n>   val = get_lr()\r\n>   return val\r\n> print('After one step with learning rate {}... '.format(get_lr()))\r\n> print('return value is {}'.format(train_step()))\r\n> \r\n> lr = 0.0\r\n> \r\n> for _ in range(10):\r\n>     print('After another step, now with learning rate {}... '.format(get_lr()))\r\n>     print('return value is {}'.format(train_step()))\r\n> ```\r\n\r\nI tried @tanzhenyu 's code with latest ```tf-nightly(2.3.0-dev20200610)``` and ```tensorflow-2.2.0```, and it seems the problem is still unresovled."]}, {"number": 31322, "title": "Improvement to tf.einsum documentation", "body": "Improve tf.einsum documentation to include outer product. Based on the consensus\r\nhttps://www.reddit.com/r/deeplearning/comments/ckxyb5/tensorflow_proposal_for_outer_product_operation/\r\n that outer product is poorly documented generally in Tensorflow, though it is supported.", "comments": []}, {"number": 31321, "title": "Build C API with visual compiler 2010", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): Visual Studio compiler 2010\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nHello,\r\n\r\nI would like to build tensorflow C api dll for cpu using compiler 2010.\r\n\r\nIs this possible? As i have tried but there are errors produced using bazel or CMake\r\nBazel errors is as following \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1- Run C:\\ProgramData\\Anaconda3\\python.exe configure.py with the default configuration\r\n2- Run bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=monolithic //tensorflow:libtensorflow.so\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 35bf2202-8201-4eeb-8b4a-696872ec1a86\r\nINFO: Build option --conlyopt has changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow:libtensorflow.so (0 packages loaded, 7854 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/protobuf_archive/BUILD:259:1: C++ compilation of rule '@protobuf_archive//:protoc_lib' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/l-madham/_bazel_l-madham/enzg4re4/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\include;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v3.5;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\bin\\NETFX 4.0 Tools\\x64;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\bin\\x64;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\bin;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\l-madham\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\l-madham\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw -w -mavx -mavx2 -mfma -mfpmath=both -msse4.2 /DHAVE_PTHREAD /wd4018 /wd4514 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/protoc_lib/plugin.obj /c external/protobuf_archive/src/google/protobuf/compiler/plugin.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncl : Command line warning D9002 : ignoring unknown option '/Gw'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mfma'\r\ncl : Command line warning D9002 : ignoring unknown option '-mfpmath=both'\r\ncl : Command line warning D9002 : ignoring unknown option '-msse4.2'\r\nexternal/protobuf_archive/src\\google/protobuf/stubs/mutex.h(33) : fatal error C1083: Cannot open include file: 'mutex': No such file or directory\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nINFO: Elapsed time: 14.823s, Critical Path: 1.04s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@gadagashwini can you help me please?", "@MaramAdham7 Just verify, did you the follow the instructions mentioned in this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md). Thanks! ", "@gadagashwini I haven't tried them before but i tried \r\nbazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test now and the same issue has been produced\r\n\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\tensorflow_r10\\tensorflow/.bazelrc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 26ce30ea-ff75-4ef8-9f87-00bdb687b600\r\nDEBUG: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nINFO: Analysed target //tensorflow/tools/lib_package:libtensorflow_test (117 packages loaded, 7972 targets configured).\r\nINFO: Found 1 test target...\r\nERROR: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/protobuf_archive/BUILD:259:1: C++ compilation of rule '@protobuf_archive//:protoc_lib' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/l-madham/_bazel_l-madham/enzg4re4/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\include;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\v3.5;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\bin\\NETFX 4.0 Tools\\x64;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\bin\\x64;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v7.0A\\bin;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\l-madham\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\l-madham\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/protoc_lib/cpp_field.obj /c external/protobuf_archive/src/google/protobuf/compiler/cpp/cpp_field.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncl : Command line warning D9002 : ignoring unknown option '/Gw'\r\nexternal/protobuf_archive/src\\google/protobuf/stubs/mutex.h(33) : fatal error C1083: Cannot open include file: 'mutex': No such file or directory\r\nTarget //tensorflow/tools/lib_package:libtensorflow_test failed to build\r\nINFO: Elapsed time: 368.224s, Critical Path: 3.94s\r\nINFO: 10 processes: 10 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nFAILED: Build did NOT complete successfully", "@MaramAdham7 Can you upgrade the Microsoft Visual studio 2017 and try build. Thanks! ", "@gadagashwini unfortunately I want the api to be used in a project using compiler 2010\r\nSo is it possible to build the api using compiler 2010?", "@MaramAdham7 Please take a look at the [tested build configuration](https://www.tensorflow.org/install/source_windows#gpu) of Tensorflow 1.13. Thanks! ", "@gadagashwini i have seen this table before, and it says that the library is tested across this compiler, also says that the available pre-built api is built using MSVC15. \r\nMy question is it possible to build tensorflow api from source using compiler version older than MSVC15 (i.e to be more specific MSVC10) ? ", "@MaramAdham7 I'm afraid the TF team does not support the MSVC10 compiler. Since this is the case, I'll close this issue to keep the tracker focused.\r\n\r\nThanks for your interest! "]}, {"number": 31320, "title": "Build error TensorFlow Lite for ARM64", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jetson Nano\r\n- TensorFlow installed from (source or binary): - \r\n- TensorFlow version: master\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\nTwo build errors occurred.\r\n1.neon_tensor_utils.cc\r\n```\r\ntensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc: In function \u2018int32x4_t tflite::tensor_utils::RoundToNearest(float32x4_t)\u2019:\r\ntensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:883:79: error: invalid static_cast from type \u2018uint32x4_t {aka __vector(4) unsigned int}\u2019 to type \u2018int32x4_t {aka __vector(4) int}\u2019\r\n   const int32x4_t mask = static_cast<int32x4_t>(vcltq_f32(input, zero_val_dup));\r\n```\r\n2.quant_lstm_sup.cc\r\n```\r\nmake: *** No rule to make target '/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/nnapi/quant_lstm_sup.o', needed by '/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a'.  Stop.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed [https://www.tensorflow.org/lite/guide/build_arm64](https://www.tensorflow.org/lite/guide/build_arm64) .\r\n[Cross-compile for arm64](https://www.tensorflow.org/lite/guide/build_arm64#cross-compile_for_arm64) and [Compile natively on arm64](https://www.tensorflow.org/lite/guide/build_arm64#compile_natively_on_arm64) are the same result.\r\n\r\nCross-compile for ARM64\r\n```\r\nsudo apt-get update\r\nsudo apt-get install crossbuild-essential-arm64\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n./tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/lite/tools/make/build_aarch64_lib.sh\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n1.neon_tensor_utils.cc\r\nThe place where a build error occurs seems to be as follows.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc#L883-L884\r\nvcltq_f32 can not be cast to int32x4_t, so I think it should be changed to vcvtq_f32_u32.\r\n```\r\n  const uint32x4_t mask = vcltq_f32(input, zero_val_dup);\r\n  const float32x4_t casted_mask = vcvtq_f32_u32(mask);\r\n```\r\n\r\n2.quant_lstm_sup.cc\r\nquant_lstm_sup.cc has moved. see this commit (1ffdcbe).\r\nHowever, the Makefile has not been changed.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/Makefile#L170", "comments": ["It is fixed it in both commits. (2f1d610 and 864d294)\r\nI cose this issue."]}, {"number": 31319, "title": "CUDA driver version is insufficient for CUDA runtime version", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nkernel version 4.9.0\r\nLInux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version:\r\nTensorflow 1.14 \r\nTensorflow-gpu 1.14\r\n- Python version:\r\nPython 3.7.3\r\n- Installed using virtualenv? pip? conda?:\r\nconda\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nNot from source but gcc is 6.3.0\r\n- CUDA/cuDNN version:\r\ncuDNN : 7.6.0\r\n- GPU model and memory:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   44C    P0    70W / 149W |      0MiB / 11441MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n**Describe the problem**\r\n\r\nSo I am trying to run a tensorflow dependent code, after fixing compatibility issues with my current tensorflow version, I run into this issue raised but from Nividia website, it looks to me the cuda meets the requirement. So I am wondering if this is from tensorflow. Is there any version pf tensorflow that's compatible with my system requirements and cuda?\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nNot applicable. I didn't bother to install any driver so as not to break my system. I only try to run my code and got to this issue.\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"TF.py\", line 48, in <module>\r\n    sess = tf.compat.v1.Session(config=sess_config)\r\n  File \"/home/jfadugba/miniconda3/envs/peexoo/lib/python3.7/site-packages/tensorf\r\nlow/python/client/session.py\", line 1570, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/jfadugba/miniconda3/envs/peexoo/lib/python3.7/site-packages/tensorf\r\nlow/python/client/session.py\", line 693, in __init__\r\n    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\ntensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. St\r\natus: CUDA driver version is insufficient for CUDA runtime version\r\n", "comments": ["@jerofad \r\nCan you go through below link and see if it helps you.Thanks!\r\nhttps://stackoverflow.com/questions/41409842/ubuntu-16-04-cuda-8-cuda-driver-version-is-insufficient-for-cuda-runtime-vers\r\nhttps://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html\r\nJust to verify did you get chance to follow instructions from [TensorFlow](https://www.tensorflow.org/install/gpu) website .Please, let us know. Thanks!", "Awesome I just downgraded my tensorflow to 1.13.1 version and everything works well with my drivers. I decided to downgrade as I am not satisfied with reinstalling my cuda drivers and nvidia drivers.\r\nAll good now."]}, {"number": 31318, "title": "InvalidArgumentError: Cannot assign a device for operation embedding_1", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I have written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS version: **Fedora 29.5.1.18 (also tested Ubuntu 18.10)**\r\n- TensorFlow installed from (source or binary): **tensorflow/tensorflow:latest-gpu-py3-jupyter**\r\n- TensorFlow version (use command below): **1.14.0**\r\n- Python version: **3.6.8**\r\n- CUDA/cuDNN version: **10.0.130**\r\n- GPU model and memory: **GeForce RTX 2080 ti, 11 Gb**\r\n\r\n**Describe the current behavior**\r\nI'm using keras. I try to to fit model that contains **Embedding** layer. When I call `model.fit_generator(...)` I get an error: \r\n\r\n_InvalidArgumentError: Cannot assign a device for operation embedding/embeddings/Initializer/random_uniform/sub: Could not satisfy explicit device specification '' because the node {{colocation_node embedding/embeddings/Initializer/random_uniform/sub}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nIdentity: GPU CPU XLA_CPU XLA_GPU \r\nConst: GPU CPU XLA_CPU XLA_GPU \r\nResourceSparseApplyRMSProp: CPU \r\nRandomUniform: GPU CPU XLA_CPU XLA_GPU \r\nReadVariableOp: GPU CPU XLA_CPU XLA_GPU \r\nSub: GPU CPU XLA_CPU XLA_GPU \r\nAdd: GPU CPU XLA_CPU XLA_GPU \r\nMul: GPU CPU XLA_CPU XLA_GPU \r\nVarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU \r\nVarHandleOp: GPU CPU XLA_CPU XLA_GPU \r\nAssignVariableOp: GPU CPU XLA_CPU XLA_GPU \r\nResourceGather: GPU CPU XLA_CPU XLA_GPU_ \r\n\r\nResourceSparseApplyRMSProp looks strange for me.\r\n\r\nAfter getting this error I cannot fit new (simplified) model, because I get this error again. I get this error even I run `tensorflow.keras.backend.get_value(model.optimizer.lr)`\r\n\r\n**Describe the expected behavior**\r\nModel fits without any problems, like the same model without Embedding layer (no _dest_input_).\r\n**Code to reproduce the issue**\r\n\r\n```\r\nlstm_input = Input(shape=(30, 5,))\r\nsteady_input = Input(shape=(3,),\r\n                    name='steady_float') #\u0442\u0443\u0442 None \u0432 shape \u043d\u0435 \u043d\u0443\u0436\u0435\u043d?\r\ndest_input = Input(shape=(1,), name='steady_dest')\r\nns_input = Input(shape=(1,))\r\n\r\nx1 = layers.Bidirectional(layers.LSTM(512, activation='relu', return_sequences=True))(lstm_input)\r\nx1 = layers.Bidirectional(layers.LSTM(256, activation='relu'))(x1)\r\nx1 = Model(inputs=lstm_input, outputs=x1)\r\n\r\nx2 = layers.Dense(512, activation=\"relu\")(steady_input)\r\nx2 = Model(inputs=steady_input, outputs=x2)\r\n\r\nx3 = layers.Embedding(12, 3)(dest_input)\r\nx3 = layers.Flatten()(x3)\r\nx3 = layers.Dense(512, activation=\"relu\")(x3)\r\nx3 = Model(inputs=dest_input, outputs=x3)\r\n\r\nx = layers.concatenate([x1.output, x2.output, x3.output])\r\nx = layers.Dense(128, activation='relu')(x)\r\n\r\ny1_output_tensor = layers.Dense(5, name='y1')(x)\r\ny2_output_tensor = layers.Dense(5, name='y2')(x)\r\nmodel = Model(inputs=[x1.input, x2.input, x3.input],\r\n                         outputs=[y1_output_tensor, y2_output_tensor])\r\n\r\nep_n = 200\r\nlearning_rate = 0.001\r\ndecay_rate = learning_rate / ep_n\r\nmomentum = 0.7\r\nmodel.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate), loss=['mae', 'mae'])\r\n\r\n#train_gen and test_get - simple generatora with shuffle\r\nbatch_size = 128\r\nhistory = model.fit_generator(train_gen,\r\n                              steps_per_epoch=1000,\r\n                              epochs=ep_n,\r\n                              validation_data=test_gen,\r\n                              validation_steps=X_test.shape[0]//batch_size)\r\n```\r\n\r\n**Other info / logs**\r\nSome times I get this error on simplified model:\r\n\r\n```\r\n# define two sets of inputs\r\ninputA = Input(shape=(1,))\r\ninputB = Input(shape=(128,))\r\n \r\n# the first branch operates on the first input\r\nx = Embedding(1000, 3)(inputA)\r\nx = layers.Flatten()(x)\r\nx = layers.Dense(4096, activation=\"relu\")(x)\r\nx = layers.Dense(2048, activation=\"relu\")(x)\r\nx = layers.Dense(1024, activation=\"relu\")(x)\r\nx = layers.Dense(512, activation=\"relu\")(x)\r\nx = layers.Dense(256, activation=\"relu\")(x)\r\nx = Dense(128, activation=\"relu\")(x)\r\nx = Dense(64, activation=\"relu\")(x)\r\nx = Model(inputs=inputA, outputs=x)\r\n \r\n# the second branch opreates on the second input\r\ny = Dense(1024, activation=\"relu\")(inputB)\r\ny = Dense(512, activation=\"relu\")(y)\r\ny = Dense(256, activation=\"relu\")(y)\r\ny = Dense(128, activation=\"relu\")(y)\r\ny = Dense(64, activation=\"relu\")(y)\r\ny = Model(inputs=inputB, outputs=y)\r\n \r\n# combine the output of the two branches\r\ncombined = layers.concatenate([x.output, y.output])\r\n \r\n# apply a FC layer and then a regression prediction on the\r\n# combined outputs\r\nz = Dense(2, activation=\"relu\")(combined)\r\nz = Dense(1, activation=\"linear\")(z)\r\n \r\n# our model will accept the inputs of the two branches and\r\n# then output a single value\r\nmodel = Model(inputs=[x.input, y.input], outputs=z)\r\n\r\nep_n = 10\r\nlearning_rate = 0.001\r\ndecay_rate = learning_rate / ep_n\r\nmomentum = 0.7\r\nmodel.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate\r\n\r\ninput_array_a = np.random.randint(1000, size=(500000, 1))\r\ninput_array_b = np.random.randint(32, size=(500000, 128))\r\noutput_array = np.random.randint(9, size=(500000, 1))\r\n\r\ndef generator_shuffle(x_a, x_b, y, batch_size=1024):\r\n    max_index = len(x_a) - 1\r\n    while 1:\r\n        rows = np.random.randint(0, max_index, batch_size)\r\n        yield [x_a[rows], x_b[rows]], y[rows]\r\n\r\ntr_gen = generator_shuffle(input_array_a,\r\n                           input_array_b,\r\n                           output_array)\r\n\r\nhistory = model.fit_generator(tr_gen, steps_per_epoch=3, epochs=10)\r\n```\r\nProbably, the issue occuring depends on CPU usage.\r\n\r\nI'm attaching full log.\r\n\r\n[gpu_error.txt](https://github.com/tensorflow/tensorflow/files/3464780/gpu_error.txt)\r\n", "comments": ["I tried yo use\r\n```\r\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,  log_device_placement=True))\r\nKB.set_session(sess)\r\n```\r\n\r\nand got new error:\r\n\r\n---------------------------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n\r\n<ipython-input-58-d121a4b39b7f> in <module>\r\n      3                               epochs=ep_n,\r\n      4                               validation_data=test_gen,\r\n----> 5                               validation_steps=X_test.shape[0]//batch_size)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1431         shuffle=shuffle,\r\n   1432         initial_epoch=initial_epoch,\r\n-> 1433         steps_name='steps_per_epoch')\r\n   1434 \r\n   1435   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    262 \r\n    263       is_deferred = not model._is_compiled\r\n--> 264       batch_outs = batch_function(*batch_data)\r\n    265       if not isinstance(batch_outs, list):\r\n    266         batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1173       self._update_sample_weight_modes(sample_weights=sample_weights)\r\n   1174       self._make_train_function()\r\n-> 1175       outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n   1176 \r\n   1177     if reset_metrics:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3290 \r\n   3291     fetched = self._callable_fn(*array_vals,\r\n-> 3292                                 run_metadata=self.run_metadata)\r\n   3293     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3294     output_structure = nest.pack_sequence_as(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1456         ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n   1457                                                self._handle, args,\r\n-> 1458                                                run_metadata_ptr)\r\n   1459         if run_metadata:\r\n   1460           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nNotFoundError: 2 root error(s) found.\r\n  (0) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.\r\n\t [[{{node embedding_1/embedding_lookup}}]]\r\n  (1) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.\r\n\t [[{{node embedding_1/embedding_lookup}}]]\r\n\t [[RMSprop_1/RMSprop/update_embedding_1/embeddings/ResourceSparseApplyRMSProp/_184]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n---------------------------------------------------------------------------------------------", "Probably, the same issue: https://github.com/fizyr/keras-maskrcnn/issues/39", "Finaly, I solved my problem. I removed the definition of momentum for RMSProp.\r\nPrevious run (with error):\r\n```\r\nep_n = 200\r\nlearning_rate = 0.001\r\ndecay_rate = learning_rate / ep_n\r\nmomentum = 0.7\r\nmodel.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate), loss=['mae', 'mae'])\r\n```\r\n\r\nCurrent run (successful):\r\n```\r\nmodel.compile(optimizer=RMSprop(), loss=['mae', 'mae'])\r\nKB.get_value(model.optimizer.lr)\r\n```\r\n\r\nAnother successful run:\r\n```\r\nep_n = 200\r\nlearning_rate = 0.001\r\ndecay_rate = learning_rate / ep_n\r\nmodel.compile(optimizer=RMSprop(lr=learning_rate, decay=decay_rate), loss=['mae', 'mae'])\r\n```\r\n\r\nSo the problem is in _momentum_ parameter.\r\n", "@AlinaYablokova Looks like you found workaround. Are you happy to close the issue. Thanks!", "@gadagashwini , yes, I have found workaround and I can continue my work now. But in my opinion, there are still a number of problems:\r\n\r\n1. It's impossible to use Embedding layer and RMSProp with non-zero momentum together. \r\n2. First workaround \r\n```\r\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,  log_device_placement=True))\r\nKB.set_session(sess)\r\n```\r\ndid not work:\r\n_\"NotFoundError: 2 root error(s) found.\r\n(0) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.\r\n[[{{node embedding_1/embedding_lookup}}]]\r\n(1) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.\r\n[[{{node embedding_1/embedding_lookup}}]]\r\n[[RMSprop_1/RMSprop/update_embedding_1/embeddings/ResourceSparseApplyRMSProp/_184]]\r\n0 successful operations.\r\n0 derived errors ignored.\"_", "@AlinaYablokova Could you please provide the complete code to reproduce the reported issue. Thanks!  ", "```\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as KB\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import Input\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import RMSprop\r\n\r\ninputA = Input(shape=(1,))\r\ninputB = Input(shape=(128,3,))\r\n \r\nx = layers.Embedding(1000, 3)(inputA)\r\nx = layers.Flatten()(x)\r\nx = layers.Dense(256, activation=\"relu\")(x)\r\nx = layers.Dense(128, activation=\"relu\")(x)\r\nx = layers.Dense(64, activation=\"relu\")(x)\r\nx = Model(inputs=inputA, outputs=x)\r\n \r\ny = layers.Bidirectional(layers.LSTM(128, activation='relu', return_sequences=True))(inputB)\r\ny = layers.Bidirectional(layers.LSTM(64, activation='relu'))(y)\r\ny = Model(inputs=inputB, outputs=y)\r\n\r\ncombined = layers.concatenate([x.output, y.output])\r\n \r\nz = layers.Dense(2, activation=\"relu\")(combined)\r\nz = layers.Dense(1, activation=\"linear\")(z)\r\n\r\nt = layers.Dense(2, activation=\"relu\")(combined)\r\nt = layers.Dense(1, activation=\"linear\")(t)\r\n \r\nmodel = Model(inputs=[x.input, y.input], outputs=[z, t])\r\nmodel.compile(RMSprop(lr=0.001, decay=0.05, momentum=0.7), ['mse', 'mse'])\r\n#model.compile(RMSprop(lr=0.001, decay=0.05, momentum=0.0), ['mse', 'mse']) OK\r\n\r\ninput_array_a = np.random.randint(1000, size=(500000, 1))\r\ninput_array_b = np.random.randint(32, size=(500000, 128, 3))\r\noutput_array1 = np.random.randint(9, size=(500000, 1))\r\noutput_array2 = np.random.randint(9, size=(500000, 1))\r\n\r\ndef generator_shuffle(x_a, x_b, y1, y2, batch_size=128):\r\n    max_index = len(x_a) - 1\r\n    while 1:\r\n        rows = np.random.randint(0, max_index, batch_size)\r\n        yield [x_a[rows], x_b[rows]], [y1[rows], y2[rows]]\r\n\r\ntr_gen = generator_shuffle(input_array_a,\r\n                           input_array_b,\r\n                           output_array1, output_array2)\r\n\r\nhistory = model.fit_generator(tr_gen, steps_per_epoch=3, epochs=10)\r\n```", "I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and TF-Nightly version '1.15.0-dev20190821'. Please take a look at colab [gist](https://colab.research.google.com/drive/1P35gSdWJJ6IemiwkUqKuQa071oaJS5Xe). Thanks!", "This is fixed with tf-nightly version '2.2.0-dev20200303'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31318\">No</a>\n", "> I tried yo use\r\n> \r\n> ```\r\n> sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,  log_device_placement=True))\r\n> KB.set_session(sess)\r\n> ```\r\n> \r\n> and got new error:\r\n> \r\n> NotFoundError Traceback (most recent call last)\r\n> \r\n> in\r\n> 3 epochs=ep_n,\r\n> 4 validation_data=test_gen,\r\n> ----> 5 validation_steps=X_test.shape[0]//batch_size)\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n> 1431 shuffle=shuffle,\r\n> 1432 initial_epoch=initial_epoch,\r\n> -> 1433 steps_name='steps_per_epoch')\r\n> 1434\r\n> 1435 def evaluate_generator(self,\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n> 262\r\n> 263 is_deferred = not model._is_compiled\r\n> --> 264 batch_outs = batch_function(*batch_data)\r\n> 265 if not isinstance(batch_outs, list):\r\n> 266 batch_outs = [batch_outs]\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n> 1173 self._update_sample_weight_modes(sample_weights=sample_weights)\r\n> 1174 self._make_train_function()\r\n> -> 1175 outputs = self.train_function(ins) # pylint: disable=not-callable\r\n> 1176\r\n> 1177 if reset_metrics:\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in **call**(self, inputs)\r\n> 3290\r\n> 3291 fetched = self._callable_fn(*array_vals,\r\n> -> 3292 run_metadata=self.run_metadata)\r\n> 3293 self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n> 3294 output_structure = nest.pack_sequence_as(\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in **call**(self, *args, **kwargs)\r\n> 1456 ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n> 1457 self._handle, args,\r\n> -> 1458 run_metadata_ptr)\r\n> 1459 if run_metadata:\r\n> 1460 proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n> \r\n> ## NotFoundError: 2 root error(s) found.\r\n> (0) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.\r\n> [[{{node embedding_1/embedding_lookup}}]]\r\n> (1) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.\r\n> [[{{node embedding_1/embedding_lookup}}]]\r\n> [[RMSprop_1/RMSprop/update_embedding_1/embeddings/ResourceSparseApplyRMSProp/_184]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored.\r\n\r\nInvalidArgumentError: Cannot assign a device for operation conv2d_1/kernel/IsInitialized/VarIsInitializedOp: node conv2d_1/kernel/IsInitialized/VarIsInitializedOp (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[conv2d_1/kernel/IsInitialized/VarIsInitializedOp]]", "> @AlinaYablokova Could you please provide the complete code to reproduce the reported issue. Thanks!\r\nhelp-me \r\n\r\nInvalidArgumentError: Cannot assign a device for operation conv2d_1/kernel/IsInitialized/VarIsInitializedOp: node conv2d_1/kernel/IsInitialized/VarIsInitializedOp (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[conv2d_1/kernel/IsInitialized/VarIsInitializedOp]]", "I was facing the same issue. \r\nI did following changes in my code\r\n\r\n1. import os\r\n    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\r\n2. Changed optimizer from RMRprop tp Adagrad\r\n3. Reduced the dimention from 6181 to 3750.\r\n\r\nand the problem was resolved. \r\n\r\n", "Interestingly, the error appeared when trying to use the optimizer\r\n`tensorflow.keras.optimizers.Adam()`, using SGD worked fine.\r\n\r\nOn Apple M1 uninstalling and installing _tensorflow-macos_, _tensorflow-metal_ and _tensorflow_ did the trick for me. "]}, {"number": 31317, "title": "[Sum/Average Loss Reduction Strategy] Why going with Sum as default ?", "body": "TF 1.14.0 introduced the following:\r\n\r\n> Set default loss reduction as AUTO for improving reliability of loss scaling with distribution strategy and custom training loops. AUTO indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE. When used in distribution strategy scope, outside of built-in training loops such as tf.keras compile and fit, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error. \r\n\r\nSource: https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0\r\n\r\n----------\r\n\r\nWhy `SUM_OVER_BATCH_SIZE` was made the default when the followings are standard practices:\r\n- averaging the loss over one batch \r\n- averaging gradients over multiple ranks\r\n\r\nCould anyone give examples of use-cases where sum would give better results than average ? And why would this made default when averaging seems far more used in the state of the art ?", "comments": ["You can find it mentioned here https://www.tensorflow.org/beta/tutorials/distribute/training_loops\r\n```\r\nNormally, on a single machine with 1 GPU/CPU, loss is divided by the number of \r\nexamples in the batch of input. So, how should the loss be calculated when using \r\na tf.distribute.Strategy?For an example, let's say you have 4 GPU's and a batch \r\nsize of 64.One batch of input is distributed across the replicas (4 GPUs), each \r\nreplica getting an input of size 16.The model on each replica does a forward pass \r\nwith its respective input and calculates the loss. Now, instead of dividing the loss\r\nby the number of examples in its respective input (BATCH_SIZE_PER_REPLICA = 16),\r\nthe loss should be divided by the GLOBAL_BATCH_SIZE (64).\r\nWhy do this?\r\nThis needs to be done because after the gradients are calculated on each replica, \r\nthey are synced across the replicas by summing them.\r\nHow to do this in TensorFlow? If you're writing a custom training loop, as in this \r\ntutorial, you should sum the per example losses and divide the sum by the \r\nGLOBAL_BATCH_SIZE: scale_loss = tf.reduce_sum(loss) * (1. / GLOBAL_BATCH_SIZE) \r\nor you can use tf.nn.compute_average_loss which takes the per example loss, optional \r\nsample weights, and GLOBAL_BATCH_SIZE as arguments and returns the scaled loss.\r\n```", "Automatically closing this out since I understand it to be resolved by @srihari-humbarwadi , but please let me know if I'm mistaken.Thanks!"]}, {"number": 31316, "title": "install_headers: fix paths of generated headers", "body": "The generated headers moved from bazel-genfiles to bazel-bin so change\r\nthe match to remove both. Also adjust the external library header files\r\nso they have the right paths to work with the base include.\r\n\r\nAll the TensorFlow header files should compile cleanly on their own\r\n(excluding the windows ones etc). To verify, run the target then install\r\nto /usr/include/tensorflow and run the following:\r\n\r\nfor i in $(find /usr/include/tensorflow -iname \"*.h\"); do \\\r\ng++ -o/dev/null -E -I/usr/include/tensorflow -I/opt/cuda/include $i \\\r\n|| echo $i; done\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nI'll file a separate PR to merge these into 1.14.1. I've added this into gentoo's tensorflow-1.14.0-r1 and verified things work to build against the headers now.", "comments": ["Thank you for the PR.\r\n\r\nOne question: Did the headers move because of a Bazel version change? We might not need to cherry-pick this on r1.14 in that case as we're trying to keep the same Bazel version for patch releases as for the main release.", "I think the change was because of: https://github.com/tensorflow/tensorflow/commit/8a3c3478ff7870c05744409adee888ff425151c1\r\nIts in 1.14.0-rc0 and upwards (and 1.12.1), looks like it moved it from bazel-genfiles to bazel-bin. I made this patch handle both cases so shouldnt be any problems. Also it fixes up some of the external/ headers so they match so its probably still worth cherry-picking", "Awesome. Then it is worth cherry-picking. Thank you"]}, {"number": 31315, "title": "AttributeError: module 'tensorflow' has no attribute 'gfile' - Version 2.0.0-alpha0", "body": "Tensorflow Version - **2.0.0-alpha0**\r\n\r\nError occurred while running a notebook from the Tensorflow site  - [**Build a linear model with Estimators**](https://www.tensorflow.org/tutorials/estimators/linear )\r\n<br>\r\nDownload the dataset:\r\n```\r\nfrom official.wide_deep import census_dataset\r\nfrom official.wide_deep import census_main\r\n\r\ncensus_dataset.download(\"/tmp/census_data/\")\r\n```\r\n\r\n> ---------------------------------------------------------------------------\r\n> AttributeError                            Traceback (most recent call last)\r\n> <ipython-input-9-fa1d43ace000> in <module>()\r\n>       2 from official.wide_deep import census_main\r\n>       3 \r\n> ----> 4 census_dataset.download(\"/tmp/census_data/\")\r\n> \r\n> /content/models/official/wide_deep/census_dataset.py in download(data_dir)\r\n>      76 def download(data_dir):\r\n>      77   \"\"\"Download census data if it is not already present.\"\"\"\r\n> ---> 78   **tf.gfile.MakeDirs(data_dir)**\r\n>      79 \r\n>      80   training_file_path = os.path.join(data_dir, TRAINING_FILE)\r\n> \r\n> **AttributeError: module 'tensorflow' has no attribute 'gfile'**\r\n<hr>\r\n\r\n## The document suggests the following changes in the file.\r\n### _Should I go about making the following changes manually in the file census_dataset.py ?_\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0625 16:04:36.412110 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:78: The name tf.gfile.MakeDirs is deprecated. Please use **tf.io.gfile.makedirs** instead.\r\n\r\nW0625 16:04:36.413802 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:81: The name tf.gfile.Exists is deprecated. Please use **tf.io.gfile.exists** instead.\r\n\r\nW0625 16:04:38.253764 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:62: The name tf.gfile.Open is deprecated. Please use **tf.io.gfile.GFile** instead.\r\n\r\nW0625 16:04:38.488776 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:73: The name tf.gfile.Remove is deprecated. Please use **tf.io.gfile.remove** instead. \r\n#28632 https://github.com/tensorflow/tensorflow/issues/28632\r\n#https://github.com/google/gin-config/issues/9\r\n#https://github.com/tobegit3hub/simple_tensorflow_serving/issues/45", "comments": ["@Gurubux The tutorial works for Tensorflow 1.14.0. You might want to give it a try instead. Thanks!", "In 2.0, `tf.gfile.*` is replaced by `tf.io.gfile.*`.\r\n\r\nIf you want, you can submit a cherry-pick for `census_dataset.py`.\r\n\r\nThank you", "@mihaimaruseac The following changes in census_dataset.py i.e \r\n`tf.gfile.* is replaced by tf.io.gfile.*`\r\n\r\nResults in \r\n`tf.io.gfile.MakeDirs(data_dir)`\r\n> **AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'MakeDirs'**\r\n\r\nThanks\r\n@gadagashwini Sure it does! Thanks\r\n", "@Gurubux https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/gfile says it is `makedirs`", "@Gurubux Did you check the @mihaimaruseac's comment. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "If you following object detection API reguard tutorial;\r\nFirst check your tensorflow version by running bellow code\r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nIf it's 2 or above then\r\npip install tensorflow==1.14.0      will run code successfully.", "If are having too much trouble, and don't want to downgrade, just do\r\n`import tensorflow.compat.v1 as tf`", "No need to downgrade, all of the gfile conversions are covered by the migration script.", "> No need to downgrade, all of the gfile conversions are covered by the migration script.\r\n\r\nIs there a migration script for C++?", "The C++ API didn't change", "@mihaimaruseac I'm working in colab and  changed `tf.gfile.GFile` to `tf.io.gfile.GFile` but still get this error\r\n`AttributeError: module 'tensorflow' has no attribute 'gfile'\r\n`", "@mitramir55 please open a new issue, fill in issue template and provide minimal code to reproduce. It's likely you have some dependency issue / some wrong version of TF installed. `pip list` should also help in identifying the issue", "> In 2.0, `tf.gfile.*` is replaced by `tf.io.gfile.*`.\r\n> \r\n> If you want, you can submit a cherry-pick for `census_dataset.py`.\r\n> \r\n> Thank you\r\n\r\n\r\nI solved the error by replacing `tf.gfile.FastGFile` to `tf.io.gfile.GFile`.\r\n"]}, {"number": 31314, "title": "pkgconfig: generate tensorflow_cc pkg-config entry", "body": "There is already a tensorflow.pc pkg-config file for libtensorflow, this generates one for libtensorflow_cc as well.\r\n\r\nAlso change the includedir to /usr/include/tensorflow to match how //tensorflow:install_headers installs the files.\r\n\r\nI'll make a separate PR to merge these into 1.14.1, and this has been added into the gentoo 1.14.0-r1 revision with no issues.", "comments": []}, {"number": 31313, "title": "systemlibs: unbundle enum34 and fix jsoncpp header links for jsoncpp 1.9", "body": "This PR unbundles enum34 like normal python libs.\r\nJsoncpp 1.8->1.9 added new headers. The current includes use a path that disagrees with how the headers are normally installed so needs symlinks to adjust the paths. This adds symlinks to these new headers.\r\n\r\nI've added these patches to the Gentoo tensorflow-1.14.0-r1 package a few days ago and had no problems so far.\r\n\r\nI'll file a separate PR to cherry-pick these also into 1.14.1 when these are merged.\r\n", "comments": []}, {"number": 31312, "title": "RAM usage during fit() on a tf.keras model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: conda repository\r\n- **TensorFlow version (use command below)**: 2.0.1\r\n- **Python version**: 3.7.3\r\n- **CUDA/cuDNN version**: CUDA 10, cuDNN 7.6\r\n- **GPU model and memory**: GeForce GTX 1050 Ti, 4 GB memory\r\n\r\n### Describe the problem\r\nSituation: running the `fit` command on a `tf.keras` model.\r\nProblem: When running on CPU, the `fit` command takes ~25MB. When running on GPU, it takes **~970MB**.\r\n\r\n\r\n### Source code / logs\r\nUse the following code, in a script called `test.py`:\r\n```\r\nfrom memory_profiler import profile\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\n@profile\r\ndef test():\r\n    optimizer = keras.optimizers.Adam(lr=1e-3)\r\n    loss = keras.losses.BinaryCrossentropy()\r\n\r\n    model = keras.models.Sequential()\r\n    model.add(keras.layers.Flatten())\r\n    model.add(keras.layers.Dense(1))\r\n\r\n    model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])\r\n\r\n    num_samples = 1280\r\n    batch_size = 64\r\n\r\n    inputs = np.random.uniform(size=(num_samples, 100, 100, 3)).astype(np.float32)\r\n    targets = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.int32)\r\n\r\n    history = model.fit(inputs, targets, epochs=3, validation_data=(inputs, targets))\r\n\r\ntest()\r\n```\r\n\r\nand use the following commands, in an environment with tensorflow-gpu and **memory_profiler**:\r\n`export CUDA_VISIBLE_DEVICES=0; python -m memory_profiler test.py`\r\n`export CUDA_VISIBLE_DEVICES=\"-1\"; python -m memory_profiler test.py`\r\n\r\nThe outputs I get:\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9    249.9 MiB    249.9 MiB   @profile\r\n    10                             def test():\r\n    11    249.9 MiB      0.0 MiB       optimizer = keras.optimizers.Adam(lr=1e-3)\r\n    12    249.9 MiB      0.0 MiB       loss = keras.losses.BinaryCrossentropy()\r\n    13                             \r\n    14    252.6 MiB      2.7 MiB       model = keras.models.Sequential()\r\n    15    252.6 MiB      0.0 MiB       model.add(keras.layers.Flatten())\r\n    16    252.6 MiB      0.0 MiB       model.add(keras.layers.Dense(1))\r\n    17                             \r\n    18    252.9 MiB      0.3 MiB       model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])\r\n    19                             \r\n    20    252.9 MiB      0.0 MiB       num_samples = 1280\r\n    21    252.9 MiB      0.0 MiB       batch_size = 64\r\n    22                             \r\n    23    399.6 MiB    146.6 MiB       inputs = np.random.uniform(size=(num_samples, 100, 100, 3)).astype(np.float32)\r\n    24    399.6 MiB      0.0 MiB       targets = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.int32)\r\n    25                             \r\n    26   1383.0 MiB    983.5 MiB       history = model.fit(inputs, targets, epochs=3, validation_data=(inputs, targets))\r\n```\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9    249.8 MiB    249.8 MiB   @profile\r\n    10                             def test():\r\n    11    249.8 MiB      0.0 MiB       optimizer = keras.optimizers.Adam(lr=1e-3)\r\n    12    249.8 MiB      0.0 MiB       loss = keras.losses.BinaryCrossentropy()\r\n    13                             \r\n    14    252.6 MiB      2.7 MiB       model = keras.models.Sequential()\r\n    15    252.6 MiB      0.0 MiB       model.add(keras.layers.Flatten())\r\n    16    252.9 MiB      0.3 MiB       model.add(keras.layers.Dense(1))\r\n    17                             \r\n    18    252.9 MiB      0.0 MiB       model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])\r\n    19                             \r\n    20    252.9 MiB      0.0 MiB       num_samples = 1280\r\n    21    252.9 MiB      0.0 MiB       batch_size = 64\r\n    22                             \r\n    23    399.5 MiB    146.6 MiB       inputs = np.random.uniform(size=(num_samples, 100, 100, 3)).astype(np.float32)\r\n    24    399.5 MiB      0.0 MiB       targets = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.int32)\r\n    25                             \r\n    26    429.3 MiB     29.8 MiB       history = model.fit(inputs, targets, epochs=3, validation_data=(inputs, targets))\r\n```\r\n\r\n**Moreover**, when I try to use the InceptionV3 model, I get the following results:\r\nRAM used when training on GPU, using the `fit` method: 13.4 GB\r\nRAM used when training on GPU, using a custom training loop: 943 MB\r\nRAM used when training on CPU, using the `fit` method: 13 GB\r\nRAM used when training on CPU, using a custom training loop: 582 MB", "comments": ["might be related to #30443", "I am experiencing the same problem. Similar to https://github.com/tensorflow/tensorflow/issues/30443, the first epoch is very slow (probably because of data loading) and the memory usage increases constantly after every batch. The other epochs run quickly and memory is stable. Calling the `model.fit` method seems to cache the data into memory (even though RAM caching was never explicitly turned on).\r\n\r\nEdit: Could this be related to https://github.com/tensorflow/tensorflow/issues/30561#issuecomment-510399901?", "Same issue here. Using `model.fit` results in surprisingly high usage of RAM, I couldn't even start training before running out of memory. Then I tried using `model.train_on_batch` hoping it would not have the problem because only one batch is presented at a time, but sadly the same issue. I'm using `tf.data.Dataset` with keras model for training.\r\n\r\nThe same model can be trained using custom training loop with no problem. Just wondering if there is a solution yet?", "I have the same issue. No problems running the exact same script with TF 1.14, but with TF 2.0 RAM usage keeps increasing with each epoch until my Jupyter Notebook freezes.", "Same issue as well. I am using Keras 2.2.4, Tensorflow 1.14.0, CPU and python 3.7.4 under Windows 10.\r\n```\r\nmodel = Sequential()\r\n# Encoder       \r\nmodel.add(LSTM(32, activation='relu', input_shape =(timesteps, n_features ), return_sequences=True))\r\nmodel.add(LSTM(16, activation='relu', return_sequences=False))\r\nmodel.add(RepeatVector(timesteps))\r\n# Decoder\r\nmodel.add(LSTM(16, activation='relu', return_sequences=True))\r\nmodel.add(LSTM(32, activation='relu', return_sequences=True))\r\nmodel.add(TimeDistributed(Dense(n_features)))\r\n```\r\n```\r\ndata.shape\r\n(23764, 20, 2)\r\nmodel.fit(data, data,       \r\n          epochs=num_epochs, \r\n          batch_size = 32)\r\n```\r\nI have 16GB of memory, and usage creeps up with each epoch. If I'm lucky I can train 30 epochs, which uses 97% of memory. With more epochs, it generally gets out of memory altogether.\r\nIs there a decent workaround until they fix this?", "I am facing the same problem with ResNet50 from keras applications. In the first epoch my 16GB memory is rapidly exceeded. Tried tensorflow 1.14 and tensorflow-gpu 1.14. Also, decreasing the batch size tremendously did not help. So far only facing the issues with models from applications.", "I have the same problem, memory usage is increasing after every call to fit. I don't know if this has to do with the callbacks history object,but after around 20-30 calls to fit, all RAM is used (I have 16 GB).", "> I am facing the same problem with ResNet50 from keras applications. In the first epoch my 16GB memory is rapidly exceeded. Tried tensorflow 1.14 and tensorflow-gpu 1.14. Also, decreasing the batch size tremendously did not help. So far only facing the issues with models from applications.\r\n\r\nUpdate: Tried TF 2.0.0 and it does not show this issue. ", "Update: I also tried updating to TF 2.0.0. Memory usage of my 16GB tops out at 98%, after 9 out of 30 epochs but seems to stabilise there, dropping to 96-97% and then going up again, but not exceeding 98%.  Seems kind of close to the brink, but it went through.\n\n \n\nFrom: engineerByNature <notifications@github.com> \nSent: 29 October 2019 16:15\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Garry <garry@business-analytic.co.uk>; Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] RAM usage during fit() on a tf.keras model (#31312)\n\n \n\nI am facing the same problem with ResNet50 from keras applications. In the first epoch my 16GB memory is rapidly exceeded. Tried tensorflow 1.14 and tensorflow-gpu 1.14. Also, decreasing the batch size tremendously did not help. So far only facing the issues with models from applications.\n\nUpdate: Tried TF 2.0.0 and it does not show this issue.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/31312?email_source=notifications&email_token=ACCWNRHY5YR3SWLB7TH5GNDQRBOP7A5CNFSM4IJETMI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECRBF5I#issuecomment-547492597> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ACCWNRGGTQDKICIWOJIEV63QRBOP7ANCNFSM4IJETMIQ> .\n\n", "I'm facing the same issue, but using TF 2.0 alpha works for me. Using the latest release spends all 25 GB of RAM on Google Colab, but with alpha it barely uses 3 GB.", "Any progress on this? I'm having the same issue with the current release of TF 2.0 in Colab, memory usage increases dramatically every epoch until the runtime crashes. I'm running an implementation of Wide ResNet. \r\n\r\nEDIT: I diagnosed my actual issue. Seems Python's garbage collection was not getting run before Colab ran out of memory. Forcing garbage collection with `gc.collect()` at every epoch solves the problem. Good excuse for me to learn how Python decides when to run garbage collection :).", "Don't know if I'm experiencing the same issue, or a different one, but I've also just started encountering an issue with Tensorflow sucking up all available RAM. However for me the trigger was a system upgrade.\r\n\r\nI was using Fedora 29 with Tensorflow 2.1.0, and everything worked fine. I then upgraded to Fedora 31, same exact code and Tensorflow version, and now `model.fit` sucks down all the system memory (~29gb) and OOMs. And it does this without making it through a single epoch. In fact it doesn't appear to make it through a single batch, as the keras progress indicators never show up.\r\nThe last thing that shows is `Epoch 1/30`", "Well after spending the entire day trying to get Tensorflow to build from source, my memory issue is resolved.\r\nWhen I was experiencing the issue I described earlier (`model.fit` consuming all available memory), I was using the 2.1.0 package from pip. After building Tensorflow from source (same version, 2.1.0) the issue has gone away, and Tensorflow is operating normally again.\r\n\r\nSo my guess is just some sort of incompatibility with some version of a library the pip package was built with, vs what Fedora 31 ships with.", "I still got this error when I'm running .fit method in jupyter notebook, it's same even for notebooks in kaggle kernels. Have anybody solve this issue?", "> Any progress on this? I'm having the same issue with the current release of TF 2.0 in Colab, memory usage increases dramatically every epoch until the runtime crashes. I'm running an implementation of Wide ResNet.\r\n> \r\n> EDIT: I diagnosed my actual issue. Seems Python's garbage collection was not getting run before Colab ran out of memory. Forcing garbage collection with `gc.collect()` at every epoch solves the problem. Good excuse for me to learn how Python decides when to run garbage collection :).\r\n\r\nHow do you use garbage collector at each epoch? Can you share code or something?", "> Well after spending the entire day trying to get Tensorflow to build from source, my memory issue is resolved.\r\n> When I was experiencing the issue I described earlier (`model.fit` consuming all available memory), I was using the 2.1.0 package from pip. After building Tensorflow from source (same version, 2.1.0) the issue has gone away, and Tensorflow is operating normally again.\r\n> \r\n> So my guess is just some sort of incompatibility with some version of a library the pip package was built with, vs what Fedora 31 ships with.\r\n\r\nThat's not okay when pip stable tensorflow version is not actually stable.\r\nHope somebody from Google team note anything about it.", "> > Any progress on this? I'm having the same issue with the current release of TF 2.0 in Colab, memory usage increases dramatically every epoch until the runtime crashes. I'm running an implementation of Wide ResNet.\r\n> > EDIT: I diagnosed my actual issue. Seems Python's garbage collection was not getting run before Colab ran out of memory. Forcing garbage collection with `gc.collect()` at every epoch solves the problem. Good excuse for me to learn how Python decides when to run garbage collection :).\r\n> \r\n> How do you use garbage collector at each epoch? Can you share code or something?\r\n\r\nI just ran it in a Keras callback. Still not really sure why this was necessary, haven't had a chance to dig into what was happening, but it solved my issue. \r\n\r\nMinimal example (based on tutorial: [https://www.tensorflow.org/guide/keras/custom_callback](https://www.tensorflow.org/guide/keras/custom_callback)):\r\n\r\n```\r\nimport tensorflow as tf\r\nimport gc\r\n\r\nclass MyCustomCallback(tf.keras.callbacks.Callback):\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    gc.collect()\r\n...\r\n_ = model.fit(x_train, y_train, ...\r\n          callbacks=[MyCustomCallback()])```\r\n", "> > > Any progress on this? I'm having the same issue with the current release of TF 2.0 in Colab, memory usage increases dramatically every epoch until the runtime crashes. I'm running an implementation of Wide ResNet.\r\n> > > EDIT: I diagnosed my actual issue. Seems Python's garbage collection was not getting run before Colab ran out of memory. Forcing garbage collection with `gc.collect()` at every epoch solves the problem. Good excuse for me to learn how Python decides when to run garbage collection :).\r\n> > \r\n> > \r\n> > How do you use garbage collector at each epoch? Can you share code or something?\r\n> \r\n> I just ran it in a Keras callback. Still not really sure why this was necessary, haven't had a chance to dig into what was happening, but it solved my issue.\r\n> \r\n> Minimal example (based on tutorial: https://www.tensorflow.org/guide/keras/custom_callback):\r\n> \r\n> `\r\n> import tensorflow as tf\r\n> import gc\r\n> \r\n> class MyCustomCallback(tf.keras.callbacks.Callback):\r\n> def on_epoch_end(self, epoch, logs=None):\r\n> gc.collect()\r\n> \r\n> ...\r\n> _ = model.fit(x_train, y_train, ...\r\n> callbacks=[MyCustomCallback()])\r\n> `\r\n\r\nOkay, thank you. I thought I was talking about another way for garbage collector. I use callbacks in my models. Gonna try to cure my tensorflow issue as you did.", "> > > Any progress on this? I'm having the same issue with the current release of TF 2.0 in Colab, memory usage increases dramatically every epoch until the runtime crashes. I'm running an implementation of Wide ResNet.\r\n> > > EDIT: I diagnosed my actual issue. Seems Python's garbage collection was not getting run before Colab ran out of memory. Forcing garbage collection with `gc.collect()` at every epoch solves the problem. Good excuse for me to learn how Python decides when to run garbage collection :).\r\n> > \r\n> > \r\n> > How do you use garbage collector at each epoch? Can you share code or something?\r\n> \r\n> I just ran it in a Keras callback. Still not really sure why this was necessary, haven't had a chance to dig into what was happening, but it solved my issue.\r\n> \r\n> Minimal example (based on tutorial: https://www.tensorflow.org/guide/keras/custom_callback):\r\n> \r\n> `\r\n> import tensorflow as tf\r\n> import gc\r\n> \r\n> class MyCustomCallback(tf.keras.callbacks.Callback):\r\n> def on_epoch_end(self, epoch, logs=None):\r\n> gc.collect()\r\n> \r\n> ...\r\n> _ = model.fit(x_train, y_train, ...\r\n> callbacks=[MyCustomCallback()])\r\n> `\r\n\r\nDidn't help anyway)", "The issue is similar to #35030, so you may check the detailed analysis there. Basically, the root cause is that  `tf.convert_to_tensor` is making a copy when called on NumPy Arrays, leading more memory usage in `model.fit()`.\r\n\r\nWe are working on the fix now. Please stay tuned. Thanks.", "> The issue is similar to #35030, so you may check the detailed analysis there. Basically, the root cause is that `tf.convert_to_tensor` is making a copy when called on NumPy Arrays, leading more memory usage in `model.fit()`.\r\n> \r\n> We are working on the fix now. Please stay tuned. Thanks.\r\n\r\nSo should I use tensorflow dataset instead of numpy dataset and there will no be memory leak? I'm asking about tensorflow 2.1.0 stable version.", "In my case, where I was having memory issues with the PIP version but compiled worked fine, I was using tf.data.Dataset, not numpy. So there may be more than one leak going around.\n\nOn February 8, 2020 3:07:56 AM EST, Dmitry Kravchuk <notifications@github.com> wrote:\n>> The issue is similar to #35030, so you may check the detailed\n>analysis there. Basically, the root cause is that\n>`tf.convert_to_tensor` is making a copy when called on NumPy Arrays,\n>leading more memory usage in `model.fit()`.\n>> \n>> We are working on the fix now. Please stay tuned. Thanks.\n>\n>So should I use tensorflow dataset instead of numpy dataset and there\n>will no be memory leak? I'm asking about tensorflow 2.1.0 stable\n>version.\n>\n>-- \n>You are receiving this because you commented.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/31312#issuecomment-583714439\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "> In my case, where I was having memory issues with the PIP version but compiled worked fine, I was using tf.data.Dataset, not numpy. So there may be more than one leak going around.\r\n> [\u2026](#)\r\n> On February 8, 2020 3:07:56 AM EST, Dmitry Kravchuk ***@***.***> wrote: > The issue is similar to #35030, so you may check the detailed analysis there. Basically, the root cause is that `tf.convert_to_tensor` is making a copy when called on NumPy Arrays, leading more memory usage in `model.fit()`. > > We are working on the fix now. Please stay tuned. Thanks. So should I use tensorflow dataset instead of numpy dataset and there will no be memory leak? I'm asking about tensorflow 2.1.0 stable version. -- You are receiving this because you commented. Reply to this email directly or view it on GitHub: [#31312 (comment)](https://github.com/tensorflow/tensorflow/issues/31312#issuecomment-583714439)\r\n> -- Sent from my Android device with K-9 Mail. Please excuse my brevity.\r\n\r\nI'll try tensorflow dataset class instead of numpy. Thx.", "Similar leakage case while using tf.data.Dataset on Colab.", "@noodlez04 I think this was resolved in recent `tf-nightly`. I ran your code in `GPU` and fit took 57.86 MB. [Here is a gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/ecf67b62788981093e5941943c33fe95/untitled876.ipynb) with GPU using `tf-nightly`.\r\n\r\nWhen I ran same code in CPU with `tf-nightly`, fit took 17.77 MB. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/864028e369e9f32ded61804fed2d55b6/untitled877.ipynb). Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "As it's already fixed, let's close this bug. Feel free to reopen it if needed.", "this bug has returned in tf 2.3.0 ", "> The issue is similar to #35030, so you may check the detailed analysis there. Basically, the root cause is that `tf.convert_to_tensor` is making a copy when called on NumPy Arrays, leading more memory usage in `model.fit()`.\r\n> \r\n> We are working on the fix now. Please stay tuned. Thanks.\r\n\r\n@yhliang2018 This conversion happens in TF 2.2 when NumPy arrays are passed to `model.fit()`. https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/data_adapter.py#L265\r\n", "@nihir27 Hi. Are there any updates on this issue? Essentially no training process can be completed while this is not resolved. ", "@lidless-vision @nihir27 @markodjordjic What TF version do you use? Could you try tf-nightly? If this issue still exists in tf-nightly, would you provide a small colab example for repro? So we can investigate the problem.\r\n\r\nBTW, feel free to open a new issue if needed.", "@yhliang2018 any TF upwards 2.0.0. The gist to what you are referring to is nowhere near a hp tuning process which can produce memory leak.", "Yeah, TF2.0.0 does have this issue, and it should be fixed in the following versions (unless there is regression in some later version). So you may try the latest stable version(2.3.0) or tf-nightly which should have no such memory leak.", "@yhliang2018 every version upwards from 2.0.0 has exactly the same issue and there is a lot of people talking about it around GH however no progress is being made.", "@yhliang2018 I have opened new issue: https://github.com/tensorflow/tensorflow/issues/43627", "@yhliang2018 However I think it just creates clutter like many other issues that have the same underlying problem, and have stayed unresolved, or someone has made a statement that version X.X is working, while it is not. ", "@yhliang2018 thanks for responding, but what is really needed is that someone pushes for this.", "@yhliang2018 TF2.3.0 nor nightly is not working, just to confirm.\r\nalgo-1-eprqd_1  | Fitting fold: 0 \r\nalgo-1-eprqd_1  | memory used: 2072.0\r\nalgo-1-eprqd_1  | Fitting fold: 0 \r\nalgo-1-eprqd_1  | memory uses: 2097.0\r\nalgo-1-eprqd_1  | Fitting fold: 0 \r\nalgo-1-eprqd_1  | memory uses: 2104.0\r\nalgo-1-eprqd_1  | Fitting fold: 0 \r\nalgo-1-eprqd_1  | memory uses: 2114.0\r\nalgo-1-eprqd_1  | Fitting fold: 0 for estimator: FFD AEN, with HP: (Dep.: 3, wid.: 26, red.:0.2, epo.:50)\r\n. . .\r\nalgo-1-eprqd_1  | memory used: 2188.0", "> @noodlez04 I think this was resolved in recent `tf-nightly`. I ran your code in `GPU` and fit took 57.86 MB. [Here is a gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/ecf67b62788981093e5941943c33fe95/untitled876.ipynb) with GPU using `tf-nightly`.\r\n> \r\n> When I ran same code in CPU with `tf-nightly`, fit took 17.77 MB. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/864028e369e9f32ded61804fed2d55b6/untitled877.ipynb). Thanks!\r\n> \r\n> Please close the issue if this was resolved for you. Thanks!\r\n\r\nAccording to this comment, we thought this issue was fixed. It might not be true in the following stable versions, sorry about it.\r\n\r\nI will look into the issue #43627 and keep you updated on the progress.", "@yhliang2018 Thank you.", "any update on this on any suggestion on a particular tf version which does not have this issue?", "Have the same problem. using model.fit() bit it stops at epoch 18 while loading the dlls. The committed RAM stacks too high.", "@ml118 and @gioxc88 Thank you for keeping this topic alive. It is sad that this issue is not yet resolved. The only workaround I was able to find is to avoid the usage of ``model.fit`` methods and run a custom training process as described in:\r\n\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\nThere is also the issue #43627 that is not attended at all, and most likely connected to this issue.", "I'll also link this other one which I opened a a few weeks ago [#45592](https://github.com/tensorflow/tensorflow/issues/45592)\r\n", "@gioxc88 Yup. That's the same issue. Incredible how wide spread the problem is, and it is still persisting; not being attended to.", "@markodjordjic Thanks for sending the link about implementing the training from scratch. Seems like i have no other solution and has to try this out.", "@ml118 You're welcome. I'm glad if I can be of assistance, especially since TF is not doing anything. I need to add a note, that the problem will not be completely removed, but it will be reduced to the point that training is possible. Also, it is recommended to remove all the unnecessary objects like the ``keras.model`` at the first point possible, and subsequently use ``gc.collect()`` and ``tf.keras.backend.clear_session()``. Let me know how this works out for you.", "@markodjordjic first i will try to expand the paging file (i am on Win10) and see if that helps. It isnt possible to use `gc.collect()` and `tf.keras.backend.clear_session()` in `model.fit()`, right? So doing the training from scratch will be my last solution. But i will try it if expanding the paging file wont help. And chare my results here for sure :)", "I've worked with this issue for years now, there's truly no solution. Good luck on your tests, but don't have high hopes. The workaround is really simple: just have your training phase in a dedicated script, call it as a shell command `os.process # or whatever`, save weights in that script and load them post-train in the calling script.", "@lefnire Oh ok. You mean something like `model.fit()` for 10 epochs. Then safe model, load again and continue for another 10 epochs? Or did you write the training from scratch and then calling it with a shell command?", "@lefnire Have you tried to use:\r\n\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\nIt has solved the problem for me. I was able to complete numerous quite lengthy training jobs on AWS without ever needing to go into saving and restoring model weights in between (certain number of) epochs.", "@ml118 There is no advantage in running the training from scratch with a shell command. Simply run the script for a training job from scratch, delete objects, ``gc.collect()`` them, and clear the session. An increase in memory will be minimal.", "Ah, I guess my situation's a bit different. I serve a model for inference, which can online-train new data. Since it's forever-on, `fit()`'s mem-leak is a show-stopper eventually. So what I do is:\r\n\r\n* run.py -> receives jobs, runs inference or training\r\n* train.py -> training only. saves model after done\r\n* inference.py -> inference only, can be imported & called directly from run.py\r\n\r\nrun.py: if it needs to train, it calls `os.subprocess('./train.py')` (I can't remember actual syntax) which will save the model after completion. The os call is blocking, so it knows when it's done; no coordination necessary. Then inference.py loads the model before use each time. \r\n\r\nKinda complex sounding, but it's really simple. TL;DR: put training in a separate script. Point being when the script dies, all RAM is re-claimed. Maybe not what you're looking for in your situation. @markodjordjic man alive, I've tried every possible combo of `clear_session()` `gc.collect()` etc, I've tried so many variants of that, don't know how you got it working but I'm cashed out \ud83d\ude13 ", "@lefnire Many thanks for sharing the details of your approach. I find it very useful for OL training. The case here was for off-line training (especially dockerized via AWS) and the secret of success was to drop the use of `model.fit()` an switch completely to training jobs from scratch. This minimized the consumption of RAM after every epoch, and allowed for training to finish.", "I raised the paging file and got to Epoch 26. Its an improvement, not very good, but better. Another thought i have is just train for 20 Epochs. Safe the model, load it again and train for another 20 Epochs with the same training/validation split. This should also work, right?", "@ml118 I have already given my best advice, and an example of what worked for me. And that is training from scratch without using ``model.fit()`` method, described here:\r\n\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\nAfter ending all the epochs, you should clear session, delete model, and ``gc.collect()``.\r\n\r\nIf you wish to use any other approach (eg. going for 20 epochs, saving, restoring, etc.) that is up to you. I have no information about that, and I cannot give any advice, except that it is unnecessary and cumbersome, especially since the training from scratch works. \r\n\r\n", "Any updates on this issue? I implemented a large and complex network inheriting from keras.Model, so of course it uses the builtin fit method. After some testing, I wanted to scale my experiments up and it looks like this is absolutely impossible with this bug. I'll try to go for the training loop from scratch solution, but I'm somewhat suprised that this issue still seems to persist in tf version 2.4.1. ", "Update: I used a combination of the workarounds recommended by @gabehope and @markodjordjic with success. I can use `model.fit()` with the following callback:\r\n\r\n```\r\nclass MyCustomCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        gc.collect()\r\n        tf.keras.backend.clear_session()\r\n```\r\n\r\nMemory leaks seem to be still present, but very minor. I can train my model now.", "Thanks to everyone who was persistently pushing for this issue to not get inactive. As basically everyone who uses TensorFlow 2.x.x and the built-in functions like **model.fit()** I was experiencing the **following issues**:\r\n1. **Loads of RAM usage** even though I am running NVIDIA GeForce RTX 2080 TI GPUs.\r\n2. **Increasing epoch times** as training progresses.\r\n3. Some kind of **memory leakage** (feels like it was somewhat linear).\r\n\r\nAll of these issues have been fixed by the **solution** posted by @rschiewer (https://github.com/tensorflow/tensorflow/issues/31312#issuecomment-813944860) and by **adding `run_eagerly=True`** in the **`model.compile()`** function. Here is a code snippet for reference:\r\n\r\n```python\r\nimport gc\r\nfrom tensorflow.keras import backend as k\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\n\r\nclass ClearMemory(Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        gc.collect()\r\n        k.clear_session()\r\n\r\n...\r\n\r\nmodel.compile(\r\n    ...,\r\n    run_eagerly=True\r\n)\r\n\r\nmodel.fit(\r\n    ...,\r\n    callbacks=ClearMemory()\r\n)\r\n```\r\n\r\nWith these solutions I am now able to train with less RAM being occupied, training is faster, epoch times stay constant and if there still is memory leakage it is negligible.\r\n\r\nHope this might help others, too.\r\n\r\n----------\r\n\r\nNotes\r\n- If none of the above works for you you might want to try writing your own training loop in TensorFlow. [Here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch) is a guide on how to do it.\r\n- People have also been reporting that using `tcmalloc` instead of the default `malloc` allocater alleviated the memory leakage to some degree. For references see [here](https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/) or [here](https://riptutorial.com/tensorflow/example/13427/use-the-tcmalloc-allocator).\r\n", "Using `run_eagerly=True` like @paweller suggests fixes the memory leak for me without any other action needed. **However, as far as I'm aware it completely removes the benefit of graph optimizations and should mean a noticeable performance hit**. For my application, it did. \r\n\r\nTwo findings if yout don't want to use `run_eagerly=True`:\r\n\r\n1. I tried to use tmalloc with no change regarding the memory leak. \r\n2. I'm currently not able to find the related issue here on github, but I read elswhere that the problem might come from using the builtin relu activation function in one or more layers. My workaround is to use `activation=None` and add a ReLu activation layer after each of my layers. While this increases clutter and sounds more than a bit esoteric, it eliminated memory leakage in my project without the need for other measures. I'm running my code in graph mode.\r\n\r\n\r\n\r\n", "@rschiewer you may mean #46475 and/or #46661.", "> **However, as far as I'm aware it completely removes the benefit of graph optimizations and should mean a noticeable performance hit**.\r\n\r\n@rschiewer: How does it make sense then that my training times per epoch decreased siginificantely to about 1/3 of what they were before? Or may that be caused by the `ClearMemory` callback? I cannot imagine it does but I might be wrong. Anyways, I will give your finding 2. a try as I am indeed using ReLU as the activation function for all my convolutional layers. I will let you know what it did for me.\r\n**Edit**: I have to revise my statement that epoch times decrease *significantly*. After doing another training on the same machine a decrease is still noticable, however it is \"only\" about 10 % and not as initially stated 66 %ish.\r\n\r\nThanks to @bersbersbers for providing the related issue links.\r\n", "@bersbersbers Yes exactly, it's [#46475](https://github.com/tensorflow/tensorflow/issues/46475). Thanks for mentioning it here!\r\n\r\n@paweller I'd be a lucky man if I understood how tensorflow graph mode really works. If your training runs faster in eager mode,  there's maybe something going wrong during the generation of the optimized graph. According to my experience, using graph mode is usually faster than not using it. But I've also sunken more than a few hours into debugging cryptic autograph errors, often enough without final success. So you might be better off with the eager execution mode, which at least works.", "I have now tried putting the activation function as an additional layer and changing it from `ReLU` to `LeakyReLU`. It solved the memory leak for me as well. I also tried training on a different machine. Here it does not make a difference in performance whether I use `run_eagerly=True` or not. Epoch times stay the same regardless (the model is a small one with only four convolutional, one dense and one lambda layer).", "I am not a 100% sure about this topic, but it seems the latests gpu docker image fixed the leakage problem for me. Maybe give that a try.", "@UntotaufUrlaub which version is it please ? ", "@wissambenhaddad I used `FROM tensorflow/tensorflow:latest-gpu`.\r\nSo i guess the latest version available back in May. Sadly I got no clue how to tell the exact tag of it today. "]}, {"number": 31311, "title": "[INTEL MKL] Enable MKL MatMul in eager mode", "body": "This PR depends on https://github.com/tensorflow/tensorflow/pull/30402 and should be merged  after it.", "comments": ["@penpornk "]}]