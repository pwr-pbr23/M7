[{"number": 52366, "title": "tf failed to allocate memory on GPU ", "body": "I have a similar problem. \r\nI tried batch size 64 or 32,16,8 all failed. \r\nI also downgrade to tensorflow and tensorflow-gpu to 2.5 and still getting the same error. \r\n\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: failed to allocate memory [Op:Mul]\r\n\r\nHere is the link to the error: https://pastebin.com/raw/TMSCH9Va\r\n\r\nI ran the same code yesterday and it was fine with batch size 64. But I needed to stop to change something and after that I keep getting this error. \r\n\r\n<img width=\"731\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137164238-6f39cb77-740d-40ff-9040-31eaed1307de.png\">\r\n\r\nThere is no running process and still \r\n\r\nSum Total of in-use chunks: 7.17GiB\r\n2021-10-13 11:14:03.092854: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] total_region_allocated_bytes_: 7773224960 memory_limit_: 7773224960 available bytes: 0 curr_region_allocation_bytes_: 15546449920\r\n2021-10-13 11:14:03.092863: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Stats:\r\nLimit:                      7773224960\r\nInUse:                      7702514176\r\nMaxInUse:                   7702514688\r\nNumAllocs:                        2285\r\nMaxAllocSize:               1761195520\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2021-10-13 11:14:03.092920: W tensorflow/core/common_runtime/bfc_allocator.cc:467] ***************************************************************************************************x\r\n\r\n$pip freeze\r\nabsl-py==0.14.1\r\nalembic==1.4.3\r\nargon2-cffi==20.1.0\r\nasn1crypto==0.24.0\r\nastunparse==1.6.3\r\nasync-generator==1.10\r\nattrs==20.3.0\r\nbackcall==0.2.0\r\nbackports.entry-points-selectable==1.1.0\r\nbleach==3.2.1\r\ncached-property==1.5.2\r\ncachetools==4.2.4\r\ncertifi==2020.12.5\r\ncertipy==0.1.3\r\ncffi==1.12.3\r\nchardet==3.0.4\r\nclang==5.0\r\nconda==4.8.1\r\nconda-package-handling==1.3.11\r\ncryptography==2.7\r\ncycler==0.10.0\r\ndecorator==4.4.2\r\ndefusedxml==0.6.0\r\ndistlib==0.3.3\r\nentrypoints==0.3\r\nfilelock==3.3.0\r\nflatbuffers==1.12\r\nfuture==0.18.2\r\ngast==0.4.0\r\ngoogle-auth==1.35.0\r\ngoogle-auth-oauthlib==0.4.6\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.34.1\r\nh5py==3.1.0\r\nidna==2.8\r\nimportlib-metadata==3.4.0\r\nipykernel==5.4.3\r\nipython==7.19.0\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.1\r\njedi==0.18.0\r\nJinja2==2.11.2\r\njoblib==1.1.0\r\njson5==0.9.5\r\njsonschema==3.2.0\r\njupyter-client==6.1.11\r\njupyter-core==4.7.0\r\njupyter-telemetry==0.1.0\r\njupyterhub==1.2.2\r\njupyterlab==2.2.9\r\njupyterlab-pygments==0.1.2\r\njupyterlab-server==1.2.0\r\nkeras==2.6.0\r\nkeras-nightly==2.5.0.dev2021032900\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.3.2\r\nlibarchive-c==2.8\r\nlibclang==11.1.0\r\nllvmlite==0.37.0\r\nMako==1.1.4\r\nMarkdown==3.3.4\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.4.3\r\nmistune==0.8.4\r\nmtcnn==0.1.1\r\nnbclient==0.5.1\r\nnbconvert==6.0.7\r\nnbformat==5.1.1\r\nnbgitpuller==0.9.0\r\nnbresuse==0.3.6\r\nnest-asyncio==1.4.3\r\nnotebook==6.1.5\r\nnteract-on-jupyter==2.1.3\r\nnumba==0.54.1\r\nnumpy==1.19.5\r\noauthlib==3.1.0\r\nopencv-python==4.5.3.56\r\nopt-einsum==3.3.0\r\npackaging==20.8\r\npamela==1.0.0\r\npandas==1.3.3\r\npandocfilters==1.4.3\r\nparso==0.8.1\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nPillow==8.3.2\r\nplatformdirs==2.4.0\r\nprometheus-client==0.9.0\r\nprompt-toolkit==3.0.10\r\nprotobuf==3.18.1\r\npsutil==5.8.0\r\nptyprocess==0.7.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycosat==0.6.3\r\npycparser==2.19\r\nPygments==2.7.4\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.7\r\npyrsistent==0.17.3\r\nPySocks==1.7.0\r\npython-dateutil==2.8.1\r\npython-editor==1.0.4\r\npython-json-logger==2.0.1\r\npytz==2021.3\r\npyzmq==21.0.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.7.2\r\nruamel.yaml.clib==0.2.2\r\nruamel_yaml==0.15.46\r\nscikit-learn==1.0\r\nSend2Trash==1.5.0\r\nsix==1.15.0\r\nSQLAlchemy==1.3.22\r\ntb-nightly==2.7.0a20211010\r\ntensorboard==2.6.0\r\ntensorboard-data-server==0.6.1\r\ntensorboard-plugin-wit==1.8.0\r\ntensorflow==2.5.0\r\ntensorflow-estimator==2.5.0\r\ntensorflow-gpu==2.5.0\r\ntensorflow-io-gcs-filesystem==0.21.0\r\ntermcolor==1.1.0\r\nterminado==0.9.2\r\ntestpath==0.4.4\r\nthreadpoolctl==3.0.0\r\ntorch==1.6.0\r\ntornado==5.1.1\r\ntqdm==4.32.1\r\ntraitlets==5.0.5\r\ntyping-extensions==3.7.4.3\r\nurllib3==1.24.2\r\nvirtualenv==20.8.1\r\nvirtualenv-clone==0.5.7\r\nwcwidth==0.2.5\r\nwebencodings==0.5.1\r\nWerkzeug==2.0.2\r\nwidgetsnbextension==3.5.1\r\nwrapt==1.12.1\r\nzipp==3.4.0\r\n\r\n_Originally posted by @maryamxasghari in https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-942425438_", "comments": ["Hi @maryamxasghari!Did you try restricting physical memory growth  as described [here](https://github.com/tensorflow/tensorflow/issues/51354#issuecomment-895316354). Please share a stand alone code to proceed further.", "Hi @mohantym, yes I tried that and it didn't make any difference. I'm still getting the same error. I ran this code on google colab and on my laptop without GPU and everything was okay. \r\n\r\nAlso the first time on the server was okay but then tenser flow didn't clear the memory and it still \ud83d\udc4d\ud83c\udffc \r\n\r\n2021-10-13 19:18:41.241950: I tensorflow/core/common_runtime/bfc_allocator.cc:1058] Sum Total of in-use chunks: 6.78GiB\r\n2021-10-13 19:18:41.241957: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] total_region_allocated_bytes_: 7773224960 memory_limit_: 7773224960 available bytes: 0 curr_region_allocation_bytes_: 8589934592\r\n2021-10-13 19:18:41.241968: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Stats:\r\nLimit:                      7773224960\r\nInUse:                      7284753152\r\nMaxInUse:                   7318323456\r\nNumAllocs:                        2280\r\nMaxAllocSize:               1761195520\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\nThis is my code: \r\n\r\nif __name__ == '__main__':\r\n \r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n    gpu = tf.config.list_physical_devices('GPU')\r\n    tf.config.experimental.set_memory_growth(gpu[0], True)\r\n    \r\n    tf.keras.backend.clear_session()\r\n\r\n    batch_size = cfg['batch_size']\r\n    train_lines = read_annotation_lines('./train.txt')\r\n    val_lines = read_annotation_lines('./val.txt')\r\n    train_path ='./data/train' \r\n    val_path = './data/val'\r\n    class_name_path = './class.txt'\r\n    train_data_gen = DataGenerator(train_lines, class_name_path, train_path)\r\n    val_data_gen = DataGenerator(val_lines, class_name_path, val_path)\r\n\r\n    checkpoint_path = './training/test4/weight{epoch:04d}.h5'\r\n    checkpoint_dir = os.path.dirname(checkpoint_path)\r\n    # Create a callback that saves the model's weights\r\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n                                                     save_weights_only=True,\r\n                                                     mode='auto', save_freq='epoch',\r\n                                                     monitor='val_loss',\r\n                                                     save_best_only=False,\r\n                                                     verbose=1)\r\n\r\n    model = Yolov4(weight_path='face.weights',class_name_path=class_name_path)\r\n\r\n    model.fit(train_data_gen, \r\n      initial_epoch=0,\r\n      batch_size = 64,\r\n      epochs=3000, \r\n      val_data_gen=val_data_gen,\r\n      callbacks=[cp_callback])\r\n\r\n    model.save_model('./training/test4/yolov4')\r\n\r\nThank you for your help", "ok @maryamxasghari ! Could you also share the links to datasets and checkpoint files through a Github repository too as it will help expedite the issue.Thanks!", "@mohantym The data is from : http://vis-www.cs.umass.edu/lfw/ \r\nAnd I trained for 20 epochs and needed to stop so I only have the weights here is the last one that saved : https://drive.google.com/file/d/1VRsK2RRwEGsyZbmELpq1d3nj-Jx8G1fl/view?usp=sharing", "Hi @sanatmpa1! Could you please look at this issue!", "@mohantym @sanatmpa1 \r\nI asked the Admin to restart the server so the memory will be available again. Now after that I tried to run the same code and I'm getting another error this time which I didn't have before. \r\n\r\nError: full error : https://pastebin.com/raw/E9g0ZGhB \r\n<img width=\"492\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137433378-fa990746-40c1-4940-a6d6-f5718e4a8d46.png\">\r\n\r\nIn my environment I have the latest version available : \r\n\r\n### System information\r\n* OS Platform and Distribution: Ubuntu 20.04\r\n* TensorFlow installed from (source or binary):  pip install --upgrade tensorflow-gpu\r\n* TensorFlow version: \r\n      tensorflow==2.6.0\r\n      tensorflow-estimator==2.6.0\r\n      tensorflow-gpu==2.6.0\r\n      tensorflow-io-gcs-filesystem==0.21.0\r\n      tf-estimator-nightly==2.7.0.dev2021092408\r\n      tf-nightly==2.7.0.dev20210922\r\n* Python version: Python 3.7.3\r\n* Installed using virtualenv? pip? conda?: (condo env) pip install --upgrade tensorflow-gpu\r\n* Bazel version (if compiling from source): N/A\r\n* GCC/Compiler version (if compiling from source): N/A\r\n* CUDA/cuDNN version: conda install cudnn=8.2.1=cuda11.3_0\r\n* <img width=\"775\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137433086-880bc549-ce5b-4a2a-9783-f3c886ed72d6.png\">\r\n<img width=\"497\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137433157-e0945319-aed7-406c-817a-19b332d57364.png\">\r\n* GPU model and memory: \r\n<img width=\"727\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137434119-0d33c6fe-d1a4-45ea-a450-afa7f4c6877a.png\">\r\n", "@sanatmpa1 @mohantym\r\n\r\nI updated the cuda and cudnn to solve the problem of the cudnn version. And now again back to the same memory issue. This time I didn't run any code and I don't understand the problem. \r\n\r\n<img width=\"616\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137601814-249841ec-ed6b-4263-9c72-f5c63de4bb3a.png\">\r\n\r\n<img width=\"609\" alt=\"image\" src=\"https://user-images.githubusercontent.com/74671619/137601822-ad0fd33e-7760-47f0-8a41-0436e101af8d.png\">\r\n\r\n\r\nI have this on top of my code:\r\n```python\r\n    #only use GPU 0 \r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n    \r\n    #If memory growth is enabled for a PhysicalDevice, the runtime initialization will not allocate all memory on the device. \r\n    #https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\r\n    gpu = tf.config.list_physical_devices('GPU')\r\n    tf.config.experimental.set_memory_growth(gpu[0], True)\r\n```\r\n", "@maryamxasghari,\r\n\r\nWhich version of cuda and cudnn are you using? From this [comment](https://github.com/tensorflow/tensorflow/issues/52366#issuecomment-944001829) I can see that you mentioned cudnn = 8.2.1 and cuda=11.3_0, whereas the tested build configuration for TF 2.6  uses cudnn = 8.1 and cuda = 11.2. You can refer [here](https://www.tensorflow.org/install/source#gpu)\r\n\r\nAlso have you got a chance to take a look at this [SO thread](https://stackoverflow.com/questions/39758094/clearing-tensorflow-gpu-memory-after-model-execution)?  You can try clearing your gpu memory before running the code and see if it helps? Thanks!", "@sanatmpa1 As I mentioned in https://github.com/tensorflow/tensorflow/issues/52366#issuecomment-945006275 I ended up updating them right now I have cudnn 8.2.4.15 and cuda 11.4 , and with that update the TensorFlow version: 2.8.0-dev20211019. \r\n\r\nI checked the thread you mentioned they suggest to use :\r\nfrom numba import cuda \r\ndevice = cuda.get_current_device()\r\ndevice.reset()\r\n\r\nBut some people mentioned that this will leave the GPU in bad stats. Is it a safe thing to try?", "@maryamxasghari,\r\n\r\nNot sure if Cuda 11.4 is supported already as I see this an open issue for now,  #51659. So can you have the environment setup as per [tested build config](https://www.tensorflow.org/install/source#gpu) to see if it helps.\r\n\r\nIn case if you don't want to try out numba, can you try adding the below line to the end of your code, which will kill the kernel upon completion?\r\n\r\n```python\r\nimport os\r\n \r\npid = os.getpid()\r\n!kill -9 $pid\r\n```", "@sanatmpa1 we ended up restarting the server to solve the problem. \r\nThen we changed the code and tried with only batch size 2 and everything is working. \r\nThank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52366\">No</a>\n"]}, {"number": 52365, "title": "'tf.Selu' op is neither a custom op nor a flex op", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS: Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: GTX 1060\r\n\r\n**Describe the current behavior**\r\nError when quantizing a model with selu activation.\r\n\r\n**Describe the expected behavior**\r\nQuantize selu activation.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no (sorry)\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport pathlib\r\n\r\ndef get_model():\r\n    input_layer = tf.keras.Input(shape=[128], dtype=tf.float32)\r\n    dense = tf.keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu')(input_layer)\r\n    return tf.keras.models.Model(inputs=input_layer, outputs=dense)\r\n\r\ndef quantize(model):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_types = [tf.float16]\r\n    tflite_model = converter.convert()\r\n\r\n    tflite_models_dir = pathlib.Path('.')\r\n    tflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n    tflite_model_file = tflite_models_dir/'unquantizable.tflite'\r\n    tflite_model_file.write_bytes(tflite_model)\r\n    pass\r\n\r\nif __name__ == '__main__':\r\n    model = get_model()\r\n    quantize(model)\r\n    pass\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nerror: 'tf.Selu' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main':\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nTF Select ops: Selu\r\nDetails:\r\n        tf.Selu(tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = \"\"}\r\n```", "comments": ["I see this is a feature missing. it works when enabling the tf ops flag. I was worried I could not use my trained weights anymore.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52365\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52365\">No</a>\n"]}, {"number": 52364, "title": "Performance of a model loading the same weights changes depending on the version of tensorflow ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.3 & 2.6.0\r\n- Python version: 3.6.13\r\n\r\n**Describe the current behavior**\r\nI received weights of 3 keras models (ResNet50V2, EfficientNetB0, and VGG16) that had been trained with a version of tensorflow 2.x (I don't know and can't find out). I do know the weights were created with the keras model checkpoint callback. I created a new venv and pip installed version 2.6.0 and loaded the weights and tested out the performance on the task. It was completely different to expected performance and terrible. It occurred for all three networks. I spent a while trying to trouble shoot and eventually thought I would try with an older version (2.4.3), and performance was as expected.\r\n\r\n**Describe the expected behavior**\r\nPerformance should not be different for models loading the same weights under 2.4.3 and 2.6.0.\r\n", "comments": ["@Jon573 ,\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "@tilakrayal \r\nSorry, I was unaware of the change. I will post it over there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52364\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52364\">No</a>\n"]}, {"number": 52363, "title": "Input name order breaks input alignment during training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see bellow.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur (11.5.2)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): --\r\n- GCC/Compiler version (if compiling from source): --\r\n- CUDA/cuDNN version: --\r\n- GPU model and memory: --\r\n\r\n**Describe the current behavior**\r\n\r\nI am training a model based on tabular/ structured data. The model itself is a combination of 2 encoders and a simple layer (model) to compute the losses (See example bellow).\r\n\r\nDepending on the input name it breaks the \"data alignment\"  during training. For instance, when the `INPUT_BX_NAME` is equal to `b0`, the model trains without any issue, but if `INPUT_BX_NAME` is set to `bx`, then it raises the follow exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bug-example.py\", line 45, in <module>\r\n    model.fit([x_a, x_b], y)\r\n  File \"/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/keras/engine/training.py\", line 1184, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 950, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\r\n    return graph_function._call_flat(\r\n  File \"/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\r\n    outputs = execute.execute(\r\n  File \"/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  Cast string to float is not supported\r\n\t [[node model_2/Cast_3 (defined at bug-example.py:45) ]] [Op:__inference_train_function_970]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\nI guess the same issue happens when the input's data types are the same, but probably it's a silent bug. \r\n\r\n**Describe the expected behavior**\r\nThe training should consistently works regardless of the name of the inputs.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nINPUT_BX_NAME = \"bx\"\r\n\r\n\r\n# NOTE: Encoder A\r\ninput_a0 = tf.keras.Input(shape=(1,), name=\"a0\", dtype=\"float32\")\r\ninput_a1 = tf.keras.Input(shape=(1,), name=\"a1\", dtype=\"float32\")\r\nall_feature_a = tf.keras.layers.concatenate([input_a0, input_a1])\r\noutput_a = tf.keras.layers.Dense(4, activation=\"relu\")(all_feature_a)\r\nencoder_a = tf.keras.Model(inputs=[input_a0, input_a1], outputs=output_a)\r\n\r\n# NOTE: Encoder B\r\ninput_b1 = tf.keras.Input(shape=(1,), name=\"b1\", dtype=\"float32\")\r\ninput_bx = tf.keras.Input(shape=(1,), name=INPUT_BX_NAME, dtype=\"string\")\r\nencoded_bx = tf.keras.layers.StringLookup(\r\n    vocabulary=(\"a\", \"b\", \"c\"), output_mode=\"one_hot\"\r\n)(input_bx)\r\nall_feature_b = tf.keras.layers.concatenate([encoded_bx, input_b1])\r\noutput_b = tf.keras.layers.Dense(4, activation=\"relu\")(all_feature_b)\r\nencoder_b = tf.keras.Model(inputs=[input_bx, input_b1], outputs=output_b)\r\n\r\n# NODE: Final model\r\na_inputs = encoder_a.inputs\r\nb_inputs = encoder_b.inputs\r\nencoded_a = encoder_a(a_inputs)\r\nencoded_b = encoder_b(b_inputs)\r\nprob = tf.keras.layers.Dense(1, activation=\"sigmoid\")(tf.abs(encoded_a - encoded_b))\r\nmodel = tf.keras.Model(inputs=[a_inputs, b_inputs], outputs=prob)\r\n\r\nx_a = dict(\r\n    a0=tf.constant([0, 1, 0, 1, 0, 1]),\r\n    a1=tf.constant([1, 1, 0, 0, 0, 1]),\r\n)\r\nx_b = {\r\n    \"b1\": tf.constant([1, 0, 0, 1, 0, 0]),\r\n    INPUT_BX_NAME: tf.constant([\"a\", \"b\", \"c\", \"a\", \"b\", \"a\"]),\r\n}\r\ny = tf.constant([1, 0, 1, 0, 1, 0])\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    loss=tf.keras.losses.binary_crossentropy,\r\n    metrics=[\"acc\"],\r\n)\r\nmodel.fit([x_a, x_b], y)\r\n```", "comments": ["Hi @Saduf2019 ! Could you please look at this issue ? It is replicating in [2.5](https://colab.research.google.com/gist/mohantym/77982755cf7114e950011c34758b0b4d/github_52366.ipynb#scrollTo=Q1kruStIfzGL),[2.6](https://colab.research.google.com/gist/mohantym/4dc0742cc95a9f0b22de8bfb00d16c3f/github_52366.ipynb#scrollTo=VYYYqiqKf1mo) and [2.7](https://colab.research.google.com/gist/mohantym/9dd7a04e33a25a08db82ef4bc5cb80ba/github_52366.ipynb#scrollTo=VYYYqiqKf1mo).", "@tlewin \r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52363\">No</a>\n"]}, {"number": 52362, "title": "the usage of -tf-input-shapes", "body": "Hi,\r\nI tried to use `tf-mlir-translate` to convert tensorflow graph to mlir file.\r\nI am confused about the usage of `-tf-input-shapes`\r\nHere is my code.\r\n![\u5716\u7247](https://user-images.githubusercontent.com/40784675/137134088-059a686d-cb4c-4805-b13c-4e40812f0c82.png)\r\n\r\nAnd here is the command I used.\r\n![\u5716\u7247](https://user-images.githubusercontent.com/40784675/137134042-a662d1ad-12d4-430c-88d9-75481a297a6f.png)\r\n\r\nThanks!\r\n", "comments": ["@chiachia36 \r\nPlease share the code and error in text format for us to look into the issue and other user could refer to it in case of similar issue.", "\r\nHere is my command.\r\n`tf-mlir-translate -graphdef-to-mlir -tf-enable-shape-inference-on-import=false pbtxt/model_conv_output.pbtxt -tf-input-arrays=x -tf-input-data-types=DT_INT32 -tf-input-shapes=1,28,28,3 -o model_conv.mlir\r\n`\r\n\r\nHere is the error message.\r\n`<unknown>:0: error: loc(\"sequential/conv2d/Conv2D\"): 'tf.Conv2D' op inferred type(s) 'tensor<1x?x?x?xi32>' are incompatible with return type(s) of operation 'tensor<*xf32>'\r\n<unknown>:0: note: loc(\"sequential/conv2d/Conv2D\"): see current operation: %0 = \"tf.Conv2D\"(%arg0, %outputs_4) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<1x28x28x3xi32>, tensor<*xf32>) -> tensor<*xf32>\r\n`\r\n\r\nHere is my code.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense, Activation\r\nfrom tensorflow.keras import Sequential\r\n\r\ninput_shape = (1, 28, 28, 3)\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Conv2D(2, 3, activation='relu', input_shape=input_shape[1:])\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nfrozen_out_path = 'pbtxt'\r\nfrozen_graph_filename = \"model_conv_output\"\r\nmodel_out = model\r\nfull_model = tf.function(lambda x:model_out(x))\r\nprint(model_out.input[0])\r\nfull_model = full_model.get_concrete_function(tf.TensorSpec(model_out.inputs[0].shape, model_out.inputs[0].dtype))\r\nfrozen_func = convert_variables_to_constants_v2(full_model)\r\nfrozen_func.graph.as_graph_def()\r\n\r\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=frozen_out_path, name=f\"{frozen_graph_filename}.pbtxt\", as_text=True)\r\n```\r\nThanks!\r\n", "@chiachia36 \r\nI ran the code shared please find the [gist here](https://colab.research.google.com/gist/Saduf2019/abe224b0bc3aa0b63ea5524b22e22cb0/untitled645.ipynb), i do not see the error reported. can you please try it on colab and let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52361, "title": "build_pip_package broken by removal of keras/api directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nbuild_pip_pacakge fails with error output\r\ncp: cannot stat /tmp/tmp.enXUtfEE9u/tensorflow/python/keras/api/_v2/keras/: No such file or directory\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n$ bazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n$ mkdir tensorflow-pkg\r\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package --cpu --project_name tensorflow_aarch64 ./tensorflow-pkg\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nSeems to be a result of the commit \r\nhttps://github.com/tensorflow/tensorflow/commit/61754492129f651f23f05b5aed889f376ec35bb8#diff-cebea885ef40f295a311dfe2879c002bba33695e437f859f7ce111645a99c0ec", "comments": ["keras/api is still referenced in the source tree\r\n$ grep -rn \"_v2.keras\" tensorflow/*\r\ntensorflow/api_template.__init__.py:90:_keras_module = \"keras.api._v2.keras\"\r\ntensorflow/api_template.__init__.py:162:    _keras_package = \"keras.api._v2.keras.\"\r\ntensorflow/compat_template.__init__.py:51:_keras_module = \"keras.api._v2.keras\"\r\ntensorflow/compat_template.__init__.py:81:    _keras_package = \"keras.api._v2.keras.\"\r\ntensorflow/python/keras/estimator/__init__.py:175:def model_to_estimator_v2(keras_model=None,\r\ntensorflow/tools/pip_package/build_pip_package.sh:226:    cp -r ${TMPDIR}/tensorflow/python/keras/api/_v2/keras/ ${TMPDIR}/tensorflow/keras/\r\n", "I think @qlzh727 sent a fix to this.", "Should be addressed in https://github.com/tensorflow/tensorflow/commit/f1e26952169956ffff630e4033488bc6ef91fd52", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52361\">No</a>\n"]}, {"number": 52360, "title": "Custom trainable function in tf.scan within keras model", "body": "I need to employ [tf.scan](https://www.tensorflow.org/api_docs/python/tf/scan) with a custom function (**fn**) in my Keras-based constructed model. The function contains a set of layers with some trainable weights; however, tf outputs some warnings which can not be ignored simply and need corrections. Please find below a very simplified code sample to reproduce the warnings.\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: yes, a sample code is provided in the next.\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: 2.6\r\n-   **Python version**: Python 3.8.10\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: 11.1\r\n-   **GPU model and memory**: 2080 ti\r\n-   **Exact command to reproduce**: a sample code is provided in the next.\r\n\r\n------------------------\r\n\r\n### Sample code:\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nclass fn(tf.keras.Model):\r\n# class fn(tf.keras.layers.Layer):\r\n  def __init__(self, units:int=512):\r\n    super(fn, self).__init__()\r\n    self.units = units\r\n    self.dense1 = tf.keras.layers.Dense(self.units, name=\"dense1\")\r\n    self.dense2 = tf.keras.layers.Dense(self.units, name=\"dense2\")\r\n  def call(self, a, x):\r\n    out = self.dense1(a) + self.dense2(x)\r\n    return out\r\n  \r\n\r\n\r\nclass solution:\r\n  \r\n  def __init__(self, a, b, units):\r\n    self.a = a\r\n    self.b = b\r\n    self.units = units\r\n    self.dense0 = tf.keras.layers.Dense(self.units, name=\"dense1\")\r\n    self.fn = fn(units=self.units)  \r\n    self.make()\r\n  \r\n  def make(self):\r\n    inputs = tf.keras.layers.Input(shape=[self.a, self.b],name=\"inputs\")\r\n    outputs = self.dense0(inputs)\r\n    outputs = tf.scan(self.fn, outputs)\r\n    # outputs = tf.scan(lambda a, x: self.fn(a,x), outputs)\r\n    self.model = tf.keras.Model(inputs=inputs, outputs=outputs) \r\n\r\nmodel = solution(a=301, b=256, units=256).model\r\n\r\n```\r\n\r\n------------------------\r\n\r\n\r\n### Model summary:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninputs (InputLayer)          [(None, 301, 256)]        0         \r\n_________________________________________________________________\r\ndense1 (Dense)               (None, 301, 256)          65792     \r\n_________________________________________________________________\r\ntf.scan_1 (TFOpLambda)       (None, 301, 256)          0         \r\n=================================================================\r\nTotal params: 65,792\r\nTrainable params: 65,792\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n------------------------\r\n\r\n\r\n### Warnings:\r\n\r\n```\r\nWARNING:tensorflow:\r\nThe following Variables were used a Lambda layer's call (tf.scan_1), but\r\nare not present in its tracked objects:\r\n  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense1/kernel:0' shape=(256, 256) dtype=float32>\r\n  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense1/bias:0' shape=(256,) dtype=float32>\r\n  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense2/kernel:0' shape=(256, 256) dtype=float32>\r\n  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense2/bias:0' shape=(256,) dtype=float32>\r\nIt is possible that this is intended behavior, but it is more likely\r\nan omission. This is a strong indication that this layer should be\r\nformulated as a subclassed Layer rather than a Lambda layer.\r\n\r\n```\r\n\r\n\r\n------------------------\r\n\r\n### Other links:\r\n\r\nI also followed some other links, but not helpful: [link1](https://stackoverflow.com/questions/66873192/subclassing-a-keras-layer-the-following-variables-were-used-a-lambda-layers-ca) [link2](https://programming.vip/docs/day-6-tensorflow2-model-subclassing-api.html) [link3](https://stackoverflow.com/questions/67724151/keras-layers-multiheadattention-gives-warning-the-following-variables-were-used)\r\n\r\n------------------------\r\n\r\n\r\n\r\n### Stackoverflow link: \r\n(unfortunately, no feedback yet)\r\n\r\n[Link](https://stackoverflow.com/questions/69546769/custome-trainable-function-in-tf-scan-variables-used-a-lambda-layers-call-are)\r\n\r\n------------------------\r\n\r\n**Deeply appreciate any help.**\r\n", "comments": ["Hi @sanatmpa1 ! Could you look at this issue , it is replicating [2.5](https://colab.research.google.com/gist/mohantym/ef977648dc3a5ded6b554106c06d14b5/github_52360.ipynb#scrollTo=ar2v-yQ2336D) ,[2.6 ](https://colab.research.google.com/gist/mohantym/6836071ceedae8a5243880d70d2314d6/github_52360.ipynb#scrollTo=ar2v-yQ2336D)and [2.7](https://colab.research.google.com/gist/mohantym/04de6800c6da092a237083d50bf8588f/github_52360.ipynb#scrollTo=IE0pODYYm8eh)", "The warning indicates that the Variables mentioned in the warning are not part of calculating gradient, you can ignore the warning if you want it that way or you need to define a layer based on the base class instead of Lambda.", "@sachinprasadhs thanks for the reply; unfortunately yes, the main problem was the gradient calculation for trainable weights that could not be ignored. Anyway, I couldn't find a good solution on the basis of above mentioned code, however, it was already solved using the following structure that might be helpful for others as well; the below structure can be employed for any complex trainable **fn** structure:\r\n\r\n```\r\nclass Custom_scan_layer(tf.keras.layers.Layer):\r\n\r\n  def __init__(self, params):\r\n    super(Custom_scan_layer, self).__init__()\r\n    self.params = params\r\n    #======================================================================\r\n    # define all trainable variables or layers which will be \r\n    # called/employed within the fn .\r\n    #======================================================================\r\n\r\n  def fn(self, a, x):\r\n    #======================================================================\r\n    # create your desired function using pre-defined variables or layers\r\n    #======================================================================\r\n    out = a + x \r\n    return out\r\n  \r\n  def call(self, inputs):\r\n    out = tf.scan(self.fn, inputs)\r\n    return out\r\n```\r\n\r\n\r\n\r\n\r\n  ", "Good to hear that you have solved the issue, could you move this issue to closed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52360\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52360\">No</a>\n"]}, {"number": 52359, "title": "Add tfl-to-tosa lowering for l2 normalization and batch matmul", "body": "L2 Normalization and batch matmul were missing their lowerings. This only includes the\r\nfloat lowering for L2 normalization. Quantized L2 normalization requires more thought for\r\nhow to handle zero values, and correct decomposition.", "comments": []}, {"number": 52358, "title": "[oneDNN] Enable fusion for Add/AddV2 which has BiasAdd semantics.", "body": "because the MR: https://github.com/tensorflow/tensorflow/pull/52226 has Google CLA problem, i create a new branch.\r\n\r\nIf an Add/AddV2 op has one input of Conv2D/DepthwiseConv2D/MatMul in NHWC format,\r\nand the other input is 1-dimension, treat this add node as BiasAdd.\r\nCo-authored-by: Yimei Sun <yimei.sun@intel.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52358) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 52356, "title": "Tensorflow on Apple M1", "body": "Hi,\r\n\r\nI would need to get a **tensorflow-gpu<2,>=1.15.2** version for a python library I would need to use on Apple M1. I successfully installed tensorflow on my mac, but the version I have is now 2.6.0. Is there a way to get one <2, or not at all (as it is my understanding at now)?\r\n\r\nThank you,\r\nSilvia", "comments": ["@spagliarini ,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.6 and check if you are facing the same issue. Thanks!", "I have version v2.6, the issue is still there (the library I'd like to use needs an earlier version). ", "@spagliarini ,\r\nPlease refer to similar issues #44751,#47782 and for installation please take a look at this [link](https://github.com/apple/tensorflow_macos). It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52355, "title": "Use vector version of the logo in README file", "body": "<img src=\"https://user-images.githubusercontent.com/29678011/137022160-44006611-08af-4c5f-9ddb-f1d387b999ee.png\" width=\"256\" />\r\n\r\nPlease replace the raster logo in the repository [README.md](https://github.com/tensorflow/tensorflow/blob/master/README.md) file with its vector (SVG) alternative available in the [brand assets](https://www.tensorflow.org/extras/tensorflow-brand-assets.zip).\r\n\r\nAlso, I think the horizontal lockup is marked as the preferred one in the [brand guidelines](https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf).\r\n\r\n\r\n", "comments": ["Hi @Saduf2019 ! Could you please look at this bug ?", "@mahozad \r\nPr has been created once its merged this issue will move to closed status.", "@Saduf2019 The new logo is still a PNG. Could you please use the SVG format instead?", "@mahozad \r\nIt is as per the guidelines hence its been approved. "]}, {"number": 52354, "title": "[oneDNN] fixing nn_fused_batchnorm_test for mkl configuration", "body": "This PR fixes the unit test : **//tensorflow/python:nn_fused_batchnorm_test** failure on mkl configuration.\r\n\r\nWith recent commit on public tensorflow : aab9998\r\nThere are new checks added to **fused_batch_norm_ops** and additional unit tests added to validate new op checks.\r\nNewly added unit test makes sure the program raises exception for when op checks are not satisfied.\r\n\r\nThese checks were not present in **mkl_fused_batch_norm_op.cc** and so it failed to raise exceptions with new unit test, hence unit test failed.\r\nWith this PR the op checks are added to **mkl_fused_batch_norm_op.cc**, making it raise exceptions and hence pass the test.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52354) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 52353, "title": "Call hexagon delegate only if TFLITE_ENABLE_HEXAGON is defined.Change-Id: I4540ff617562c25144d2ab99f84835ab6b6553f5", "body": null, "comments": ["@NAgarwalla  Can you please check @multiverse-tf's comments and keep us posted ? Thanks!", "I'll resubmit as two separate commits."]}, {"number": 52352, "title": "Added external delegate support to CMAKE build of benchmark tool.  Fixed CMAKE build of benchmark tool for aarch64", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52352) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52352) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52352) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 52350, "title": "Merge branch 'master' of https://github.com/tensorflow/tensorflow\nChange-Id: I5d3282113250bef588be3ae49ea3abc75aa0b5cc", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52350) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 52349, "title": "[oneDNN] enable bf16 for Erf", "body": null, "comments": ["Hi @penpornk : this is another small bf16 change, maybe you can review this one too?", "@sherhut : can you please review this small change? Thanks!"]}, {"number": 52348, "title": "Documentation problem for gradient clipping parameter", "body": "This issue reflect documentation of this: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer\r\n\r\nAs stated in the above documentation (relevant section c and pasted below), there is no difference between clipnorm and global_clipnorm.  However, looking at the code shows these parameters do 2 different things.  Recommend to update documentation.\r\n\r\n\r\nclipnorm | float\u00a0or\u00a0None. If set, clips gradients to a maximum norm.\r\nclipvalue | float\u00a0or\u00a0None. If set, clips gradients to a maximum value.\r\nglobal_clipnorm | float\u00a0or\u00a0None. If set, clips gradients to a maximum norm.\r\niterations\r\n", "comments": ["@isaacgerg \r\nif clipnorm(float) is set, the gradient of each weight is individually clipped so that its norm is no higher than this value. If global_clipnorm (float) is set the gradient of all weights is clipped so that their global norm is no higher than this set value.\r\nThis has been clearly mentioned in **kwargs.", "Sure.  I was just saying you should update the online documentation. \r\n\r\nI do think the **kwargs is slightly unclear:  \"If clipvalue (float) is set, the gradient of each weight is clipped to be no higher than this value. If clipnorm (float) is set, the gradient of each weight is individually clipped so that its norm is no higher than this value. If global_clipnorm (float) is set the gradient of all weights is clipped so that their global norm is no higher than this value.\"\r\n\r\nFrom reading this, I would expect this behavior\r\nIt might be a good idea to provide a simple example also to be clear. For example, suppose the gradient vector is w = [-2];  Is the following true?: clipnorm(1)=>-1, clipvalue(1)=>-2, and global_clipnorm(1) = -1.  \r\n\r\nEach weight is really just a scalar unless you are considering groups of weights, a \"weight,\" like a convolution kernel.  The use of norm here makes this unclear.  I think you need to be absolutely clear here so users know exactly what they are getting.", "@isaacgerg \r\nPlease feel free to create a pr for the same.", "You didn't answer my question in my last response so I cannot move forward.", "@isaacgerg \r\nCan you please create the PR for the statement mentioned above.Thanks", "@tilakrayal Unfortunately, my employer does not allow me to submit PRs.", "@isaacgerg,\r\n\r\n```\r\nclipnorm | float or None. If set, the gradient of each weight is individually clipped so that its norm is no higher than this value.\r\nclipvalue | float or None. If set, the gradient of each weight is clipped to be no higher than this value.\r\nglobal_clipnorm | float or None. If set, the gradient of all weights is clipped so that their global norm is no higher than this value\r\n```\r\n\r\nDo you think modifying the description as mentioned above would make it clear? I can submit a PR if that looks good. Thanks!\r\n\r\n", "@sanatmpa1 It is unclear to me if the description you list is correct.\r\n\r\nA gradient is really a group of scalar values.  Its unclear how clipnorm differs from global_clipnorm.  My guess is that clipnorm groups weights by their function but its not clear to me.  Having an example to make this completely clear would be very helpful.  \r\n\r\nAlso, does clipvalue clip truly  by value only or by magnitude and preserve the sign?", "Let's consider the below optimizer,\r\n```\r\noptimizer = keras.optimizers.SGD(clipvalue=1.0)\r\nmodel.compile(loss=\"mse\", optimizer=optimizer)\r\n```\r\n\r\nThis optimizer will clip every component of the gradient vector to a value between \u20131.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between \u20131.0 and 1.0. Note that it may change the orientation of the gradient vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9,1.0], which points roughly in the diagonal between the two axes. \r\nIn practice, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting `clipnorm` instead of `clipvalue`. This will clip the whole gradient if its \u2113 norm is greater than the threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost\r\neliminating the first component. ", "@sachinprasadhs You should add your commentary to the documentation.\r\n\r\n What does global_clipnorm do?", "As mentioned below also as per the document about `global_clipnorm`.\r\n> @isaacgerg if clipnorm(float) is set, the gradient of each weight is individually clipped so that its norm is no higher than this value. If global_clipnorm (float) is set the gradient of all weights is clipped so that their global norm is no higher than this set value. This has been clearly mentioned in **kwargs.\r\n\r\n", "I don't see how clip_norm is different than global_clipnorm.  \r\n\r\nClip_ value examines each scalar and hard clips based on threshold but preserves sign.  Clip_norm rescales all the scalars so that the norm is the specified value.  But, what does global_clipnorm do?\r\n\r\nAnother way to ask the question is, what do you mean by \"global norm?\"  Do you consider the collection of weights for a function its own entity?  Mathematically speaking, the gradient of a NN is a list of scalars. Perhaps giving me an example of how clip_norm and global_clipnorm perform different on a test vector would be helpful.", "Does the description provided [here](https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm) helps you.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I'm still here but its approaching the holidays in the USA.", "@sachinprasadhs Sort of.  Is the gradient of a model broken up into sections and put into a list?  I presume it must otherwise, clip_by_global_norm will never take in a list.", "Note that gradient of NN is a list of Tensors, not scalars and in the code it internally calls `tf.clip_by_global_norm` check [here](https://github.com/keras-team/keras/blob/v2.7.0/keras/optimizer_v2/optimizer_v2.py#L428-L439) and [here](https://github.com/keras-team/keras/blob/2c48a3b38b6b6139be2da501982fd2f61d7d48fe/keras/optimizer_v2/utils.py#L106).\r\nIn future with the new release of keras optimizer it will be more easier for users and will be much clear to understand.\r\nI will create a PR for the changes pointing to the `tf.clip_by_global_norm` for the users to understand better. Thanks!", "@ sachinprasadhs  This sounds great!  Thank you for your efforts!"]}, {"number": 52347, "title": "[oneDNN] Enable bf16 for SparseSoftmaxCrossEntropyWithLogits", "body": null, "comments": []}, {"number": 52346, "title": "Restore from checkpoint loads optimizer incorrectly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: 1080TI 11Gb\r\n\r\n**Describe the current behavior**\r\n\r\n1. After checkpoint restoration optimizer weights are different from optimizer weights before saving checkpoint.\r\n2. As `assert_consumed` notifies checkpoint file has unresolved optimizer slots (variables). \r\n```\r\nUnresolved object in checkpoint (root).optimizer.iter: attributes {\r\n  name: \"VARIABLE_VALUE\"\r\n  full_name: \"Adam/iter\"\r\n  checkpoint_key: \"optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE\"\r\n}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n1. The optimizer weights should be the same before save and after load.\r\n2. The `assert_consumed` should not warn about anything.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): - \r\n\r\n**Standalone code to reproduce the issue**\r\nThe reproducible code example is presented in [colab](https://colab.research.google.com/drive/1R01obneq7_jSfBRdUabTAMQIYxuzShqe?usp=sharing).\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@RomanSteinberg ,\r\nI do not have access to the link you have provided. Could you please provide the required permissions to view the files.Thanks!", "Sorry! @tilakrayal Here [it](https://colab.research.google.com/drive/1R01obneq7_jSfBRdUabTAMQIYxuzShqe?usp=sharing) is and I edited colab link in previous post.", "@RomanSteinberg ,\r\nCan you please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/33150) with the similar error.It helps.Thanks!", "@tilakrayal I read that issue thread and found two workarounds but not the solution. Workarounds are\r\n**1. Use `predict` method to warm up the model before loading weights.** I don't think that it is ok to predict some garbage before loading weights. I'm sure Tensorflow is not designed to work this way.  \r\n**2. Use `tf.Variable` to specify Adam parameters when construct optimizer.** There is no information that Adam optimizer should be constructed like. Moreover default construct behavior is to specify `float`, but not `tf.Variable`. I'm sure Tensorflow is not designed to work this way.\r\n\r\nSo, please, tell me if I missed something in that issue. Please, tell me if my code contradicts with some TF design, TF pattern or some usage guide. If not, please, admit that it is a bug. I'm ready to contribute to TF to fix it. But I need instructions on what way it should be fixed not to break any idea in one of TF modules. ", "@tilakrayal I would like to notice that there was a `build` method in previous versions of TF. It is still in the source but it absent in the docs and guides of TF 2.6.0. Is it going to be deprecated and I shouldn't even try to use it? ", "@sanatmpa1 ,\r\nI was able to reproduce the issue in tf [v2.6](https://colab.research.google.com/gist/tilakrayal/dde9d622b807c51fb5aaa77c76d36c00/2-6checkpoint_save_load_test-1.ipynb), and in [v2.5](https://colab.research.google.com/gist/tilakrayal/1e04a6162b7df86530933b8fb91c8801/2-5checkpoint_save_load_test-1.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/aee3994394cb13aaf5322ac93d0b7515/nightly-checkpoint_save_load_test-1.ipynb) it is failing with different error.Please find the gist here.", "Looping in @k-w-w, who may know more about the checkpointing issue.", "I'm having this issue too\r\nMy workflow is:\r\n\r\n0. (import libs, declare functions, hyperparameters, datasets, checkpoint paths, etc)\r\n1. build model (using `tf.keras.models.load_model` on a .h5 model on disk)\r\n2. load checkpoints\r\n3. train\r\n4. build model unfreezing layers (using the same command and model as step 1)\r\n5. load best checkpoints of previous training stage\r\n6. train\r\n\r\nI get the following errors right after starting step 6:\r\n\r\n```\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used.\r\n```\r\nI too have read the linked thread, but I'd like a solution other than those workarounds which IMO just bloat the jupyter notebook I use", "This is delayed restoration (https://www.tensorflow.org/guide/checkpoint#delayed_restorations) in action. If an additional `train_step()` is called after `checkpoint.restore()`, `assert_consumed()` will report no errors. You can also observe, for example, that `optimizer.iterations` is 11 after the additional `train_step()` -- the value of 10 was restored and then incremented by 1 for the additional `train_step()`.\r\n\r\nHope that helps explain what is going on! While delayed restoration is a necessary feature for eager execution, the group is aware that it can be surprising and unintuitive (even for us...), and there are discussions on what can be done about it.", "@pcish thank you! But don't you think that `assert_consumed` is designed to verify the correct loading? And if it fails, then developer should understand that something is wrong. But you say: no, it is ok. I think it is much more then surprising...", "You ask a great question about what `assert_consumed()` is _designed_ to do. Searching through our archives, this is what the original author wrote:\r\n\r\n> This allows fully dynamic dependencies, but leads to some possibly surprising corner cases: deferred restoration on object A, creation and modification of variables in object B, then a new dependency from A to B will overwrite the modifications to variables in B with values restored from the checkpoint.\r\nTo help users avoid cases like this and for easier debugging, restore will return a status object which can assert loading is complete and no further assignments will take place (assert_consumed())\r\n\r\nElsewhere, the original author provides this example usage code:\r\n```\r\nstatus = root.restore(tf.train.latest_checkpoint(checkpoint_directory))\r\nfor _ in range(num_training_steps):\r\n  ...\r\nstatus.assert_consumed()\r\nroot.save(checkpoint_prefix)\r\n```\r\n\r\nDigesting this information, I feel the original author's intention isn't fully coming through. What do you think?", "@pcish I would like to highlight you this [guide](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint). It has code and suggestions you mentioned, but also suggests to use it after restore. Consider [this part](https://i.imgur.com/L5RlcKF.png).\r\n\r\n> This is delayed restoration (https://www.tensorflow.org/guide/checkpoint#delayed_restorations) in action. \r\n1. Could you suggest a good way to force restoration and avoid problems I described? \r\n2. Don't you think that `assert_consumed` should force restoration because the developer wishes to verify the restored weights?\r\n", "> @pcish I would like to highlight you this [guide](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint). It has code and suggestions you mentioned, but also suggests to use it after restore. Consider [this part](https://i.imgur.com/L5RlcKF.png).\r\n\r\nThanks for the links, I'll work on updating the documentation to try to be more clear, and probably not recommend assert_consumed().\r\n\r\n> \r\n> > This is delayed restoration (https://www.tensorflow.org/guide/checkpoint#delayed_restorations) in action.\r\n> \r\n> 1. Could you suggest a good way to force restoration and avoid problems I described?\r\n\r\nYou could try running a \"fake\" training step (e.g. set loss to 0) to force a complete restoration.\r\n\r\n> 2. Don't you think that `assert_consumed` should force restoration because the developer wishes to verify the restored weights?\r\n\r\nI have to disagree. I'd be rather upset if I wrote:\r\n```\r\nx = has_something_happened()  // false\r\nassert_true(x)\r\n```\r\nand `assert_true(x)` went ahead and `does_something()` because clearly that's what I wished for.\r\n\r\nHowever, I think what you might be getting at is that a `try_consume_everything_now()` function would be helpful?\r\n\r\nI'd also like to take a step back and ask if you can share what your high-level goals are that led you to use `assert_consumed()`? Maybe there are other ways to achieve the goal that does not involve a forced restoration or `assert_consumed()`?\r\n", "@pcish thank you! These are quite clear answers. I agree both points (1-2).\r\n\r\nMy high-level goals are typical. Train model (experiments), pick out trained model and deploy it into production service. Unfortunately, I use TF irregularly and cann't follow all trends it suggests from time to time. I mean TF changes usage best practices from time to time. A year or two ago I tried to use TFX, but it was almost impossible to use at that moment. The problem which led me to this issue was to take Checkpoint and make tow things: code for inference in production service and use it later in training. \r\n\r\na) I tried to load Checkpoint and convert it into SavedModel for inference. It is quite tricky, so I stopped. Decided to use Checkpoint in production service. So, I need to load it and start inference, but it led to use ridiculous code like `model.predict(np.zeros(shape))` after load. I tried to use `model.build`, but as I can see it was removed from guides and is not recommended so. \r\n\r\nb) On other side I'm trying to completely load checkpoint before continue training, and there are some problems with loading optimizer weights (as I showed in this issue). I've noticed incompleteness and started to use `assert_consumed` and ...\r\n\r\nPS: A bit of my thoughts. The TF usage became not user friendly. It was easy in v1.0-1.14 and idea to take keras in is the last good step towards simplification of usage.", "Thanks for sharing your use case and the issues you encountered @RomanSteinberg!\r\n\r\nRegarding the original question, I found there's already an open internal feature request for adding an option to disable delayed restoration while loading a checkpoint, which is roughly equivalent to adding a `try_consume_everything_now()` function, so I've added a reference to this conversation on that feature request.\r\n\r\nA follow up on your use case: it sounds like you have the source code for a model, and also a Checkpoint for it. Also from the example you gave at the start, I assume you are using Keras. Loading a Checkpoint with the model's source code, then calling `tf.keras.models.save_model()` with the model should export the source code and the weights from the Checkpoint as a SavedModel (that can be used for inference). Was this what you tried but found something tricky?\r\n```\r\nnet = load(ckpt_dir, False)  # change load to return net instead of weights\r\ntf.keras.models.save_model(net, '/tmp/saved_model')\r\n\r\ninference_model = tf.keras.models.load_model('/tmp/saved_model')\r\n```\r\n\r\n", "@pcish Yes, I tried to. But, as far as I know, SavedModel format is not saving the whole training state, for example optimizer state. So, it is not suitable in my case.\r\n", "I've logged the feature request and updated related documentation, so going to close this issue. Please reopen if you think there is something still missing from the documentation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52346\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52346\">No</a>\n"]}, {"number": 52345, "title": "ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory. I am using servers", "body": "When I run a tensorflow model then I am getting the error - ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory. \r\n\r\nThe missing files are present at the specified location. I have install cuda 11.4 and cuda 9.0. \r\n\r\nExporting the files from terminal or adding them on .bashrc file does not work.\r\n\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n\r\nSystem Configurations :\r\nTotal RAM - 125.65 GB\r\nUsed Ram - 2.87 GB", "comments": ["Hi @purushottam22 ! Please look at the answers of similar issues [link1](https://stackoverflow.com/questions/51019509/tensorflow-importerror-libcudnn-so-7-cannot-open-shared-object-file-no-such),[link2](https://github.com/tensorflow/tensorflow/issues/20271) . Could you please confirm the Tensorflow version too?", "> Hi @purushottam22 ! Please look at the answers of similar issues [link1](https://stackoverflow.com/questions/51019509/tensorflow-importerror-libcudnn-so-7-cannot-open-shared-object-file-no-such),[link2](https://github.com/tensorflow/tensorflow/issues/20271) . Could you please confirm the Tensorflow version too?\r\n\r\nTensorflow version is 2.6.0\r\n\r\nI tried both links but issue remains same.", "Ok @purushottam22! Could you please share a stand alone code to reproduce this too?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52345\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52345\">No</a>\n"]}, {"number": 52342, "title": "Add Support for TensorRT 8.2", "body": "1. Add stub files for TensorRT 8.2\r\n2. Fix accuracy regression with MobilenetV2 i.e. set dynamic range for (-6, 6)\r\n3. Limit GPU memory utilization for TFTRT Integration tests\r\n4. Fix incorrect usage of TRT version to string API\r\n5. TensorRT model_tests update\r\n- Add separate perf tolerance for FP32, FP16, and INT8\r\n- Add separate accuracy tolerance for FP32, FP16, and INT8\r\n- Add functionality to dump histograms for better accuracy validation \r\n\r\nCC: @DEKHTIARJonathan, @tfeher , and @bixia1 ", "comments": ["Converted the PR to Draft as it is still a WIP.", "@jhalakp-nvidia Any update on this PR? Please. Thanks!", "I am taking over the TensorRT 8.2-relevant work for @jhalakp-nvidia . Please see the new PR here https://github.com/tensorflow/tensorflow/pull/52932. @jhalakp-nvidia includes  some other features here that either are already covered by separate PRs or can go into a separate PR.", "@jhalakp-nvidia Any update on this PR? Please. Thanks!"]}, {"number": 52341, "title": "Update TF Security Readme", "body": null, "comments": []}, {"number": 52340, "title": "Update ISSUES.md - minor format change", "body": "Add Numbering to the issues list", "comments": []}, {"number": 52339, "title": "2.7-rc1 cherry-pick request: Add shape checks to FusedBatchNorm kernels.", "body": "Without this change, passing invalid shapes to various FusedBatchNorm ops can cause invalid memory reads or writes.", "comments": []}, {"number": 52338, "title": "[INTEL oneDNN] Fix Pooling3D unit test failure", "body": "This PR fixes unit test failure of a newly added test case within\r\ntensorflow/python/kernel_tests/pooling_ops_3d_test.py\r\n\r\nThis fix is similar to that of Eigen implementation (core/kernels/pooling_ops_common.h),\r\nin the same way of handling ZERO pooling size.", "comments": []}, {"number": 52337, "title": "Use Two-level autotuner for Cudnn Frontend APIs", "body": "This PR reduces the long startup time by applying a two-level autotuning when using the cudnn frontend APIs. Cudnn frontend supports two engine lists: heuristics and fallback. Heuristics engines are normally faster. In the two-level autotuner, we evaluate the fallback engines only when none of the heuristics engines work. \r\n\r\nWith the two-level autotuner, we no longer filter out those slow eng0, since they are usually in the fallback list and won't be swept over if any heuristics engines work. We keep them to maintain a better functional coverage.\r\n\r\nAlso, we upgrade the heuristics mode from the decision tree to a neural network based solution (i.e., CUDNN_HEUR_MODE_INSTANT to CUDNN_HEUR_MODE_B): https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-820\r\n\r\ncc. @nluehr ", "comments": ["@kaixih  Can you please resolve conflicts? Thanks!", "@kaixih could you please resolve conflicts.", "> @kaixih could you please resolve conflicts.\r\n\r\nI'm in the middle of a sequence of 2 rollbacks and later roll-forwards, of changes that this PR was just rebased onto.  It may be better to wait for those to settle, at which point the conflicts should vanish or at least be much easier (and not involve undoing/redoing the same merges).", "@kaixih can you please resolve conflicts ?", "@kaixih can you please resolve conflicts ?", "Just pushed a new change to add the two-level autotuner to the fused ConvBiasRelu op.", "@reedwm , shall we make this as draft for now? The cudnn v8.3 changes the lengths of heuristics and fallback list and now heur list is much longer than fallback, making this two-level autotuning mechanism less beneficial. So, we are working on some tweaks to cap the first round autotuning.", "I'd rather submit this PR now, then later update the code again for cudnn 8.3. The reason is that this change is needed for determinism. Alternatively, are there any other ways to fix determinism without cudnn 8.3? We could enable `eng0` when determinism is enabled, but I fear that would add a long startup time when determinism is enabled.\r\n\r\nUnfortunately, 1362e1936dd292bd850f56996ab4f60daca58e4c is causing this PR to not build. @awpr, do you plan on submitting any more changes in the near future which conflict with this change?", "Sure. That sounds reasonable. Let's continue to upstream the PR.\r\n\r\nI can work on a patch to better support cudnn v8.3 later on.", "Sounds good. Since the build failures are easy to fix, I'll fix them when merging this PR, so no need to update this PR again.", "Welcome back, @kaixih :)"]}, {"number": 52336, "title": "Unspecified data type in image_dataset_from_directory", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: https://keras.io/api/preprocessing/image/#imagedatasetfromdirectory-function\r\n\r\nThere are issues with the follwoing entries: \r\n\r\n**labels:** Either \"inferred\" (labels are generated from the directory structure), None (no labels), or a list/tuple of integer labels of the same size as the number of image files found in the directory. Labels should be sorted according to the alphanumeric order of the image file paths (obtained via os.walk(directory) in Python).\r\n**label_mode:** - 'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). - 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). - 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). - None (no labels).\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe data types for input are not clearly described and there are no examples of what would be an acceptable input if its is not feasible to supply a data directory organized into files by class labels.  Currently when i try this approach the fucntion does not even recognize that there are files in the directory I am giving it, but when I move up in the directory and supply the file containing the images instead I get a Dataset object for training a single class. \r\n\r\nThe errors are not useful and confusing.  It is not clear why the function does not recognize that there are image files in the folder in one case and not another.  Examples would make implementing this usecase much easier.\r\n\r\n", "comments": ["Hi @AngCamp! \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) \r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52335, "title": "tensoflow.keras.preprocessing.image_dataset_from_directory doesn't recognize there are files in a directory when labels are supplied as a list/tupple", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nhttps://keras.io/api/preprocessing/image/#imagedatasetfromdirectory-function\r\n\r\nMy code:\r\n\r\n```\r\nimg_width = 224\r\nimg_hieght = 224\r\nbatch_size = 100\r\ntrain_labels = train_metadat_df.sort_values(by='filename')['sirna'].tolist()\r\n\r\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n  \"../input/recursion-cellular-image-classification-224-jpg/train/train\",\r\n  validation_split=0.2,\r\n  subset=\"training\",\r\n  labels= train_labels,\r\n  label_mode= \"categorical\",\r\n  seed=123,\r\n  image_size=(img_height, img_width),\r\n  batch_size=batch_size)\r\n```\r\n\r\nThis function normally takes a pointer for a directory with images organized by lab into directories ( i.e. all flowers in a file called flowers, all animals in a file called animals) and then outputs a dataset object.  But there is also an option to provide a list of \r\nWhen providing a list of file labels instead but when this happens the following error is produced:\r\n\r\n`ValueError: Expected the lengths of `labels` to match the number of files in the target directory. len(labels) is 73030 while we found 0 files in ../input/recursion-cellular-image-classification-224-jpg/train/train.`\r\n\r\nThere are ~73000 .jpeg files in that directory and when I run it on ../input/recursion-cellular-image-classification-224-jpg/train it finds ~54000 of them and trains them all on one class (because they are all stored in one file).\r\n\r\n**System information**\r\n- using Tensorflow in a kaggle notebook\r\n- currently no GPU is turned on\r\n- tensorflow version 2.4.1\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@AngCamp \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52335\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52335\">No</a>\n"]}, {"number": 52334, "title": "Fixes for various tests on Ampere GPUs", "body": "Fixes various tests mostly related to TF32 execution on Ampere.\r\n\r\nCC: @reedwm ", "comments": []}, {"number": 52333, "title": "oneDNN : update python version to 3.8 in docker", "body": null, "comments": ["Hi @angerson,  can you take a look at this PR. We need this PR merged to get the Intel OneDNN presubmit jobs working again.", "@agramesh1 In the interest of getting things un-broken, I'll approve this. Can you help me understand why the Intel OneDNN jobs depend on an un-documented Dockerfile in the TensorFlow repo? If we can, I'd rather remove that dependency on us for your CI just to remove friction all around.", "> @agramesh1 In the interest of getting things un-broken, I'll approve this. Can you help me understand why the Intel OneDNN jobs depend on an un-documented Dockerfile in the TensorFlow repo? If we can, I'd rather remove that dependency on us for your CI just to remove friction all around.\r\n\r\nYes, we don't need this change and we can work around and use our own docker files in CI, we will close this. Thanks again.", "Closing as this is not required any more."]}]