[{"number": 33776, "title": "LSTMCell name is ignored in trainable_variables when wrapped in keras.layers.RNN", "body": "I created a model with several LSTMCell cells, wrapped in keras.layers.RNN.\r\nWhen I print trainable_variables, cell names are ignored, which results into duplicate variable names (several same recurrent_kernel/etc), which confuses Tensorboard for example.\r\n\r\nCollab notebook to reproduce: https://colab.research.google.com/drive/11W6ntLFGj4gqh5sS09CeVgb0LMNb_Fiu\r\nEnvironment: tensorflow 2 release", "comments": ["Issue replicating for the given code - TF-2.0.", "I would like to work on this one. @qlzh727 ", "Thanks for reporting this issue. I think this is a historical issue since the we directly invoke cell.call() method rather than cell.__call__(). This cause the name scope in the __call__ to be ignored, which result into the variable name doesn't have the prefix. \r\n\r\nThe fix might need some special care since it affects the existing saved model (since the variable name changed, as well as the ops under the cell, they will now have a prefix of the cell's name). For anyone who want to try a fix, I would suggest to make sure the save model and checkpoint is explicitly verified.", "@qlzh727 thank you! Maybe you have an idea of a simple workaround (not sure if calling __call__ explicitly for each cell would help) until the proper fix is done?", "@qlzh727 How do you recommend to manually set the LSTM weights before the first call of `.call()` ?\r\n The `Layer.set_weights` method will say that the layer was expecting 0 weights, but was provided with a list of N weights.", "The weights of the layer is initialized in build(), so you have to build the layer first with proper input_shape, so that it can init the weights with proper shape. then set_weights() should work fine.", "I tried to build each cell, but the names are still duplicated", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33776\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33776\">No</a>\n"]}, {"number": 33775, "title": "For tf2.0, why both tf.keras and tf.nn exist in the new version of Tensorflow? ", "body": "I found that tf.keras and tf.nn could do the same thing, so why keep both tf.keras and tf.nn existed in the new version of tf2.0?", "comments": ["A subset of ```tf.keras``` functions are available in ```tf.nn``` module. However the parameters of these functions are different.\r\nFor instance we may compare the parameters of [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) and  [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) functions.\r\nIt offers more flexibility to the users for same implementation.", "As `tf.keras` api could provide more flexibility in contrast to `tf.nn` api, so I think there is no need to keep `tf.nn` api in the new version of tf2.0 any more. If both are kept, it could cause more confusion to tensorflow users.", "tf.nn.conv2d and tf.keras.layers.Conv2D for example are not the same. The former does not yet include bias or activation. Personally I find tf.nn more flexible in particular for custom models whereas tf.keras is easier to use.", "@Zhengtq Yes. As mentioned in the above comment, tf.nn is used for flexibility for custom models and tf.keras is based on usability.\r\n\r\nFor more information you can follow the folllowing [thread](https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d). Thanks!"]}, {"number": 33774, "title": "tflite_convert always required '--saved_model_dir --keras_model_file' arguments, even I specified '--graph_def_file'", "body": "## Computer spec:\r\n-  Windows 10 update 1903 (build 18362.449)\r\n-  Python 3.7 (run with venv)\r\n-  Powershell Core console under VS Code\r\n- Tensorflow 2.0.0 (install with `pip`)\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/lite/convert/cmdline_examples\r\n\r\n## Description of issue (what needs changing):\r\n\r\nExample defined what I can't follow, and the result always error with no specific arguments `--saved_model_dir` or `--keras_model_file`. \r\nMoreover, if I specified `--keras_model_file` with file name, it won't count as a file, it turn out the code logic count as it a folder path. \r\n\r\n### Clear description\r\n\r\nIn my case: Basic example -> [Convert a Tensorflow Graphdef](https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_graphdef_). I did download file as specify in `curl` command then extracted it to `./tmp`. Copy all you code in example code frame and run it. `tflite_convert --output_file=./tmp/foo.tflite --graph_def_file=./tmp/mobilenet_v1_0.50_128/frozen_graph.pb --input_arrays=input --output_arrays=MobilenetV1/Predictions/Reshape_1` then it throw with error. \r\n```\r\nusage: tflite_convert [-h] --output_file OUTPUT_FILE\r\n                      (--saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\ntflite_convert: error: one of the arguments --saved_model_dir --keras_model_file is required\r\n```\r\n\r\n### Correct links\r\n\r\n**Basic** example should no some kind like this error. And I'm the beginner here who need to learn from basic example code you prepared.\r\n\r\n### Parameters defined\r\n\r\nAs you can see above, that was the command defined in you document page and I just copy + paste into command console.\r\n\r\n### Returns defined\r\n\r\nNo, but it shouldn't error like this.\r\n\r\n### Raises listed and defined\r\n\r\nAbove, I wrote it.\r\n\r\n### Usage example\r\n\r\nAbove, I wrote it.\r\n\r\n### Request visuals, if applicable\r\n\r\nRun yourself, you will see what I got.\r\n\r\n### Submit a pull request?\r\n\r\nNo.\r\n", "comments": ["cannot  reproduce it ", "Update:\r\nChange environment to a newly created Virtual Machine on Azure\r\n-  CentOS 7.7.1908\r\n-  Python3.6\r\n-  pip version 19.3.1\r\n-  Tensorflow version 2.0.0 (install via `pip install`)\r\n\r\nResult:\r\n```\r\ntflite_convert \\\r\n>   --output_file=/tmp/foo.tflite \\\r\n>   --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \\\r\n>   --input_arrays=input \\\r\n>   --output_arrays=MobilenetV1/Predictions/Reshape_1\r\nusage: tflite_convert [-h] --output_file OUTPUT_FILE\r\n                      (--saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\ntflite_convert: error: one of the arguments --saved_model_dir --keras_model_file is required\r\n```", "The [example](https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_graphdef_) you are referring to contains documentation on using the ```tflite_convert``` command line tool in TensorFlow 1.X. Therefore the above example works in TF 1.X and fails in TF 2.X\r\nFor using ```tflite_convert``` command line tool in TF 2.X you may see https://www.tensorflow.org/lite/convert/index\r\n ", "Then,\r\nCan I use model `.pb` file from the [example](https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_graphdef_) (the stuff defined in example `curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.50_128_frozen.tgz   | tar xzv -C /tmp` to be a convert source model?\r\n\r\nThe truth is I tried once and it look like it's the **multiple functions** that the convert tool doesn't be implemented yet.\r\n```\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\n\r\nAnd it has same result as I tried with downloaded model from **Azure Custom Vision**.\r\n\r\nTo clarify more specifically for what I want. I want to run [\"Android example project of Object Detection\"](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/README.md) with the `.tflite` trained model which downloaded from **Azure Custom Vision**.\r\n\r\nFirst, I tried with `.tflite` version: result -> app crashed immediately with \"byte buffer specific number is not correct\" error message.\r\nThen, I want to tried from convert `.pb` version to `.tflite` as what I understand now the `.pb` file I got it should be a \"saved model\". But result is same **Converting multiple functions is under development**\r\n\r\nAs a new to Machine Learning like me, I just need an example that can follow and see output without error, so, I can use it as reference what is different from what I got. The thing I found right now make my head confuse and dizzy because I spend time for this issue for 7 days.", "@xenogew You are trying to use a classification model in an object detection example. As stated explained in the [Android example project of Object Detection](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/README.md) page you referred to, you should start from a quantized [MobileNet SSD](https://github.com/tensorflow/models/tree/master/research/object_detection) model instead.\r\n\r\n**classification models != object detection models**", "@freedomtan OK, in the case you mentioned, I tried once with create new project on Azure Custom Vision in Object Detection type - trained - download `.tflite` file and replace in the Android example code. But it still crashed with exception.\r\n```\r\n...\r\n...\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 21344\r\n    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 2076672 bytes and a ByteBuffer with 270000 bytes.\r\n...\r\n...\r\n```\r\nDo you have suggestion about this issue?\r\n\r\nAnd what you mentioned about \"Quantized MobileNet SSD\", how can I recognize it, cause it's not appear about it in anywhere? Do Tensorflow have command line to detect type of the model?\r\n", "That looks like tensor size mismatch. I am not familiar with Azure related stuff, are you sure that models generated by the service is compatible with the app? My suggestion is to start from something like the [TFLite object detection doc](https://www.tensorflow.org/lite/models/object_detection/overview) and read [quantization](https://www.tensorflow.org/lite/performance/model_optimization) related stuff.", "@freedomtan Yes, sure, I downloaded from the 'Export' option which provide a lot of type of model compatible and Tensorflow is the one of them, it also provide choices of Tensorflow, Tensorflow Lite, Tensorflow.js.\r\n\r\nSo, the version compatible will be related to this issue? For example: \"Android example use the 'Model' created by Tensorflow 2.0\" but \"Azure thing is created from Tensorflow 1.7\" (I don't know, is it really  be?, but just raise example to ask you about backward compatibility of Model)\r\n\r\n### The tensor size you mentioned, what is it?\r\n-  Is it related to anything about technical term/word of Tensorflow?\r\n-  Or it's just a file size that mismatch to the defined variable in the code?\r\n-  I thought the code will be defined with variable, so the size of it shouldn't related to the model file that much. I thought it should read the size number from the file that be defined the file reference (path/file name) and the code read it and recognize the file size itself. Am I wrong?\r\n-  If you please, could you help me point to the place in the code, which I should learn and make change to it to make the application can run without crash?\r\n\r\nAnd thanks to the guide links you mentioned, I'll take a look at them.", "> Update:\r\n> Change environment to a newly created Virtual Machine on Azure\r\n> \r\n> * CentOS 7.7.1908\r\n> * Python3.6\r\n> * pip version 19.3.1\r\n> * Tensorflow version 2.0.0 (install via `pip install`)\r\n> \r\n> Result:\r\n> \r\n> ```\r\n> tflite_convert \\\r\n> >   --output_file=/tmp/foo.tflite \\\r\n> >   --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \\\r\n> >   --input_arrays=input \\\r\n> >   --output_arrays=MobilenetV1/Predictions/Reshape_1\r\n> usage: tflite_convert [-h] --output_file OUTPUT_FILE\r\n>                       (--saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\n> tflite_convert: error: one of the arguments --saved_model_dir --keras_model_file is required\r\n> ```\r\n\r\nApologies for the delay in response. You may add the `--enable_v1_converter` flag to get support for converting graphdefs from the command line in TF 2.\r\n```\r\ntflite_convert \\\r\n  --output_file=/tmp/foo.tflite \\\r\n  --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \\\r\n  --enable_v1_converter\\\r\n  --input_arrays=input \\\r\n   --output_arrays=MobilenetV1/Predictions/Reshape_1\r\n```"]}, {"number": 33773, "title": "Add documentation for tf.math.add", "body": "Added documentation for tf.math.add - MklAddV2 Issue #25846", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33773) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33773) for more info**.\n\n<!-- ok -->"]}, {"number": 33772, "title": "Init node weights/Assign doesn't exist in graph happens when convert to tflite model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nthe same code as \r\nhttps://colab.research.google.com/gist/jvishnuvardhan/bc515f885097fc6fd01076a3859e7e5b/tf_31609_tflite.ipynb#scrollTo=XVatsdZZuL_x\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6.9\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ncannot convert tf model to tf lite model, with this error: Init node weights/Assign doesn't exist in graph\r\n\r\n**Code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\n\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nvar = tf.get_variable(name=\"weights\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + var\r\nout = tf.identity(val, name=\"out\")\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  converter = tf.lite.TFLiteConverter.from_session(sess, [img], [out])\r\n  tflite_model = converter.convert()\r\n  open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n**Other info / logs**\r\nWARNING:tensorflow:From /Users/dragonx/code/dcscn-super-resolution-master/test/tf_converter_test.py:3: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From /Users/dragonx/code/dcscn-super-resolution-master/test/tf_converter_test.py:4: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nWARNING:tensorflow:From /Users/dragonx/code/dcscn-super-resolution-master/test/tf_converter_test.py:8: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-10-28 11:53:03.451728: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-28 11:53:03.461136: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8f442dc3f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-10-28 11:53:03.461147: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /Users/dragonx/code/dcscn-super-resolution-master/test/tf_converter_test.py:9: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-10-28 11:53:03.471663: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-10-28 11:53:03.471717: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-28 11:53:03.472863: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2019-10-28 11:53:03.472873: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-10-28 11:53:03.472877: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nWARNING:tensorflow:From /Users/dragonx/anaconda3/envs/scripts_env/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /Users/dragonx/anaconda3/envs/scripts_env/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n2019-10-28 11:53:03.478324: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-10-28 11:53:03.478372: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-28 11:53:03.479042: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node weights/Assign doesn't exist in graph", "comments": ["it seems that the model did comes out correctly but with a Error Log", "tf2.0"]}, {"number": 33771, "title": "Val loss behaves strange while using custom training loop in tensorflow 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, I defined my own training and validation loop and also my own keras model.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nwindows 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo.\r\n\r\n- TensorFlow installed from (source or binary):\r\nFrom source.\r\n\r\n- TensorFlow version (use command below):\r\nTensorflow 2.0.0 (gpu version)\r\n\r\n- Python version:\r\nPython 3.6.2\r\n\r\n**Describe the current behavior**\r\nI'm using a VGG16 model written in tf2.0 to train on my own datasets. Some BatchNormalization layers were included in the model and the \"training\" argument were set to True during training time and False during validation time as described in many tutorials. The train_loss decreased to a certain level during training as expected.\r\nHowever, the val_loss behaves really strange. I checked out the output of the model after training and found out that, if I set the training argument to True, the output is quite correct, but if I set it to False, the result is incorrect at all. According to the tutorials in tensorflow website, when training is set to False , the model will normalize its inputs using the mean and variance of its moving statistics learned during training but it doesn't seem so. Am I missing something?\r\n\r\n**Describe the expected behavior**\r\nI think in the validation step if I set ```training``` to ```False``` the model should use the learned moving mean and variance as expected but it doesnt seem so.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nimport os\r\nimport matplotlib.pyplot as plt\r\nfrom PIL import Image\r\nfrom tensorflow.keras.preprocessing import image\r\nimport glob\r\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, ReLU, Conv2DTranspose, add, concatenate\r\nfrom scipy.io import loadmat\r\nimport numpy as np\r\n# from mobilev3 import MobileNetV3Large\r\nfrom vgg_pr import VGG_PR\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\nimport logging\r\nimport cv2\r\n\r\n# parameters\r\nimg_size = (299,299)\r\nbatch_size = 8\r\nnum_label = 20\r\ninitial_lr = 0.001\r\ntotal_epoch = 100\r\nrepeat_times = 5\r\n# case number\r\ncase_num = 9\r\n\r\nos.chdir(os.getcwd())\r\n\r\ntrain_img_list = sorted(glob.glob('../dataset/train/intensity/*.mat'))\r\ntrain_label_list = sorted(glob.glob('../dataset/train/phase/*.txt'))\r\nval_img_list = sorted(glob.glob('../dataset/validate/intensity/*.mat'))\r\nval_label_list = sorted(glob.glob('../dataset/validate/phase/*.txt'))\r\nckpt_path = '../checkpoints/VGG-{epoch}.ckpt'\r\nlog_path = '../log/{}/'\r\nif not os.path.exists(log_path.format(case_num)):\r\n    os.mkdir(log_path.format(case_num))\r\n\r\n# read data\r\ndef read_img(filename):\r\n    image_dict = loadmat(filename.decode('utf-8'))\r\n    exp_thresh = 1e4\r\n    image_decoded = image_dict['Iz']\r\n    image_decoded = cv2.resize(image_decoded, img_size, interpolation=cv2.INTER_AREA)\r\n    image_decoded[image_decoded>exp_thresh] = exp_thresh\r\n    image_decoded /= exp_thresh\r\n    image_resized = np.float32(np.expand_dims(image_decoded, axis=-1))\r\n    return image_resized\r\n\r\ndef read_label(filename):\r\n    label = open(filename).read()\r\n    label = label.strip().split(' ')\r\n    label = [np.float32(i) for i in label if i!='']\r\n    label = np.reshape(label, [1,1,-1])\r\n    label = np.array(label) + 0.5  \r\n    return label\r\ndef parse_function(image_filename, label_filename):\r\n    img = tf.numpy_function(read_img, [image_filename], tf.float32)\r\n    label = tf.numpy_function(read_label, [label_filename], tf.float32)\r\n    return img, label\r\ndef train():\r\n    logging.basicConfig(level=logging.INFO)\r\n    tdataset = tf.data.Dataset.from_tensor_slices((train_img_list[:200], train_label_list[:200]))\r\n    tdataset = tdataset.map(parse_function, 3).shuffle(buffer_size=200).batch(batch_size).repeat(repeat_times)\r\n    vdataset = tf.data.Dataset.from_tensor_slices((val_img_list[:100], val_label_list[:100]))\r\n    vdataset = vdataset.map(parse_function, 3).batch(batch_size)\r\n\r\n    ### Vgg model\r\n    model = VGG_PR(num_classes=num_label)\r\n\r\n    logging.info('Model loaded')\r\n\r\n    start_epoch = 0\r\n    latest_ckpt = tf.train.latest_checkpoint(os.path.dirname(ckpt_path))\r\n    if latest_ckpt:\r\n        start_epoch = int(latest_ckpt.split('-')[1].split('.')[0])\r\n        model.load_weights(latest_ckpt)\r\n        logging.info('model resumed from: {}, start at epoch: {}'.format(latest_ckpt, start_epoch))\r\n    else:\r\n        logging.info('training from scratch since weights no there')\r\n\r\n    ######## training loop ########\r\n    loss_object = tf.keras.losses.MeanSquaredError()\r\n    val_loss_object = tf.keras.losses.MeanSquaredError()\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\r\n    train_loss = tf.metrics.Mean(name='train_loss') \r\n    val_loss = tf.metrics.Mean(name='val_loss')\r\n    writer = tf.summary.create_file_writer(log_path.format(case_num))\r\n\r\n    with writer.as_default():\r\n        for epoch in range(start_epoch, total_epoch):\r\n            print('start training')\r\n            try:\r\n                for batch, data in enumerate(tdataset):\r\n                    images, labels = data\r\n                    with tf.GradientTape() as tape:\r\n                        pred = model(images, training=True)\r\n                        if len(pred.shape) == 2:\r\n                            pred = tf.reshape(pred,[-1, 1, 1, num_label])\r\n                        loss = loss_object(pred, labels)\r\n                    gradients = tape.gradient(loss, model.trainable_variables)\r\n                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n                    if batch % 20 ==0:\r\n                        logging.info('Epoch: {}, iter: {}, loss:{}'.format(epoch, batch, loss.numpy()))\r\n                    tf.summary.scalar('train_loss', loss.numpy(), step=epoch*1250*repeat_times+batch)      # the tdataset has been repeated 5 times..\r\n                    tf.summary.text('Zernike_coe_pred', tf.as_string(tf.squeeze(pred)), step=epoch*1250*repeat_times+batch)\r\n                    tf.summary.text('Zernike_coe_gt', tf.as_string(tf.squeeze(labels)), step=epoch*1250*repeat_times+batch)\r\n\r\n                    writer.flush()\r\n                    train_loss(loss)\r\n                model.save_weights(ckpt_path.format(epoch=epoch))\r\n            except KeyboardInterrupt:\r\n                logging.info('interrupted.')\r\n                model.save_weights(ckpt_path.format(epoch=epoch))\r\n                logging.info('model saved into {}'.format(ckpt_path.format(epoch=epoch)))\r\n                exit(0)\r\n            # validation step\r\n            for batch, data in enumerate(vdataset):\r\n                images, labels = data\r\n                val_pred = model(images, training=False)\r\n                if len(val_pred.shape) == 2:\r\n                    val_pred = tf.reshape(val_pred,[-1, 1, 1, num_label])\r\n                v_loss = val_loss_object(val_pred, labels)\r\n                val_loss(v_loss)\r\n            logging.info('Epoch: {}, average train_loss:{}, val_loss: {}'.format(epoch, train_loss.result(), val_loss.result()))\r\n            tf.summary.scalar('val_loss', val_loss.result(), step = epoch)\r\n            writer.flush()\r\n            train_loss.reset_states()\r\n            val_loss.reset_states()\r\n        model.save_weights(ckpt_path.format(epoch=epoch))\r\n```\r\nAnd here is my vgg_pr model:\r\n```python\r\nimport tensorflow as tf\r\n\r\n# ------------------------------- Layers part -------------------------------\r\nclass BatchNormalization(tf.keras.layers.Layer):\r\n    \"\"\"All our convolutional layers use batch-normalization\r\n    layers with average decay of 0.99.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__(name=\"BatchNormalization\")\r\n        self.bn = tf.keras.layers.BatchNormalization(\r\n            momentum=0.99,\r\n            name=\"BatchNorm\")\r\n\r\n    def call(self, input, training):\r\n        return self.bn(input, training)\r\n\r\nclass ConvBnAct(tf.keras.layers.Layer):\r\n    def __init__(\r\n            self,\r\n            filters=64,\r\n            kernel_size=(3,3),\r\n            activation='relu',\r\n            padding='same',\r\n            name='conv'):\r\n        super().__init__(name=\"ConvBnAct\")\r\n\r\n        self.conv = tf.keras.layers.Conv2D(\r\n            filters=filters,\r\n            kernel_size=kernel_size,\r\n            activation=activation,\r\n            padding=padding,\r\n            name=name)\r\n        # self.norm = BatchNormalization()\r\n        self.norm = tf.keras.layers.BatchNormalization(name='BatchNorm')\r\n\r\n    def call(self, input, training):\r\n        x = self.conv(input)\r\n        x = self.norm(x,training=training)\r\n        return x\r\n\r\nclass Block_1(tf.keras.layers.Layer):\r\n    def __init__(\r\n            self):\r\n        super().__init__(name=\"Block_1\")\r\n        self.conv1 = ConvBnAct(64,name='block1_conv1')\r\n        self.conv2 = ConvBnAct(64,name='block1_conv2')\r\n        self.pool = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')\r\n\r\n    def call(self, input,training=False):\r\n        x = self.conv1(input,training)\r\n        x = self.conv2(x,training)\r\n        x = self.pool(x)\r\n        return x\r\n\r\nclass Block_2(tf.keras.layers.Layer):\r\n    def __init__(\r\n            self):\r\n        super().__init__(name=\"Block_2\")\r\n        self.conv1 = ConvBnAct(128,name='block2_conv1')\r\n        self.conv2 = ConvBnAct(128,name='block2_conv2')\r\n        self.pool = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')\r\n\r\n    def call(self, input,training=False):\r\n        x = self.conv1(input,training)\r\n        x = self.conv2(x,training)\r\n        x = self.pool(x)\r\n        return x\r\n\r\nclass Block_3(tf.keras.layers.Layer):\r\n    def __init__(\r\n            self):\r\n        super().__init__(name=\"Block_3\")\r\n        self.conv1 = ConvBnAct(256,name='block3_conv1')\r\n        self.conv2 = ConvBnAct(256,name='block3_conv2')\r\n        self.conv3 = ConvBnAct(256,name='block3_conv3')\r\n        self.pool = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')\r\n\r\n    def call(self, input ,training=False):\r\n        x = self.conv1(input,training)\r\n        x = self.conv2(x,training)\r\n        x = self.conv3(x,training)\r\n        x = self.pool(x)\r\n        return x\r\n\r\nclass Block_4(tf.keras.layers.Layer):\r\n    def __init__(\r\n            self):\r\n        super().__init__(name=\"Block_4\")\r\n        self.conv1 = ConvBnAct(512,name='block4_conv1')\r\n        self.conv2 = ConvBnAct(512,name='block4_conv2')\r\n        self.conv3 = ConvBnAct(512,name='block4_conv3')\r\n        self.pool = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')\r\n\r\n    def call(self, input,training=False):\r\n        x = self.conv1(input,training)\r\n        x = self.conv2(x,training)\r\n        x = self.conv3(x,training)\r\n        x = self.pool(x)\r\n        return x\r\n\r\nclass Block_5(tf.keras.layers.Layer):\r\n    def __init__(\r\n            self):\r\n        super().__init__(name=\"Block_5\")\r\n        self.conv1 = ConvBnAct(512,name='block5_conv1')\r\n        self.conv2 = ConvBnAct(512,name='block5_conv2')\r\n        self.conv3 = ConvBnAct(512,name='block5_conv3')\r\n        self.pool = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')\r\n\r\n    def call(self, input,training=False):\r\n        x = self.conv1(input,training)\r\n        x = self.conv2(x,training)\r\n        x = self.conv3(x,training)\r\n        x = self.pool(x)\r\n        return x\r\n\r\nclass VGG_PR(tf.keras.Model):\r\n    def __init__(self,num_classes):\r\n        super(VGG_PR, self).__init__()\r\n        self.block1 = Block_1()\r\n        self.block2 = Block_2()\r\n        self.block3 = Block_3()\r\n        self.block4 = Block_4()\r\n        self.block5 = Block_5()\r\n        self.avg = tf.keras.layers.GlobalAveragePooling2D()\r\n        self.fc1 = tf.keras.layers.Dense(256, activation='relu', name='fc1')\r\n        self.fc2 = tf.keras.layers.Dense(128, activation='relu', name='fc2')\r\n        self.fc3 = tf.keras.layers.Dense(num_classes,activation='linear',name='predictions')\r\n\r\n    def call(self, input, training=False):\r\n        x = self.block1(input,training)\r\n        x = self.block2(x,training)\r\n        x = self.block3(x,training)\r\n        x = self.block4(x,training)\r\n        x = self.block5(x,training)\r\n        x = self.avg(x)\r\n        # print(\"output1:{}\".format(x))\r\n        x = self.fc1(x)\r\n        # print(\"output2:{}\".format(x))\r\n        x = self.fc2(x)\r\n        x = self.fc3(x)\r\n        return x\r\n\r\n```\r\n", "comments": ["I thought it's because of some problems with ```tf.keras.layers.BatchNormalization()``` but I didn't find the way to solve it. ", "@SuperCrystal \r\n\r\nLooks like code is incomplete. Please, help us with simple standalone code to reproduce the issue in our environment. It helps us to localize the issue faster. Thanks!", "@ravikyram Hello! I've added the complete code. Is there any suggestions on how to solve this problem? Thx!", "@SuperCrystal \r\nI tried reproducing the issue and i am seeing the below error.`ModuleNotFoundError: No module named 'mobilev3'`.Also, please share supporting files to reproduce the issue. Thanks!", "@ravikyram Sorry for that! I have corrected it. See whether you can reproduce the issue this time? Thanks a lot!", "@ymodak Sorry but is there any possible solution or am I missing sth? Thx a lot :)", "I'm also having an issue with val loss behaving strangely. Although in my case, I'm using `tf.keras `for the training loop. I find that val loss behaves fine if I disable v2 behavior `tf.compat.v1.disable_v2_behavior()`", "@JadTawil-theonly @lminer  Can you please post a new issue and provide a minimal reproducible code snippet? Thanks!", "BatchNormalization behaves differently during training and validation. This MAY be your issue, idk you let me know.\r\n\r\nDuring training, the statistics of the current batch are used to normalize the features. The computed mean and variance are used to update a running mean and running variance.\r\nDuring validation, the running mean and running variance are used, and nothing is updated.\r\nYou may get very different results during validation because of this.\r\nOver time, the running mean and running variance will have values that are closer to the batch mean and variance. You can control this with the momentum term.\r\nAnother solution would be to normalize your data around zero, by subtracting the mean and dividing by std of each input channel (assuming image type data). Doing this, you don't have to adjust the momentum.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@SuperCrystal Were you able to find a solution to this problem? I am not using model subclassing, but still have the same problem at hand. Thanks!"]}, {"number": 33770, "title": "Is it possible to support gif images in ImageDataGenerator?", "body": "**System information**\r\n- TensorFlow version (you are using):\r\n 2.0.0\r\n- Are you willing to contribute it (Yes/No):\r\nNot sure how to :(\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nImageDataGenerator currently ignores gif images in its methods (like `flow_from_directory`), why doesn't it take gif? As gif is also widely used over the Internet it should be supported too.\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nWhoever that has a bunch of gif in their dataset\r\n", "comments": ["@ZxMYS Thanks for the issue!\r\n\r\nFuture work in data handling is happening in `tf.data` and `tf.io`. You can decode a gif using this method: https://www.tensorflow.org/api_docs/python/tf/io/decode_gif"]}, {"number": 33769, "title": "For tensorflow1.15: undefined symbol: ncclBroadcast", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip3.7\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda10.0/cuDNN7.5.1\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nThis has happened after install  Tensorflow1.15.0 from the source\r\n1. bazel build:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nand then:\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow-1.15\r\nwe got:\r\n/tmp/tensorflow-1.15/tensorflow-1.15.0-cp37-cp37m-linux_x86_64.whl\r\n\r\n2. install tensorflow:\r\npip install /tmp/tensorflow-1.15/tensorflow-1.15.0-cp37-cp37m-linux_x86_64.whl\r\n\r\n3. Check if the installation is successful:\r\n#python -c \"import tensorflow as tf; print(tf.__version__)\"\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: ncclBroadcast\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 99, in <module>\r\n    from tensorflow_core import *\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/local/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: ncclBroadcast\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow website](https://www.tensorflow.org/install/source) . Did you install those drivers for [GPU support ](https://www.tensorflow.org/install/gpu).Please, let us know. Thanks!", "> Just to verify did you get chance to follow instructions from [TensorFlow website](https://www.tensorflow.org/install/source) . Did you install those drivers for [GPU support ](https://www.tensorflow.org/install/gpu).Please, let us know. Thanks!\r\n\r\nThe source is bazel build by: cuda10.0 and cuDNN7.5.1\r\n\r\nThe GPU machine is:\r\nnvidia-smi \r\nTue Oct 29 19:27:16 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P40           On   | 00000000:04:00.0 Off |                  Off |\r\n| N/A   28C    P8     9W / 250W |      0MiB / 24451MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33769\">No</a>\n"]}, {"number": 33768, "title": "Metrics disappear on subclassed Keras model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab TPU\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n%tensorflow_version 2.x\r\n\r\ninputs = tf.keras.Input((10,))\r\noutputs = tf.keras.layers.Dense(10)(inputs)\r\nunsubclassed = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nunsubclassed.compile(loss='categorical_crossentropy', metrics=['accuracy'])\r\n_ = unsubclassed(tf.random.uniform((10, 10)))\r\n\r\nclass Subclassed(tf.keras.Model):\r\n    def __init__(self,  **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(10)\r\n\r\n    @tf.function\r\n    def call(self, x, training=None):\r\n        return self.dense(x)\r\n\r\nsubclassed = Subclassed()\r\nsubclassed.compile(loss='categorical_crossentropy', metrics=['accuracy'])\r\n_ = subclassed(tf.random.uniform((10, 10)))\r\n\r\nprint(unsubclassed.metrics_names, unsubclassed.metrics)\r\nprint(subclassed.metrics_names, subclassed.metrics)\r\n```\r\n\r\ngives:\r\n\r\n```\r\n['loss', 'accuracy'] [<tensorflow.python.keras.metrics.MeanMetricWrapper object at 0x7ffa0a6c53c8>]\r\n['loss'] []\r\n```\r\n\r\n**Expected Result**\r\n\r\nSubclassed model should not eat metrics.", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-dev20191028 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e47f4408b29f582a77d683344b2cfa77/untitled311.ipynb)Thanks!", "So there is some difference for the subclass model vs normal model.\r\n\r\nWith normal model, we know the input/output shape when we construct the model, hence we properly config the model loss and metrics when compile() is invoked. \r\n\r\nWith subclass model, the input/output shape is unknown to us until it is first tested with proper data. In the compile() method, we will do a deferred compile and wait for the proper data.\r\n\r\nIf you change your code to subclassed.predict(), then the metric value is properly show up.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33768\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33768\">No</a>\n"]}, {"number": 33767, "title": "AutoGraph: Error transforming entity / AssertionError: Bad argument number for Name: 3, expecting 4", "body": "**System information**\r\n- OS Platform and Distribution: Arch Linux, 5.3.7-arch1-1-ARCH\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Kerasversion (use command below): 2.2.4-tf\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: CUDA 10.1.243 / cuDNN 7.6.2.24\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\nvarious errors/warnings as: \"Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0f2846c320> could not be transformed and will be executed as-is\"\r\n\r\n> 2019-10-27 22:54:04,017 - INFO - Error transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0f2846c320>\r\n> Traceback (most recent call last):\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n>     converted_f = conversion.convert(target_entity, program_ctx)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n>     free_nonglobal_var_names)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache\r\n>     entity, program_ctx)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast\r\n>     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast\r\n>     node = node_to_graph(node, context)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 698, in node_to_graph\r\n>     node = converter.standard_analysis(node, context, is_initial=True)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 383, in standard_analysis\r\n>     node = qual_names.resolve(node)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/qual_names.py\", line 254, in resolve\r\n>     return QnResolver().visit(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 262, in visit\r\n>     return visitor(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 317, in generic_visit\r\n>     value = self.visit(value)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 262, in visit\r\n>     return visitor(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 317, in generic_visit\r\n>     value = self.visit(value)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 262, in visit\r\n>     return visitor(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 326, in generic_visit\r\n>     new_node = self.visit(old_value)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 262, in visit\r\n>     return visitor(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 317, in generic_visit\r\n>     value = self.visit(value)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 262, in visit\r\n>     return visitor(node)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/qual_names.py\", line 236, in visit_Subscript\r\n>     if isinstance(s.value, gast.Num):\r\n> AttributeError: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0f2846c320> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> 2019-10-27 22:54:04,017 - WARNING - Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0f2846c320> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> 2019-10-27 22:54:06.670890: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_gru_10450_11011' and '__inference___backward_cudnn_gru_with_fallback_9300_9441_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_12398' both implement 'gru_ee50c0e8-e326-45b7-b98e-88e06e2f6f01' but their signatures do not match.\r\n> \r\n\r\n\r\n> 2019-10-27 22:54:01,401 - INFO - Error transforming entity <bound method Output.call of <model.Output object at 0x7f0f3c96aa90>>\r\n> Traceback (most recent call last):\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n>     converted_f = conversion.convert(target_entity, program_ctx)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n>     free_nonglobal_var_names)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache\r\n>     entity, program_ctx)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 471, in convert_entity_to_ast\r\n>     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast\r\n>     node = node_to_graph(node, context)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 699, in node_to_graph\r\n>     node = converter.apply_(node, context, function_scopes)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 409, in apply_\r\n>     node = converter_module.transform(node, context)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 120, in transform\r\n>     return FunctionBodyTransformer(ctx).visit(node)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n>     return super(Base, self).visit(node)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n>     result = super(Base, self).visit(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 262, in visit\r\n>     return visitor(node)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 87, in visit_FunctionDef\r\n>     node = self.generic_visit(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 317, in generic_visit\r\n>     value = self.visit(value)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n>     return super(Base, self).visit(node)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n>     result = super(Base, self).visit(node)\r\n>   File \"/usr/lib/python3.7/ast.py\", line 262, in visit\r\n>     return visitor(node)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 44, in visit_Return\r\n>     value=node.value)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 261, in replace\r\n>     replacements[k] = _convert_to_ast(replacements[k])\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 223, in _convert_to_ast\r\n>     return gast.Name(id=n, ctx=None, annotation=None)\r\n>   File \"/usr/lib/python3.7/site-packages/gast/gast.py\", line 19, in create_node\r\n>     format(Name, nbparam, len(Fields))\r\n> AssertionError: Bad argument number for Name: 3, expecting 4\r\n> WARNING:tensorflow:Entity <bound method Output.call of <model.Output object at 0x7f0f3c96aa90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n> 2019-10-27 22:54:01,401 - WARNING - Entity <bound method Output.call of <model.Output object at 0x7f0f3c96aa90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n> WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5783: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version", "comments": ["@olk ,\r\nCan you share a simple and standalone code to reproduce the issue reported?Thanks!", "@oanush it's not so easy to produce standalone code - I could give you the link to the git repo (contains Makefiles to generate the data and to train the model) alternatively I could post the model code.\r\nIt is the same code that causes issue #33809 '_MirroredStrategy compared to OneDeviceStrategy slower and much weaker learning_' - maybe both issues are related.", "- Model:\r\n```\r\nclass FeatureExtraction(Layer):\r\n    def __init__(self, conv_filters, pool_size, name='feature-extraction', **kwargs):\r\n        super(FeatureExtraction, self).__init__(name=name, **kwargs)\r\n        self.conv1 = Conv2D(filters=conv_filters, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal', name='conv1')\r\n        self.conv2 = Conv2D(filters=conv_filters, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal', name='conv2')\r\n        self.max1 = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')\r\n        self.max2 = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')\r\n\r\n    def call(self, inputs):\r\n        x = self.conv1(inputs)\r\n        x = self.max1(x)\r\n        x = self.conv2(x)\r\n        return self.max2(x)\r\n\r\n    def get_config(self):\r\n        return super(FeatureExtraction, self).get_config()\r\n\r\n\r\nclass FeatureReduction(Layer):\r\n    def __init__(self, img_w, img_h, pool_size, conv_filters, name='feature-reduction', **kwargs):\r\n        super(FeatureReduction, self).__init__(name=name, **kwargs)\r\n        target_shape = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\r\n        self.reshape = Reshape(target_shape=target_shape, name='reshape')\r\n        self.dense = Dense(32, activation='relu', name='dense')\r\n\r\n    def call(self, inputs):\r\n        x = self.reshape(inputs)\r\n        return self.dense(x)\r\n\r\n    def get_config(self):\r\n        return super(FeatureReduction, self).get_config()\r\n\r\n\r\nclass SequentialLearner(Layer):\r\n    def __init__(self, name='sequential-learner', **kwargs):\r\n        super(SequentialLearner, self).__init__(name=name, **kwargs)\r\n        self.gru_1a = GRU(512, return_sequences=True, kernel_initializer='he_normal', name='gru_1a')\r\n        self.gru_1b = GRU(512, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru_1b')\r\n        self.gru_2a = GRU(512, return_sequences=True, kernel_initializer='he_normal', name='gru_2a')\r\n        self.gru_2b = GRU(512, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru_2b')\r\n\r\n    def call(self, inputs):\r\n        x_1a = self.gru_1a(inputs)\r\n        x_1b = self.gru_1b(inputs)\r\n        x = add([x_1a, x_1b])\r\n        x_2a = self.gru_2a(x)\r\n        x_2b = self.gru_2b(x)\r\n        return concatenate([x_2a, x_2b])\r\n\r\n    def get_config(self):\r\n        return super(SequentialLearner, self).get_config()\r\n\r\n\r\nclass Output(Layer):\r\n    def __init__(self, output_size, name='output', **kwargs):\r\n        super(Output, self).__init__(name=name, **kwargs)\r\n        self.dense = Dense(output_size, kernel_initializer='he_normal', name='dense')\r\n        self.softmax = Activation('softmax', name='softmax')\r\n\r\n    def call(self, inputs):\r\n        x = self.dense(inputs)\r\n        return self.softmax(x)\r\n\r\n    def get_config(self):\r\n        return super(Output, self).get_config()\r\n\r\n\r\nclass OCRNet(Model):\r\n    def __init__(self, output_size, img_w, img_h, max_text_len, name='OCRNet', **kwargs):\r\n        # parameters\r\n        conv_filters = 16\r\n        pool_size = 2\r\n        # define layers\r\n        feature_extraction = FeatureExtraction(conv_filters=conv_filters, pool_size=pool_size)\r\n        sequential_learner = SequentialLearner()\r\n        feature_reduction = FeatureReduction(img_w=img_w, img_h=img_h, pool_size=pool_size, conv_filters=conv_filters)\r\n        output = Output(output_size)\r\n        # NHWC == channels_last NCHW == channels_first\r\n        # initialize input shape\r\n        if 'channels_first' == K.image_data_format():\r\n            input_shape = (1, img_w, img_h)\r\n        else:\r\n            input_shape = (img_w, img_h, 1)\r\n        # input\r\n        inputs = Input(name='the_input', shape=input_shape, dtype='float32')\r\n        labels = Input(name='the_labels', shape=[max_text_len], dtype='float32')\r\n        input_length = Input(name='input_length', shape=[1], dtype='int64')\r\n        label_length = Input(name='label_length', shape=[1], dtype='int64')\r\n        # call layers\r\n        x = feature_extraction(inputs)\r\n        x = feature_reduction(x)\r\n        x = sequential_learner(x)\r\n        predictions = output(x)\r\n        # Keras doesn't currently support loss funcs with extra parameters\r\n        # so CTC loss is implemented in a lambda layer\r\n        loss_out = Lambda(self._ctc_lambda_func, output_shape=(1,), name='ctc')([predictions, labels, input_length, label_length])\r\n        super(OCRNet, self).__init__(\r\n                inputs=[inputs, labels, input_length, label_length], outputs=loss_out,\r\n                name=name, **kwargs)\r\n\r\n        # ctc decoder\r\n        flattened_input_length = K.reshape(input_length, (-1,))\r\n        top_k_decoded, _ = K.ctc_decode(predictions, flattened_input_length)\r\n        self.decoder = K.function([inputs, flattened_input_length], [top_k_decoded[0]])\r\n\r\n    # loss and train functions, network architecture\r\n    def _ctc_lambda_func(self, args):\r\n        predictions, labels, input_length, label_length = args\r\n        # the 2 is critical here since the first couple outputs of the RNN\r\n        # tend to be garbage\r\n        predictions = predictions[:, 2:, :]\r\n        return K.ctc_batch_cost(labels, predictions, input_length, label_length)\r\n```\r\n- training (stripped):\r\n```\r\n    ...\r\n    strategy = tf.distribute.MirroredStrategy() if 1 < ngpus else tf.distribute.OneDeviceStrategy(device=\"/gpu:1\")\r\n    batch_size = batch_size * strategy.num_replicas_in_sync\r\n   ...\r\n    with strategy.scope():\r\n        model = OCRNet(train_gen.output_size, img_w, img_h, max_text_len)\r\n        model.summary()\r\n        adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\r\n        model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam, metrics=['accuracy'])\r\n    callbacks = []\r\n    start = time.perf_counter()\r\n    model.fit(\r\n            train_gen,\r\n            validation_data=val_gen,\r\n            epochs=epochs,\r\n            shuffle=False,\r\n            use_multiprocessing=True,\r\n            workers=6,\r\n            callbacks=callbacks)\r\n    elapsed = time.perf_counter() - start\r\n    logger.info('elapsed: {:0.3f}'.format(elapsed))\r\n```", "Hi, this issue should go away if you downgrade gast to 0.2.2 (`pip install gast==0.2.2`).\r\n\r\nThe dependencies in TF 2.0 pull the latest version of gast, which breaks compatibility at 0.3. This should be fixed in 2.1 which pins to this older version.\r\n\r\nPlease reopen the issue if downgrading didn't work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33767\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33767\">No</a>\n", "Thanks a lot \uff01\r\n\r\n> Hi, this issue should go away if you downgrade gast to 0.2.2 (`pip install gast==0.2.2`).\r\n> \r\n> The dependencies in TF 2.0 pull the latest version of gast, which breaks compatibility at 0.3. This should be fixed in 2.1 which pins to this older version.\r\n> \r\n> Please reopen the issue if downgrading didn't work.\r\n\r\n"]}, {"number": 33766, "title": " Failed to place the graph without changing the devices of some resources. ", "body": "- Have I written custom code :NO\r\n- Linux Ubuntu 18\r\n- TensorFlow installed from (source or binary): from pip install \r\n- TensorFlow version (use command below): \r\n```\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n ('v2.0.0-rc2-26-g64c3d38', '2.0.0')\r\n```\r\nHi, \r\n\r\nI followed the guide of https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/cityscapes.md\r\n\r\nafter converting the data, I gave the provided command for trainin:\r\n```# From tensorflow/models/research/\r\n\r\npython deeplab/train.py \\\r\n    --logtostderr \\\r\n    --training_number_of_steps=90000 \\\r\n    --train_split=\"train\" \\\r\n    --model_variant=\"xception_65\" \\\r\n    --atrous_rates=6 \\\r\n    --atrous_rates=12 \\\r\n    --atrous_rates=18 \\\r\n    --output_stride=16 \\\r\n    --decoder_output_stride=4 \\\r\n    --train_crop_size=\"769,769\" \\\r\n    --train_batch_size=1 \\\r\n    --dataset=\"cityscapes\" \\\r\n    --tf_initial_checkpoint=${PATH_TO_INITIAL_CHECKPOINT} \\\r\n    --train_logdir=${PATH_TO_TRAIN_DIR} \\\r\n    --dataset_dir=${PATH_TO_DATASET}\r\n```\r\n\r\nBut I got an error of \"Failed to place the graph without changing the devices of some resources\":\r\n\r\n```\r\nW tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:`\r\n\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nApplyMomentum: CPU \r\nIsVariableInitialized: CPU \r\nAssign: CPU \r\nIdentity: CPU XLA_CPU \r\nVariableV2: CPU \r\nConst: CPU XLA_CPU \r\n```\r\n\r\nhere is the following output information I got, I think it says only \"Segmentation fault\"\r\n```\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  logits/semantic/biases/Initializer/zeros (Const) \r\n  logits/semantic/biases (VariableV2) /device:GPU:0\r\n  logits/semantic/biases/Assign (Assign) /device:GPU:0\r\n  logits/semantic/biases/read (Identity) /device:GPU:0\r\n  logits/semantic/biases/Momentum/Initializer/zeros (Const) /device:GPU:0\r\n  logits/semantic/biases/Momentum (VariableV2) /device:GPU:0\r\n  logits/semantic/biases/Momentum/Assign (Assign) /device:GPU:0\r\n  logits/semantic/biases/Momentum/read (Identity) /device:GPU:0\r\n  Momentum/update_logits/semantic/biases/ApplyMomentum (ApplyMomentum) /device:GPU:0\r\n  report_uninitialized_variables/IsVariableInitialized_732 (IsVariableInitialized) /device:GPU:0\r\n  report_uninitialized_variables/IsVariableInitialized_1172 (IsVariableInitialized) /device:GPU:0\r\n  report_uninitialized_variables_1/IsVariableInitialized_732 (IsVariableInitialized) /device:GPU:0\r\n  report_uninitialized_variables_1/IsVariableInitialized_1172 (IsVariableInitialized) /device:GPU:0\r\n  save/Assign_113 (Assign) /device:GPU:0\r\n  save/Assign_114 (Assign) /device:GPU:0\r\n\r\nINFO:tensorflow:Running local_init_op.\r\nI1027 21:25:31.706468 140671729727296 session_manager.py:500] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI1027 21:25:31.916163 140671729727296 session_manager.py:502] Done running local_init_op.\r\nFatal Python error: Segmentation fault\r\n\r\nThread 0x00007ff0b07af740 (most recent call first):\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429 in _call_tf_sessionrun\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341 in _run_fn\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356 in _do_call\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350 in _do_run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173 in _run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950 in run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1169 in run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1411 in run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1338 in run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1252 in run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 754 in run\r\n  File \"deeplab/train.py\", line 513 in main\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/zwang/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"deeplab/train.py\", line 519 in <module>\r\nSegmentation fault\r\n```", "comments": ["ok problem solved , I didn' realise that ${PATH_TO_INITIAL_CHECKPOINT}  need to be set."]}, {"number": 33765, "title": "Problem   trying to convert from tf keras model to tflite does not work/let save", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO --I can save the tensorflow keras model to an h5 file but, when subsequently try to convert to TFLite using code below it fails. Following directions at https://www.tensorflow.org/tutorials/keras/save_and_load to perform conversion to TFLite (see code below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0 and nightly version(today)\r\n- Python version: 3.6.1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nhave done locally (pip install) and recently trying anaconda \r\n` conda create -n tf-n python  \r\n conda activate tf-n  \r\n pip install tf-nightly`\r\n\r\n**Describe the current behavior**\r\nHere is error --says save method is not available.  \r\n`Model: \"sequential_2\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d_4 (Conv2D)            (None, 148, 148, 32)      896       \r\n_________________________________________________________________\r\nmax_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            (None, 72, 72, 64)        18496     \r\n_________________________________________________________________\r\nmax_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \r\n_________________________________________________________________\r\nconv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \r\n_________________________________________________________________\r\nmax_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \r\n_________________________________________________________________\r\nconv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \r\n_________________________________________________________________\r\nmax_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 7, 7, 128)         0         \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 6272)              0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 512)               3211776   \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 2)                 1026      \r\n=================================================================\r\nTotal params: 3,453,634\r\nTrainable params: 3,453,634\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n want to save tflite_fileC:\\Grewe\\Classes\\CS663\\Mat\\TensorFlow\\cats_and_dogs_filtered\\my_tflite_model.h5\r\nconverter = <tensorflow.lite.python.lite.TFLiteConverterV2 object at 0x00000225A2142240>\r\nIOPub data rate exceeded.\r\nThe notebook server will temporarily stop sending output\r\nto the client in order to avoid crashing it.\r\nTo change this limit, set the config variable\r\n--NotebookApp.iopub_data_rate_limit.\r\n\r\nCurrent values:\r\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\r\nNotebookApp.rate_limit_window=3.0 (secs)\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-20-f45b37f63b2a> in <module>()\r\n     13 \r\n     14 #now save the tflite model to the file\r\n---> 15 tflite_model.save(tflite_file)\r\n\r\nAttributeError: 'bytes' object has no attribute 'save'\r\n`\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`#print out model summary so can see it for bug report\r\nmodel.summary()\r\n\r\n#from tensorflow import lite\r\ntflite_file  = os.path.join(BASE_DIRECTORY, 'my_tflite_model.h5')\r\nprint(\" want to save tflite_file\" + tflite_file)\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nprint(\"converter = \" + str(converter))\r\ntflite_model = converter.convert()\r\nprint(tflite_model)\r\n\r\n\r\n#now save the tflite model to the file\r\ntflite_model.save(tflite_file)`\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["For those of you looking at this --there is no example in the url for saving the tflite file and had assumed would have save() method like the regular tesorflow model.    This is not the case and no documentation found on tensorflow site (easily at least--I looked) went elsewhere and seems have to use the command show in the new code below --- HOWEVER, I have not yet tested if the python file writing will work with a load in Android app as of yet.   BUT, am closing with the hope this is the case.   IF anyone know if this solution for sure would work...please comment.\r\n\r\n`#from tensorflow import lite\r\ntflite_file  = os.path.join(BASE_DIRECTORY, 'my_tflite_model.h5')\r\nprint(\" want to save tflite_file\" + tflite_file)\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n\r\n#now save the tflite model to the file\r\n#tflite_model.save(tflite_file)   #Note this does not seem to work although in google documentation\r\nopen(tflite_file, \"wb\").write(tflite_model)`"]}, {"number": 33764, "title": "Unnecessary tf.distribute.MirroredStrategy() scopes in the distributed custom training tutorial", "body": "##  URL(s) with the issue: \r\nhttps://www.tensorflow.org/tutorials/distribute/custom_training\r\n\r\n## Description of issue (what needs changing):\r\nIn the tutorial, `with strategy.scope()` appears almost everywhere, which gives the impression that it is required that those locations. However, when checking [this example](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/densenet/distributed_train.py), I found that the scope is not required at all! Only `strategy.experimental_run_v2` and `strategy.reduce` suffice. Therefore, I would propose to modify the tutorial code to keep only a minimum amount of `with strategy.scope()` when necessary (e.g. when defining the model).\r\n\r\n### Submit a pull request?\r\n\r\nYes, I can create a PR but only when you have approved that this is valid. Thanks.\r\n", "comments": ["The example has been modified to use scopes wherever necessary. Since it is a notebook and all cells are independent, we need to add scopes. This is not true for python files where you can only have one scope and everything underneath it.\r\n\r\nClosing, feel free to reopen.", "@yashk2810 You didn't get it. The point is that not all code should be under the strategy scope.\r\n\r\nFor example, look at this code in the tutorial:\r\n\r\n```python\r\nwith strategy.scope():\r\n  def train_step(inputs):\r\n    images, labels = inputs\r\n\r\n    with tf.GradientTape() as tape:\r\n      predictions = model(images, training=True)\r\n      loss = compute_loss(labels, predictions)\r\n\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n    train_accuracy.update_state(labels, predictions)\r\n    return loss \r\n\r\n  def test_step(inputs):\r\n    images, labels = inputs\r\n\r\n    predictions = model(images, training=False)\r\n    t_loss = loss_object(labels, predictions)\r\n\r\n    test_loss.update_state(t_loss)\r\n    test_accuracy.update_state(labels, predictions)\r\n```\r\n\r\nDo `def train_step()` and `def test_step()` have to be inside the scope? I don't think so.\r\n\r\nAnother example:\r\n\r\n```python\r\nwith strategy.scope():\r\n  # `experimental_run_v2` replicates the provided computation and runs it\r\n  # with the distributed input.\r\n  @tf.function\r\n  def distributed_train_step(dataset_inputs):\r\n    per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                      args=(dataset_inputs,))\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                           axis=None)\r\n \r\n  @tf.function\r\n  def distributed_test_step(dataset_inputs):\r\n    return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\r\n\r\n  for epoch in range(EPOCHS):\r\n    # TRAIN LOOP\r\n    total_loss = 0.0\r\n    num_batches = 0\r\n    for x in train_dist_dataset:\r\n      total_loss += distributed_train_step(x)\r\n      num_batches += 1\r\n    train_loss = total_loss / num_batches\r\n\r\n    # TEST LOOP\r\n    for x in test_dist_dataset:\r\n      distributed_test_step(x)\r\n\r\n    if epoch % 2 == 0:\r\n      checkpoint.save(checkpoint_prefix)\r\n\r\n    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\r\n                \"Test Accuracy: {}\")\r\n    print (template.format(epoch+1, train_loss,\r\n                           train_accuracy.result()*100, test_loss.result(),\r\n                           test_accuracy.result()*100))\r\n\r\n    test_loss.reset_states()\r\n    train_accuracy.reset_states()\r\n    test_accuracy.reset_states()\r\n```\r\n\r\nI don't see why the scope is necessary here.\r\n\r\nThe rest of the tutorial contains a few more examples like this. My apologies if I said something incorrect."]}, {"number": 33763, "title": "Better (simpler!) model checkpointing", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current `tf.train.Checkpoint` is really a bit too complicated and not easy to use. Suppose that I want to save **the best model** (that has the best validation accuracy) during my **custom training loop**, and that I want to save **only the best model because I don't want to waste disk space**. My common sense tells me that this is so simple, just like in all other libraries:\r\n\r\n```\r\nif best_acc < epoch_acc:\r\n    best_acc = epoch_acc\r\n    checkpoint.save(os.path.join(checkpoint_dir, 'best'))\r\n```\r\n\r\nResults after 3 epochs:\r\n```\r\nbest-1.index\r\nbest-1.data-00001-of-00002\r\nbest-1.data-00000-of-00002\r\nbest-2.index\r\nbest-2.data-00001-of-00002\r\nbest-2.data-00000-of-00002\r\nbest-3.index\r\nbest-3.data-00001-of-00002\r\nbest-3.data-00000-of-00002\r\n```\r\nOops... My disk does not like this! And there's no option to turn off the counter and choose the exact file names that I want!\r\n\r\nSame problem for **saving only the last checkpoint**. You would tell me that one can use `tf.train.CheckpointManager` for this task (by setting `max_to_keep = 1`), but, **a checkpoint object and then a checkpoint manager object to manage checkpoints, seriously?** Yet another additional layer of complexity! (Not to say that `CheckpointManager` cannot deal with the best model saving described above).\r\n\r\nI think one of the major limitations of TensorFlow was complexity, really. In TensorFlow 2 this has been significantly reduced, but I believe we can do more.\r\n\r\nIt would be best to give the users the simplest option to `save(filepath)` and `restore(filepath)` in a **single file** (instead of *.index plus a bunch of data files), with whatever `filepath` they want. Just like in any other libraries such as PyTorch or Keras! (Keras's usage cannot be applied here because the model may not be compiled in a custom training loop). \r\n\r\nFinally: my apologies if such a solution already exists and I am not aware of that because of my lack of knowledge! (I have wrestled with `tf.train.Checkpoint` over the last days, and have checked the official [guide](https://www.tensorflow.org/guide) and [tutorials](https://www.tensorflow.org/tutorials) but could not find anything like that.)\r\n\r\nI am just a TF beginner but will try to contribute, if you are open to changes. Thanks.\r\n\r\n**Will this change the current api? How?**\r\nI guess. I don't know how exactly, though.\r\n\r\n**Who will benefit with this feature?**\r\nEvery user who wants to manually save checkpoints.", "comments": ["FYI this is a hack that I did to achieve what I want, which may be useful to others:\r\n\r\n```\r\nclass CheckpointWrapper(object):\r\n    def __init__(self, **kargs):\r\n        self.checkpoint = tf.train.Checkpoint(**kargs)\r\n\r\n    def save(self, prefix, counter=None):\r\n        \"\"\"\r\n        NOTE: this wrapper assumes that no two checkpoints have the same prefix,\r\n        therefore, this function will delete existing checkpoints with the\r\n        same prefix before saving (here prefix acts like a file path).\r\n        counter: save with this counter instead of the value in save_counter\r\n        \"\"\"\r\n        save_dir = os.path.dirname(prefix)\r\n        if not os.path.isdir(save_dir):\r\n            os.makedirs(save_dir)\r\n\r\n        # save to temporary files first, to avoid overwriting existing files too early\r\n        random_string = '_' + str(random.randint(1e6, 1e7)) + '_'\r\n        temp_prefix = prefix + random_string\r\n        if counter is not None:\r\n            stored_counter = self.checkpoint.save_counter\r\n            self.checkpoint.save_counter.assign(counter - 1)\r\n        self.checkpoint.save(temp_prefix)\r\n        # restore the original counter (incremented by 1 as we have saved one)\r\n        if counter is not None:\r\n            self.checkpoint.save_counter.assign(stored_counter + 1)\r\n\r\n        # success. \"Overwrite\" (well: delete) previous checkpoint with same prefix\r\n        prfx = os.path.basename(prefix)\r\n        # search and delete all files prefix-<integer>.index, prefix-<integer>.data-<integer>-of-<integer>\r\n        pattern = re.compile(r\"{}-\\d+\\.index|{}-\\d+\\.data-\\d+-of-\\d+\".format(prfx, prfx))\r\n        for fname in os.listdir(save_dir):\r\n            if pattern.match(fname):\r\n                os.remove(os.path.join(save_dir, fname))\r\n        # rename the saved checkpoint\r\n        for fname in os.listdir(save_dir):\r\n            os.rename(os.path.join(save_dir, fname),\r\n                      os.path.join(save_dir, fname.replace(random_string, '')))\r\n\r\n    def restore(self, prefix):\r\n        \"\"\"\r\n        restore from the last checkpoint with given prefix\r\n        \"\"\"\r\n        save_dir = os.path.dirname(prefix)\r\n        prfx = os.path.basename(prefix)\r\n        pattern = re.compile(r\"{}-\\d+\\.index\".format(prfx))\r\n        restore_prefix = None\r\n        for fname in os.listdir(save_dir):\r\n            if pattern.match(fname):\r\n                restore_prefix = os.path.join(save_dir, os.path.splitext(fname)[0])\r\n                break\r\n        self.checkpoint.restore(restore_prefix)\r\n        return restore_prefix\r\n```", "@netw0rkf10w I think your issue is similar to [this resource](https://github.com/tensorflow/tensorflow/issues/30695#issuecomment-511932452).\r\nPlease follow the resolution provided in that resource and let us know if you are looking for anything different. Thanks!\r\n\r\n", "Hi @jvishnuvardhan. Thanks for your reply. Unfortunately that resource doest not help because it is for Keras `.fit()` only (I believe). My issue is specifically for custom training loop with GradientTape.\r\n\r\nI also mentioned this in the post:\r\n\r\n> It would be best to give the users the simplest option to save(filepath) and restore(filepath) in a single file (instead of *.index plus a bunch of data files), with whatever filepath they want. Just like in any other libraries such as PyTorch or **Keras**! (**Keras's usage cannot be applied here because the model may not be compiled in a custom training loop**).", "You can use `tf.train.Checkpoint.write` if you want a no-frills API that just saves to a given prefix.", "@allenlavoie I obtained the following error when restoring a checkpoint in a distributed training:\r\n\r\n```\r\nFile \"/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:GPU:1. The edge src node is Func/while/body/_1/input/_2741 , and the dst node is while/body/_1/Adam/Adam/update/update_1/ResourceApplyAdam [Op:__inference_distributed_train_epoch_31342]\r\n```\r\n\r\nShould I create a new issue? Thanks.", "Probably the optimizer's slot variable creation has moved to where the primary variable is created rather than happening during apply_gradients. That shouldn't generally affect placement, but looks like it's breaking something in your code.\r\n\r\nAnyway, it's definitely not this issue. Feel free to open another one."]}, {"number": 33762, "title": "Cannot Get Older Versions to Build", "body": "**System information**\r\n- Arch Linux\r\n- TensorFlow installed from source\r\n- TensorFlow version: 1.13.0\r\n- Python version: 3.7\r\n- Not Installed using virtualenv, pip or conda\r\n- Bazel version: 0.25.3\r\n- GCC/Compiler version: 9.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: RS880M (Mobility Radeon HD 4225/4250) (256MB)\r\n\r\nI've been trying to build version 1.13.0 of tensorflow for the past two days. I need a build that was compiled with \"-mno-avx\" because my CPU does not support the AVX instruction set and tensorflow crashes when I run it. I need tensorflow for a project I'm trying to install.\r\n\r\nI read that new versions of glibc conflict with the source code and that the source code must be patched to build with as newer version of glibc. I am getting errors about \"gettid()\" being ambiguous. I found reference to someone patching the source code and changing gettid() to sys_gettid(), but I cannot find the patch that was used.\r\n\r\nI also tried changing the function calls manually in the ~/.cache/bazel directory but Bazel will not compile with the cached source code. It seems to redownload the source code before every build. \r\n\r\nThe command that I used to start the build:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nError:\r\n```\r\nERROR: /home/***/.cache/bazel/_bazel_***/06b70ca2a990d8a0456251480c3991dd/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\n                 external/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n43 | static long gettid(void) { return syscall(__NR_gettid); }        |             ^~~~~~                                         \r\nIn file included from /usr/include/unistd.h:1170,                                   from external/grpc/src/core/lib/gpr/log_linux.cc:41:                                                                 \r\n/usr/include/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'                                                           \r\n34 | extern __pid_t gettid (void) __THROW;                            |                ^~~~~~                                      \r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]                 \r\n43 | static long gettid(void) { return syscall(__NR_gettid); }        |             ^~~~~~                                         \r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build                                                               \r\nUse --verbose_failures to see the command lines of failed build steps.                                                                INFO: Elapsed time: 14263.127s, Critical Path: 457.22s\r\nINFO: 4301 processes: 4301 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n![Screenshot_20191027-114439](https://user-images.githubusercontent.com/19521547/67638409-11eb2100-f8b2-11e9-8767-aed639721262.png)\r\n", "comments": ["I got 1.13.1 to compile by setting up a local_repository for grpc in the WORKSPACE file for tensorflow and changing all the gettid function names to sys_gettid in the local_repository.\r\n\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 12803.103s, Critical Path: 439.59s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 4414 processes: 4414 local.\r\nINFO: Build completed successfully, 4757 total actions\r\n```\r\n\r\nI downloaded the tarball for the version of grpc that the version of tensorflow that I'm compiling uses. I extracted the tarball to ~/grpc and manually changed gettid to sys_gettid in the source code. It was only three or four edits. \r\n\r\nI set up a local repository of grpc and added it to the tensorflow WORKSPACE file as a local_repository. I commented out the grpc repository in the tensorflow/workspace.bzl file.\r\n\r\nWORKSPACE: (added to bottom of file)\r\n```\r\nlocal_repository(\r\n    name = \"grpc\",                                                     \r\n    path = \"/home/loophole/grpc\",\r\n) \r\n```\r\ntensorflow/workspace.bzl: (I had to change the sha256 hash and I removed the mirror.bazel.build mirror. That may not be necessary but I downloaded the file from the python.org mirror used the sha256 hash of that file, but I didn't check the hash of the one on bazel.build.)\r\n\r\nEDIT: I'm having some weird formatting issue. Change the sha256 value to \"fcaec9796c8cc3618899b4aeb62d1a4741830b682b2d8db502a05f9b93c08937\" and comment out or delete the bazel.build mirror in \"filegroup_external\" with the name of \"org_python_license\"\r\n\r\nThese are the files that needed to have gettid changed to sys_gettid. First I used grep to see which files had gettid in them, then I used nano and pressed ctrl+w to search for gettid:\r\n```\r\n./src/core/lib/iomgr/ev_epollex_linux.cc\r\n./grpc/src/core/lib/gpr/log_posix.cc\r\n./grpc/src/core/lib/gpr/log_linux.cc\r\n```\r\n\r\nMake sure you get the version of Bazel that was tested with the version of tensorflow you want to compile. There's a table at the bottom of this page: https://www.tensorflow.org/install/source\r\n\r\nI used this command to remove the previous version of Bazel:\r\n```\r\nrm -rf ~/.bazel ~/.bazelrc ~/.cache/bazel ~/bin/bazel\r\n```\r\nI installed Bazel with --user, so if you installed without that flag then you may also need to remove: (before installing another version of Bazel)\r\n```\r\n/usr/local/bin/bazel\r\n```", "@l0ophole ,\r\nHello,looks like the issue is resolved. kindly let us know if we can close the issue.Thanks!", "@oanush well I was able to get it to build but it still crashes except instead of saying \"Illegal instruction\" it says \"Segmentation fault\"", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33762\">No</a>\n"]}, {"number": 33761, "title": "Issue #26211 updated docs", "body": "Updated docs for tf.keras.activations.relu issue #26211. ", "comments": []}, {"number": 33760, "title": "if branch didn't do anything", "body": "if-else is not necessary, if branch didn't do anything in the code.", "comments": ["We will not be encouraging one liner changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac "]}, {"number": 33758, "title": "Update grpc dependency for glibc 2.30 compatibility", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Arch Linux**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/A**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version: **2.0.0**\r\n- Python version: **3.7.4**\r\n- Installed using virtualenv? pip? conda?: **n/A**\r\n- Bazel version (if compiling from source): **bazel 0.29.1- (@non-git)**\r\n- GCC/Compiler version (if compiling from source): **gcc-8 (GCC) 8.3.0**\r\n- CUDA/cuDNN version: **10.1.243-1**/**7.6.4.38-1**\r\n- GPU model and memory: **NVIDIA GeForce GTX 760 4GB**\r\n\r\n**Describe the problem**\r\n\r\nWhen building tensorflow 2.0.0 on a system with `glibc` version 2.30, the build fails due to a function name clash issue in grpc, which is already fixed (in https://github.com/grpc/grpc/pull/18950) and there are grpc releases available with this fix. I believe updating the grpc dependency should fix this issue in Tensorflow.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nOn Arch Linux:\r\n```\r\ncd $(mktemp -d /tmp/tensorflow-test-build-XXX)\r\ncurl -L -o PKGBUILD 'https://git.archlinux.org/svntogit/community.git/plain/trunk/PKGBUILD?h=packages/tensorflow'\r\nmakepkg\r\n```\r\n\r\nIf you're not on Arch Linux, have a look at the build script in the [PKGBUILD](https://git.archlinux.org/svntogit/community.git/tree/trunk/PKGBUILD?h=packages/tensorflow) file - it contains all the environment variable definitions and build commands, and is very readable.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nEnd of build log:\r\n\r\n```\r\n...\r\nINFO: From Compiling external/llvm/lib/DebugInfo/CodeView/TypeRecordMapping.cpp:\r\nexternal/llvm/lib/DebugInfo/CodeView/TypeRecordMapping.cpp: In member function 'virtual llvm::Error llvm::codeview::TypeRecordMapping::visitKnownRecord(llvm::codeview::CVType&, llvm::codeview::VFTableShapeRecord&)':\r\nexternal/llvm/lib/DebugInfo/CodeView/TypeRecordMapping.cpp:293:61: warning: 'Byte' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n  293 |         Record.Slots.push_back(static_cast<VFTableSlotKind>(Byte >> 4));\r\n      |                                                             ^~~~\r\nERROR: /tmp/bazel/michiel/output/41c10338046435fcb3c7d7f27ec34951/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n   43 | static long gettid(void) { return syscall(__NR_gettid); }\r\n      |             ^~~~~~\r\nIn file included from /usr/include/unistd.h:1170,\r\n                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:\r\n/usr/include/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'\r\n   34 | extern __pid_t gettid (void) __THROW;\r\n      |                ^~~~~~\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]\r\n   43 | static long gettid(void) { return syscall(__NR_gettid); }\r\n      |             ^~~~~~\r\nINFO: Elapsed time: 744.187s, Critical Path: 41.65s\r\nINFO: 3096 processes: 3096 local.\r\nFAILED: Build did NOT complete successfully\r\n==> ERROR: A failure occurred in build().\r\n    Aborting...\r\n```\r\n\r\n**What I've tried so far to fix this**\r\nI tried to emulate the grpc version update from https://github.com/tensorflow/tensorflow/commit/061c3597b84d45a9878b8adf831e39a5573859ec and bump grpc to v1.24.3. Since v1.19.x, which the currently referenced grpc version seems to be from, grpc has added a dependency on https://github.com/protocolbuffers/upb, and it wasn't clear to me how to add and initialise this dependency correctly in a way that's consistent with tensorflow's use of bazel. I specifically wasn't sure where to call `grpc_deps()`/`upb_deps()` from, or what equivalent action was required instead.\r\n\r\n**Context**\r\nIn case it matters, I'm trying to build the Arch Linux Package from source, so that I can add 3.0 to `TF_CUDA_COMPUTE_CAPABILITIES`, which is required for my graphics card. I managed to do this for an earlier version of tensorflow some time ago without too much difficulty.", "comments": ["I'm having this exact issue. I've been trying to build version 1.13.0 for the past two days.\r\n\r\nI am trying to build version 1.13.0 with the compile flag \"-mno-avx\" because my CPU doesn't support the AVX instruction set. I keep getting errors regarding gettid() being ambiguous. I found the post about the patch to change the function names but I couldn't actually find the patch file that he used.\r\n\r\nI also changed the function names by hand in the ~/. cache/bazel directory but Bazel will not compile the source code that is in it's cache. It redownloads it before starting every build. ", "I may have found a workaround. I downloaded the tarball for the version of grpc that the version of tensorflow that I'm compiling uses. I extracted the tarball to ~/grpc and manually changed gettid to sys_gettid in the source code. It was only three or four edits. \r\n\r\nI set up a local repository of grpc and added it to the tensorflow WORKSPACE file as a local_repository. I commented out the grpc repository in the tensorflow/workspace.bzl file.\r\n\r\nWORKSPACE: (added to bottom of file)\r\n```\r\nlocal_repository(\r\n    name = \"grpc\",                                                     \r\n    path = \"/home/loophole/grpc\",\r\n) \r\n```\r\ntensorflow/workspace.bzl: (I had to change the sha256 hash and I removed the mirror.bazel.build mirror. That may not be necessary but I downloaded the file from the python.org mirror used the sha256 hash of that file, but I didn't check the hash of the one on bazel.build.)\r\n\r\nEDIT: I'm having some weird formatting issue. Change the sha256 value to \"fcaec9796c8cc3618899b4aeb62d1a4741830b682b2d8db502a05f9b93c08937\" and comment out or delete the bazel.build mirror in \"filegroup_external\" with the name of \"org_python_license\"\r\n\r\nThese are the files that needed to have gettid changed to sys_gettid. First I used grep to see which files had gettid in them, then I used nano and pressed ctrl+w to search for gettid:\r\n```\r\n./src/core/lib/iomgr/ev_epollex_linux.cc\r\n./grpc/src/core/lib/gpr/log_posix.cc\r\n./grpc/src/core/lib/gpr/log_linux.cc\r\n```\r\n\r\nIt's still compiling, so I don't know if it's going to work yet.\r\n\r\nMake sure you get the version of Bazel that was tested with the version of tensorflow you want to compile. There's a table at the bottom of this page: https://www.tensorflow.org/install/source\r\n\r\nI used this command to remove the previous version of Bazel:\r\n```\r\nrm -rf ~/.bazel ~/.bazelrc ~/.cache/bazel ~/bin/bazel\r\n```\r\nI installed Bazel with --user, so if you installed without that flag then you may also need to remove: (before installing another version of Bazel)\r\n```\r\n/usr/local/bin/bazel\r\n```", "```\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 12803.103s, Critical Path: 439.59s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 4414 processes: 4414 local.\r\nINFO: Build completed successfully, 4757 total actions\r\n```", "Ah, that's a good workaround approach @l0ophole. There's another way to do that - letting `bazel` do the patching for us:\r\n\r\n1. Create `.patch` file with the `gettid` rename changes using your method, or:\r\n```\r\ngit clone https://github.com/grpc/grpc.git && cd grpc\r\ngit checkout 4566c2a29ebec0835643b972eb99f4306c4234a3\r\ngit cherry-pick 57586a1ca7f17b1916aed3dea4ff8de872dbf853\r\n# resolve conflicts\r\ngit diff [--cached] > /path/to/tensorflow-2.0.0/third_party/grpc/gettid.patch\r\n```\r\n2. Make the following change to `tensorflow/workspace.bzl`:\r\n```\r\n--- a/tensorflow/workspace.bzl\t2019-09-27 22:56:33.000000000 +0100\r\n+++ b/tensorflow/workspace.bzl\t2019-10-28 19:19:32.441547370 +0000\r\n@@ -519,6 +519,7 @@\r\n             \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz\",\r\n             \"https://github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz\",\r\n         ],\r\n+        patch_file = clean_dep(\"//third_party/grpc:gettid.patch\"),\r\n     )\r\n \r\n     tf_http_archive(\r\n```\r\n3. build.\r\n\r\n(I do think updating grpc would be cleaner)", "Can you send a PR with this approach please?\r\n\r\nOr, do you consider it worthwhile to upgrade grpc?", "I would certainly prefer the grpc upgrade: currently tensorflow depends on a specific commit of grpc. This would be a good opportunity to change that dependency to a released version tag. Changing it to a dependency on a specific commit of grpc + patch feels like going in the wrong direction, unless there's a really good justification I'm not aware of - if ever had to file a bug against grpc I'd much rather do that against a released version than against effectively a custom branch.\r\n\r\nThere's the following note above the grpc dependency in `workspace.bzl`:\r\n```\r\n# WARNING: make sure ncteisen@ and vpai@ are cc-ed on any CL to change the below rule\r\n```\r\nPerhaps they know more (I'm not sure if those are github user ids).\r\n(Apologies if your comment wasn't aimed at me)", "@m01 thanks for the alternate approach. I had never used bazel before trying to compile this project so I'm sure there are much better ways of patching than the way I did it. :-)\r\n\r\nrecent versions of grpc aren't affected by this issue, are they? I figured I was having issues because I was trying to compile an old version of tensorflow that had a dependency on an old version of grpc. I thought the gettid patch was already merged into the repository. I was compiling a tarball for an old version from the releases page.", "I'll attempt a grpc update this week. It might take a while though.", "I'm also on Arch and had the same problem when trying to build jax (which uses xla bits of tensorflow), but following @m01's approach, I managed to build it, thanks a lot!\r\n\r\nJust in case other people want a quick temporary measure before @mihaimaruseac completes grpc update, I made a patch here https://github.com/tensorflow/tensorflow/compare/master...hi-ogawa:grpc-backport-pr-18950.\r\n\r\nUsing GitHub's \".patch\" magic url, probably something like this would work:\r\n\r\n```\r\ncurl -L https://github.com/tensorflow/tensorflow/compare/master...hi-ogawa:grpc-backport-pr-18950.patch | git apply\r\n```\r\n", "> I'll attempt a grpc update this week. It might take a while though.\r\n\r\nHi @mihaimaruseac, just curious if there is any update on this? I think as of today's `master` branch, I still run into this issue. Thanks!", "@abcdabcd987\r\n\r\nI was able to build tf 1.14.0 with python 3.8 by modifying the PKGBUILD in the tensorflow114 AUR package and adding a couple patches:\r\n\r\ntensorflow114/PKGBUILD\r\nhttps://pastebin.com/kQue1cps\r\n\r\ntensorflow114/tensorflow-1.14-python3.8.diff\r\nhttps://pastebin.com/MLrrGXKD\r\n\r\ntensorflow114/src/Add-grpc-fix-for-gettid.patch\r\nhttps://pastebin.com/XDvwPzdL\r\n", "There was an attempt in 8497ae4f2810fc7af4a14c6ea433a98fd275a01d but that got rolled back.\r\n\r\nIt doesn't look likely that we can pin to a release. Let me try it again today", "It seems we cannot upgrade this yet as we need work to support upb (micro protocol buffers)", "@mihaimaruseac Thanks for the effort. If upgrade is not an option now, could you add the `gettid` patch into the bazel workspace?", "The one mentioned at https://github.com/tensorflow/tensorflow/issues/33758#issuecomment-547143723, right?", "Right.", "I will attempt another grpc update using a different direction for patching, but it will take a few days.", "@hi-ogawa thanks for the oneliner. @mihaimaruseac maybe we can merge that while you update to latest grpc ? ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33758\">No</a>\n", "Apologies for the long delay. We got this fixed now, I think (not my work, but of a concurrent contributor)", "I ran into this as well, @hi-ogawa's workaround worked. Which commit fixes it in the mainline tree?", "![s5po5u1qiYp](https://user-images.githubusercontent.com/323199/76004198-f9bd8b00-5ebd-11ea-852d-70635f24a432.png)\r\n", "Ah, good to know, thanks!"]}, {"number": 33757, "title": "Inclusion of a model re-training example", "body": "## URL(s) with the issue: [https://www.tensorflow.org/tutorials/keras/save_and_load](https://www.tensorflow.org/tutorials/keras/save_and_load)\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load\r\n\r\n## Description of issue (what needs changing):\r\n\r\nSince model re-training is quite vital in both applied and research-based environments, I think it would be great to include an example on the same in this tutorial. \r\n\r\n### Clear description\r\n\r\nThe tutorial shows how to save and load models using various options. It does mention that using the model checkpoints it is possible to train the model from the point it was left off. However, currently, there is no section in the tutorial that shows how to do that in the correct way. \r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\nThere are several instances where a model may have to be retrained:\r\n- There is new data and the model needs to re-trained on that\r\n- If we are on local machines and if there is a power failure or bottlenecks that cause the training process to stop, we can always load up the latest checkpoints and re-train the models from there. ", "comments": ["Hi @sayakpaul, late response here, but does the[ Save checkpoints during training](https://www.tensorflow.org/tutorials/keras/save_and_load#save_checkpoints_during_training) section of the guide provide adequate information?"]}, {"number": 33756, "title": "Docs do not link to source", "body": "One really useful feature in the sklearn docs is that each and every item has a link back to its source on GitHub. For example, see [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html):\r\n\r\n> class sklearn.ensemble.RandomForestRegressor(n_estimators=\u2019warn\u2019, criterion=\u2019mse\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;--> **[[source]](https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/ensemble/forest.py#L1046)** <--\r\n\r\nWould be nice if the TF docs could do the same.", "comments": ["Every TF API is now doing that. There is a big Github button at the top :)\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/concat", "I noticed. Thanks a lot! \ud83d\udc4d"]}, {"number": 33754, "title": "[BUG] The gradient computed for bias in tf.contrib.slim.conv2d + tf.contrib.slim.batch_norm and tf.contrib.slim.conv2d(normalizer=tf.contrib.slim.batch_norm) are different!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow version (use command below): tensorflow==1.15.0rc2\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1/7.5\r\n- GPU model and memory: RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nThe gradient computed for bias in tf.contrib.slim.conv2d + tf.contrib.slim.batch_norm and tf.contrib.slim.conv2d(normalizer=tf.contrib.slim.batch_norm) are different!\r\n\r\n**Describe the expected behavior**\r\nThe gradients should be the same. Right now, it seems like `tf.contrib.slim.conv2d + tf.contrib.slim.batch_norm` gives the correct gradient.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nimport numpy as np\r\n\r\n# tf.enable_eager_execution()\r\n\r\nprint(\"GRAPH 1\")\r\ngraph_1 = tf.Graph()\r\nwith graph_1.as_default():\r\n    input_placeholder_1 = tf.placeholder(tf.float32, shape=([1, 10, 10, 3]))\r\n    target_placeholder_1 = tf.placeholder(tf.float32, shape=([1, 10, 10, 1]))\r\n\r\n    input_tensor_1 = tf.pad(input_placeholder_1, [[0, 0], [1, 1], [1, 1], [0, 0]])\r\n    # forward_op = slim.conv2d(inputs=input_tensor, num_outputs=1, kernel_size=3, stride=1, padding=\"VALID\", normalizer_fn=slim.batch_norm, normalizer_params={\"center\": True, \"scale\": True}, weights_initializer=tf.constant_initializer(value=0.0), biases_initializer=None)\r\n    forward_op_1 = slim.conv2d(inputs=input_tensor_1, num_outputs=1, kernel_size=3, stride=1, padding=\"VALID\", normalizer_fn=None, weights_initializer=tf.constant_initializer(value=0), biases_initializer=None)\r\n    forward_op_1 = slim.batch_norm(forward_op_1, scale=True)\r\n    forward_op_1 = tf.sigmoid(forward_op_1)\r\n\r\n    loss_1 = tf.losses.sigmoid_cross_entropy(target_placeholder_1, forward_op_1, reduction=\"weighted_sum\")\r\n    backward_op_1 = tf.gradients(loss_1, tf.trainable_variables())\r\n    sess_1 = tf.Session(graph=graph_1)\r\n    sess_1.run(tf.global_variables_initializer())\r\n    train_vars_1 = sess_1.run(tf.trainable_variables())\r\n    for train_var_1 in train_vars_1:\r\n        print(train_var_1)\r\n\r\nprint(\"GRAPH 2\")\r\ngraph_2 = tf.Graph()\r\nwith graph_2.as_default():\r\n    input_placeholder_2 = tf.placeholder(tf.float32, shape=([1, 10, 10, 3]))\r\n    target_placeholder_2 = tf.placeholder(tf.float32, shape=([1, 10, 10, 1]))\r\n\r\n    input_tensor_2 = tf.pad(input_placeholder_2, [[0, 0], [1, 1], [1, 1], [0, 0]])\r\n    forward_op_2 = slim.conv2d(inputs=input_tensor_2, num_outputs=1, kernel_size=3, stride=1, padding=\"VALID\", normalizer_fn=slim.batch_norm, normalizer_params={\"center\": True, \"scale\": True}, weights_initializer=tf.constant_initializer(value=0.0), biases_initializer=None)\r\n    # forward_op_2 = slim.conv2d(inputs=input_tensor_2, num_outputs=1, kernel_size=3, stride=1, padding=\"VALID\", normalizer_fn=None, weights_initializer=tf.constant_initializer(value=0), biases_initializer=tf.constant_initializer(value=0))\r\n    # forward_op_2 = slim.batch_norm(forward_op_2, scale=True)\r\n    forward_op_2 = tf.sigmoid(forward_op_2)\r\n\r\n    loss_2 = tf.losses.sigmoid_cross_entropy(target_placeholder_2, forward_op_2, reduction=\"weighted_sum\")\r\n    backward_op_2 = tf.gradients(loss_2, tf.trainable_variables())\r\n    sess_2 = tf.Session(graph=graph_2)\r\n    sess_2.run(tf.global_variables_initializer())\r\n    train_vars_2 = sess_2.run(tf.trainable_variables())\r\n    for train_var_2 in train_vars_2:\r\n        print(train_var_2)\r\n\r\nfor i in range(10):\r\n    print(\"RUN \", i)\r\n    input_ = np.random.randn(1, 10, 10, 3)\r\n    target = np.random.randint(2, size=(1, 10, 10, 1))\r\n    grad_1 = sess_1.run(backward_op_1, feed_dict={input_placeholder_1: input_, target_placeholder_1: target})\r\n    print(len(grad_1))\r\n    print(grad_1)\r\n    grad_2 = sess_2.run(backward_op_2, feed_dict={input_placeholder_2: input_, target_placeholder_2: target})\r\n    print(len(grad_2))\r\n    print(grad_2)\r\n```\r\n**logs**\r\n```\r\nGRAPH 1\r\n[[[[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]]\r\n\r\n\r\n [[[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]]\r\n\r\n\r\n [[[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]]]\r\n[1.]\r\n[0.]\r\nGRAPH 2\r\n[[[[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]]\r\n\r\n\r\n [[[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]]\r\n\r\n\r\n [[[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]\r\n\r\n  [[0.]\r\n   [0.]\r\n   [0.]]]]\r\n[1.]\r\n[0.]\r\nRUN  0\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([3.8114848], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  1\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([1.3114842], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  2\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([1.3114835], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  3\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([4.811485], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  4\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.8114846], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  5\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([4.3114843], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  6\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([1.8114848], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  7\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.06148279], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  8\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([3.0614848], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\nRUN  9\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([4.0614843], dtype=float32)]]\r\n1\r\n[[array([[[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]],\r\n\r\n\r\n       [[[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]],\r\n\r\n        [[0.],\r\n         [0.],\r\n         [0.]]]], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]]\r\n```\r\n\r\n**Other info **\r\nI understand that tf.contrib.slim is no longer being maintained but I just want to know if it is really a bug or a misuse. Right now, based on my minimal testing code, it seems like a bug.\r\n", "comments": []}, {"number": 33753, "title": "Support tensor arguments in clip.", "body": "Adding support for tensor arguments in clip, to align with the [semantics](https://github.com/keras-team/keras/pull/11442/files) of `backend.clip` in stand-alone Keras. See #24827.", "comments": []}, {"number": 33752, "title": "Invalid loop structure: Loop \"Preprocessor/map/while/while_context\"", "body": "I'm trying to map the output of one graph to the input of another but I'm getting following error while trying to do so. I'm using `export_meta_graph` and `import_scoped_meta_graph` with `input_map`. Following is my code :\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport cv2\r\n\r\ngrid_shape = (5,5)\r\nimage_shape=(150, 150)\r\nnum_channels=3\r\n\r\nimg = cv2.imread(\"/Users/vedanshu/test.jpeg\")\r\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n\r\ndef get_frozen_graph(graph_file):\r\n    \"\"\"Read Frozen Graph file from disk.\"\"\"\r\n    with tf.gfile.GFile(graph_file, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    return graph_def\r\n\r\n# Import frozen graph for level 1\r\npb_fname1 = \"/Users/vedanshu/frozen_graph/ssd_tomato_l1_frozen_graph.pb\"\r\ntrt_graph1 = get_frozen_graph(pb_fname1)\r\ndetection_graph1 = tf.Graph()\r\nwith detection_graph1.as_default():\r\n    tf.import_graph_def(trt_graph1, name='')\r\n    tf_sess1 = tf.Session(graph=detection_graph1)\r\n\r\n# Import frozen graph for level 2\r\npb_fname2 = \"/Users/vedanshu/frozen_graph/faster_rcnn_tomato_l2_frozen_graph.pb\"\r\ntrt_graph2 = get_frozen_graph(pb_fname2)\r\ndetection_graph2 = tf.Graph()\r\nwith detection_graph2.as_default():\r\n    tf.import_graph_def(trt_graph2, name='')\r\n    tf_sess2 = tf.Session(graph=detection_graph2)\r\n\r\nfrom tensorflow.python.framework import meta_graph\r\n\r\ngraph = tf.get_default_graph()\r\ntf_sess_main = tf.Session(graph=graph)\r\n\r\n# Importing Level 1 graph to the default graph with namespace of ved_graph1\r\nmeta_graph1 = tf.train.export_meta_graph(graph=detection_graph1)\r\nmeta_graph.import_scoped_meta_graph(meta_graph1, import_scope='ved_graph1',)\r\n\r\ntf_input1 = graph.get_tensor_by_name('ved_graph1/image_tensor:0')\r\ntf_scores1 = graph.get_tensor_by_name('ved_graph1/detection_scores:0')\r\ntf_boxes1 = graph.get_tensor_by_name('ved_graph1/detection_boxes:0')\r\ntf_classes1 = graph.get_tensor_by_name('ved_graph1/detection_classes:0')\r\ntf_num_detections1 = graph.get_tensor_by_name('ved_graph1/num_detections:0')\r\n\r\nboxes1 = tf_boxes1[0]  # index by 0 to remove batch dimension\r\nscores1 = tf_scores1[0]\r\nclasses1 = tf_classes1[0]\r\nnum_detections1 = tf.dtypes.cast(tf_num_detections1[0], tf.int32)\r\n\r\ndef image_grid(input_tensor, grid_shape=(5,5), image_shape=(150, 150), num_channels=3):\r\n    # https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/contrib/gan/python/eval/python/eval_utils_impl.py#L34\r\n    height, width = grid_shape[0] * image_shape[0], grid_shape[1] * image_shape[1]\r\n    input_tensor = tf.reshape(\r\n      input_tensor, tuple(grid_shape) + tuple(image_shape) + (num_channels,))\r\n    input_tensor = tf.transpose(input_tensor, [0, 1, 3, 2, 4])\r\n    input_tensor = tf.reshape(\r\n      input_tensor, [grid_shape[0], width, image_shape[0], num_channels])\r\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\r\n    input_tensor = tf.reshape(\r\n      input_tensor, [height, width, num_channels])\r\n    return input_tensor\r\n\r\ndef get_grid_roies():\r\n    def condition1(i, boxes_pixels):\r\n        return tf.less(i, num_detections1)\r\n    def body1(i, boxes_pixels):\r\n        normalizer = [tf.shape(tf_input1[0])[0], tf.shape(tf_input1[0])[1], tf.shape(tf_input1[0])[0], tf.shape(tf_input1[0])[1]]\r\n        box = tf.multiply(boxes1[i], tf.dtypes.cast(normalizer, tf.float32))\r\n        box = tf.dtypes.cast(tf.round(box), tf.int32)\r\n        boxes_pixels = boxes_pixels.write(i, box)\r\n        return [tf.add(i, 1), boxes_pixels]\r\n\r\n    i = tf.constant(0)\r\n    boxes_pixels = tf.TensorArray(dtype=tf.int32,size=1,dynamic_size=True,clear_after_read=False)\r\n\r\n    _, boxes_pixels = tf.while_loop(condition1,body1,[i, boxes_pixels])\r\n    boxes_pixels = boxes_pixels.stack()\r\n\r\n    def condition2(j, boxes_pixels, roies):\r\n        return tf.less(j, tf.shape(boxes_pixels)[0])\r\n    def body2(j, boxes_pixels, roies):\r\n        startY =  boxes_pixels[j][0]\r\n        startX =  boxes_pixels[j][1]\r\n        endY =  boxes_pixels[j][2]\r\n        endX =  boxes_pixels[j][3]\r\n        roi = tf_input1[0, startY:endY, startX:endX] # batch: 0\r\n        roi = tf.image.resize_image_with_pad(roi,image_shape[0],image_shape[1])\r\n        roi = tf.dtypes.cast(roi, tf.uint8)\r\n        roies = roies.write(j, roi)\r\n        return [tf.add(j, 1), boxes_pixels, roies]\r\n\r\n    j = tf.constant(0)\r\n    roies = tf.TensorArray(dtype=tf.uint8,size=1,dynamic_size=True,clear_after_read=False,\r\n                           infer_shape=False)\r\n\r\n    _, _, roies = tf.while_loop(condition2,body2,[j, boxes_pixels, roies])\r\n\r\n    # Adding padding for making grid\r\n    roies = roies.stack()\r\n    zero_pad = tf.zeros([1,image_shape[0],image_shape[1],num_channels], tf.uint8)\r\n    _no_pad = tf.mod(tf.shape(roies)[0], tf.constant(grid_shape[0]*grid_shape[1]))\r\n    no_pad = tf.cond(tf.equal(_no_pad, tf.constant(0)), lambda: tf.constant(0), \r\n                     lambda: tf.subtract(tf.constant(grid_shape[0]*grid_shape[1]), _no_pad))\r\n    zero_pad = tf.tile(zero_pad, [no_pad,1,1,1])\r\n    roies = tf.concat([roies, zero_pad], axis=0)\r\n\r\n    # Creating batch of images of size grid_shape[0]*image_shape[0]\r\n    size = grid_shape[0]*grid_shape[1]\r\n    n_iter = tf.dtypes.cast(tf.divide(tf.shape(roies)[0], tf.constant(size)), tf.int32)\r\n    k = tf.constant(0)\r\n    grid_roies = tf.TensorArray(dtype=tf.uint8,size=1,dynamic_size=True,clear_after_read=False,infer_shape=False)\r\n\r\n    def condition3(k, grid_roies):\r\n        return tf.less(k, n_iter)\r\n\r\n    def body3(k, grid_roies):\r\n        grid_roi = image_grid(roies[size*k:size*(k+1)], grid_shape, image_shape, num_channels)\r\n        grid_roies = grid_roies.write(k, grid_roi)\r\n        return [tf.add(k, 1), grid_roies]\r\n\r\n    _, grid_roies = tf.while_loop(condition3, body3, [k, grid_roies])\r\n\r\n    grid_roies = grid_roies.stack() \r\n\r\n    return grid_roies\r\n\r\ndef create_empty_grid():\r\n    grid_roies = tf.zeros([1,image_shape[0]*grid_shape[0],image_shape[1]*grid_shape[1],num_channels], tf.uint8)\r\n    return grid_roies\r\n\r\n# Grid is filled along the column first\r\ngrid_roies = tf.cond(tf.equal(num_detections1, tf.constant(0)), create_empty_grid, get_grid_roies)\r\n\r\n# Importing Level 1 graph to the default graph with namespace of ved_graph2\r\nmeta_graph2 = tf.train.export_meta_graph(graph=detection_graph2)\r\nmeta_graph.import_scoped_meta_graph(meta_graph2, input_map={'image_tensor': grid_roies}, import_scope='ved_graph2',)\r\n\r\ntf_scores2 = graph.get_tensor_by_name('ved_graph2/detection_scores:0')\r\ntf_boxes2 = graph.get_tensor_by_name('ved_graph2/detection_boxes:0')\r\ntf_classes2 = graph.get_tensor_by_name('ved_graph2/detection_classes:0')\r\ntf_num_detections2 = graph.get_tensor_by_name('ved_graph2/num_detections:0')\r\n\r\nnum_detections2 = tf_sess_main.run(tf_num_detections2, feed_dict={tf_input1: img[None, ...]})\r\n```\r\n\r\nThe last line is throwing me following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nInvalidArgumentError: Invalid loop structure: Loop \"Preprocessor/map/while/while_context\" has more than one LoopCond node: {{node ved_graph2/Preprocessor/map/while/LoopCond}} and {{node ved_graph1/Preprocessor/map/while/LoopCond}}. This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-19-f4ccb146461b> in <module>\r\n----> 1 num_detections2 = tf_sess_main.run(tf_num_detections2, feed_dict={tf_input1: img[None, ...]})\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Invalid loop structure: Loop \"Preprocessor/map/while/while_context\" has more than one LoopCond node: node ved_graph2/Preprocessor/map/while/LoopCond (defined at <ipython-input-16-efb8693a303c>:3)  and node ved_graph1/Preprocessor/map/while/LoopCond (defined at <ipython-input-7-ce9e14bfefad>:8) . This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n```\r\n\r\nMy specifications are:\r\nOS : Mac 10.13.6\r\nTF version: 1.13.1", "comments": ["Thanks to the issue reported [here](https://github.com/tensorflow/tensorflow/issues/22162) and comment [here](https://github.com/tensorflow/tensorflow/issues/22162#issuecomment-428091121), I was able to fix this error. It does some renaming with the following code:\r\n\r\n```\r\ndef rename_frame_name(graphdef, suffix):\r\n    for n in graphdef.node:\r\n        if \"while\" in n.name:\r\n            if \"frame_name\" in n.attr:\r\n                n.attr[\"frame_name\"].s = str(n.attr[\"frame_name\"]).replace(\"while_context\",\r\n                                                                           \"while_context\" + suffix).encode('utf-8')\r\n\r\n```\r\n\r\nWas this fixed in recent versions of tensorflow? \r\n", "This is still an issue with recent TF version 1.15.0\r\nI will close this issue since you have found a workaround and we can track this bug on the other thread you mentioned. Thanks!", "This note is for is as a help to other people who may have face similar problem but may be in different context.\r\nI faced same error as pasted below \r\n\"\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid loop structure: Loop \"s: \"Preprocessor/map/while/while_contextgpu_id\"\r\n\" has more than one LoopCond node: {{node 0/Preprocessor/map/while/LoopCond}} and {{node 1/Preprocessor/map/while/LoopCond}}. This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n\"\r\nI was using Google Object detection API faster RCNN on TF v1.13.1 with RT 5.0 and CUDANN 7.5 cuda kit 9.0.\r\nmy intention was to load inference graph on multiple GPU and i started getting this error at runtime.\r\nI applied suggested solution of including below function\r\ndef rename_frame_name(graphdef, suffix):\r\n    for n in graphdef.node:\r\n        if \"while\" in n.name:\r\n            if \"frame_name\" in n.attr:\r\n                n.attr[\"frame_name\"].s = str(n.attr[\"frame_name\"]).replace(\"while_context\",\r\n                                                                           \"while_context\" + suffix).encode('utf-8')\r\n\r\n\r\nAfter using this patch just after graph def was build using loading PB file code was working smoothly.\r\n\r\nthanks for help."]}, {"number": 33751, "title": "Resize layers in model producing LookupError when computing gradients", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- TensorFlow version (use command below):2.0\r\n- Python version:3.6.8\r\n- CUDA/cuDNN version:10.0/6.7.0\r\n- GPU model and memory:GTX 1080ti\r\n\r\n**Describe the current behavior**\r\nCurrently, I'm getting `LookupError: gradient registry has no entry for ResizeBilinearGrad` when its computing tape.gradient(d_loss, D.trainable_variables) in the code below.\r\n\r\n**Describe the expected behavior**\r\nThe code should just run and update the parameters per usual.\r\n\r\n**Code to reproduce the issue**\r\n~~~~\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, ReLU, Lambda, MaxPool2D, BatchNormalization\r\nfrom tensorflow.keras import Model, Sequential\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport argparse\r\nimport os\r\nimport glob\r\nimport sys\r\nimport time\r\nsys.path.append('..')\r\n\r\n########################\r\n## MAKE YOUR NET HERE ##\r\n########################\r\ndef conv_mean_pool(inputs, filters, kernel_size):\r\n    INPUT = Input(shape=inputs.shape[1:])\r\n    x = Conv2D(filters, kernel_size, padding='same')(INPUT)\r\n    OUTPUT= (x[::2, ::2, :] + x[::2, 1::2, :] + x[1::2, ::2, :] + x[1::2, 1::2, :])/4\r\n    return Model(inputs = INPUT, outputs = OUTPUT)\r\n\r\ndef upsample_conv(inputs, filters, kernel_size, input_shape):\r\n    INPUT=Input(shape = inputs.shape[1:])\r\n    x=tf.concat([INPUT, INPUT, INPUT, INPUT], axis = -1)\r\n    x=tf.nn.depth_to_space(x, 2)\r\n    OUTPUT=Conv2D(filters, kernel_size, padding = 'same')(x)\r\n    return Model(inputs = INPUT, outputs = OUTPUT)\r\n\r\n\r\ndef residual_block(name, inputs, filters, kernel_size, strides, input_shape, output_shape, upsample = False):\r\n    INPUT=Input(shape = inputs.shape[1:])\r\n    x=BatchNormalization()(INPUT)\r\n    x=ReLU()(x)\r\n\r\n    if upsample == True:\r\n        x=upsample_conv(x, filters, kernel_size, input_shape)(x)\r\n        x=Lambda(lambda z: tf.image.resize(z, size=output_shape, method='nearest'))(x)\r\n        original_scaled= x\r\n        x= BatchNormalization()(x)\r\n        x= ReLU()(x)\r\n        OUTPUT= Conv2D(filters, 3, padding='same')(x) + original_scaled\r\n        return Model(inputs=INPUT, outputs=OUTPUT, name=name)\r\n    else:\r\n        x= Conv2D(filters, 3)(x)\r\n        x=Lambda(lambda z: tf.image.resize(z, size=output_shape))(x)\r\n        original_scaled= x\r\n        x= BatchNormalization()(x)\r\n        x= ReLU()(x)\r\n        OUTPUT= conv_mean_pool(x, filters, kernel_size)(x)\r\n        OUTPUT= Conv2D(filters, 3, padding='same')(x) + original_scaled\r\n        return Model(inputs=INPUT, outputs=OUTPUT, name=name)\r\n\r\n\r\ndef create_discriminator(z_dim, name='Discriminator'):\r\n    INPUT= Input(shape=[100, 100, 3])\r\n    x=Lambda(lambda z: tf.image.resize(z, size=[128, 128]))(INPUT)\r\n    x=Conv2D(3, 3, padding='same')(x)\r\n    x=residual_block('res1', x, 32, 3, 1, [128, 128], [64, 64])(x)\r\n    x=residual_block('res2', x, 32, 3, 1, [64, 64], [32, 32])(x)\r\n    x=residual_block('res3', x, 32, 3, 1, [32, 32], [16, 16])(x)\r\n    x=residual_block('res4', x, 32, 3, 1, [16, 16], [8, 8])(x)\r\n    x=residual_block('res5', x, 32, 3, 1, [8, 8], [8, 8])(x)\r\n    x=Flatten()(x)\r\n    OUTPUT=Dense(1)(x)\r\n    return Model(inputs=INPUT, outputs=OUTPUT)\r\n\r\n\r\ndef create_generator(z_dim, name='Generator'):\r\n    INPUT=Input((z_dim,))\r\n    x=Dense(z_dim*4*4)(INPUT)\r\n    x=tf.reshape(x, (-1, 8, 8, 1))\r\n    x=residual_block('res1', x, 64, 3, 1, [8, 8], [8, 8], upsample = True)(x)\r\n    x=residual_block('res2', x, 32, 3, 1, [8, 8], [16, 16], upsample = True)(x)\r\n    x=residual_block('res3', x, 16, 3, 1, [16, 16], [32, 32], upsample = True)(x)\r\n    x=residual_block('res4', x, 8, 3, 1, [32, 32], [64, 64], upsample = True)(x)\r\n    x=residual_block('res5', x, 4, 3, 1, [64, 64], [128, 128], upsample = True)(x)\r\n    x=BatchNormalization()(x)\r\n    x=ReLU()(x)\r\n    x=Conv2D(3, 3, padding = 'same', activation = 'sigmoid')(x)\r\n    OUTPUT=Lambda(lambda z: tf.image.resize(z, size=[100, 100]))(x)\r\n    return Model(inputs=INPUT, outputs=OUTPUT, name=name)\r\n\r\nif __name__ == '__main__':\r\n    parser= argparse.ArgumentParser(description = 'Inputs')\r\n    parser.add_argument('--epochs', type = int, default =100)\r\n    parser.add_argument('--batch_size', type = int, default =64)\r\n    parser.add_argument('--z_dim', type = int, default =128)\r\n    parser.add_argument('--n_critic', type = int, default=5)\r\n    parser.add_argument('--LAMBDA', type=float, default=10)\r\n    parser.add_argument('--shuffle', type = bool, default =True)\r\n    parser.add_argument('--num_parallel_calls', type = int, default =4)\r\n    parser.add_argument('--buffer_size', type = int, default =1000)\r\n    parser.add_argument('--prefetch', type = int, default =1000)\r\n    parser.add_argument('--learning_rate', type = float, default =0.001)\r\n    parser.add_argument('--beta_1', type = float, default =0.0)\r\n    parser.add_argument('--beta_2', type = float, default =0.999)\r\n    args=parser.parse_args('')\r\n\r\n    ##########################################################\r\n    ## MAKING DATALOADERS FOR TRAINING, VALIDATION, TESTING ##\r\n    ##########################################################\r\n    train_data = tf.random.normal(shape=(1, 100, 100, 3))\r\n    train_dataloader = tf.data.Dataset.from_tensor_slices(train_data).batch(1)\r\n\r\n    #######################################\r\n    ## INITIALIZE MODEL, LOSSES, METRICS ##\r\n    #######################################\r\n    G=create_generator(args.z_dim, 'Generator')\r\n    D=create_discriminator(args.z_dim, 'Discriminator')\r\n    fixed_noise=tf.random.normal(shape=(16, args.z_dim))\r\n\r\n    loss_object=tf.keras.losses.BinaryCrossentropy()\r\n    train_loss= tf.keras.metrics.Mean(name = 'train_loss')\r\n    test_accuracy= tf.keras.metrics.Accuracy(name = 'test_accuracy')\r\n    optimizerG=tf.keras.optimizers.Adam(\r\n        learning_rate=args.learning_rate,\r\n        beta_1=args.beta_1,\r\n        beta_2=args.beta_2,\r\n    )\r\n    optimizerD = tf.keras.optimizers.Adam(\r\n        learning_rate=args.learning_rate,\r\n        beta_1=args.beta_1,\r\n        beta_2=args.beta_2,\r\n    )\r\n\r\n    G.summary(print_fn=logging.info)\r\n    D.summary(print_fn=logging.info)\r\n\r\n    ##############################################\r\n    ## Loss computations and training functions ##\r\n    ##############################################\r\n    @tf.function\r\n    def discriminator_step(fake_image, real_image):\r\n        with tf.GradientTape() as tape:\r\n            epsilon = tf.random.uniform(\r\n                shape=[fake_image.shape[0], 1, 1, 1], minval=0, maxval=1)\r\n            interpolated_image = epsilon*fake_image + (1-epsilon)*real_image\r\n            d_interpolated = D(interpolated_image)\r\n            d_fake = D(fake_image)\r\n            d_real = D(real_image)\r\n\r\n            grad_d = tf.gradients(d_interpolated, [interpolated_image])[0]\r\n            slopes = tf.sqrt(\r\n                1e-8 + tf.reduce_sum(tf.square(grad_d), axis=[1, 2, 3]))\r\n            gradient_penalty = tf.reduce_mean((slopes-1.) ** 2)\r\n\r\n            d_loss = tf.reduce_mean(\r\n                d_fake) - tf.reduce_mean(d_real) + args.LAMBDA * gradient_penalty\r\n\r\n            gradients = tape.gradient(d_loss, D.trainable_variables)\r\n            optimizerD.apply_gradients(\r\n                zip(gradients, D.trainable_variables))\r\n\r\n            return d_loss\r\n\r\n    @tf.function\r\n    def generator_step(fake_image):\r\n        with tf.GradientTape() as tape:\r\n            d_fake = -D(fake_image)\r\n            g_loss = tf.reduce_mean(d_fake)\r\n\r\n            gradients = tape.gradient(g_loss, G.trainable_variables)\r\n            optimizerG.apply_gradients(\r\n                zip(gradients, G.trainable_variables))\r\n\r\n            return g_loss\r\n\r\n    ###################\r\n    ## TRAINING LOOP ##\r\n    ###################\r\n    iter = 0\r\n    d_loss = 0\r\n    g_loss = 0\r\n    for epoch in range(args.epochs):\r\n        start = time.time()\r\n        for real_image in train_dataloader:\r\n            z = tf.random.normal(shape=(real_image.shape[0], args.z_dim))\r\n            fake_image = G(z)\r\n            d_loss = discriminator_step(fake_image, real_image)\r\n\r\n            if iter % args.n_critic == 0:\r\n                z = tf.random.normal()\r\n                fake_image = G(z)\r\n                g_loss = generator_step(fake_image)\r\n~~~~\r\n\r\n**Other info / logs**\r\n~~~~\r\nTraceback (most recent call last):\r\n  File \"code/WGAN-GP_issues.py\", line 191, in <module>\r\n    d_loss = discriminator_step(fake_image, real_image)\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\ntensorflow.python.autograph.impl.api.StagingError: in converted code:\r\n\r\n    code/WGAN-GP_issues.py:162 discriminator_step  *\r\n        gradients = tape.gradient(d_loss, D.trainable_variables)\r\n    /home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:1014 gradient\r\n        unconnected_gradients=unconnected_gradients)\r\n    /home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py:76 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:134 _gradient_function\r\n        grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\r\n    /home/xiavatar/tf-2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/registry.py:97 lookup\r\n        \"%s registry has no entry for: %s\" % (self._name, name))\r\n\r\n    LookupError: gradient registry has no entry for: ResizeBilinearGrad\r\n~~~~", "comments": ["@sxsheng \r\nI tried reproducing the issue. However i am getting error `SystemExit: 2`. I am attaching the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/07a0d6cb3795ab7be0f1c00bdfb88f4d/untitled312.ipynb) Please, help us to reproduce the issue. It is easy for localizing the issue faster. Thanks!", "Hi, I fixed the code in the jupyter notebook. It seems parse_args needed an empty string as an argument to run correctly in the notebook. Now it shows the correct error with LookupError: gradient registry has no entry for :ResizeBilinearGrad.", "I have tried on colab with TF version 2.0, 2.1.0-dev20191103 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/12d921004bc6dfb167c7646a18c40a46/untitled328.ipynb). Thanks!", "@sxsheng Can you please simple standalone code to reproduce the issue. The current code is too long to find root-cause within the time limitations. Thanks!", "Okay I edited and made a simpler version that reproduces the same error. You can find the notebook here [Link](https://colab.research.google.com/gist/sxsheng/d728fd700c408ac83c12fa269163e47a/untitled328.ipynb#scrollTo=-OujnW85Yza2)", "Maybe use this?\r\n\r\n@tf.RegisterGradient(\"ResizeBilinearGrad\")\r\ndef _ResizeBilinearGrad(op, grad):\r\n    return (array_ops.zeros(shape=array_ops.shape(op.inputs[0]), dtype=op.inputs[0].dtype),\r\n             tf.raw_ops.ResizeBilinearGrad(grads=grad, original_image=op.inputs[1]))\r\n\r\nDon't know.. can't find proper doc about what's going on here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py", "The gradient for this op is registered, [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_grad.py)\r\n\r\nI think the issue is that in discriminator_step, you should use tape.gradient, not tf.gradients. Though the error message is pretty confusing.", "BTW since you're using gradient twice, you should probably do tf.GradientTape(persistant=True)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33751\">No</a>\n", "The issue is that there're no grad of grad of resize, which is an known issue since [long time ago](https://github.com/tensorflow/tensorflow/issues/22208).", "#22208", "#7641", "I have tried on colab with TF version 2.2-rc4 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/70303d02ab954cb4863247f1efe374df/untitled859.ipynb). Thanks!", "I have tried on colab with TF nightly version(`2.4.0-dev20200902`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b3adb700eccbaa92749d43ca165a1d40/untitled304.ipynb). Thanks!", "Has there been any progress in this issue? If someone has implemented or knows how to implement this, can you post it here please?", "I am a beginner on machine learning so my question may sound dummy. But why the following code (_part of the code to reproduce the error_)\r\n~~~python\r\ntape.gradient(d_loss, D.trainable_variables)\r\n~~~\r\n requires the grad of `ResizeBilinearGrad`? I understand that `ResizeBilinear` is introduced by `tf.image.resize`. ", "Was able to reproduce the issue in TF 2.6.0-dev20210527 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/f8b02f3ab14c31ac3d7d916592b4e39f/untitled24.ipynb)..Thanks !", "Was able to reproduce the issue in TF 2.7.0 ,please find the gist [here](https://colab.research.google.com/gist/kumariko/57ab05a2b50e5a858f9f1f5ee7329b7e/untitled24.ipynb#scrollTo=z6dkPf95LYsn)..Thanks !", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33751\">No</a>\n"]}, {"number": 33750, "title": "Initial optimized fully-connected kernel for Xtensa HiFi3 (int8/uint8)", "body": "This PR introduces an optimization for the Xtensa HiFi3 platform for TF Micro's fully connected kernel. This PR demonstrates where to place optimized TF Micro kernels in the source tree. To trigger a build and utilize the intrinsics - this command can be made to the Makefile:\r\n\r\n```sh\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test_kernel_fully_connected_test TAGS=xtensa-hifi3 TARGET=xtensa-xpg\r\n```\r\n\r\nWith this first pass at optimization - the FC unit tests is sped up 20%. The remaining increase in speed will be swapping out the gemmlow methods with optimized Xtensa bits. I'll tackle that in a future PR.", "comments": []}, {"number": 33748, "title": "Using tf.contrib.framework.nest functions in TF2.0", "body": "The tf.contrib.framework.nest module seems to have been moved to tf.nest after contrib has been deprecated in TF2.0.\r\n\r\nHowever, not all the functions have been exported.\r\ntf.contrib.framework.nest (https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/framework/nest) has the following functions:\r\nassert_same_structure(...)\r\nassert_shallow_structure(..)\r\nflatten(...)\r\nflatten_dict_items(...)\r\nflatten_up_to(...)\r\nflatten_with_joined_string_paths(...)\r\nflatten_with_tuple_paths(...)\r\nflatten_with_tuple_paths_up_to(...)\r\nget_traverse_shallow_structure(...)\r\nis_nested(...)\r\nis_sequence(...)\r\nis_sequence_or_composite(...)\r\nmap_structure(...)\r\nmap_structure_up_to(...)\r\nmap_structure_with_paths(...)\r\nmap_structure_with_tuple_paths(...)\r\nmap_structure_with_tuple_paths_up_to(...)\r\npack_sequence_as(...)\r\nyield_flat_paths(...)\r\n\r\nwhere tf.nest (https://www.tensorflow.org/api_docs/python/tf/nest/map_structure) only has the following:\r\nassert_same_structure(...)\r\nflatten(...)\r\nis_nested(...)\r\nmap_structure(...)\r\npack_sequence_as(...)\r\n\r\n\r\nThis is limiting to our application since we depend on specifying a flat structure to limit the nesting of the functions.\r\nMore specifically, we would like to use the *_up_to variants and the *_with_paths variants which seems to have been removed in tf.nest\r\n\r\nIs there a way to use the previous API (the contrib one) for these functions in TF2.0", "comments": ["Hi @aravic, we have recently released a subset of tf.contrib.framework.nest functions as a separate [library](http://pypi.org/project/dm-tree). It does not have exactly the same API as nest but it is fairly close, and most of the functions [are there](http://tree.readthedocs.io).  Please have a look, and let us know if tree works for you.\r\n", "This is great. Thank you!", "Feel free to close the issue and open any new ones on the tree [issue tracker](https://github.com/deepmind/tree/issues)."]}, {"number": 33747, "title": "TF2.0 TPU Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:0/device:CPU:0 in order to run AutoShardDataset: Unable to parse tensor proto", "body": "I am trying to run the MNIST example on the TPU using tf2.0. However, the following error occurred.\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:0/device:CPU:0 in order to run AutoShardDataset: Unable to parse tensor proto\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572103296.361904413\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to parse tensor proto\",\"grpc_status\":3} [Op:AutoShardDataset]\r\n\r\ncode: \r\n```\r\nimport os\r\n\r\n# # TF 1.x\r\n# resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n# tf.tpu.experimental.initialize_tpu_system(resolver)\r\n# strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n# TF 2.0\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.compile(\r\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    metrics=[tf.keras.metrics.sparse_categorical_accuracy])\r\n\r\nmodel.fit(\r\n  x_train.astype(np.float32), y_train.astype(np.float32),\r\n  epochs=5,\r\n  steps_per_epoch=60,\r\n  validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n  validation_freq=5\r\n)\r\n\r\nmodel.save_weights('./fashion_mnist.h5', overwrite=True)\r\n```", "comments": ["The provided code snippet looks incomplete. Please update the snippet with minimal complete code to reproduce the issue reported. Thanks!", "## import\r\n```python\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n\r\nimport tensorflow as tf\r\n```\r\n\r\n## Data\r\n\r\n```python\r\nimport numpy as np\r\n\r\n# import distutils\r\n# if distutils.version.LooseVersion(tf.__version__) < '1.14':\r\n#     raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/fashion_mnist.ipynb')\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\r\n\r\n# add empty color dimension\r\nx_train = np.expand_dims(x_train, -1)\r\nx_test = np.expand_dims(x_test, -1)\r\n```\r\n\r\n## Define model\r\n\r\n```python\r\ndef create_model():\r\n  model = tf.keras.models.Sequential()\r\n\r\n  model.add(tf.keras.layers.Conv2D(128, (3, 3), input_shape=x_train.shape[1:]))\r\n  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n  model.add(tf.keras.layers.Activation('elu'))\r\n\r\n  model.add(tf.keras.layers.Flatten())\r\n  model.add(tf.keras.layers.Dense(10))\r\n  model.add(tf.keras.layers.Activation('softmax'))\r\n  \r\n  return model\r\n```\r\n\r\n## Train on the TPU\r\n\r\n```python\r\nimport os\r\n\r\n# # TF 1.x\r\n# resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n# tf.tpu.experimental.initialize_tpu_system(resolver)\r\n# strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n# TF 2.0\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.compile(\r\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    metrics=[tf.keras.metrics.sparse_categorical_accuracy])\r\n\r\nmodel.fit(\r\n  x_train.astype(np.float32), y_train.astype(np.float32),\r\n  epochs=5,\r\n  steps_per_epoch=60,\r\n  validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n  validation_freq=5\r\n)\r\n\r\nmodel.save_weights('./fashion_mnist.h5', overwrite=True)\r\n```\r\n\r\n## Error ouput\r\n\r\n```\r\nINFO:tensorflow:Initializing the TPU system: 10.98.37.42:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-5-ccaaa18be0df> in <module>()\r\n     24   steps_per_epoch=60,\r\n     25   validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n---> 26   validation_freq=5\r\n     27 )\r\n     28 \r\n```\r\n\r\n------\r\n\r\n13 frames\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:0/device:CPU:0 in order to run AutoShardDataset: Unable to parse tensor proto\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572653688.122628027\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to parse tensor proto\",\"grpc_status\":3} [Op:AutoShardDataset]\r\n```\r\n\r\n## colab link\r\n\r\nhttps://colab.research.google.com/github/huan/tensorflow-handbook-tpu/blob/master/tensorflow-handbook-tpu-example.ipynb", "I have the same issue. There seem to be also tensorflow/datasets#832 - but simply casting to float32 didn't work for me.", "@donttal - I just noticed that `tf.compat.v1.disable_eager_execution()` allows me to train on a TPU - see #34391.", "@donttal Is this still an issue?", "> @donttal Is this still an issue?\r\n\r\nI think it be solved", "I believe I have found a way to fix this issue. See https://github.com/huan/tensorflow-handbook-tpu/issues/1#issuecomment-606189444", "`tf.identity` raises errors when GPU is involved. Using `tf.device('CPU')` will prevent the error:\r\n```\r\nwith tf.device('CPU'):\r\n    tensor2 = tf.identity(tensor1)         # .take(-1)\r\n```\r\nRemember tensor2 is a Variant. You may use take(-1) in case tensor1 is a Dataset and you want tensor2 to be one of the Dataset family only.\r\nActually the error is generated for the Datasets in first place."]}, {"number": 33746, "title": "cannot import tensor flow at newest Mac OS X system catalina!", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Mac OS X catalina):\r\n- TensorFlow installed from ( binary PyPi):\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- Installed using pip\r\n\r\n\r\nimport tensorflow \r\n\r\n[libprotobuf ERROR google/protobuf/descriptor_database.cc:394] Invalid file descriptor data passed to EncodedDescriptorDatabase::Add().\r\n[libprotobuf FATAL google/protobuf/descriptor.cc:1359] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):\r\nlibc++abi.dylib: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):\r\n", "comments": ["after I reinstall all of the pip package, problem disappeared! not clearly known why.", "Had the same problem, fixed it with pip reinstall tensorflow and protobuf\r\n```bash\r\npip3 uninstall tensorflow protobuf && pip3 install tensorflow protobuf\r\n```", "@Ferluci you save my life. ", "That really help thanks\r\n", "Hey guys, this didn't work for me. I'm still getting the same error\r\n<img width=\"1183\" alt=\"Screen Shot 2020-01-27 at 10 20 36 AM\" src=\"https://user-images.githubusercontent.com/22390818/73153853-bc135880-40ee-11ea-8816-7e99ab176f69.png\">\r\n", "Hello @SirPhemmiey did you find the solution of your error? I am also getting same error and finding solution", "Solution is\r\n\r\n```\r\npython -m pip install --upgrade pip\r\npython -m pip uninstall tensorflow tensorflow_estimator protobuf\r\npython -m pip install tensorflow\r\n```\r\n\r\n(if you use a virtualenv, you might not need the `python -m` prefix)\r\n\r\nLocking conversation as this is an old issue. If there are still issues with installing please open a new issue and fill in the template"]}, {"number": 33745, "title": "Fix typo in wrapper.py", "body": "Fix a typo, from \"backard_layer\" to \"backward_layer\"\r\nin an example of bidirectional custom backward layer.", "comments": []}, {"number": 33744, "title": "Tensor zero_padding2d_1_input:0, specified in either feed_devices or fetch_devices was not found in the Graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n_custom_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\nPRETTY_NAME=\"Debian GNU/Linux 10 (buster)\"\r\nNAME=\"Debian GNU/Linux\"\r\nVERSION_ID=\"10\"\r\nVERSION=\"10 (buster)\"\r\nVERSION_CODENAME=buster\r\nID=debian\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nnone\r\n- TensorFlow installed from (source or binary):\r\n_binary_\r\n- TensorFlow version (use command below):\r\n`v1.14.0-rc1-22-gaf24dc9 1.14.0`\r\n- Python version:\r\nPython 3.7.4\r\n- Bazel version (if compiling from source):\r\n_none_\r\n- GCC/Compiler version (if compiling from source):\r\n_none_\r\n- CUDA/cuDNN version:\r\n_none_\r\n- GPU model and memory:\r\nCPU model\r\n\r\n**Describe the current behavior**\r\nThis issue happens when running the Tensorflow model inference in a Tornado server started as a Thread.\r\n```python\r\nclass WebServer(threading.Thread):\r\n    def run(self):\r\n        application.listen(8888)\r\n        #asyncio.set_event_loop(asyncio.new_event_loop())\r\n        asyncio.set_event_loop(AnyThreadEventLoopPolicy())\r\n        tornado.ioloop.IOLoop.instance().start()\r\nWebServer.start()\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nInference running without any issue. The issue does not occur when it starts running on the main thread instead:\r\n\r\n```python\r\napplication.listen(8888)\r\napplication.listen(8888)\r\ntornado.ioloop.IOLoop.instance().start()\r\n```\r\n\r\n\r\n**Other info / logs**\r\nThis error happens on both Tensorflow models I run, so I assume it's a threading problem when running the Graph in a thread that it is not the main thread.\r\n\r\nThe only error logging I get is\r\n` Tensor zero_padding2d_1_input:0, specified in either feed_devices or fetch_devices was not found in the Graph`", "comments": ["**[NOTE]**\r\nAccording to [here](https://stackoverflow.com/questions/54652536/keras-tensorflow-backend-error-tensor-input-10-specified-in-either-feed-de), I have tried to run `model.predict` in my session so I did\r\n\r\n```python\r\n    with session.as_default():\r\n        with session.graph.as_default():\r\n            labels = np.round(model.predict(X)[0])\r\n        \r\n            prediction = []\r\n\r\n            for i in range(0, len(labels)):\r\n                if labels[i] == 1:\r\n                    prediction.append(labels_group[i])\r\n\r\n            return prediction\r\n```\r\n\r\nbut now getting a \r\n\r\n```\r\nError while reading resource variable block4_conv2/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/block4_conv2/kernel)\r\n\t [[{{node sequential_1_3/block4_conv2/convolution/ReadVariableOp}}]]\r\n```", "**[NOTE]**\r\nI was able to solve this issue creating a new `global session` and then setting it to keras before loading the model as stated in https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-495005162:\r\n\r\n```python\r\n   def load_tensorflow_shared_session(self):\r\n        \"\"\" Load a Tensorflow/Keras shared session \"\"\"\r\n        # LP: create a config by gpu cpu backend\r\n        if os.getenv('HAS_GPU', '0') == '1':\r\n            config = tf.ConfigProto(\r\n                device_count={'GPU': 1},\r\n                intra_op_parallelism_threads=1,\r\n                allow_soft_placement=True\r\n            )\r\n            config.gpu_options.allow_growth = True\r\n            config.gpu_options.per_process_gpu_memory_fraction = 0.6\r\n        else:\r\n            config = tf.ConfigProto(\r\n                intra_op_parallelism_threads=1,\r\n                allow_soft_placement=True\r\n            )\r\n        # LP: create session by config\r\n        self.tf_session = tf.Session(config=config)\r\n\r\n        return self.tf_session\r\n```\r\n\r\nand then passing this shared session to all TF /Keras models:\r\n\r\n```python\r\ndef load__model(MODEL_PATH, session):\r\n    '''\r\n        Load the model\r\n    '''\r\n\r\n    # IMPORTANT: models have to be loaded AFTER SETTING THE SESSION for keras! \r\n    # Otherwise, their weights will be unavailable in the threads after the session there has been set\r\n    set_session(session)\r\n    model = keras.models.load_model(MODEL_PATH)\r\n    model._make_predict_function()\r\n```\r\n\r\nInstead of using `global session` object I'm using a singleton object that retains `this.tf_session`, by the way it works in both ways.", "@loretoparisi Is this still an issue or was it resolved? Please close the issue if it was resolved already. Thanks!", "@jvishnuvardhan please let me check, thank you.", "@loretoparisi When you check, can you also check with latest versions in TF1.x and TF2.x? Thanks!", "So for sure we were talking about of version TF 1.x.", "@loretoparisi Can you please check with the latest versions in TF1.15.3 and let us know whether the issue persists? Thanks!\r\n\r\nPlease feel free to close the issue if the issue was resolved for you. Thanks!", "@jvishnuvardhan ok I will confirm.", "@loretoparisi Are you confirming that this is an issue with `TF1.15.3` or your are confirming that this is already resolved for you? Thanks!", "@jvishnuvardhan I confirm that this has been resolved. I have investigated the solution. The problem was related to a wrong handling of the TF session through different models loaded on the same server (multi-processing application server so with the same shared session). The solution was a common shared session, passed through all loaded models.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33744\">No</a>\n"]}]