[{"number": 23572, "title": "How to reuse kernel weight with tf.keras.layers.Conv2D", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win10\r\n- **TensorFlow installed from (source or binary)**: conda\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: Titan XP, 12G\r\n\r\nI want to reuse kernel weight in keras with tensorflow backend.\r\n\r\nMy code is as following :\r\n\r\n```\r\nfrom tensorflow.python.keras.layers import  Input\r\nfrom tensorflow.python.keras import regularizers\r\ndef infer3(data_input, Reuse):\r\n    with tf.variable_scope('Network', reuse=Reuse):\r\n        inputs = Input(tensor = data_input)\r\n        network = tf.keras.layers.Conv2D(kernel_size=3, strides=2, filters=64, padding='same',kernel_regularizer=regularizers.l2(1),\r\n                    activation='linear', kernel_initializer=\"glorot_normal\", name='conv1', bias_initializer='zeros')(inputs)\r\n\r\n    return network\r\n\r\ntf.reset_default_graph() \r\ninput_tensor = tf.placeholder(tf.float32,shape=[BATCH_SIZE,img_H,img_W,1])\r\noutput_tensor = tf.placeholder(tf.float32,shape=[BATCH_SIZE,img_H,img_W,1]) \r\nin_training_mode = tf.placeholder(tf.bool)     \r\n\r\n\r\nnetwork = infer3(input_tensor,False)\r\nnetwork_test = infer3(input_tensor,True)\r\n```\r\n\r\nWhen I type \" tf.trainable_variables() \"\r\n\r\nIt show message:\r\n\r\n```\r\n[<tf.Variable 'Network/conv1/kernel:0' shape=(3, 3, 1, 64) dtype=float32>,\r\n <tf.Variable 'Network/conv1/bias:0' shape=(64,) dtype=float32>,\r\n <tf.Variable 'Network_1/conv1/kernel:0' shape=(3, 3, 1, 64) dtype=float32>,\r\n <tf.Variable 'Network_1/conv1/bias:0' shape=(64,) dtype=float32>]\r\n```\r\nHow can I reuse kernel weight?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23571, "title": "Fix broken OrderedEnqueuer class in keras.utils.data_utils", "body": "The `OrderedEnqueuer` class in `tensorflow.python.keras.utils.data_utils` isn't working at the head of the master branch when `use_multiprocessing=True`. For example:\r\n\r\n```\r\nIn [1]: from tensorflow.python import keras \r\n   ...: import numpy as np \r\n   ...:  \r\n   ...: class TestSequence(keras.utils.data_utils.Sequence): \r\n   ...:  \r\n   ...:   def __init__(self, shape, value=1.): \r\n   ...:     self.shape = shape \r\n   ...:     self.inner = value \r\n   ...:  \r\n   ...:   def __getitem__(self, item): \r\n   ...:     return np.ones(self.shape, dtype=np.uint32) * item * self.inner \r\n   ...:  \r\n   ...:   def __len__(self): \r\n   ...:     return 100 \r\n   ...:  \r\n   ...:   def on_epoch_end(self): \r\n   ...:     self.inner *= 5.0 \r\n   ...:  \r\n   ...: enqueuer = keras.utils.data_utils.OrderedEnqueuer( \r\n   ...:     TestSequence([3, 200, 200, 3]), use_multiprocessing=True) \r\n   ...: enqueuer.start(3, 10) \r\n   ...: gen_output = enqueuer.get() \r\n   ...: next(gen_output)[0, 0, 0, 0] \r\n   ...:                                                                                             \r\nException in thread Thread-30:\r\nTraceback (most recent call last):\r\n  File \"/Users/freiss/dl/tensorflow-fred/testenv/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/Users/freiss/dl/tensorflow-fred/testenv/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/Users/freiss/dl/tensorflow-fred/testenv/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 619, in _run\r\n    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\r\n  File \"/Users/freiss/dl/tensorflow-fred/testenv/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 601, in pool_fn\r\n    initargs=(seqs, self.random_seed))\r\nAttributeError: 'OrderedEnqueuer' object has no attribute 'random_seed'\r\n\r\n[Python process hangs, control never returns to user]\r\n```\r\n\r\nThis error happens because the code in `data_utils.py` that is supposed to set up the worker pool for `OrderedEnqueuer` contains a reference to a class field that doesn't exist and crashes. The main thread enqueues tasks, but no workers are created to perform the tasks.\r\n\r\nThis problem also causes the automated regression test `//tensorflow/python/keras:data_utils_test` to hang.\r\n\r\nThis PR fixes the problem by setting the \"random seed\" argument to `None`.\r\n\r\nAfter this change, I'm able to run the above test code manually, i.e.:\r\n\r\n```\r\nIn [1]: from tensorflow.python import keras \r\n   ...: import numpy as np \r\n   ...:  \r\n   ...: class TestSequence(keras.utils.data_utils.Sequence): \r\n   ...:  \r\n   ...:   def __init__(self, shape, value=1.): \r\n   ...:     self.shape = shape \r\n   ...:     self.inner = value \r\n   ...:  \r\n   ...:   def __getitem__(self, item): \r\n   ...:     return np.ones(self.shape, dtype=np.uint32) * item * self.inner \r\n   ...:  \r\n   ...:   def __len__(self): \r\n   ...:     return 100 \r\n   ...:  \r\n   ...:   def on_epoch_end(self): \r\n   ...:     self.inner *= 5.0 \r\n   ...:  \r\n   ...: enqueuer = keras.utils.data_utils.OrderedEnqueuer( \r\n   ...:     TestSequence([3, 200, 200, 3]), use_multiprocessing=True) \r\n   ...: enqueuer.start(3, 10) \r\n   ...: gen_output = enqueuer.get() \r\n   ...: next(gen_output)[0, 0, 0, 0] \r\n   ...:                                                                         \r\nOut[1]: 0.0\r\n\r\nIn [2]:\r\n```\r\n\r\nHowever, the automated regression test `//tensorflow/python/keras:data_utils_test` still hangs due to a second problem that I have not yet isolated.\r\n", "comments": ["Just squashed the commits on this branch, so you may need to git pull to get the latest version.", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23570, "title": "Having trouble importing tf", "body": "I am new to tensorflow, and so far been using CPU tensorflow/keras (was working fine with CPU). Recently got a gpu and can't seem to find proper solution. Any suggestions on how to fix it? Thanks in advance.\r\n\r\nP.S. could it be because i installed GPU version without uninstalling CPU version?\r\n\r\nUsing Windows 10\r\nCuda 9\r\ncuDNN 7\r\nGPU - gtx 1080, 8gb\r\neGPU - razer core x\r\nPython 3.5.3 \r\n\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Nozim\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["Did you update the CUDA path variable? Please refer this [windows setup](https://www.tensorflow.org/install/gpu#windows_setup) to add the path.", "Yes, they are all in place. Fixed it with reinstalling cuda with local. My bin folder had 3 files : ). Now it shows \"adding visible gpu devises: 0\"  \r\n\r\n\r\n"]}, {"number": 23569, "title": "fatal error when import /tensorflow/.tf_configure.bazelrc during installation", "body": "\r\n**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from source\r\n- Bazel version : 18.1\r\n\r\nHello\r\n\r\nI want to install tensorflow from source.\r\nAfter I configured with ./configure I try to launch the .so file, it wants to import a /tf.configure.bazelrc which is not even in the git tensorflow file. I am not sure what to do.\r\n```\r\n~/Documents/molecular dynamics/tensorflow/tensorflow$ bazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so\r\n[bazel INFO src/main/cpp/option_processor.cc:362] Looking for the following rc files: /etc/bazel.bazelrc,/home/kotik/Documents/molecular dynamics/tensorflow/.bazelrc,/home/kotik/.bazelrc\r\n[bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /etc/bazel.bazelrc\r\n[bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/kotik/Documents/molecular dynamics/tensorflow/.bazelrc\r\n[bazel FATAL src/main/cpp/blaze.cc:1324] Invalid import declaration in .blazerc file '/home/kotik/Documents/molecular dynamics/tensorflow/.bazelrc': 'import /home/kotik/Documents/molecular dynamics/tensorflow/.tf_configure.bazelrc' (are you in your source checkout/WORKSPACE?)\r\n```\r\n\r\n", "comments": ["@gunan  PTAL", "@b0risbe  - Hi, please use  Bazel 0.15.0 and see if the issue persists.", "Hello,\r\n\r\nI had difficulties to uninstall bazel 19 and finally succeeded to install bazel 0.15.0.\r\n\r\nInstallation of Bazel seems to be ok.\r\n\r\n```\r\nkotik@kotik:~/T\u00e9l\u00e9chargements$ ./bazel-0.15.0-installer-linux-x86_64.sh --user\r\nBazel installer\r\n---------------\r\n\r\nBazel is bundled with software licensed under the GPLv2 with Classpath exception.\r\nYou can find the sources next to the installer on our release page:\r\n   https://github.com/bazelbuild/bazel/releases\r\n\r\n# Release 0.15.0 (2018-06-26)\r\n\r\nBaseline: b93ae42e8e693ccbcc387841a17f58259966fa38\r\n\r\nCherry picks:\r\n   + 4b80f2455e7e49a95f3a4c9102a67a57dad52207:\r\n     Add option to enable Docker sandboxing.\r\n   + 6b1635279e8b33dc1ac505ac81825e38f8797a14:\r\n     Allow disabling the simple blob caches via CLI flag overrides.\r\n   + 4ec0a7524913ab2c4641368e3f8c09b347351a08:\r\n     Use BUILD.bazel instead of BUILD for external projects\r\n\r\nIncompatible changes:\r\n\r\n  - Bazel now always runs binaries in with \"bazel run\" in\r\n    interactive mode. The \"--nodirect_run\" command line option is now\r\n    a no-op.\r\n  - \"bazel run --noas_test\" is not supported anymore.\r\n  - Indentation on the first line of a file was previously ignored.\r\n    This is now fixed.\r\n\r\nNew features:\r\n\r\n  - C++,runfiles: to access data-dependencies (runfiles) in C++\r\n    programs, use the runfiles library built into Bazel. For usage\r\n    info, see\r\n    https://github.com/bazelbuild/bazel/blob/master/tools/cpp/runfiles\r\n    /runfiles.h\r\n\r\nImportant changes:\r\n\r\n  - Bazel now allows almost all 7-bit ASCII characters in labels.\r\n  - Remove vestigial java_plugin.data attribute\r\n  - Bazel supports including select Java 8 APIs into Android apps\r\n    targeting pre-Nougat Android devices with\r\n    --experimental_desugar_java8_libs\r\n  - Flag `--incompatible_disable_glob_tracking` is removed.\r\n  - SkyQuery's rbuildfiles now returns targets corresponding to\r\n    broken packages.\r\n  - Introduce build support for providing cache prefetch hints.\r\n  - Update the skylark DefaultInfo documentation to spell out\r\n    runfiles, data_runfiles and default_runfiles\r\n  - An internal action for symlinking runfiles will use Command\r\n    instead of a Spawns. This should have no functional chages; the\r\n    only user visible consequence should be that the internal action\r\n    is no longer be included in statistics when calculating processes\r\n    count.\r\n  - --batch is deprecated\r\n  - execution strategies line no longer handles differently the case\r\n    where all processes have the same strategy.\r\n  - The --experimental_remote_spawn_cache flag is now enabled by\r\n    default, and remote caching no longer needs --*_strategy=remote\r\n    flags (it will fail if they are specified).\r\n  - android_binary.aapt_version='aapt2' now supports en_XA and ar_XB\r\n  - Added --apple_enable_auto_dsym_dbg flag.\r\n  - non_propagated_deps has been removed from objc_library and\r\n    apple_binary.\r\n  - For Android projects, Bazel now supports building fonts as\r\n    resources. See\r\n    https://developer.android.com/guide/topics/ui/look-and-feel/fonts-in-xml\r\n    for more information on the feature.\r\n  - With --incompatible_no_support_tools_in_action_inputs enabled, Skylark\r\n    action inputs are no longer scanned for tools. Move any such\r\n    inputs to the newly introduced 'tools' attribute.\r\n\r\n## Build informations\r\n   - [Commit](https://github.com/bazelbuild/bazel/commit/2ff8c5f)\r\nUncompressing......Extracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n.\r\n\r\nBazel is now installed!\r\n\r\nMake sure you have \"/home/kotik/bin\" in your path. You can also activate bash\r\ncompletion by adding the following line to your ~/.bashrc:\r\n  source /home/kotik/.bazel/bin/bazel-complete.bash\r\n\r\nSee http://bazel.build/docs/getting-started.html to start a new project!\r\nkotik@kotik:~/T\u00e9l\u00e9chargements$ bazel version\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.15.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 26 12:10:19 2018 (1530015019)\r\nBuild timestamp: 1530015019\r\nBuild timestamp as int: 1530015019\r\n```\r\n\r\nBut I still have some trouble when I try to run the .so with bazel.\r\n\r\n\r\n```\r\nkotik@kotik:~/Documents/molecular dynamics$ git clone https://github.com/tensorflow/tensorflow tensorflow\r\nClonage dans 'tensorflow'...\r\nremote: Enumerating objects: 160, done.\r\nremote: Counting objects: 100% (160/160), done.\r\nremote: Compressing objects: 100% (117/117), done.\r\nremote: Total 458136 (delta 67), reused 89 (delta 42), pack-reused 457976\r\nR\u00e9ception d'objets: 100% (458136/458136), 268.59 MiB | 796.00 KiB/s, fait.\r\nR\u00e9solution des deltas: 100% (366820/366820), fait.\r\nExtraction des fichiers: 100% (13437/13437), fait.\r\nkotik@kotik:~/Documents/molecular dynamics$ cd tensorflow\r\nkotik@kotik:~/Documents/molecular dynamics/tensorflow$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n/usr/lib/python3/dist-packages\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -march=native -Wno-sign-compare\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apacha Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\nConfiguration finished\r\nkotik@kotik:~/Documents/molecular dynamics/tensorflow$ bazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so\r\nStarting local Bazel server and connecting to it...\r\n...............................\r\nERROR: bazel does not currently work properly from paths containing spaces (com.google.devtools.build.lib.runtime.BlazeWorkspace@40c42c43).\r\nkotik@kotik:~/Documents/molecular dynamics/tensorflow$ bazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so\r\nERROR: bazel does not currently work properly from paths containing spaces (com.google.devtools.build.lib.runtime.BlazeWorkspace@40c42c43).\r\n```\r\n", "Can you try renaming `molecular dynamics` folder to something with no spaces in it (i.e., `molecular_dynamics`)? I think that should solve the issue", "Thank you very much @harshini-gadige and @mihaimaruseac. It seems to work. \r\n", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23569)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23569)\r\n"]}, {"number": 23568, "title": "Fix AWS, GCP, HDFS, Kafka and Ignite disable config options", "body": "The logic implemented in commit 3437098ba5b111817ef6ac5906d86934168704b7 sets `no_<package>_support=true` if the `no<package>` config option is provided, but checks for a false value (in the tensorflow/BUILD file) to disable the package.\r\n\r\nThis should fully fix Issue https://github.com/tensorflow/tensorflow/issues/22819", "comments": ["It looks like disabling GCP causes an error when trying to use a package in tensorflow/contrib, as the init file automatically imports cloud regardless of whether the user opted to disable it (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/__init__.py#L30).  Do you have any suggestions as to how we could fix this?", "We should be able to make the cloud module under contrib non-breaking if gcp is not used.\r\nWe may want to merge that one first."]}, {"number": 23567, "title": "Add a new Batched NMS OP", "body": "This change adds a new Batched NMS operation (called NonMaxSuppressionLite). The entire NMS operation for all batches and across all classes is performed in one kernel operation.\r\nThe change also adds unit tests for the new kernel operation.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@azaks2 Could you please check whether there is anything left to do for this PR.", "@azaks2 Thanks for approving. I just added another file based on the failing tests from the Internal Build CI. Please take a look and re-run these tests if necessary", "@azaks2 Done, please review", " you should run\r\n$ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True", "It ran successfully, but didn't generate any new files. Am I missing something?\r\nINFO: Elapsed time: 1833.833s, Critical Path: 319.09s\r\nINFO: 4249 processes: 4249 local.\r\nINFO: Build completed successfully, 4403 total actions\r\n\r\nroot@ae98e93f0b3f:/tensorflow_local/tensorflow# bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\nsssWARNING:tensorflow:From /tensorflow_local/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/util/decorator_utils.py:127: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.GraphKeys.GLOBAL_VARIABLES` instead.\r\n....\r\n----------------------------------------------------------------------\r\nRan 7 tests in 0.505s\r\n\r\nOK (skipped=3)", "It shouldn't generate new files, it should modify the existing ones.", "Since we are returning tensors do we really need the selected_indices. we do lose information but it might only be useful if there are some other data that needs to be joined", "I added the changes for golden API.", "@azaks2 We don't currently need to return the indices, we added it only for some possible future use cases. If you think it's not necessary to have it now, we can remove it. Please let me know.", "> If you think it's not necessary to have it now, we can remove it. Please let me know.\r\nsure lets remove them\r\n", "@azaks2 The returned indices are removed now. \r\nThanks @supriyar ", "Please regenerate api defs for V1 as well - thanks", "@azaks2 Updated the V1 API too. Did it manually because `api_compatibility_test` didn't update it for me for V1.", "Hi @pooyadavoodi, would you help to resolve the conflicts? I'll retry the merge after that.\r\nThanks.", "@aaroey I rebased onto master. "]}, {"number": 23565, "title": "//tensorflow/core:util_tensor_slice_set_test fails on 1.12", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: Using CPU\r\n- GPU model and memory: Using CPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen running tests from source on r1.12, the test //tensorflow/core:util_tensor_slice_set_test fails.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI'm following the steps at https://www.tensorflow.org/install/sourcepip. This error occurs when I run\r\n`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[test.log](https://github.com/tensorflow/tensorflow/files/2555024/test.log)\r\n\r\n", "comments": ["I also had a similar problem with failing tensorflow/core:util_tensor_slice_set_test. When I change from gcc-7.3.0 to gcc-4.8.5 (with everything else staying the same), the problem does not occur.\r\n\r\n-OS: Ubuntu 18.04.1 LTS\r\n-TF: 1.12\r\n-Python: 3.6\r\n-Running TF build and test under virtualenv\r\n-Bazel: 0.15.2\r\n-CUDA: 10.0\r\n-cuDNN: 7.0\r\n\r\nI am following\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/gpu/run_cc_core.sh](url) with a slight modification to build and test:\r\n`bazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test -k --test_lang_filters=cc --jobs=${N_JOBS} --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --local_test_jobs=1 --config=opt --test_size_filters=small,medium -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`\r\n\r\ntest.log file output is as follows:\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //tensorflow/core:util_tensor_slice_set_test\r\nRunning main() from test_main.cc\r\n[==========] Running 2 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 2 tests from TensorSliceSetTest\r\n[ RUN      ] TensorSliceSetTest.QueryTwoD\r\n[       OK ] TensorSliceSetTest.QueryTwoD (0 ms)\r\n[ RUN      ] TensorSliceSetTest.QueryMetaTwoD\r\ntensorflow/core/util/tensor_slice_set_test.cc:221: Failure\r\nExpected equality of these values:\r\n  \"2,2:0,3\"\r\n  results[0].first.DebugString()\r\n    Which is: \"0,2:-\"\r\ntensorflow/core/util/tensor_slice_set_test.cc:222: Failure\r\nExpected equality of these values:\r\n  \"slice_2\"\r\n  results[0].second\r\n    Which is: \"slice_1\"\r\ntensorflow/core/util/tensor_slice_set_test.cc:223: Failure\r\nExpected equality of these values:\r\n  \"0,2:-\"\r\n  results[1].first.DebugString()\r\n    Which is: \"2,2:0,3\"\r\ntensorflow/core/util/tensor_slice_set_test.cc:224: Failure\r\nExpected equality of these values:\r\n  \"slice_1\"\r\n  results[1].second\r\n    Which is: \"slice_2\"\r\n[  FAILED  ] TensorSliceSetTest.QueryMetaTwoD (0 ms)\r\n[----------] 2 tests from TensorSliceSetTest (0 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 2 tests from 1 test case ran. (1 ms total)\r\n[  PASSED  ] 1 test.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] TensorSliceSetTest.QueryMetaTwoD\r\n\r\n 1 FAILED TEST", "Hi @MorganR ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Please visit these links to upgrade your codebase to latest versions.Ref [1](https://www.tensorflow.org/addons),[2](https://www.tensorflow.org/guide/migrate). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23564, "title": "//tensorflow/contrib/lookup:lookup_ops_test fails on 1.12", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: Using CPU\r\n- GPU model and memory: Using CPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen running tests from source on r1.12, the test //tensorflow/contrib/lookup:lookup_ops_test fails.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI'm following the steps at https://www.tensorflow.org/install/sourcepip. This error occurs when I run\r\n`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n[test.log](https://github.com/tensorflow/tensorflow/files/2555016/test.log)\r\n", "comments": ["Could you try at head and see if that works?", "I ran `bazel clean` and tried again at head of Master, but I saw exactly the same array mismatch error.\r\n\r\nNote that this time I compiled with CUDA 10.0 and cuDNN 7.3 support, using an NVIDIA RTX 2080 Ti (11GB). Everything else is the same.", "Here is the recent log.\r\n\r\nAlso here is my list of pip installs, if it's to do with a library version issue:\r\n\r\nabsl-py (0.6.1)\r\nastor (0.7.1)\r\nastroid (2.0.4)\r\ncontextlib2 (0.5.5)\r\ndm-sonnet (1.26)\r\ngast (0.2.0)\r\ngrpcio (1.16.0)\r\nh5py (2.8.0)\r\nisort (4.3.4)\r\nKeras (2.2.4)\r\nKeras-Applications (1.0.6)\r\nKeras-Preprocessing (1.0.5)\r\nlazy-object-proxy (1.3.1)\r\nMarkdown (3.0.1)\r\nmccabe (0.6.1)\r\nmelee (0.1.0)\r\nmock (2.0.0)\r\nnumpy (1.15.4)\r\npbr (5.1.1)\r\npip (9.0.1)\r\npkg-resources (0.0.0)\r\nportpicker (1.2.0)\r\nprotobuf (3.6.1)\r\npylint (2.1.1)\r\nPyYAML (3.13)\r\nscikit-learn (0.20.0)\r\nscipy (1.1.0)\r\nsemantic-version (2.6.0)\r\nsetuptools (39.0.1)\r\nsix (1.11.0)\r\nsklearn (0.0)\r\ntensorboard (1.12.0)\r\ntermcolor (1.1.0)\r\ntrfl (1.0)\r\ntyped-ast (1.1.0)\r\nWerkzeug (0.14.1)\r\nwheel (0.32.2)\r\nwrapt (1.10.11)\r\n\r\n[gputest.log](https://github.com/tensorflow/tensorflow/files/2601335/gputest.log)\r\n", "This test is consistently passing for us. Can you try running just `MutableHashTableOpTest.testMutableHashTableOfTensors`?", "If I'm reading the test right, it looks like the problem is just that `np.sort` is not sorting the result as intended, but that Tensorflow is doing the right things. `np.sort` is sorting the last axis, so it checks that each pair is sorted, which is satisfied. I think the test should be sorting the keys, so the following changes are needed:\r\n\r\n```\r\nsorted_values = np.sort(exported_values.eval(), axis=0)\r\n...\r\nself.assertAllEqual([[0, 1], [2, 3], [4, 5]], sorted_values)\r\n```\r\n\r\nJFYI, here are the latest tests I did: I've deleted all the other tests in `lookup_ops_test` except for `MutableHashTableOpTest.testMutableHashTableOfTensors`. It seems `--test_filter` doesn't work right now with python tests.\r\n\r\nI've run it with the following commands:\r\n`bazel test --config=opt --config=cuda --cache_test_results=no --test_output=all  //tensorflow/contrib/lookup:lookup_ops_test`\r\n`bazel test --config=opt --config=cuda --cache_test_results=no --test_output=all  --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/contrib/lookup:lookup_ops_test`\r\n\r\nThese each provide the same error output:\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //tensorflow/contrib/lookup:lookup_ops_test\r\n-----------------------------------------------------------------------------\r\n2018-11-22 17:16:33.328021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-22 17:16:33.328383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.73GiB freeMemory: 10.10GiB\r\n2018-11-22 17:16:33.328393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-22 17:16:33.528223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-22 17:16:33.528243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2018-11-22 17:16:33.528247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2018-11-22 17:16:33.528371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3295 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nF.\r\n======================================================================\r\nFAIL: testMutableHashTableOfTensors (__main__.MutableHashTableOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/morgan/.cache/bazel/_bazel_morgan/a1bbb0cd5ee11ffbec286625841ddec2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/lookup/lookup_ops_test.runfiles/org_tensorflow/tensorflow/contrib/lookup/lookup_ops_test.py\", line 73, in testMutableHashTableOfTensors\r\n    self.assertAllEqual([[4, 5], [2, 3], [0, 1]], sorted_values)\r\n  File \"/home/morgan/.cache/bazel/_bazel_morgan/a1bbb0cd5ee11ffbec286625841ddec2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/lookup/lookup_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1691, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b, err_msg=\"\\n\".join(msgs))\r\n  File \"/home/morgan/.pyenv/smashgpu/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 865, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/home/morgan/.pyenv/smashgpu/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 789, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\nnot equal where = (array([1, 1, 2, 2]), array([0, 1, 0, 1]))\r\nnot equal lhs = [2 3 0 1]\r\nnot equal rhs = [0 1 2 3]\r\n(mismatch 66.66666666666666%)\r\n x: array([4, 5, 2, 3, 0, 1])\r\n y: array([4, 5, 0, 1, 2, 3])\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.353s\r\n\r\nFAILED (failures=1)\r\n```\r\n", "I created a pull request\r\nhttps://github.com/tensorflow/tensorflow/pull/23927", "Is this still an issue?", "@MorganR,\r\nCan you please let us know if it is still an issue and if we can close this? Thanks!", "1.12 is no longer supported, so we cannot operate on this anymore."]}, {"number": 23563, "title": "After Tflite toco, conv2d layer becomes depthwiseconv2d.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.10.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: on CPU\r\n- GPU model and memory: on CPU\r\n\r\n**Describe the current behavior**\r\nI'm using tensorflow speech_command example's tiny_conv model [from here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands). I've trained it and froze it (with quantization set to true) following [the official tutorial](https://www.tensorflow.org/tutorials/sequences/audio_recognition). \r\nThe model itself is simple: one conv2d layer and one fully connected layer.\r\nNow, I send the froze .pb file to tensorflow lite. I can successfully get the .tflite file. However, when I check the model, the original tiny_conv model's conv2d layer becomes a depthwiseconv2d layer.( I have tried several toco converts on other models, they do not have this behavior. ) I'm wondering how could this happen? I have the .pb model shown below, followed by the .tflite model:\r\n![freeze](https://user-images.githubusercontent.com/31113955/48089764-10fd4300-e1ba-11e8-952c-3c25c854103e.JPG)\r\n![tflite](https://user-images.githubusercontent.com/31113955/48089771-165a8d80-e1ba-11e8-9558-2986b76dd9f8.JPG)\r\nAs you can see, the conv2d layer becomes depthwiseconv2d.\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is after toco, the .tflite still has one conv2d layer and one fully connected layer.\r\n\r\n**Code to reproduce the issue**\r\nAfter training and freezing, in toco, I have following code:\r\n`graph_def_file = \"my_frozen.pb\"     //This is the .pb file.\r\ninput_arrays = [\"Reshape_1\"]        //This is the name of the input node\r\noutput_arrays = [\"labels_softmax\"]  //This is the name of the ouput node\r\n\r\n//This is the main code to call toco\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\n//Since fake quantization during training, we quantize the graph during converting\r\nconverter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0] : (0., 2.)}  //mean, std_dev [from this tensorflow tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech)\r\ntflite_model = converter.convert()\r\nopen(\"tiny_conv.tflite\", \"wb\").write(tflite_model)   //The resulting .tflite file.`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Anyone solve this issue?", "I am facing the similar issue, My model has many convolutional layers and while converting my tensorflow model to .tflite format the TFLiteConverter changes the first convolutional layer into depthwise convolutional layer. Is there any alternative or a way to revert the changes? ", "I would guess this happens because the input has only 1 channel so depthwise conv and conv will be the same in that case.", "> Anyone solve this issue?\r\n\r\nhave you ever solve this issue? I met it  but the harsdware did not surpport paramter 'depth multiplier'"]}, {"number": 23561, "title": "Custom op kernels compiled with C++17 are binary incompatible with tensorflow", "body": "- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.5 (in docker)\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.6.4\r\n- GCC/Compiler version (if compiling from source): 7.3.1\r\n\r\n**current behavior**\r\nWhen compiling a custom op kernel with c++17 features, compilation succeeds, but loading the generated library fails at runtime with the following error:\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib64/python3.6/site-packages/package-0.0.1-py3.6-linux-x86_64.egg/_package_ops.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceESt17basic_string_viewIcSt11char_traitsIcEEPSs\r\n```\r\n**expected behavior**\r\nLoading the library should succeed\r\n\r\n**Code to reproduce the issue**\r\nAny custom op using `GetAttr` should reproduce this, e.g. the custom op in the custom op tutorial\r\n\r\n**Other info / logs**\r\nThis symbol that is undefined is `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, std::basic_string_view<char, std::char_traits<char> >, std::basic_string<char, std::char_traits<char>, std::allocator<char> >*)`\r\n\r\nthe symbol that it should have linked against (which exists in libtensorflow_framework.so) is `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::string_view, std::basic_string<char, std::char_traits<char>, std::allocator<char> >*)`\r\n\r\nthe difference is `std::basic_string<char, std::char_traits<char>` vs. `absl::string_view` for the second argument.\r\n\r\nIt appears to be this way because https://github.com/abseil/abseil-cpp/blob/master/absl/strings/string_view.h uses `std::string_view` to provide `absl::string_view` when `std::string_view` is available (i.e. when compiling using c++17 standard)\r\n\r\nThe tensorflow binary is compiled with c++11, so it has absl::string_view provided by the custom absl implementation, whereas my custom op, compiled with c++17 gets the standard library implementation, so they are binary incompatible.\r\n", "comments": ["a workaround is to do\r\n```\r\n#include \"absl/base/config.h\"\r\n#undef ABSL_HAVE_STD_STRING_VIEW\r\n```\r\nbefore\r\n```\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n```", "A note from https://abseil.io/about/compatibility:\r\n> Do not depend on a compiled representation of Abseil. We do not promise any ABI compatibility \u2014 we intend for Abseil to be built from source, hopefully from head. The internal layout of our types may change at any point, without notice. Building Abseil in the presence of different C++ standard library types may change Abseil types, especially for pre-adopted types (string_view, variant, etc) \u2014 these will become typedefs and their ABI will change accordingly.", "upstream: https://github.com/abseil/abseil-cpp/issues/211", "Not just abseil, none of c++ standard promises any ABI compatibility when compilers or compiler versions are different.\r\nFor this, we have a longer term goal of providing a C API for writing kernels, and that will be the solution to this problem. Until then, unfortunately c++17 features will almost always guarantee binary incompatibility, because you will have to use a different compiler.\r\n\r\nOne solution you can use is rebuild core TF package and custom ops on the same machine with the same compiler.", "@gunan Building TF and custom ops on the same machine doesn't fix the problem: I still get these errors.  Do you know an alternate workaround?", "That is quite puzzling to me.\r\nI may need to look into it. Do you have detailed reproduction instructions?", "@gunan Here's a minimized test case: https://github.com/girving/custom-op-bug\r\n\r\nMy current setup is macOS 10.14.2, bazel 0.20.0, python 3.7.1, tensorflow 3856a33dc65e7aff1df7ee4e940479ef37e9934b built from source.", "Thank you very much for sharing this!\r\nI have not been able to look at this yet, but this is a priority for me.\r\nAlso cc @yifeif ", "@gunan @yifeif Let me know if that test case isn't reproducible, or if more information would help for some other reason.", "I had some spare time to try out, but I ran into build issues when trying to rebuild tf with gcc 7.\r\nSo had to start going down the rabbit hole and got distracted.\r\nI will retry on a macos.", "There's a good chance this is Mac specific, since the linker is quite different on Mac vs. Linux.", "@gunan Any activity on this?  It's been blocking me from running some of my unit tests for over a month.", "I just reproduced the problem, after shaving a few yaks and fixing a few more issues on macos.\r\n\r\nNow I will try to see what happens if I build TF \"--cxxopt=-std=c++1z\"", "I see that the build fails with this when I set `--cxxopt=-std=c++1z`\r\n```\r\ntensorflow/core/kernels/sdca_internal.cc:313:8: error: no member named 'random_shuffle' in namespace 'std'\r\n  std::random_shuffle(sampled_index_.begin(), sampled_index_.end());\r\n```\r\n\r\nI think the ABI incompatibility is due to the difference in build flags. I think the workaround, until we have a C API is, we need to make TF build with `--cxxopt=-std=c++1z`", "OK, I just sent a change to remove usage of `std::random_shuffle` internally.\r\nThere is also another bug that injects the `-lrt` linker flag, which I also sent another change for.\r\nOnce those get merged, you can build TF pip package with `--cxxopt=-std=c++1z --cxxopt=-stdlib=libc++`\r\nThen, I can execute your reproduction script without any problems. I am sorry for the long time this took for me to debug.\r\nWould the workaround described above help?", "Thank you @gunan!  I'm happy to use that flag workaround for now.  Will wait until those fixes are merged and then try it out.", "Ok, both changes are merged, so you should be able to build with `--cxxopt=-std=c++1z` flag now.\r\n\r\nMaybe we should also create a `--config=c++1z` or `--config=c++17`?", "I tried it out.  It seems to fix my `custom-op-bug` repo but doesn't fix the unminimized case that I care about.  Will need to investigate more to figure out why.", "Actually I'd failed to test it correctly.  It works!  Thank you @gunan!", "I have similar issue when compiling my custom op with C++17. I did try to compile TF 1.14 with the mentioned flags: `--config=c++1z`, but got hit by compilation errors. Any ideas?", "I think the `-config=c++1z/17` will add `-stdlib=libc++` to the params, which is only recognized by `Clang`, is there some solution to `custom_op` issue when building with GCC 7 and C++17 in Ubuntu 16.04?"]}, {"number": 23560, "title": "Removed no longer supported call `in_eager_execution()`", "body": "changes to `model_analyser.analyse(...)`:\r\n\r\n- Swapped `context.in_eager_execution()` to the currently supported `context.executing_eagerly()`.\r\n- Added negation to eager check. In all likelihood, the negation was always supposed to be there since getting default graph in eager mode does not make sense. The current `if` condition is likely a bug. The proposed fix is consistent with other functions in this module, e.g., `profile(...)`, line 339.", "comments": []}, {"number": 23559, "title": "Unenforce StreamHandler in tf_logging", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the tf_logging API will always add its own StreamHandler to the tensorflow logger (see: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py#L121-L124). According to the [Python logging documentation](https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library) this is discouraged:\r\n\r\n> **Note:** It is strongly advised that you do not add any handlers other than NullHandler to your library\u2019s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers \u2018under the hood\u2019, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements. \r\n\r\n**Will this change the current api? How?**\r\nIf some optional configuration parameter is added to not use the StreamHandler, this does not need to affect how the tf_logging API is used today.\r\n\r\n**Who will benefit with this feature?**\r\nAny developer (like myself) that work on complex projects where tensorflow is not the only component that performs logging. I want to propagate tensorflow LogRecords to a handler at the root logger object, so that I might decide myself how best to print them or save them.", "comments": ["I am lazy and should have read the code with a bit more thoroughness. I see that an attempt have been made at avoiding adding the handler if the root logger is already configured. From `_get_logger()`:\r\n```\r\n# Don't further configure the TensorFlow logger if the root logger is\r\n# already configured. This prevents double logging in those cases.\r\nif not _logging.getLogger().handlers:\r\n...\r\n```\r\nThe problem is that this function is already called when importing tensorflow. So if you import tensorflow in your `__main__` module, you don't get to configure the root logger before the tensorflow logger is initialized, and the StreamHandler is added. If this is not the intended behaviour I guess this issue should rather be made into a bug report. If it _is_ the intended behaviour I still think it is quite ugly and should be reconsidered.\r\n", "@harahu as we move to tf 2.0, the plan is that the logging will disappear. So you will essentially use your own logging solution. I'm happy to review PRs if you have a good fix, but at the end of the day, the plan is that this will become more or less dead code.", "I consider removing tf logging an acceptable solution to the initial problem. But I am curious as to the reasoning behind it. Is there no need for logging in the core tf codebase? I see some logging is happening in C++ at the moment. Any plans to make that part of logging configurable through the python API in 2.0?", "@harahu,\r\nSorry for the delayed response The code you mentioned in [this PR](https://github.com/tensorflow/datasets/issues/354#issue-425485972) works without any error. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/60281a6cbbf21797a6a2644cb3aa2024/gh_23559.ipynb) of the working code. Thanks!", "> @harahu,\r\n> Sorry for the delayed response The code you mentioned in [this PR](https://github.com/tensorflow/datasets/issues/354#issue-425485972) works without any error. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/60281a6cbbf21797a6a2644cb3aa2024/gh_23559.ipynb) of the working code. Thanks!\r\n\r\nThis is fixed, at least in tf 2.x, so we can close this issue.\r\n"]}, {"number": 23558, "title": "tf.nn.embedding_lookup_sparse Converting sparse IndexedSlices Warning", "body": "Using embedding_lookup_sparse raises the following warning:\r\n\r\n`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \"Converting sparse IndexedSlices to a dense Tensor of unknown shape.\"`\r\n\r\nLooking at embedding_lookup_sparse implementation, I suspect the issue comes from the `gather` after embedding lookup. \r\n\r\n```\r\nids = sp_ids.values\r\nids, idx = array_ops.unique(ids)\r\n\r\nembeddings = embedding_lookup(...)\r\n...\r\nembeddings = array_ops.gather(embeddings, idx)\r\n```\r\nIs there a reason this op does not call `embedding_lookup` on `sp_ids.values` directly? As opposed to \r\nlooking up the unique ids and calling gather on those rows ? \r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23556, "title": "Persistent Concat op bug with mkl dnn", "body": "This issue appears to be related to #17494 which was closed.  I am opening a new issue to return attention to this.\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, and I have produced a boiled down script that can reproduce the problem standalone \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux (RH family), running on Intel Xeon Phi (KNL)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, from  intel's build: https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: Observed in 3.5 and 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen using the concat operation with mkl_dnn, the operation fails if run after a convolution with the following error message:\r\n\r\n```\r\ntensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_concat_op.cc:814 : Aborted: Operation received an exception:Status: 5, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:811\r\nINFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: Operation received an exception:Status: 5, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:811\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should not fail to create a concat primitive descriptor\r\n\r\n**Code to reproduce the issue**\r\nPlease see this gist with the bare minimum python script to reproduce this issue: https://gist.github.com/coreyjadams/35e89feb1fb191c2788792f6268448fc\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@coreyjadams  We fixed the concat descriptor and it has been merged to the master https://github.com/tensorflow/tensorflow/pull/22606  Let us know if this fixes the problem.", "@coreyjadams I have verified with your test script.", "Sorry for some delay - our compute nodes were down most of last week and I couldn't do any testing.\r\n\r\nI am trying to test this but I can not build the master branch of tensorflow - I get an error of `ModuleNotFoundError: No module named 'keras_preprocessing'`.  I have all of the packages suggested here (https://stackoverflow.com/questions/51771039/error-compiling-tensorflow-from-source-no-module-named-keras-applications/51774943#51774943)\r\n\r\nShould I open an new issue about this keras_preprocessing issue? FWIW, the build command I am using is \r\n```\r\n../bazel-0.18.1/output/bazel build --config=mkl -c opt --copt=-g --strip=never --copt='-Wl,rpath=/opt/gcc/7.3.0/snos' --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er --copt='-mtune=knl' --copt=\"-DEIGEN_USE_VML\" --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n", "Ok.  The build was a pain, but eventually I succeeded.  I can confirm that my test script runs successfully on a Xeon Phi KNL with mkl-dnn, so that's great!\r\n\r\nI'm going to close this issue since the fix is working."]}, {"number": 23555, "title": "slim resnet batchnorm missing", "body": "I just did a MWE of resnet v1 (see code below). However, when I inspect the graph in tensorboard, I could not see the batch norm layers.\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nimport tensorflow.contrib.slim.nets \r\nimport numpy as np\r\n\r\ninput = tf.placeholder(shape=[None, 224, 224, 3], dtype=tf.float32)\r\nresnet = tf.contrib.slim.nets.resnet_v1.resnet_v1_50(input, 1000, is_training=True)\r\ntf.summary.histogram(\"resnet\", resnet[0])\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    \r\n    train_writer = tf.summary.FileWriter( './tflogs ', sess.graph)\r\n    \r\n    merge = tf.summary.merge_all()\r\n    \r\n    summary, out = sess.run([merge, resnet], feed_dict={input: np.random.random((2, 224, 224, 3))})\r\n    \r\n    train_writer.add_summary(summary)\r\n```", "comments": ["You should use the resnet_arg_scope see\r\nhttps://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/contrib/slim/python/slim/nets/resnet_v1.py#L43", "```\r\ninput = tf.placeholder(shape=[None, 224, 224, 3], dtype=tf.float32)\r\nwith tf.contrib.slim.arg_scope(tf.contrib.slim.nets.resnet_v1. resnet_arg_scope()):\r\n  resnet = tf.contrib.slim.nets.resnet_v1.resnet_v1_50(input, 1000, is_training=True)\r\n```"]}, {"number": 23554, "title": "Build of tensorflow==1.12.0 on broadwell fails", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm unable to build tensorflow for CPU from source.\r\nI'm using command `bazel build --verbose_failures -c opt --copt=-march=native --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package`\r\nFor tensorflow==1.12.0 I'm getting:\r\n```\r\nINFO: From Compiling tensorflow/contrib/lite/toco/import_tensorflow.cc:\r\nIn file included from external/gemmlowp/fixedpoint/fixedpoint.h:874:0,\r\n                 from ./tensorflow/contrib/lite/kernels/internal/common.h:48,\r\n                 from ./tensorflow/contrib/lite/toco/runtime/types.h:18,\r\n                 from ./tensorflow/contrib/lite/toco/model.h:28,\r\n                 from ./tensorflow/contrib/lite/toco/import_tensorflow.h:20,\r\n                 from tensorflow/contrib/lite/toco/import_tensorflow.cc:15:\r\nexternal/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n struct FixedPointRawTypeTraits<__m128i> {\r\n                                       ^\r\nINFO: From Compiling tensorflow/python/framework/fast_tensor_util.cpp:\r\nIn file included from bazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/ndarraytypes.h:1821:0,\r\n                 from bazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/ndarrayobject.h:18,\r\n                 from bazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/arrayobject.h:4,\r\n                 from bazel-out/k8-fastbuild/genfiles/tensorflow/python/framework/fast_tensor_util.cpp:581:\r\nbazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n #warning \"Using deprecated NumPy API, disable it by \" \\\r\n  ^~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4582.121s, Critical Path: 791.83s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 7555 processes: 4 local, 7551 processwrapper-sandbox.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n./configure\r\nbazel build -c opt --copt=-march=native --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nI'm trying this on the GCE, 32 core broadwell machine in the `us-central1-f` zone.\r\n\r\n**Any other info / logs**\r\nI've also tried to build the package by:\r\n```\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n./configure\r\nbazel build --verbose_failures --config=opt -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nGetting exactly the sameproblem.\r\n\r\n**EDIT:**\r\nI've tried exactly the same bazel build command with tensorflow==1.11.0 and everything works well. Unfortunately I'd like to use tensorflow==1.12.0 this way. Are there some specific things I've to change for building tensorflow==1.12.0?\r\n\r\n**EDIT2:**\r\nAfter a bit of searching and trying different stuff, such as installing `libc-ares-dev`. I was able to find in the stack trace following error:\r\n```\r\nERROR: /tensorflow/tensorflow/python/BUILD:3865:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -shared -o bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script bazel-out/host/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds ... (remaining 66 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_begin'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_next'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_find'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_read'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_istream_from_buffer'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_varint'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_tag'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_skip_field'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_make_string_substream'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_close_string_substream'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_noinit'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_delimited'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_svarint'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_fixed32'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_fixed64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_ostream_from_buffer'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_write'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_varint'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_svarint'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_fixed32'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_fixed64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_tag'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_tag_for_field'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_get_encoded_size'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_string'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_submessage'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_delimited'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "comments": ["Resolved I'm able to build tensorflow==1.12.0 using bazet 0.18.1. It does not work with bazel 0.19.0 (tensorflow==1.11.0 works also with bazel 0.19.0)", "@ziky90 \r\nSorry to bother, I tried to build tensorflow==1.12.0 with bazel 0.19.0 but failed.\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Processed legacy workspace file d:\\tensorflow-1.12.0/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nERROR: SymlinkDirectories(C:\\Users\\Administrator/_bazel_Administrator/install/9b4a3d8963d27679daf2ef8226ad0888, c:\\users\\administrator\\_bazel_administrator\\pve46h2d/install): CreateJunction:\r\nFATAL: failed to create installation symlink 'c:\\users\\administrator\\_bazel_administrator\\pve46h2d/install': success\r\n```", "I was able to build using bazel 0.18.1, python3.6.\r\n", "Able to get successful compilation using the following setup:\r\nGoogle App Engine n1-standard-4 (4 vCPUs, 15 GB memory)\r\nUbuntu 18.04.2 LTS\r\nBazel: 0.18.1\r\nPython: 3.6\r\ntf: r1.12\r\n\r\ndifferent python version or bazel version didn't work for me"]}, {"number": 23553, "title": "Relationships between Grappler, GraphOptimizer and GraphOptimizationPass", "body": "There are so many graph optimization tools in TensorFlow Runtime such as `Grappler`, `GraphOptimizer` and `GraphOptimizationPass`. This makes me confused.  Is there any plan to unify these interfaces\uff1fCurrently, it seems that these optimizers could not replace with each other.\r\n\r\n`Grappler` is very light weight because it does graph optimization on `GraphDef` .The limitation is that  these `optimizers` can only run after `Placer.run()`.  However,  some optimizers in `Grappler` will rewrite placement information. Therefore,  this will possibly cause some placement errors without the protection of `Placer`(Is it reasonable to  run `Placer` after running grappler?). Another, the `FeedBack` function is not being used in framework.\r\n\r\n`GraphOptimizationPass` does optimization on `Graph` object. The advantage is that these optimizers can be specified with execution stage(PRE_PLACEMENT, POST_PLACEMENT......) .This is necessary. I think the execution stage of graph optimizers should be treated differently. Some optimizers should be done before placement and the other may be suitable for post partitioning and etc. \r\n\r\nAs for `GraphOptimizer`, I heard that it will not be updated in the future. \r\n\r\nIt will be nice to unify them and bring their strength together.  @caisq ", "comments": ["Could you please refer [this](https://www.tensorflow.org/guide/graph_optimization) Graph Optimization documentation in the recent Tensorflow version and compare the different Graph Optimizer, If you still feel the issue is not resolved feel free to open a new issue."]}, {"number": 23552, "title": "if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?", "body": "I want to have a dynamic input in my net, so, I set the input with 2 unknown dim, (1:batch, 2:sequencelength, 3:feature dims),and I defined another variable to specify the real shape of input in forwarding.\r\ni have a test as followings:\r\n\r\n    a = tf.placeholder(tf.float32, shape=[None, None, 40], name = \"tensor_a\") \r\n    b = tf.placeholder(tf.int32, shape=[2], name = \"tensor_b\")  # b[0] indicate the sequence length\r\n\r\n    split_tensor_a = split_a(a, b)\r\n    sess = tf.Session()\r\n\r\n    array_a = np.logspace(1.0, 2.0, num = 120).reshape(1,3,40)\r\n    array_b = np.array([3, 40])\r\n    feed_dict = {a: array_a, b: array_b}\r\n    split_b_value = sess.run(split_tensor_a, feed_dict = feed_dict)\r\n\r\n\r\nand the py_func is:\r\n\r\n    def split_a(tensor_a, tensor_b):\r\n        sp_tensor =tf.py_func(_split_a, [tensor_a, tensor_b], tf.float32, \"split\")\r\n        return sp_tensor\r\n\r\n    def _split_a(a, b):\r\n        tensor_tup = ()\r\n        newsp_a = np.reshape(a, (-1, b[0], a.shape[-1]))\r\n        for n in range(b[0]):\r\n            tensor_tup = tensor_tup + (newsp_a[:, n, :],)\r\n        print(tensor_tup)\r\n        return tensor_tup\r\n\r\nIn _split_a I can get the value of tensor_tup, but when I run the sess, I got an error:\r\n`InvalidArgumentError (see above for traceback): pyfunc_0 returns 3 values, but expects to see 1 values.\r\n\t [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_INT32], Tout=[DT_FLOAT], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_tensor_a_0_0, _arg_tensor_b_0_1)]]\r\n`\r\nI want to know how to split the unknown dim of tensor?\r\nIf the net has 2 unknown dim,how to deal with the input data by one timestep?\r\nThe net has cnn layer and lstm layer, I donot want to change the input shape to (-1, featdim) in cnn layer, but it can only do forward by one time step,  the follwing lstm layer must have shape of (batch, numstep, featdim), so, what should I do ?\r\nCan anyone help me?\r\nThanks~", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23551, "title": "[Resolved] Inconsistent output shape for tf.image.resize_images() when preserve_aspect_ratio=True", "body": "TF Version: r1.11\r\n\r\nI found a bug in `tf.image.resize_images() ` when specifying `preserve_aspect_ratio=True`.\r\nThis is how to reproduce the problem.\r\n\r\n```python\r\ndef resize_images_issue(img_shape, target):\r\n    img_ph = tf.placeholder(tf.uint8, name=\"img\", shape=[None, None, 3])\r\n    img = tf.image.resize_images(img_ph, [target, target], preserve_aspect_ratio=True)\r\n\r\n    with tf.Session() as sess:\r\n        img_np  = np.zeros(img_shape, dtype=np.uint8)\r\n        result = sess.run(img, feed_dict={img_ph: img_np})\r\n\r\n    if np.max(result.shape) != target:\r\n        print(\"Error!! Length of the longer edge is {} though target size is {}\".format(np.max(result.shape), target))\r\n    else:\r\n        print(\"ok\")\r\n```\r\n\r\nFor some sets of input shape and output shape, it returns inconsistent shape.\r\n\r\n```python\r\nresize_images_issue((200, 100, 3), 50)\r\n# ok\r\n\r\nresize_images_issue((437, 779, 3), 225)\r\n# ok\r\n\r\nresize_images_issue((437, 779, 3), 224)\r\n# Error!! Length of the longer edge is 223 though target size is 224\r\n```", "comments": ["I found this bug is fixed in r1.12 \ud83d\udc4d \r\nhttps://github.com/tensorflow/tensorflow/commit/7cf39dde90f83e584d14ce1c371ff17477a1e57e\r\n\r\nSince it is not the stable version currently, we can solve this problem by using the method in r1.12 :)\r\nhttps://github.com/tensorflow/tensorflow/blob/7cf39dde90f83e584d14ce1c371ff17477a1e57e/tensorflow/python/ops/image_ops_impl.py#L944\r\n"]}, {"number": 23550, "title": "ERROR: Unrecognized option: --input_file=/Users/jyy/model/frozen_inference_graph.pb", "body": "Hello, I trained the \". pb\" file on the cloud server through ssdmobilenet. Now I want to convert it into tflite format, but it failed, and the following error occurred. This is my code. Who can help me? Thank you.\r\n\r\nbazel run --config=opt tensorflow/contrib/lite/toco:toco --input_file=/Users/jyy/model/frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/Users/jyy/wechat_model/wechat.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=predictions --input_shapes=1,224,224,3\r\nERROR: Unrecognized option: --input_file=/Users/jyy/model/frozen_inference_graph.pb\r\n\r\n", "comments": ["@wangyongqi \r\nThe official tutorial is below.\r\nhttps://www.tensorflow.org/lite/convert/cmdline_examples\r\n\r\nIf you do not mention environmental information, no one can answer accurately.\r\n[example] OS, Hardware, Tensorflow version, pip or source build", "@PINTO0309 sorry,I seem to have found the reason. At present, there are few OPS officially supported by tflite, so many complex models can not run on tflite. At present, only the mobilenet model has been successfully converted. Inception V3, ssd-mobilenet and other models can not be successfully converted by toco, because the op used in the model does not support it. I use the ssd-mobilenet model.What do you mean?", "@wangyongqi\r\nThe error message was \"Unrecognized option: --input_file\".\r\nHowever, according to the official tutorial, it seems that you specified a .pb file with the option \"--graph_def_file\".\r\nBy the way, the official tutorial is not a \"toco\" binary.\r\nI do not know the type of script you solved, but it was good that the problem was resolved.\r\n", "@wangyongqi  Feel free to close this issue if it is resolved. Also please post any questions(if you have) on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) if it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@harshini-gadige okay\uff0cI know. Thanks.", "when I use  mobile_ssd\uff0c this erro `Unrecognized option: --input_file=` still exist @wangyongqi "]}, {"number": 23549, "title": "Can the ops in pb files be fully supported by android tensorflow  Mobile ?", "body": "It seems tensorflow mobile(android_tensorflow_lib) contains only part of the  core/kenerns files.\r\nIf it is not fully supported ,  can we know how many ops not supported in Tensorflow mobile\uff1f", "comments": ["You can refer this [document](https://www.tensorflow.org/lite/tf_ops_compatibility) to know more about TF mobile supported ops.", "Are inference  ops on Tensorflow mobile \uff08not tensorflow lite\uff09 the same with inference ops in a desktop environment?\r\nCan all the tensorFlow model that's successfully working in a desktop environment also sucessfully working  on a mobile application with Tensorflow mobile?\uff08not  with tensorflow lite\uff09,", "> Can all the tensorFlow model that's successfully working in a desktop environment also sucessfully working on a mobile application with Tensorflow mobile?\r\n\r\nUnfortunately, not always. TensorFlow Mobile only supports a subset of the desktop ops, kernels and types. At the moment, there's no way to know with 100% certainty a priori that your model will run with TensorFlow Mobile short of actually running it. As an example, take a look at the [Android ops listing](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/kernels/BUILD?rcl=221644777&l=5241)."]}, {"number": 23548, "title": "update references and unify citation style", "body": "Refer to published work if possible.\r\nUse (author-year) style in text and have links in `References` field of docs.", "comments": ["Nagging Reviewer @martinwicke: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Are you going to go through all of the docs? That would be nice.", "@martinwicke I don't have much time currently, but if I don't forget it, I could definitely give it a try!"]}, {"number": 23547, "title": "How to make output sequence length equal to the input sequence length in seq2seq model?", "body": "I am trying to formulate a resource mapping problem using seq2seq model where the input sequence length varies and the output sequence length must be equal to the input sequence length.\r\nFrom the manual of seq2deq in tensorflow, it seems that the output sequence length is determined by the end token or the maximal time step. So how can I make output length exactly equal to input length when building the seq2seq model?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23546, "title": "Add absl_int128 to abseil libraries", "body": " * absl_int128 is referenced by str_format_internal", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 23544, "title": "tensorflow_probability 0.4 can't work with tensorflow 1.12", "body": "After I upgrade tensorflow to version 1.12. I get the following error message. This error doesn't happen before upgrade.\r\n\r\n>    import tensorflow_probability as tfp;\r\n  File \"C:\\Users\\yi.xie\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_probability\\__init__.py\", line 21, in <module>\r\n    from tensorflow_probability.python import *  # pylint: disable=wildcard-import\r\n  File \"C:\\Users\\yi.xie\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py\", line 22, in <module>\r\n    from tensorflow_probability.python import distributions\r\n  File \"C:\\Users\\yi.xie\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\__init__.py\", line 77, in <module>\r\n    from tensorflow_probability.python.distributions.vector_diffeomixture import quadrature_scheme_softmaxnormal_gauss_hermite\r\n  File \"C:\\Users\\yi.xie\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\vector_diffeomixture.py\", line 28, in <module>\r\n    from tensorflow.contrib.linalg.python.ops import linear_operator_addition as linop_add_lib\r\nModuleNotFoundError: No module named 'tensorflow.contrib.linalg'\r\n\r\npython version: 3.6\r\ntensorflow version: 1.12.0\r\ntensorflow probability version: 0.4.0\r\nos: windows 10", "comments": ["Do check 1.12 release notes:  \"Remove tf.contrib.linalg. tf.linalg should be used instead.\"", "so ths is a bug to fix.", "Please try installing tensorflow-probability==0.5.0-rc1; the 0.5 release of TFP is compatible with TF 1.12. We'll have the final 0.5 release out later today.", "@breadbread1984  Have you tried with tensorflow-probability==0.5.0-rc1 as asked above ?", "Also, TFP 0.5 is out now! ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23543, "title": "Fix bug in image_captioning_with_attention.ipynb", "body": "Fixes #23465 as discussed in the issue's comments. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Signed the CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "@yashk2810 opened a pull request. Hope this looks good! ", "One more change. index_word is integrated into tokenizer. So can you please remove that initialization and change where index_word is used to tokenizer.index_word.\r\n\r\nThanks for doing this!", "@MarkDaoust This notebook will be different than the one in docs after this PR has been merged :smile: ", "LGTM", "> @MarkDaoust This notebook will be different than the one in docs after this PR has been merged\r\n\r\nAcknowledged. I'll re-sync all of these before finalizing the move.", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yashk2810 @aaroey @alextp hey guys, anything I can help with to close this PR? ", "I'm pulling this PR manually now.", "This was merged.", "Thank you! "]}, {"number": 23542, "title": "No C++ symbols exported after built libtensorflow_cc with bazel on windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:v1.11.0\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):0.18.0 official build\r\n- GCC/Compiler version (if compiling from source):VC++ 2015.3 v14.00 (v140) MSVC\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFollowing the official documentation, I use `bazel build -c opt //tensorflow:libtensorflow_cc.so` to build tensorflow with c++ api on windows, since cmake is deprecated now.\r\nEverything goes well, and `libtensorflow_cc.so` is generated in bazel out dir, which can be renamed to .dll, but I didn't find `libtensorflow_cc.lib` which should contain the exported symbols. Furthermore, I use `dumpbin /exports libtensorflow_cc.so` to view all the exported symbols (the output is attached in the next section), only `TFE_*` , `TF_*` and some other symbols are exported. Obviously no **C++** symbols are exported.\r\nI dig into the source code and found that there're extra parameters passed to the compiler which specifying a list of symbols to export on darwin and posix, but windows (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L480-L488). I'm not familiar to Microsoft toolchains, but can similar thing be passed to MSVC if necessary so that we can get a proper libtensorflow_cc with necessary symbols exported on windows?\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nOutput of `dumpbin /exports libtensorflow_cc.so`:\r\n\r\n```\r\nMicrosoft (R) COFF/PE Dumper Version 14.15.26730.0\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n\r\nDump of file F:\\libtensorflow_cc\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\libtensorflow_cc.so\r\n\r\nFile Type: DLL\r\n\r\n  Section contains the following exports for libtensorflow_cc.so\r\n\r\n    00000000 characteristics\r\n    5BDF0174 time date stamp Sun Nov  4 22:25:56 2018\r\n        0.00 version\r\n           1 ordinal base\r\n         233 number of functions\r\n         233 number of names\r\n\r\n    ordinal hint RVA      name\r\n\r\n          1    0 02C79C50 ?DEVICE_CPU@tensorflow@@3QEBDEB\r\n          2    1 02C79C58 ?DEVICE_GPU@tensorflow@@3QEBDEB\r\n          3    2 02C79C60 ?DEVICE_SYCL@tensorflow@@3QEBDEB\r\n          4    3 02618400 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z\r\n          5    4 02618CA0 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@Uhalf@Eigen@@@2@H2HMPEAV52@H@Z\r\n          6    5 02619540 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11NAEBV?$DeviceMemory@N@2@H2HNPEAV52@H@Z\r\n          7    6 02619DE0 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11V?$complex@M@std@@AEBV?$DeviceMemory@V?$complex@M@std@@@2@H3H2PEAV72@H@Z\r\n          8    7 0261A670 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11V?$complex@N@std@@AEBV?$DeviceMemory@V?$complex@N@std@@@2@H3H2PEAV72@H@Z\r\n          9    8 02C86690 ?kDatasetGraphKey@DatasetBase@tensorflow@@2QBDB\r\n         10    9 02C866A0 ?kDatasetGraphOutputNodeKey@DatasetBase@tensorflow@@2QBDB\r\n         11    A 001C8370 TFE_ContextAddFunction\r\n         12    B 001C83C0 TFE_ContextAddFunctionDef\r\n         13    C 001C8490 TFE_ContextAsyncClearError\r\n         14    D 001C84A0 TFE_ContextAsyncWait\r\n         15    E 001C84F0 TFE_ContextClearCaches\r\n         16    F 001C8500 TFE_ContextDisableRunMetadata\r\n         17   10 001C8510 TFE_ContextEnableRunMetadata\r\n         18   11 001C8520 TFE_ContextEndStep\r\n         19   12 001C8530 TFE_ContextExportRunMetadata\r\n         20   13 001C8620 TFE_ContextGetDevicePlacementPolicy\r\n         21   14 001C8630 TFE_ContextListDevices\r\n         22   15 001C86A0 TFE_ContextOptionsSetAsync\r\n         23   16 001C86B0 TFE_ContextOptionsSetConfig\r\n         24   17 001C86C0 TFE_ContextOptionsSetDevicePlacementPolicy\r\n         25   18 001C86D0 TFE_ContextSetAsyncForThread\r\n         26   19 001C8720 TFE_ContextSetServerDef\r\n         27   1A 001C87E0 TFE_ContextSetThreadLocalDevicePlacementPolicy\r\n         28   1B 001C87F0 TFE_ContextStartStep\r\n         29   1C 001C8800 TFE_DeleteContext\r\n         30   1D 001C8830 TFE_DeleteContextOptions\r\n         31   1E 001C8870 TFE_DeleteOp\r\n         32   1F 001BFB40 TFE_DeleteTensorDebugInfo\r\n         33   20 001C88A0 TFE_DeleteTensorHandle\r\n         34   21 001C88F0 TFE_Execute\r\n         35   22 001C8A50 TFE_NewContext\r\n         36   23 001C8C40 TFE_NewContextOptions\r\n         37   24 001C8C80 TFE_NewOp\r\n         38   25 001C8E70 TFE_NewTensorHandle\r\n         39   26 001C8F60 TFE_OpAddInput\r\n         40   27 001C8F70 TFE_OpGetAttrType\r\n         41   28 001C90E0 TFE_OpGetDevice\r\n         42   29 001C9120 TFE_OpNameGetAttrType\r\n         43   2A 001C91A0 TFE_OpSetAttrBool\r\n         44   2B 001C9200 TFE_OpSetAttrBoolList\r\n         45   2C 001C93B0 TFE_OpSetAttrFloat\r\n         46   2D 001C9430 TFE_OpSetAttrFloatList\r\n         47   2E 001C94B0 TFE_OpSetAttrFunction\r\n         48   2F 001C9640 TFE_OpSetAttrFunctionList\r\n         49   30 001C9850 TFE_OpSetAttrInt\r\n         50   31 001C98D0 TFE_OpSetAttrIntList\r\n         51   32 001C9950 TFE_OpSetAttrShape\r\n         52   33 001C9C30 TFE_OpSetAttrShapeList\r\n         53   34 001C9FF0 TFE_OpSetAttrString\r\n         54   35 001CA070 TFE_OpSetAttrStringList\r\n         55   36 001CA1E0 TFE_OpSetAttrType\r\n         56   37 001CA230 TFE_OpSetAttrTypeList\r\n         57   38 001CA2B0 TFE_OpSetDevice\r\n         58   39 001CA300 TFE_OpSetXLACompilation\r\n         59   3A 001BFB90 TFE_TensorDebugInfoOnDeviceDim\r\n         60   3B 001BFBA0 TFE_TensorDebugInfoOnDeviceNumDims\r\n         61   3C 001CA370 TFE_TensorHandleCopyToDevice\r\n         62   3D 001CA420 TFE_TensorHandleDataType\r\n         63   3E 001CA430 TFE_TensorHandleDeviceName\r\n         64   3F 001CA510 TFE_TensorHandleDim\r\n         65   40 001CA5C0 TFE_TensorHandleNumDims\r\n         66   41 001CA670 TFE_TensorHandleResolve\r\n         67   42 001BFBB0 TFE_TensorHandleTensorDebugInfo\r\n         68   43 00372290 TF_AbortWhile\r\n         69   44 003722A0 TF_AddControlInput\r\n         70   45 003722B0 TF_AddGradients\r\n         71   46 00372300 TF_AddGradientsWithPrefix\r\n         72   47 00372BB0 TF_AddInput\r\n         73   48 00372BC0 TF_AddInputList\r\n         74   49 00372E50 TF_AllocateTensor\r\n         75   4A 00372EC0 TF_ApiDefMapGet\r\n         76   4B 00373070 TF_ApiDefMapPut\r\n         77   4C 00373200 TF_CloseDeprecatedSession\r\n         78   4D 00373200 TF_CloseSession\r\n         79   4E 00373250 TF_ColocateWith\r\n         80   4F 00373380 TF_DataTypeSize\r\n         81   50 00373390 TF_DeleteApiDefMap\r\n         82   51 003733C0 TF_DeleteBuffer\r\n         83   52 00373400 TF_DeleteDeprecatedSession\r\n         84   53 00373480 TF_DeleteDeviceList\r\n         85   54 00366220 TF_DeleteFunction\r\n         86   55 003734B0 TF_DeleteGraph\r\n         87   56 00373520 TF_DeleteImportGraphDefOptions\r\n         88   57 00373570 TF_DeleteImportGraphDefResults\r\n         89   58 003735A0 TF_DeleteLibraryHandle\r\n         90   59 003735D0 TF_DeletePRunHandle\r\n         91   5A 003735E0 TF_DeleteSession\r\n         92   5B 00373720 TF_DeleteSessionOptions\r\n         93   5C 00373760 TF_DeleteStatus\r\n         94   5D 003737B0 TF_DeleteTensor\r\n         95   5E 00373820 TF_DeprecatedSessionListDevices\r\n         96   5F 003738B0 TF_DeviceListCount\r\n         97   60 003738E0 TF_DeviceListIncarnation\r\n         98   61 00373A30 TF_DeviceListMemoryBytes\r\n         99   62 00373B80 TF_DeviceListName\r\n        100   63 00373CD0 TF_DeviceListType\r\n        101   64 00373E20 TF_Dim\r\n        102   65 00373E30 TF_ExtendGraph\r\n        103   66 00373EF0 TF_FinishOperation\r\n        104   67 00374250 TF_FinishWhile\r\n        105   68 00366250 TF_FunctionGetAttrValueProto\r\n        106   69 00366430 TF_FunctionImportFunctionDef\r\n        107   6A 00366540 TF_FunctionName\r\n        108   6B 00366570 TF_FunctionSetAttrValueProto\r\n        109   6C 00366760 TF_FunctionToFunctionDef\r\n        110   6D 00374890 TF_GetAllOpList\r\n        111   6E 00374B60 TF_GetAllRegisteredKernels\r\n        112   6F 00374C40 TF_GetBuffer\r\n        113   70 001E4360 TF_GetCode\r\n        114   71 00374C60 TF_GetOpList\r\n        115   72 00374C80 TF_GetRegisteredKernelsForOp\r\n        116   73 003667B0 TF_GraphCopyFunction\r\n        117   74 003669D0 TF_GraphGetFunctions\r\n        118   75 00374D90 TF_GraphGetOpDef\r\n        119   76 00374F20 TF_GraphGetTensorNumDims\r\n        120   77 00375050 TF_GraphGetTensorShape\r\n        121   78 00375260 TF_GraphImportGraphDef\r\n        122   79 003752A0 TF_GraphImportGraphDefWithResults\r\n        123   7A 003753F0 TF_GraphImportGraphDefWithReturnOutputs\r\n        124   7B 00375610 TF_GraphNextOperation\r\n        125   7C 00366B80 TF_GraphNumFunctions\r\n        126   7D 003756D0 TF_GraphOperationByName\r\n        127   7E 003757F0 TF_GraphSetTensorShape\r\n        128   7F 00366BF0 TF_GraphToFunction\r\n        129   80 00375950 TF_GraphToGraphDef\r\n        130   81 00375A10 TF_GraphVersions\r\n        131   82 00375AD0 TF_ImportGraphDefOptionsAddControlDependency\r\n        132   83 00375B00 TF_ImportGraphDefOptionsAddInputMapping\r\n        133   84 00375CF0 TF_ImportGraphDefOptionsAddReturnOperation\r\n        134   85 00375E70 TF_ImportGraphDefOptionsAddReturnOutput\r\n        135   86 00375FC0 TF_ImportGraphDefOptionsNumReturnOperations\r\n        136   87 00375FD0 TF_ImportGraphDefOptionsNumReturnOutputs\r\n        137   88 00376000 TF_ImportGraphDefOptionsRemapControlDependency\r\n        138   89 00376110 TF_ImportGraphDefOptionsSetPrefix\r\n        139   8A 00376130 TF_ImportGraphDefOptionsSetUniquifyNames\r\n        140   8B 00376140 TF_ImportGraphDefOptionsSetUniquifyPrefix\r\n        141   8C 00376150 TF_ImportGraphDefResultsMissingUnusedInputMappings\r\n        142   8D 00376170 TF_ImportGraphDefResultsReturnOperations\r\n        143   8E 00376190 TF_ImportGraphDefResultsReturnOutputs\r\n        144   8F 003761B0 TF_LoadLibrary\r\n        145   90 00376260 TF_LoadSessionFromSavedModel\r\n        146   91 00376890 TF_Message\r\n        147   92 003768C0 TF_NewApiDefMap\r\n        148   93 003769B0 TF_NewBuffer\r\n        149   94 003769E0 TF_NewBufferFromString\r\n        150   95 00376A40 TF_NewDeprecatedSession\r\n        151   96 00376AD0 TF_NewGraph\r\n        152   97 00376B00 TF_NewImportGraphDefOptions\r\n        153   98 00376B70 TF_NewOperation\r\n        154   99 00376BE0 TF_NewSession\r\n        155   9A 00376D30 TF_NewSessionOptions\r\n        156   9B 00376D60 TF_NewStatus\r\n        157   9C 00376D90 TF_NewTensor\r\n        158   9D 00377010 TF_NewWhile\r\n        159   9E 00377430 TF_NumDims\r\n        160   9F 00377440 TF_OperationDevice\r\n        161   A0 00377460 TF_OperationGetAttrBool\r\n        162   A1 00377500 TF_OperationGetAttrBoolList\r\n        163   A2 003775F0 TF_OperationGetAttrFloat\r\n        164   A3 003776A0 TF_OperationGetAttrFloatList\r\n        165   A4 00377790 TF_OperationGetAttrInt\r\n        166   A5 00377840 TF_OperationGetAttrIntList\r\n        167   A6 00377930 TF_OperationGetAttrMetadata\r\n        168   A7 00377F90 TF_OperationGetAttrShape\r\n        169   A8 003780F0 TF_OperationGetAttrShapeList\r\n        170   A9 003783B0 TF_OperationGetAttrString\r\n        171   AA 00378490 TF_OperationGetAttrStringList\r\n        172   AB 00378640 TF_OperationGetAttrTensor\r\n        173   AC 00378730 TF_OperationGetAttrTensorList\r\n        174   AD 00378910 TF_OperationGetAttrTensorShapeProto\r\n        175   AE 003789B0 TF_OperationGetAttrTensorShapeProtoList\r\n        176   AF 00378B60 TF_OperationGetAttrType\r\n        177   B0 00378C00 TF_OperationGetAttrTypeList\r\n        178   B1 00378CF0 TF_OperationGetAttrValueProto\r\n        179   B2 00378D60 TF_OperationGetControlInputs\r\n        180   B3 00378ED0 TF_OperationGetControlOutputs\r\n        181   B4 00379040 TF_OperationInput\r\n        182   B5 003790D0 TF_OperationInputListLength\r\n        183   B6 00379290 TF_OperationInputType\r\n        184   B7 003792A0 TF_OperationName\r\n        185   B8 003792C0 TF_OperationNumControlInputs\r\n        186   B9 003793F0 TF_OperationNumControlOutputs\r\n        187   BA 00379520 TF_OperationNumInputs\r\n        188   BB 00379530 TF_OperationNumOutputs\r\n        189   BC 00379540 TF_OperationOpType\r\n        190   BD 00379560 TF_OperationOutputConsumers\r\n        191   BE 00379700 TF_OperationOutputListLength\r\n        192   BF 003798C0 TF_OperationOutputNumConsumers\r\n        193   C0 003799F0 TF_OperationOutputType\r\n        194   C1 00379A00 TF_OperationToNodeDef\r\n        195   C2 00379A60 TF_PRun\r\n        196   C3 00379E00 TF_PRunSetup\r\n        197   C4 0037A2E0 TF_Reset\r\n        198   C5 0037A2F0 TF_Run\r\n        199   C6 00373820 TF_SessionListDevices\r\n        200   C7 0037A6A0 TF_SessionPRun\r\n        201   C8 0037AB50 TF_SessionPRunSetup\r\n        202   C9 0037B130 TF_SessionRun\r\n        203   CA 0037B5F0 TF_SetAttrBool\r\n        204   CB 0037B640 TF_SetAttrBoolList\r\n        205   CC 0037B7E0 TF_SetAttrFloat\r\n        206   CD 0037B820 TF_SetAttrFloatList\r\n        207   CE 0037B880 TF_SetAttrFuncName\r\n        208   CF 0037BA10 TF_SetAttrInt\r\n        209   D0 0037BA50 TF_SetAttrIntList\r\n        210   D1 0037BAB0 TF_SetAttrShape\r\n        211   D2 0037BBC0 TF_SetAttrShapeList\r\n        212   D3 0037BE20 TF_SetAttrString\r\n        213   D4 0037BE80 TF_SetAttrStringList\r\n        214   D5 0037C150 TF_SetAttrTensor\r\n        215   D6 0037C230 TF_SetAttrTensorList\r\n        216   D7 0037C590 TF_SetAttrTensorShapeProto\r\n        217   D8 0037C720 TF_SetAttrTensorShapeProtoList\r\n        218   D9 0037C9A0 TF_SetAttrType\r\n        219   DA 0037C9E0 TF_SetAttrTypeList\r\n        220   DB 0037CA40 TF_SetAttrValueProto\r\n        221   DC 0037CC70 TF_SetConfig\r\n        222   DD 0037CCD0 TF_SetDevice\r\n        223   DE 0037CD10 TF_SetStatus\r\n        224   DF 0037CDB0 TF_SetTarget\r\n        225   E0 0037CDE0 TF_StringDecode\r\n        226   E1 0037CF00 TF_StringEncode\r\n        227   E2 0037CFE0 TF_StringEncodedSize\r\n        228   E3 0037D000 TF_TensorByteSize\r\n        229   E4 0037D010 TF_TensorData\r\n        230   E5 0037D020 TF_TensorMaybeMove\r\n        231   E6 0037D080 TF_TensorType\r\n        232   E7 0037D090 TF_TryEvaluateConstant\r\n        233   E8 0037D1F0 TF_Version\r\n\r\n  Summary\r\n\r\n      2D4000 .data\r\n        1000 .gfids\r\n      184000 .pdata\r\n      B36000 .rdata\r\n       53000 .reloc\r\n     2949000 .text\r\n        1000 .tls\r\n       11000 _RDATA\r\n```", "comments": ["### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.12\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.15\r\n- **GCC/Compiler version (if compiling from source)**: vc140\r\n- **CUDA/cuDNN version**: CUDA 10 / cuDNN 7.4.1.5\r\n- **GPU model and memory**: GTX 1080Ti / 11 GB\r\n- **Exact command to reproduce**:\r\n\r\nSimilar problem over here. I followed the official instructions to build Tensorflow in Windows using bazel (although I had to make a patch for eigen, as described [here](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2)), and executed `bazel build \u2013config=opt \u2013config=cuda //tensorflow:libtensorflow_cc.so`. The compilation succeded.\r\n\r\nHowever, the problems come when I want to include it in my own project (which I have in Visual Studio). I could not find any official documentation on how to do this; instead I had to search over many blogs or issues... Most of the explanations (if not all) refer to using the cmake build of Tensorflow, which now is not possible to do with the latest version.\r\n\r\nAnyway, I found all the necessary headers around the bazel build. I had to include the following directories:\r\n```\r\nC:\\development\\tensorflow\\bazel-tensorflow\\external\\eigen_archive\r\nC:\\development\\tensorflow\\bazel-tensorflow\\external\\com_google_absl\r\nC:\\development\\tensorflow\\bazel-tensorflow\\external\\protobuf_archive_src\r\nC:\\development\\tensorflow\\bazel-genfiles\r\nC:\\development\\tensorflow\\bazel-tensorflow\r\n```\r\nBut the problem comes when linking...\r\n\r\nWhen I execute `dumpbin /exports libtensorflow_cc.so` I get far more symbols than you (3061). To be able to link them, I had to build the .lib file from libtensorflow_cc.so. I did `dumpbin /EXPORTS libtensorflow_cc.so > myfile.exports`, I copied all the functions names and pasted them in a file called myfile.def (and added a line with the word EXPORTS at the beginning), and then did `lib /def:myfile.def /out:tensorflow.lib /machine:x64`. Then in Visual Studio I included tensorflow.lib.\r\n\r\nHowever, when compiling my project, it is still requiring more symbols than those present in libtensorflow_cc.so. This is the output I get:\r\n\r\n```1>------ Build started: Project: BrainServer, Configuration: Release x64 ------\r\n1>stdafx.cpp\r\n1>BrainServer.cpp\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): warning C4005: 'COMPILER_MSVC': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): note: command-line arguments:  see previous definition of 'COMPILER_MSVC'\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): warning C4005: 'NOMINMAX': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): note: command-line arguments:  see previous definition of 'NOMINMAX'\r\n1>BrainServer.cpp(62): warning C4101: 'm_logLevel': unreferenced local variable\r\n1>BrainServerConfig.cpp\r\n1>ClassificationRunner.cpp\r\n1>c:\\development\\daview\\brainserver\\brainserver\\ClassificationRunner.h(3): warning C4005: 'COMPILER_MSVC': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\ClassificationRunner.h(3): note: command-line arguments:  see previous definition of 'COMPILER_MSVC'\r\n1>c:\\development\\daview\\brainserver\\brainserver\\ClassificationRunner.h(4): warning C4005: 'NOMINMAX': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\ClassificationRunner.h(4): note: command-line arguments:  see previous definition of 'NOMINMAX'\r\n1>Crop.cpp\r\n1>SharedListsManager.cpp\r\n1>ImageSaver.cpp\r\n1>NetworkRunner.cpp\r\n1>c:\\development\\daview\\brainserver\\brainserver\\NetworkRunner.h(3): warning C4005: 'COMPILER_MSVC': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\NetworkRunner.h(3): note: command-line arguments:  see previous definition of 'COMPILER_MSVC'\r\n1>c:\\development\\daview\\brainserver\\brainserver\\NetworkRunner.h(4): warning C4005: 'NOMINMAX': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\NetworkRunner.h(4): note: command-line arguments:  see previous definition of 'NOMINMAX'\r\n1>NetworkRunner.cpp(205): warning C4101: 'ex': unreferenced local variable\r\n1>Query.cpp\r\n1>Server.cpp\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): warning C4005: 'COMPILER_MSVC': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): note: command-line arguments:  see previous definition of 'COMPILER_MSVC'\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): warning C4005: 'NOMINMAX': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): note: command-line arguments:  see previous definition of 'NOMINMAX'\r\n1>Session.cpp\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): warning C4005: 'COMPILER_MSVC': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): note: command-line arguments:  see previous definition of 'COMPILER_MSVC'\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): warning C4005: 'NOMINMAX': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): note: command-line arguments:  see previous definition of 'NOMINMAX'\r\n1>TamperingRunner.cpp\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): warning C4005: 'COMPILER_MSVC': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(3): note: command-line arguments:  see previous definition of 'COMPILER_MSVC'\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): warning C4005: 'NOMINMAX': macro redefinition\r\n1>c:\\development\\daview\\brainserver\\brainserver\\TamperingRunner.h(4): note: command-line arguments:  see previous definition of 'NOMINMAX'\r\n1>tinystr.cpp\r\n1>tinyxml.cpp\r\n1>tinyxmlerror.cpp\r\n1>tinyxmlparser.cpp\r\n1>BrainServer.obj : error LNK2001: unresolved external symbol \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)\" (?GetVarint32PtrFallback@core@tensorflow@@YAPEBDPEBD0PEAI@Z)\r\n1>BrainServer.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Tensor::~Tensor(void)\" (??1Tensor@tensorflow@@QEAA@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: void __cdecl google::protobuf::internal::LogFinisher::operator=(class google::protobuf::internal::LogMessage &)\" (??4LogFinisher@internal@protobuf@google@@QEAAXAEAVLogMessage@123@@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: class google::protobuf::internal::LogMessage & __cdecl google::protobuf::internal::LogMessage::operator<<(char const *)\" (??6LogMessage@internal@protobuf@google@@QEAAAEAV0123@PEBD@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl google::protobuf::internal::LogMessage::LogMessage(enum google::protobuf::LogLevel,char const *,int)\" (??0LogMessage@internal@protobuf@google@@QEAA@W4LogLevel@23@PEBDH@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl google::protobuf::internal::LogMessage::~LogMessage(void)\" (??1LogMessage@internal@protobuf@google@@QEAA@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)\" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)\" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"private: void __cdecl tensorflow::TensorShapeRep::SlowCopyFrom(class tensorflow::TensorShapeRep const &)\" (?SlowCopyFrom@TensorShapeRep@tensorflow@@AEAAXAEBV12@@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"private: void __cdecl tensorflow::TensorShapeRep::DestructorOutOfLine(void)\" (?DestructorOutOfLine@TensorShapeRep@tensorflow@@AEAAXXZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Tensor::Tensor(void)\" (??0Tensor@tensorflow@@QEAA@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Tensor::Tensor(enum tensorflow::DataType,class tensorflow::TensorShape const &)\" (??0Tensor@tensorflow@@QEAA@W4DataType@1@AEBVTensorShape@1@@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::TensorShapeBase<class tensorflow::TensorShape>::TensorShapeBase<class tensorflow::TensorShape>(void)\" (??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::TensorShapeBase<class tensorflow::TensorShape>::TensorShapeBase<class tensorflow::TensorShape>(class absl::Span<__int64 const >)\" (??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)\" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)\" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: class std::basic_ostream<char,struct std::char_traits<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::ForVar2(void)\" (?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::NewString(void)\" (?NewString@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"private: void __cdecl tensorflow::Tensor::CheckIsAlignedAndSingleElement(void)const \" (?CheckIsAlignedAndSingleElement@Tensor@tensorflow@@AEBAXXZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"private: static class absl::InlinedVector<__int64,4,class std::allocator<__int64> > __cdecl tensorflow::Tensor::ComputeFlatInnerDims(class absl::Span<__int64 const >,__int64)\" (?ComputeFlatInnerDims@Tensor@tensorflow@@CA?AV?$InlinedVector@_J$03V?$allocator@_J@std@@@absl@@V?$Span@$$CB_J@4@_J@Z)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"public: class absl::InlinedVector<__int64,4,class std::allocator<__int64> > __cdecl tensorflow::TensorShapeBase<class tensorflow::TensorShape>::dim_sizes(void)const \" (?dim_sizes@?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEBA?AV?$InlinedVector@_J$03V?$allocator@_J@std@@@absl@@XZ)\r\n1>ClassificationRunner.obj : error LNK2001: unresolved external symbol \"private: void __cdecl tensorflow::Tensor::CheckTypeAndIsAligned(enum tensorflow::DataType)const \" (?CheckTypeAndIsAligned@Tensor@tensorflow@@AEBAXW4DataType@2@@Z)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::GraphDef::GraphDef(void)\" (??0GraphDef@tensorflow@@QEAA@XZ)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"public: virtual __cdecl tensorflow::GraphDef::~GraphDef(void)\" (??1GraphDef@tensorflow@@UEAA@XZ)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"public: __int64 __cdecl tensorflow::TensorShapeBase<class tensorflow::TensorShape>::dim_size(int)const \" (?dim_size@?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEBA_JH@Z)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"public: virtual __cdecl tensorflow::ConfigProto::~ConfigProto(void)\" (??1ConfigProto@tensorflow@@UEAA@XZ)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::ReadBinaryProto(class tensorflow::Env *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class google::protobuf::MessageLite *)\" (?ReadBinaryProto@tensorflow@@YA?AVStatus@1@PEAVEnv@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVMessageLite@protobuf@google@@@Z)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"public: static class tensorflow::Env * __cdecl tensorflow::Env::Default(void)\" (?Default@Env@tensorflow@@SAPEAV12@XZ)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::ReadTextProto(class tensorflow::Env *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class google::protobuf::Message *)\" (?ReadTextProto@tensorflow@@YA?AVStatus@1@PEAVEnv@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVMessage@protobuf@google@@@Z)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"private: static class tensorflow::GPUOptions * __cdecl google::protobuf::Arena::CreateMaybeMessage<class tensorflow::GPUOptions>(class google::protobuf::Arena *)\" (??$CreateMaybeMessage@VGPUOptions@tensorflow@@$$V@Arena@protobuf@google@@CAPEAVGPUOptions@tensorflow@@PEAV012@@Z)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"private: void __cdecl tensorflow::TensorShape::CheckDimsEqual(int)const \" (?CheckDimsEqual@TensorShape@tensorflow@@AEBAXH@Z)\r\n1>NetworkRunner.obj : error LNK2001: unresolved external symbol \"private: void __cdecl tensorflow::TensorShape::CheckDimsAtLeast(int)const \" (?CheckDimsAtLeast@TensorShape@tensorflow@@AEBAXH@Z)\r\n1>C:\\development\\daview\\BrainServer\\x64\\Release\\BrainServer.exe : fatal error LNK1120: 34 unresolved externals\r\n1>Done building project \"BrainServer.vcxproj\" -- FAILED.\r\n========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========\r\n```\r\n\r\nIn any case, it would be appreciated if there were some instructions on how to use the C++ API of Tensorflow in an independent project (not having to build it with bazel).", "Same problem here:\r\nCompiled v1.11.0 from source on Windows 10 using Bazel 0.18.1 with Python 3.5.4. \r\nUsing `bazel build --config=opt //tensorflow:libtensorflow_cc.so` and renamed the so to .dll, and linked against the .lib. \r\n\r\nI get the following linker errors when linking against it:\r\n```\r\n0>E:\\VS\\VC\\bin\\x86_amd64\\link.exe /ERRORREPORT:PROMPT /OUT:\"D:\\dev\\sources\\quantib-dependencies\\tensorflow\\inference\\source\\build\\Release\\inference_cc.exe\" /INCREMENTAL:NO /NOLOGO \"D:\\dev\\sources\\quantib-dependencies\\tensorflow\\source\\bazel-bin\\tensorflow\\bin\\libtensorflow_cc.lib\" \"D:\\dev\\sources\\quantib-dependencies\\tensorflow\\build\\deps\\protobuf\\bin\\lib\\libprotobuf.lib\" kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTUAC:\"level='asInvoker' uiAccess='false'\" /manifest:embed /PDB:\"D:/dev/sources/quantib-dependencies/tensorflow/inference/source/build/Release/inference_cc.pdb\" /SUBSYSTEM:CONSOLE /TLBID:1 /DYNAMICBASE /NXCOMPAT /IMPLIB:\"D:/dev/sources/quantib-dependencies/tensorflow/inference/source/build/Release/inference_cc.lib\" /MACHINE:X64  /machine:x64 inference_cc.dir\\Release\\inference_cc.obj\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::MetaGraphDef::MetaGraphDef(void)\" (??0MetaGraphDef@tensorflow@@QEAA@XZ) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: virtual __cdecl tensorflow::MetaGraphDef::~MetaGraphDef(void)\" (??1MetaGraphDef@tensorflow@@UEAA@XZ) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)\" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z) referenced in function \"private: void __cdecl tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1>(class absl::Span<__int64 const >,class std::array<__int64,1> *)const \" (??$FillDimsAndValidateCompatibleShape@$00@Tensor@tensorflow@@AEBAXV?$Span@$$CB_J@absl@@PEAV?$array@_J$00@std@@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)\" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ) referenced in function \"private: void __cdecl tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1>(class absl::Span<__int64 const >,class std::array<__int64,1> *)const \" (??$FillDimsAndValidateCompatibleShape@$00@Tensor@tensorflow@@AEBAXV?$Span@$$CB_J@absl@@PEAV?$array@_J$00@std@@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)\" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z) referenced in function \"private: void __cdecl tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1>(class absl::Span<__int64 const >,class std::array<__int64,1> *)const \" (??$FillDimsAndValidateCompatibleShape@$00@Tensor@tensorflow@@AEBAXV?$Span@$$CB_J@absl@@PEAV?$array@_J$00@std@@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)\" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ) referenced in function \"private: void __cdecl tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1>(class absl::Span<__int64 const >,class std::array<__int64,1> *)const \" (??$FillDimsAndValidateCompatibleShape@$00@Tensor@tensorflow@@AEBAXV?$Span@$$CB_J@absl@@PEAV?$array@_J$00@std@@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: class std::basic_ostream<char,struct std::char_traits<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::ForVar2(void)\" (?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ) referenced in function \"private: void __cdecl tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1>(class absl::Span<__int64 const >,class std::array<__int64,1> *)const \" (??$FillDimsAndValidateCompatibleShape@$00@Tensor@tensorflow@@AEBAXV?$Span@$$CB_J@absl@@PEAV?$array@_J$00@std@@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::NewString(void)\" (?NewString@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ) referenced in function \"private: void __cdecl tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1>(class absl::Span<__int64 const >,class std::array<__int64,1> *)const \" (??$FillDimsAndValidateCompatibleShape@$00@Tensor@tensorflow@@AEBAXV?$Span@$$CB_J@absl@@PEAV?$array@_J$00@std@@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)\" (?GetVarint32PtrFallback@core@tensorflow@@YAPEBDPEBD0PEAI@Z) referenced in function \"char const * __cdecl tensorflow::core::GetVarint32Ptr(char const *,char const *,unsigned int *)\" (?GetVarint32Ptr@core@tensorflow@@YAPEBDPEBD0PEAI@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::Status::ToString(void)const \" (?ToString@Status@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ) referenced in function \"public: bool __cdecl tensorflow::Status::operator==(class tensorflow::Status const &)const \" (??8Status@tensorflow@@QEBA_NAEBV01@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"private: void __cdecl tensorflow::Status::SlowCopyFrom(struct tensorflow::Status::State const *)\" (?SlowCopyFrom@Status@tensorflow@@AEAAXPEBUState@12@@Z) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::TfCheckOpHelperOutOfLine(class tensorflow::Status const &,char const *)\" (?TfCheckOpHelperOutOfLine@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVStatus@1@PEBD@Z) referenced in function main\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"private: void __cdecl tensorflow::TensorShapeRep::DestructorOutOfLine(void)\" (?DestructorOutOfLine@TensorShapeRep@tensorflow@@AEAAXXZ) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"private: void __cdecl tensorflow::TensorShapeRep::SlowCopyFrom(class tensorflow::TensorShapeRep const &)\" (?SlowCopyFrom@TensorShapeRep@tensorflow@@AEAAXAEBV12@@Z) referenced in function \"public: __cdecl std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor>::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor>(struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor> const &)\" (??0?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensor@tensorflow@@@std@@QEAA@AEBU01@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::TensorShapeBase<class tensorflow::TensorShape>::TensorShapeBase<class tensorflow::TensorShape>(class absl::Span<__int64 const >)\" (??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z) referenced in function main\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::TensorShapeBase<class tensorflow::TensorShape>::TensorShapeBase<class tensorflow::TensorShape>(void)\" (??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@XZ) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Tensor::Tensor(enum tensorflow::DataType,class tensorflow::TensorShape const &)\" (??0Tensor@tensorflow@@QEAA@W4DataType@1@AEBVTensorShape@1@@Z) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Tensor::~Tensor(void)\" (??1Tensor@tensorflow@@QEAA@XZ) referenced in function \"void __cdecl std::_Destroy_range1<class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor> >,struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor> *>(struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor> *,struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor> *,struct std::_Wrap_alloc<class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::Tensor> > > &,struct std::integral_constant<bool,0>)\" (??$_Destroy_range1@V?$allocator@U?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensor@tensorflow@@@std@@@std@@PEAU?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensor@tensorflow@@@2@@std@@YAXPEAU?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensor@tensorflow@@@0@0AEAU?$_Wrap_alloc@V?$allocator@U?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensor@tensorflow@@@std@@@std@@@0@U?$integral_constant@_N$0A@@0@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::Tensor::DebugString(void)const \" (?DebugString@Tensor@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ) referenced in function main\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"private: void __cdecl tensorflow::Tensor::CheckTypeAndIsAligned(enum tensorflow::DataType)const \" (?CheckTypeAndIsAligned@Tensor@tensorflow@@AEBAXW4DataType@2@@Z) referenced in function \"public: class Eigen::TensorMap<class Eigen::Tensor<float,1,1,__int64>,16,struct Eigen::MakePointer> __cdecl tensorflow::Tensor::shaped<float,1>(class absl::Span<__int64 const >)\" (??$shaped@M$00@Tensor@tensorflow@@QEAA?AV?$TensorMap@V?$Tensor@M$00$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@V?$Span@$$CB_J@absl@@@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"private: void __cdecl tensorflow::Tensor::CheckIsAlignedAndSingleElement(void)const \" (?CheckIsAlignedAndSingleElement@Tensor@tensorflow@@AEBAXXZ) referenced in function \"public: class Eigen::TensorMap<class Eigen::TensorFixedSize<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct Eigen::Sizes<>,1,__int64>,16,struct Eigen::MakePointer> __cdecl tensorflow::Tensor::scalar<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(void)\" (??$scalar@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Tensor@tensorflow@@QEAA?AV?$TensorMap@V?$TensorFixedSize@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$Sizes@$S@Eigen@@$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@XZ)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: static class tensorflow::Env * __cdecl tensorflow::Env::Default(void)\" (?Default@Env@tensorflow@@SAPEAV12@XZ) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::ReadBinaryProto(class tensorflow::Env *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class google::protobuf::MessageLite *)\" (?ReadBinaryProto@tensorflow@@YA?AVStatus@1@PEAVEnv@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVMessageLite@protobuf@google@@@Z) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: virtual __cdecl tensorflow::ConfigProto::~ConfigProto(void)\" (??1ConfigProto@tensorflow@@UEAA@XZ) referenced in function main\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ) referenced in function main\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &,class tensorflow::Session * *)\" (?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z) referenced in function main\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"class tensorflow::GraphDefDefaultTypeInternal tensorflow::_GraphDef_default_instance_\" (?_GraphDef_default_instance_@tensorflow@@3VGraphDefDefaultTypeInternal@1@A) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>inference_cc.obj(0,0): Error LNK2019: unresolved external symbol \"class tensorflow::SaverDefDefaultTypeInternal tensorflow::_SaverDef_default_instance_\" (?_SaverDef_default_instance_@tensorflow@@3VSaverDefDefaultTypeInternal@1@A) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n0>D:\\dev\\sources\\quantib-dependencies\\tensorflow\\inference\\source\\build\\Release\\inference_cc.exe(0,0): Error LNK1120: 28 unresolved externals\r\n0>------- Project finished: inference_cc. Succeeded: False. Errors: 29. Warnings: 0\r\n```\r\n\r\nConfiguration used for ./configure:\r\n```\r\nset PYTHON_BIN_PATH=%SCRIPT_DIR%\\venv\\Scripts\\python.exe\r\nset PYTHON_LIB_PATH=%SCRIPT_DIR%\\venv\\lib\\site-packages\\\r\n\r\nset TF_NEED_GCP=0\r\nset TF_NEED_CUDA=0\r\nset TF_NEED_AWS=0\r\nset TF_NEED_JEMALLOC=0\r\nset TF_NEED_GCP=0\r\nset TF_NEED_HDFS=0\r\nset TF_NEED_KAFKA=0\r\nset TF_ENABLE_XLA=0\r\nset TF_NEED_GDR=0\r\nset TF_NEED_VERBS=0\r\nset TF_NEED_NGRAPH=0\r\nset TF_CUDA_CLANG=0\r\nset TF_NEED_OPENCL_SYCL=0\r\nset TF_NEED_MPI=0\r\nset TF_DOWNLOAD_CLANG=0\r\nset CC_OPT_FLAGS=\"/arch:AVX2\"\r\nset TF_SET_ANDROID_WORKSPACE=0\r\nset TF_OVERRIDE_EIGEN_STRONG_INLINE=1\r\n```\r\nWhat I find weird is: I compiled it with the same settings on Ubuntu 16.04 with Python 3.6.7 and there the libtensorflow.so file is around 600MB in size, while on Windows only 60 Mb. So makes sense that we miss some symbols. \r\n", "In addition, I solved my problem by using the approach [here](https://github.com/tensorflow/tensorflow/issues/22047#issuecomment-421452033). I added the following symbols to the `tf_exported_symbols_msvc.lds` file for the [inference_cc example](https://github.com/PatWie/tensorflow-cmake/tree/master/inference/cc).\r\n```\r\nLIBRARY tensorflow_cc\r\nEXPORTS\r\n    ??0MetaGraphDef@tensorflow@@QEAA@XZ\r\n    ??1MetaGraphDef@tensorflow@@UEAA@XZ\r\n    ??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z\r\n    ??1LogMessageFatal@internal@tensorflow@@UEAA@XZ\r\n    ??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z\r\n    ??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ\r\n    ?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ\r\n    ?NewString@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ\r\n    ?GetVarint32PtrFallback@core@tensorflow@@YAPEBDPEBD0PEAI@Z\r\n    ?ToString@Status@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ\r\n    ?SlowCopyFrom@Status@tensorflow@@AEAAXPEBUState@12@@Z\r\n    ?_GraphDef_default_instance_@tensorflow@@3VGraphDefDefaultTypeInternal@1@A\r\n    ?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z\r\n    ??0SessionOptions@tensorflow@@QEAA@XZ\r\n    ??1ConfigProto@tensorflow@@UEAA@XZ\r\n    ?ReadBinaryProto@tensorflow@@YA?AVStatus@1@PEAVEnv@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVMessageLite@protobuf@google@@@Z\r\n    ?Default@Env@tensorflow@@SAPEAV12@XZ\r\n    ?CheckIsAlignedAndSingleElement@Tensor@tensorflow@@AEBAXXZ\r\n    ?_SaverDef_default_instance_@tensorflow@@3VSaverDefDefaultTypeInternal@1@A\r\n    ?CheckTypeAndIsAligned@Tensor@tensorflow@@AEBAXW4DataType@2@@Z\r\n    ??1Tensor@tensorflow@@QEAA@XZ\r\n    ??0Tensor@tensorflow@@QEAA@W4DataType@1@AEBVTensorShape@1@@Z\r\n    ??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@XZ\r\n    ??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z\r\n    ?DestructorOutOfLine@TensorShapeRep@tensorflow@@AEAAXXZ\r\n    ?TfCheckOpHelperOutOfLine@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVStatus@1@PEBD@Z\r\n    ?DebugString@Tensor@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ\r\n    ?SlowCopyFrom@TensorShapeRep@tensorflow@@AEAAXAEBV12@@Z\r\n```\r\nNow it links correctly! No renaming needed for the libtensorflow_cc.so file, only when adding it near the executable during runtime.", "@Steroes : Can you please provide ex syntax of how to use create_def_file.py to generate symbols. I am getting error of file cannot be found.\r\nIn reference with the link you have posted. \r\nI am trying to run a cpp program using tensorflow(1.8) build using bazel\r\nAnd getting the linker error when i am trying to run the sample program using qt.\r\nI just had a doubt that bazel generates many lib files when we build tensorflow:\r\nIn my case C:\\tensorflow-r1.8\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\cc directory has lot of lib files relevant to cpp.\r\nI tried to run a sample program linking all those relevant lib files but was getting linkage error.\r\nI am asking this because when i build the sample program using bazel it gets build successfully without any error.\r\nSo i was thinking exporting the symbols to the tensorflow.lib files will make any sense specially in my case.\r\n\r\n", "@Steroes Could you please kindly offer a short tutorial on how to solve the problem and configurations on linking?\r\nI have asked a [question](https://stackoverflow.com/questions/53809398/how-to-program-with-c-api-library-on-windows-using-bazel) on stackoverflow, and I hope people with similar problems can work together to solve it.", "See my [post](https://github.com/tensorflow/tensorflow/issues/22047#issuecomment-447992343). Let me know if it doesnt work.", "> @Steroes : Can you please provide ex syntax of how to use create_def_file.py to generate symbols. I am getting error of file cannot be found.\r\n> In reference with the link you have posted.\r\n> I am trying to run a cpp program using tensorflow(1.8) build using bazel\r\n> And getting the linker error when i am trying to run the sample program using qt.\r\n> I just had a doubt that bazel generates many lib files when we build tensorflow:\r\n> In my case C:\\tensorflow-r1.8\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\cc directory has lot of lib files relevant to cpp.\r\n> I tried to run a sample program linking all those relevant lib files but was getting linkage error.\r\n> I am asking this because when i build the sample program using bazel it gets build successfully without any error.\r\n> So i was thinking exporting the symbols to the tensorflow.lib files will make any sense specially in my case.\r\n\r\nI dont use the scripts, because they generate too many symbols which the linker cannot handle. I add them based on my linker errors. Just copy the signatures to the .lds file, see my link above to the contents of that file.", "@Steroes could you please tell me how to get the tf_exported_symbols_msvc files?\r\nI want to write complete tf_exported_symbols_msvc.lds, but I don't know how to get the name of the those function.", "> @Steroes could you please tell me how to get the tf_exported_symbols_msvc files?\r\n> I want to write complete tf_exported_symbols_msvc.lds, but I don't know how to get the name of the those function.\r\n\r\nThe file you can create manually, and put it in `tensorflow\\tensorflow\\` directory. The content will be like \r\n```\r\nLIBRARY tensorflow_cc\r\nEXPORTS\r\n    ??0MetaGraphDef@tensorflow@@QEAA@XZ\r\n    ??1MetaGraphDef@tensorflow@@UEAA@XZ\r\n    ??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@\r\n```\r\nThis content you can get for your specific project by linking against the tensorflow lib (which will link against the dll). The linker errors that you will get from Visual Studio for example, will have the symbols in it. Look for the signatures like `??0MetaGraphDef@tensorflow@@QEAA@XZ`. Copy and paste these in the tf_exported_symbols_msvc.lds file, and rerun the bazel build command. Then the dll will be relinked with the new symbols. Repeat until there are no more linker errors.", "@Steroes  Thanks a lot. but there is till bug when using \r\n ```sess_options.config.mutable_gpu_options()->set_visible_device_list(gpu_d)```\r\n to config the session\r\nIn version 1.5, I fix the bug with add tf_proto_cc.lib before the tensorflow.lib to the Linker. so i guess some function about sess_options in tensorflow.lib was overried by the function  in tf_proto_cc.lib.\r\n but in version 1.12, I can't found tf_proto_cc.lib  after build the tensorflow with bazel, so I want to try   add the functions whitch in  tf_proto_cc.lib to tf_exported_symbols_msvc.lds to fix the bug.  But I can't get the functions name, beacuse there is no linker errors when  build my project. ", "@kingstarcraft What error do you get then?", "@kingstarcraft Did you solve this problem? Facing the same issue.", "Hello,\r\n@Steroes I did what you mentioned. But the linker errors are not going. \r\n\r\nI think it has to do with the fact that I am using TF 2.0 and the modifications in the BUILD file are different.\r\nI added the missing symbols to the tf_exported_symbols_mscv.lds and ran the build again but that does not solve my linking errors. I get the same symbols as missing.\r\n\r\nSome background on how I ran the build->\r\nFor TF 2.0 BUILD file I cannot use the git -diff file with the changes mentioned in (https://github.com/tensorflow/tensorflow/issues/22047#issuecomment-421452033).\r\nSo I made the required changes manually (line 538 and 661 in this case), added the tf_exported_symbols.lds with my missing symbols and ran the bazel build //tensorflow:tensorflow_cc.\r\n\r\nLine 558 of the tensorflow/BUILD file in r2.0 branch has the following:\r\nadd win_def_file for tensorflow_cc\r\n    win_def_file = select({\r\n        # We need this DEF file to properly export symbols on Windows\r\n        \"//tensorflow:windows\": \":tensorflow_filtered_def_file\",\r\n        \"//conditions:default\": None,\r\n    }),\r\n\r\nBut I cant figure out the syntax of adding the tf_exported_symbols_mscv.lds\r\nPlease help ", "@rounakskm Is this still an issue? Please check or create issue on https://github.com/guikarist/tensorflow-windows-build-script otherwise.", "> @Steroes Thanks a lot. but there is till bug when using\r\n> `sess_options.config.mutable_gpu_options()->set_visible_device_list(gpu_d)`\r\n> to config the session\r\n> In version 1.5, I fix the bug with add tf_proto_cc.lib before the tensorflow.lib to the Linker. so i guess some function about sess_options in tensorflow.lib was overried by the function in tf_proto_cc.lib.\r\n> but in version 1.12, I can't found tf_proto_cc.lib after build the tensorflow with bazel, so I want to try add the functions whitch in tf_proto_cc.lib to tf_exported_symbols_msvc.lds to fix the bug. But I can't get the functions name, beacuse there is no linker errors when build my project.\r\n\r\nSame error. Wondering how to solve the problem.", "@ttdd11 Same issue. Did you fix this?", "> @ttdd11 Same issue. Did you fix this?\r\n\r\n@ZhuoranLyu +1 .Did you fix this?", "> > @ttdd11 Same issue. Did you fix this?\r\n> \r\n> @ZhuoranLyu +1 .Did you fix this?\r\n\r\nI figured it out. Just add the functions to tf_exported_symbols_msvc.lds.", "I was able to fix this for myself, building C++ on Windows, not using the tf_exported_symbols_msvc.lds method, but by using the TF_EXPORT method. \r\n\r\nI made a README explaining my solution. The tutorial is step by step and starts at the very beginning, so you may have to scroll down past steps you have already done, like checking your hardware, installing Bazel etc. \r\nHere is the url: https://ashley-tharp.medium.com/btw-if-you-enjoy-my-tutorial-i-always-appreciate-endorsements-on-my-linkedin-https-www-linkedin-a6d6fcba1e44\r\n\r\nProbably you will want to scroll all the way down to step 7\r\n\r\nIt shows how to pass command to create .lib and .dll. \r\n\r\nThen to test your .lib you should link into your c++ project, \r\n\r\nThen it will show you how to identify and fix the missing symbols using the TF_EXPORT macro\r\n\r\nI am actively working on making this tutorial better so feel free to respond to my comment here or email: ashley.tharp@gmail.com with feedback or if you are confused on a particular part. :)\r\n\r\n", "@sitting-duck Thank you for your guide. I've been trying your steps with the latest Tensorflow 2.0 code, Python 3.5.4, protobuf 3.8.0, Bazel 0.26.1, CUDA 10.1, cudnn 7.4 and Visual Studio 2017 Build Tools. I got to the last step where I see unresolved external symbols. I modified the source code of a few files with `TF_EXPORT` and `#include \"tensorflow/core/platform/macros.h\"` Then I redid the line for building tensorflow.lib. I noticed that the date modified of tensorflow.lib didn't change at this step but tensorflow.dll did change. And now the errors in my Visual Studio project that try to use the tensorflow.lib have a slightly different error:\r\n`unresolved external symbol __declspec(dllimport) public: __cdecl tensorflow::ops::Subtract` ...\r\n\r\nBased on looking at macros.h, If `TF_COMPILE_LIBRARY` had been set, then the macro would have been `__declspec(dllexport) ...`. Would that have avoided the error? This was my reading on that https://stackoverflow.com/questions/8863193/what-does-declspecdllimport-really-mean\r\nhttps://docs.microsoft.com/en-us/cpp/cpp/dllexport-dllimport?view=vs-2019\r\n\r\nWhat are your Additional Include Directories and do they have the modified files with the `TF_EXPORT` and `#include \"tensorflow/core/platform/macros.h\"`?\r\n\r\nAnother issue is that some of the files I need to edit with TF_EXPORT are machine generated such as \"\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\cc\\ops\\math_ops.h\". I don't think it makes sense to edit these files.", "@sitting-duck Many thanks for the README file, it is indeed very useful, I followed all the steps and could build tf, generating the .dll and .lib function, however, similar to the people in this thread I have problems with unresolved external symbol.  I made the modifications as indicated:\r\n\r\n- in the file session.h I added the line:\r\n#include \"tensorflow/core/platform/macros.h\"\r\njust after \r\n#ifndef TENSORFLOW_CORE_PUBLIC_SESSION_H_\r\n#define TENSORFLOW_CORE_PUBLIC_SESSION_H_\r\n\r\n- I added TF_EXPORT as follows:\r\nclass Session {\r\n public:\r\n  TF_EXPORT Session();\r\n\r\n- then I built the .lib again using: \r\n bazel build --config=cuda tensorflow:tensorflow.lib\r\n\r\nHowever, after these steps I still have 2 unresolved external sumbol errors:\r\n\r\n1. Error\tLNK2019\tunresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ) referenced in function xxxxx\r\n\r\n2. Error\tLNK2019\tunresolved external symbol \"class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z) referenced in function xxxx\r\n\r\nI tried building both a Release and Debug version, both failed with same errors.\r\n\r\nAny hint on how can this be fixed ?\t\r\n", "@lattard Can you find where SessionOptions and NewSession are defined and put in TF_EXPORT and #include \"tensorflow/core/platform/macros.h\" like you did for Session? ", "@DBraun Apologies for the delay in response. If you cure one missing symbol and then find another that is good progress in my experience with this. I usually only see the Windows MSVC compiler report one or maybe two missing symbols at a time. I seem to have to fix these errors before I can see the next one.\r\n\r\nThe next step (my guess) is to basically locate that new symbol, tensorflow::ops::Subtract and then TF_EXPORT for it as well. You may have to do this for each symbol that your code uses that Tensorflow doesn't export by default. I had to add TF_EXPORT and then build my .lib many times to get my C++ project code to compile. \r\n\r\nTensorflow source code has many symbolic links in it, so searching the dir recursively (perhaps using grep -rn) did not work for me because I guess grep couldn't follow the symbolic links. \r\n\r\nI have always had to find the line of code in my project that was calling the symbol (like in your case, find my code that calls tensorflow::ops::Subtract) and then right click and follow symbol in the IDE I used.\r\n\r\nI have had great luck just searching for the function name I see in the missing symbol error on the actual Github site for Tensorflow, https://github.com/tensorflow/tensorflow . I literally just use the search bar on the website and usually find the file the symbol is defined in. \r\n\r\nI am very interested in learning a better way to locate symbols in the Tensorflow code so if anyone does know a superior method please let me know. \r\n\r\nAs for my include directories, if I am accessing a symbol, for example, tensorflow::ops::Subtract, I also have to #include the .h or .hpp file that contains that symbol definition. Same for Session, NewSession etc. \r\n\r\nI am not sure about the machine generated files to be completely honest, but I suppose there is no harm in trying it out, I found that just making the edit and compiling the .lib file usually does not take me too long. \r\n\r\nWhen you ask, \"Can you find where SessionOptions and NewSession are defined and put in TF_EXPORT and #include \"tensorflow/core/platform/macros.h\" like you did for Session?\"\r\n\r\nI had to use TF_EXPORT for those symbols as well. I located them using the same method (right click -> Go to symbol) to find them or by searching on the website.", "@lattard I had to repeat the process for those symbols as well. The compiler seems to only show one or two missing symbol errors at a time. \r\n\r\nI had to repeat this process for SessionOptions and NewSession in my code as well. \r\n\r\nIf you are adding it to a function definition, make sure you put it after the return type. \r\n\r\n(CORRECT) void TF_EXPORT functionName();\r\n(NOT CORRECT) TF_EXPORT void functionName();\r\n\r\nI think you are on the right track, you just need to repeat the process for each missing symbol. I read somewhere that Tensorflow does not export ALL the symbols by default because there is a 60000 symbol limit for dlls. \r\n\r\nThis is why I had to build the Tensorflow code myself and modify the code to get the symbols I needed. \r\n\r\nI hope that helps. Feel free to reach out, my comment to @DBraun may also have some related info that hopefully can be useful to you. :)", "@DBraun Thanks! In fact I've done that for those symbols also now and I've solved the issue.", "Ok thanks for the reassurance. I did the TF_EXPORT stuff a bit more carefully and turned 21 linker errors into just 8. Now the errors that remain are all related to code in machine generated files \"\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\cc\\ops\\math_ops.h\". I want to use Div and Sub because they're used here https://github.com/tensorflow/tensorflow/blob/a4794bcbf836e78984b75a545b67f42a05455721/tensorflow/examples/label_image/main.cc#L170 but easy-to-modify declarations of those classes don't exist in the source code I think. I might just try to avoid using Div and Sub in the meantime.", "> Ok thanks for the reassurance. I did the TF_EXPORT stuff a bit more carefully and turned 21 linker errors into just 8. Now the errors that remain are all related to code in machine generated files \"\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\cc\\ops\\math_ops.h\". I want to use Div and Sub because they're used here\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/a4794bcbf836e78984b75a545b67f42a05455721/tensorflow/examples/label_image/main.cc#L170\r\n> \r\n> but easy-to-modify declarations of those classes don't exist in the source code I think. I might just try to avoid using Div and Sub in the meantime.\r\n\r\nI dont see any image related code on your Github. If you push any related code to h\r\nGithub or want to share it with me please let me know I love to collaborate. You can reach me at ashley.tharp@gmail.com", "@sitting-duck Hi, many thanks for your solution. It seems to be working well. \r\n\r\nHowever, I'm unable to find the function definition of grapf_def() and saver_def(). Right click, 'go to source' takes me to meta_graph.pb.h, which has 'DO NOT EDIT' at the top of the file. \r\n\r\nUsing TF2.0, I'm trying LoadModel function from https://github.com/PatWie/tensorflow-cmake/blob/master/examples/keras/inference.cpp and this is the error message I get,\r\n\r\n![image](https://user-images.githubusercontent.com/7346268/69545690-4957f680-0f60-11ea-9ec3-6243591ba59b.png)\r\n\r\n\r\nCould someone point me to the class/function I should add TF_EXPORT?\r\n\r\nthanks!", "@AshwinAKannan \r\nI have seen other people using a method that involves a ``tf_exported_symbols_msvc.lds``\r\nhttps://github.com/tensorflow/tensorflow/issues/23542#issuecomment-464661172\r\n\r\nI have seen an explanation of that method at the link above, hopefully if that doesn't help, it may give you some good keywords for googling that ``.lds`` file method. \r\n\r\nPeople have reported success using that method from what I have seen, but I myself was never able to get it to work. \r\n\r\nI cannot think of any other way at the moment besides the .lds file method, and the TF_EXPORT, but there may be some other way I have not found yet,\r\n\r\nWere you able to get this resolved? I hope to work again on this kind of issue this weekend, as I am trying to create a nice all in one guide for compiling and linking Tensorflow for Windows.\r\n\r\n", "@sitting-duck I tried that, couldn\u2019t get it to work though ", "@AshwinAKannan @sitting-duck From my investigations on 1.15, it looks like TF moved away from pulling symbols from a named def file (such as tf_exported_symbols_msvc.lds), and uses a procedure to automatically compile a dummy dll, poll that to extract all symbols, then filter that (see def_file_filter.py.tpl) to reduce the symbols below the 64k number of exportable symbols limit.\r\n\r\nThe TF_EXPORT trick will work, you can also try adding the missing symbols directly to the def_file_filter.py.tpl file. I've added the following to mine for example:\r\n`    def_fp.write(\"\\t ??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z\\n\")\r\n    def_fp.write(\"\\t ?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z\\n\")\r\n    def_fp.write(\"\\t ??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z\\n\")\r\n    def_fp.write(\"\\t ??1Scope@tensorflow@@QEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?NewRootScope@Scope@tensorflow@@SA?AV12@XZ\\n\")\r\n    def_fp.write(\"\\t ?GetUniqueNameForOp@Scope@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV34@@Z\\n\")\r\n    def_fp.write(\"\\t ?UpdateStatus@Scope@tensorflow@@QEBAXAEBVStatus@2@@Z\\n\")\r\n    def_fp.write(\"\\t ?UpdateBuilder@Scope@tensorflow@@QEBAXPEAVNodeBuilder@2@@Z\\n\")\r\n    def_fp.write(\"\\t ?ok@Scope@tensorflow@@QEBA_NXZ\\n\")\r\n    def_fp.write(\"\\t ?graph@Scope@tensorflow@@QEBAPEAVGraph@2@XZ\\n\")\r\n    def_fp.write(\"\\t ?ToGraphDef@Scope@tensorflow@@QEBA?AVStatus@2@PEAVGraphDef@2@@Z\\n\")\r\n    def_fp.write(\"\\t ?DoShapeInference@Scope@tensorflow@@QEBA?AVStatus@2@PEAVNode@2@@Z\\n\")\r\n    def_fp.write(\"\\t ?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z\\n\")\r\n    def_fp.write(\"\\t ?AsNodeOut@ops@tensorflow@@YA?AUNodeOut@NodeBuilder@2@AEBVScope@2@AEBVInput@2@@Z\\n\")\r\n    def_fp.write(\"\\t ??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0MatMul@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0Sqrt@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z\\n\")\r\n    def_fp.write(\"\\t ??0Sum@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0SessionOptions@tensorflow@@QEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z\\n\")\r\n    def_fp.write(\"\\t ??_7GraphDef@tensorflow@@6B@\\n\")\r\n    def_fp.write(\"\\t ??0Square@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z\\n\")\r\n    def_fp.write(\"\\t ?_GPUOptions_default_instance_@tensorflow@@3VGPUOptionsDefaultTypeInternal@1@A\\n\")\r\n    def_fp.write(\"\\t ?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z\\n\")\r\n`\r\nThis symbol problem is very annoying. However I'm also hitting all sorts of other issues on Windows with v1.15.0, I have yet to see an example where a shared 'tensorflow_cc.dll' can be used in an external exe without a lot of work. For example, the tutorial 'example_trainer' behaves very strangely, I get issues similar to this and can't find a way to work around them: https://github.com/tensorflow/tensorflow/issues/5379\r\n\r\nI would love if the TF devs could show the 'example_trainer' being built and linked against the shared 'tensorflow_cc.dll'.", "Have you tried @dianlujitao the below solution?\r\n\r\nThere is easy way to rebuild import library (.lib) and DLL library on Windows with adding missing unresolved symbols in **a few seconds**.\r\n\r\nIn my case when I build TF (of course **Release** version), I get the following files in **bazel-genfiles**:\r\n\r\n```\r\nX\\                              <- folder where clone of TF was made\r\n   tensorflow\\                   <- main folder of tf         \r\n     bazel-genfiles\\             <- symbolic links..\r\n         tensorflow\\\r\n              tensorflow_cc.dll            // dynamic library\r\n              tensorflow_cc.dll.if.lib     // import library \r\n              tensorflow_filtered_def_file.def\r\n              tensorflow_cc.dll-2.params\r\n```\r\n\r\nFile **tensorflow_filtered_def_file.def** contains all imported/exported symbols.\r\nFile **tensorflow_cc.dll-2.params** contains list of static library which are used to generate final .DLL library (By the way, content of this file can be easily used to generate one BIG static library of TF - its weight is around 3 GB, instead of DLL - you can deal with this static lib until you have working release version, then switch to use DLL version and rebuild it with adding ass missing symbols).\r\n\r\nNow, when TF has been built, there is a log file named **command.log** (I don't specify output directory for **Bazel**, so generated files are put **C:\\Users\\NAME\\bazel_NAME\\buildHASH**, and in this folder we should be looking for this log file), when we open it, we can see:\r\n \r\n```\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC/14.24.28314/bin/HostX64/x64/link.exe \r\n     /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib \r\n     -ignore:4221 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll-2.params \r\n     /OPT:ICF /OPT:REF /DEF:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_filtered_def_file.def /ignore:4070\r\n```\r\nthis command uses MSVC linker to rebuild DLL/LIB. It takes two files menotied above (it is obvious that path to *link.exe* may be different).\r\n\r\nAll we have to do is to make file **tensorflow_filtered_def_file.def** as writeable, open it and put missing symbols, for example:\r\n\r\n```\r\nEXPORTS\r\n    ?SetTotalBytesLimit@CodedInputStream@io@protobuf@google@@QEAAXH@Z\r\n    ?default_recursion_limit_@CodedInputStream@io@protobuf@google@@0HA\r\n    ??_7GraphDef@tensorflow@@6B@\r\n    ??0SessionOptions@tensorflow@@QEAA@XZ\r\n    ??1CodedInputStream@io@protobuf@google@@QEAA@XZ\r\n    ?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z\r\n```\r\n\r\nthen we enter into parent directory of TF (**X\\tensorflow**) in new opened command line Window with enabled Visual Build Tools (for example from VS2019 - _x64 Native Tools Command Prompt_, we have to do it - linker of Visual **link.exe** must be somehow found), and just run this command. **The key point ->** in a few seconds new import library and DLL will be generated with added all missing symbols.\r\n\r\nTested on r2.0 and r2.1 with **success** (there are some problems in source code on r.2.2 branch, where is used VLA - variable length array in MKL library - it could work on G++ on Linux where VLA are allowed, but not on MSVC ). Are you testing building process only on Linux ? Another thing, if you want to build with OpenMP support on Windows you should find proper _mkl.build_ file, and replace `-fopenmp` by `/openmp`  or `-openmp` (is difference between `-fopenmp` and `-openmp`, isn't it?).\r\n\r\n", "Anyone try this again recently with the latest TensorFlow? I think we have to use Bazel 2.0.0  https://github.com/tensorflow/tensorflow/blob/95be0349fb11e735efc122fa4c54816e27279684/configure.py#L53 but now I'm having trouble with `bazel build --config=cuda tensorflow:tensorflow.dll` \r\nThis is one of my environment variables.\r\n`BAZEL_VS`: `C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools`\r\nAccording to the error output I think it's using the 2017 tools. Maybe that's the issue?\r\n\r\n    ERROR: C:/users/-----/documents/github/tensorflow/tensorflow/core/kernels/BUILD:5427:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_tensor_dense_matmul_op_gpu' failed (Exit 2)\r\n    c:\\users\\-----\\_bazel_-----\\dktb5wq4\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/XprHelper.h(114): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration", "> > > @ttdd11 Same issue. Did you fix this?\r\n> > \r\n> > \r\n> > @ZhuoranLyu +1 .Did you fix this?\r\n> \r\n> I figured it out. Just add the functions to tf_exported_symbols_msvc.lds.\r\n\r\nhow did you konw the functions name? ", "> \r\n> \r\n> Anyone try this again recently with the latest TensorFlow? I think we have to use Bazel 2.0.0\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/95be0349fb11e735efc122fa4c54816e27279684/configure.py#L53\r\n> but now I'm having trouble with `bazel build --config=cuda tensorflow:tensorflow.dll`\r\n> This is one of my environment variables.\r\n> `BAZEL_VS`: `C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools`\r\n> According to the error output I think it's using the 2017 tools. Maybe that's the issue?\r\n> \r\n> ```\r\n> ERROR: C:/users/-----/documents/github/tensorflow/tensorflow/core/kernels/BUILD:5427:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_tensor_dense_matmul_op_gpu' failed (Exit 2)\r\n> c:\\users\\-----\\_bazel_-----\\dktb5wq4\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/XprHelper.h(114): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n> ```\r\n\r\nI have the same problem,but after recompile,it passed.", "@07rafix Thank you for your instructions.\r\n\r\nI'm using Visual Studio Community. I also happen to be building to a file named `tensorflow.dll` not `tensorflow_cc.dll` so this is my command:\r\n`\"C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.25.28610/bin/Hostx64/x64/link.exe\" /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib -ignore:4221 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll-2.params /OPT:ICF /OPT:REF /DEF:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_filtered_def_file.def /ignore:4070`\r\n\r\nI had to change the file permissions of `libtensorflow.dll.ifso` and `tensorflow.dll`\r\n\r\nWhen I run, I get some warnings but no errors. I noticed that the date modified of tensorflow.dll increases, but tensorflow.lib stays the same. Should tensorflow.lib get updated? My other C++ project which has tensorflow.lib in Linker>Input is still failing to build because of missing symbols. I figure it's because tensorflow.lib isn't updating.\r\n\r\nI shouldn't have to do `bazel build --config=cuda tensorflow:tensorflow.lib` again right?", "I shouldn't have to do bazel build --config=cuda\ntensorflow:tensorflow.lib again\nright?\n\nI believe you do have to build the lib again, this has been my experience.\n\n\n\nOn Sun, May 3, 2020 at 12:34 PM David <notifications@github.com> wrote:\n\n> @07rafix <https://github.com/07rafix> Thank you for your instructions.\n>\n> I'm using Visual Studio Community. I also happen to be building to a file\n> named tensorflow.dll not tensorflow_cc.dll so this is my command:\n> \"C:/Program Files (x86)/Microsoft Visual\n> Studio/2019/Community/VC/Tools/MSVC/14.25.28610/bin/Hostx64/x64/link.exe\"\n> /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib\n> -DEFAULTLIB:advapi32.lib -ignore:4221 /MACHINE:X64\n> @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll-2.params /OPT:ICF\n> /OPT:REF\n> /DEF:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_filtered_def_file.def\n> /ignore:4070\n>\n> I had to change the file permissions of libtensorflow.dll.ifso and\n> tensorflow.dll\n>\n> When I run, I get some warnings but no errors. I noticed that the date\n> modified of tensorflow.dll increases, but tensorflow.lib stays the same.\n> Should tensorflow.lib get updated? My other C++ project which has\n> tensorflow.lib in Linker>Input is still failing to build because of missing\n> symbols. I figure it's because tensorflow.lib isn't updating.\n>\n> I shouldn't have to do bazel build --config=cuda tensorflow:tensorflow.lib\n> again right?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23542#issuecomment-623149485>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJ23ZVXCJ7JY4GZFZA7QCDRPWTKDANCNFSM4GB6Y42Q>\n> .\n>\n", "Cool but now I'm noticing that `bazel build --config=cuda\r\ntensorflow:tensorflow.lib` will replace `tensorflow_filtered_def_file.def` with a version without my added symbols. It also doesn't update or replace the `tensorflow.lib` unless I delete it beforehand.\r\n\r\nThere's probably a way of using lib.exe instead of link.exe to make tensorflow.lib without overwriting the definitions file. https://docs.microsoft.com/en-us/cpp/build/reference/running-lib?view=vs-2019", "Wow there's light at the end of this tunnel! I used lib.exe and got a tensorflow.lib\r\n\r\n`\"C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.25.28610/bin/Hostx64/x64/lib.exe\" /nologo /SUBSYSTEM:CONSOLE /MACHINE:X64 /DEF:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_filtered_def_file.def /out:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.lib /ignore:4070 -ignore:4221`\r\n\r\nMy C++ program compiles and runs too.\r\n\r\nActually I spoke too soon.\r\n\r\nWhen running link.exe it can't find these symbols which I've put in the def file\r\n    \r\n    ?CatPieces@internal@strings@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@Vstring_view@absl@@@5@@Z\r\n\r\n    ??0Status@tensorflow@@QEAA@W4Code@error@1@Vstring_view@absl@@@Z\r\n\r\n    ??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z\r\n\r\n    ?JoinPathImpl@internal@io@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@Vstring_view@absl@@@5@@Z", "@DBraun Every source file of Tensorflow is built to _.lib_ (static libary) file.\r\n\r\nFor example in _tensorflow/core/platform/strcat.h_ there is\r\n\r\n```\r\nnamespace tensorflow {\r\n  namespace strings {\r\n    namespace internal {\r\n         string CatPieces(std::initializer_list<StringPiece> pieces);\r\n    } } }\r\n```\r\nwhere `StringPiece `is typedef for `absl::string_view`. And this function is your missing symbol. You can find this symbol by calling the below command inside the folder where binary files of _tensorflow/core/platform_ are generated:\r\n\r\n`dumpbin /SYMBOLS strcat.lib`\r\n\r\nwhere _strcat.lib_ should be located: `bin\\tensorflow\\core\\platform` folder.\r\n\r\nIn output you can see:\r\n\r\n`150 00000000 SECT41 notype ()    External     | ?CatPieces@internal@strings@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@Vstring_view@absl@@@5@@Z (class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::strings::internal::CatPieces(class std::initializer_list<class absl::string_view>))\r\n`\r\n\r\n\r\nHow it is related to your problem?\r\nThe file _strcat.lib_ should be listed in **tensorflow_cc.dll-2.params** (it contains all static library which are used by LINK.exe to generate DLL and import library):\r\n\r\n```\r\n/WHOLEARCHIVE:bazel-out/x64_windows-opt/bin/tensorflow/core/platform/annotation.lib\r\n/WHOLEARCHIVE:bazel-out/x64_windows-opt/bin/tensorflow/core/platform/strcat.lib\r\n/WHOLEARCHIVE:bazel-out/x64_windows-opt/bin/tensorflow/core/platform/numbers.lib\r\n```\r\n\r\nyou have to pass this file when calling _LINK.exe_ (This file is passed in the command which I attached in my first answer). So, passing both file .params and .def (which contains added missing symbols) all should work fine.\r\n\r\n\r\n", "Skip to update at bottom\r\n\r\nThank you @07rafix. In my case the file `libstrcat.a` has the symbols:\r\n`198 00000000 SECT41 notype ()    External     | ?CatPieces@internal@strings@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@Vstring_view@lts_2020_02_25@absl@@@5@@Z (class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::strings::internal::CatPieces(class std::initializer_list<class absl::lts_2020_02_25::string_view>))`\r\n\r\nI've noticed that the ending `@lts_2020_02_25@absl@@@5@@Z` is slightly different from what my C++ project says is missing:\r\n`Error\tLNK2001\tunresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::strings::internal::CatPieces(class std::initializer_list<class absl::string_view>)\" (?CatPieces@internal@strings@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@Vstring_view@absl@@@5@@Z)`\r\n\r\nNote `@Vstring_view@lts_2020_02_25@absl@@@5@@Z` vs.  `@Vstring_view@absl@@@5@@Z`\r\n\r\nSo if I add the symbol that actually came out of the dumpbin command, the longer one with `@lts_2020_02_25` in it, link.exe does build a new dll.\r\n\r\nThis doesn't create a new .lib file, so I did this:\r\n`\"C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.25.28610/bin/Hostx64/x64/lib.exe\" /nologo /SUBSYSTEM:CONSOLE /MACHINE:X64 /DEF:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_filtered_def_file.def /out:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.lib /ignore:4070 -ignore:4221`\r\n\r\nThat created a new .lib, which I referenced in Linker>Input in my C++ project, but Visual Studio reports the same missing symbol, the one without `@lts_2020_02_25` in it. If I do include that full symbol in `tensorflow_filtered_def_file.def` then link.exe fails.\r\n\r\nIn an alternative method, if I use the symbol from the dumpbin, link.exe works. Then I did \r\n`bazel build --config=cuda tensorflow:tensorflow.lib` but that reset my customized `tensorflow_filtered_def_file.def` and my C++ project fails with all of its 22 missing symbols as opposed to just 4 that I mentioned earlier. Isn't your .def file getting reset too? At what moment do you get your `.lib` file? Thanks for your help.\r\n\r\nUpdate: It appears to have been a problem with abseil-cpp. I switched to https://github.com/abseil/abseil-cpp/releases/tag/20200225 and got zero errors. I'll go ahead with testing my program now. Thank you!", "So this issue is 3 years old and still open, proving that using tf in C++ on Windows is strictly impossible without manually intervening in the compilation process, with advanced knowledge.\r\n\r\nI think this situation is far from satisfying and that this fact can be established :\r\n**Tensorflow CANNOT be used in C++ on Windows**", "Agreed. Sadly this and a similar issue has caused us to move to PyTorch, which has been a much more pleasant experience in deploying ML on multiple platforms.", "@MoetaYuko We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "@kumariko Thanks for your attention. It's been 3 years and I personally no longer need a fix. Dunno if other community members still need help.", "@MoetaYuko Thanks for the update! Can anyone from the community please confirm , if this issue still persists? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23542\">No</a>\n"]}, {"number": 23541, "title": "compiled error when build android  TFLite Model Benchmark Tool", "body": "tensorflow version: 0.12\r\ndescrible:\r\nwhen i build the android  TFLite Model Benchmark Tool use the command: \r\nbazel build -c opt \\\r\n--config=android_arm \\\r\n--cxxopt='--std=c++11' \\\r\ntensorflow/lite/tools/benchmark:benchmark_model\r\n\r\nsome errors occurs:\r\nno such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz, https://github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz] to /root/.cache/bazel/_bazel_root/2f52c1a17bc0577f4e941b98cd34b371/external/com_google_absl/f95179062eb65ce40895cc76f1398cce25394369.tar.gz: All mirrors are down: [Could not initialize class sun.security.ssl.SSLContextImpl$DefaultSSLContext] and referenced by '//tensorflow/lite/kernels/internal:tensor_utils'\r\n\r\nbuild aborted: no such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz, https://github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz] to /root/.cache/bazel/_bazel_root/2f52c1a17bc0577f4e941b98cd34b371/external/com_google_absl/f95179062eb65ce40895cc76f1398cce25394369.tar.gz: All mirrors are down: [Could not initialize class sun.security.ssl.SSLContextImpl$DefaultSSLContext]\r\n\r\n\r\n\r\n\r\n", "comments": ["Hmm, this looks more like a bazel issue than a TensorFlow/TensorFlow Lite issue. Looks like the absl mirror is failing? Are you able to access https://mirror.bazel.build/github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz from your device?", "@jdduke sorry. i can't access to the  https://mirror.bazel.build/github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz  from my device", "@gunan I'm not sure there's anything we can do if devs can't access mirrors for our dependencies? Do you have any suggestions?", "Mirrors are hosted on GCS. If users do not have access to GCS, we do not have a great answer to that.\r\nHow about the original. Is that working for you?", "my mistake. sorry "]}, {"number": 23540, "title": "Broken links in tensorflow for poets2 - codelabs", "body": "Hello,\r\nLinks are broken in tensorflow for poets 2 [codelabs](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2) \r\nBroken link : \r\n1] [TensorFlow Lite Optimizing Converter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/README.md) \r\n2] [quantized graphs](https://www.tensorflow.org/performance/quantization)\r\n\r\n", "comments": ["I found the same broken link in the below.\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\n\r\nI searched \"TensorFlow Lite Optimizing Converter\" or \"TOCO\" in the website, but there are no information in website. I will let you know when I have more information.", "@ymodak  & @suharshs some other links are also broken , please check other links from  codelab as well\r\n:) ", "Thanks for bringing these to our attention. Fixed the broken links. \r\n\r\nFor TOCO, please see here:\r\nhttps://www.tensorflow.org/lite/convert\r\n\r\nThanks!"]}, {"number": 23539, "title": "[INTEL MKL] Fix build error for //tensorflow/core:platform_port unit test.", "body": "This PR fixes the build error (shown below) that occurs when running `//tensorflow/core:platform_port` unit test.\r\n```\r\ntensorflow/core/platform/posix/port.cc:16:40: fatal error: absl/base/internal/sysinfo.h: No such file or directory\r\n #include \"absl/base/internal/sysinfo.h\"\r\n```", "comments": []}]