[{"number": 39708, "title": "`Model.predict(...)` seems incapable of handling two inputs when using the tf.data-API", "body": "**System information**\r\n- Have I written custom code: *Yes, see sample below.*\r\n- OS Platform and Distribution: *Ubuntu Server 20.04*\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): *2.2.0*\r\n- Python version: *3.8.2*\r\n- CUDA version: *10.1*\r\n- GPU model and memory: *NVIDIA RTX 2080 Ti*\r\n\r\n---\r\n\r\n**Describe the current behavior**\r\n\r\nWhile developing a model which expects two inputs (`X1` and `X2`) and generates one output (`y`), I noticed that `model.predict(...)` does not work as expected together with the `tf.data`-API.\r\n\r\nIn a simpler example, a model with only one input (`X`) can be used to make predictions by calling `y_pred = model.predict(X)`. Everything behaves as one would expected. You could also call `predict` on the full training set such as a `tf.data.Dataset` which consists of both inputs and expected outputs (for ex.: `Xy = tf.data.Dataset.zip((X, y_true))` followed by `y_pred = model.predict(Xy)`).\r\n\r\nBut let's get back to the previous example with two inputs. The following sample too works as expected:\r\n\r\n```\r\nX = dataset = tf.data.Dataset.zip((X1, X2))\r\nXy = dataset = tf.data.Dataset.zip((X, y_true))\r\ny_pred = model1.predict(Xy)\r\n```\r\n\r\nBut the more realistic application of `predict` were one only passes the inputs (without the true labels), does result into a crash:\r\n\r\n```\r\nX12 = dataset = tf.data.Dataset.zip((X1, X2))\r\ny_pred = model2.predict(X12)  # <- crash :,(\r\n```\r\n\r\n... with the following error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/hohl/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/home/hohl/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/hohl/projects/hohl.thesis/test.py\", line 25, in <module>\r\n    y_pred = model.predict(X)  # <- crashes\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 88, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1268, in predict\r\n    tmp_batch_outputs = predict_function(iterator)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 505, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2657, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAssertionError: in user code:\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\r\n        return self(x, training=False)\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717 call\r\n        return self._run_internal_graph(\r\n    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:899 _run_internal_graph\r\n        assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)\r\n    AssertionError: Could not compute output Tensor(\"dense/Identity:0\", shape=(None, 1), dtype=float32)\r\n```\r\n\r\nThe following usage, on the hand, executes without any troubles:\r\n\r\n```\r\nX12 = dataset = tf.data.Dataset.zip((X1, X2))\r\nX12y = dataset = tf.data.Dataset.zip((X12, y_true))\r\ny_pred = model2.predict(X12y)  # <- fine :)\r\n```\r\n\r\nThis way of usage works fine, so I guess in the previous example the model though I fed it inputs with labels instead of inputs consisting of two separate inputs. However, the latter example is a rather unrealistic usage for any real-word application. Once deployed you would not know the true labels, that's exactly why you would want to predict them.\r\n\r\n---\r\n\r\n**Describe the expected behavior**\r\n\r\nI would always expect `predict` to work with `y_pred = model.predict(X)` independently of the number of inputs. \r\n\r\nI guess the support for also accepting a combined dataset (inputs and labels) as input can come handy when testing single-input models, but I would consider it as even far more important that `model.predict(X)` works for models with any number of inputs.\r\n\r\nOne more thing: if the current behaviour is supposed to stay this way, it would be very helpful if the error message gets a bit more self-explanatory and the above fact gets mentioned explicitly in the [`predict`-documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict). It took me quite a while to figure out, why my model crashes with the ambiguous `AssertionError: Could not compute output` error message.\r\n\r\n---\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere is the above sample as a single Python script which you can run to quickly reproduce the issue:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Input, Model, layers\r\n\r\n\r\ndef build_model():\r\n    input1 = Input(shape=(1,), dtype=tf.float32)\r\n    input2 = Input(shape=(1,), dtype=tf.float32)\r\n    y = layers.Concatenate(axis=0)([input1, input2])\r\n    y = layers.Dense(1)(y)\r\n    return Model(inputs=[input1, input2], outputs=y)\r\n\r\n\r\ndef make_dummy_data():\r\n    X1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))\r\n    X2 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))\r\n    X = tf.data.Dataset.zip((X1, X2))\r\n    y_true = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))\r\n    return X, y_true\r\n\r\n\r\nX, y_true = make_dummy_data()\r\nXy = tf.data.Dataset.zip((X, y_true))\r\nmodel = build_model()\r\nmodel.compile(loss='mse')\r\nmodel.fit(Xy, batch_size=32)\r\ny_pred = model.predict(X)  # <- crashes with: \"AssertionError: Could not compute output ...\"\r\n#y_pred = model.predict(Xy)  # <- works\r\n```\r\n", "comments": ["I have tried in colab with TF version in 2.2 ,2.3.0-dev20200519 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/58553b02bc7e9b7a69bdeb6533f50d91/untitled915.ipynb).Thanks!", "You need to use `tf.data.Dataset.zip(((X1, X2),))`", "Hi @hohl! \r\nWe are checking to see whether you still need help in this issue . Did you check with above [suggestion](https://github.com/tensorflow/tensorflow/issues/39708#issuecomment-851420069) yet? Thanks!", "> Hi @hohl!\r\n> We are checking to see whether you still need help in this issue . Did you check with above [suggestion](https://github.com/tensorflow/tensorflow/issues/39708#issuecomment-851420069) yet? Thanks!\r\n\r\nSorry, I can\u2018t tell you anymore what I did to work around. I probably used `zip` as suggested, but I\u2019m not sure anymore as it\u2018s already over a year ago. But I for sure figured out something to get the job done (either `zip` or some way to avoid the issue in the first place.)", "Ok @hohl! Is this issue good to close then?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39708\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39708\">No</a>\n", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}, {"number": 39707, "title": "matmul & slice make incorrect result for some specific dims", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1060, 16G\r\n\r\n**Describe the current behavior**\r\nIf we have matrix **a**, **m**, if **b** is a sub-matrix of **a**,\r\nthen **matmul(b, m)** should also be a sub-matrix of **matmul(a, m)**,\r\nbut for some specific dims, there are some minor difference.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/17PyKnnxWc6UGZ_f-uxvlPBd6fL_9k9je?usp=sharing\r\n``` \r\nimport tensorflow as tf\r\n\r\ndef test(d1, d2):\r\n    a = tf.random.uniform([d1 + 1, d2], dtype=tf.float32)\r\n    b = tf.slice(a, [0, 0], [d1, d2])\r\n    m = tf.random.uniform([d2, d2], dtype=tf.float32)\r\n    return (tf.slice(a @ m, [0, 0], [d1, d2]) == b @ m).numpy().all()\r\n\r\nfor i in range(1, 20):\r\n    print('{}\\t{}'.format(i, test(i, 100)))\r\n```\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39707\">No</a>\n"]}, {"number": 39706, "title": "Multiple step predict seems to be wrong", "body": "![20200520_163852](https://user-images.githubusercontent.com/31833270/82420430-aa4d2b00-9aba-11ea-9ca2-160fba8afd6c.png)\r\n\r\n\r\nThe original data of x_test has a non-linear random walk property,\r\n\r\nbut the newly predicted 20 values have a linear shape.\r\n\r\nClearly, did I make the wrong prediction?\r\n\r\nyou can see full source here\r\nhttps://colab.research.google.com/drive/1kk24KjpZQEZpdlBxr4D4DO-IGHJ0439v?usp=sharing\r\n\r\nand my tf version is 2.1.0 and python 3.7.7\r\n![image](https://user-images.githubusercontent.com/31833270/82441869-9238d400-9ad9-11ea-9ca0-1d17d649e00f.png)\r\n", "comments": ["@Lay4U \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/9a681b694058b0b0b4965fd2aa8003d6/untitled192.ipynb)\r\nAlso let us know the tensor flow version on which you face this issue.", "@Saduf2019 \r\nhttps://colab.research.google.com/drive/1kk24KjpZQEZpdlBxr4D4DO-IGHJ0439v?usp=sharing\r\nis working?\r\n", "@Lay4U\r\nCan you please try the same on later versions of tensor flow and let us know if you still facing the same issue. as this is by default 1.15 version on colab please try on later versions and update.\r\n\r\n", "@Saduf2019 \r\nI tried colab, tf2.1 and tf2.2 but problem is same\r\n\r\nI don't think this is a version issue.", "  I am able to replicate the gist shared, please find the replicated [gist here](https://colab.sandbox.google.com/gist/Saduf2019/f88177e2b310971932652df0156f4f6f/untitled193.ipynb)", "> I am able to replicate the gist shared, please find the replicated [gist here](https://colab.sandbox.google.com/gist/Saduf2019/f88177e2b310971932652df0156f4f6f/untitled193.ipynb)\r\n\r\nI don't understand your mean\r\n\r\nif I do translate with google\r\n\r\n\"You can duplicate the shared point. Find the cloned point here.\"\r\n\r\nis my colab page wrong?", "Was my question forgotten? or Is it being solved?", "@Lay4U Thanks for the issue!\r\n\r\nThis just seems like a Model quality issue. Here is a simple example showing that `predict` is working correctly:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n  def build(self, _):\r\n    self.v = tf.Variable(2.)\r\n\r\n  def call(self, x):\r\n    return self.v * x\r\n\r\nmodel = tf.keras.Sequential([MyLayer()])\r\nmodel.compile('sgd', 'mse')\r\nmodel.predict(x=np.arange(10).astype(np.float32))\r\n```", "@omalleyt12 \r\nThere is something I don't understand. you create a MyLayer and multiply it by 2, but I know that the actual lstm doesn't work like this. In other words, I can't find a connection between your answer and my question. Could you please tell me the Full source for my question if you possible?"]}, {"number": 39705, "title": "Failed to load the native TensorFlow runtime Error", "body": "I am new to trying to use Spleeter after trying to look for a way to split/separate the instruments into individual instrumental stems (keyboard, drum, horns, guitar, etc into its own solo tracks)  played in an original track I am working on having recreated live. I received this error when trying to use the default audio_example.mp3 file as a test. I have the spleeter_master folder downloaded in my download folder, and miniconda3 saved to my Users/UserName folder directory. I installed spleeter using the pip install code displayed here and ran anaconda prompt from the search field.\r\n\r\n(base) C:\\Users\\UserName>spleeter separate -i audio_example.mp3 -p spleeter:2stems -o output\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\username\\miniconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\username\\miniconda3\\Scripts\\spleeter.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\spleeter\\__main__.py\", line 54, in entrypoint\r\n    main(sys.argv)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\spleeter\\__main__.py\", line 36, in main\r\n    enable_logging()\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\spleeter\\utils\\logging.py\", line 60, in enable_logging\r\n    tf_logger = get_tensorflow_logger()\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\spleeter\\utils\\logging.py\", line 27, in get_tensorflow_logger\r\n    from tensorflow.compat.v1 import logging\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\r\n    from tensorflow_core import *\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\username\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\username\\miniconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@wmlmusic \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download [the latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).Please, go through this [link](https://stackoverflow.com/questions/49932993/importerror-dll-load-failed-a-dynamic-link-library-dll-initialization-routin) and see if it helps you\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #23683 #28713 #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Thanks for the follow-up and suggestions. My processor is an AMD Phenom II N970 Quad Core 2.20GHz.\r\n\r\nI downloaded the 64-bit Miniconda that was on the spleeter github link.\r\n\r\nI will try the troubleshooting options you've suggested and reply soon as I do so.", "@wmlmusic \r\n\r\nIs this still an issue?. Please, close this thread if your issue was resolved.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39705\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39705\">No</a>\n"]}, {"number": 39704, "title": "TFLiteConverter.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, CONCATENATION, CONV_2D, FULLY_CONNECTED, GATHER, RESHAPE, REVERSE_V2, SOFTMAX, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dhirajgite,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/39464#issuecomment-630018299) from a similar issue and let us know if it helps. Thanks!", "Tried suggestion from comment worked"]}, {"number": 42803, "title": "Tensorflow\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u300cOverfit and underfit\u300d\u306e\u4e0d\u5099", "body": "\u300c\u521d\u5fc3\u8005\u306e\u305f\u3081\u306eTensorFlow 2.0 \u5165\u9580\u300d\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u3042\u308b\u300coverfit and underfit\u300d\u3067\u65e5\u672c\u8a9e\u7248\u3067\u306f\u3001Google Colab\u3067\u5b9f\u884c\u3059\u308b\u3068\u30e1\u30e2\u30ea\u4e0d\u8db3\u3067\u30af\u30e9\u30c3\u30b7\u30e5\u3059\u308b\u3002", "comments": ["Hi, @jsato0807 thank you for your reporting! Let me take a look.", "I succeed to reproduce your reporting issue. We can reproduce it from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit?hl=ja\r\n\r\nI guess we can fix this issue by updating it, however, I found another issue on the original English notebook. To keep the English notebook as the single source of the truth, I will try to fix this issue by following steps:\r\n\r\n1. Fix the original English notebook.\r\n2. Update [overfit and underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit?hl=ja) notebook.\r\n\r\nAgain, thank you for your reporting!", "\u5ff5\u306e\u305f\u3081\u65e5\u672c\u8a9e\u3067\u3082\u66f8\u3044\u3066\u304a\u304d\u307e\u3059\u3002\r\n\r\n\u624b\u5143\u3067\u518d\u73fe\u3067\u304d\u307e\u3057\u305f\u3002\u6700\u65b0\u7248\u306e\u82f1\u8a9e\u306e notebook \u306b\u8ffd\u5f93\u3059\u308b\u3053\u3068\u3067\u89e3\u6c7a\u3067\u304d\u305d\u3046\u3067\u3059\u304c\u3001\u82f1\u8a9e\u306e notebook \u306b\u3082\u8ab2\u984c\u304c\u767a\u898b\u3055\u308c\u305f\u3068\u3044\u3046\u72b6\u6cc1\u3067\u3059\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3064\u3044\u3066\u306f\u82f1\u8a9e\u7248\u3092\u6b63\u3068\u3059\u308b\u30dd\u30ea\u30b7\u30fc\u306a\u306e\u3067\u3001\u6b21\u306e\u9806\u3067\u5bfe\u5fdc\u3059\u308b\u4e88\u5b9a\u3067\u3059\u3002\r\n\r\n1. \u82f1\u8a9e\u7248\u306e\u8ab2\u984c\u306e\u89e3\u6c7a\r\n2. \u65e5\u672c\u8a9e\u7248\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\r\n\r\n\u306a\u306e\u3067\u3001\u5c11\u3057\u6642\u9593\u304b\u304b\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u304c\u3001\u304a\u5f85\u3061\u3044\u305f\u3060\u3051\u308b\u3068\u3042\u308a\u304c\u305f\u3044\u3067\u3059\u3002", "I created the PR for step 1. https://github.com/tensorflow/docs/pull/1580", "\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002", "https://github.com/tensorflow/docs/pull/1580 was merged so I'm going to close this issue.\r\nFeel free to reopen if there's more to be done. Thanks!\r\n"]}, {"number": 39703, "title": "404 Not found", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):using pip \r\n- TensorFlow version (use command below):2.2.0-rc1\r\n- Python version:3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version:10.1/\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCannot able to import the resnet 50 model from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/resnet50.py\r\n**Describe the expected behavior**\r\nI want to use the above resnet50 model and not the tf.keras.applications.ResNet50 for the intermediate outputs\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom tf.python.eager.benchmarks #not working\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@AlbinDavid \r\nI ran the standalone code shared by you and face an error,please have a look at the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/4c19b8104323e52210001d12f1ae9732/untitled190.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39703\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39703\">No</a>\n"]}, {"number": 39702, "title": "Customized loss function requires eager tensor, but symbolic tensor is passed", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.2.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: Quadro RTX 8000 / 48GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"` v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n\r\n**Describe the current behavior**\r\nI need to apply a binary mask to the model output for computing loss. My current implementation uses a model that takes two inputs (the data and the mask), and use function closure to implement the customized loss. \r\n\r\nHowever, this raises the error \"tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors\".\r\n\r\nApparently, the mask input is treated as symbolic tensor.\r\n\r\n**Describe the expected behavior**\r\n\r\nThis only happens in the eager mode. Apply `disable_eager_execution()` will eliminate the problem. However, I want to know if there is any way to make this work in the eager mode.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThis is the [gist](https://colab.research.google.com/drive/1D_GitVtknoYk33hURTXzC0murA60UBH0?usp=sharing)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimport tensorflow.keras.backend as K\r\n\r\nfrom tensorflow.keras.layers import Input, Flatten, Dense\r\nfrom tensorflow.keras import Model\r\n\r\nx_data = np.zeros((32, 28, 28))\r\nx_mask = np.zeros((32, 10))\r\ny = np.zeros((32, 10))\r\n\r\ninput_data = Input(shape=(28, 28))\r\ninput_mask = Input(shape=(10,))\r\n\r\noutput= Flatten()(input_data)\r\noutput = Dense(64, activation='relu')(output)\r\noutput = Dense(10)(output)\r\nmodel = Model(inputs=[input_data, input_mask], outputs=output)\r\n\r\n\r\ndef custom_loss():\r\n    def loss(y_true, y_pred):\r\n        # This line causes the error\r\n        return K.mean(K.square(y_true - y_pred * model.inputs[1]), axis=-1)   \r\n\r\n        # This line doesn't cause the error\r\n        # return K.mean(K.square(y_true - y_pred), axis=-1)   \r\n    return loss\r\n\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.SGD(),\r\n    loss=custom_loss(),\r\n    metrics=['accuracy'])\r\n\r\nfor i in range (2):\r\n    print(i)\r\n    model.train_on_batch([x_data, x_mask], y)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.2 ,nightly version(2.3.0-dev20200519) and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/fb1237bf883632c27c2c617d100fbf44/untitled913.ipynb).Thanks!", "@chuanli11 Thanks for the issue!\r\n\r\nYes, the `model.inputs` are symbolic Tensors. For Functional Models, these Tensors are used to *build* the Model with a static graph, but in eager mode the Model is then *executed* with a `tf.function`. This means that `model.inputs` can't be used in losses, metrics, etc.\r\n\r\nInstead, here's how I'd recommend achieving your use case. Essentially, `sample_weight` will handle this, rather than trying to access the mask an a `keras.Input`:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.layers import Input, Flatten, Dense\r\nfrom tensorflow.keras import Model\r\n\r\nx_data = np.zeros((32, 28, 28))\r\nx_mask = np.zeros((32, 10))\r\ny = np.zeros((32, 10))\r\n\r\ninput_data = Input(shape=(28, 28))\r\noutput= Flatten()(input_data)\r\noutput = Dense(64, activation='relu')(output)\r\noutput = Dense(10)(output)\r\nmodel = Model(inputs=input_data, outputs=output)\r\n\r\n\r\nclass MyLoss(tf.keras.losses.Loss):\r\n  def call(self, y_true, y_pred):\r\n      return (y_true - y_pred) ** 2  \r\n\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.SGD(),\r\n    loss=MyLoss(name='loss'),\r\n    metrics=['accuracy'])\r\n\r\nfor i in range (2):\r\n    print(i)\r\n    loss = model.train_on_batch(x_data, y, sample_weight=x_mask)\r\n```\r\n\r\nHope that helps!\r\n\r\nClosing out as this is intended behavior", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39702\">No</a>\n", "@omalleyt12 Thank you. This is really helpful.", "@omalleyt12 I'm in a similar situation but I can not convert my code to not use model.inputs. Is there any other workaround for this?\r\n\r\nIn v2.0.1, I was using `experimental_run_tf_function=False` in `model.compile()` to hack around this issue. But doing `run_eagerly=True` on v.2.3.0 doesn't seem to work. \r\n\r\nI have custom losses and metrics both using inputs. I am also using TFRecordDataset.\r\n\r\nUsing `run_eagerly=True`, I get this -> `AttributeError: 'Tensor' object has no attribute 'numpy'`. \r\n\r\nHere is the [**gist**](https://colab.research.google.com/drive/1wpL8WNT7gtOdL6V9_tK5AQ0M6mBPVkNI?usp=sharing)\r\n\r\nAnd as above, I have verified that it works fine without custom losses and metrics.\r\n\r\nAny help is appreciated.", "Tagging @amahendrakar and @jvishnuvardhan as you had tagged this solution in other issue threads.", "@lastmansleeping Can you please create a new issue with the standalone code you provided above? Thanks!"]}, {"number": 39701, "title": "GatherV2Grad prints a deprecation warning", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installed from: binary (from pip)\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nGatherV2Grad prints a deprecation warning when used. The warning is from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L644\r\n\r\n> WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n> Use tf.identity instead.\r\n\r\nThis issue is similar to https://github.com/tensorflow/tensorflow/issues/32049\r\n\r\n**Describe the expected behavior**\r\n\r\nNo deprecation warning.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n  def __init__(self,):\r\n    super(Model, self).__init__()\r\n    self.layer = tf.keras.layers.Dense(100)\r\n  def call(self, x):\r\n    return self.layer(x)\r\n\r\nmodel = Model()\r\nmodel.build((1,2))\r\nweights = model.trainable_variables\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    output = model(tf.zeros([1,2]))\r\n\r\ngradients = tape.jacobian(output, weights, experimental_use_pfor=False)\r\n```\r\n\r\nThis code is based on https://github.com/tensorflow/tensorflow/issues/36227, which was about something else, but does also show the deprecation warning.", "comments": ["@bparr,\r\nTo suppress the warning, add these lines of code to your program\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\nPlease check [this gist](https://colab.research.google.com/gist/amahendrakar/d60beb96146856cbbb00fca1156effe5/39701.ipynb) for reference. Thanks!", "Added a PR #39731 to remove the deprecation message.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 39700, "title": "Yashlocal", "body": "fixed debugging statement and space format", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39700) for more info**.\n\n<!-- need_sender_cla -->", "@yashjain1129 Thank you for your contribution. Can you please sign CLA? Thanks!", "Can you also use a relevant title for the PR please? Also, can you edit the first message field to include a short description?", "- [ ] Please sign CLA\r\n- [ ] Please change issue title\r\n- [ ] Please describe what the change is doing and why. What issue does it solve? What feature does it bring?\r\n\r\nUntil the above checkboxes are ticked, we won't be able to land this.", "I will add more to it then again create another Pull Request. For the above point yes the above suggestion for 'i' instead of 1 so it would be in for loop. Thanks for your response. Although in Reduce Op there is a bug. Can you also look at the issue https://github.com/tensorflow/tensorflow/issues/39792"]}, {"number": 39699, "title": "why the pb model(saved by keras API) structure is so complicated\uff1f", "body": "Hi, I use keras API to save my model in pb format, but the model structure is so complicated!\r\n![image](https://user-images.githubusercontent.com/11664658/82403250-f84d3900-9a90-11ea-92ad-5d2965c16328.png)\r\n\r\n\r\n\r\nOn the contrary, if I save the model in h5 format, the model structure is very simple, which is exactly what I want.\r\n![image](https://user-images.githubusercontent.com/11664658/82403218-e4a1d280-9a90-11ea-8590-95befec014d1.png)\r\n\r\nwhat is the reason, and how can I get a simple model in pb format?", "comments": ["Please provide a reproducible code to investigate. Thanks!", "```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\ndef get_model():\r\n    model = tf.keras.Sequential()\r\n    # Adds a densely-connected layer with 64 units to the model:\r\n    model.add(layers.Dense(64, activation='relu'))\r\n    # Add another:\r\n    model.add(layers.Dense(64, activation='relu'))\r\n    # Add an output layer with 10 output units:\r\n    model.add(layers.Dense(10))\r\n    model.compile(optimizer='adam', loss='mean_squared_error')\r\n    return model\r\n\r\nmodel = get_model()\r\n\r\ntest_input = np.random.random((128,32))\r\ntest_target = np.random.random((128, 1))\r\nmodel.fit(test_input, test_target)\r\n\r\n# Save model in h5 format\r\nmodel.save('./model.h5')\r\n# Save model in pb format\r\nmodel.save('./model_pb')\r\n```\r\n\r\nMy python script is based on [Keras Overview](https://www.tensorflow.org/guide/keras/overview), \r\n tensorflow version is 2.2.0, and platform is Ubuntu 18.04.\r\n\r\n\r\n", "> Please provide a reproducible code to investigate. Thanks!\r\n\r\nHi, My code is here.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\ndef get_model():\r\n    model = tf.keras.Sequential()\r\n    # Adds a densely-connected layer with 64 units to the model:\r\n    model.add(layers.Dense(64, activation='relu'))\r\n    # Add another:\r\n    model.add(layers.Dense(64, activation='relu'))\r\n    # Add an output layer with 10 output units:\r\n    model.add(layers.Dense(10))\r\n    model.compile(optimizer='adam', loss='mean_squared_error')\r\n    return model\r\n\r\nmodel = get_model()\r\n\r\ntest_input = np.random.random((128,32))\r\ntest_target = np.random.random((128, 1))\r\nmodel.fit(test_input, test_target)\r\n\r\n# Save model in h5 format\r\nmodel.save('./model.h5')\r\n# Save model in pb format\r\nmodel.save('./model_pb')\r\n```\r\n\r\nMy python script is based on Keras Overview,\r\ntensorflow version is 2.2.0, and platform is Ubuntu 18.04.\r\n", "@ymodak ", "This is caused due to the tool you are using to visualize the h5 and pb files.\r\nThere are two kinds of graph representations : op-level graph and conceptual graph.\r\nIn your case, the h5 visuals depict conceptual graph where as the pb visuals show op-level graph.\r\nYou may try using tensorboard to visualize your model.\r\nBy default it displays op level graph.\r\nSee https://www.tensorflow.org/tensorboard/graphs#op-level_graph\r\nThanks!", "@ymodak \r\n\r\nIn fact, I use Netron to visualize the model, and I will try it later using Tensorboard, thanks a lot!", "Closing this issue for now. Feel free to reopen if necessary. Thanks!"]}, {"number": 39698, "title": "[tflite] nnapi delegate max_delegated_partitions=0", "body": "make it possible to specify `max_delegated_partitions=0`. In the\r\noriginal code, when `--max_delegated_partitions=0` (or less than 0)\r\nis specified, it's ignored. That is, the maximum number of\r\npartitions is set to 3 (nnapi delegate's default number).", "comments": ["Can you elaborate on the use-case here @freedomtan ? max_delegated_partitions=0 is equivalent to not delegating at all - are you trying to verify if this is true?", "@srjoglekar246 my understanding is `max_delegated_partitions=0` means no limit on the number of partitions, which is the original behavior before this `max_delegated_partitions` was introduced.", "@srjoglekar246 as far as I can tell, benchmark_model set `max_delegated_partitions=0` to be compatible with original behavior. When you feed a model that has more than 3 partitions to `benchmark_model`, since `max_delegated_partitions=0` is ignored, only 3 partitions will be delegated. That's different from previous behavior.", "> @srjoglekar246 my understanding is `max_delegated_partitions=0` means no limit on the number of partitions, which is the original behavior before this `max_delegated_partitions` was introduced.\r\n\r\nI checked delegate source code, and found that this parameter has inconsistent meaning across different delegate implementations. For NNAPI delegate, a \"<=0\" indeed means no limit. But for other delegates (i.e. GPU, Hexagon, CoreML), it means \"no delegatation\".\r\n\r\nTherefore, I'd like to have a change that ensures the consistency of this parameter. I tend to change the NNAPI delegate implementation to follow the literal meaning of \"max_delegated_partitions\". \r\n@galarragas, what're your thoughts on this? It's a breaking change to existing nnapi delegate users who have explicitly specified max_delegated_partitions<=0 in the code.", "@multiverse-tf, @srjoglekar246, and @galarragas either way (no limit or no delegation) is fine if it's consistent across delegates and no unexpected results from existing tools such as the `benchmark_model`. And there should be a way to specify \"no limit\".", "> > @srjoglekar246 my understanding is `max_delegated_partitions=0` means no limit on the number of partitions, which is the original behavior before this `max_delegated_partitions` was introduced.\r\n> \r\n> I checked delegate source code, and found that this parameter has inconsistent meaning across different delegate implementations. For NNAPI delegate, a \"<=0\" indeed means no limit. But for other delegates (i.e. GPU, Hexagon, CoreML), it means \"no delegatation\".\r\n\r\nSorry, I was wrong here after checking the code again. The behavior is indeed inconsistent when a \"<=0\" value is set as the following:\r\n1. GPU delegate: the actual \"max_delegated_partitions\" is set to 1 when the delegate is created. In other words, \"<=0\" is considered as a value to let the actual delegate implementation to choose a default value.\r\n2. Hexagon delegate: the actual \"max_delegated_partitions\" is set to the pre-defined kMaxHexagonGraphs value.\r\n3. CoreML delegate: no limit for the partition.\r\n4. NNAPI delegate: no limit for the partition.\r\n\r\nTherefore, I think we probably need to change the GPU delegate to ensure the consistency.\r\n> \r\n> Therefore, I'd like to have a change that ensures the consistency of this parameter. I tend to change the NNAPI delegate implementation to follow the literal meaning of \"max_delegated_partitions\".\r\n> @galarragas, what're your thoughts on this? It's a breaking change to existing nnapi delegate users who have explicitly specified max_delegated_partitions<=0 in the code.\r\n\r\n", "@multiverse-tf, @freedomtan thanks for checking and good to know that most of the delegates are using 0 to remove the limit. \r\n\r\nAbout this specific change, I think that any way to disable the delegate using the number of partition would be in the end confusing.    ", "TBH, `max_delegated_partitions=0` being interpreted as 'delegate's choice' is a little confusing to me, since the literal interpretation would be 'I want no delegated partitions'. Technically a user should never specify 0 for that value, but some folks might do it as a sanity check or in unit tests.\r\n\r\nI leave the final decision to @multiverse-tf  :-). But lets clarify the meaning of the parameter values in documentation and make it consistent across delegates at the minimum.", "@multiverse-tf Any update on this PR? Please. Thanks!\r\n"]}, {"number": 39697, "title": "Weird block of RNN in TF2.2", "body": "**System information**\r\nThis bug exists in TF v2.2.0-rc4-8-g2b96f3662b 2.2.0 and on windows, linux, both CPU and GPU version. However, it does not exist in TF 2.1.\r\n\r\n**Describe the current behavior**\r\nJust check the code\r\n```python\r\nrnn = tf.keras.layers.GRU(3)\r\nrnn(tf.keras.Input([None, 2]), tf.keras.Input([3, ]))\r\n\r\n\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(None, None, 2)),\r\n                              tf.TensorSpec(shape=(None, 3))])\r\ndef test(x, i):\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        rnn(x, initial_state=i)\r\n\r\n\r\ntest(np.random.randn(3, 2, 2).astype(np.float32),\r\n     np.random.randn(3, 3).astype(np.float32))\r\n```\r\n\r\nThe code will block in `test`, and the memory will grow unlimitedly.\r\n\r\nThe code will also block if `GRU` is replaced by `LSTM`. But it will run well if it is `SimpleRNN`.\r\n\r\nThis bug is a little complicated bacause when only all `rnn(tf.keras.Input...`, `@tf.function(input_signature=...)` and `with tf.GradientTape(persistent=True)...` exist, the block shows up.\r\n", "comments": ["I have tried in colab with TF version 2.1 , 2.2 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/48ae035e8dd66abf4ea0350827b5cc29/untitled910.ipynb).Thanks!", "@ravikyram \ud83d\udc4d", "Thanks for reporting the issue. I think there is some regression/corner case in the tf.function. I will check with core team first and reply if there is any findings.", "Any updates or potential work-arounds for this issue?  For my research I need intermediate jacobian values for use in loss calculation, and assume i need a persistent tape for jacobian wrt input columns and loss wrt model trainable variables.  Can I accomplish the same with nesting of some kind?    ", "@BlueFisher  I tried  to reproduce the issue but  I am not facing errors in TF 2.5 and TF-Nightly.  Could you please confirm it from your end?.  Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/1b48207aab4c2db870acb6b7e9a33eda/untitled82.ipynb).Thanks!", "@saikumarchalla It runs perfectly. Problem finally solved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39697\">No</a>\n"]}, {"number": 39696, "title": "Different SHA256 for mkl_dnn", "body": "in [tensorflow/workspace.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl):\r\n`tf_http_archive(\r\n        name = \"mkl_dnn\",\r\n        build_file = clean_dep(\"//third_party/mkl_dnn:mkldnn.BUILD\"),\r\n        sha256 = \"31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\",\r\n        strip_prefix = \"mkl-dnn-0.21.3\",\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz\",\r\n            \"https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz\",\r\n        ],\r\n    )`\r\nWhen the mkl_dnn is downloaded from the first url( \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz\"), the SHA256  is 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f , which is right.\r\nBut when the mkl_dnn is downloaded from the second url(https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz), the SHA256 is a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec, which will fail to compile.\r\nWhen I unzip the compressed package downloaded from the second url, its directory is oneDNN 0.21.3. But the files in them are exactly the same. Maybe this is the reason. \r\nhope you can check it and fix it. Thanks.\r\n", "comments": ["See #38517 and #38671", "@mihaimaruseac @cxyanhk The issue is the renaming of mkl-dnn to oneDNN, so GitHub re-archives the package (with different directory). The old link in mkl_dnn now is an alias of oneDNN which has a different top-level directory inside the archive.\r\n\r\nCreate a PR #39725 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39696\">No</a>\n"]}, {"number": 39695, "title": "Added initial arm64v8 support for dockerfiles", "body": "Pretty straight forward addition of aarch64 support for dockerfiles, with the exception of installing several python3 packages using apt instead of pip because there is some dependency issue that pip could not satisfy but apt could.  On a positive note, it is much faster to install the pre-built python3 packages than building them.  At the moment this is just the devel-cpu-aarch64.Dockerfile (and jupyter variant), but my intention is to add the cpu-aarch64.Dockerfile as soon as possible.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39695) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39695) for more info**.\n\n<!-- ok -->", "Btw, in case someone is interested in what failed using the normal pip install method, here's the tail end of the log: \r\n[docker_build_failure.log](https://github.com/tensorflow/tensorflow/files/4654492/docker_build_failure.log)\r\n\r\nDoing some digging, the apt install method installed the following packages, which includes some blas and lapack libraries:\r\n[apt_get_install.log](https://github.com/tensorflow/tensorflow/files/4654496/apt_get_install.log)\r\n\r\nHowever, if I first installed using the apt flow and then tried to upgrade those python3 packages via pip (see below) the same error was thrown so maybe compiling scipy really needs mkl, openblas, or atlas.  Note that enum34 needed to be installed last or it would introduce an error before the aforementioned error.  At this point I stopped trying to install these python3 packages via the pip flow and just went with the apt way.\r\n\r\n```\r\nRUN apt-get update && apt-get install -y \\\r\n    python3-pil \\\r\n    python3-h5py \\\r\n    python3-keras-preprocessing \\\r\n    python3-matplotlib \\\r\n    python3-mock \\\r\n    python3-numpy \\\r\n    python3-scipy \\\r\n    python3-sklearn \\\r\n    python3-pandas \\\r\n    python3-portpicker\r\n\r\nRUN python3 -m pip --no-cache-dir install --upgrade \\\r\n    Pillow \\\r\n    h5py \\\r\n    keras_preprocessing \\\r\n    matplotlib \\\r\n    mock \\\r\n    numpy \\\r\n    scipy \\\r\n    sklearn \\\r\n    pandas \\\r\n    portpicker\r\n\r\nRUN python3 -m pip --no-cache-dir install \\\r\n    enum34\r\n```\r\n", "Thanks for the PR!\r\n\r\n- apt packages are going to be much older than the pip installations, aren't they? Can you fork bazelbuild.partial.Dockerfile into an aarch64-specific partial and use that in your image instead? \r\n- Once you make the change, please follow the instructions in the README to regenerate the dockerfiles.", "@angerson You're right, nearly all the apt python packages are much older those available from pip (comparison included below).  I created a specific bazelbuild arm variant partial Dockerfile as you suggested and then re-generated the full Dockerfiles.\r\n\r\n|Package|Pip|Apt (Ubuntu 18.04)|Apt (Ubuntu 20.04)|\r\n|-|-|-|-|\r\n|Pillow|7.1.2|5.1.0-1ubuntu0.2|7.0.0-4build1|\r\n|h5py|2.10.0|2.7.1-2|2.10.0-2build2|\r\n|Keras_Preprocessing|1.1.2|N/A|1.0.5-1|\r\n|matplotlib|3.2.1|2.1.1-2ubuntu3|3.1.2-1ubuntu4|\r\n|mock|4.0.2|2.0.0-3|3.0.5-1build1|\r\n|numpy|1.18.4|1:1.13.3-2ubuntu1|1:1.17.4-5ubuntu3|\r\n|scipy|1.4.1|0.19.1-2ubuntu1|1.3.3-3build1|\r\n|sklearn|0.0|0.19.1-3|0.22.2.post1+dfsg-5|\r\n|pandas|1.0.3|0.22.0-4ubuntu1|0.25.3+dfsg-7|\r\n|portpicker|1.3.1|1.2.0-1|1.3.1-3|", "@angerson After a little digging I found the missing dependencies for scipy if installing via pip.  It appears that this python package is available as a prebuilt wheel file for amd64, but only as source on arm64v8 which requires the three additional packages prepended below.  However, I'm not sure if these are the preferred blas and lapack library implementations, or alternatively from atlas or openblas for example.  How would you like to proceed?\r\n\r\n```\r\nRUN apt-get update && apt-get install -y \\\r\n    gfortran \\\r\n    libblas-dev \\\r\n    liblapack-dev\r\n\r\nRUN python3 -m pip --no-cache-dir install \\\r\n    Pillow \\\r\n    h5py \\\r\n    keras_preprocessing \\\r\n    matplotlib \\\r\n    mock \\\r\n    numpy \\\r\n    scipy \\\r\n    sklearn \\\r\n    pandas \\\r\n    portpicker \\\r\n    enum34\r\n```", "@settle Can you please resolve conflicts? Thanks!", "@gbaned Conflicts should be resolved now, thanks!"]}, {"number": 39694, "title": "TextLineDataset could be more expressive", "body": "TextLineDataset could be more expressive. For instance, it could have more arguments like:\r\n\r\n```python\r\ntrain_dataset = tf.data.TextLineDataset(\r\n                                     file_path, # dataset file path\r\n                                     format, # file format: jsonl, csv, etc.\r\n                                     fields # a set of colums\r\n)\r\n```\r\n\r\nIt would also be incredible if there was a way to indicate how to tokenize each of the fields.", "comments": ["@Ceceu \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "`TextLineDataset` is for reading text lines. For different types of format, there are dedicated tf.data sources, such as [tf.data.experimental.CsvDataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/readers.py#L603) or [JsonIODataset](https://github.com/tensorflow/io/blob/master/tensorflow_io/core/python/ops/json_dataset_ops.py) from the `tensorflow/io` repository."]}, {"number": 39693, "title": "Trying to run l08c09_forecasting_with_cnn.ipynb on spyder: \"WARNING:tensorflow:AutoGraph could not transform <function seq2seq_window_dataset.<locals>.<lambda> at 0x0000024D6C641D38> and will run it as-is.\"", "body": "**System information**\r\n- I am using a stock example script provided in Google Colab\r\nhttps://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c09_forecasting_with_cnn.ipynb#scrollTo=PgYwn9VM8OJi\r\n\r\n- OS Platform and Distribution: Windows 10\r\n\r\n- TensorFlow installed from (source or binary): installed using pip\r\n- TensorFlow version (use command below): tf-nightly\r\n- Python version: 3.7.6\r\n- GPU model and memory: GTX-1060\r\n\r\n**Describe the current behavior**\r\n\r\nTrying to reproduce script from google colab in my PC using Spyder 4.\r\n\r\n**Describe the expected behavior**\r\n\r\nWARNING: AutoGraph could not transform <function seq2seq_window_dataset.<locals>.<lambda> at 0x0000024D6A385EE8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\pedro\\OneDrive\\\u00c1rea de Trabalho\\keras_BTC_cnn.py\", line 131, in <module>\r\n    callbacks=[early_stopping, model_checkpoint])\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 235, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 614, in _process_training_inputs\r\n    distribution_strategy=distribution_strategy)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 706, in _process_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 702, in __init__\r\n    x = standardize_function(x)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 684, in standardize_function\r\n    return dataset.map(map_fn, num_parallel_calls=dataset_ops.AUTOTUNE)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 1591, in map\r\n    self, map_func, num_parallel_calls, preserve_cardinality=True)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 3926, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 3147, in __init__\r\n    self._function = wrapper_fn._get_concrete_function_internal()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2395, in _get_concrete_function_internal\r\n    *args, **kwargs)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 3140, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 3082, in _wrapper_helper\r\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 237, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\n\r\nValueError: in converted code:\r\n\r\n    C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py:677 map_fn\r\n        batch_size=None)\r\n    C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py:2410 _standardize_tensors\r\n        exception_prefix='input')\r\n    C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py:573 standardize_input_data\r\n        'with shape ' + str(data_shape))\r\n\r\n    ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (None, None)\r\n\r\n**Standalone code to reproduce the issue**\r\nLink above", "comments": ["@pedromspereira,\r\nI was able to run the code without any issues with the stable version TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1806d4cb11f6389cbc963a2aae4df1f9/39693.ipynb).\r\n\r\nCould you please check if you are able to run the code with TF v2.2. Thanks!", "Tf 2.2 gives me the same problem as ticket https://github.com/tensorflow/tensorflow/issues/22512\r\n\r\nhad to move back to tf 2.0\r\n\r\nnow when running this part of the code:\r\n\r\nhistory = model.fit(train_set, epochs=500,\r\n                    validation_data=valid_set,\r\n                    callbacks=[early_stopping, model_checkpoint])\r\n\r\nI get the following error:\r\n\r\nEpoch 1/500\r\n      1/Unknown - 0s 7ms/stepWARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\nWARNING:tensorflow:Can save best model only with val_loss available, skipping.\r\n      1/Unknown - 0s 9ms/stepTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-17-aa1ecd95da91>\", line 3, in <module>\r\n    callbacks=[early_stopping, model_checkpoint])\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 487, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 76, in quick_execute\r\n    raise e\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: ^sequential/conv1d/BiasAdd/ReadVariableOp:0\r\n", "> * I am using a stock example script provided in Google Colab\r\n>   https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c09_forecasting_with_cnn.ipynb#scrollTo=PgYwn9VM8OJi\r\n\r\n@pedromspereira,\r\nPlease try running this code in a virtual environment with TF v2.2 and let us know if you are facing the same issue. Thanks!", "now it worked, many thanks"]}, {"number": 39692, "title": "tf.lite.experimental.load_delegate throws exception with try except block", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04, macOS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 & v2.2.0-0-g2b96f3662b 2.2.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCurrently `tf.lite.experimental.load_delegate` throws `OSError` when the passed in library doesn't exist. However when use it with `try except` block, it will throw \r\n\r\n> Exception ignored in: <bound method Delegate.__del__ of <tensorflow.lite.python.interpreter.Delegate object at 0x7f6fc561db00>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 125, in __del__\r\n    if self._library is not None:\r\n        AttributeError: 'Delegate' object has no attribute '_library'\r\n\r\n**Describe the expected behavior**\r\nException should be caught with `try except` block.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nWithout `try except` block, the behavior looks fine\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.lite.experimental.load_delegate('some invalid lib')  # expect an exception\r\n```\r\nWith `try except` block, it throws an exception which can't be caught.\r\n```python\r\nimport tensorflow as tf\r\ntry:\r\n    tf.lite.experimental.load_delegate('some invalid lib')\r\nexcept Exception as e:\r\n    # do something to handle exception\r\n    pass\r\n```\r\n>Exception ignored in: <bound method Delegate.__del__ of <tensorflow.lite.python.interpreter.Delegate object at 0x7f6fc561d518>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 125, in __del__\r\n    if self._library is not None:\r\nAttributeError: 'Delegate' object has no attribute '_library'\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/ac3dd9b17766586f192533578fd101c0/untitled190.ipynb)", "Nupur, could you take a look?", "You may try with;\r\n```python\r\nimport tensorflow as tf\r\ntry:\r\n    tf.lite.experimental.load_delegate('some invalid lib')\r\nexcept tf.errors.InvalidArgumentError as e:\r\n    # do something to handle exception\r\n    pass\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39692\">No</a>\n"]}, {"number": 39691, "title": "Cadence HiFi Mini NN Library: Integrated optimized kernels", "body": "Integrated Cadence HiFi Mini optimized functions for \r\nfully_connected, softmax and svdf kernels.\r\nThese optimized functions gives better performance for keyword_benchmark application.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39691) for more info**.\n\n<!-- need_sender_cla -->", "@bhanuprakashbv Thank you for your contribution. Can you please sign CLA? Thanks!\r\n", "Hi,\r\n\r\nCould you please help me to sign CLA. I did already, which is showing like below.\r\nI don\u2019t know how to add my contributions without CLA issues.\r\n\r\nThanks,\r\nBhanu\r\n[cid:image002.jpg@01D62E83.185E8750]\r\n\r\nFrom: gbaned <notifications@github.com>\r\nSent: Wednesday, May 20, 2020 8:40 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Bhanu Prakash Bandaru Venkata <bhanup@cadence.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Cadence HiFi Mini NN Library: Integrated optimized kernels (#39691)\r\n\r\nEXTERNAL MAIL\r\n\r\n@bhanuprakashbv<https://urldefense.com/v3/__https:/github.com/bhanuprakashbv__;!!EHscmS1ygiU1lA!R4RVTw6SUjDGoAlsGbl1uPagyqAZ0bgLUTCslWMFnPithg7VZ4e6hmOUfGNIZg$> Thank you for your contribution. Can you please sign CLA? Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://urldefense.com/v3/__https:/github.com/tensorflow/tensorflow/pull/39691*issuecomment-631208437__;Iw!!EHscmS1ygiU1lA!R4RVTw6SUjDGoAlsGbl1uPagyqAZ0bgLUTCslWMFnPithg7VZ4e6hmMjGO8Lcg$>, or unsubscribe<https://urldefense.com/v3/__https:/github.com/notifications/unsubscribe-auth/APAXUL6LKXXYF46UMOBA3R3RSNCZTANCNFSM4NFH2HVA__;!!EHscmS1ygiU1lA!R4RVTw6SUjDGoAlsGbl1uPagyqAZ0bgLUTCslWMFnPithg7VZ4e6hmPmA23P_w$>.\r\n", "@googlebot I signed it!\r\nAfter signing CLA, status is like below.\r\nI don\u2019t know whether it is completed or anything else do I need to do.\r\nCould you please help me to resolve.\r\n\r\n[cid:image002.jpg@01D62E85.01FCA170]\r\n\r\nFrom: googlebot <notifications@github.com>\r\nSent: Wednesday, May 20, 2020 12:01 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Bhanu Prakash Bandaru Venkata <bhanup@cadence.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Cadence HiFi Mini NN Library: Integrated optimized kernels (#39691)\r\n\r\nEXTERNAL MAIL\r\n\r\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n\r\n\ud83d\udcdd Please visit https://cla.developers.google.com/<https://urldefense.com/v3/__https:/cla.developers.google.com/__;!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJuI_3Bmyg$> to sign.\r\n\r\nOnce you've signed (or fixed any issues), please reply here with @googlebot I signed it! and we'll verify it.\r\n\r\n________________________________\r\nWhat to do if you already signed the CLA\r\nIndividual signers\r\n\r\n  *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data<https://urldefense.com/v3/__https:/cla.developers.google.com/clas__;!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJutNgbAwQ$> and verify that your email is set on your git commits<https://urldefense.com/v3/__https:/help.github.com/articles/setting-your-email-in-git/__;!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJsY5Zp0-A$>.\r\n\r\nCorporate signers\r\n\r\n  *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot<https://urldefense.com/v3/__http:/go/cla*troubleshoot__;Iw!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJt3CD3bUA$> (Public version<https://urldefense.com/v3/__https:/opensource.google/docs/cla/*troubleshoot__;Iw!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJvb3g-S6g$>).\r\n  *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data<https://urldefense.com/v3/__https:/cla.developers.google.com/clas__;!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJutNgbAwQ$> and verify that your email is set on your git commits<https://urldefense.com/v3/__https:/help.github.com/articles/setting-your-email-in-git/__;!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJsY5Zp0-A$>.\r\n  *   The email used to register you as an authorized contributor must also be attached to your GitHub account<https://urldefense.com/v3/__https:/github.com/settings/emails__;!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJtkeFYs8Q$>.\r\n\r\n\u2139\ufe0f Googlers: Go here<https://urldefense.com/v3/__https:/goto.google.com/prinfo/https*3A*2F*2Fgithub.com*2Ftensorflow*2Ftensorflow*2Fpull*2F39691__;JSUlJSUlJQ!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJtsx1yGbQ$> for more info.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://urldefense.com/v3/__https:/github.com/tensorflow/tensorflow/pull/39691*issuecomment-631001647__;Iw!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJvZkOtStw$>, or unsubscribe<https://urldefense.com/v3/__https:/github.com/notifications/unsubscribe-auth/APAXUL7CY6PU3NSCYA2TZE3RSLF67ANCNFSM4NFH2HVA__;!!EHscmS1ygiU1lA!U1CXd1P6PflKpAV07uidcN3heJCkMB2tOgH6Cs1UdcXKrS-NdmGdiJukQBXF9Q$>.\r\n", "@bhanuprakashbv It still shows CLA is pending, can you use please make sure to use same GitHub username and email-id associated with it.", "Hi,\r\n\r\nI am using same user name and email-id but I don\u2019t know how to sign corporate CLA.\r\nCould you please help/guide me to do that.\r\n\r\nThanks,\r\nBhanu\r\n\r\nFrom: gbaned <notifications@github.com>\r\nSent: Wednesday, May 27, 2020 6:58 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Bhanu Prakash Bandaru Venkata <bhanup@cadence.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Cadence HiFi Mini NN Library: Integrated optimized kernels (#39691)\r\n\r\nEXTERNAL MAIL\r\n\r\n@bhanuprakashbv<https://urldefense.com/v3/__https:/github.com/bhanuprakashbv__;!!EHscmS1ygiU1lA!Rp4jUycEPP-zLbAaIRkAy2GtgzDKaAA-Nli8CnnwL91dRyM2YlOtLcKjER0u_Q$> It still shows CLA is pending, can you use please make sure to use same GitHub username and email-id associated with it.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://urldefense.com/v3/__https:/github.com/tensorflow/tensorflow/pull/39691*issuecomment-634660738__;Iw!!EHscmS1ygiU1lA!Rp4jUycEPP-zLbAaIRkAy2GtgzDKaAA-Nli8CnnwL91dRyM2YlOtLcKAtn61Wg$>, or unsubscribe<https://urldefense.com/v3/__https:/github.com/notifications/unsubscribe-auth/APAXUL2YKIO3JUPA4YZTER3RTUIMHANCNFSM4NFH2HVA__;!!EHscmS1ygiU1lA!Rp4jUycEPP-zLbAaIRkAy2GtgzDKaAA-Nli8CnnwL91dRyM2YlOtLcLCfY5mzw$>.\r\n", "@googlebot I signed it! ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39691) for more info**.\n\n<!-- ok -->", "@bhanuprakashbv  Can you please resolve conflicts? Thanks!", "@bhanuprakashbv Can you please resolve conflicts? Thanks!", "> Apologies for the delay. We have shifted optimized kernels around internally to make space for this PR.\r\n\r\nHi njeffrie,\r\nDo I need to do any rebase to resolve the conflicting files?\r\nThanks,\r\nBhanu\r\n", "@bhanuprakashbv yes please rebase and resolve conflicts.", "> @bhanuprakashbv yes please rebase and resolve conflicts.\r\n@rthadur did rebase and resolved the conflicts.\r\n", "> It's been brought to my attention that we need a tensorflow license in all source files we check in, similar to what you have in tensorfllow/lite/micro/kernels/xtensa-hifimini/*.cc\r\n> \r\n> Please update all files in this change to make sure the tensorflow license text can be found in all of them.\r\n\r\n@njeffrie Added tensorflow license in all source files. Please check and let me know.\r\nThanks,\r\nBhanu", "> @bhanuprakashbv can you please make below changes , thank you\r\n\r\n@rthadur I have updated code based on your comments. Could you please check and let me know.\r\n\r\nThanks,\r\nBhanu", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 39690, "title": "Add explicit block_size to TriangularSolveExpander constructor", "body": "This small patch allows passing block_size explicitly, removing hardcoded\r\nvalue of 128.\r\n\r\nProvide test for triangular solve expander using different block_size values", "comments": ["@mattjj Can you please review this PR ? Thanks!", "Thanks for the ping! For some reason these notifications only went to an old email address that I don't usually check.\r\n\r\nI'm not sure if I'm a great choice for reviewer because I don't know C++, so for example I don't know what the syntax on line 457 here means. The edits here look pretty simple so maybe I can figure them out, but alternatively I wonder if this would be much quicker work for @hawkinsp. Peter, are you up for reviewing this?", "Addressed issue with buildifier, but it seems that it requires approval again. Is it preferable to add commits to pull request, not rewrite them?  "]}, {"number": 39689, "title": "Unclear Documentation for Keras SavedModel format", "body": "In the [documentation](https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format) for keras for saving a model, it states that you can save a model in a SavedModel format by passing in a name for a directory. This creates a directory with a pb file and 2 other folders containing assets and weights. \r\n\r\nThis doesn't seem to me to be a true SavedModel format as it's not a single pb file. Is there documentation explaining how to create the single pb that can be used for loading SavedModels (like Tensorflow Java or DL4J)? Loading doesn't seem to be possible from a directory, only a SavedModel pb file.\r\n\r\nMaybe I'm missing something? Thank you!\r\n", "comments": ["Though `model.save() or tf.keras.models.save_model()` \r\ncreates a folder containing model file along with assets and variables folder. This is the new and updated method of saving the model.\r\nThis saves the complete model including the weights, optimizer info and the information used using the compilation of model. \r\nBut,\r\nyou can save it in a singhle file by using\r\n`model.save('model_name',format='h5') or model.save('model_name.h5')`\r\nor\r\n`model.save('model_name.pb')`", "@sushantag9  `model.save('model_name.pb')` does not save a single pb file. It creates a folder `model_name.pb` that contains the same contents.\r\n\r\n![Screen Shot 2020-05-19 at 5 04 48 PM](https://user-images.githubusercontent.com/22605641/82378312-e5842700-99f2-11ea-94b4-1ed1153016c4.png)\r\n\r\nAnd h5 is considered old and not recommended by the [Tensorflow Docs](https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading)\r\n\r\nIs there not a way to create a single protobuf file from a keras model?\r\n\r\n", "# Save to ./model/tf_model.pb `\ntf.train.write_graph(frozen_graph, \"model\", \"tf_model.pb\", as_text=False)`\nIn TF 2.0 there is no way to save it directly to a single file.\nThe above code can be used to save the graph but by using older version of TF\n", "@Ben-Epstein \r\n\r\nPlease, go through the[ link](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and see if it helps you.Thanks!", "@ravikyram  Thank you for the link. That's the source of my confusion, is it possible to save the model as a single .pb file? It seems like it must be a directory. ", "@Ben-Epstein I think the confusion is between `Freeze graph` API and `SavedModel` API. `Freeze graph` API is to freeze tensorflow graph and related files in a session and serve it for inference. However, `Freeze graph` API works only in TF1.x and is deprecated in TF2.x. In TF2.x, the primary export format is SavedModels so APIs are built to directly support SavedModels. So, you can use `SavedModel` for saving model and serving for inference.\r\n  \r\n\r\n> Loading doesn't seem to be possible from a directory, only a SavedModel pb file.\r\n\r\nThe *.pb file contains only the Tensorflow graph of your model. So, loading only graph doesn't work as it requires `Assets` and `variables` for tuning the saved model and inference. Please check [here](https://www.tensorflow.org/guide/saved_model) for more details on `SavedModel` API.\r\n", "Understood. My use case requires a single object (not a directory) so I will have to stick with the older h5 format. Thanks for the help!"]}, {"number": 39688, "title": "Wrong with converted tflite model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10\r\n- TensorFlow installed from (source or binary):pip install\r\n- TensorFlow version (or github SHA if from source):1.13.1\r\n I downloaded [ssd_mobilenet_v2_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz) and tried to convert it to tflite format using following script:\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\ngraph_def_file = \"E:/Tensorflow_detection_model_zoo/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph.pb\"\r\ninput_arrays=[\"normalized_input_image_tensor\"]\r\noutput_arrays=['TFLite_Detection_PostProcess', 'TFLite_Detection_PostProcess:1', 'TFLite_Detection_PostProcess:2', 'TFLite_Detection_PostProcess:3']\r\ninput_shape={\"normalized_input_image_tensor\": [1, 300, 300, 3]}\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shape)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops = True\r\nconverter.post_training_quantize = True\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0]: (0., 1.)}  # mean, std_dev\r\ntflite_uint8_model = converter.convert()\r\nopen(\"uint8_model_converted_from_\"+os.path.basename(os.path.dirname(graph_def_file))+\".tflite\", \"wb\").write(tflite_uint8_model)\r\n```\r\nSo i got the output file : **uint8_model_converted_from_ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tflite**.\r\nThen i used the following script to test this tflite model:\r\n```\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nimport cv2 as cv\r\nfrom PIL import Image\r\nprint(tf.__version__)\r\nprint(os.getcwd())\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"uint8_model_converted_from_ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(\"input_details : \", input_details)\r\nprint(\"output_details : \", output_details)\r\n\r\nimage = cv.imread(\"test_images/traffic.png\")\r\nresize_img = cv.resize(image, (300, 300), interpolation=cv.INTER_CUBIC)\r\nreshape_image = resize_img.reshape(300, 300, 3)\r\nimage_np_expanded = np.expand_dims(reshape_image, axis=0)\r\nimage_np_expanded = image_np_expanded.astype('uint8')\r\ninterpreter.set_tensor(input_details[0]['index'], image_np_expanded) \r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\noutput_data_1 = interpreter.get_tensor(output_details[1]['index'])\r\noutput_data_2 = interpreter.get_tensor(output_details[2]['index'])\r\noutput_data_3 = interpreter.get_tensor(output_details[3]['index'])\r\n\r\noriginal_image_height, original_image_width, _ = image.shape\r\ndetect_image = cv.rectangle(image,\r\n                            (int(output_data[0][0][1]*original_image_width),  \r\n                             int(output_data[0][0][0]*original_image_height)),\r\n                            (int(output_data[0][0][3]*original_image_width),\r\n                             int(output_data[0][0][2]*original_image_height)),\r\n                            (0, 255, 0), 3)\r\nImage.fromarray(detect_image).save('test_result.png')\r\nprint(original_image_width)\r\nprint(int(output_data[0][0][3]*original_image_width))\r\nprint(original_image_height)\r\nprint(int(output_data[0][0][2]*original_image_height))\r\ncv.imshow('detect_result', detect_image)\r\ncv.waitKey(0)\r\n```\r\nBut after i run this script, pycharm report this error:\r\n![YIOczF.png](https://s1.ax1x.com/2020/05/19/YIOczF.png)\r\nObviously the shape is array(**[1, 0, 4]**) which is unnormal, the correct one should be array([**1, 10, 4]**) where  10 means output **10 classes**. So where is error come from ? and how to fix it ?", "comments": ["Was able to reproduce the issue. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/68352df5fa8d2bfa8a79117a88aa9dc7/39688.ipynb). Thanks!", "@srjoglekar246 could you take a look?", "Actually, 1.13.1 is quite old, can you please try with the latest release (2.2.0)?", "Ah sounds like it still repros w/ the latest nightly.", "@wwdok Can you check with the following converter parameters:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shape)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.allow_custom_ops = True\r\n# converter.post_training_quantize = True\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0]: (128.0, 128.0)}  # mean, std_dev\r\ntflite_uint8_model = converter.convert()\r\n```\r\n\r\nYou don't need `post_training_quantize` since the model is already quantized. And the mean & std_dev for converting quantized SSD models is 128, 128 as mentioned [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md#running-on-mobile-with-tensorflow-lite)\r\n", "@wwdok Thanks @srjoglekar246 It is working as expected. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/321551f11317d0789027fab2f63186ff/ssd_mobilenet_quantized_model_tflite_converter.ipynb). I tried with `tf-nightly` but you could try `TF1.15.2` also.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@srjoglekar246 @jvishnuvardhan @jdduke @amahendrakar .Thanks everyone ! Now it works correctly !"]}, {"number": 39687, "title": "Getting error while making a custom layer", "body": "```\r\nclass FiltersChangeResidualBlock(Layer):\r\n\r\n    def __init__(self, out_filters, **kwargs):\r\n        \"\"\"\r\n        The class initialiser should call the base class initialiser, passing any keyword\r\n        arguments along. It should also set the number of filters as a class attribute.\r\n        \"\"\"\r\n        super(FiltersChangeResidualBlock, self).__init__(**kwargs)\r\n        self.out_filters = out_filters\r\n        \r\n        \r\n    def build(self, input_shape):\r\n        \"\"\"\r\n        This method should build the layers according to the above specification. Make sure \r\n        to use the input_shape argument to get the correct number of filters, and to set the\r\n        input_shape of the first layer in the block.\r\n        \"\"\"\r\n        self.bn1_1 = BatchNormalization(input_shape=input_shape)\r\n        self.conv1_1 = Conv2D(input_shape[1],(3,3), padding='SAME')\r\n        self.bn2_2 = BatchNormalization()\r\n        self.conv2_2 = Conv2D(self.out_filters,(3,3), padding='SAME')\r\n        self.conv3_3 = Conv2D(self.out_filters, (1,1), padding='SAME')\r\n\r\n\r\n        \r\n        \r\n    def call(self, inputs, training=False):\r\n        \"\"\"\r\n        This method should contain the code for calling the layer according to the above\r\n        specification, using the layer objects set up in the build method.\r\n        \"\"\"\r\n        h = self.bn1_1(inputs, training=training)\r\n        h = tf.nn.relu(h)\r\n        h = self.conv1_1(h)\r\n        h = self.bn2_2(training=training)(h)\r\n        h = tf.nn.relu(h)\r\n        h = self.conv2_2(h)\r\n        x = self.conv3_3(inputs)\r\n        k = Add()([x,h])\r\n        return k\r\n\r\n\r\ntest_model = tf.keras.Sequential([FiltersChangeResidualBlock(16, input_shape=(32, 32, 3), name=\"fc_resnet_block\")])\r\ntest_model.summary()\r\n ```\r\nWhile executing the above code I'm getting the below error.\r\n```\r\nValueError: in user code:\r\n\r\n    <ipython-input-28-8301127c223d>:40 call  *\r\n        h = self.bn2_2(training=training)(h)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:800 __call__  **\r\n        'The first argument to `Layer.call` must always be passed.')\r\n\r\n    ValueError: The first argument to `Layer.call` must always be passed.\r\n```", "comments": ["Hey, @Saduf2019 any idea why this is causing?", "@abhinavsp0730 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n"]}, {"number": 39686, "title": "Unresolved external symbol C ++ version", "body": "## Operating environment\r\nwindows 10\r\nvs 2019\r\ntensorflow 2.1 C++\r\n\r\n## error\r\nCompile the C ++ version of tensorflow 2.1, and report unresolved external symbols when using it\r\n![image](https://user-images.githubusercontent.com/57739867/82340969-6c042d00-9a22-11ea-9e77-a8778c52b04e.png)\r\n\r\n\r\n\r\n## my code\r\n#include <sstream>\r\n#include <iostream>\r\n\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include <tensorflow/core/framework/graph.pb.h>\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/protobuf/meta_graph.pb.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/cc/saved_model/loader.h\"\r\n\r\n#include <fstream>\r\n#include <iostream>\r\n#include <string>\r\n#include <vector>\r\n#include <stdlib.h>\r\n\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n//#include \"tensorflow/core/lib/core/threadool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\nusing namespace tensorflow;\r\nusing tensorflow::Flag;\r\nusing tensorflow::Status;\r\nusing tensorflow::string;\r\nusing tensorflow::Tensor;\r\n\r\nusing std::string;\r\n\r\n//using namespace tensorflow;\r\n\r\nint main(int argc, char argv[]) {\r\n\tstd::string model_path = R\"(E:/pro_vs2019/test_vs2019/tf_model/)\";\r\n\r\n\tSession* session;\r\n\tSessionOptions options;\r\n\toptions.config.mutable_gpu_options()->set_visible_device_list(\"0\"); //\u8bbe\u7f6e\u4f7f\u7528\u7684gpu\r\n\toptions.config.mutable_gpu_options()->set_allow_growth(true); //\u8bbe\u7f6eGPU\u5185\u5b58\u81ea\u52a8\u589e\u957f\r\n\tTF_CHECK_OK(NewSession(options, &session));//\u521b\u5efa\u65b0\u4f1a\u8bddSession\r\n\tGraphDef graphdef; //Graph Definition for current model\r\n\tTF_CHECK_OK(ReadBinaryProto(Env::Default(), model_path, &graphdef)); //\u4ecepb\u6587\u4ef6\u4e2d\u8bfb\u53d6\u56fe\u6a21\u578b;\r\n\tTF_CHECK_OK(session->Create(graphdef)); //\u5c06\u6a21\u578b\u5bfc\u5165\u4f1a\u8bddSession\u4e2d;\r\n\r\n\t//tensorflow::Status status;\r\n\t//// SessionOptions sessionOptions;\r\n\t//// RunOptions runOptions;\r\n\t//// SavedModelBundle bundle;\r\n\r\n\t//// load everything\r\n\t//// status = LoadSavedModel(sessionOptions, runOptions, model_path, { \"serve\" }, &bundle);\r\n\r\n\t//GraphDef graphdef;\r\n\r\n\t//status = ReadBinaryProto(Env::Default(), model_path, &graphdef);\r\n\r\n\t//if (!status.ok()) {\r\n\t//\tstd::cout << status.ToString() << std::endl;\r\n\t//\treturn -1;\r\n\t//}\r\n\t//else std::cout << \"Succesfully have model loaded: \" << model_path << std::endl;\r\n\r\n\treturn 0;\r\n}", "comments": ["Please:\r\n\r\n1. fill in issue template\r\n1. minimize your code to only have the needed lines to reproduce the issue\r\n1. use proper markdown formatting around code and error blocks so it is readable", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39686\">No</a>\n"]}, {"number": 39685, "title": "Unable to import Tensor Flow - Windows10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version:Tensor flow 2.1\r\n- Python version:Python 3.7.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version:Cuda 10.2.89\r\n- GPU model and memory:NVidia GE Force GTX1050\r\n\r\n**Describe the problem**\r\nI have tried installing tensor flow from both conda as well pip and also tried creating venv.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTyped std command: \r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n\r\n\r\n**Any other info / logs**\r\n(venv) C:\\Users\\Shree>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Shree\\anaconda3\\envs\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nPlease help me in resolving issue. \r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@nikhilg108,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from a similar issue and let us know if it helps. Thanks!", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nJust to sample over 100 similar issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\nPlease make sure you do a search in the future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39685\">No</a>\n"]}, {"number": 39684, "title": "How to create an NLP pipeline in Tensorflow 2 in a simple and efficient approach?", "body": "Using a `TorchText` to create a NLP pipeline is simple as one, two, three:\r\n\r\n```python\r\n# 1. indicate the fields and how to tokenizer them:\r\nTEXT = data.Field(\r\n    tokenize=tokenizer.encode,\r\n    batch_first=True,\r\n    fix_length=32\r\n)\r\nLABELS = data.Field()\r\n\r\n# 2. bind it to the dataset path\r\ntrain, val, test = data.TabularDataset.splits(\r\n    path=dataset_path,\r\n    train='train.json',\r\n    validation='val.json',\r\n    test='test.json', \r\n    format='json',\r\n    fields=[('text', TEXT), ('labels', LABELS)])\r\n\r\n# 3. get the train, val and test batches\r\ntrain_iter, valid_iter, test_iter = data.BucketIterator.splits(\r\n    (train, valid, test), batch_sizes=(16, 256, 256)\r\n)\r\n \r\n```\r\n\r\nThis is what you expect from a Deep Learning framework in which you don't need a lot of coding simply to prepare the data for each new project.\r\n\r\nIs there anything similar in Tensorflow 2?\r\n", "comments": ["@Ceceu \r\nPlease check these links for reference [link](https://towardsdatascience.com/best-practices-for-nlp-classification-in-tensorflow-2-0-a5a3d43b7b73) [link1](https://cs230.stanford.edu/blog/datapipeline/) [link2](https://stackoverflow.com/questions/59177925/tensorflow-how-to-build-efficient-nlp-pipeline-using-tf-dataset) \r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nAs i see you have already raised this on stack over flow i am moving this issue to closed status.\r\n\r\n"]}, {"number": 39683, "title": "Added Dot op", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39683) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 39682, "title": "Please Providing tensorflowLite dynamic library  for iOS platform.", "body": "Please Providing tensorflowLite dynamic library  for iOS platform.", "comments": ["@JunyiXie \r\n\r\nCan you go through the [link1](https://www.tensorflow.org/lite/guide/ios) and [link2](https://www.tensorflow.org/lite/examples) and see if it helps you.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39682\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39682\">No</a>\n"]}, {"number": 39681, "title": "Colab Runtime Crashes training on musdb18", "body": "\r\n\r\n**System information**\r\n- Have I written custom code :Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Using Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):2.0\r\n- Python version:3.6\r\n\r\nI have built an audio separation model and am trying to train it on the musdb18 dataset. It contains a total of 150 audio tracks, 100 for training and 50 for test. I load the dataset like: I load a song, preprocess it, then feed it to the network. Train my model for one epoch, then load another song and train it. I don't think it should exceed RAM limit, but somehow the runtime crashes after a few epochs, with a message that the runtime crashed using all RAM. Could someone please guide me why this is so.\r\nWhen I trained my previous tensordlow model on a single song it worked smoothly.", "comments": ["@tantheta01,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the dataset you are using. Thanks!", "@tantheta01 \r\nThe colab has a memory restriction while training on larger data or very deep models, so try using the data loaders/generators which loads the data in batches during training, testing and validation.\r\n**This helps a lot in keeping colab notebook from crashing**\r\nYou can refer to the documentation [Data Generators](https://keras.io/api/preprocessing/image/#image-data-preprocessing)\r\n\r\nYou need to use `model.fit_generator()` while using them.", "@sushantag9 \r\nI had seen Data Generators earlier but I couldn't figure out how to use them for audio datasets.\r\nMy dataset is composed of mp4 files and I extract wav audio files from it for training.\r\nCould you please suggest how to use the Data Generators for this purpose", "@amahendrakar \r\nhttps://colab.research.google.com/drive/1PbjaZHC7S2ot-8sYDVr6_SRCRxy-5Zwa?usp=sharing\r\nThis is my colab notebook\r\nI am using the [MUSDB18 Dataset](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)\r\nIt has restricted access thus [Link to the folder on my drive](https://drive.google.com/drive/folders/11kKsY6UJVtib-a2nZ9cHfH14mAF9QG7L?usp=sharing)", "I have tried using the fit generator, the issue is that the RAM utilization is exceeding the allocated memory.\r\nYou can use GCP for training the model, there you can allocate a larger cluster and them train the model. Initially you will get $300 for first year for free any the service is 'pay as you go'.", "@sushantag9  I can use GCP but I don't think the free tier version of GCP comes with access to a GPU or a TPU or can be availed using the credit we recieve. It would be very hard to train my model on a CPU I guess.", "In GCP you can create a virtual machine and use the terminal to run your code. There are many kinds of options available.\n\nRefer to this link https://cloud.google.com/free/\nThose $300 can be used to host a compute engine with a larger RAM and with a GPU support.\n\nYou can also install jupyter notebook in it and use it from external IP of the compute engine ", "@sushantag9 \r\nI have initiated creating an account at GCP so I guess it shall be created in a day. Then I shall try to train my model on that. Thanks.", "> I have initiated creating an account at GCP so I guess it shall be created in a day. Then I shall try to train my model on that. Thanks.\r\n\r\n@tantheta01,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "> https://colab.research.google.com/drive/1PbjaZHC7S2ot-8sYDVr6_SRCRxy-5Zwa?usp=sharing\r\n> This is my colab notebook\r\n\r\n@tantheta01,\r\nI am facing an error stating `TypeError: datapreprocessing() missing 1 required positional argument: 'file'` while running the code. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/cc56f1c4fc5ac3aeb6f9ad06f9db91fa/39681.ipynb#scrollTo=YHZpoXMPc5Ix). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39680, "title": "TF 2.2: Build failure on Win10 (Bad address issue)", "body": "Dear experts,\r\n\r\nI am trying to compile Tensorflow 2.2.0 from source on a Win 10 system, including GPU support. My actual goal is compile the dll (but I tried to build the pip package as well and it does not seem to make a difference for this issue). I have been following the instructions on the official website (https://www.tensorflow.org/install/source_windows) as closely as possible, and have started from a completely fresh Windows install. However, I run into the issue that the build, at some point, always aborts with a strange \"Bad address\" failure of some tool (see below). I have no idea what else to try or how to get a more meaningful hint to the problem. Please help me out with advice here. Thanks in advance!\r\n\r\n\r\n<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 1809\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.5.6 (also tried 3.7.7, same issue)\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): MS Visual Studio Buildtools 2019 (v 14.25.28610)\r\n- CUDA/cuDNN version: CUDA 10.1 Update 2 / cuDNN 7.6.5.32\r\n- GPU model and memory: NVidia Tesla K80\r\n\r\n- MSYS2: packges as required in installation instructions of TF, all updated to the latest version\r\n\r\n\r\n**Describe the problem**\r\nThe build aborts with a message like the following:\r\n\r\nINFO: Analyzed target //tensorflow:tensorflow.dll (174 packages loaded, 15684 targets configured).\r\nINFO: Found 1 target...\r\n**ERROR: C:/users/admin.ml/_bazel_admin.ml/mamyapdv/external/llvm-project/llvm/BUILD:45:1: Executing genrule @llvm-project//llvm:config_gen failed (Exit 126)\r\n/usr/bin/bash: bazel-out/x64_windows-opt/bin/third_party/llvm/expand_cmake_vars.exe: Bad address**\r\nTarget //tensorflow:tensorflow.dll failed to build\r\n\r\nDuring multiple attempts, I saw different executables failing with the \"Bad address\" issue, so it seems to be related to some non-deterministic behaviour in the tool chain.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n>python configure.py\r\nYou have bazel 2.0.0 installed.\r\nPlease specify the location of python. [Default is C:\\ProgramData\\Miniconda3\\python.exe]:\r\n\r\nFound possible Python library paths:\r\n  C:\\ProgramData\\Miniconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\ProgramData\\Miniconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10.1\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:/cuDNN/cuda,C:/cuDNN/cuda/bin,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/bin\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/cuDNN/cuda/lib/x64\r\n    C:/cuDNN/cuda/include\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n[Default is: 3.5,7.0]:\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\n\r\n>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings tensorflow:tensorflow.dll\r\n```\r\n\r\n", "comments": ["Most likely this is some llvm issue, not a TF one.", "Actually I don't think it is llvm-related. As I wrote above, I am getting this kind of errors during compilation of all kinds of rules, seemingly at random. For example, I just got:\r\n\r\n```\r\nERROR: C:/tf/tensorflow-2.2.0/tensorflow/core/util/BUILD:345:1: Executing genrule //tensorflow/core/util:version_info_gen failed (Exit 126)\r\n/usr/bin/bash: bazel-out/x64_windows-opt/bin/tensorflow/tools/git/gen_git_source.exe: Bad address\r\nTarget //tensorflow:tensorflow.dll failed to build\r\n```\r\n\r\nSo I believe part of the build toolchain has a problem, but I have no idea which part. Has anyone ever seen something like this or could give me a hint how to progress in debugging?", "Ok, I made one more observation after making Bazel print its executed commands. The failing commands so far all seem to start with the following:\r\n\r\n`C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; <some_other_command>\r\n`\r\n\r\nWhen I try to run this line interactively, it also does not work. The reason is that quotes are missing:\r\n\r\n`C:/msys64/usr/bin/bash.exe -c \"source external/bazel_tools/tools/genrule/genrule-setup.sh\"\r\n`\r\n\r\nwould be working. Who is responsible for assembling this command? Is it Bazel, or the TF build instructions? And could anybody with a working Windows build confirm that quotes are either used or not required there, for whatever reason? Thanks!\r\n\r\n\r\n", "I do not think this is related to anything in TF build at all.\r\nCould you first confirm that your OS, and msys installations are all 64 bits?\r\nNext, could you try just running `git` under msys?\r\n\r\nIf you see the same failure, it may be related to this:\r\nhttps://stackoverflow.com/questions/41699029/cant-run-git-in-git-bash-bash-mingw32-bin-git-bad-address", "@SchJens, have you found a file which is responsible for the generated commands without the quotes?", "@SchJens, \r\nPlease update as per above comment", "Unfortunately I have not yet found the place where these commands are created. Would you have any hint how to identify that place? And how can it be that this is a problem just on my machine?\r\n\r\nRegarding 64bit: I verified that both, OS and MSYS are actually 64 bit.", "Sadly, I have no hint about the solution. I faced the same one and couldn't find the right place to fix it. Now I'm trying to build the library on Linux OS. There are no such problem, at least on master branch.", "We need to include this =>  label:\"stat:awaiting response\" ", "@SchJens I'm curious, which Windows shell/command prompt are you using to run the build that gives this \">\" prefix, or was it changed manually when posted here? A regular `cmd.exe` is the one best for building TF.\r\n\r\nInstead of the miniconda, one might also try as clean and minimalistic env for building as possible.\r\n\r\nSomething as simple as\r\n```\r\n\\py38\\python -m venv \\venv-tf2build\r\n\\venv-tf-build\\Scripts\\activate\r\npython -m pip install --upgrade pip\r\npip install six numpy wheel keras_applications keras_preprocessing h5py --no-deps\r\n```\r\nFor python, always worth double-checking with `python -VV` that it's 64-bit and the expected version.\r\n\r\nAfter everything works, conda could be perfectly fine, just in case of more esoteric issues, I'd first try with venv.", "Are there any updates on this issue? I have the same problem when I try to build TensorFlow for windows with Cuda. Without Cuda everything seems fine and I can build it successfully. I also use `cmd.exe` to build with msvc 2019.", "I have the similar issue https://github.com/tensorflow/tensorflow/issues/46118\r\n\r\n@SchJens Have you found solution for this issue ?", "I had this same problem, but then I noticed in the Windows instructions ([here](https://www.tensorflow.org/install/source_windows#disable_msys_path_conversion)), it says to \r\nset `MSYS_NO_PATHCONV` and `MSYS2_ARG_CONV_EXCL`, which fixed the problem for me.\r\n\r\nThe Stackoverflow link above seems indicative of the msys path conversion falling down.", "Seems like I have figured out the issue with Bad address ... most of the time it happens because some third party is not available in google storage or through google mirror ...\r\n\r\nWhen I switch to github, package downloaded properly and build proceed\r\n\r\nProbably that is why all works on Google's side, because they have access to their mirrors ... (", "@SchJens,\r\n\r\nCan you take a look at above comment by @redradist and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39680\">No</a>\n"]}]