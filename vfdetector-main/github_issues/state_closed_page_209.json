[{"number": 48391, "title": "Toy model crashes on multiple GPUs with \"No unary variant device copy function\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): 0.29.1-1.8\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Various, including TITAN Xp 12 GB\r\n\r\n**Describe the current behavior**\r\nA toy model consisting of several Conv2D and SyncBatchNorm layers, when training on 4 GPUs, fails with the following error:\r\n```\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2021-04-08 03:24:12.084199: W external/org_tensorflow/tensorflow/core/grappler/optimizers/meta_optimizer.cc:560] dependency_optimizer failed: Deadline exceeded: dependency_optimizer exceeded deadline., time = 11444.1797ms.\r\n2021-04-08 03:24:18.334984: W external/org_tensorflow/tensorflow/core/common_runtime/process_function_library_runtime.cc:733] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.\r\nTraceback (most recent call last):\r\n  File \"/.../train_keras_model.py\", line 109, in <module>\r\n    main()\r\n  File \"/.../train_keras_model.py\", line 103, in main\r\n    verbose=2,\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py\", line 708, in _call\r\n    return function_lib.defun(fn_with_cond)(*canon_args, **canon_kwds)\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter\r\n\t [[{{node cond/inner_args_0_7/_2117/_10568}}]]\r\n\t [[cond/else/_1/StatefulPartitionedCall/Adam/Adam/update_69/update_2/ResourceApplyAdam/_51423]]\r\n  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter\r\n\t [[{{node cond/inner_args_0_7/_2117/_10568}}]]\r\n0 successful operations.\r\n3 derived errors ignored. [Op:__inference_fn_with_cond_527347]\r\n\r\nFunction call stack:\r\nfn_with_cond -> fn_with_cond\r\n```\r\n\r\n**Describe the expected behavior**\r\nModel should train without crashing.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis standalone code reproduces the problem for us.\r\n```\r\nimport logging\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.metrics import AUC\r\nfrom tensorflow.keras.layers import Dense, Conv2D\r\nfrom tensorflow.keras.layers.experimental import SyncBatchNormalization\r\n\r\ndef train_dataset_input():\r\n    batch_size = 4\r\n    x_value = tuple(tf.random.uniform((1, batch_size, 10, 10, 3)) for _ in range(5))\r\n    y_value = tf.random.uniform((1, batch_size, 5))\r\n    dataset = tf.data.Dataset.from_tensor_slices((x_value, y_value))\r\n    dataset = dataset.repeat()\r\n    return dataset\r\n\r\nclass ConvBN(tf.keras.layers.Layer):\r\n    def __init__(self,):\r\n        super(ConvBN, self).__init__()\r\n        self.conv = Conv2D(filters=32, kernel_size=1)\r\n        self.bn1 = SyncBatchNormalization()\r\n        self.bn2 = SyncBatchNormalization()\r\n        self.bn3 = SyncBatchNormalization()\r\n\r\n    def call(self, inputs, training=None, **kwargs):\r\n        x = self.conv(inputs)\r\n        x = self.bn1(x)\r\n        x = self.bn2(x)\r\n        x = self.bn3(x)\r\n        return x\r\n\r\nclass MultiFrameModel(keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(MultiFrameModel, self).__init__(name=\"multi_frame_model\", **kwargs)\r\n        self.backbone = keras.Sequential([ConvBN() for _ in range(17)])\r\n        self.dense = Dense(5)\r\n\r\n    def call(self, samples, training=False):\r\n        backbone_outputs = []\r\n        assert len(samples) == 5\r\n        for img in samples:\r\n            backbone_outputs.append(self.backbone(img))\r\n\r\n        concat_sample = tf.concat(values=backbone_outputs, axis=3)\r\n        x = tf.reduce_sum(concat_sample, [2, 3])\r\n        x = self.dense(x)\r\n        return x\r\n\r\nclass AucFromLogits(AUC):\r\n    def update_state(self, y_true, logits, sample_weight=None):\r\n        y_pred = tf.math.sigmoid(logits)\r\n        super(AucFromLogits, self).update_state(y_true, y_pred, sample_weight)\r\n\r\ndef main():\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    assert strategy.num_replicas_in_sync == 4\r\n\r\n    train_dataset = train_dataset_input()\r\n    with strategy.scope():\r\n        metrics = [\r\n            AucFromLogits(\r\n                name=\"auc_from_logits\",\r\n                num_thresholds=100,\r\n                curve=\"PR\",\r\n                multi_label=True,\r\n                label_weights=[1.0, 0.0, 0.0, 0.0, 0.0],\r\n            )\r\n        ]\r\n\r\n        model = MultiFrameModel()\r\n        model.compile(\r\n            optimizer=keras.optimizers.Adam(),\r\n            loss=keras.losses.MeanSquaredError(),\r\n            metrics=metrics,\r\n        )\r\n\r\n    model.fit(\r\n        x=train_dataset,\r\n        validation_data=train_dataset,\r\n        steps_per_epoch=100,  # number of training steps between eval epochs\r\n        epochs=120,  # epochs = total number of training steps / steps_per_epoch\r\n        validation_steps=100,\r\n        validation_freq=1,\r\n        verbose=2,\r\n    )\r\n\r\nif __name__ == \"__main__\":\r\n    logging.getLogger().setLevel(logging.INFO)\r\n    main()\r\n```\r\n\r\nPlease note that this code is sufficient to reproduce the crash in our environment. However, because the crash only occurs when training with multiple GPUs, and Colab only allows training on one GPU, this will run but not reproduce the crash on Colab. I tried splitting the GPU into 4 virtual GPUs, but it did not reproduce the problem.\r\n\r\n**Other info / logs**\r\nThe following features of the model appear to be important; removing any of them \"fixes\" the problem.\r\n- The model consists of SyncBatchNorm and Conv2D layers, and is fairly deep (it consists of a block repeated 17 times; repeating it, say, 10 times does not trigger the issue).\r\n- The input is a tuple of 5 tensors, and the model backbone is applied to each of the tensors.\r\n- There is a slightly modified AUC metric.\r\n- The model runs on 4 GPUs with the Keras MirroredStrategy.\r\n\r\nI'm quite confused as to how these four features should interact to cause a crash, especially the metric; it's quite surprising that adding a metric should somehow cause training to fail.", "comments": ["@MinasTyuru,\r\nI was able to run the code without any errors on a machine with TensorFlow v2.4.1 and 2 GPUs. Please check the below screenshot for reference.\r\n\r\n![image](https://user-images.githubusercontent.com/57165142/114044461-e3de7e80-98a4-11eb-80f7-287353717fd5.png)\r\n\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue? Thanks!", "Also, try limiting the GPU memory growth using any of the methods listed [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it helps. Thanks!", "@amahendrakar Thanks, Abhilash! To clarify, the failure only occurs on 4 GPUs, not 2; when I try 2 GPUs locally it also trains without any problems. Are you able to access a machine with 4 physical GPUs for testing?\r\n\r\nThe way our production build system integrates TensorFlow is a little delicate, and as a result, it is quite nontrivial for us to upgrade TensorFlow versions. When I run in a virtualenv with a fresh pip install of TF2.4.1, that trains correctly, whereas if I try a fresh pip install of TF2.2.0rc4, that fails. Would you be able to see if you can reproduce the problem on TF2.2.0rc4 with 4 GPUs? And given that it seems to happen only on TF2.2.0rc4 (at least for this case; it's possible it's a symptom of some problem that could occur for some other model on TF2.4.1), would we be able to get support on resolving the problem there, or would we have to just upgrade to TF2.4.1?\r\n\r\nI also tried both of the memory growth limits in that link and it didn't seem to make a difference.\r\n\r\nThanks again!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Would you be able to see if you can reproduce the problem on TF2.2.0rc4 with 4 GPUs?\r\n\r\n@MinasTyuru,\r\nOn running the code with TensorFlow v2.2.0rc4 and 4 GPUs, I did not face any errors. Please check the below screenshot for reference. \r\n![Screenshot 2021-04-19 7 37 44 PM](https://user-images.githubusercontent.com/57165142/115249969-c16f1f80-a146-11eb-809e-dbf6c6530315.png)\r\n\r\n> When I run in a virtualenv with a fresh pip install of TF2.4.1, that trains correctly, whereas if I try a fresh pip install of TF2.2.0rc4, that fails.\r\n\r\nIn this case, I'd suggest you to update TensorFlow to v2.4.1 or v2.5.0rc1. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48391\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48391\">No</a>\n"]}, {"number": 48390, "title": "Update sequence.py", "body": "Documentation for [tf.keras.preprocessing.sequence.TimeseriesGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/TimeseriesGenerator) has a broken link for tf.keras.utils.Sequence\r\n\r\nUpdated it to redirect to appropriate page.", "comments": []}, {"number": 48388, "title": "tensorRT does not support defining OP.", "body": "1. Use TensorFlow package: TFRA(https://github.com/tensorflow/recommenders-addons)\r\n2. After saving the model, use tensorRT for model conversion:\r\n\r\nTFRA code:\r\n```\r\ndeep_dynamic_variables = dynamic_embedding.get_variable(\r\n        name=\"deep_dynamic_embeddings\",\r\n        initializer=initializer,\r\n        dim=embedding_size,\r\n        devices=ps_list,\r\n        trainable=is_training,\r\n        partitioner=addone_partition_fn,\r\n        checkpoint=True\r\n    )\r\n```\r\n\r\ntensorRT conversion code:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_recommenders_addons as tfra\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\npath = 'models/de'\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=path)\r\nconverter.convert()\r\n\r\nconverted_model_path = 'models/c'\r\nconverter.save(converted_model_path)\r\n```\r\n3. Error message:\r\nKeyError: \"The name 'deep_dynamic_embeddings' refers to an Operation not in the graph.\"", "comments": ["@Mr-Nineteen ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the the exact sequence of commands / steps that you executed before running into the problem\r\n\r\n\r\nThanks!", "@tilakrayal \r\n\r\nThe verification is the fusion of TFRA and TRT, without using the GPU environment.\r\n\r\nconda env\uff1a\r\n\r\n1. python version : 3\r\n2. TF related installation library:\r\n```\r\ntensorflow                     2.4.1\r\ntensorflow-datasets            4.2.0\r\ntensorflow-estimator           2.4.0\r\ntensorflow-metadata            0.29.0\r\ntensorflow-recommenders-addons 0.1.0\r\n```\r\n3. Test model address: https://github.com/Mr-Nineteen/models/blob/main/1617949817.zip\r\n4. test code\uff1a\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_recommenders_addons as tfra\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\npath = '1617949817'\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=path)\r\nconverter.convert()\r\n\r\nconverted_model_path = 'c'\r\nconverter.save(converted_model_path)\r\n```\r\n5. Error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/dxwang/repo/pycharm-repo/tensorflow-2.0-demo/com/opensource/demo/tftrt/trtconvert-yz.py\", line 8, in <module>\r\n    converter.convert()\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1094, in convert\r\n    self._input_saved_model_tags)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 859, in load\r\n    return load_internal(export_dir, tags, options)[\"root\"]\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 909, in load_internal\r\n    root = load_v1_in_v2.load(export_dir, tags)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py\", line 279, in load\r\n    return loader.load(tags=tags)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py\", line 225, in load\r\n    signature=[])\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py\", line 628, in wrap_function\r\n    collections={}),\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py\", line 87, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py\", line 93, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py\", line 93, in load_graph\r\n    meta_graph_def)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1485, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 886, in import_scoped_meta_graph_with_return_elements\r\n    ops.prepend_name_scope(value, scope_to_prepend_to_names))\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3726, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/Users/dxwang/opt/anaconda3/envs/tfra/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3786, in _as_graph_element_locked\r\n    \"graph.\" % repr(name))\r\nKeyError: \"The name 'deep_dynamic_embeddings' refers to an Operation not in the graph.\"\r\n```\r\n\r\nThanks!\r\n\r\n\r\n", "@ymodak,\r\nI was able to reproduce the issue with TF v2.4.1, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1c82bdfec34ba52b3cc064609424fcd1/48388.ipynb#scrollTo=32-gTBbyV99S).\r\n\r\n Whereas with TF v2.5.0rc1 and TF-nightly, the dependency `tensorflow-recommenders-addons` is not compatible. Thanks! \r\n", "Did you try converting your model using [tf.experimental.tensorrt.Converter()](https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter)?", "@ymodak  it's ok, thx.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48388\">No</a>\n"]}, {"number": 48387, "title": "r2.5-rc1 cherry-pick request: Upgrade curl to 7.76.0", "body": "Original PR: #48351 (Merged into master today)\r\n> List of CVE's that are fixed in this release. \r\n| Vulnerability | Date | CVSS v2 | CVSS v3 | Type\r\n  | CVE-2020-8286 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n  | CVE-2020-8231 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n  | CVE-2020-8285 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n  | CVE-2020-8169 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n  | CVE-2020-8177 | 2020/12/15 | 4.6 | 7.1 | Exact match\r\n  | CVE-2020-8284 | 2020/12/15 | 4.3 | 3.7 | Exact match", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48387) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48387) for more info**.\n\n<!-- need_author_consent -->", "@rsketine Could you please help reply to googlebot as well? Thank you!", "@googlebot I consent", "@penpornk  done."]}, {"number": 48386, "title": "[Graph C API] Updating warning messages", "body": "This PR is a patch for [Modular TensorFlow Graph C API](https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md).\r\n\r\nIt continues https://github.com/tensorflow/tensorflow/pull/47969 which was reverted.", "comments": ["Hi @penpornk @ezhulenev, please help to have a review. Thanks!", "@Nirzak thanks for taking time to approve , please avoid reviewing PRs which are not assigned you as a reviewer , thank you so much.\r\ncc @mihaimaruseac ", "@penpornk Any update on this PR? Please. Thanks!"]}, {"number": 48385, "title": "[Graph C API] Remove GrapplerItemMap", "body": "This PR is a patch for [Modular TensorFlow Graph C API](https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md). \r\n\r\nIt removes global GrapplerItemMap. Optimizer API has changed from `void (*optimize_func)(void*, TF_Buffer*, TF_Buffer*, TF_Status*);` to `void (*optimize_func)(void*, const TF_Buffer*, const TF_GrapplerItem*, TF_Buffer*, TF_Status*);`", "comments": ["Hi @penpornk @ezhulenev, please help to have a review. Thanks!", "OK. Now this PR is only targeting removing global GrapplerItemMap.", "@kulinseth FYI. We removed GrapplerItem map and just passed the GrapplerItem directly to the optimize function.", "> @kulinseth FYI. We removed GrapplerItem map and just passed the GrapplerItem directly to the optimize function.\r\n\r\nThanks! for letting us know @penpornk ", "Hi @ShengYang1 , thanks for the PR.  Looks like this wasn't part of the v2.5 release. \r\nI am curious why this didn't make it to 2.5 release? Thanks.\r\n\r\nKulin", "That was my fault. I got feedback to remove the map about a week before the branch cut, but I was worried it could complicate our already tight PluggableDevice-related PR merge/test schedule. So I waited until after 2.5 branch cut. And since cherrypick was technically for bug fixes, I didn't cherrypick it in. Sorry for making diverge code paths! :'(", "> That was my fault. I got feedback to remove the map about a week before the branch cut, but I was worried it could complicate our already tight PluggableDevice-related PR merge/test schedule. So I waited until after 2.5 branch cut. And since cherrypick was technically for bug fixes, I didn't cherrypick it in. Sorry for making diverge code paths! :'(\r\n\r\nNo issues, thanks for the clarification @penpornk . The only reason I brought up was that since there is an API change, it won't be a breaking change for v2.5 but for TF master we would at somepoint adopt this new change."]}, {"number": 48384, "title": "release 2.5-rc1 cherry-pick request: Fix MirroredStrategy eager performance regression", "body": "We have noticed ~15% performance regression in eager benchmarks with MirroredStrategy. This PR fixes the regression.", "comments": []}, {"number": 48383, "title": "[XLA] Fix and generalize to row_vectorization few waves", "body": "@timshen91 \r\n\r\nThis re-enable what was meant to be enabled by https://github.com/tensorflow/tensorflow/pull/45343\r\nSo this speed up the GELU kernel from 79us to 73us on V100.\r\n\r\nI added extra tests to make sure that it continue to work.\r\n\r\nIt also extend PR https://github.com/tensorflow/tensorflow/pull/48527 to enable few_waves when row vectorization is enabled. This speed up even more that cases. Including the bellow kernel on V100 from 589us to 572us and on A100 from 670us to 547us.\r\n\r\n```\r\n%fused_computation.5_new (param_0.89: f32[672], param_1.118: f32[672], param_2.79: f32[672], param_3.62: f32[672], param_4.56: f16[512,14,14,672], param_5.54: f32[672], param_6.48: f16[512,14,14,672], param_7.61: f32[672]) -> f16[512,14,14,672] {\r\n  %param_2.79 = f32[672]{0} parameter(2)\r\n  %constant_157 = f32[] constant(1), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %broadcast.186 = f32[672]{0} broadcast(f32[] %constant_157), dimensions={}, metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %param_5.54 = f32[672]{0} parameter(5)\r\n  %constant_56 = f32[] constant(9.96492327e-06)\r\n  %broadcast.185 = f32[672]{0} broadcast(f32[] %constant_56), dimensions={}\r\n  %multiply.155 = f32[672]{0} multiply(f32[672]{0} %param_5.54, f32[672]{0} %broadcast.185), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %param_3.62 = f32[672]{0} parameter(3)\r\n  %multiply.154 = f32[672]{0} multiply(f32[672]{0} %param_3.62, f32[672]{0} %broadcast.185), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %multiply.153 = f32[672]{0} multiply(f32[672]{0} %multiply.154, f32[672]{0} %multiply.154), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %subtract.55 = f32[672]{0} subtract(f32[672]{0} %multiply.155, f32[672]{0} %multiply.153), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %constant_155 = f32[] constant(0.001), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %broadcast.184 = f32[672]{0} broadcast(f32[] %constant_155), dimensions={}\r\n  %add.54 = f32[672]{0} add(f32[672]{0} %subtract.55, f32[672]{0} %broadcast.184), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %rsqrt.23 = f32[672]{0} rsqrt(f32[672]{0} %add.54), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %multiply.152 = f32[672]{0} multiply(f32[672]{0} %rsqrt.23, f32[672]{0} %rsqrt.23), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %divide.14 = f32[672]{0} divide(f32[672]{0} %broadcast.186, f32[672]{0} %multiply.152), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %rsqrt.7 = f32[672]{0} rsqrt(f32[672]{0} %divide.14), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %multiply.29 = f32[672]{0} multiply(f32[672]{0} %param_2.79, f32[672]{0} %rsqrt.7), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %multiply.28 = f32[672]{0} multiply(f32[672]{0} %multiply.29, f32[672]{0} %broadcast.185), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %broadcast.47 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %multiply.28), dimensions={3}\r\n  %param_6.48 = f16[512,14,14,672]{3,2,1,0} parameter(6)\r\n  %constant_194 = f16[] constant(1), metadata={op_type=\"AddV2\" op_name=\"add\"}\r\n  %broadcast.256 = f16[512,14,14,672]{3,2,1,0} broadcast(f16[] %constant_194), dimensions={}\r\n  %param_4.56 = f16[512,14,14,672]{3,2,1,0} parameter(4)\r\n  %convert.66 = f32[512,14,14,672]{3,2,1,0} convert(f16[512,14,14,672]{3,2,1,0} %param_4.56), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %broadcast.254 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %multiply.154), dimensions={3}, metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %subtract.82 = f32[512,14,14,672]{3,2,1,0} subtract(f32[512,14,14,672]{3,2,1,0} %convert.66, f32[512,14,14,672]{3,2,1,0} %broadcast.254), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %broadcast.251 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %rsqrt.23), dimensions={3}\r\n  %multiply.219 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %subtract.82, f32[512,14,14,672]{3,2,1,0} %broadcast.251), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchN\r\normV3\"}\r\n  %broadcast.250 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_2.79), dimensions={3}, metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %multiply.218 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %multiply.219, f32[512,14,14,672]{3,2,1,0} %broadcast.250), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatch\r\nNormV3\"}\r\n  %param_7.61 = f32[672]{0} parameter(7)\r\n  %broadcast.249 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_7.61), dimensions={3}, metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %add.79 = f32[512,14,14,672]{3,2,1,0} add(f32[512,14,14,672]{3,2,1,0} %multiply.218, f32[512,14,14,672]{3,2,1,0} %broadcast.249), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %convert.65 = f16[512,14,14,672]{3,2,1,0} convert(f32[512,14,14,672]{3,2,1,0} %add.79), metadata={op_type=\"FusedBatchNormV3\" op_name=\"foo/batch_normalization/FusedBatchNormV3\"}\r\n  %negate.12 = f16[512,14,14,672]{3,2,1,0} negate(f16[512,14,14,672]{3,2,1,0} %convert.65)\r\n  %exponential.10 = f16[512,14,14,672]{3,2,1,0} exponential(f16[512,14,14,672]{3,2,1,0} %negate.12)\r\n  %add.78 = f16[512,14,14,672]{3,2,1,0} add(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %exponential.10)\r\n  %divide.20 = f16[512,14,14,672]{3,2,1,0} divide(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %add.78), metadata={op_type=\"Sigmoid\" op_name=\"foo/activation/Sigmoid\"}\r\n  %subtract.77 = f16[512,14,14,672]{3,2,1,0} subtract(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %divide.20), metadata={op_type=\"Sub\" op_name=\"sub\"}\r\n  %multiply.211 = f16[512,14,14,672]{3,2,1,0} multiply(f16[512,14,14,672]{3,2,1,0} %convert.65, f16[512,14,14,672]{3,2,1,0} %subtract.77), metadata={op_type=\"Mul\" op_name=\"mul\"}\r\n  %add.75 = f16[512,14,14,672]{3,2,1,0} add(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %multiply.211), metadata={op_type=\"AddV2\" op_name=\"add\"}\r\n  %multiply.210 = f16[512,14,14,672]{3,2,1,0} multiply(f16[512,14,14,672]{3,2,1,0} %divide.20, f16[512,14,14,672]{3,2,1,0} %add.75), metadata={op_type=\"Mul\" op_name=\"mul_1\"}\r\n  %multiply.209 = f16[512,14,14,672]{3,2,1,0} multiply(f16[512,14,14,672]{3,2,1,0} %param_6.48, f16[512,14,14,672]{3,2,1,0} %multiply.210), metadata={op_type=\"Mul\" op_name=\"mul_2\"}\r\n  %convert.8 = f32[512,14,14,672]{3,2,1,0} convert(f16[512,14,14,672]{3,2,1,0} %multiply.209), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %constant_48 = f32[] constant(100352), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %broadcast.46 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[] %constant_48), dimensions={}, metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %multiply.27 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %convert.8, f32[512,14,14,672]{3,2,1,0} %broadcast.46), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalizati\r\non/FusedBatchNormGradV3\"}\r\n  %param_1.118 = f32[672]{0} parameter(1)\r\n  %broadcast.45 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_1.118), dimensions={3}, metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %subtract.10 = f32[512,14,14,672]{3,2,1,0} subtract(f32[512,14,14,672]{3,2,1,0} %multiply.27, f32[512,14,14,672]{3,2,1,0} %broadcast.45), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normaliza\r\ntion/FusedBatchNormGradV3\"}\r\n  %param_0.89 = f32[672]{0} parameter(0)\r\n  %broadcast.44 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_0.89), dimensions={3}, metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %multiply.26 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %broadcast.44, f32[512,14,14,672]{3,2,1,0} %subtract.82), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n  %broadcast.42 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %divide.14), dimensions={3}\r\n  %divide.6 = f32[512,14,14,672]{3,2,1,0} divide(f32[512,14,14,672]{3,2,1,0} %multiply.26, f32[512,14,14,672]{3,2,1,0} %broadcast.42), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/\r\nFusedBatchNormGradV3\"}\r\n  %subtract.9 = f32[512,14,14,672]{3,2,1,0} subtract(f32[512,14,14,672]{3,2,1,0} %subtract.10, f32[512,14,14,672]{3,2,1,0} %divide.6), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/\r\nFusedBatchNormGradV3\"}\r\n  %multiply.25 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %broadcast.47, f32[512,14,14,672]{3,2,1,0} %subtract.9), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalizat\r\nion/FusedBatchNormGradV3\"}\r\n  ROOT %convert.7 = f16[512,14,14,672]{3,2,1,0} convert(f32[512,14,14,672]{3,2,1,0} %multiply.25), metadata={op_type=\"FusedBatchNormGradV3\" op_name=\"gradient_tape/foo/batch_normalization/FusedBatchNormGradV3\"}\r\n}\r\n```", "comments": ["One test is failing. But I have an updated version of this PR that also enable few_waves in combination of that recently merged PR:\r\nhttps://github.com/tensorflow/tensorflow/pull/48381\r\n\r\nI think it will be better to merge the both changes at the same time. I'll update this PR in a few days when I have checked that we do not have speed regression with it.", "@nouiz  Any update on this PR? Please. Thanks!", "I have the code working locally. I started today some new speed test to make sure it doesn't give regression in other cases. When it is all fine here, I'll update it.", "This PR is ready for review @timshen91 @cheshire.\r\nI updated the description to explain what it does.", "I did all the comments."]}, {"number": 48382, "title": "Update the use of setAlpha/setBeta for Cudnn frontend APIs", "body": "The Cudnn frontend APIs overload the setAlpha/setBeta to allow either float or double parameters. The backend stores two copies of the value and will pick the value of correct data type when the build() is called.\r\n\r\nThis PR follows this behavior and simplifies the usage.\r\n\r\ncc. @nluehr ", "comments": []}, {"number": 48381, "title": "[XLA] Enable vectorization of row broadcasting", "body": "@timshen91 @cheshire \r\n\r\nThis speed up the \"HloModule EfficientNetSwish\" test included in this PR on A100 from 740us to 667us\r\n\r\nMostly, if a kernel contains row broadcasting operation(s) and only point-wise operation and scalar broadcasting, then we align the block size to the row size and simplify the indexing. This way, LLVM is able to vectorize the loads of the row broadcasting.", "comments": []}, {"number": 48380, "title": "Docker build: no such package '@local_cuda//': The repository '@local_cuda' could not be resolved and referenced by '@cub_archive//:cub'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source, branch r2.5\r\n- TensorFlow version: r2.5\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- CUDA/cuDNN version: 10.2/8.1.1\r\n- GPU model and memory: n/a, but I want to use on P100 and V100 nodes\r\n\r\n**Describe the problem**\r\nI'm trying to compile tensorflow from source in a docker container, but I get the following error:\r\n```\r\nno such package '@local_cuda//': The repository '@local_cuda' could not be resolved and referenced by '@cub_archive//:cub'\r\n```\r\nSee attached dockerfile\r\n[dockerfile.txt](https://github.com/tensorflow/tensorflow/files/6274261/dockerfile.txt)\r\n\r\nIf line 101 in the above dockerfile is changed from `yes \"\" | ./configure && \\` to ` ./configure && \\`, the same issue persists.\r\n\r\n**Any other info / logs**\r\nSee attached traceback\r\n[traceback.txt](https://github.com/tensorflow/tensorflow/files/6274260/traceback.txt)\r\n\r\n\r\nThank you so much for your help with this issue!", "comments": ["@jennyfothergill  please check https://github.com/tensorflow/tensorflow/pull/48310", "@freedomtan and @jennyfothergill  - Please check new PR for the same issue https://github.com/tensorflow/tensorflow/pull/48393. I closed the earlier one due to merge issues.", "This issue is still not resolved by #48393 ... ", "@jennyfothergill \r\nAs the Pr are merged, please confirm if this is still an issue.", "Hi, sorry for the delay I am not as familiar with this issue anymore. @RainierBarrett has successfully compiled tensorflow in [this container](https://github.com/cmelab/containers/blob/main/containers/dockerfile_gputensorflow). He did have to [manually link certain cuda libs](https://github.com/cmelab/containers/blob/4fa231eb8f6543ea46723108939b5fecb8682ef3/containers/dockerfile_gputensorflow#L82-L88), but otherwise--yes I think this issue is resolved. \r\n\r\nThank you for your help! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48380\">No</a>\n"]}, {"number": 48379, "title": "[CherryPick:r2.5] Update runtime version to 2.5.0", "body": null, "comments": []}, {"number": 48378, "title": "Added Reference Link to numpy.dtype", "body": "Added Reference link to numpy.dtype doc. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48378) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 48377, "title": "INFO log get printed in tf2.5-rc0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 20H2 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0rc0\r\n- Python version: 3.9.2\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): msvc 19.28.29913\r\n- CUDA/cuDNN version: 11.2/8.1.1\r\n- GPU model and memory: GTX1650 4GB\r\n\r\n**Describe the current behavior**\r\n\r\nsimply doing ``import tensorflow as tf`` will print out all log at INFO level to Jupyter notebook\r\n\r\n![image](https://user-images.githubusercontent.com/28623434/113896873-56156d00-9798-11eb-834a-d5a232222cb7.png)\r\n\r\n**Describe the expected behavior**\r\n\r\nLogs at INFO level will not get print to Jupyter notebook\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1-rOtR-xYJYymNIv4aksle_ZI9W90WLBh?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@henrysky \r\nI didn't face any info errors while running the code.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/b2faec222ca8d58ca3d2acacf8a708f2/-48377.ipynb#scrollTo=1kQicu7NMWf9) here.Thanks", "I just ran your colab notebook those INFO get printed\r\n\r\n![image](https://user-images.githubusercontent.com/28623434/113908499-9d095f80-97a4-11eb-8b02-ce51a82980bf.png)\r\n", "@ymodak,\r\nI was able to reproduce the issue with TF v2.5.0rc0, `INFO` logs are printed on importing TensorFlow for the first time.\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/UsharaniPagadala/9cf5e09b254a630b09625f6697b8902a/-48377.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Can you bisect with nightly versions to see at what nightly this got introduced?", "The behavior is introduced in this commit https://github.com/tensorflow/tensorflow/commit/373883e4cc074c5ad0efa8c3d3875ae102c4005a", "This is fixed with latest release of  TF 2.5-rc3 and should be in TF 2.5 final version as well.\r\nPlease check this [gist](https://colab.research.google.com/gist/ymodak/15e0203253582cc830f110ac8206fea7/-48377.ipynb) for reference.", "indeed fixed in rc3, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48377\">No</a>\n"]}, {"number": 48376, "title": "tensorflow.experimental.numpy operations return tf.Tensor instead of ndarray<tf.Tensor>", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian 10.9\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0rc0\r\n- Python version: Python 3.7.5\r\n\r\n**Describe the current behavior**\r\nIn tensorflow 2.5.0rc0 these operations\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.experimental.numpy as tnp\r\n\r\nprint(tnp.real(tf.constant(1)))\r\nprint(tnp.real(tf.constant(1+1j)))\r\nprint(tnp.imag(tf.constant(1+1j)))\r\nprint(tnp.array(tf.constant(1)))\r\n```\r\nreturn the following results\r\n```python\r\ntf.Tensor(1, shape=(), dtype=int32)\r\ntf.Tensor(1.0, shape=(), dtype=float64)\r\ntf.Tensor(1.0, shape=(), dtype=float64)\r\ntf.Tensor(1, shape=(), dtype=int32)\r\n```\r\n\r\n**Describe the expected behavior**\r\nIn tensorflow 2.4.1 they used to return ndarrays\r\n```python\r\nndarray<tf.Tensor(1, shape=(), dtype=int32)>\r\nndarray<tf.Tensor(1.0, shape=(), dtype=float64)>\r\nndarray<tf.Tensor(1.0, shape=(), dtype=float64)>\r\nndarray<tf.Tensor(1, shape=(), dtype=int32)>\r\n```\r\n\r\n**Other info / logs**\r\nI didn't test any other methods.\r\n\r\n**Edit**\r\nI don't know if this actually is a bug or if it was one in tensorflow 2.4.1. But I couldn't find out which one is the intended behaviour. I for my part would expect these functions to return an ndarray but the introduction [article](https://www.tensorflow.org/guide/tf_numpy#tftensor_and_nd_array) seems to have a different opinion.", "comments": ["@ymodak \r\n\r\nI was able to reproduce the issue. Please find the [gist](https://colab.research.google.com/gist/tilakrayal/44d02613d4cc21e5161da3f495ad8050/48376-2-4.ipynb) of it here. Thanks!", "`ndarray` wrapper is removed intentionally with TF 2.5.0-rc1\r\nSee commit [0b9ff2e](https://github.com/tensorflow/tensorflow/commit/0b9ff2eb1a097602206c6b29823543768bfb34fe#diff-ff9428d2d7e920852bc429bd4ffd46ae182d27730a55d16d31d34a5d21d38ac6)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48376\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48376\">No</a>\n"]}, {"number": 48375, "title": "tf.switch_case not working with KerasTensor", "body": "**System information**\r\n- Documentation exemple : https://www.tensorflow.org/api_docs/python/tf/switch_case\r\n- Windows 10\r\n- TensorFlow version 2\r\n- Python version: 3.7\r\n\r\n**Current behavior**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\ndef f1(): return tf.constant(17)\r\ndef f2(): return tf.constant(31)\r\ndef f3(): return tf.constant(-1)\r\nt_input = Input(shape=(1,), name=\"t_input\")\r\nr = tf.switch_case(t_input, branch_fns={0: f1, 1: f2}, default=f3)\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-9-bd285228541c>\", line 5, in <module>\r\n    r = tf.switch_case(t_input, branch_fns={0: f1, 1: f2}, default=f3)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3616, in switch_case\r\n    return _indexed_case_helper(branch_fns, default, branch_index, name)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3315, in _indexed_case_helper\r\n    branch_fns, default, branch_index)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3249, in _indexed_case_verify_and_canonicalize_args\r\n    type(branch_index)))\r\nTypeError: branch_index must a Tensor, got <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>\r\n```\r\n", "comments": ["@Vaunorage,\r\nAs per the [documentation](https://www.tensorflow.org/api_docs/python/tf/switch_case), **branch_index** should be an **int Tensor**.\r\n\r\nWith an int Tensor passed as the argument, I was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2d3343ae70db798f9d85b7c3a35aaa80/48375.ipynb). Thanks!", "Thank you,\r\n\r\nHowever, I need to use it with an input which is a KerasTensor ? How this would work ? \r\nOtherwise, I can use tf.case but it is less practical ..\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi, for those who wonder how to train multiple branches of a neural network with Keras. just make a function yourself like so :\r\n\r\nhttps://stackoverflow.com/questions/67065868/training-different-branches-of-model-network-with-tf-switch-case/67067644#67067644", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48375\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48375\">No</a>\n"]}, {"number": 48374, "title": "MultiWorkerMirroredStrategy looks a lot slower than non-distributed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI'm using code from here: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras (MNIST example) with small alterations, see below for the code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nAmazon linux - 4.14.225-168.357.amzn2.x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNot applicable\r\n- TensorFlow installed from (source or binary):\r\nVersion 2.4.1 installed using `pip3 install --user tensorflow==2.4.1`\r\n- TensorFlow version (use command below):\r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n2021-04-07 15:52:47.046112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nv2.4.0-49-g85c8b2a817f 2.4.1\r\n```\r\n- Python version:\r\n3.7.9\r\n- Bazel version (if compiling from source):\r\nNot applicable\r\n- GCC/Compiler version (if compiling from source):\r\nNot applicable\r\n- CUDA/cuDNN version:\r\nLooks like CUDA 11\r\n- GPU model and memory:\r\nNot applicable\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nOn a single m5.large node in AWS training with the MNIST model takes 1m 48s. With 3 machines of that type and MultiWorkerMirroredStrategy in place, it takes 4 mins 30 seconds.\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior would be that the multi-worker execution is much faster than the single-node. The point of using a multi-worker distributed capability is to allow for a much faster, scalable processing (model training).\r\n\r\n**Standalone code to reproduce the issue**\r\n**Non-distributed**\r\n```\r\nimport json\r\nimport os\r\nimport sys\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nif \".\" not in sys.path:\r\n    sys.path.insert(0, \".\")\r\n\r\ndef mnist_dataset(batch_size):\r\n    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n    # The `x` arrays are in uint8 and have values in the range [0, 255].\r\n    # You need to convert them to float32 with values in the range [0, 1]\r\n    x_train = x_train / np.float32(255)\r\n    y_train = y_train.astype(np.int64)\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\r\n    return train_dataset\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential(\r\n        [\r\n            tf.keras.Input(shape=(28, 28)),\r\n            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n            tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(128, activation=\"relu\"),\r\n            tf.keras.layers.Dense(10),\r\n        ]\r\n    )\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=[\"accuracy\"],\r\n    )\r\n    return model\r\n\r\nstart_time = time.time()\r\nglobal_batch_size = 64\r\nmulti_worker_dataset = mnist_dataset(global_batch_size)\r\nmulti_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(multi_worker_dataset, epochs=50, steps_per_epoch=70)\r\nelapsed_time = time.time() - start_time\r\nstr_elapsed_time = time.strftime(\"%H : %M : %S\", time.gmtime(elapsed_time))\r\nprint(\">> Finished. Time elapsed: {}.\".format(str_elapsed_time))\r\n```\r\n**Distributed:**\r\n```\r\nimport json\r\nimport os\r\nimport sys\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nif \".\" not in sys.path:\r\n    sys.path.insert(0, \".\")\r\n\r\ndef mnist_dataset(batch_size):\r\n    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n    # The `x` arrays are in uint8 and have values in the range [0, 255].\r\n    # You need to convert them to float32 with values in the range [0, 1]\r\n    x_train = x_train / np.float32(255)\r\n    y_train = y_train.astype(np.int64)\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\r\n    return train_dataset\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential(\r\n        [\r\n            tf.keras.Input(shape=(28, 28)),\r\n            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n            tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(128, activation=\"relu\"),\r\n            tf.keras.layers.Dense(10),\r\n        ]\r\n    )\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=[\"accuracy\"],\r\n    )\r\n    return model\r\n\r\nstart_time = time.time()\r\n\r\nper_worker_batch_size = 64\r\ntf_config = json.loads(os.environ[\"TF_CONFIG\"])\r\nnum_workers = len(tf_config[\"cluster\"][\"worker\"])\r\n\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\r\nglobal_batch_size = 64\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\nmulti_worker_dataset = mnist_dataset(global_batch_size)\r\nmulti_worker_dataset_with_shrd = multi_worker_dataset.with_options(options)\r\n\r\nwith strategy.scope():\r\n    # Model building/compiling need to be within `strategy.scope()`.\r\n    multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(multi_worker_dataset_with_shrd, epochs=50, steps_per_epoch=70)\r\nelapsed_time = time.time() - start_time\r\nstr_elapsed_time = time.strftime(\"%H : %M : %S\", time.gmtime(elapsed_time))\r\nprint(\">> Finished. Time elapsed: {}.\".format(str_elapsed_time))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n**Non-distributed run log:**\r\n```\r\n2021-04-07 15:29:39.627372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-04-07 15:29:41.291031: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-07 15:29:41.292029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-04-07 15:29:41.371907: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-04-07 15:29:41.371985: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-249-213.awsinternal.audiomack.com): /proc/driver/nvidia/version does not exist\r\n2021-04-07 15:29:41.372562: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-07 15:29:41.372737: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nEpoch 1/50\r\n2021-04-07 15:29:42.005401: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-04-07 15:29:42.006824: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\r\n70/70 [==============================] - 3s 32ms/step - loss: 2.2895 - accuracy: 0.1377\r\nEpoch 2/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 2.2368 - accuracy: 0.3166\r\nEpoch 3/50\r\n70/70 [==============================] - 2s 29ms/step - loss: 2.1805 - accuracy: 0.4483\r\nEpoch 4/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 2.1005 - accuracy: 0.5943\r\nEpoch 5/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 2.0082 - accuracy: 0.6545\r\nEpoch 6/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 1.8875 - accuracy: 0.6937\r\nEpoch 7/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 1.7216 - accuracy: 0.7262\r\nEpoch 8/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 1.5662 - accuracy: 0.7549\r\nEpoch 9/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 1.3967 - accuracy: 0.7727\r\nEpoch 10/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 1.2423 - accuracy: 0.7823\r\nEpoch 11/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 1.0856 - accuracy: 0.8145\r\nEpoch 12/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.9729 - accuracy: 0.8060\r\nEpoch 13/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.8768 - accuracy: 0.8278\r\nEpoch 14/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.7688 - accuracy: 0.8481\r\nEpoch 15/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.7315 - accuracy: 0.8442\r\nEpoch 16/50\r\n70/70 [==============================] - 2s 29ms/step - loss: 0.6648 - accuracy: 0.8584\r\nEpoch 17/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.6287 - accuracy: 0.8635\r\nEpoch 18/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.6144 - accuracy: 0.8416\r\nEpoch 19/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.5623 - accuracy: 0.8624\r\nEpoch 20/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.5672 - accuracy: 0.8665\r\nEpoch 21/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.5544 - accuracy: 0.8607\r\nEpoch 22/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.5045 - accuracy: 0.8795\r\nEpoch 23/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4929 - accuracy: 0.8729\r\nEpoch 24/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4719 - accuracy: 0.8756\r\nEpoch 25/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4753 - accuracy: 0.8746\r\nEpoch 26/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4741 - accuracy: 0.8728\r\nEpoch 27/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4696 - accuracy: 0.8760\r\nEpoch 28/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4531 - accuracy: 0.8835\r\nEpoch 29/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4407 - accuracy: 0.8855\r\nEpoch 30/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4087 - accuracy: 0.8910\r\nEpoch 31/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.4413 - accuracy: 0.8808\r\nEpoch 32/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4110 - accuracy: 0.8839\r\nEpoch 33/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4152 - accuracy: 0.8959\r\nEpoch 34/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4159 - accuracy: 0.8870\r\nEpoch 35/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.4061 - accuracy: 0.8767\r\nEpoch 36/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.4077 - accuracy: 0.8890\r\nEpoch 37/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3841 - accuracy: 0.8951\r\nEpoch 38/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3976 - accuracy: 0.8895\r\nEpoch 39/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.3873 - accuracy: 0.8968\r\nEpoch 40/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3711 - accuracy: 0.8943\r\nEpoch 41/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3778 - accuracy: 0.8906\r\nEpoch 42/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3692 - accuracy: 0.9022\r\nEpoch 43/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3809 - accuracy: 0.8925\r\nEpoch 44/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3921 - accuracy: 0.8910\r\nEpoch 45/50\r\n70/70 [==============================] - 2s 30ms/step - loss: 0.3620 - accuracy: 0.9006\r\nEpoch 46/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3519 - accuracy: 0.9026\r\nEpoch 47/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3732 - accuracy: 0.9030\r\nEpoch 48/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3670 - accuracy: 0.8874\r\nEpoch 49/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3756 - accuracy: 0.8920\r\nEpoch 50/50\r\n70/70 [==============================] - 2s 31ms/step - loss: 0.3572 - accuracy: 0.8954\r\n>> Finished. Time elapsed: 00 : 01 : 48.\r\n```\r\n**Logs from the distributed run:**\r\n```\r\n2021-04-07 15:33:57.473623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-04-07 15:33:58.819768: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-07 15:33:58.820710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-04-07 15:33:58.893993: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-04-07 15:33:58.894042: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-249-213.awsinternal.audiomack.com): /proc/driver/nvidia/version does not exist\r\n2021-04-07 15:33:58.895043: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-07 15:33:58.895208: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-07 15:33:58.895622: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-07 15:33:58.899579: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.2.249.213:2121, 1 -> 10.2.252.56:2121, 2 -> 10.2.252.97:2121}\r\n2021-04-07 15:33:58.899839: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://10.2.249.213:2121\r\n2021-04-07 15:34:04.181014: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-04-07 15:34:04.200033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\r\nEpoch 1/50\r\n70/70 [==============================] - 9s 85ms/step - loss: 2.2927 - accuracy: 0.1324\r\nEpoch 2/50\r\n70/70 [==============================] - 5s 77ms/step - loss: 2.2476 - accuracy: 0.2718\r\nEpoch 3/50\r\n70/70 [==============================] - 5s 74ms/step - loss: 2.1945 - accuracy: 0.4580\r\nEpoch 4/50\r\n70/70 [==============================] - 5s 74ms/step - loss: 2.1318 - accuracy: 0.5667\r\nEpoch 5/50\r\n70/70 [==============================] - 5s 74ms/step - loss: 2.0503 - accuracy: 0.6541\r\nEpoch 6/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 1.9535 - accuracy: 0.6747\r\nEpoch 7/50\r\n70/70 [==============================] - 5s 77ms/step - loss: 1.8103 - accuracy: 0.7182\r\nEpoch 8/50\r\n70/70 [==============================] - 5s 77ms/step - loss: 1.6723 - accuracy: 0.7167\r\nEpoch 9/50\r\n70/70 [==============================] - 6s 79ms/step - loss: 1.5215 - accuracy: 0.7448\r\nEpoch 10/50\r\n70/70 [==============================] - 6s 79ms/step - loss: 1.3525 - accuracy: 0.7807\r\nEpoch 11/50\r\n70/70 [==============================] - 5s 78ms/step - loss: 1.2328 - accuracy: 0.7737\r\nEpoch 12/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 1.1053 - accuracy: 0.7949\r\nEpoch 13/50\r\n70/70 [==============================] - 5s 74ms/step - loss: 0.9806 - accuracy: 0.8146\r\nEpoch 14/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.8712 - accuracy: 0.8216\r\nEpoch 15/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.8108 - accuracy: 0.8377\r\nEpoch 16/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.7336 - accuracy: 0.8502\r\nEpoch 17/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.6911 - accuracy: 0.8486\r\nEpoch 18/50\r\n70/70 [==============================] - 5s 74ms/step - loss: 0.6502 - accuracy: 0.8593\r\nEpoch 19/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.6725 - accuracy: 0.8402\r\nEpoch 20/50\r\n70/70 [==============================] - 5s 74ms/step - loss: 0.5815 - accuracy: 0.8741\r\nEpoch 21/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.5819 - accuracy: 0.8524\r\nEpoch 22/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.5522 - accuracy: 0.8588\r\nEpoch 23/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.4860 - accuracy: 0.8780\r\nEpoch 24/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.5223 - accuracy: 0.8645\r\nEpoch 25/50\r\n70/70 [==============================] - 5s 74ms/step - loss: 0.5226 - accuracy: 0.8651\r\nEpoch 26/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4903 - accuracy: 0.8724\r\nEpoch 27/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.5156 - accuracy: 0.8642\r\nEpoch 28/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4501 - accuracy: 0.8855\r\nEpoch 29/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4403 - accuracy: 0.8916\r\nEpoch 30/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4491 - accuracy: 0.8890\r\nEpoch 31/50\r\n70/70 [==============================] - 5s 77ms/step - loss: 0.4170 - accuracy: 0.8920\r\nEpoch 32/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4621 - accuracy: 0.8793\r\nEpoch 33/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4379 - accuracy: 0.8799\r\nEpoch 34/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.4217 - accuracy: 0.8864\r\nEpoch 35/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4094 - accuracy: 0.8932\r\nEpoch 36/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.4042 - accuracy: 0.8941\r\nEpoch 37/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4123 - accuracy: 0.8895\r\nEpoch 38/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.3895 - accuracy: 0.9001\r\nEpoch 39/50\r\n70/70 [==============================] - 5s 77ms/step - loss: 0.4069 - accuracy: 0.8939\r\nEpoch 40/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.3772 - accuracy: 0.8923\r\nEpoch 41/50\r\n70/70 [==============================] - 5s 77ms/step - loss: 0.4016 - accuracy: 0.8863\r\nEpoch 42/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.3842 - accuracy: 0.9015\r\nEpoch 43/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.4055 - accuracy: 0.8925\r\nEpoch 44/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.3969 - accuracy: 0.8927\r\nEpoch 45/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.3609 - accuracy: 0.8984\r\nEpoch 46/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.4033 - accuracy: 0.8918\r\nEpoch 47/50\r\n70/70 [==============================] - 5s 76ms/step - loss: 0.3661 - accuracy: 0.8934\r\nEpoch 48/50\r\n70/70 [==============================] - 5s 75ms/step - loss: 0.3691 - accuracy: 0.8995\r\nEpoch 49/50\r\n70/70 [==============================] - 5s 77ms/step - loss: 0.3702 - accuracy: 0.8982\r\nEpoch 50/50\r\n70/70 [==============================] - 5s 78ms/step - loss: 0.3697 - accuracy: 0.8991\r\n>> Finished. Time elapsed: 00 : 04 : 34.\r\n```\r\n**Other info**\r\n`TF_CONFIG` is set in the environment of all 3 machines, as follows:\r\n```\r\n{\"cluster\": {\"worker\": [\"xxx:2121\", \"yyy:2121\", \"zzz:2121\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}\r\n{\"cluster\": {\"worker\": [\"xxx:2121\", \"yyy:2121\", \"zzz:2121\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}\r\n{\"cluster\": {\"worker\": [\"xxx:2121\", \"yyy:2121\", \"zzz:2121\"]}, \"task\": {\"type\": \"worker\", \"index\": 2}}\r\n```\r\n", "comments": ["Hi @dgoldenberg-audiomack, I don't think this finding is too surprising. There is always some amount of overhead associated with distributed training. If you're using `MirroredStrategy` then there is overhead at the end of each step to synchronize the gradients across replicas. That overhead is larger in the case of `MultiWorkerMirroredStrategy` as the gradients have to be synchronized across the replicas on a machine and then across all machines in a cluster.  The key is to make sure that the host successfully keeps the worker devices occupied by offloading enough work. For a small MNIST model and dataset, the overhead of `MultiWorkerMirroredStrategy` might not be worth it.\r\n\r\nSomething else to keep in mind is that as you add more replicas, you need to scale your batch size,  as this allows for higher device utilization and amortize the costs of communication across multiple workers. The benefit of synchronous distributed training strategies is that you're able to process more data on a single step, and thus each epoch takes less time (Also, personally I would recommend not explicitly `steps_per_epoch` unless it's really necessary for your use case as you will also have to keep track of scaling the step number as you scale the batch size, and that's just added complication) \r\n\r\nScaling the batch size is simple:\r\n```\r\nper_replica_batch_size = 64\r\nglobal_batch_size = per_replica_batch_size * strategy.num_replicas_in_sync\r\n\r\n...\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(global_batch_size)\r\n```\r\n\r\nFor more details on performance debugging, please see the [GPU performance guide](https://www.tensorflow.org/guide/gpu_performance_analysis). We have not added in details to this guide yet on how to debug multiworker setups, but I think most of the findings/tips for the single and multi GPU case are still useful to understand.\r\n\r\nHope this helps!", "Hi @nikitamaia, thanks for your response.  I think you may be the perfect person to talk to because so much of this is doc-related esp. for folks new-ish to the framework.\r\n\r\n> I don't think this finding is too surprising.\r\n\r\nWell, it is in fact very much surprising for a \"noob\". So much so that I'd venture to wonder why the doc about distributed processing doesn't utilize a larger dataset which would unequivocally show the benefits of something like `MultiWorkerMirroredStrategy`, when used in a multi-node scenario, right off the bat.  This would save developers quite a bit of trouble; I've seen similar questions in places like stackoverflow.  I've wondered if MNIST was too small a dataset but it would have saved me (and probably quite a few others) the time wondering and typing posts.\r\n\r\n> For more details on performance debugging, please see the GPU performance guide. \r\n\r\nAnd that is another big point of user confusion, right there.  Where does the multi-worker begin or end and where does the GPU support begin or end?  If you look at the guides discussing `MultiWorkerMirroredStrategy`, [the very first thing the code there does](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) is turn the GPU support **off**.  Quote:\r\n\r\n> Disable all GPUs. This prevents errors caused by the workers all trying to use the same GPU. For a real application each worker would be on a different machine.\r\n> os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n\r\nIn all, the confusion is this: I have 2 GB of data. I want to distribute model training. I'm using AWS; maybe I need a cluster of m machines, maybe I need a cluster of p3 machines?  Do I want one machine with N gpu devices?  Do I want to use X number of m machines which are cheaper?\r\n\r\nHow do things differ if I have 40 GB? 100 GB?\r\n\r\n[The doc](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) says to disable GPU support for multi-worker.  Why?  Can I possibly combine the benefits of multi-worker with those of GPU support?\r\n\r\nBasically, what's lacking, IMO, is a fully functional set of samples of large-ish datasets that walk the user through the steps of setting up a machine or a cluster and how to do the sizing and resource allocation properly.  Not only that, but how to do this with the specifics of AWS and GCP in mind.\r\n\r\nFor example, as things stand, it's not clear to me at all how one would go about getting TF to work in AWS EMR. Tensorflow can be installed there, but how do you configure it?  TF multi-worker requires the `TF_CONFIG` env var to be set on specific boxes using specific IP addresses; that's not very friendly to environments such as EMR because it requires each box to be aware of all the other cluster boxes.\r\n\r\nBeyond that (separate topic), I can't seem to use Spark to fully handle all the data processing.  It appears I'd have to pre-generate my datasets, drop them into S3, and have Tensorflow's own loaders load the data into its own datasets.  There is a [Spark TF distributor](https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-distributor) but it's entirely unclear how to use and configure it. I've also filed [this](https://github.com/tensorflow/tensorflow/issues/46236).\r\n\r\nAll of this is a huge jigsaw puzzle one has to deal with. Some of this is usability and some of this is doc. The framework screams for a full how-to, otherwise it's weeks of effort for each individual developer trying to piece all this together.\r\n\r\n", "Hi @dgoldenberg-audiomack, thanks for all the feedback!\r\nThe tutorial you're referencing is intended to demonstrate just the mechanics of using `MultiWorkerMirroredStrategy`, so the use of MNIST is to reduce any added complication from modeling/data requirements. This is mentioned in the guide in the \"Train the model\" section: \r\n``` The goal here was not to improve the training time, but only to give an example of multi-worker training.```\r\nBut in general, `MultiWorkerMirroredStrategy` works for both machines with GPUs and machines without GPUs. All of the guides on tensorflow.org are [runnable as colab notebooks,](https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb) so the disabling of the GPU in that line you're referencing is so that you can simulate a multi machine environment within colab.\r\n\r\nAs for end to end tutorials, [I published one for MirroredStrategy](https://blog.tensorflow.org/2020/12/getting-started-with-distributed-tensorflow-on-gcp.html) a few months ago. A large portion of the tutorial is GCP specific, but the explanations of how MirroredStrategy works and how to use it are applicable more generally. And there should be a follow up for MultiWorkerMirroredStrategy coming soon. Unfortunately, we don't have the bandwidth (or expertise) to provide tutorials for every cloud service, so community help here is greatly appreciated! AWS' solution architects also publish lots of blueprints for running TensorFlow on their services, so my suggestion would be to check out their examples.\r\n\r\nSimilarly, it's difficult for us to provide generic guidelines on what cluster configuration is best since we don't have control over cloud VM prices or know details about the networking, or user's data and model requirements. We have to rely on the cloud platforms to fill in the gaps here! But my personal opinion on this is to try the simpler configurations first (ie `MirroredStrategy`) and then add more resources if you think the time per epoch is still too large. Moving from single host to multi-host results in more overhead, so start with the simpler configuration first to determine if you need more scaling.\r\n\r\n[The GPU performance guide](https://www.tensorflow.org/guide/gpu_performance_analysis) also provides a formula to get a sense for what the overhead of distribution will be (ie the time to AllReduce). This might help you to determine the resources needed.\r\n```(number of parameters * 4bytes)/ (communication bandwidth)```\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Whether MirroredStrategy or MultiWorkerMirroredStrategy, I get errors such as the following:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Requires start <= limit when delta > 0: 2147483000/-2147483296\r\n```\r\n\r\nAny ideas as to what might be causing this? This is now blocking me from making any progress on this issue also.\r\n\r\nI've filed a ticket with tf-recommenders [here](https://github.com/tensorflow/recommenders/issues/274).", "Unfortunately, I don't have much context on tf-recommenders so your best bet is to wait until someone in that repo gets a chance to take a look. But one thing to check that can help determine if the issue is with the `tf.distribute.Strategy` API is to see what happens when you use `tf.distribute.get_strategy()`, instead of `MirroredStrategy` or `MultiWorkerMirroredStrategy`, which implements the tf.distribute.Strategy interface but is a pass-through and provides no actual distribution.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48373, "title": "tf.switch_case not working", "body": "**System information**\r\n- Documentation exemple : https://www.tensorflow.org/api_docs/python/tf/switch_case\r\n- Windows 10\r\n- TensorFlow version 2\r\n- Python version: 3.7\r\n\r\n**Current behavior**\r\n\r\n```\r\ndef f1(): return tf.constant(17)\r\ndef f2(): return tf.constant(31)\r\ndef f3(): return tf.constant(-1)\r\nr = tf.switch_case(tf.convert_to_tensor([1, 0]), branch_fns={0: f1, 1: f2}, default=f3)\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-6-04e5a20bbcd0>\", line 4, in <module>\r\n    r = tf.switch_case(tf.convert_to_tensor([1, 0]), branch_fns={0: f1, 1: f2}, default=f3)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3616, in switch_case\r\n    return _indexed_case_helper(branch_fns, default, branch_index, name)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3321, in _indexed_case_helper\r\n    len(branch_fns) - 1, branch_index)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 4483, in where\r\n    return gen_math_ops.select(condition=condition, x=x, y=y, name=name)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 8675, in select\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\gen06917\\PycharmProjects\\BaysianTarnet\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'then' must be at least a vector, but saw shape: [] [Op:Select]\r\n```\r\n", "comments": ["@Vaunorage,\r\nLooks like this is a duplicate of issue [#48375](https://github.com/tensorflow/tensorflow/issues/48375). \r\n\r\nCan we close this issue since it is already being tracked there? Thanks!", "It is the same function in both cases but not the same error message", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48373\">No</a>\n"]}, {"number": 48371, "title": "Update TF official devel images to Ubuntu 20.04 and precommit hooks", "body": "Update TF official devel images to Ubuntu 20.04 LTS and install pytlint.\r\n\r\n`ci_build.sh` is currently broker for `ci_sanity.sh` caused by a misalignment  between CI sanity and local images on developer machine (See https://github.com/tensorflow/tensorflow/issues/47989).\r\nNow that https://github.com/tensorflow/tensorflow/pull/48291 is merged this will let developer/contributors to locally execute python linting steps in the docker devel container with:\r\n```\r\ndocker run --rm -it -v $PWD:/tensorflow -w /tensorflow tensorflow/tensorflow:devel tensorflow/tools/ci_build/ci_sanity.sh --pylint --incremental\r\n```\r\n\r\nIt could be nice if later the full CI sanity could be run exactly on the same image where developer are working every day to avoid other potential misalignment.\r\n\r\nIf this works we could change the documentation for linting. \r\n\r\nI could expand this PR or with a new one to introduce other steps (e.g. include `--clang_format` ?) so that we could have a working minimal configuration that could be also contributed in the repository as a  pre-commit hook  considering that the the whole `ci_sanity.sh` is too heavy to be run as a git hook.\r\n\r\n/cc @mihaimaruseac @angerson \r\n\r\n", "comments": [":+1: It would be great to solve #39232, too.", "> :+1: It would be great to solve #29232, too.\n\nThat ticket Is merged. Are you sure is it correct? ", "> > +1 It would be great to solve #29232, too.\r\n> \r\n> That ticket Is merged. Are you sure is it correct?\r\n\r\nSorry, updated.", "For the other images we could probably waiting for the Dockerfile \"jungle\" refactoring at https://github.com/tensorflow/build/pull/21", "Thanks for this -- how does this do on the tests?", "> Thanks for this -- how does this do on the tests?\r\n\r\nDo we have a string to execute tests in checkout against a pip installed TF? Or do you want to execute tests after compilation in the container? ", "The tests in the Dockerfiles directory will work to start.  That would be something like:\r\n\r\n```\r\nasm_images --release nightly --arg _TAG_PREFIX=\"\" --build_images --run_tests_path=$(realpath tests) \r\n```", "> asm_images --release nightly --arg _TAG_PREFIX=\"\" --build_images --run_tests_path=$(realpath tests)\r\n\r\nYes I know this command it is in the Readme but that tests require `build_cpu.sh` and so we are in a similar case as https://github.com/tensorflow/tensorflow/pull/48421. I need hours and hours of compile time to test every image on a local machine.\r\n", "I've added a pre-commit hook prototype.", "/cc @perfinion  @theadactyl  @nikitamaia ", "@angerson  Any update on this PR? Please. Thanks!", "/cc @angerson How we could public test that this update will  lint with `ci_sanity.sh` and build Tensorflow correctly?\r\n\r\nWe need this (python3.8) to let the user run `ci_sanity.sh` on a reference developer local env.", "Do you need anything else here?", "Gently ping", "Having Python 3.8 on official docker images would be a fantastic feature. Is there anything I can help?", "Weekly ping", "@angerson gently weakly \r\n![1f3d3](https://user-images.githubusercontent.com/7026512/124187253-cae80280-da7a-11eb-92de-bc8d3d6589e5.png)\r\n", "@angerson Can you please review this PR ? Thanks!", "We passed 3 months. \r\nI think that currently we don't have the resource to review this, thanks anyway"]}, {"number": 48370, "title": "Broken/outdated Links in Docs ", "body": "While going through the documentation I encountered a few broken/outdated links, and I've found the correct link for the same \r\nKindly assign this issue to me so that I can make the required changes.\r\n", "comments": ["@UsharaniPagadala Kindly assign the issue to me", "Fixed all the broken link  \r\nhere is the PR\r\n[https://github.com/tensorflow/examples/pull/303](https://github.com/tensorflow/examples/pull/303)", "@uzair-ali10 \r\n\r\nThis issue will be closed once the PR is merged.Thanks\r\n\r\n\r\n\r\n", "@UsharaniPagadala \r\n\r\nAlright, Thanks for merging!!!"]}, {"number": 48369, "title": "Gradients are all None in GradientTape", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.15.4\r\n\r\n- TensorFlow installed from (source or binary): 2.4.1 from binary\r\n- TensorFlow version (use command below):pip install tf \r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 1.6ghz i5, 8gb ddr3\r\n\r\n\r\n**Describe the current behavior**\r\nWe get the error that no gradients have been found for the variable in our model. When printing the intermittent results, we see that all the gradients are none.\r\n**Describe the expected behavior**\r\nWe expected gradients to be not none\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n`import tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\nimport time\r\nfrom Retrieve_data import retrieve_data\r\nfrom sklearn.model_selection import train_test_split\r\nfrom Model import model_P\r\nimport keras as K\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import Dropout\r\nfrom keras.layers import Embedding\r\nfrom keras.layers import Bidirectional\r\nfrom keras.layers import Attention\r\nfrom keras.layers import Concatenate\r\nfrom keras.layers import TimeDistributed\r\nfrom keras.layers import Reshape\r\nfrom keras_self_attention import SeqSelfAttention\r\nfrom tensorflow import keras\r\n\r\n\r\n# Prepare the training dataset.\r\nbase_directory = '/Users/Desktop/Companies/'\r\nX_pricing, Y_pricing, X_reports = retrieve_data(base_directory, 'ADP')\r\nX_reports_train, X_reports_test, X_pricing_train, X_pricing_test, Y_train, Y_test = train_test_split(X_reports, X_pricing, Y_pricing, test_size=0.50, shuffle=False)\r\n\r\nparam_grid = {\r\n    'batch_size': 64,\r\n    'epochs': 50,\r\n    'embedding_dimensions': 64,\r\n    'units_LSTM_1': 128,\r\n    'units_LSTM_2': 128,\r\n    'units_BiLSTM_1': 128,\r\n    'units_BiLSTM_2': 128,\r\n    'units_Dense': 128,\r\n    'optimizer': ['ADAM'],\r\n    'loss': 'MeanSquaredError',\r\n    'activation_LSTM_1': 'softsign',\r\n    'activation_LSTM_2': 'softsign',\r\n    'activation_BiLSTM_1': 'softsign',\r\n    'activation_BiLSTM_2': 'softsign',\r\n    'dropout_LSTM_1': 0.5,\r\n    'dropout_LSTM_2': 0.5,\r\n    'dropout_BiLSTM_1': 0.5,\r\n    'dropout_BiLSTM_2': 0.5,\r\n    'metrics': 'mse',\r\n    'learning_rate': 0.000001\r\n}\r\n\r\n\r\ninput_layer = K.Input(shape=(X_pricing.shape[1], X_pricing.shape[2]), batch_size=param_grid['batch_size'])\r\n\r\n# Defining the Bidirectional LSTM Layer for Pricing\r\nBiLSTM_pricing = Bidirectional(LSTM(units=param_grid['units_LSTM_1'], return_sequences=True, activation='softsign', dropout=param_grid['dropout_BiLSTM_1']))(input_layer)\r\n\r\n    # Defining the Attention Layer for the Pricing\r\nattention_pricing = SeqSelfAttention(attention_activation='softsign')(BiLSTM_pricing)\r\n\r\n    # Adding Output Layer\r\noutput_layer = Dense(units=param_grid['units_Dense'], activation='relu')(attention_pricing)\r\n\r\n    # Building Model\r\n\r\nPricingModel = K.Model(inputs=input_layer, outputs=output_layer, name=\"PricingModel\")\r\n\r\n# Instantiate an optimizer to train the model.\r\noptimizer = keras.optimizers.SGD(learning_rate=1e-3)\r\n# Instantiate a loss function.\r\nloss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n# Prepare the metrics.\r\ntrain_acc_metric = keras.metrics.SparseCategoricalAccuracy()\r\nval_acc_metric = keras.metrics.SparseCategoricalAccuracy()\r\n\r\nx_train = X_pricing_train\r\nx_test = X_pricing_test\r\ny_train = Y_train\r\ny_test = Y_test\r\n\r\n\r\nprint(x_train.shape)\r\nprint(y_train.shape)\r\n\r\nprint(len(x_train))\r\n# Prepare the training dataset.\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(param_grid['batch_size'])\r\n\r\n\r\nepochs = 1\r\nfor epoch in range(epochs):\r\n    print(\"\\nStart of epoch %d\" % (epoch,))\r\n    start_time = time.time()\r\n\r\n    # Iterate over the batches of the dataset.\r\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\r\n        with tf.GradientTape() as tape:\r\n\r\n            logits = PricingModel(x_batch_train, training=True)\r\n            logits = np.reshape(logits, (-1, (logits.shape[2]*logits.shape[1])))\r\n            print(logits)\r\n            print(logits.shape)\r\n            loss_value = loss_fn(y_batch_train, logits)\r\n            print(loss_value)\r\n        tape.watch(loss_value)\r\n        grads = tape.gradient(loss_value, PricingModel.trainable_weights)\r\n        #print(PricingModel.trainable_weights)\r\n        #print(len(PricingModel.trainable_weights))\r\n        print(grads)\r\n        optimizer.apply_gradients(zip(grads, PricingModel.trainable_weights))\r\n\r\n        # Update training metric.\r\n        train_acc_metric.update_state(y_batch_train, logits)\r\n\r\n        # Log every 200 batches.\r\n        if step % 200 == 0:\r\n            print(\r\n                \"Training loss (for one batch) at step %d: %.4f\"\r\n                % (step, float(loss_value))\r\n            )\r\n            print(\"Seen so far: %d samples\" % ((step + 1) * param_grid['batch_size']))\r\n\r\n    # Display metrics at the end of each epoch.\r\n    train_acc = train_acc_metric.result()\r\n    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\r\n\r\n    # Reset training metrics at the end of each epoch\r\n    train_acc_metric.reset_states()\r\n\r\n`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe code produces the following error:\r\nValueError: No gradients provided for any variable: ['bidirectional/forward_lstm/lstm_cell_1/kernel:0', 'bidirectional/forward_lstm/lstm_cell_1/recurrent_kernel:0', 'bidirectional/forward_lstm/lstm_cell_1/bias:0', 'bidirectional/backward_lstm/lstm_cell_2/kernel:0', 'bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel:0', 'bidirectional/backward_lstm/lstm_cell_2/bias:0', 'seq_self_attention/seq_self_attention_Add_Wt:0', 'seq_self_attention/seq_self_attention_Add_Wx:0', 'seq_self_attention/seq_self_attention_Add_bh:0', 'seq_self_attention/seq_self_attention_Add_Wa:0', 'seq_self_attention/seq_self_attention_Add_ba:0', 'dense/kernel:0', 'dense/bias:0'].\r\n\r\n\r\n", "comments": ["@Maximevdm ,\r\n\r\nOn running the given code, I am facing an error stating **ModuleNotFoundError: No module named 'Retrieve_data'**. Please find the [gist](https://colab.research.google.com/gist/tilakrayal/6d776a58b332d314f79ca77b49f058e0/48369-2-4.ipynb) of it here and share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\nThanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48369\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48369\">No</a>\n"]}, {"number": 48368, "title": "AttributeError: 'NoneType' object has no attribute 'shape'", "body": "Hello,\r\n\r\nI'm trying to train a model using the `model.fit()` function, but it keeps throwing me an error which I cannot resolve. \r\nThe error is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 355, in <module>\r\n    main()\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"train.py\", line 338, in main\r\n    history = model.fit(\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in user code:\r\n\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:758 train_step\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:387 update_state\r\n        self.build(y_pred, y_true)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:317 build\r\n        self._metrics = nest.map_structure_up_to(y_pred, self._get_metric_objects,\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1159 map_structure_up_to\r\n        return map_structure_with_tuple_paths_up_to(\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1257 map_structure_with_tuple_paths_up_to\r\n        results = [\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1258 <listcomp>\r\n        func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1161 <lambda>\r\n        lambda _, *values: func(*values),  # Discards the path arg.\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:418 _get_metric_objects\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:418 <listcomp>\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:439 _get_metric_object\r\n        y_t_rank = len(y_t.shape.as_list())\r\n\r\n    AttributeError: 'NoneType' object has no attribute 'shape'\r\n```\r\n\r\nThis is my `.fit()` function:\r\n\r\n```\r\nhistory = model.fit(\r\n        train_ds, \r\n        validation_data=val_ds, \r\n        epochs=EPOCHS, \r\n        batch_size=BATCH_SIZE,\r\n        callbacks=[earlyStopping, modelCheckpoint]\r\n    )\r\n```\r\n\r\nMy train dataset is a `tf.data.Dataset` and is of type: `<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>` \r\n\r\nThe model is building and compiling fine, but training...\r\n\r\nWhen I try to look at the dataset using:\r\n\r\n```\r\nfor batch in train_ds.take(1):\r\n    input_shape = batch[\"spectrogram\"].shape\r\n    label_shape = batch[\"label\"].shape\r\n    print('Input shape:', input_shape)\r\n```\r\n\r\nIt outputs the shape just fine.\r\n\r\nI'm looking forward for help:)", "comments": ["@redmlr,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "All you need is in this simple colab  https://colab.research.google.com/drive/1MuPS8uruRzWo_tEFJjgVPUOD8NqGPrQG?usp=sharing", "@redmlr,\r\nThank you for the update.\r\n\r\nHowever, I do not have access to the Colab notebook. Could you please provide the required permissions to view the file?", "@amahendrakar it should work now:) https://colab.research.google.com/drive/1MuPS8uruRzWo_tEFJjgVPUOD8NqGPrQG?usp=sharing", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@redmlr,\r\nThank you for the gist. \r\n\r\nIn the given code snippet, the `mod_func` function returns a Python dictionary. And since Python dictionaries do not have the `shape` attribute the `AttributeError: 'dict' object has no attribute 'shape'` error is thrown. \r\n\r\nIf you require the `shape` attribute, I'd suggest you to use an alternative data structure. Thanks! ", "@amahendrakar Thank you!\nWhich data structure do you suggest for a model with 2 Input() layers?", "@redmlr,\r\nIn this case, you can either return the strings as a Tensor or use the `len()` function instead of shape for the dictionary. \r\n\r\nPlease check [this gist](https://colab.research.google.com/gist/amahendrakar/9e01e74b727fe4b5041982bdfdbfff77/48368.ipynb#scrollTo=yDEvUrPfqfif&line=2&uniqifier=1) for reference. Thanks!", "@amahendrakar Well, unfortunately it's not that simple. The error occurred in the `model.fit()` function where I don't have any control over using `len()` or `.shape`.", "@redmlr,\r\nCould you please provide the complete code snippet along with the `model.fit()` function, so that we can look into this? Thanks!", "@amahendrakar, This is the main function: \r\n```\r\ndef main():\r\n    train_list, validation_list, test_list = load_json_into_lists(TRAIN_DS_PATH, TEST_DS_PATH)\r\n    train_ds = preprocess_dataset(train_list)\r\n    val_ds = preprocess_dataset(validation_list)\r\n    test_ds = preprocess_dataset(test_list)\r\n\r\n    for spectrogram, label in train_ds.take(1):\r\n        input_shape = spectrogram.shape\r\n\r\n    train_ds = train_ds.cache().prefetch(AUTOTUNE)\r\n    val_ds = val_ds.cache().prefetch(AUTOTUNE)\r\n\r\n    model = build_model(input_shape)\r\n    model.summary()\r\n\r\n    earlyStopping = Keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, restore_best_weights=True)\r\n\r\n    train_ds = train_ds.batch(BATCH_SIZE)\r\n    val_ds = val_ds.batch(BATCH_SIZE)\r\n\r\n    print(type(train_ds))\r\n\r\n    history = model.fit(\r\n        train_ds, \r\n        validation_data=val_ds, \r\n        epochs=EPOCHS, \r\n        callbacks=[earlyStopping]\r\n    )\r\n\r\n```\r\n\r\n```\r\ndef preprocess_dataset(files):\r\n\r\n    # Creating a new list, just including the filenames\r\n    filenames_train_list = [sample[0] for sample in files]\r\n\r\n    # Converting the filenames into tensors\r\n    filename_tensors = string_to_tensor(tf.constant(filenames_train_list))\r\n\r\n    # Creating a tensorflow dataset containing audio filenames\r\n    files_ds = tf.data.Dataset.from_tensor_slices(filename_tensors)\r\n\r\n    # Conversion into a waveform dataset (filenames converted to waveforms)\r\n    waveform_ds = files_ds.map(lambda file_path: tf.py_function(get_waveform_and_label, [file_path], [tf.float32, tf.string]), num_parallel_calls=AUTOTUNE) \r\n\r\n    # Next Conversion: waveforms into spectrograms\r\n    spectrogram_ds = waveform_ds.map(\r\n        get_spectrogram_and_label, num_parallel_calls=AUTOTUNE\r\n    )\r\n\r\n    return spectrogram_ds \r\n```\r\n\r\nI think you can use every model that you want to reproduce the error.", "@redmlr,\r\nYou have defined the functions but are not calling them anywhere. On calling the main() function, I am facing an error stating `NameError: name 'load_json_into_lists' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3ebad085a7493cce8c3d154feb0d8de6/48368.ipynb#scrollTo=jDObUjkfolzd).\r\n\r\n\r\nCould you please provide the complete code along with the dataset, so that we can reproduce the issue. Alternatively, you can run the code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks!", "@amahendrakar Sorry! I updated the gist with some code and comment instructions now.", "@redmlr,\r\nLooks like the changes were not saved in the gist. I still see the same code. \r\n\r\n![Screenshot 2021-04-22 9 15 51 PM](https://user-images.githubusercontent.com/57165142/115745681-48bfcb80-a3b1-11eb-9d47-412064136899.png)\r\n\r\nCould you please save the changes and share the updated link. You can also save the gist using the following method 'File' -> 'Save a copy as Github Gist', and share the link of the new window. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48368\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48368\">No</a>\n"]}, {"number": 48367, "title": "Tensorflow 1.13.1 GPU xla training hang at cuDevicePrimaryCtxRetain (NVIDIA MPS)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binay\r\n- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.4.1.5-1+cuda10.0\r\n- GPU model and memory: Tesla V100-SXM2, 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\npython process hang\r\n\r\n```\r\nma-user     273     11  0 Apr03 ?        00:06:34 /home/ma-user/anaconda/bin/python thumt/bin/trainer_beta.py --input=en.train__ur.train --batch_size=2048 --spm_model=spm.model --references=ur.dev --vocab=en.vocab.txt__ur.vocab.txt --type=deep_25_3 --validation=en.dev --data_url=/home/ma-user/ma/inputs/data_url_0/ --train_url=/home/ma-user/ma/outputs/train_url_0/\r\n```\r\n\r\npy-spy dump info\r\n\r\n![py-spy](https://user-images.githubusercontent.com/8072378/113856754-3dc34380-97d4-11eb-98d9-32481aa51db3.png)\r\n\r\n\r\nfinal logs\r\n\r\n```\r\nINFO:tensorflow:Total trainable variables size: 159322624\r\nINFO:tensorflow:Total trainable variables size: 159322624\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Create EvaluationHook.\r\nINFO:tensorflow:Create EvaluationHook.\r\nINFO:tensorflow:Making dir: /home/ma-user/ma/inputs/train_url_0/eval\r\nINFO:tensorflow:Making dir: /home/ma-user/ma/inputs/train_url_0/eval\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:set by user per_process_gpu_memory_fraction = 0.450000\r\nINFO:tensorflow:set by user per_process_gpu_memory_fraction = 0.450000\r\n2021-04-03 04:15:11.694498: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n```\r\n\r\ngdb bt (partial)\r\n\r\n```\r\nThread 91 (Thread 0x7f01057fe700 (LWP 511)):\r\n#0  0x00007f043b8d726d in __lll_lock_wait ()\r\n   from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#1  0x00007f043b8d3288 in pthread_rwlock_rdlock ()\r\n   from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#2  0x00007f035129d5a1 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1\r\n#3  0x00007f035129eb1e in ?? () from /usr/local/nvidia/lib64/libcuda.so.1\r\n#4  0x00007f035129f2c9 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1\r\n#5  0x00007f0351348fbe in ?? () from /usr/local/nvidia/lib64/libcuda.so.1\r\n#6  0x00007f035134b0d7 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1\r\n#7  0x00007f0351275719 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1\r\n#8  0x00007f03513e715e in cuDevicePrimaryCtxRetain ()\r\n   from /usr/local/nvidia/lib64/libcuda.so.1\r\n#9  0x00007f03605b942d in stream_executor::cuda::CUDADriver::CreateContext(int, stream_executor::DeviceOptions const&, stream_executor::cuda::CudaContext**) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#10 0x00007f03605c15a4 in stream_executor::cuda::CUDAExecutor::Init(int, stream_executor::DeviceOptions) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#11 0x00007f03604fa7b7 in stream_executor::StreamExecutor::Init(int, stream_executor::DeviceOptions) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#12 0x00007f03605c839d in stream_executor::cuda::CudaPlatform::GetUncachedExecutor(stream_executor::StreamExecutorConfig const&) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#13 0x00007f03605c78dc in std::_Function_handler<stream_executor::port::StatusOr<std::unique_ptr<stream_executor::StreamExecutor, std::default_delete<stream_executor::StreamExecutor> > > (), stream_executor::cuda::CudaPlatform::GetExecutor(stream_executor::StreamExecutorConfig const&)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#14 0x00007f0360432343 in stream_executor::ExecutorCache::GetOrCreate(stream_executor::StreamExecutorConfig const&, std::function<stream_executor::port::StatusOr<std::unique_ptr<stream_executor::StreamExecutor, std::default_delete<stream_executor::StreamExecutor> > > ()> const&) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#15 0x00007f03605c7960 in stream_executor::cuda::CudaPlatform::GetExecutor(stream_executor::StreamExecutorConfig const&) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#16 0x00007f03683499a6 in xla::PlatformUtil::GetStreamExecutors(stream_executor::Platform*)::{lambda()#2}::operator()() const ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#17 0x00007f036015fdc6 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#18 0x00007f036015ec84 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#19 0x00007f0352626c5c in std::execute_native_thread_routine_compat (\r\n    __p=<optimized out>)\r\n    at /home/msarahan/miniconda2/conda-bld/compilers_linux-64_1507259624353/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110\r\n#20 0x00007f043b8ce6ba in start_thread ()\r\n   from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#21 0x00007f043b60441d in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\nspecially, the training job use the NVIDIA GPU by NVIDIA MPS\r\n\r\nmps control log (i can't see any 04-03 log)\r\n\r\n```\r\n[2021-04-02 20:15:11.789 Control    10] Accepting connection...\r\n[2021-04-02 20:15:11.789 Control    10] NEW CLIENT 0 from user 1000: Server already exists\r\n[2021-04-06 01:26:33.903 Control    10] Accepting connection...\r\n[2021-04-06 01:26:33.903 Control    10] User did not send valid credentials\r\n[2021-04-06 01:26:33.903 Control    10] Accepting connection...\r\n[2021-04-06 01:26:33.903 Control    10] NEW CLIENT 0 from user 1000: Server already exists\r\n```\r\n\r\nmps server log (i can't see any 04-03 log too)\r\n\r\n```\r\n[2021-04-02 20:15:11.789 Other   331] Volta MPS: Creating worker thread\r\n[2021-04-02 20:15:11.789 Other   331] Volta MPS: Device Tesla V100-SXM2-32GB (uuid 0x9be86048-0x5b1bd801-0x1850e545-0xac577163) is associated\r\n[2021-04-06 00:59:57.679 Other   331] Receive command failed, assuming client exit\r\n[2021-04-06 00:59:57.679 Other   331] Volta MPS: Client process disconnected\r\n[2021-04-06 00:59:57.681 Other   331] Receive command failed, assuming client exit\r\n[2021-04-06 00:59:57.681 Other   331] Receive command failed, assuming client exit\r\n[2021-04-06 00:59:57.681 Other   331] Volta MPS: Client disconnected. Number of active client contexts is 9\r\n[2021-04-06 00:59:57.681 Other   331] Volta MPS: Client disconnected. Number of active client contexts is 8\r\n```\r\n\r\nit's very odd, how can this thing happen, cuda can't connect to the MPS ? thanks for any help for this problem\r\n", "comments": ["@zrss \r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "thanks for the reply, i could try, but i am not sure of tensorflow can run on MPS, does tensorflow support officially MPS ", "it seems `mps-server` crash: restart the job not work, but i kill the existed`mps-server` proc, and restart the job, it works", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48367\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48367\">No</a>\n"]}, {"number": 48366, "title": "tf.image.resize_with_pad slows the computation enormously", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): tf-nightly==2.6.0.dev20210331\r\n- TensorFlow version (use command below): 2.6.0-dev20210331\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2.2 / 8.1.1\r\n- GPU model and memory: RTX 3090 24267MiB\r\n\r\n**Describe the current behavior**\r\nWhen using `tf.image.resize_with_pad` with `tf.data.Dataset`, the training of the model becomes so slow. Surprisingly, commenting `tf.image.resize_with_pad` out and using `tf.image.resize` instead, speeds up the computation at least > 5x times.   \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nShouldn't both of them give more or less the same performance? Or is this a bug?\r\n\r\n**Standalone code to reproduce the issue**\r\nSample used function:\r\n\r\n```\r\n    @tf.function\r\n    def resize(self, image):\r\n        # for portrait images, the aspect ratio would stay the same\r\n        # no distortion of images, would be padded with zeros\r\n        image = tf.image.resize_with_pad(\r\n            image,\r\n            self.target_height,\r\n            self.target_width,\r\n            method=tf.image.ResizeMethod.BILINEAR,\r\n            antialias=False\r\n        )\r\n\r\n        # it is much much faster with normal resize\r\n        # image = tf.image.resize(image, [self.target_height, self.target_width])\r\n\r\n        return image\r\n```\r\n\r\n", "comments": ["Just tried other `tf.image.*` functions such as `tf.image.random_crop` and also slowed down the training. \r\n\r\nFor now, I am avoiding using any of these functions and just preprocess with `tf.image.resize` for faster training. :unamused:", "You could speed it up via ```tf.config.optimizer.set_experimental_options({'debug_stripper': True})``` and it's normal that ```random_crop``` slower than ```resize```.", "@fsx950223 didn't make much difference. It is normal to be slower, but > 5x slower? ", "@hashishoya ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Thanks!", "@tilakrayal , I am sharing a working sample of a code which reproduces the mentioned issue:\r\n\r\n```python\r\nimport numpy as np\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nCFG = dict(\r\n            dataset_path=os.path.join('datasets', 'grassclover'),\r\n            train_data_txt='train.txt',\r\n            validation_data_txt='val.txt',\r\n            test_data_txt='val.txt',\r\n            output_dir=os.path.join('outputs', 'grassclover'),\r\n            # network hyperparameters\r\n            batch_size=8,\r\n            n_classes=15,\r\n            epochs=100,\r\n            target_height=512,\r\n            target_width=512,\r\n            n_channels=3,\r\n            replace_with_label=15\r\n        )\r\n\r\n\r\nclass CreateDataset:\r\n    def __init__(self):\r\n        self.cfg = CFG\r\n\r\n        self.step = 32  # model required divisible image size\r\n        self.target_height = tf.cast(\r\n            tf.math.multiply(tf.math.ceil(tf.math.divide(self.cfg[\"target_height\"], self.step)), self.step),\r\n            tf.int32\r\n        ).numpy()\r\n\r\n        self.target_width = tf.cast(\r\n            tf.math.multiply(tf.math.ceil(tf.math.divide(self.cfg[\"target_width\"], self.step)), self.step),\r\n            tf.int32\r\n        ).numpy()\r\n\r\n        self.is_only_resize = True  # normalize resize, False -> random cropping and resize\r\n\r\n    @tf.function\r\n    def normalize(self, input_image: tf.Tensor):\r\n        \"\"\"\r\n        normalizes image pixel values between 0.0 and 1.0\r\n        :param input_image: Tensor containing image : dim [Size, Size, 3]\r\n        :return: normalized image\r\n        \"\"\"\r\n\r\n        input_image = tf.cast(input_image, tf.float32) / 255.0\r\n\r\n        return input_image\r\n\r\n    @tf.function\r\n    def read_file(self, full_path):\r\n        img = tf.io.read_file(full_path)\r\n\r\n        return img\r\n\r\n    @tf.function\r\n    def decode_image(self, image):\r\n        image = tf.image.decode_jpeg(image, channels=3)\r\n        image = tf.image.convert_image_dtype(image, tf.uint8)\r\n\r\n        return image\r\n\r\n    @tf.function\r\n    def decode_mask(self, mask):\r\n        mask = tf.image.decode_png(mask, channels=1)\r\n\r\n        return mask\r\n\r\n    @tf.function\r\n    def resize_crop(self, image, seed, channels):\r\n        dimension = tf.cond(tf.math.greater(tf.shape(image)[0], tf.shape(image)[1]), lambda: tf.shape(image)[1], lambda: tf.shape(image)[0])\r\n\r\n        cropped_image = tf.image.random_crop(\r\n            image,\r\n            size=[dimension, dimension, channels],\r\n            seed=seed\r\n        )\r\n        image = tf.image.resize(\r\n            cropped_image,\r\n            [self.target_height, self.target_width],\r\n            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\r\n        )\r\n\r\n        # OR\r\n        # image = tf.image.resize_with_pad(image, self.target_height, self.target_width)\r\n\r\n        return image\r\n\r\n    @tf.function\r\n    def resize(self, image):\r\n        # it is much much faster with normal resize\r\n        return tf.image.resize(image, [self.target_height, self.target_width])\r\n\r\n    def load_and_preprocess_image(self, path):\r\n        image = self.read_file(path)\r\n        image = self.decode_image(image)\r\n\r\n        if self.is_only_resize:\r\n            image = self.resize(image)\r\n        else:\r\n            image = self.resize_crop(image, self.seed, channels=3)\r\n\r\n        image = self.normalize(image)\r\n\r\n        return image\r\n\r\n    def load_and_preprocess_mask(self, path):\r\n        mask = self.read_file(path)\r\n        mask = self.decode_mask(mask)\r\n        if self.is_only_resize:\r\n            mask = self.resize(mask)\r\n        else:\r\n            mask = self.resize_crop(mask, self.seed, channels=1)\r\n\r\n        return mask\r\n\r\n    def create_dataset_train_val_test(self, path_dataset, txt_training_data, txt_validation_data, txt_test_data):\r\n        \"\"\"\r\n        Loads training, validation, and test datasets\r\n        Args:\r\n            path_dataset: path to dataset folder\r\n            path_training_data: path to training data\r\n            path_validation_data: path to validation data\r\n            path_test_data: path to test data\r\n\r\n        Returns: dictionary containing train, validation, and test datasets\r\n\r\n        \"\"\"\r\n        for set_txt, mode in zip([txt_training_data, txt_validation_data, txt_test_data], ['train', 'val', 'test']):\r\n            with open(os.path.join(self.cfg[\"dataset_path\"], set_txt), 'r') as fr:\r\n                pairs = fr.read().splitlines()\r\n\r\n                img_paths, lb_paths = [], []\r\n                for pair in pairs:\r\n                    imgpth, lbpth = pair.split(',')\r\n                    img_paths.append(os.path.join(path_dataset, imgpth))\r\n                    lb_paths.append(os.path.join(path_dataset, lbpth))\r\n\r\n                if mode == 'train':\r\n                    assert len(img_paths) == len(lb_paths)\r\n                    print(f\"The training dataset contains {len(img_paths)} images.\")\r\n\r\n                    self.length_training_data = len(img_paths)\r\n\r\n                    train_img_ds = tf.data.Dataset.from_tensor_slices(img_paths)\r\n                    train_msk_ds = tf.data.Dataset.from_tensor_slices(lb_paths)\r\n\r\n                    self.seed = np.random.randint(0, 100)\r\n                    train_img_ds = train_img_ds.map(self.load_and_preprocess_image)\r\n                    train_msk_ds = train_msk_ds.map(self.load_and_preprocess_mask)\r\n\r\n                    train_dataset = tf.data.Dataset.zip((train_img_ds, train_msk_ds))\r\n\r\n                elif mode == 'val':\r\n                    assert len(img_paths) == len(lb_paths)\r\n                    print(f\"The validation dataset contains {len(img_paths)} images.\")\r\n\r\n                    self.length_validation_data = len(img_paths)\r\n\r\n                    val_img_ds = tf.data.Dataset.from_tensor_slices(img_paths)\r\n                    val_msk_ds = tf.data.Dataset.from_tensor_slices(lb_paths)\r\n\r\n                    val_img_ds = val_img_ds.map(self.load_and_preprocess_image)\r\n                    val_msk_ds = val_msk_ds.map(self.load_and_preprocess_mask)\r\n\r\n                    val_dataset = tf.data.Dataset.zip((val_img_ds, val_msk_ds))\r\n\r\n                else:\r\n                    assert len(img_paths) == len(lb_paths)\r\n                    print(f\"The test dataset contains {len(img_paths)} images.\")\r\n\r\n                    self.length_test_data = len(img_paths)\r\n\r\n                    test_img_ds = tf.data.Dataset.from_tensor_slices(img_paths)\r\n                    test_msk_ds = tf.data.Dataset.from_tensor_slices(lb_paths)\r\n\r\n                    test_img_ds = test_img_ds.map(self.load_and_preprocess_image)\r\n                    test_msk_ds = test_msk_ds.map(self.load_and_preprocess_mask)\r\n\r\n                    test_dataset = tf.data.Dataset.zip((test_img_ds, test_msk_ds))\r\n\r\n        train_val_test_dataset = {\r\n            \"train\": train_dataset,\r\n            \"val\": val_dataset,\r\n            \"test\": test_dataset\r\n        }\r\n\r\n        return train_val_test_dataset\r\n\r\n\r\nclass TrainUNet:\r\n    def __init__(self):\r\n        self.cfg = CFG\r\n        self.seed = 42\r\n\r\n        self.create_dataset = CreateDataset()\r\n\r\n        self.train_batch_set, self.val_batch_set, self.test_batch_set = self.prepare_dataset()\r\n\r\n    def prepare_dataset(self):\r\n        dataset = self.create_dataset.create_dataset_train_val_test(\r\n            self.cfg[\"dataset_path\"],\r\n            self.cfg[\"train_data_txt\"],\r\n            self.cfg[\"validation_data_txt\"],\r\n            self.cfg[\"test_data_txt\"]\r\n        )\r\n\r\n        # -- Training Dataset --#\r\n        train_dataset = dataset['train']\r\n        batch_train_dataset = train_dataset.batch(self.cfg[\"batch_size\"])\r\n\r\n        # -- Validation Dataset --#\r\n        val_dataset = dataset['val']\r\n        batch_val_dataset = val_dataset.batch(self.cfg[\"batch_size\"])\r\n\r\n        # -- Test Dataset --#\r\n        test_dataset = dataset['test']\r\n        batch_test_dataset = test_dataset.batch(self.cfg[\"batch_size\"])\r\n\r\n        return batch_train_dataset, batch_val_dataset, batch_test_dataset\r\n\r\n    def get_model(self, input_size, initializer='he_normal'):\r\n        # -- Encoder -- #\r\n        # Block encoder 1\r\n        inputs = tf.keras.layers.Input(shape=input_size)\r\n        conv_enc_1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=initializer)(inputs)\r\n        conv_enc_1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv_enc_1)\r\n\r\n        # Block encoder 2\r\n        max_pool_enc_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv_enc_1)\r\n        conv_enc_2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=initializer)(max_pool_enc_2)\r\n        conv_enc_2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv_enc_2)\r\n\r\n        # Block  encoder 3\r\n        max_pool_enc_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv_enc_2)\r\n        conv_enc_3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=initializer)(max_pool_enc_3)\r\n        conv_enc_3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv_enc_3)\r\n        # -- Encoder -- #\r\n\r\n        # ----------- #\r\n        max_pool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv_enc_3)\r\n        conv = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer=initializer)(max_pool)\r\n        conv = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv)\r\n        # ----------- #\r\n\r\n        # -- Decoder -- #\r\n        # Block decoder 1\r\n        up_dec_1 = tf.keras.layers.Conv2D(256, 2, activation='relu', padding='same', kernel_initializer=initializer)(tf.keras.layers.UpSampling2D(size=(2, 2))(conv))\r\n        merge_dec_1 = tf.keras.layers.concatenate([conv_enc_3, up_dec_1], axis=3)\r\n        conv_dec_1 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=initializer)(merge_dec_1)\r\n        conv_dec_1 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv_dec_1)\r\n\r\n        # Block decoder 2\r\n        up_dec_2 = tf.keras.layers.Conv2D(128, 2, activation='relu', padding='same', kernel_initializer=initializer)(tf.keras.layers.UpSampling2D(size=(2, 2))(conv_dec_1))\r\n        merge_dec_2 = tf.keras.layers.concatenate([conv_enc_2, up_dec_2], axis=3)\r\n        conv_dec_2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=initializer)(merge_dec_2)\r\n        conv_dec_2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv_dec_2)\r\n\r\n        # Block decoder 3\r\n        up_dec_3 = tf.keras.layers.Conv2D(64, 2, activation='relu', padding='same', kernel_initializer=initializer)(tf.keras.layers.UpSampling2D(size=(2, 2))(conv_dec_2))\r\n        merge_dec_3 = tf.keras.layers.concatenate([conv_enc_1, up_dec_3], axis=3)\r\n        conv_dec_3 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=initializer)(merge_dec_3)\r\n        conv_dec_3 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv_dec_3)\r\n        conv_dec_3 = tf.keras.layers.Conv2D(2, 3, activation='relu', padding='same', kernel_initializer=initializer)(conv_dec_3)\r\n        # -- Decoder -- #\r\n\r\n        output = tf.keras.layers.Conv2D(self.cfg[\"n_classes\"], 1, activation='softmax')(conv_dec_3)\r\n\r\n        return tf.keras.Model(inputs=inputs, outputs=output)\r\n\r\n    def train(self):\r\n        model = self.get_model([self.create_dataset.target_height, self.create_dataset.target_width, 3])\r\n        model.compile(\r\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n            optimizer=\"adam\",\r\n            metrics=[\"accuracy\"]\r\n        )\r\n\r\n        history = model.fit(\r\n            self.train_batch_set,\r\n            validation_data=self.val_batch_set,\r\n            epochs=self.cfg[\"epochs\"]\r\n        )\r\n\r\n\r\ndef main():\r\n    train_unet = TrainUNet()\r\n    train_unet.train()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\nI'm using [Grassclover](https://vision.eng.au.dk/grass-clover-dataset/) dataset, however, since it is a large one, I made a smaller version of it and uploaded it to [OneDrive](https://1drv.ms/u/s!AqiDxJkDCNL4gYs71eo8Z0lujE0FwQ?e=AM46W7). Should be unzipped to the root folder as `datasets/grassclover/`.\r\n\r\nPlease in order to check the difference in the performance, simply change  `self.is_only_resize=True` to `self.is_only_resize=False`. \r\n\r\nThanks!\r\n", "You should remove ```@tf.function```, since it retraces different shape images.", "@hashishoya ,\r\n\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status.", "@tilakrayal nope, removing `@tf.function` didn't solve the issue. ", "@hashishoya \r\nThe reduction in speed is expected because tf has to pad after resizing, as there is large amounts of padding in your case. [which is obvious as you have to pad across 2 dimension of a 3d image]\r\n\r\nCan you try manually padding after resizing and compare it with the resize_with_pad function to check the difference and let us know.\r\n\r\nAlso check these self-contained walk-throughs : https://www.tensorflow.org/guide/data_performance and https://www.tensorflow.org/guide/profiler\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48364, "title": "LSTM layer not possible with channels_first", "body": "The following:\r\n```\r\ntf.keras.backend.set_image_data_format(\"channels_first\")\r\ntf.keras.layers.LSTM(units=256)(x)\r\n```\r\nReturns:\r\n```ValueError: Shape must be at least rank 3 but is rank 2 for '{{node lstm/lstm_cell_1/BiasAdd}} = BiasAdd[T=DT_HALF, data_format=\"NCHW\"](lstm/lstm_cell_1/MatMul, lstm/lstm_cell_1/split_1)' with input shapes: [?,256], [256].```\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: NVIDIA V100", "comments": ["#38942", "@JeffreyWardman \r\nI ran the code shared but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/bafddf15f69ea5591a6d5a5b1d9e87f1/untitled583.ipynb) and share all dependencies such that we can replicate the issue reported .", "@Saduf2019 `x` can be anything", "@JeffreyWardman\r\nplease share a colab gist with the issue reported.", "@Saduf2019 see this [gist](https://colab.research.google.com/gist/JeffreyWardman/b61f01c52c433b7dbfc60babcb82ea31/untitled583.ipynb#scrollTo=npnrkMQiPkLzl)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@JeffreyWardman \r\n\r\nThe error in the gist is self explanatory, Your input layer is incorrect, please refer to this [link](https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/#:~:text=Tips%20for%20LSTM%20Input,-This%20section%20lists&text=The%20LSTM%20input%20layer%20must,on%20the%20first%20hidden%20layer.) for the correct input shape.\r\n\r\n\r\nYou may refer to this working example to [fix your code](https://keras.io/api/layers/recurrent_layers/lstm/)\r\n\r\nFor any further queries please create a [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue as this is not a bug or a performance issue and there is a big community to support and learn from your questions.", "@Saduf2019 could you please provide a working gist that works with channels first? That way there is evidence of it working.", "@JeffreyWardman \r\nPlease find your code[ here](https://machinelearningmastery.com/a-gentle-introduction-to-channels-first-and-channels-last-image-formats-for-deep-learning/) for any further queries kindly open a stackoverflow issue.", "> @JeffreyWardman\r\n> I ran the code shared but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/bafddf15f69ea5591a6d5a5b1d9e87f1/untitled583.ipynb) and share all dependencies such that we can replicate the issue reported .\r\n\r\ni have done some changes with the gist ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48364\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48364\">No</a>\n", "Please reopen this issue. This problem remains with tensorflow version 2.4 and 2.5.\r\n\r\nThe following works:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.keras.backend.set_image_data_format(\"channels_last\")\r\nx = tf.zeros((15, 256))\r\nprint(x.shape)\r\nx = tf.keras.layers.Input(shape=x.shape)\r\nx = tf.keras.layers.LSTM(units=32)(x)\r\n```\r\n\r\nthis **does not** work:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.keras.backend.set_image_data_format(\"channels_first\")\r\nx = tf.zeros((15, 256))\r\nprint(x.shape)\r\nx = tf.keras.layers.Input(shape=x.shape)\r\nx = tf.keras.layers.LSTM(units=32)(x)\r\n```\r\n\r\nAccording to documentation for keras.layers.LSTM:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\r\n\r\nthe default input has to have shape (batch_dim, timesteps, features).\r\n\r\nThis is the case in both examples, with timesteps = 15 and features = 256.\r\n\r\nIt is unclear why the LSTM layer should be affected by the \"channels_first\" or \"channels_last\" setting.\r\n\r\n", "This seems to be related to how biases are implemented within the LSTM layer as the following example works as well:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.keras.backend.set_image_data_format(\"channels_first\")\r\nx = tf.zeros((15, 256))\r\nprint(x.shape)\r\nx = tf.keras.layers.Input(shape=x.shape)\r\nx = tf.keras.layers.LSTM(units=32, use_bias=False)(x)\r\n```"]}, {"number": 48362, "title": "While converting from HLO to linalg the conversion of mhlo.convolution(contains padding ) -> linalg.conv fails  with error \"non-zero padding unsupported yet\" ", "body": "While converting from HLO to linalg the conversion of mhlo.convolution(contains padding ) -> linalg.conv fails \r\nwith error \"non-zero padding unsupported yet\"  \r\n \r\n \"mhlo.convolution\"(%510, %532) {batch_group_count =  1 : i64, dimension_numbers =  {input_batch_dimension =  0 : i64, input_feature_dimension =  3 : i64, input_spatial_dimensions =  dense<[1, 2]> : tensor<2xi64>, kernel_input_feature_dimension =  2 : i64, kernel_output_feature_dimension =  3 : i64, kernel_spatial_dimensions =  dense<[0, 1]> : tensor<2xi64>, output_batch_dimension =  0 : i64, output_feature_dimension =  3 : i64, output_spatial_dimensions =  dense<[1, 2]> : tensor<2xi64>}, feature_group_count =  1 : i64, padding =  dense<1> : tensor<2x2xi64>, rhs_dilation =  dense<1> : tensor<2xi64>, window_strides =  dense<1> : tensor<2xi64>} : (tensor<4x56x56x64xf32>, tensor<3x3x64x64xf32>) -> tensor<4x56x56x64xf32>\r\n\r\nThe documentation @ https://mlir.llvm.org/docs/Dialects/Linalg/#linalgpad_tensor-mlirlinalgpadtensorop\r\nfor linalg.conv (::mlir::linalg::ConvOp) seems to contain attribute \r\npadding | ::mlir::DenseIntElementsAttr | 64-bit signless integer elements attribute\r\n \r\nIs the padding support missing in linalg.conv as of now and is due for implementation if so when could it be added.", "comments": ["@dinkdeep,\r\nCould you please provide the \r\n- TensorFlow version\r\n- minimal code snippet to reproduce the error\r\n- and the exact sequence of commands / steps that you executed before running into the error\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48361, "title": "Nested model not training for distributed gradient tapes", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.7.1\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Tesla V100 each with 16 GB\r\n\r\n\r\n**Describe the current behavior**\r\nNested model (Encoder) not training/ model parameters not changing\r\n\r\n**Describe the expected behavior**\r\nNested (Encoder) model should be trained\r\n\r\n\r\n\r\nI had created a model for an encoder that is used in both the generator as well as the discriminator. While the same encoder model shows in total params for both the generator and discriminator, it doesn't train at all. When the same encoder model is later called on random inputs it will always give the same result of 0.\r\n\r\n    \r\n    tf.keras.backend.clear_session()\r\n\r\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.02, stddev=0.02) \r\n    def ENCODER(shape=(int(456/2), int(456/2), 3)):\r\n        y0 = tf.keras.Input(shape=shape,name = 'encoder')\r\n        y1 = tf.keras.applications.MobileNetV3Small(include_top=False,weights=None,input_shape=shape)(y0)\r\n        y1 = tf.keras.layers.GlobalAveragePooling2D()(y1)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(y1)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        encoding = tf.keras.layers.Dense(32)(x)\r\n        encoding = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1),name=\"L2_normalized_encodings\")(encoding)\r\n        model = tf.keras.Model(y0, encoding)\r\n        return model\r\n    \r\n    def EMBEDDER(shape = (1,),classes = NUM_CLASSES):\r\n        input_ = tf.keras.Input(shape=shape,name = 'embedder')\r\n        embedings_raw = tf.keras.layers.Embedding(NUM_CLASSES,32)(input_)\r\n        embedings = tf.keras.layers.Lambda(lambda x:x[:,0])(embedings_raw)\r\n        embedings = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1),name=\"L2_normalized_embeddings\")(embedings)\r\n        return tf.keras.Model(input_,embedings)\r\n    \r\n    def DISCRIMANTOR(encoder):\r\n        generation = tf.keras.Input(shape=(int(456/2), int(456/2), 3), name='input_1_disc')\r\n        embedings = tf.keras.Input(shape=(32,), name='input_2_disc')\r\n        delta_layer = encoder(generation,training=True)\r\n        ##SIGMOID OUTPUT MODULE\r\n        modified_delta = tf.keras.layers.Add()([delta_layer,embedings])\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(modified_delta)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        x = tf.keras.layers.Dense(16)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        x = tf.keras.layers.Dense(8)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        output_layer = tf.keras.layers.Dense(1,activation = 'sigmoid')(x)\r\n        return tf.keras.Model(inputs =[generation,embedings],outputs = [output_layer])\r\n\r\n    def GENERATOR():\r\n        encodings = tf.keras.Input(shape=(32,), name='input_1_gen')\r\n        embedings = tf.keras.Input(shape=(32,), name='input_2_gen')\r\n        composed_layer = tf.keras.layers.Concatenate(axis=-1)([encodings,embedings])\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(composed_layer)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        x = tf.keras.layers.Dense(256)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        encodings_for_label = tf.keras.layers.Dense(128*2*2)(x)\r\n        image_format = tf.keras.layers.Reshape((2,2, 128), name='de_reshape')(encodings_for_label)\r\n        first_image = tf.keras.layers.Conv2DTranspose(filters = 256,kernel_size=(2, 2) ,strides = 2)(image_format)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(first_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        second_image = tf.keras.layers.Conv2DTranspose(filters = 256,kernel_size=(3, 3),strides = 2)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(second_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        third_image = tf.keras.layers.Conv2DTranspose(filters = 128,kernel_size=(3, 3) ,strides = 2)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(third_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        fourth_image = tf.keras.layers.Conv2DTranspose(filters = 128,kernel_size=(1, 1))(tf.keras.layers.Add()([x,tf.keras.layers.Conv2DTranspose(filters = 128,kernel_size=(7, 7),strides = 4)(first_image)]))\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(fourth_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        second_image = tf.keras.layers.Conv2DTranspose(filters = 64,kernel_size=(2, 2),strides = 3)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(second_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        third_image = tf.keras.layers.Conv2DTranspose(filters = 64,kernel_size=(2, 2) ,strides = 2)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(third_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        \r\n        fourth_image =  tf.keras.layers.Conv2DTranspose(filters = 32,kernel_size=(1, 1))(tf.keras.layers.Add()([x,tf.keras.layers.Conv2DTranspose(filters = 64,kernel_size=(6, 6),strides = 6)(fourth_image)]))\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(fourth_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        second_image = tf.keras.layers.Conv2DTranspose(filters = 16,kernel_size=(1, 1),strides = 2)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(second_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        third_image = tf.keras.layers.Conv2DTranspose(filters = 8,kernel_size=(1, 1))(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(third_image)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        fourth_image = tf.keras.layers.Conv2DTranspose(filters = 3,kernel_size = 1,name=\"fourth_output\")(x)\r\n        return tf.keras.Model(inputs=[encodings,embedings],outputs = [fourth_image]) def DISCRIMANTOR(encoder):\r\n        generation = tf.keras.Input(shape=(int(456/2), int(456/2), 3), name='input_1_disc')\r\n        embedings = tf.keras.Input(shape=(32,), name='input_2_disc')\r\n        delta_layer = encoder(generation,training=True)\r\n        ##SIGMOID OUTPUT MODULE\r\n        modified_delta = tf.keras.layers.Add()([delta_layer,embedings])\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(modified_delta)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        x = tf.keras.layers.Dense(16)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        x = tf.keras.layers.Dense(8)(x)\r\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\r\n        x = tf.keras.layers.Dropout(0.4)(x)\r\n        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\r\n        output_layer = tf.keras.layers.Dense(1,activation = 'sigmoid')(x)\r\n        return tf.keras.Model(inputs =[generation,embedings],outputs = [output_layer])\r\n```\r\n\r\n\r\nfor the training, I use the following loop:- \r\n\r\nwith stratergy.scope()\r\n    input_image =tf.keras.Input(shape=(228,228, 3))\r\n    input_label =tf.keras.Input(shape=(1,))\r\n    ENC = ENCODER()\r\n    encoding = ENC(input_image)\r\n    embeding = EMBEDDER()(input_label)\r\n    image_new = GENERATOR()([encoding,embeding])\r\n    reality_check = DISCRIMANTOR(ENC)([input_image,embeding])\r\n    discriminator = tf.keras.Model(inputs = [input_image,input_label],outputs = [reality_check])   \r\n    gen_trainer = tf.keras.Model(inputs = [input_image,input_label],outputs = [encoding,image_new])\r\n\r\ndef train_complete():\r\n    def train_step(inputs):\r\n        image, labels = inputs\r\n        #Precitions for generator:- [image,encoding_original,encoding_new]\r\n        #Precitions for discriminator:- [softmax output,embedding+centre]\r\n        #DISCRIMINATOR TRAINING\r\n        discrim_labels = tf.concat([tf.ones_like(labels),tf.zeros_like(labels)],axis = 0)\r\n        gen_discim = tf.concat([tf.zeros_like(labels),tf.ones_like(labels)],axis = 0)\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            image_,encoding_original,encoding_new = gen_trainer(inputs,training=True)\r\n            image_loss = GEN_IMAGE_LOSS(image,image_)\r\n            encoding_loss = ENCODER_LOSS(encoding_original,encoding_new)\r\n            \r\n            binary_preds = discriminator([tf.concat([image/1.,image_],axis = 0),tf.concat([labels,labels],axis = 0)],training=True)\r\n            binary_loss = DISC_SOFTMAX_LOSS(discrim_labels,binary_preds)\r\n            loss_final_gen = encoding_loss + 10*image_loss + DISC_SOFTMAX_LOSS(gen_discim,binary_preds)\r\n            loss_final_disc = binary_loss + centre_trip_loss\r\n            \r\n        gradients = tape.gradient(loss_final_disc, discriminator.trainable_variables)\r\n        \r\n        var_list_disc = discriminator.trainable_variables\r\n        \r\n        #CLIP BY VALUE\r\n        sam_gradients = [tf.clip_by_value(grad, -1., 1.) for grad in gradients]\r\n        #APPLY TO GRADIENTS\r\n        discriminator.optimizer.apply_gradients(zip(sam_gradients, discriminator.trainable_variables),experimental_aggregate_gradients=False)\r\n        \r\n        \r\n        ##GENERATOR TRAINING\r\n          \r\n        gradients = tape.gradient(loss_final_gen, gen_trainer.trainable_variables)\r\n        \r\n        var_list_gen = gen_trainer.trainable_variables\r\n        \r\n        #CLIP BY VALUE\r\n        sam_gradients = [tf.clip_by_value(grad, -1., 1.) for grad in gradients]\r\n        #APPLY TO GRADIENTS\r\n        gen_trainer.optimizer.apply_gradients(zip(sam_gradients, gen_trainer.trainable_variables),experimental_aggregate_gradients=False)\r\n        del tape\r\n        return binary_loss,image_loss,encoding_loss\r\n    @tf.function\r\n    def distributed_train_step(inputs):\r\n        loss1,loss2,loss3= strategy.run(train_step, args=(inputs,))\r\n        #train_metric.update_state(strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None))\r\n        loss1 = strategy.reduce(tf.distribute.ReduceOp.SUM, loss1,axis=None)\r\n        loss2 = strategy.reduce(tf.distribute.ReduceOp.SUM, loss2,axis=None)\r\n        loss3 = strategy.reduce(tf.distribute.ReduceOp.SUM, loss3,axis=None)\r\n        return loss1,loss2,loss3\r\n    return distributed_train_step\r\ntrain_function = train_complete()\r\nfor _ in range(80):\r\n    total_loss = np.array([0.0,0.0,0.0])\r\n    num_batches = 0\r\n    for x in train_dataset:\r\n          total_loss += train_function(x)\r\n          num_batches += 1\r\n          train_loss = total_loss / num_batches\r\n```\r\n\r\nThe loss functions here are as follows:-\r\n\r\n```\r\n    @tf.function\r\n    def DISC_EMBED_LOSS(labels, predictions):\r\n        per_example_loss = TripletSemiHardLoss(distance_metric = pairwise_distance,reduction=tf.keras.losses.Reduction.NONE)(labels, predictions)\r\n        return per_example_loss/ strategy.num_replicas_in_sync\r\n    @tf.function\r\n    def ENCODER_LOSS(labels,predictions):\r\n        per_example_loss = tf.nn.compute_average_loss(1.+tf.keras.losses.CosineSimilarity(axis=-1,reduction=tf.keras.losses.Reduction.NONE)(labels,predictions), global_batch_size=GLOBAL_BATCH_SIZE)\r\n        return per_example_loss\r\n    @tf.function\r\n    def DISC_SOFTMAX_LOSS(labels, predictions):\r\n        per_example_loss =  tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE,from_logits=True)(labels, predictions)\r\n        return per_example_loss/ strategy.num_replicas_in_sync\r\n    @tf.function\r\n    def GEN_IMAGE_LOSS(labels, predictions):\r\n        per_example_loss = tf.math.reduce_mean(tf.math.abs(labels/255. - predictions/255.))\r\n        return per_example_loss/ strategy.num_replicas_in_sync\r\n```\r\n\r\nWith all this being as it is when I call the encoder on any other value the prediction is always 0 irrespective of input or number of epochs. Which I believe is due to it not getting trained. What could be the issue? Does the error lie with gradient tapes, because when the encoder was trained by itself with train_on_batch, the encoder's weights were updated. Also, note there isn't any graph disconnection error in the generator and discriminator. \r\n\r\n", "comments": ["Hi @Orpheus23, a few questions:\r\n\r\n- what strategy are you using? (I'm assuming `MirroredStrategy`?)\r\n- Are you distributing your data with [experimental_distribute_dataset](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset)?\r\n- have you verified that things are working as expected in the non distributed case?", "Hello, thanks for the reply,\r\n\r\n- I am using MirroredStrategy.\r\n- Yes the data is distributed with experimental_distribute_dataset.\r\n- Things aren't working as expected even in the non-distributed case, the same thing is happening there too.", "Ah okay, in that case I think it makes sense to try and debug your model first without a distribution strategy and see if you can get it to train. In general, you want to make sure your model trains as expected in the non distributed case before modifying the code for MirroredStrategy.", "Oh okay well, my original issue was the model, not training. I solved it by creating my model within the scope rather in one shot rather than by creating it via the functions. Anyhow, thanks for the help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48361\">No</a>\n"]}, {"number": 48360, "title": "Operator validation result reporting for NNAPI", "body": "NNAPI delegate reports the validation failures for operations in the\r\ncomputation graph. However this information is not visible to the user.\r\n\r\nThe patch causes NNAPI delegate to print messages for any refused operator appearing in the computational graph, if compiler with NNAPI_VERBOSE_VALIDATION. Also the patch adds the option to activate the NNAPI_VERBOSE_VALIDATION into CMake. ", "comments": ["@miaowang14 @terryheo could you review this PR?", "@robert-kalmar  Can you please check @terryheo's comments and keep us posted ? Thanks!", "@gbaned, updated the PR based on @terryheo comment. "]}, {"number": 48359, "title": "3D NMS", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):tensorflow2.3.0\r\n- Are you willing to contribute it (Yes/No):No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDoes tensorflow currently provide NMS for 3D boxes?\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\nCan you add an algorithm to non_max_suppression of the 3D frame", "comments": ["@Luoxiaoyu828 \r\nCan you please refer to similar issue here and let us know if it helps.\r\n#48070", "Check NMS in our Tensorflow 3d repository https://github.com/google-research/google-research/blob/master/tf3d/object_detection/box_utils/box_ops.py", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48358, "title": "predict_on_batch score from tensorflow 1.15 to tensorflow 2.3", "body": "Dear all,\r\n\r\nI'm facing a strange behavior in keras-retinanet 1.0. I have recently updated retinanet from version 0.5 to version 1.0. I have two classes, one with a validation mAP of 0.87 and the other with a validation mAP of 0.3 both in version 0.5 and 1.0. I have trained retinanet 0.5 on the training set, and, after doing that, I have converted the trained model to an inference model. I have used predict_on_batch (as in examples) to get the predictions on the test set and I have saved them in a csv file. The predictions obtained with version 0.5 contain both classes with a score that goes from 0.2 to 1, as expected. But the predictions obtained using version 1.0 provide the predictions of one class (the one with 0.87 mAP) with a score between 0 and 1, while for the prediction of the other class the score is always below 0.5. I set up the score threshold to 0.5 during training in both cases (version 0.5 and 1.0). I don't understand why in version 1.0 the model seems not to find any object of the second class with a score greater than 0.5. The first version is in Tensorflow 1.15 with Keras 2.2.4 while the second is in Tensorflow 2.3 with Keras 2.4. Can someone help me? Does he function predict_on_batch change from tensorflow 1.15 to Tensorflow 2.3?\r\n\r\nBest regards,\r\nmdatres", "comments": ["@mdatres,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n", "Unfortunately, I can't share the dataset since it is private. I can share to you the link of the model's implementation: `https://github.com/fizyr/keras-retinanet `\r\nI have used the code contained in the Jupiter notebook example/ResNet50RetinaNet.ipynb to obtain the predictions (I have ran predict_on_batch on the test_set using a for loop). Let me know if I can share something else with you. \r\n\r\nBest regards, \r\nmdatres ", "In the following .txt file, the code that I used for the prediction is reported. \r\n[code.txt](https://github.com/tensorflow/tensorflow/files/6276600/code.txt) \r\n\r\nHope this help. \r\nBest regards, \r\n\r\nmdatres\r\n", "@mdatres,\r\nOn running the code, I am facing an error stating `ModuleNotFoundError: No module named 'read_test_data'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5e81b077e0203cbbbdc7527b244e5120/48358.ipynb). \r\n\r\nWithout a reproducible code snippet it would be difficult for us to debug the issue.\r\n\r\n\r\n\r\n> Unfortunately, I can't share the dataset since it is private\r\n\r\nIn this case, you can provide a dummy dataset and code to reproduce the issue reported here. Alternatively, you can also run the code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48358\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48358\">No</a>\n"]}]