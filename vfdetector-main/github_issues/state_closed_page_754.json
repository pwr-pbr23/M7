[{"number": 30941, "title": "Update 1.14 behavioral changes to mention...", "body": "tf.keras.optimizers.Adadelta's default learning rate change", "comments": ["Why do we have the change against master?\r\nI thought these were for the patch release?", "> Thanks for the PR, meanwhile would you mind adding the following as well:\r\n> \r\n> 1. Adagrad changed from 0.01 to 0.001\r\n> 2. Adamax changed from 0.002 to 0.001\r\n> 3. NAdam changed from 0.002 to 0.001\r\n> 4. Adadelta changed from 1.000 to 0.001\r\n> \r\n> Thanks!\r\n\r\nAdded - thanks!", "> Why do we have the change against master?\r\n> I thought these were for the patch release?\r\n\r\nThese were missed in the 1.14 relnotes, not changes in the patch release."]}, {"number": 30940, "title": "Update CONTRIBUTING.md", "body": "", "comments": []}, {"number": 30939, "title": "added min_epochs to EarlyStopping", "body": "Per the discussion\u00a0[here](https://github.com/tensorflow/tensorflow/issues/30767).\r\n\r\n\r\nTo rephrase: \r\nAssume one defines of an epoch to just mean how often the model is evaluated on the validation data.\r\n\r\nSay it takes 10 epochs to make it through the entire dataset. After making 2 passes through the training data, i.e. after epoch 20, the user would like to enable EarlyStopping after\u00a0patience\u00a0epochs if performance does not improve. This feature would enable that use case, where the user would set\u00a0min_epochs=20\u00a0when creating the callback.\r\n\r\n\r\n- added new input parameter with a default\r\n- added if statement in `on_epoch_end()`", "comments": ["@rchao Just wondering if you have any thoughts on this.", "Can one of the admins verify this patch?", "Just wondering if anyone has any thoughts on this Pull Request", "Thanks for the comment. I've added a test. As the commit says,\r\n\r\n> This test is the same as `test_EarlyStopping_reuse` but includes `min_epochs`, such that the number of epochs the model was trained for is gt or eq to the `patience+min_epochs`\r\n", "@rthadur , @rchao Any further thoughts on this?"]}, {"number": 30938, "title": "shape_a should be an arange not just a list ", "body": "`\r\n#I think it should be \r\nshape_a = np.arange(a.get_shape[0])\r\n#instead of \r\nshape_a = a.get_shape().as_list()\r\n`\r\n<em>Please make sure that this is a bug. As per our [GitHub ### ### Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github](https://github.com/tensorflow/tensorflow/issues/new/choose) new issue template.\r\n\r\nAlso, can you elaborate more on your issue with code.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 30937, "title": "Add `min_epochs` flag to EarlyStopping - trouble with author email", "body": "Per the discussion [here](https://github.com/tensorflow/tensorflow/issues/30767)\r\n\r\nTo rephrase:\r\nAssume one defines of an epoch to just mean how often the model is evaluated on the validation data. \r\n\r\nSay it takes 10 epochs to make it through the entire dataset. After making 2 passes through the training data, i.e. after epoch 20, the user would like to enable EarlyStopping  after `patience` epochs if performance does not improve. This feature would enable that use case, where the user would set `min_epochs=20` when creating the callback.\r\n\r\n\r\nThe following changes have been made:\r\n- the comments have been updated\r\n- new argument with a default has been added, and is stored as a variable in the callback\r\n- this is checked against in `on_epoch_end()`, and if the check passes, EarlyStopping measurements proceed as have been previously implemented.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30937) for more info**.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30937) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 30936, "title": "[XLA:GPU][ROCm] Refactor IR emitter to cope with both NVPTX and AMDGPU for workgroup dims.", "body": "- Remove `EmitDeviceFunctionCall` from a member function in `GpuElementalIrEmitter` and put inside `target_util` module.\r\n- Revise `TargetIntrinsics` in `target_util` module so it could contain either an LLVM intrinsic, or a custom function. This is to cope with the fact that certain operations on AMDGPU target aren't mapped to LLVM intrinsics but rather than device library functions.\r\n- Remove explicit dependency to NVVM intrinsics in IR emitters.", "comments": ["The sole failure in `Linux GPU` target is not related to changes proposed in this PR.", "@thomasjoerg a gentle ping. may i understand the next steps for this PR? failures on CI don't seem to be relevant to this PR."]}, {"number": 30935, "title": "Wrong URL for Cuda download in dll not found error message", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windos 10 64Bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 2.0.0-beta1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: 1050\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen Tensorflow 2.0.0-beta1 is installed with pip, the error message stating that dll for Cuda is not found gives a wrong URL to [Cuda 9.0](https://developer.nvidia.com/cuda-90-download-archive) instead of [10.0, which is required by TF 2.0](https://developer.nvidia.com/cuda-10.0-download-archive).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- `pip install tensorflow-gpu==2.0.0-beta1`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n> >>> import tensorflow as tf\r\n> Traceback (most recent call last):\r\n>   File \"C:\\ProgramData\\Anaconda3\\envs\\py368\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n>     ctypes.WinDLL(build_info.cudart_dll_name)\r\n>   File \"C:\\ProgramData\\Anaconda3\\envs\\py368\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n>     self._handle = _dlopen(self._name, mode)\r\n> OSError: [WinError 126] The specified module could not be found\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"C:\\ProgramData\\Anaconda3\\envs\\py368\\lib\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n>   File \"C:\\ProgramData\\Anaconda3\\envs\\py368\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\ProgramData\\Anaconda3\\envs\\py368\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n>     self_check.preload_check()\r\n>   File \"C:\\ProgramData\\Anaconda3\\envs\\py368\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n>     % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\n> ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow ](https://www.tensorflow.org/install/gpu)website .Please, let us know. Thanks!", "@ravikyram I've already installed it correctly, just reporting a mistyped error message in source code.", "I have the same problem!"]}, {"number": 30934, "title": "[BUG] [TF 2.0 Keras] Pointwise Conv2D numerically inconsistent in keras model vs manual run", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Docker container**: 2.0.0b1-gpu-py3-jupyter\r\n- **CUDA/cuDNN version**: CUDA 10.2\r\n- **GPU model and memory**: GeForce GTX 1080\r\n- **Exact command to reproduce**: See script below.\r\n\r\n### Describe the problem\r\nWrapping an identical tf.nn.conv2d operation **that has ksize of (1, 1)** in a tf.keras.Model and calling the model, with or without predict, on identical data, produces different results.\r\n\r\nThe differences are small but accumulate through a deep network if many pointwise convs are used.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(123)\r\npool = np.ones([32, 64, 64, 64], 'float32')\r\nw1 = np.random.randn(1, 1, 64, 64).astype('float32')\r\n\r\n# Manual convolution\r\nconv1 = tf.nn.conv2d(pool, w1, [1, 1], padding='SAME').numpy()\r\n\r\n# The same convolution op via keras model\r\ntmp_input = tf.keras.Input(shape=[64, 64, 64], dtype='float32')\r\ntmp_out = tf.nn.conv2d(tmp_input, w1, [1, 1], padding='SAME)\r\n\r\n# .predict can also be removed\r\nconv2 = tf.keras.Model(inputs=tmp_input, outputs=tmp_out).predict(pool)\r\n\r\n# The individual error is small, but it compounds through a deep network.\r\nprint('Disagreement between manual and keras-model wrapped conv:', np.abs(conv1 - conv2).sum())\r\n```\r\n\r\nThis script can be run in a fresh pull of the docker container.", "comments": ["I tried executing the code on Colab and I could able to reproduce the issue with TF 2.0.0.beta1. Please see the gist of [Colab](https://colab.research.google.com/drive/1fJaDnYtOjFjNklmmecfIXUIp_vKOO8aj).Thanks! ", "@Kaynato I think this was resolved in `tf-nightly-2.0-preview==2.0.0.dev20190718`. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/39386ed6c56b2c16b6466cb41bb29228/tf_30934.ipynb). I don't see any issues with tf-nightly-2.0. Thanks!", "The issue is in fact not present on the CPU-only version, but still present in the latest GPU build.\r\n\r\nI updated to tf-nightly-gpu-2.0-preview 2.0.0.dev20190724 and the issue is still present.\r\n\r\nI care mostly about this issue in the **gpu version** of tensorflow - in fact I have a feeling that the discrepancy has to do with how data is passed to and from the GPU.", "@Kaynato Can you check my [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/af4efafb809c4558189b31a9463aa5a6/tf_30934.ipynb) and let me know what you think? When I ran the gist it results in `Disagreement between manual and keras-model wrapped conv: 0.0` which means it working as expected right?\r\n\r\nCould you create a gist and share it with the results. Thanks!", "This gist works, but I can't replicate the fix on my local machine even with the same version of tensorflow. Maybe it has to do with the drivers, but it's unclear.\r\n\r\nHere is the script, with a print at the beginning to verify that the version is in fact tf.nightly-gpu-2.0-preview==2.0.0.dev20190718\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('Tensorflow Version:', tf.__version__)\r\n\r\nnp.random.seed(123)\r\npool = np.ones([32, 64, 64, 64], 'float32')\r\nw1 = np.random.randn(1, 1, 64, 64).astype('float32')\r\n\r\n# Manual convolution\r\nconv1 = tf.nn.conv2d(pool, w1, [1, 1], padding='SAME').numpy()\r\n\r\n# The same convolution op via keras model\r\ntmp_input = tf.keras.Input(shape=[64, 64, 64], dtype='float32')\r\ntmp_out = tf.nn.conv2d(tmp_input, w1, [1, 1], padding='SAME)\r\n\r\n# .predict can also be removed\r\nconv2 = tf.keras.Model(inputs=tmp_input, outputs=tmp_out).predict(pool)\r\n\r\n# The individual error is small, but it compounds through a deep network.\r\nprint('Disagreement between manual and keras-model wrapped conv:', np.abs(conv1 - conv2).sum())\r\n```\r\n\r\nHere is the output log I receive:\r\n\r\n```\r\n2019-07-25 14:59:44.966783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-07-25 14:59:45.341260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:87:00.0\r\n2019-07-25 14:59:45.343798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:88:00.0\r\n2019-07-25 14:59:45.344169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-25 14:59:45.346193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-25 14:59:45.347898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-25 14:59:45.348303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-25 14:59:45.350655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-25 14:59:45.352361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-25 14:59:45.357594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-25 14:59:45.364630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-07-25 14:59:45.365093: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-25 14:59:45.826040: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ccd7c0 executing computations on platform CUDA. Devices:\r\n2019-07-25 14:59:45.826109: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2019-07-25 14:59:45.826125: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX 1080, Compute Capability 6.1\r\n2019-07-25 14:59:45.851060: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200005000 Hz\r\n2019-07-25 14:59:45.855590: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5d44940 executing computations on platform Host. Devices:\r\n2019-07-25 14:59:45.855631: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-25 14:59:46.043201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:87:00.0\r\n2019-07-25 14:59:46.043856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:88:00.0\r\n2019-07-25 14:59:46.043903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-25 14:59:46.043919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-25 14:59:46.043932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-25 14:59:46.043945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-25 14:59:46.043957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-25 14:59:46.043970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-25 14:59:46.043984: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-25 14:59:46.046411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-07-25 14:59:46.046452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-25 14:59:46.048207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-25 14:59:46.048223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2019-07-25 14:59:46.048231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y \r\n2019-07-25 14:59:46.048237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N \r\n2019-07-25 14:59:46.050196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7606 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:87:00.0, compute capability: 6.1)\r\n2019-07-25 14:59:46.051497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7606 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:88:00.0, compute capability: 6.1)\r\n2019-07-25 14:59:49.843891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-25 14:59:50.415257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nTensorflow Version: 2.0.0-dev20190724\r\nDisagreement between manual and keras-model wrapped conv: 5.8203125\r\n```", "@Kaynato Thanks for your patience. Can you try with `!pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190724` which is what I used in my recent gist. Thanks!", "Hello, thank you for your help. Though the gist works correctly in a colab instance, **the gist is not reproducible on the local machine despite both being clean installations and fresh container pulls.**\r\n\r\nEither the issue is likely a **driver / hardware bug** and so at least should be made known (that perhaps some version of CUDA or CUDNN causes issues here, or that there is something else going on.\r\n\r\nIn either case, it is unexpected behavior.\r\n\r\nTo demonstrate that I am using the correct version of tensorflow, I recorded the procedure on the local machine and demonstrate that the inconsistency is still present.\r\n\r\n[screencapture.zip](https://github.com/tensorflow/tensorflow/files/3437068/screencapture.zip)\r\n\r\nThe device information is the same as before and described in the more verbose log in my previous comment here.", "@Kaynato I don't know what else we can do to find root-cause of this issue as we cannot access your local machine. But surely we can say this is not related to Tensorflow. May be downgrading to CUDA10.0 and test again might help i think. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30934\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30934\">No</a>\n"]}, {"number": 30933, "title": "Fix large (>4GB) files reading on windows", "body": "There is a bug which prevents reading files larger than 4GB on Windows.\r\nTF uses ::ReadFile winapi function (see pread in windows_file_system.cc)\r\nThis function accepts requested bytes number as DWORD, which is 32 bit on both 32bit\r\nand 64bit systems. But WindowsRandomAccessFile::Read passes number of\r\nbytes as size_t which is 64 bit on 64 bit systems. Then there is a\r\nstatic_cast from 64 bit size_t to 32 bit DWORD, which causes the error.\r\nChanged to read such files in portions of no more than\r\nstd::numeric_limits\\<DWORD\\>::max() bytes.", "comments": []}, {"number": 30932, "title": "[BUG] tf.saved_model.load cannot load Keras compiled model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Elementary OS Loki\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190601\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ntf.saved_model.load cannot load compiled keras model, saved as SavedModel in graph mode.\r\n**Describe the expected behavior**\r\nNo Error Expected\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python3\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential()\r\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\r\ntf.saved_model.save(model, \"/tmp/model\")\r\n```\r\nNow loading the model:\r\n```python3\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\ntf.saved_model.load(\"/tmp/model\")\r\n```\r\nThis raises the following error:\r\n`ValueError: '_RestoredOptimizer' is not a valid scope name`\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py\", line 407, in load\r\n    export_dir)\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py\", line 58, in __init__\r\n    self._load_all()\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py\", line 157, in _load_all\r\n    node, setter = self._recreate(proto)\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py\", line 240, in _recreate\r\n    return factory[kind]()\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py\", line 227, in <lambda>\r\n    \"user_object\": lambda: self._recreate_user_object(proto.user_object),\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py\", line 244, in _recreate_user_object\r\n    looked_up = revived_types.deserialize(proto)\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/revived_types.py\", line 166, in deserialize\r\n    return (type_registration.from_proto(proto), type_registration.setter)\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/revived_types.py\", line 87, in from_proto\r\n    return self._object_factory(proto)\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 1039, in <lambda>\r\n    object_factory=lambda proto: _RestoredOptimizer(),\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 1025, in __init__\r\n    super(_RestoredOptimizer, self).__init__(\"_RestoredOptimizer\")\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 263, in __init__\r\n    with backend.name_scope(self._name) as name_scope:\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 6523, in __enter__\r\n    return self._name_scope.__enter__()\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 4320, in name_scope\r\n    raise ValueError(\"'%s' is not a valid scope name\" % name)\r\nValueError: '_RestoredOptimizer' is not a valid scope name\r\n```\r\nCC:\r\n@vbardiovskyg \r\n@srjoglekar246 ", "comments": ["I have tried on colab with TF version tf-nightly-2.0-preview==2.0.0.dev20190601 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1853YfSmRK0AtNnrK5uiu4poZVmGPt7JL) here.Thanks!\r\n\r\n", "@captain-pool I cannot reproduce the issue when I use `!pip install tf-nightly-2.0-preview==2.0.0.dev20190718`. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/cd9804dbb3f558688dbc8c339f1e269f/untitled59.ipynb). This issue might have been resolved after `2.0.0.dev20190601` version. Thanks!", "> @captain-pool I cannot reproduce the issue when I use `!pip install tf-nightly-2.0-preview==2.0.0.dev20190718`. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/cd9804dbb3f558688dbc8c339f1e269f/untitled59.ipynb). This issue might have been resolved after `2.0.0.dev20190601` version. Thanks!\r\n\r\nI tried on 2.0.0-dev20190723, Looks like it's Solved. Thanks for the heads up :)\r\nClosing the issue.\r\nCC:\r\n@vbardiovskyg \r\n@srjoglekar246 ", "Hi,\r\nI am having the same issue with TF 1.14.0\r\nIs there any other solution besides upgrading TensorFlow to 2.0?\r\nUpdate: When I tried to save with tf.saved_model.simple_save(), it worked fine.\r\n@jvishnuvardhan ", "Hey @ktobah, don't compile the model before saving. That should help you fix this.", "Hey @captain-pool,\r\nThank you for your response. Actually, it is a pre-trained model. All I did was loading it again using \r\n\r\n> ft.keras.models.load_model(model_path)\r\n\r\nI did not compile it or anything.", "Hey @ktobah, just to confirm are you trying to load .h5 file, or is it a SavedModel file?", "I'm having the same issue. ", "Hi @captain-pool,\r\nWhat I do is the following:\r\n1. I load a keras h5 model with ft.keras.models.load_model(model_path)\r\n2. Don't compile or anything, just save it as SavedModel with tf.saved_model.save(model, path) \r\nEverything works fine here. Then,\r\n3. Load the SavedModel with tf.saved_model.load(path). I get the error:\r\n> '_RestoredOptimizer' is not a valid scope name\r\n\r\nWhen I tried to save with tf.saved_model.simple_save(), it worked fine and I could load the SavedModel just fine.\r\nI am using tf 1.14.0"]}, {"number": 30931, "title": "[INTEL MKL] Build failure fix for MKL-DNN v1.x", "body": "Since we have already included all the `*.h` and `*.cpp` files under `src/cpu` via the `*` wildcard, we no longer need to explicitly include files that are under `src/cpu/jit_utils` for `v1.x`.", "comments": []}, {"number": 30930, "title": "Refactor AutoShardDatasetOp", "body": "This PR refactors and adds the tests for `AutoShardDatasetOp`.\r\n\r\ncc: @jsimsa \r\n\r\n", "comments": ["@frankchn could you please take a look?", "@frankchn Thanks for your quick review! The comments are addressed here (https://github.com/tensorflow/tensorflow/pull/30930/commits/200c062c11968feb42a27a55c0a48940f02095ea). Could you please take a look when you get a chance?", "Great, thanks for adding the tests!"]}, {"number": 30929, "title": "tf.data.experimental.prefetch_to_device(\"/gpu:0\") moves tensors back to CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: Python 3.6.8\r\n- CUDA/cuDNN version: CUDA Version: 10.0 / cuDNN 7.6.0\r\n- GPU model and memory: Tesla T4  15079MiB\r\n\r\n**Describe the current behavior**\r\n`tf.data.experimental.prefetch_to_device(\"/gpu:0\")` moves tensors back to CPU.\r\n\r\n**Describe the expected behavior**\r\n`tf.data.experimental.prefetch_to_device(\"/gpu:0\")` should leave tensors on GPU.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = np.array([[1, 2],[3, 4]])\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\ndataset = dataset.apply(tf.data.experimental.prefetch_to_device(\"/gpu:0\"))\r\n\r\n#dataset = dataset.apply(tf.data.experimental.copy_to_device(\"/gpu:0\"))\r\n#\r\n#Uncommenting the line above line will print:\r\n#\r\n#Tensor [1 2] is on device /job:localhost/replica:0/task:0/device:GPU:0\r\n#Tensor [3 4] is on device /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\nfor datum in dataset:\r\n  print(f'Tensor {datum} is on device {datum.device}')\r\n  #Prints\r\n  #Tensor [1 2] is on device /job:localhost/replica:0/task:0/device:CPU:0\r\n  #Tensor [3 4] is on device /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\n\r\nIt is convinient to reproduce it as [a notebook in Google Colaboratory](https://colab.research.google.com/drive/1LpOX04L6l8SW3krQvzFdQVvXE-LK3ehH).\r\n\r\n", "comments": ["I have tried on colab with TF version 2.0.0-beta1 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1UB-vuD2MNChCJBAsf1aWM5pIw17AAtWm) here.Thanks!", "Reproduce this issue with Tf 2.1 and Tf-nightly==2.2.0.dev20200312\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/56ac5217e9ee6d137bc43f62f9bfc965/untitled454.ipynb). Thanks!", "@OutSorcerer This is intended behaviour only. Please check the comment on similar issue [here](https://github.com/tensorflow/tensorflow/issues/28610#issuecomment-494171724). Thanks!", "It is not intended behavior and there is an outstanding PR to fix this bug: https://github.com/tensorflow/tensorflow/pull/37277", "@jsimsa Sorry, I was wrong. Thanks for the correction. ", "@OutSorcerer \r\nas there is a pr to monitor this issue [there is an outstanding PR to fix this bug: #37277], please confirm if we may move this to closed status", "@Saduf2019, thanks for the answer, it is great that fixing this is work in progress!\r\n\r\nI hope #37277 will be merged, but if it would not be merged and this issue would be closed now there would be no open issues/PRs on the topic, so I suggest to link this issue to the PR instead, so that [when it is merged the issue is closed automatically](https://github.blog/2013-05-14-closing-issues-via-pull-requests/).", "@OutSorcerer\r\nAs the Pr is merged, please confirm if we may move this issue to closed status.", "The PR has not been merged, as mentioned in this comment it was rolled back https://github.com/tensorflow/tensorflow/pull/37277#issuecomment-620202464", "This issue should be fixed by https://github.com/tensorflow/tensorflow/commit/8be4d61574f29568c8699708d88945b441bfd317", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30929\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30929\">No</a>\n"]}, {"number": 30928, "title": "BatchNormalization() does not work with autograph", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0-beta\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Nvidia 1060 6GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using the `tf.keras.layers.BatchNormalization()` function using autograph the following error occurs: `ValueError: ('Input has undefined rank:', TensorShape(None))`.\r\n\r\n**Describe the expected behavior**\r\n\r\nModel will compile and train without issue. As it does without the `@tf.function` decorator.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nHere is an example of a simple model where this error would occur. This will train without problem in eager mode, but the addition of the `@tf.function` decorator causes the error.\r\n\r\n```\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def__init__(self):\r\n        super(MyModel, self).__init__()\r\n\r\n        self.conv1 = tf.keras.layers.Conv2d(8)\r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.conv2 = tf.keras.layers.Conv2d(8)\r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n\r\n    def call(inputs):\r\n\r\n        net = self.conv1(inputs)\r\n        net = self.bn1(net)\r\n        net = self.conv2(inputs)\r\n        net = self.bn2(net)\r\n\r\n        return tf.nn.softmax(net)\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 594, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1713, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py\", line 252, in build\r\n    raise ValueError('Input has undefined rank:', input_shape)\r\nValueError: ('Input has undefined rank:', TensorShape(None))\r\n```\r\n\r\n", "comments": ["@dgriffiths3 , Please provide us the complete code to reproduce the issue. Thanks!", "I wrote a small script to recreate the issue using standard `tf.keras.layers` ops and there was no problem. I have now found out my error comes when I pass in a tensor from a custom c++ op. The tensor returned from the custom op does not contain the shape information. I'll close this as it's not a bug!"]}, {"number": 30927, "title": "Correct an typo Error", "body": "Here is a gist that show the corrected output after correcting typo. Thanks!", "comments": ["@shubhamgoyal12 thank you for your contribution, there is already a PR opened [here](https://github.com/tensorflow/tensorflow/pull/30826) for similar change, will be closing this."]}, {"number": 30926, "title": "New Updated curl to 7.65.1 ", "body": "Updated curl to latest version 7.65.1", "comments": ["@shubham769 can you please resolve conflicts and address review comments."]}, {"number": 30925, "title": "How do you feel about Shape Variables?", "body": "**System information**\r\n- TensorFlow version (you are using): 2b1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRagged/variable shapes are currently described with (None, None, 10) type syntax. This is not informative for the user or the compiler.\r\n\r\nHow do you feel about the concept of Shape Variables? Shape syntax with strings or variables like (\"batch_size\", \"n_atoms\", 10) could be matched across the graph, potentially this could help with input signatures in tf.function and compiler code. \r\n\r\nthis would be nice for reinforcement learning projects, and shape variables may be extremely useful in multi-task models and architecture search projects. Also this could improve explainability and interpretability of code\r\n\r\n**Will this change the current api? How?**\r\nYes, users can specify variable \"ragged\" dimensions with strings or variables instead of **None**\r\n**Who will benefit with this feature?**\r\nEveryone who works on biochem, NLP, RL, multitask learning, and neural architecture search\r\n**Any Other info.**\r\nJust curious because we built a recursive NAS system for tf.keras in 2.0b1 this weekend and want to apply it to a bunch of different tasks, but they all have different shapes ... sometimes the shapes are variable and we need output shapes to match input shapes. \"shape hell\" sucks", "comments": ["Closest thing I can think of to this is Strongly Typed Genetic Programming\r\n\r\n![image](https://user-images.githubusercontent.com/24532336/61715882-07a60d80-ad2c-11e9-9508-993e99a622a0.png)\r\n\r\n", "@edloper @ymodak \r\n\r\nif we want to map from (None, 10) -> code_shape -> (None, 3), how do we know the size of the output?\r\n\r\ni guess we just do (-1, 3) ?\r\n\r\nbut then we need to make sure we have flattened size of 3N ...but we don't know N\r\n\r\nif instead we have **`(n_atoms, d_features) ---> (*code_shape) ---> (n_atoms, d_out)`** then we have a way to calculate the size we need to decode the output\r\n\r\ndoes this make sense?", "It's a cool idea but a very large project. Many people have tried (see the contrib labeled_tensor for an attempt https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/labeled_tensor ) and none of this attempts was really functional.\r\n\r\nSpecifically, it's hard to preserve semantic names through operations which mix dimensions like convolutions and padding.", "@alextp @ymodak Yeah, it is a big project, but my concern is, None causes serious issues in the code, reads weirdly, and adds a bunch of boilerplate, like we have to write out all these Nones or it's gonna bug out. \r\n\r\nNone means \"nothing\" -- but in Keras, None means \"Anything\" like a wildcard. \r\nTF/Keras \"None\" means the **_exact opposite_** of English \"None\" -- \r\nZero means Nothing and Nothing == None, but 0 != None?\r\n\r\nIf we need recurrent or recursive nets to produce a certain shape, there's no great way for the modules to figure out the shape for \"None\". The only way to really handle to \"push\" the shape onto the module (map over a placeholder) or track the shape variables in a lookup table, which is doable, but just adds more code.. \r\n\r\nEven the Input layer, automatically mutates shape to add None at the start. This shifts all the indices, and doesn't tell users, readers, or compiler anything. It's adding something, but the something is nothing, and nothing means anything? \r\n\r\nIt doesn't need to be fancy. Plain python \"int\" type would be a lot more useful for \r\neverything with sequential data or variable shapes\r\n\r\n```\r\nbatch_size = 4\r\nn_atoms = None\r\ninput = K.Input((batch_size, n_atoms, 10))\r\n\r\nlater on...\r\noutput = tfpl.IndependentNormal((batch_size, n_atoms, 3))(x)  # just works\r\n```"]}, {"number": 30924, "title": "Removed the deprecated API from contrib module", "body": "", "comments": []}, {"number": 30923, "title": "[tflite] fix a typo in evaluation tool doc", "body": "a trivial error becasue of copy & paste?", "comments": []}, {"number": 30922, "title": "Java tensorflow got negative value when use package class insdead of primitive class to create Tensor.", "body": "**System information**\r\n- custom code : \r\n```\r\n            Tensor t = Tensor.create(new Integer[][]{{0, 1, 5, 10, 15, 20, 1, 1, 1, 1, 1, 1}});\r\n            int[][] ids = new int[1][12];\r\n            t.copyTo(ids);\r\n            System.out.println(ids[0][0]);//-223646206\r\n            System.out.println(ids[0][1]);//-223646204\r\n            System.out.println(ids[0][2]);//-223646186\r\n            System.out.println(ids[0][3]);//-223646176\r\n            System.out.println(ids[0][4]);//-223646166\r\n```\r\n- OS Platform and Distribution : macOS 10.14.4\r\n- TensorFlow version (use command below):\r\n```\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>libtensorflow</artifactId>\r\n            <version>1.13.1</version>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>proto</artifactId>\r\n            <version>1.13.1</version>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>libtensorflow_jni</artifactId>\r\n            <version>1.13.1</version>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow</artifactId>\r\n            <version>1.13.1</version>\r\n        </dependency>\r\n```\r\nThanks for help.\r\nWhen I use package class to create Tensor, then I will got negative value and the output result have a pattern as you can see. While there is no problem when I use primitive class.\r\nAnd thanks for help agin.\r\n\r\n\r\n", "comments": ["Ok, I got this info from that.\r\n```\r\n// Copy a 1-D array of Java primitive types to the tensor buffer dst.\r\n// Returns the number of bytes written to dst.\r\nsize_t write1DArray(JNIEnv* env, jarray array, TF_DataType dtype, void* dst, size_t dst_size){...}\r\n```\r\nSo, will you support package class ?", "@Dcyx Sorry for the late reply. Is this still an issue? Thanks!", "@Dcyx Sorry for the late reply. Is this still an issue? Thanks!", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!"]}, {"number": 30921, "title": "[tflite] write debug info only when not empty", "body": "this fixed https://github.com/tensorflow/tensorflow/issues/30851", "comments": ["@freedomtan Could you please resolve the conflicts? Thanks!", "@gbaned thanks, close this since a more complete patch is in master branch now"]}, {"number": 30920, "title": "Update metric_ops.py for incorrect docstring", "body": "fix the C0301(line-too-long) error in the sanity check #30848", "comments": []}, {"number": 30919, "title": "tf.keras.layers.Embedding set trainable to False does not work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.5.2\r\n\r\n\r\nThe output of the 2 print statements should be the same. Same issue for other layer like Dense, please confirm trainable parameter behavior.\r\n```\r\nembedding = np.random.rand(2,3).astype(np.float32)\r\nemb_layer = tf.keras.layers.Embedding(2, 3, weights=[embedding], trainable=False)\r\nemb = emb_layer(tf.constant([0],tf.int32))\r\nopt = tf.train.AdamOptimizer()\r\nloss = tf.nn.l2_loss(tf.reduce_mean(emb)-0)\r\ntrain_op = opt.minimize(loss)\r\nsess.run(tf.initialize_all_variables())\r\nout = np.mean(sess.run(emb_layer.variables[0])[0])\r\nsess.run(train_op)\r\nprint(out)\r\nprint(np.mean(sess.run(emb_layer.variables[0])[0]))\r\n```\r\n", "comments": ["I could able to reproduce the issue with Tensorflow 1.14 on Colab. Find the gist of Colab [here](https://colab.research.google.com/drive/1tQ1C5jnwxXotnxR-H7WGWD0QvCXnLybX). Thanks!", "I could reproduce the issue even with `tf-nightly`. Thanks!", "This is expected behavior. `trainable=False` is a keras concept; it doesn't automatically override the graph and op behavior of tf 1.x. In order to do what you want, you should use the new style `tf.keras.optimizers` optimizers (or `GradientTape`) which accept a list of variables to differentiate with respect to and the `.trainable_weights` attribute of Layers and Models which will filter based on `.trainable`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30919\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30919\">No</a>\n"]}, {"number": 30918, "title": "Keras fails to initiate training with custom BERT layer.", "body": "**System information**\r\n- Have I manipulated custom code in attempt to build a Keras layer for BERT. Following this example: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b \r\n- Windows 10:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA 10\r\n- GPU model and memory:  NVIDIA GTX1080ti\r\n\r\n**Describe the current behavior**\r\nThe training data is pre-processed and loaded into memory. The model is compiled and the correct model output is produced with model.summary(). See logs bellow...\r\n\r\nOn model.fit(), nothing happens... The GPU is at 3% utilization and one CPU core is at 100%.\r\n\r\n**Describe the expected behavior**\r\nI was expecting the keras training logging to be printed post model.fit(). It doesn't appear to be training at all.\r\n\r\n**Code to reproduce the issue**\r\n\r\n**Custom Code:**\r\n```\r\nimport tensorflow as tf\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom tensorflow.keras import backend as K\r\nimport sys\r\n\r\nfrom BertLayer import BertLayer\r\nfrom preprocessing import MyDocs\r\n\r\nsess = tf.Session()\r\n\r\ndef build_model(bert_path, max_seq_length):\r\n    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\r\n    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\r\n    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\r\n    bert_inputs = [in_id, in_mask, in_segment]\r\n\r\n    bert_output = BertLayer(n_fine_tune_layers=3, bert_path=bert_path, pooling=\"first\")(bert_inputs)\r\n    dense = tf.keras.layers.Dense(256, activation=\"relu\")(bert_output)\r\n    pred = tf.keras.layers.Dense(5, activation=\"sigmoid\")(dense)  # change this to build classifier\r\n\r\n    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\r\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  metrics=['binary_accuracy', 'categorical_accuracy'])\r\n    model.summary()\r\n\r\n    return model\r\n\r\n\r\ndef initialize_vars(sess):\r\n    sess.run(tf.compat.v1.local_variables_initializer())\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    sess.run(tf.compat.v1.tables_initializer())\r\n    K.set_session(sess)\r\n\r\ndef main():\r\n    # Params for bert model and tokenization\r\n    bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\r\n    max_seq_length = 256\r\n\r\n    corpus = MyDocs(\"datasets/bbc/raw\", bert_path, max_seq_length)\r\n\r\n    ids = []\r\n    masks = []\r\n    segment_ids = []\r\n    for id, mask, segment, label in corpus:\r\n        ids.append(id)\r\n        masks.append(masks)\r\n        segment_ids.append(segment)\r\n    X = [ids, masks, segment_ids]\r\n\r\n    labels = corpus.labels\r\n    label_encoder = OneHotEncoder()\r\n    y = label_encoder.fit_transform(np.array(labels).reshape(-1, 1)).todense()\r\n    print('Dimension of labels input is {}.'.format(y.shape))\r\n\r\n    print('Building model...')\r\n    model = build_model(bert_path, max_seq_length)\r\n\r\n    print('Training model...')\r\n    history = model.fit(X, y,\r\n                        validation_split=0.2,\r\n                        epochs=1,\r\n                        batch_size=1,\r\n                        verbose=2,\r\n                        use_multiprocessing=True)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n**BertLayer**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nimport tensorflow_hub as hub\r\n\r\nclass BertLayer(tf.keras.layers.Layer):\r\n    def __init__(\r\n        self,\r\n        n_fine_tune_layers=10,\r\n        pooling=\"mean\",\r\n        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\r\n        **kwargs,\r\n    ):\r\n        self.n_fine_tune_layers = n_fine_tune_layers\r\n        self.trainable = True\r\n        self.output_size = 768\r\n        self.pooling = pooling\r\n        self.bert_path = bert_path\r\n\r\n        if self.pooling not in [\"first\", \"mean\"]:\r\n            raise NameError(\r\n                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\r\n                )\r\n\r\n        super(BertLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.bert = hub.Module(\r\n            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\r\n        )\r\n\r\n        # Remove unused layers\r\n        trainable_vars = self.bert.variables\r\n        if self.pooling == \"first\":\r\n            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\r\n            trainable_layers = [\"pooler/dense\"]\r\n\r\n        elif self.pooling == \"mean\":\r\n            trainable_vars = [\r\n                var\r\n                for var in trainable_vars\r\n                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\r\n            ]\r\n            trainable_layers = []\r\n        else:\r\n            raise NameError(\r\n                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\r\n            )\r\n\r\n        # Select how many layers to fine tune\r\n        for i in range(self.n_fine_tune_layers):\r\n            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\r\n\r\n        # Update trainable vars to contain only the specified layers\r\n        trainable_vars = [\r\n            var\r\n            for var in trainable_vars\r\n            if any([l in var.name for l in trainable_layers])\r\n        ]\r\n\r\n        # Add to trainable weights\r\n        for var in trainable_vars:\r\n            self._trainable_weights.append(var)\r\n\r\n        for var in self.bert.variables:\r\n            if var not in self._trainable_weights:\r\n                self._non_trainable_weights.append(var)\r\n\r\n        super(BertLayer, self).build(input_shape)\r\n\r\n    def call(self, inputs):\r\n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\r\n        input_ids, input_mask, segment_ids = inputs\r\n        bert_inputs = dict(\r\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\r\n        )\r\n        if self.pooling == \"first\":\r\n            # pooled output of the entire sequence [batch, hidden_size]\r\n            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\"pooled_output\"]\r\n        elif self.pooling == \"mean\":\r\n            # representation of every token in the sequence [batch, max_seq_length, hidden_size]\r\n            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\"sequence_output\"]\r\n\r\n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\r\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\r\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\r\n            input_mask = tf.cast(input_mask, tf.float32)\r\n            pooled = masked_reduce_mean(result, input_mask)\r\n        else:\r\n            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\r\n\r\n        return pooled\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], self.output_size)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nmodel.summary()\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_ids (InputLayer)          [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\ninput_masks (InputLayer)        [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\nsegment_ids (InputLayer)        [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\nbert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \r\n                                                                 input_masks[0][0]                \r\n                                                                 segment_ids[0][0]                \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 5)            1285        dense[0][0]                      \r\n==================================================================================================\r\nTotal params: 110,303,039\r\nTrainable params: 22,052,357\r\nNon-trainable params: 88,250,682\r\n__________________________________________________________________________________________________\r\n```\r\n\r\nOther logs:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0722 16:38:28.879759 14352 deprecation_wrapper.py:119] From C:\\Users\\jorda\\OneDrive\\Documents\\Programs\\aria_projects\\document_classifier\\BERT\\venv\\preprocessing.py:7: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-07-22 16:38:28.917117: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-07-22 16:38:29.034609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\npciBusID: 0000:08:00.0\r\n2019-07-22 16:38:29.034810: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-22 16:38:29.035881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-22 16:38:29.041192: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-07-22 16:38:29.044644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\npciBusID: 0000:08:00.0\r\n2019-07-22 16:38:29.044900: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-22 16:38:29.045354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-22 16:38:30.462499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-22 16:38:30.462613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-22 16:38:30.462745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-22 16:38:30.464681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8788 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2019-07-22 16:38:30.470977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\npciBusID: 0000:08:00.0\r\n2019-07-22 16:38:30.471127: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\nBuilding docs...\r\n2019-07-22 16:38:30.471801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-22 16:38:30.471966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-22 16:38:30.472067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-22 16:38:30.472126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-22 16:38:30.472605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8788 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\nW0722 16:38:34.138283 14352 deprecation_wrapper.py:119] From C:\\Users\\jorda\\OneDrive\\Documents\\Programs\\aria_projects\\document_classifier\\BERT\\venv\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nDimension of labels input is (2225, 5).\r\nBuilding model...\r\nW0722 16:39:05.050155 14352 deprecation.py:506] From C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0722 16:39:05.109156 14352 deprecation.py:323] From C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nModel: \"model\"\r\n```", "comments": ["How long did you wait for?\r\n\r\nIn the background (it is not printed to console) the weights for the TF Hub module will have to be downloaded (500MB for BERTBASE, 1GB for BERTLARGE), and then the BERT layer instantiated. This does take a fair amount of time for the download, and then to build the graph.\r\n\r\nAlso, note that in the medium post they made a few mistakes e.g. the way they implemented `fine_tune_layers` is completely wrong.\r\n\r\nI have a much improved version in this Colab notebook:\r\nhttps://colab.research.google.com/github/NVAITC/examples/blob/master/keras_bert_amp.ipynb\r\nYou are free to take this and use it.", "> How long did you wait for?\r\n\r\nI waited for about 48 hours...\r\n\r\n> I have a much improved version in this Colab notebook:\r\n\r\nI will have a look and implement this version and post back my findings. \r\n\r\n> You are free to take this and use it.\r\n\r\nYou rock! Thanks. \r\n\r\n", "@tlkh \r\n\r\n> In the background (it is not printed to console) the weights for the TF Hub module will have to be downloaded (500MB for BERTBASE, 1GB for BERTLARGE), and then the BERT layer instantiated. This does take a fair amount of time for the download, and then to build the graph.\r\n\r\nI have taken the BERT class from your script and adjusted my code to fit; however, I am still getting the same issue. Training fails to start. I waited 6 hours. Network i/o was not evidencing any download either. \r\n\r\n** main.py **\r\n\r\n```\r\ndef build_model(bert_path, max_seq_length, tune_cells):\r\n    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\r\n    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\r\n    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\r\n    bert_inputs = [in_id, in_mask, in_segment]\r\n\r\n    bert_output = BERT(finetune_cells=tune_cells, bert_path=bert_path)(bert_inputs)\r\n    pooled = tf.keras.layers.GlobalMaxPooling1D()(bert_output)\r\n    dense = tf.keras.layers.Dense(256, activation=\"relu\")(pooled)\r\n    pred = tf.keras.layers.Dense(5, activation=\"sigmoid\")(dense)\r\n\r\n    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\r\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  metrics=['binary_accuracy', 'categorical_accuracy'])\r\n    model.summary()\r\n\r\n    return model\r\n\r\ndef set_session(USE_XLA=True, MIXED_PRECISION=True):\r\n    \"\"\"\r\n    Set the session config to optimise GPU computations.\r\n    - Automatic Mixed Precision is available for server grade GPUs.\r\n    \"\"\"\r\n    config = tf.ConfigProto()\r\n    if USE_XLA:\r\n        opt_level = tf.OptimizerOptions.ON_1\r\n        tf.enable_resource_variables()\r\n    else:\r\n        opt_level = tf.OptimizerOptions.OFF\r\n\r\n    config.graph_options.optimizer_options.global_jit_level = opt_level\r\n\r\n    config.graph_options.rewrite_options.auto_mixed_precision = MIXED_PRECISION\r\n\r\n    sess = tf.Session(config=config)\r\n    tf.keras.backend.set_session(sess)\r\n\r\n    return tf.keras.backend.get_session()\r\n\r\ndef initialize_sess(sess):\r\n    sess.run(tf.local_variables_initializer())\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.tables_initializer())\r\n    tf.keras.backend.set_session(sess)\r\n\r\n    K.set_session(sess)\r\n\r\ndef main():\r\n    \"\"\"\r\n    0. Set Global Variables\r\n    \"\"\"\r\n    sess = set_session()\r\n    bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\r\n    max_seq_length = 256\r\n    tune_layers = 1\r\n    epochs = 2\r\n    batch_size = 64\r\n\r\n    \"\"\"\r\n    1. Build Data Set\r\n    \"\"\"\r\n    corpus = MyDocs(\"datasets/bbc/raw\", sess, bert_path, max_seq_length)\r\n\r\n    ids = []\r\n    masks = []\r\n    segment_ids = []\r\n    for id, mask, segment, label in corpus:\r\n        ids.append(id)\r\n        masks.append(masks)\r\n        segment_ids.append(segment)\r\n    X = [ids, masks, segment_ids]\r\n\r\n    labels = corpus.labels\r\n    label_encoder = OneHotEncoder()\r\n    y = label_encoder.fit_transform(np.array(labels).reshape(-1, 1)).todense()\r\n    print('Dimension of labels input is {}.'.format(y.shape))\r\n\r\n    \"\"\"\r\n    2. Build Model and Train\r\n    \"\"\"\r\n    print('Building model...')\r\n    model = build_model(bert_path, max_seq_length, tune_layers)\r\n\r\n    # Instantiate variables\r\n    initialize_sess(sess)\r\n\r\n    print('Training model...')\r\n    history = model.fit(X, y,\r\n                        validation_split=0.2,\r\n                        epochs=epochs,\r\n                        batch_size=batch_size,\r\n                        verbose=2,\r\n                        use_multiprocessing=True)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n", "@jordanparker6 Thanks for the bug! `verbose=2` will only print 1 line per epoch. Could it be that your Model is taking a huge amount of time to run through an epoch? Try `verbose=1`, which outputs on every batch. Please let me know if you still don't see any output", "@omalleyt12 No change in the output post changing the verbose level. I let it run for 4 days...\r\n", "The only thing that stand out to me is that `model.fit(..., use_multiprocessing=True)` should be `False` here, as `True` is only relevant to keras.Sequence or generator objects. Could you try that and let me know if it starts training?", "Sorry for the late reply. I will try it tonight and get back to you. ", "@omalleyt12 that didn't work either...", "@omalleyt12 is the fact that i am using a GTX1080ti with 11gb of GPU RAM an issue?", "@jordanparker6 \r\nIs this still an issue", "@Saduf2019 no many other libraries have provided a work around. See huggingfaces transformer library or fastbert.", "@jordanparker6 \r\nIN that case please confirm if we may move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30918\">No</a>\n"]}, {"number": 30917, "title": "[mlir] make `//tensorflow/compiler/mlir/...` build again", "body": "Except syncing up with MLIR repo, there are two problems:\r\n1. there is no `absl/base/integral_types.h` in public absl (is this a google internal one?), so remove it, and s/int64/int64_t/\r\n2. this is not `::testing::EqualsProto` in public gtest (is this a google internal one?): need to disable using of `EqualsProto` or implement it\r\n\r\n@jpienaar: FYR", "comments": ["Thanks, sent out a couple of changes to address this too, should be fixed tomorrow (modulo review latency). We'll move to a different export shortly to work around a current failure and avoid the current version skew problems.", "close this since fixes are coming."]}, {"number": 30916, "title": "Problems with running keras models in colab TPU", "body": "```\r\nfilepath = \"./xlnet_models/batch-128/saved-model-128-{epoch:02d}-{acc:.2f}--{loss:.5f}.hdf5\"\r\nsave_callback = tf.keras.callbacks.ModelCheckpoint(filepath)\r\nmodel.fit_generator(train_generator_128,epochs=20,callbacks=[save_callback])\r\n\r\n```\r\n`fit_generator` is not supported for models compiled with tf.distribute.Strategy.\r\n\r\nIs there any way to solve this?", "comments": ["@kushalj001 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nIn order to expedite the trouble-shooting process, please provide a complete code to reproduce the issue reported here. Thanks!\r\n\r\n"]}, {"number": 30915, "title": "Dataset Iterator is not an iterator when using fit_generator", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_generator.py#L363\r\n\r\n\r\nbelow is ok.\r\n```\r\n    if isinstance(generator, (iterator_ops.IteratorV2, iterator_ops.Iterator)):\r\n363        generator_output = generator.get_next()\r\n364     else:\r\n365        generator_output = next(generator)\r\n```\r\n\r\n", "comments": ["Please provide following information whichever is applicable in your case.\r\nIt will help us to get some context. Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30915\">No</a>\n"]}, {"number": 30913, "title": "Maybe there's a bug in SparseApplyFtrlOp for the sparse solution", "body": "in the class SparseApplyFtrlOp from tensorflow/core/kernel/training_ops.cc\r\n\r\n```\r\n#define COMPUTE_FTRL(grad, grad_maybe_with_shrinkage)                          \\\r\n  auto new_accum = accum + grad.square();                                      \\\r\n  if (lr_power_scalar == static_cast<T>(-0.5)) {                               \\\r\n    linear += grad_maybe_with_shrinkage -                                      \\\r\n              (new_accum.sqrt() - accum.sqrt()) / lr_scalar * var;             \\\r\n  } else {                                                                     \\\r\n    linear += grad_maybe_with_shrinkage - (new_accum.pow(-lr_power_scalar) -   \\\r\n                                           accum.pow(-lr_power_scalar)) /      \\\r\n                                              lr_scalar * var;                 \\\r\n  }                                                                            \\\r\n  auto l1_reg_adjust = linear.cwiseMin(l1_scalar).cwiseMax(-l1_scalar);        \\\r\n  auto x = l1_reg_adjust - linear;                                             \\\r\n  if (lr_power_scalar == static_cast<T>(-0.5)) {                               \\\r\n    auto y = new_accum.sqrt() / new_accum.constant(lr_scalar) +                \\\r\n             linear.constant(static_cast<T>(2) * l2_scalar);                   \\\r\n    var = x / y;                                                               \\\r\n  } else {                                                                     \\\r\n    auto y = new_accum.pow(-lr_power_scalar) / new_accum.constant(lr_scalar) + \\\r\n             linear.constant(static_cast<T>(2) * l2_scalar);                   \\\r\n    var = x / y;                                                               \\\r\n  }                                                                            \\\r\n  accum += grad.square();\r\n```\r\n\r\nI only find the update of `var = x / y;`  directly, not considering the situation var = 0 when abs(linear) <= l1_scalar according to the paper\r\n![ftrl_paper](https://user-images.githubusercontent.com/12267324/61602302-8a1ac880-ac6b-11e9-80dc-d982f6ebdc50.png)\r\n", "comments": ["@sjtusmartboy Please provide details about TensorFlow version. Also provide the Github link of tensorflow/core/kernel/training_ops.cc Thanks!\r\n", "> @sjtusmartboy Please provide details about TensorFlow version. Also provide the Github link of tensorflow/core/kernel/training_ops.cc Thanks!\r\n\r\n@gadagashwini \r\nI'm using the tf 1.14\r\nplease go to the link [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L2300)\r\n", "I believe l1_reg_adjust is the thing responsible for setting it to zero (note how it's defined to be equal to linear if -l1 < linear < l1, which the subtraction below will make zero)."]}, {"number": 30912, "title": "TF-GPU v1.14 CUDA runs out of memory ", "body": "Hello TF Team,\r\n\r\nI am hoping you can please pull down my custom TF-GPU, CUDA X, and Anaconda container solution with Jupyter. I am hoping you can assist because whatever the problem is, the fix for the container is same as Ubuntu. Link to repo, [here](https://github.com/joehoeller/Anaconda-CUDA-Accelerated-TensorFlowGPU-Development-Environment).\r\nHere is the error when running Tensorboard and benchmarks.py (see instructions in README.md as to how to run it after spinning up container).\r\n\r\nThis is the error that I get, scroll right to read entire line:\r\n\r\n```\r\nroot@e71bda560638:/apps/apps/gpu_benchmarks# python tensorboard.py \r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0721 23:23:02.790582 139901430245184 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nW0721 23:23:02.810480 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:191: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nW0721 23:23:02.810840 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:161: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nW0721 23:23:02.810928 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:163: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nW0721 23:23:02.811073 139901430245184 deprecation.py:323] From tensorboard.py:20: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nW0721 23:23:02.811128 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nW0721 23:23:02.811218 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use urllib or similar directly.\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nW0721 23:23:04.074006 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nW0721 23:23:04.547974 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nW0721 23:23:04.550884 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nW0721 23:23:05.421007 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nW0721 23:23:05.558143 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:22: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\r\n\r\n2019-07-21 23:23:05.558423: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-07-21 23:23:05.562806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-07-21 23:23:05.563094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e55a519380 executing computations on platform Host. Devices:\r\n2019-07-21 23:23:05.563108: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-21 23:23:05.563894: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-07-21 23:23:05.580061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:05.580637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\r\npciBusID: 0000:01:00.0\r\n2019-07-21 23:23:05.580748: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-07-21 23:23:05.581760: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-07-21 23:23:05.582704: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-07-21 23:23:05.582852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-07-21 23:23:05.583850: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-07-21 23:23:05.584484: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-07-21 23:23:05.586678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-21 23:23:05.586776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:05.587222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:05.587602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-21 23:23:05.587629: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-07-21 23:23:05.646174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-21 23:23:05.646191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-21 23:23:05.646197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-21 23:23:05.646385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:05.646756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:05.647144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:05.647446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 170 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-07-21 23:23:05.649957: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e55ba1ac60 executing computations on platform CUDA. Devices:\r\n2019-07-21 23:23:05.649969: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nW0721 23:23:05.650787 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:27: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nW0721 23:23:05.652657 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:32: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\r\n\r\nW0721 23:23:05.653614 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:37: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\r\n\r\nW0721 23:23:05.658960 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:49: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nW0721 23:23:05.666438 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:55: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\r\n\r\nW0721 23:23:05.681653 139901430245184 deprecation.py:506] From tensorboard.py:84: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nW0721 23:23:05.714577 139901430245184 deprecation.py:323] From tensorboard.py:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\n\r\nW0721 23:23:05.727325 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:106: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nW0721 23:23:05.796838 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:118: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\r\n\r\nW0721 23:23:05.797843 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:119: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\r\n\r\nW0721 23:23:05.817948 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:121: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-07-21 23:23:06.136829: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-07-21 23:23:06.224853: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-07-21 23:23:06.226553: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-07-21 23:23:06.228049: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.228406: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.228869: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.229269: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.229750: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.230151: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.230537: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.230914: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.231278: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.231646: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.232019: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.232382: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.232740: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.233104: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.233457: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.233823: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.234184: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.234533: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.234906: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.235263: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.235914: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.236498: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.236888: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:06.237265: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:16.229311: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:16.230144: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:16.230171: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 29.91MiB (rounded to 31360000).  Current allocation summary follows.\r\n2019-07-21 23:23:16.230190: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): \tTotal Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 240B client-requested in use in bin.\r\n2019-07-21 23:23:16.230203: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230215: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-07-21 23:23:16.230226: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): \tTotal Chunks: 5, Chunks in use: 5. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 9.8KiB client-requested in use in bin.\r\n2019-07-21 23:23:16.230237: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230250: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230275: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): \tTotal Chunks: 5, Chunks in use: 4. 97.8KiB allocated for chunks. 79.0KiB in use in bin. 78.1KiB client-requested in use in bin.\r\n2019-07-21 23:23:16.230286: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230299: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): \tTotal Chunks: 1, Chunks in use: 1. 78.2KiB allocated for chunks. 78.2KiB in use in bin. 78.1KiB client-requested in use in bin.\r\n2019-07-21 23:23:16.230311: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230325: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): \tTotal Chunks: 2, Chunks in use: 1. 833.0KiB allocated for chunks. 390.8KiB in use in bin. 390.6KiB client-requested in use in bin.\r\n2019-07-21 23:23:16.230336: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230348: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 3. 4.49MiB allocated for chunks. 4.49MiB in use in bin. 4.49MiB client-requested in use in bin.\r\n2019-07-21 23:23:16.230361: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 1. 4.50MiB allocated for chunks. 2.00MiB in use in bin. 1.50MiB client-requested in use in bin.\r\n2019-07-21 23:23:16.230373: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 5.01MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230386: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230398: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230409: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230420: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230434: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230444: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:16.230454: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 29.91MiB was 16.00MiB, Chunk State: \r\n2019-07-21 23:23:16.230463: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 4194304\r\n2019-07-21 23:23:16.230473: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3b73000000 next 15 of size 1568000\r\n2019-07-21 23:23:16.230484: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3b7317ed00 next 18446744073709551615 of size 2626304\r\n2019-07-21 23:23:16.230493: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 8388608\r\n2019-07-21 23:23:16.230502: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3b73400000 next 19 of size 1568000\r\n2019-07-21 23:23:16.230511: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3b7357ed00 next 27 of size 1568000\r\n2019-07-21 23:23:16.230521: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3b736fda00 next 18446744073709551615 of size 5252608\r\n2019-07-21 23:23:16.230529: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 1048576\r\n2019-07-21 23:23:16.230540: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000000 next 1 of size 256\r\n2019-07-21 23:23:16.230555: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000100 next 2 of size 256\r\n2019-07-21 23:23:16.230570: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000200 next 3 of size 2048\r\n2019-07-21 23:23:16.230583: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000a00 next 4 of size 256\r\n2019-07-21 23:23:16.230597: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000b00 next 5 of size 256\r\n2019-07-21 23:23:16.230611: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000c00 next 6 of size 256\r\n2019-07-21 23:23:16.230625: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000d00 next 7 of size 2048\r\n2019-07-21 23:23:16.230654: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf001500 next 8 of size 256\r\n2019-07-21 23:23:16.230669: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf001600 next 10 of size 20224\r\n2019-07-21 23:23:16.230684: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006500 next 11 of size 1280\r\n2019-07-21 23:23:16.230697: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006a00 next 12 of size 256\r\n2019-07-21 23:23:16.230709: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006b00 next 13 of size 256\r\n2019-07-21 23:23:16.230723: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006c00 next 16 of size 2048\r\n2019-07-21 23:23:16.230735: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007400 next 29 of size 256\r\n2019-07-21 23:23:16.230747: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007500 next 30 of size 256\r\n2019-07-21 23:23:16.230760: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007600 next 32 of size 256\r\n2019-07-21 23:23:16.230773: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007700 next 33 of size 256\r\n2019-07-21 23:23:16.230785: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3bdf007800 next 17 of size 19200\r\n2019-07-21 23:23:16.230796: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf00c300 next 20 of size 256\r\n2019-07-21 23:23:16.230809: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf00c400 next 21 of size 20224\r\n2019-07-21 23:23:16.230823: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf011300 next 22 of size 20224\r\n2019-07-21 23:23:16.230836: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf016200 next 23 of size 2048\r\n2019-07-21 23:23:16.230848: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf016a00 next 24 of size 2048\r\n2019-07-21 23:23:16.230861: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf017200 next 25 of size 256\r\n2019-07-21 23:23:16.230874: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf017300 next 26 of size 256\r\n2019-07-21 23:23:16.230886: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf017400 next 28 of size 20224\r\n2019-07-21 23:23:16.230899: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf01c300 next 31 of size 400128\r\n2019-07-21 23:23:16.230911: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf07de00 next 35 of size 80128\r\n2019-07-21 23:23:16.230924: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3bdf091700 next 18446744073709551615 of size 452864\r\n2019-07-21 23:23:16.230938: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 2097152\r\n2019-07-21 23:23:16.230953: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf400000 next 18446744073709551615 of size 2097152\r\n2019-07-21 23:23:16.230967: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: \r\n2019-07-21 23:23:16.230983: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 15 Chunks of size 256 totalling 3.8KiB\r\n2019-07-21 23:23:16.230995: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.2KiB\r\n2019-07-21 23:23:16.231007: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 5 Chunks of size 2048 totalling 10.0KiB\r\n2019-07-21 23:23:16.231023: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 20224 totalling 79.0KiB\r\n2019-07-21 23:23:16.231039: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 80128 totalling 78.2KiB\r\n2019-07-21 23:23:16.231053: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 400128 totalling 390.8KiB\r\n2019-07-21 23:23:16.231068: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 3 Chunks of size 1568000 totalling 4.49MiB\r\n2019-07-21 23:23:16.231085: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 2097152 totalling 2.00MiB\r\n2019-07-21 23:23:16.231102: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 7.04MiB\r\n2019-07-21 23:23:16.231111: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 15728640 memory_limit_: 178782208 available bytes: 163053568 curr_region_allocation_bytes_: 33554432\r\n2019-07-21 23:23:16.231125: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: \r\nLimit:                   178782208\r\nInUse:                     7377664\r\nMaxInUse:                 10028288\r\nNumAllocs:                      61\r\nMaxAllocSize:              2626304\r\n\r\n2019-07-21 23:23:16.231150: W tensorflow/core/common_runtime/bfc_allocator.cc:319] **********________________*********************_________________________________****__***********xxx\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n  (1) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n\t [[layer1/Wx_plus_b/add/_25]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorboard.py\", line 191, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tensorboard.py\", line 164, in main\r\n    train()\r\n  File \"tensorboard.py\", line 139, in train\r\n    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n  (1) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n\t [[layer1/Wx_plus_b/add/_25]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\nroot@e71bda560638:/apps/apps/gpu_benchmarks# python tensorboard.py \r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0721 23:23:48.399826 139765773813568 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nW0721 23:23:48.418498 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:191: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nW0721 23:23:48.418813 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:161: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nW0721 23:23:48.418894 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:162: The name tf.gfile.DeleteRecursively is deprecated. Please use tf.io.gfile.rmtree instead.\r\n\r\nW0721 23:23:48.419052 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:163: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nW0721 23:23:48.419144 139765773813568 deprecation.py:323] From tensorboard.py:20: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nW0721 23:23:48.419198 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nW0721 23:23:48.419269 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nW0721 23:23:48.555204 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nW0721 23:23:48.555730 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nW0721 23:23:48.580618 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nW0721 23:23:48.672224 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:22: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\r\n\r\n2019-07-21 23:23:48.672475: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-07-21 23:23:48.676436: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-07-21 23:23:48.676919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e235e7cac0 executing computations on platform Host. Devices:\r\n2019-07-21 23:23:48.676941: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-21 23:23:48.677678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-07-21 23:23:48.699374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:48.699807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\r\npciBusID: 0000:01:00.0\r\n2019-07-21 23:23:48.699921: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-07-21 23:23:48.701017: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-07-21 23:23:48.702035: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-07-21 23:23:48.702201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-07-21 23:23:48.703269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-07-21 23:23:48.704016: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-07-21 23:23:48.706595: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-21 23:23:48.706691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:48.707180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:48.707594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-21 23:23:48.707623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-07-21 23:23:48.767926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-21 23:23:48.767946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-21 23:23:48.767953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-21 23:23:48.768148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:48.768525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:48.768871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:23:48.769198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 161 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-07-21 23:23:48.770262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e237153f80 executing computations on platform CUDA. Devices:\r\n2019-07-21 23:23:48.770273: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nW0721 23:23:48.770924 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:27: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nW0721 23:23:48.772640 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:32: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\r\n\r\nW0721 23:23:48.773568 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:37: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\r\n\r\nW0721 23:23:48.778774 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:49: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nW0721 23:23:48.786193 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:55: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\r\n\r\nW0721 23:23:48.801232 139765773813568 deprecation.py:506] From tensorboard.py:84: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nW0721 23:23:48.833851 139765773813568 deprecation.py:323] From tensorboard.py:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\n\r\nW0721 23:23:48.846037 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:106: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nW0721 23:23:48.914657 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:118: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\r\n\r\nW0721 23:23:48.915626 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:119: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\r\n\r\nW0721 23:23:48.934774 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:121: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-07-21 23:23:49.254414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-07-21 23:23:49.324353: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-07-21 23:23:49.330331: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-07-21 23:23:49.331837: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.332226: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.332706: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.333106: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.333517: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.333936: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.334331: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.334722: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.335239: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.335604: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.336030: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.336411: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.336789: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.337165: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.337532: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.337899: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.338252: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:49.338602: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:59.333998: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:59.336335: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:23:59.336404: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 29.91MiB (rounded to 31360000).  Current allocation summary follows.\r\n2019-07-21 23:23:59.336453: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): \tTotal Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 240B client-requested in use in bin.\r\n2019-07-21 23:23:59.336491: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): \tTotal Chunks: 2, Chunks in use: 0. 1.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336534: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-07-21 23:23:59.336588: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): \tTotal Chunks: 5, Chunks in use: 5. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 9.8KiB client-requested in use in bin.\r\n2019-07-21 23:23:59.336636: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336683: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336711: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): \tTotal Chunks: 5, Chunks in use: 4. 96.5KiB allocated for chunks. 79.0KiB in use in bin. 78.1KiB client-requested in use in bin.\r\n2019-07-21 23:23:59.336719: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336725: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): \tTotal Chunks: 1, Chunks in use: 1. 78.2KiB allocated for chunks. 78.2KiB in use in bin. 78.1KiB client-requested in use in bin.\r\n2019-07-21 23:23:59.336732: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336740: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): \tTotal Chunks: 2, Chunks in use: 1. 833.0KiB allocated for chunks. 390.8KiB in use in bin. 390.6KiB client-requested in use in bin.\r\n2019-07-21 23:23:59.336748: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336758: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 2. 4.49MiB allocated for chunks. 2.99MiB in use in bin. 2.99MiB client-requested in use in bin.\r\n2019-07-21 23:23:59.336765: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 2.99MiB client-requested in use in bin.\r\n2019-07-21 23:23:59.336774: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 5.01MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336782: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336789: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336796: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336803: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336811: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336818: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:23:59.336827: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 29.91MiB was 16.00MiB, Chunk State: \r\n2019-07-21 23:23:59.336832: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 4194304\r\n2019-07-21 23:23:59.336840: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1bdf000000 next 16 of size 1568000\r\n2019-07-21 23:23:59.336846: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1bdf17ed00 next 18446744073709551615 of size 2626304\r\n2019-07-21 23:23:59.336850: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 8388608\r\n2019-07-21 23:23:59.336856: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1bdf400000 next 20 of size 1568000\r\n2019-07-21 23:23:59.336862: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1bdf57ed00 next 28 of size 1568000\r\n2019-07-21 23:23:59.336869: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1bdf6fda00 next 18446744073709551615 of size 5252608\r\n2019-07-21 23:23:59.336875: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 1048576\r\n2019-07-21 23:23:59.336880: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000000 next 1 of size 256\r\n2019-07-21 23:23:59.336886: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000100 next 2 of size 256\r\n2019-07-21 23:23:59.336892: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000200 next 3 of size 2048\r\n2019-07-21 23:23:59.336898: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000a00 next 4 of size 256\r\n2019-07-21 23:23:59.336903: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000b00 next 5 of size 256\r\n2019-07-21 23:23:59.336909: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000c00 next 6 of size 256\r\n2019-07-21 23:23:59.336915: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000d00 next 7 of size 2048\r\n2019-07-21 23:23:59.336920: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f001500 next 8 of size 256\r\n2019-07-21 23:23:59.336926: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f001600 next 10 of size 20224\r\n2019-07-21 23:23:59.336933: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006500 next 11 of size 1280\r\n2019-07-21 23:23:59.336939: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006a00 next 29 of size 256\r\n2019-07-21 23:23:59.336945: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006b00 next 30 of size 256\r\n2019-07-21 23:23:59.336951: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f006c00 next 34 of size 768\r\n2019-07-21 23:23:59.336957: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006f00 next 35 of size 256\r\n2019-07-21 23:23:59.336963: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f007000 next 38 of size 512\r\n2019-07-21 23:23:59.336968: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f007200 next 39 of size 256\r\n2019-07-21 23:23:59.336974: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f007300 next 12 of size 17920\r\n2019-07-21 23:23:59.336980: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f00b900 next 13 of size 256\r\n2019-07-21 23:23:59.336986: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f00ba00 next 14 of size 256\r\n2019-07-21 23:23:59.336992: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f00bb00 next 17 of size 20224\r\n2019-07-21 23:23:59.336998: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f010a00 next 18 of size 20224\r\n2019-07-21 23:23:59.337004: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f015900 next 21 of size 256\r\n2019-07-21 23:23:59.337010: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f015a00 next 22 of size 2048\r\n2019-07-21 23:23:59.337016: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f016200 next 23 of size 2048\r\n2019-07-21 23:23:59.337022: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f016a00 next 24 of size 256\r\n2019-07-21 23:23:59.337028: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f016b00 next 25 of size 2048\r\n2019-07-21 23:23:59.337034: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f017300 next 26 of size 256\r\n2019-07-21 23:23:59.337040: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f017400 next 27 of size 20224\r\n2019-07-21 23:23:59.337046: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f01c300 next 32 of size 400128\r\n2019-07-21 23:23:59.337052: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f07de00 next 36 of size 80128\r\n2019-07-21 23:23:59.337058: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f091700 next 18446744073709551615 of size 452864\r\n2019-07-21 23:23:59.337064: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 2097152\r\n2019-07-21 23:23:59.337070: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f400000 next 18446744073709551615 of size 2097152\r\n2019-07-21 23:23:59.337076: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: \r\n2019-07-21 23:23:59.337084: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 15 Chunks of size 256 totalling 3.8KiB\r\n2019-07-21 23:23:59.337091: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.2KiB\r\n2019-07-21 23:23:59.337097: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 5 Chunks of size 2048 totalling 10.0KiB\r\n2019-07-21 23:23:59.337104: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 20224 totalling 79.0KiB\r\n2019-07-21 23:23:59.337111: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 80128 totalling 78.2KiB\r\n2019-07-21 23:23:59.337118: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 400128 totalling 390.8KiB\r\n2019-07-21 23:23:59.337124: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 1568000 totalling 2.99MiB\r\n2019-07-21 23:23:59.337130: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 2097152 totalling 2.00MiB\r\n2019-07-21 23:23:59.337137: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 2626304 totalling 2.50MiB\r\n2019-07-21 23:23:59.337143: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 8.04MiB\r\n2019-07-21 23:23:59.337149: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 15728640 memory_limit_: 168951808 available bytes: 153223168 curr_region_allocation_bytes_: 33554432\r\n2019-07-21 23:23:59.337158: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: \r\nLimit:                   168951808\r\nInUse:                     8435968\r\nMaxInUse:                 10029568\r\nNumAllocs:                      61\r\nMaxAllocSize:              2626304\r\n\r\n2019-07-21 23:23:59.337168: W tensorflow/core/common_runtime/bfc_allocator.cc:319] ********************xxxxxxx_________***********_________________________________****__***********xxx\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n  (1) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n\t [[layer1/Wx_plus_b/add/_25]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorboard.py\", line 191, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tensorboard.py\", line 164, in main\r\n    train()\r\n  File \"tensorboard.py\", line 139, in train\r\n    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n  (1) Internal: Dst tensor is not initialized.\r\n\t [[{{node _arg_input/x-input_0_1}}]]\r\n\t [[layer1/Wx_plus_b/add/_25]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\nroot@e71bda560638:/apps/apps/gpu_benchmarks# ls\r\n__pycache__  benchmark.py  tensorboard.py\r\nroot@e71bda560638:/apps/apps/gpu_benchmarks# python benchmark.py gpu 10000\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0721 23:26:34.769160 140354788767552 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nW0721 23:26:34.788027 140354788767552 deprecation_wrapper.py:119] From benchmark.py:18: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\r\n\r\nW0721 23:26:34.792794 140354788767552 deprecation_wrapper.py:119] From benchmark.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\nW0721 23:26:34.792872 140354788767552 deprecation_wrapper.py:119] From benchmark.py:24: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\n2019-07-21 23:26:34.792978: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-07-21 23:26:34.818743: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-07-21 23:26:34.819354: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560751921f80 executing computations on platform Host. Devices:\r\n2019-07-21 23:26:34.819370: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-21 23:26:34.819978: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-07-21 23:26:34.834675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:26:34.835206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\r\npciBusID: 0000:01:00.0\r\n2019-07-21 23:26:34.835319: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-07-21 23:26:34.836303: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-07-21 23:26:34.837230: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-07-21 23:26:34.837382: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-07-21 23:26:34.838334: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-07-21 23:26:34.838907: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-07-21 23:26:34.840924: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-21 23:26:34.841007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:26:34.841449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:26:34.841823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-21 23:26:34.841845: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-07-21 23:26:34.894491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-21 23:26:34.894509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-21 23:26:34.894513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-21 23:26:34.894703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:26:34.895050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:26:34.895367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-21 23:26:34.895665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 159 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-07-21 23:26:34.896687: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560752b2a3a0 executing computations on platform CUDA. Devices:\r\n2019-07-21 23:26:34.896697: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n2019-07-21 23:26:34.897194: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n\r\nrandom_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897731: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897741: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897746: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897751: I tensorflow/core/common_runtime/placer.cc:54] random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0\r\ntranspose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897757: I tensorflow/core/common_runtime/placer.cc:54] transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897761: I tensorflow/core/common_runtime/placer.cc:54] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\nSum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897771: I tensorflow/core/common_runtime/placer.cc:54] Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897781: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897790: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897802: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\ntranspose/perm: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897810: I tensorflow/core/common_runtime/placer.cc:54] transpose/perm: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nConst: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.897817: I tensorflow/core/common_runtime/placer.cc:54] Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2019-07-21 23:26:34.901219: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-07-21 23:26:35.082216: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 159.69M (167444480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:35.082576: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 143.72M (150700032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:35.082925: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 129.35M (135630080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:35.083263: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 116.41M (122067200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:35.083600: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 104.77M (109860608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:35.083937: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 94.29M (98874624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:35.084275: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 84.86M (88987392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:35.084614: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 76.38M (80088832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-07-21 23:26:45.085467: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 381.47MiB (rounded to 400000000).  Current allocation summary follows.\r\n2019-07-21 23:26:45.085552: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085613: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085661: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-07-21 23:26:45.085701: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085739: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085779: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085816: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085857: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085897: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085936: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.085975: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086012: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086043: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086078: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086122: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086154: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086187: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086217: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086247: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0. 68.74MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086281: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086317: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-07-21 23:26:45.086361: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 381.47MiB was 256.00MiB, Chunk State: \r\n2019-07-21 23:26:45.086394: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 72080128\r\n2019-07-21 23:26:45.086432: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7fa540000000 next 1 of size 1280\r\n2019-07-21 23:26:45.086468: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7fa540000500 next 18446744073709551615 of size 72078848\r\n2019-07-21 23:26:45.086493: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: \r\n2019-07-21 23:26:45.086521: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.2KiB\r\n2019-07-21 23:26:45.086546: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 1.2KiB\r\n2019-07-21 23:26:45.086575: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 72080128 memory_limit_: 167444480 available bytes: 95364352 curr_region_allocation_bytes_: 334888960\r\n2019-07-21 23:26:45.086610: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: \r\nLimit:                   167444480\r\nInUse:                        1280\r\nMaxInUse:                     1280\r\nNumAllocs:                       1\r\nMaxAllocSize:                 1280\r\n\r\n2019-07-21 23:26:45.086709: W tensorflow/core/common_runtime/bfc_allocator.cc:319] *___________________________________________________________________________________________________\r\n2019-07-21 23:26:45.086813: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at random_op.cc:76 : Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node random_uniform/RandomUniform}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node random_uniform/RandomUniform}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Sum/_1]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"benchmark.py\", line 25, in <module>\r\n    result = session.run(sum_operation)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node random_uniform/RandomUniform (defined at benchmark.py:18) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node random_uniform/RandomUniform (defined at benchmark.py:18) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Sum/_1]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'random_uniform/RandomUniform':\r\n  File \"benchmark.py\", line 18, in <module>\r\n    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py\", line 247, in random_uniform\r\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 820, in random_uniform\r\n    name=name)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n\r\n```\r\n", "comments": ["I have addt'l erros and info here:\r\n[https://github.com/joehoeller/Anaconda-CUDA-Accelerated-TensorFlowGPU-Development-Environment/issues/1](https://github.com/joehoeller/Anaconda-CUDA-Accelerated-TensorFlowGPU-Development-Environment/issues/1)", "Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30912\">No</a>\n", "Is it resolved?  @joehoeller ", "Yes plz close\n\nSent from my iPhone\n\n> On Apr 8, 2020, at 8:55 PM, super.single430 <notifications@github.com> wrote:\n> \n> \ufeff\n> Is it resolved? @joehoeller\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n"]}, {"number": 30911, "title": "tensorflow-gpu without nvidia-runtime.", "body": "This PR [symlinks](https://en.wikipedia.org/wiki/Symbolic_link) the required CUDA library stubs to the location where tensorflow expects to find them. That means `tensorflow-gpu` can be used even if a container is started based on the image without using the [nvidia runtime](https://github.com/NVIDIA/nvidia-docker). When the nvidia runtime is used, the stubs are overwritten by the real libraries and one can access the GPU as expected.\r\n\r\nThe issue with library paths already seems to be fixed within tensorflow for `tensorflow-gpu>=1.14`. This patch would support using `tensorflow-gpu<1.14` using a universal docker image.\r\n\r\ncc @angersson.", "comments": ["@angersson, good point. In short, the following command fails today but will succeed after this PR is merged. Some more information here: https://github.com/tillahoffmann/universal_tensorflow_image/\r\n\r\n```\r\ndocker run --rm tensorflow/tensorflow:1.13.2-gpu-py3 python -c \"import tensorflow\" \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```"]}]