[{"number": 48491, "title": "Remove deprecated function call in local.py", "body": "Please refer #48486 ", "comments": ["@bhack \r\nThanks for the prompt reply!", "@rthadur \r\nThanks for the review!", "Thanks for the quick response!"]}, {"number": 48490, "title": "Cholesky decomposition was not successful. The input might not be valid.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.5.0rc0**\r\n- Python version: **3.8.8**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **tried with 11.1/11.2 same issue (cudnn 8.1.1.33)**\r\n- GPU model and memory: **RTX3080 10GB**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n**v1.12.1-53831-ga8b6d5ff93a 2.5.0-rc0**\r\n\r\n**Describe the current behavior**\r\nI was trying to run the sample code from TFP homepage (https://blog.tensorflow.org/2019/03/structural-time-series-modeling-in.html)\r\nI ran this in google colab its fully working.\r\nBut when I ran in my own Jupyter it gives error \"**_Cholesky decomposition was not successful. The input might not be valid._**\" when I reached the code `component_dists = sts.decompose_by_component(\r\n    co2_model,\r\n    observed_time_series=co2_by_month,\r\n    parameter_samples=q_samples_co2_)`\r\n\r\nWhen I run the forecast `tfp.sts.forecast` the result is different with the colab result significantly too.\r\n\r\n**Describe the expected behavior**\r\nThe result from colab is same pretty much the same with the result shown on the webpage. I expect the result from the code running in my pc should be the same too.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Structural_Time_Series_Modeling_Case_Studies_Atmospheric_CO2_and_Electricity_Demand.ipynb](url)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ronnist  Seems the issue is more related to TF probability repo. Could you please submit a new issue [here](https://github.com/tensorflow/probability/issues/new). Thanks!", "Alright I will try to submit at TFP!\r\n\r\n[https://github.com/tensorflow/probability/issues/1306](url)"]}, {"number": 48489, "title": "Handle DT_VARIANT in DataTypeToPrimitiveType.", "body": "", "comments": ["Hi @jakeh-gc,\r\n\r\nCan you please state why you need this?\r\n\r\nThanks!\r\n\r\nCC @jpienaar @yunxing ", "DT_VARIANT types should be handled instead by passes like tensorlist decomposition rather than a blanket mapping to void* IMO. This seems convenient but lossy and creates an ordering dependency. ", "Since creating this PR I've created a workaround for this. So I don't think it's needed."]}, {"number": 48488, "title": "Fixing typo for estimator", "body": "", "comments": []}, {"number": 48486, "title": "Deprecation Warning in LocallyConnected1D", "body": "Tensorflow version 2.4.1\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/layers/local.py#L39-L338\r\n\r\nAnd also (perhaps more specifically)\r\nhttps://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/layers/local.py#L780\r\n\r\nIn a LocallyConnected1D layer, if you use `implementation==2` you get a deprecation warning for using `math_ops.sparse_matmul`.\r\n\r\nPossible solution:\r\nChange that line to use `tf.linalg.matmul` as the deprecation warning recommends.\r\n\r\nTo reproduce:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Create a tensor to run the layer on.\r\n# The specifics of the shape don't matter as long as it's at least 3 long so that the\r\n# layer will process properly\r\n\r\ntensor = tf.random.normal((1, 32, 5))\r\n\r\n# Again, the specifics don't really matter as long as the input shape is compatible.\r\n# The important part is the `implementation` parameter.\r\nlc_layer = tf.keras.layers.LocallyConnected1D(10, 1, implementation=2)\r\n\r\nprint(lc_layer(tensor))\r\n```\r\noutput:\r\n```\r\nWARNING:tensorflow:From c:\\project_dir\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\local.py:780: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.linalg.matmul` instead\r\n```", "comments": ["Please refer #48491 \r\n", "Very fast response! Thanks!"]}, {"number": 48485, "title": "Tensorflow Lite - Flatbuffers.h error [cmake]", "body": "Hello !\r\n\r\nOur aim is to install TensorFlow Lite in our embedded board (iMX6). To reach this aim, firstly we made a recipe to build TFLite in our Yocto framework. After this step our static tensorflow-lite library was generated.\r\n\r\nThen, we wanted to try the functionality of the library with a basic helloworld program. For this step, we chose to start with minimal.cpp which is from official github repo of tensorflow.\r\n\r\nFor this helloworld (minimal.cpp) program we created a recipe which includes our static tensorflow-lite library inside DEPENDS part. When we bitbake with cmake recipe we get this error about flatbuffers.h ;\r\n\r\n`/home/student/fsl-release-bsp/build-analytics-tflite/tmp/sysroots/analytics/usr/include/tensorflow/contrib/lite/schema/schema_generated.h:21:37: fatal error: flatbuffers/flatbuffers.h: No such file or directory\r\n|  #include \"flatbuffers/flatbuffers.h\"`\r\n\r\n**-Please check below for files-**\r\n\r\n- #include part of minimal.cpp :\r\n\r\n`#include <cstdio>\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/optional_debug_tools.h\"`\r\n\r\n- Our recipe (helloworld.bb) is as follows:\r\n\r\n`# Recipe for the Hello World program\r\n\r\nSUMMARY = \"Recipe for the Hello World program\"\r\nLICENSE = \"CLOSED\"\r\nLIC_FILES_CHKSUM = \"\"\r\nDEPENDS += \"opencv\"\r\nDEPENDS += \"libconfig\"\r\nDEPENDS += \"tensorflow-lite\"\r\nSRC_URI = \"git://github.com/aniladar/patch.git;protocol=https;branch=main\"\r\nSRCREV = \"${AUTOREV}\"\r\nS = \"${WORKDIR}/git\"\r\nB = \"${S}\"\r\ninherit cmake`\r\n\r\n- The CMakeLists.txt file is as follows:\r\n\r\n`cmake_minimum_required(VERSION 3.14)\r\nproject(patch)\r\nadd_executable(minimal minimal.cc)`\r\n\r\nMaybe we need some additional things inside CMakeLists.txt file or maybe we have missing DEPENDS inside our recipe. If you brighten up our path it would be perfect. Your guidance and clarifications are important for us.\r\n\r\nThank you in advance !", "comments": ["@terryheo could you take a look?", "> @terryheo could you take a look?\r\n\r\nIt would be appreciated if you can guide us", "I believe flatbuffers is a dependency that would normally be pulled by bazel in a \"traditional\" build. I see you are using CMake, so you may reference [1] or try cross-compilation with bazel [2].\r\n\r\n[1] https://www.tensorflow.org/lite/guide/build_cmake_arm \r\n[2] https://www.tensorflow.org/lite/guide/build_arm#cross-compilation_for_arm_with_bazel", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48485\">No</a>\n"]}, {"number": 48483, "title": "GPU is not working even after fine installation", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version:2.4.1\r\n- Python version:3.8.8\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11\r\n- GPU model and memory:GTX1660TI\r\n\r\n![Problem](https://user-images.githubusercontent.com/56079922/114399450-e4944f00-9bdb-11eb-9d28-c0c2829b2d3f.PNG)\r\n\r\nGood Morning- Or Night- My friends.\r\nI'm an student learning TF and ML stuffs with plenty of interests.\r\nI installed the CUDA, CDNN, Tensorflow-gpu properly, and python console also answered me that I have proper graphic card for \r\nthe tensorflow. However, whenever I try to run my tf codes, only cpu usage makes tons of works, while my GPU is chillin' like a summer vacation. I think the reason might be the fact that I'm using laptop right now, FX705-DU, which made several issues about gpu while gaming. \r\n\r\nDoes anyone suffering from same problem like me? \r\nTF Device check answers me that I got right GPU and even TF does not claim about absence of GPU while running the code,\r\nsmiling happily with lines of 'successfully loaded .....' \r\nHow can I make this GPU work? I'll happily wait for the answer:)\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["I had the same issue. I think perhaps 2.4 updated its cuDNN requirements. I have cuDNN64_7, and it's looking for cuDNN64_8.\r\n\r\nI suspect this was a quiet update, and it was a bit of a surprise to see that error. I'm looking into how to get cuDNN64_8 now.", "> I had the same issue. I think perhaps 2.4 updated its cuDNN requirements. I have cuDNN64_7, and it's looking for cuDNN64_8.\r\n> \r\n> I suspect this was a quiet update, and it was a bit of a surprise to see that error. I'm looking into how to get cuDNN64_8 now.\r\n\r\nwell I'm totally noob here, so I always look for the stable or latest version for both cuda and cuDNN... \r\nmine is 8.1.1, and I don't think it's some kind of package problem, but maybe it's the problem for laptop computers which got both internal graphic cards and additional GPUs...  but again, I'm totally noob about this, thanks for the idea :)", "Did you follow the instructions here:\r\nhttps://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781\r\n\r\n?\r\n\r\nThey're a little outdated, but they got me there with Cuda 10 and cuDNN64_7 about 6 months ago.", "> Did you follow the instructions here:\r\n> https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781\r\n> \r\n> ?\r\n> \r\n> They're a little outdated, but they got me there with Cuda 10 and cuDNN64_7 about 6 months ago.\r\n\r\nAgain thanks for the help, \r\nI got both true response from both of the test-utility functions...\r\nyet my gpu is sleeping...\r\n", "Can you get the logs ?\nMaybe the logs will point out the problem .\nThe most possible reason is that the program cannot find the required libraries or wrong version .", "> Can you get the logs ?\r\n> Maybe the logs will point out the problem .\r\n> The most possible reason is that the program cannot find the required libraries or wrong version .\r\n\r\nLuckily, I could make my GPU work by using link up there. \r\nNow, I have to use it for my pycharm. \r\nWhen I run my code in anaconda prompt, there's no problem for now. GPU cores are successfully utilized :)\r\nI think pycharm will not gonna be a problem if I make right environment variables settings:) I'll try first...\r\nSince I'm rly noob here, I think I have to ask sth......do I have to close this issue now then make another issue if I get stuck with pycharm? or keep it until I get through this also?", "@Barleysack ,\r\n\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status.\r\nFor  issues with Pycharm, please submit a new issue in Pycharm repo.\r\n\r\nThanks!", "> @Barleysack ,\r\n> \r\n> Could you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status.\r\n> For issues with Pycharm, please submit a new issue in Pycharm repo.\r\n> \r\n> Thanks!\r\n\r\nProblem solved by using vscode :) thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48483\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48483\">No</a>\n"]}, {"number": 48482, "title": "Model converts to TFlite but invocation fails", "body": "**System information**\r\n- OS Platform: Windows, Linux:\r\n- TensorFlow 2.4.1:\r\n\r\n**Standalone code to reproduce the issue** \r\nhttps://colab.research.google.com/drive/1f22ow0a4p1WQLdlm7AIZ0V3DHbxI8EjG\r\n\r\n**or if you like. here is the same code as in Google Colab to reproduce the problem** \r\n```\r\nfrom tensorflow.keras.layers import concatenate, Input, LSTM, Bidirectional, Embedding, Dense, TimeDistributed,SpatialDropout1D\r\nfrom tensorflow.keras.models import Model\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n# Create Tensorflow model \r\nword_in = Input(shape=(300,), name=\"input_wor\")\r\nemb_wor = Embedding(input_dim=1834, output_dim=16, input_length=300, mask_zero=True, name=\"emb_wor\")(word_in)\r\nchar_in = Input(shape=(300, 20 ,), name=\"input_char\")\r\nemb_char = TimeDistributed(Embedding(input_dim=132, output_dim=32, input_length=20, mask_zero=True, name=\"emb_char\"))(char_in)\r\nchar_enc = TimeDistributed(LSTM(units=32, return_sequences=False, recurrent_dropout=0.15, name=\"char_enc\"))(emb_char)\r\ninput_pos = Input(shape=(300, 4, ), name=\"input_pos\")\r\ninput_par = Input(shape=(300, 3, ), name=\"input_par\")\r\n\r\nx = concatenate([emb_wor, char_enc, input_pos, input_par])\r\nx = SpatialDropout1D(0.1)(x)\r\nmain_lstm = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0., recurrent_dropout=0.1, name=\"main_lstm\"))(x)\r\ninputs=[word_in,char_in, input_pos, input_par]\r\noutputs = TimeDistributed(Dense(4, activation=\"softmax\", name=\"out\"))(main_lstm)\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\r\nprint(model.summary())\r\n\r\n\r\n# Convert Model to Tensorflow Lite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nwith open(\"model.tflite\", 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n\r\n# # Install tflite_runtime\r\n# !pip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\r\n\r\nuse_tflite_runtime = False # If True then you need to first restart runtime before running this code\r\nimport numpy as np\r\n\r\nif(use_tflite_runtime):\r\n  import tflite_runtime.interpreter as tflite\r\n  interpreter =  tflite.Interpreter(model_path=\"model.tflite\")\r\nelse:\r\n  import tensorflow as tf\r\n  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\n\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nprint(\"input_details\", input_details)\r\nprint(\"output_details\", output_details)\r\n\r\n# Set random values\r\nfor i in range(len(input_details)):\r\n    x = np.random.random(input_details[i][\"shape\"])\r\n    interpreter.set_tensor(i, x.astype(input_details[i][\"dtype\"]))\r\n    print(i, input_details[i][\"name\"], input_details[i][\"shape\"], input_details[i][\"dtype\"], \"/\", x.shape)\r\n\r\n# Invoke\r\ninterpreter.invoke()\r\n```\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2021-04-12 13:24:02.507539: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2021-04-12 13:24:07.691525: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-04-12 13:24:07.691860: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n2021-04-12 13:24:07.730306: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 447 nodes (0), 564 edges (0), time = 6.973ms.\r\n  function_optimizer: Graph size after: 447 nodes (0), 564 edges (0), time = 6.713ms.\r\nOptimization results for grappler item: model_bidirectional_forward_main_lstm_while_body_19625\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\nOptimization results for grappler item: model_bidirectional_backward_main_lstm_while_cond_19891\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\nOptimization results for grappler item: model_time_distributed_1_char_enc_while_body_19340\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\nOptimization results for grappler item: model_bidirectional_forward_main_lstm_while_cond_19624\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\nOptimization results for grappler item: model_time_distributed_1_char_enc_while_cond_19339\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\nOptimization results for grappler item: model_bidirectional_backward_main_lstm_while_body_19892\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n\r\n2021-04-12 13:24:08.058077: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.\r\n2021-04-12 13:24:08.058217: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.\r\n2021-04-12 13:24:08.086587: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-04-12 13:24:08.172281: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1782] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\r\nFlex ops: FlexAll\r\nDetails:\r\n\ttf.All {device = \"\", keep_dims = false}\r\n```\r\n\r\n\r\nWhen I run the invocation using `tflite_runtime` i get this error:\r\n\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 16 (FlexAll) failed to prepare.\r\n\r\n```\r\n\r\nWhen I run the invocation using tensorflow i get this following error\r\n\r\n```\r\nRuntimeError: external/org_tensorflow/tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (300 != 1)Node number 37 (CONCATENATION) failed to prepare.\r\nNode number 49 (WHILE) failed to invoke.\r\n\r\n```\r\n\r\nEither way, the invocation fails and it seems that is has something to do with the `concatenate`  layer. \r\nI would highly appreciate an answer or eventually a solution. As you can see the model does convert, but the invocation doesn't run. I tested it on both Windows and Linux,. same problem and same error.\r\n\r\n   \r\n", "comments": ["Could you try with the tf-nightly version? We have improved for the similar cases at the recent changes.", "FYI, tf.all op is now a part of TFLite builtin op set in the tf-nightly.", "> Could you try with the tf-nightly version? We have improved for the similar cases at the recent changes.\r\n\r\nI did try and i still get the exact same error.", "If you didnt convert the model with the tf-nightly, please conduct the conversion again and re-try the model executation.", "> If you didnt convert the model with the tf-nightly, please conduct the conversion again and re-try the model executation.\r\n\r\nI just did that, i tested with `tf-nightly-2.6.0.dev20210412` and the error is exactly the same.\r\nI updated the Google Colab code where you can run and it will reproduce the exact same problem with the latest tf-nightly https://colab.research.google.com/drive/1f22ow0a4p1WQLdlm7AIZ0V3DHbxI8EjG", "Via the above gist, I confirmed that the tf-nightly version also have the same issue.", "Let me take a look at the model structure.", "I confirmed that this issue is related to the pass for lowering static tensor list. Thanks for the bug report!", "@haozha111 could you take a look at this bug report, which is related to the tensor list lowering?", "@raduslmn I landed the fix for this issue. Please make sure enable the select TF option as your conversion code and verify the fix with the tomorrow's tf-nightly version.", "https://github.com/tensorflow/tensorflow/commit/a5382257ac60dc0806c2fca7efaa5a133ee24ef6 is the fix.", "> [a538225](https://github.com/tensorflow/tensorflow/commit/a5382257ac60dc0806c2fca7efaa5a133ee24ef6) is the fix.\r\n \r\nI did try today with the latest `tf-nightly` version and still the same error. I notice that you pushed  [a538225](https://github.com/tensorflow/tensorflow/commit/a5382257ac60dc0806c2fca7efaa5a133ee24ef6) which should be the fix but you also pushed another commit [8b07beb](https://github.com/tensorflow/tensorflow/commit/8b07beb41611302c4779f60031262745cacd20d9#diff-23e5220f335dd3aa115eb368edc4c5cfea78499328f7b3d8bce900ae706130a5) that reverts the fix back to the code before the fix. Was that intentional? is there a reason for that or was it a mistake? If it was a mistake, could push again the fix as now it's still the  old code prior to the fix: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/tf_tfl_passes.cc\r\n\r\nAlso another question, will this fix also work for `tflite_runtime` as well? Currently `tflite_runtime` has the exact same problem during invocation.  ", "Sorry. The above change got rollbacked. However, we landed again with the experimental flag, `converter._experimental_lower_tensor_list_ops = True`.\r\n\r\nIf the `tflite_runtime` is based on TF 2.5 or beyonds, it will work.", "> Sorry. The above change got rollbacked. However, we landed again with the experimental flag, `converter._experimental_lower_tensor_list_ops = True`.\r\n\r\nDoes it mean that we first wait for the `tf-nightly` version and then try run the convertion again using `converter._experimental_lower_tensor_list_ops = True` flag. Is that correct?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48482\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48482\">No</a>\n", "https://github.com/tensorflow/tensorflow/commit/904b3926ed1c6c70380d5313d282d248a776baa1 is the fix. I can expect that the tomorrow's tf-nightly version will have it.", "Please disable it for your case. \r\n```converter._experimental_lower_tensor_list_ops = False```", "> [904b392](https://github.com/tensorflow/tensorflow/commit/904b3926ed1c6c70380d5313d282d248a776baa1) is the fix. I can expect that the tomorrow's tf-nightly version will have it.\r\n\r\nThe invocation works now when using Tensorflow Lite from the `tf-nightly==2.6.0-dev20210414` Thank you. @abattery \r\n\r\nSo this has solved only half of the problem, when I run the invocation using `tflite_runtime` alone I get this error:\r\n\r\n`ValueError: Op builtin_code out of range: 140. Are you using old TFLite binary with newer model?Registration failed.`\r\n\r\non model import:\r\n```\r\nimport tflite_runtime.interpreter as tflite\r\ninterpreter =  tflite.Interpreter(model_path=\"model.tflite\")\r\n```\r\n\r\nYou @abattery said that \r\n> If the `tflite_runtime` is based on TF 2.5 or beyonds, it will work.\r\n\r\nNow, i used `tf-nightly==2.6.0-dev20210414` when creating the model, theoretically this should work, but it doesn't load the model. If you want you can see the error or replicate the problem here  using this [Google Colab](https://colab.research.google.com/drive/1f22ow0a4p1WQLdlm7AIZ0V3DHbxI8EjG#scrollTo=eTYhPLmiDK6T) code\r\n\r\nIs there anything that could be done to make this model load and run invocation using `tflite_runtime`? As of now the only version that i can find is `tflite-runtime-2.5.0` is there any other versions that i could use which are more recent? \r\n\r\n", "Since you are using tf 2.6 for conversion. The tflite-runtime should have the same TF version, 2.6 or beyonds. TF 2.6 version is too early version to have the prebuilt binaries.\r\n\r\nFor now, you need to compile your own tflite-runtime or just depend on the TF pip package.", "@abattery i have now tested with the built from source `tflite-runtime` on Linux and Windows.\r\nWhen using Tflite from the full Tensorflow library then it works, but when using the built from source `tflite-runtime` then it still throws get error:\r\n\r\n`RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 20 (FlexTensorListFromTensor) failed to prepare.`\r\n\r\nYou can reproduce the same error in the Google Colab which will build from source the `tflite-runtime` and then  run the entire logic:  https://colab.research.google.com/drive/1f22ow0a4p1WQLdlm7AIZ0V3DHbxI8EjG\r\n\r\nIs there anyway this can be fixed? This is actually critical for production run.  It's fine that Tflite is now working with the full TF library but that only helps during development and model training but it's useless unless we can use `tflite-runtime` inference during production run without the loading the full TensorFlow library\r\n", "That is an intended behavior. The `tflite-runtime` package does not include the Select TF option. Please use the tensorflow package for your use case.", "@abattery will do, thank you fixing the lower tensor problem. When will Tensorflow 2.6 which includes this fix be officially released?  By the way, this issue can be closed as it is resolved thanks to you ;)    ", "I don't know when the 2.6 version will be released. FYI, the TF 2.5 version is still in the rc stage.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48482\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48482\">No</a>\n", "Invocation also fails in my case, 3DConvolution + 3DMaxPool, I was able eventually to convert them by specifying     `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]` prior to conversion; nevertheless invocation fails, with exit code \"random integer 192391234313\"", "@TimbusCalin , try this:\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter._experimental_lower_tensor_list_ops = False\r\n```"]}, {"number": 48481, "title": "Matmul with int32 datatype is not supported on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow 2.4\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version:  N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.debugging.set_log_device_placement(True)\r\n\r\na = np.random.rand(8,8)\r\nb = np.random.rand(8,8)\r\n\r\nwith tf.device('/GPU:0'):\r\n  x = tf.Variable(a, dtype=\"int32\")\r\n  y = tf.Variable(b, dtype=\"int32\")\r\n  c_GPU = tf.matmul(x, y)\r\n```\r\n\r\nOutput:\r\n```\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\n\r\nSee the [gist](https://colab.research.google.com/gist/lugalUrim/07e68fbce476c25bdd41daea6a5b24d1/matmul-for-int32.ipynb#scrollTo=EguARwuQxuIH) here.\r\n\r\n**Describe the current behavior**\r\n\r\nThe op `MatMul` (with datatype `int32`) is actually executed on CPU (as a fallback, I suppose), instead of GPU as is specified by the user.\r\n\r\n**Describe the expected behavior**\r\n\r\nCurrently `MatMul` for `float32` is well-supported on GPU. It would be great if the kernel of `MatMul` for `int32` on GPU can also be implemented.\r\nThere should be significant speedup on GPU compared with CPU implementation. Pytorch invokes `CuBLAS` to benefit from the special hardware instructions like `Tensor Core`.", "comments": ["I guess the problem doesn't have much to do with `tf.device`, because the op `MatMul` does not have int-32 support itself.\r\n\r\nRun\r\n```\r\nimport tensorflow\r\nfrom tensorflow.python.framework import kernels\r\nmatmul_kernel = kernels.get_registered_kernels_for_op('MatMul')\r\nprint(matmul_kernel)\r\n```\r\nand it turns out that `MatMul` is not registered with device_type=GPU and type=DT_INT32.", "@lugalUrim \r\nPlease share your GPU model, Cuda, cudnn version.\r\nPlease refer to [this issue](https://stackoverflow.com/questions/47362590/tensorflow-gpu-error-invalidargumenterror-cannot-assign-a-device-for-operation) with same error.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48481\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48481\">No</a>\n"]}, {"number": 48480, "title": "Lua Wrapper", "body": "I wrapped it with Swig. But, I can not wrap TF_NewWhile because of the const pointer. I do not know why it says that it is read-only, because I tested it with another C program and it compiled. If anyone can explain why it says it is read-only I would like to learn the reason it refuses to compile it.", "comments": ["@darkSKULL-labs ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n\r\nThanks!", "I don't think you can help I don't think it is a tensorflow problem.", "@darkSKULL-labs ,\r\n\r\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using.\r\n\r\nAlso, if this is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there. Thanks!", "I am pretty sure it is a problem with GCC, the swig wrapper is so simple you could make it in five seconds if you want to test it.", "This is the swig wrapper \r\n\r\n```haskell\r\n%module lua_tensorflow\r\n%{\r\n#include <tensorflow/c/c_api.h>\r\n%}\r\n\r\n%ignore TF_NewWhile;\r\n%include <tensorflow/c/c_api.h>\r\n```\r\n\r\nThe problem is GCC will not assign the result from TF_NewWhile because ti believes it is read only but it doesn't make any sense to me why it thinks that.\r\n\r\nI made a simple test in C to copy the TF_WhileParams and it copies fine, so I am fairly sure it is a GCC bug that might be fixed in GCC10.\r\n\r\n", "@darkSKULL-labs If this is not a TF problem, then please post it in Stackoverflow where there is a huge community to help each other.\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks", "Ok thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48480\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48480\">No</a>\n", "Yes, I found out what the problem was and can fix it.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yes, I just wanted to show why Google uses Swig for tensorflow. Thanks.", "Butler, you are supposed to learn now and close the thread.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48480\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48480\">No</a>\n"]}, {"number": 48479, "title": "Update the `compiler_pieces` of the RPi ARM compiler.", "body": "`arm_compiler.BUILD` is used as the build file for the Raspberry Pi Arm-v7a cross-compiler registered in the workspace here: https://github.com/tensorflow/tensorflow/blob/52e009a5118632b13672ee1d06cdaa81a2e56702/tensorflow/workspace2.bzl#L215-L224\r\n\r\n(The same build file is also used for the [aarch64 cross-compiler](https://github.com/tensorflow/tensorflow/blob/52e009a5118632b13672ee1d06cdaa81a2e56702/tensorflow/workspace2.bzl#L226-L238), but there's a separate (correct) [`aarch64_compiler_pieces`](aarch64_compiler_pieces) defined for that case.)\r\n\r\nCurrently, the filegroup `compiler_pieces` defined in this build file does not match the directory structure of the cross-compiler that's downloaded from https://github.com/rvagg/rpi-newer-crosstools. The commit that's downloaded, after applying the `strip_prefix`, has [this directory structure](https://github.com/rvagg/rpi-newer-crosstools/tree/eb68350c5c8ec1663b7fe52c742ac4271e3217c5/x64-gcc-6.5.0/arm-rpi-linux-gnueabihf). Note the 'real' directories `arm-rpi-linux-gnueabihf` and `lib/gcc/arm-rpi-linux-gnueabihf`, which don't match the current glob patterns. This PR updates the glob patterns to match.", "comments": []}, {"number": 48478, "title": "Functions attributes error \"'Functional' object has no attribute '_make_train_function'\" and eager mode and non-eager returns different logits", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/a\r\n- TensorFlow installed from (source or binary):pip3\r\n- TensorFlow version (use command below):2.4.1\r\n- Python version:3.7.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:N/a\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`\r\ndef adversarial_training(model, embedding_name, epsilon=1):\r\n\t\r\n\tif model.train_function is None:  # \u5982\u679c\u8fd8\u6ca1\u6709\u8bad\u7ec3\u51fd\u6570\r\n\t\tmodel._make_train_function()  # \u624b\u52a8make\r\n\told_train_function = model.train_function  # \u5907\u4efd\u65e7\u7684\u8bad\u7ec3\u51fd\u6570\r\n\r\n\t# \u67e5\u627eEmbedding\u5c42\r\n\tfor output in model.outputs:\r\n\t\tembedding_layer = search_layer(output, embedding_name)\r\n\t\tif embedding_layer is not None:\r\n\t\t\tbreak\r\n\tif embedding_layer is None:\r\n\t\traise Exception('Embedding layer not found')\r\n\r\n\t# \u6c42Embedding\u68af\u5ea6\r\n\tembeddings = embedding_layer.embeddings  # Embedding\u77e9\u9635\r\n\tgradients = K.gradients(model.total_loss, [embeddings])  # Embedding\u68af\u5ea6\r\n\tgradients = K.zeros_like(embeddings) + gradients[0]  # \u8f6c\u4e3adense tensor\r\n\r\n\t# \u5c01\u88c5\u4e3a\u51fd\u6570\r\n\tinputs = (\r\n\t\tmodel._feed_inputs + model._feed_targets + model._feed_sample_weights\r\n\t)  # \u6240\u6709\u8f93\u5165\u5c42\r\n\tembedding_gradients = K.function(\r\n\t\tinputs=inputs,\r\n\t\toutputs=[gradients],\r\n\t\tname='embedding_gradients',\r\n\t)  # \u5c01\u88c5\u4e3a\u51fd\u6570\r\n\r\n\tdef train_function(inputs):  # \u91cd\u65b0\u5b9a\u4e49\u8bad\u7ec3\u51fd\u6570\r\n\t\tgrads = embedding_gradients(inputs)[0]  # Embedding\u68af\u5ea6\r\n\t\tdelta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # \u8ba1\u7b97\u6270\u52a8\r\n\t\tK.set_value(embeddings, K.eval(embeddings) + delta)  # \u6ce8\u5165\u6270\u52a8\r\n\t\toutputs = old_train_function(inputs)  # \u68af\u5ea6\u4e0b\u964d\r\n\t\tK.set_value(embeddings, K.eval(embeddings) - delta)  # \u5220\u9664\u6270\u52a8\r\n\t\treturn outputs\r\n\r\n\tmodel.train_function = train_function  # \u8986\u76d6\u539f\u8bad\u7ec3\u51fd\u6570\r\npops\r\n'Functional' object has no attribute '_make_train_function'\r\nand when I turned on disable_eager_execution(), no errors pops.\r\n\r\nWhen in prediction, eager call  model(inputs) returns wrong logits when I finished training and save the model to h5 format then reloaded it through tf.kears.models.load_model. This process is done in non-eager execution mode\r\n\r\nBut in the meanwhile I transform the h5 model to tf saved model(*.pb) file and run it in non-eager mode. the correct logits appears.\r\n\r\n**Describe the expected behavior**\r\nshouldn't these two calls return the same logits?\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/bojone/bert4keras/blob/master/examples/task_sentiment_virtual_adversarial_training.py\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n`Traceback` (most recent call last):\r\n  File \"/home/pycharmProjs/bert4keras/examples/task_proxy_recognization.py\", line 217, in <module>\r\n    adversarial_training(test_model,'Embedding-Token', 0.5)\r\n  File \"/home/pycharmProjs/bert4keras/examples/task_proxy_recognization.py\", line 174, in adversarial_training\r\n    model._make_train_function()  # \u624b\u52a8make\r\nAttributeError: 'Functional' object has no attribute '_make_train_function'", "comments": ["@axelning \r\nI ran the code shared on tf 2.4 and see error due to third party dependencies, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e3d84ef25821c21f31342f11c8cace5d/untitled588.ipynb).\r\nPlease share code such that there are no third party dependencies and if the error reported is due to these, please open the issue in the respective repository.\r\nIf possible share a simple code or colab gist with the error reported due to tensorflow.", "there's somthing missing, if using tf.keras, os.environ[\"TF_KERAS\"] = \"1\" is needed before the bert4keras is imported. or else it will call native keras instead\r\n\r\nthis 3rd dependency it just called tf.keras actually and encapsulate bert related model define only , and this functions runs well in tf 2.2 , so I wonder there's some change in tf.keras.\r\n\r\nstill , if it is caused by third party dependencies, why just disable eager mode can sidestep this issue\r\n\r\nThe thing is, when in non-eager mode, the tf.keras model got _make_train_function, and in eager mode, this attribute is lost", "import tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(784,)))\r\nmodel.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax))\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\nmodel._make_train_function()\r\n\r\n\r\n\r\nruns well , when without tf.compat.v1.disable_eager_execution() the same error pops", "@axelning \r\nI ran with out \"tf.compat.v1.disable_eager_execution() \" and no error popped up, please find [gist here](https://colab.research.google.com/gist/Saduf2019/0497ab113346b00090f50ae5dbeeaa02/untitled589.ipynb)", "I ran the gist and got\r\n\r\nINFO:tensorflow:Enabling eager execution\r\nINFO:tensorflow:Enabling v2 tensorshape\r\nINFO:tensorflow:Enabling resource variables\r\nINFO:tensorflow:Enabling tensor equality\r\nINFO:tensorflow:Enabling control flow v2\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-8d086c4bccef> in <module>()\r\n      8 optimizer='rmsprop',\r\n      9 metrics=['accuracy'])\r\n---> 10 model._make_train_function()\r\n\r\nAttributeError: 'Sequential' object has no attribute '_make_train_function'", "ummm, any progress?", "@axelning I changed last line of your code from \r\n`model._make_train_function()` to `model.make_train_function()` and with that change it doesn't throw any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d45923375c3cc9988a4e93d83feaddba/untitled589.ipynb).\r\n\r\nLooks like `model._make_train_function()` is only available under `tensorflow/python/keras/engine/training_v1.py` hence it was working only when eager mode is disabled. Thanks!\r\n", "> @axelning I changed last line of your code from\r\n> `model._make_train_function()` to `model.make_train_function()` and with that change it doesn't throw any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d45923375c3cc9988a4e93d83feaddba/untitled589.ipynb).\r\n> \r\n> Looks like `model._make_train_function()` is only available under `tensorflow/python/keras/engine/training_v1.py` hence it was working only when eager mode is disabled. Thanks!\r\n@jvishnuvardhan \r\nis this the reason leading to prediction difference when in eager mode and non-eager mode?  I trained one model with non-eager mode, by self defined function through model._make_train_function() and use the model in eager mode, the logits the model output is absolutely wrong, but in non-eager mode, the logits returned normal", "@axelning Not sure about it. Please share a simple and complete standalone code to reproduce the issue. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48478\">No</a>\n"]}, {"number": 48477, "title": "Failed to designate device placement on GPU with tf.device and executed without warning", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA Version: 11.2   \r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe operation within a `with tf.device('/GPU:0'):` context runs on CPU, with no warnings received.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe operations within a `with tf.device` context should run in that device. \r\n\r\nIf the device is not available or the operations are not supported on the particular device, warnings or exceptions are expected.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.debugging.set_log_device_placement(True)\r\n\r\na = np.random.rand(2,2)\r\nb = np.random.rand(2,2)\r\n\r\nwith tf.device('/GPU:0'):\r\n\r\n  x = tf.Variable(a, dtype=tf.bfloat16)\r\n  y = tf.Variable(b, dtype=tf.bfloat16)\r\n  c_GPU = tf.matmul(x, y)\r\n  \r\n```\r\n\r\nOutputs:\r\n```\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\n\r\nIf I change the datatype from `bfloat16` to `float32`, the output is expected:\r\n```\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\n", "comments": ["@namursag \r\nPlease refer to below link, which says about GPUs support for float16 and float32, you may also refer to [[link](https://github.com/tensorflow/tensorflow/issues/21317#issuecomment-413365605) and [link1](https://stackoverflow.com/questions/51602313/bfloat16-training-in-gpus)]\r\n(NVIDIA GPUs support using a mix of float16 and float32, while TPUs support a mix of bfloat16 and float32.):\r\n\r\nhttps://www.tensorflow.org/guide/mixed_precision#supported_hardware\r\n\r\nCuda 11.2 is tested against nightly, can you downgrade to 11.0.\r\n", "@Saduf2019 After I downgrade to 11.0, the output is still the same.", "@lugalUrim \r\nCan you please open a new issue with the details of your issue as this issue is for the GPU usage for bfloat16 which is not supported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48477\">No</a>\n"]}, {"number": 48476, "title": "Conv2D for GPU is not currently supported without cudnn", "body": "**System information**\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n    TensorFlow installed from (source or binary): binary\r\n    TensorFlow version (use command below): 2.4.1\r\n    Python version: 3.7\r\n    Bazel version (if compiling from source): N/A\r\n    GCC/Compiler version (if compiling from source): N/A\r\n    CUDA/cuDNN version: N/A\r\n    GPU model and memory: N/A\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = np.random.rand(1, 4, 4, 1)\r\nb = np.random.rand(4, 4, 1, 1)\r\n\r\ntf.raw_ops.Conv2D(input=a, filter=b, strides=[1, 1, 1, 1], padding='SAME', use_cudnn_on_gpu=False)\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nRaises UnimplementedError: Conv2D for GPU is not currently supported without cudnn [Op:Conv2D].\r\n\r\n\r\n**Describe the expected behavior**\r\nExpect Conv2D to run on GPU without cudnn, like pytorch.\r\n", "comments": ["@namursag ,\r\n\r\nI ran the code shared on tf v2.5.0rc0, tf nightly and do not see any error, please find the [gist here](https://colab.research.google.com/gist/tilakrayal/3ae2f7513d110e9584fc3f7741ccc23f/48476.ipynb)", "Thanks @tilakrayal . Updating to tf-nightly solves my problem indeed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48476\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48476\">No</a>\n"]}, {"number": 48475, "title": "Fixed unlinking the shared memory region on non-Android platform", "body": "Current implementation does not unlink the shared memory region for non-Android platforms. ", "comments": ["@miaowang14 could you review this PR?", "@robert-kalmar  Can you please check @miaowang14's comments and keep us posted ? Thanks!", "@robert-kalmar Any update on this PR? Please. Thanks!"]}, {"number": 48473, "title": "Documentation for tf.tensor_scatter_nd_add mentions non existing method", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation for `tensor_scatter_nd_add` mentions `tf.scatter_nd_add`, but that method no longer exists.", "comments": ["@scarlehoff \r\nCreated pr #48536 to fix this issue, once the pr is merged this issue will be automatically closed.\r\nThanks!"]}, {"number": 48472, "title": "micro: add INT8 support to CUMSUM op", "body": "Added support for INT8 to the CUMSUM operator.\r\n\r\nReference Issue #47290", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48471, "title": "app crash when load mlmodel with modelWithContentsOfURL:configuration:error", "body": "## \u2753Question\r\nApp probabilistic crash when load mlmodel with modelWithContentsOfURL:configuration:error\uff1b\r\n\r\ncrash stack\uff1a\r\n0  Espresso                       0x1b3d38c10 Espresso::cpu_context_transfer_algo_t::cpu_context_transfer_algo_t(Espresso::cpu_context_transfer_algo_options const&) + 228\r\n1  Espresso                       0x1b3d9e13c std::__1::__shared_ptr_emplace<Espresso::cpu_context_transfer_algo_t, std::__1::allocator<Espresso::cpu_context_transfer_algo_t> >::__shared_ptr_emplace<Espresso::cpu_context_transfer_algo_options&>(std::__1::allocator<Espresso::cpu_context_transfer_algo_t>, Espresso::cpu_context_transfer_algo_options&) + 48\r\n2  Espresso                       0x1b3d9b054 Espresso::get_net_info_ir(std::__1::shared_ptr<Espresso::abstract_context>, std::__1::shared_ptr<Espresso::net>, std::__1::vector<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::allocator<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> > > const&, Espresso::network_shape const&, Espresso::compute_path const&, Espresso::platform const&, Espresso::compute_path const&, std::__1::shared_ptr<Espresso::cpu_context_transfer_algo_t>&, std::__1::shared_ptr<Espresso::net_info_ir_t>&, std::__1::shared_ptr<Espresso::kernels_validation_status_t>&) + 440\r\n3  Espresso                       0x1b3d9b450 try_dispatch(std::__1::shared_ptr<Espresso::abstract_context>, std::__1::shared_ptr<Espresso::net>, std::__1::vector<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::allocator<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> > > const&, Espresso::network_shape const&, Espresso::compute_path const&, std::__1::basic_istream<char, std::__1::char_traits<char> >*, Espresso::platform const&, Espresso::compute_path const&) + 468\r\n4  Espresso                       0x1b3d9c56c Espresso::run_dispatch_v2(std::__1::shared_ptr<Espresso::abstract_context>, std::__1::shared_ptr<Espresso::net>, std::__1::vector<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::allocator<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> > > const&, Espresso::network_shape const&, Espresso::compute_path const&, std::__1::basic_istream<char, std::__1::char_traits<char> >*) + 1680\r\n5  Espresso                       0x1b3dac470 Espresso::load_network_layers_internal(std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<Espresso::abstract_context> const&, Espresso::network_shape const&, std::__1::basic_istream<char, std::__1::char_traits<char> >*, Espresso::compute_path, bool, std::__1::shared_ptr<Espresso::blob_storage_abstract> const&) + 244\r\n6  Espresso                       0x1b3da8850 Espresso::load_and_shape_network(std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<Espresso::abstract_context> const&, Espresso::network_shape const&, Espresso::compute_path, std::__1::shared_ptr<Espresso::blob_storage_abstract> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 508\r\n7  Espresso                       0x1b3dab31c Espresso::load_network(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<Espresso::abstract_context> const&, Espresso::compute_path, bool) + 1892\r\n8  Espresso                       0x1b3ccce60 EspressoLight::espresso_plan::add_network(char const*, espresso_storage_type_t) + 304\r\n9  Espresso                       0x1b3cdb2d0 espresso_plan_add_network + 340\r\n10 CoreML                         0x1b389c36c -[MLNeuralNetworkEngine _setupContextAndPlanWithConfiguration:usingCPU:error:] + 752\r\n11 CoreML                         0x1b389d754 -[MLNeuralNetworkEngine initWithContainer:configuration:error:] + 384\r\n12 CoreML                         0x1b38a28e8 +[MLNeuralNetworkEngine loadModelFromCompiledArchive:modelVersionInfo:compilerVersionInfo:configuration:error:] + 208\r\n13 CoreML                         0x1b38f4cb0 +[MLLoader loadModelFromArchive:configuration:loaderEvent:error:] + 624\r\n14 CoreML                         0x1b38f61ec +[MLLoader loadModelFromAssetAtURL:configuration:loaderEvent:error:] + 280\r\n15 CoreML                         0x1b38f648c +[MLLoader loadModelFromAssetAtURL:configuration:error:] + 120\r\n16 CoreML                         0x1b38dcacc -[MLModelAsset load:] + 220\r\n17 CoreML                         0x1b38dc8d0 -[MLModelAsset modelWithError:] + 68\r\n18 CoreML                         0x1b391f28c +[MLModel modelWithContentsOfURL:configuration:error:] + 156\r\n19 appName                      0x105156438 XYAIBridge::CoremlInit(char const*, XYAIBridge::tag_Config const&, void**) + 4385399864\r\n\r\nLoad the mlmodel code\uff1a\r\n`  if (@available(iOS 12.0, *)) {\r\n    MLModelConfiguration* config = [MLModelConfiguration alloc];\r\n    config.computeUnits = MLComputeUnitsAll;\r\n    _model = [MLModel modelWithContentsOfURL:compileUrl configuration:config error:&error];\r\n  } else {\r\n    _model = [MLModel modelWithContentsOfURL:compileUrl error:&error];\r\n  }`\r\n\r\n## System Information\r\niPhone 11\r\niOS 14\r\n", "comments": ["@ToBigboss ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48470, "title": "Conv2DTranspose crashes with filters=0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\nSample code:\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput = np.random.rand(2, 8, 8, 8)\r\nx = tf.keras.Input([None, None, 8])\r\ny = tf.keras.layers.Conv2DTranspose(filters=0,kernel_size=3, padding='same', dilation_rate=(1,1))(x)\r\nmodel = tf.keras.Model(x, y)\r\nz = model(input).numpy()\r\nprint(z.mean())\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nThe process dies after calling `model(input)`.\r\n\r\n\r\n**Describe the expected behavior**\r\nExpect a `ValueError` raised if `filters`=`0` is not supported. It seems that conv2d supports this, for example:\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput = np.random.rand(2, 8, 8, 8)\r\nx = tf.keras.Input([None, None, 8])\r\n\r\ny = tf.keras.layers.Conv2D(0, kernel_size=3)(x)\r\nmodel = tf.keras.Model(x, y)\r\nz = model(input).numpy()\r\nprint(z.shape)\r\n```\r\n\r\noutputs `(2, 6, 6, 0)`.", "comments": ["I have tried in colab with TF v2.4,TF v2.5.0rc0, TF-nightly and noticed that session is being crashed.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/0320e88d57e19455caae287cb8e0a208/48470.ipynb) here", "Thanks for the report. I would suggest raising `ValueError` for all conv layers in case `filters` is `0`. Would you be able to open a PR implementing this change (including tests)?", "Thanks @fchollet . I agree that `ValueError` should be raised whenever `filters` is `0`. \r\nI noticed that the following functions also crash when `filters`=`0`.\r\n```\r\nConv1DTranspose\r\nConv3DTranspose\r\nConvLSTM2D\r\n```\r\n\r\nwhile these run silently, but exception should be raised actually.\r\n```\r\nConv1D\r\nConv3D\r\n```\r\n", "@fchollet I'll open a PR for this issue implementing the change.", "/cc @nikitamaia (Assign/In progress)", "@nikitamaia , @fchollet , @bhack I have sent in a [PR](https://github.com/tensorflow/tensorflow/pull/48566) for this issue. Can you please review it ?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48470\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48470\">No</a>\n"]}, {"number": 48468, "title": "Build bazel error", "body": "**System information**\r\n- OS Platform Linux Ubuntu 18.04:\r\n- TensorFlow installing from source or binary\r\n- Python version: 3.6.9\r\n- Bazel version: 3.7.2\r\n- GCC/Compiler version: 7.5.0\r\n- CUDA/cuDNN version: 10.2 / 7\r\n- GPU model and memory: GeForce GTX 1060 3GB/PCIe/SSE2\r\n\r\nAfter run ./configure, i call `bazel build --local_ram_resources=HOST_RAM*.5 --local_cpu_resources=HOST_CPUS-1 --config=cuda  --spawn_strategy=standalone --config=v2 --verbose_failures //tensorflow/tools/pip_package:build_pip_package`\r\nand after several minutes thowing error:\r\n`ERROR: /home/dinar/tensorflow/tensorflow/core/kernels/BUILD:1221:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1)`\r\n\r\nFull output:\r\n\r\n```ERROR: /home/dinar/tensorflow/tensorflow/core/kernels/BUILD:1221:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/dinar/.cache/bazel/_bazel_dinar/573fdf00de65bc899e7e19da07d6af8e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/dinar/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/dinar/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/host/bin/external/mkl_dnn_v1 -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/mkl_dnn_v1/include -isystem bazel-out/host/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/host/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/inplace_ops_functor_gpu.cu.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/com_google_absl/absl/functional/function_ref.h:100:29: error: parameter packs not expanded with '...':\r\n   template <typename F, typename = EnableIfCompatible<const F&>>\r\n                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                \r\nexternal/com_google_absl/absl/functional/function_ref.h:100:29: note:         'Args'\r\nexternal/com_google_absl/absl/functional/function_ref.h:114:13: error: parameter packs not expanded with '...':\r\n       typename F, typename = EnableIfCompatible<F*>,\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                \r\nexternal/com_google_absl/absl/functional/function_ref.h:114:13: note:         'Args'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/dinar/tensorflow/tensorflow/python/tools/BUILD:225:10 C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/dinar/.cache/bazel/_bazel_dinar/573fdf00de65bc899e7e19da07d6af8e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/dinar/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/dinar/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/host/bin/external/mkl_dnn_v1 -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/mkl_dnn_v1/include -isystem bazel-out/host/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/host/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/inplace_ops_functor_gpu.cu.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 2006.565s, Critical Path: 81.30s\r\nINFO: 519 processes: 53 internal, 466 local.\r\nFAILED: Build did NOT complete successfully```\r\n", "comments": ["Now i run `bazel clean --expunge` \r\nand after this call \r\n`bazel build --config=cuda --config=opt --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package`\r\nand get this error:\r\n```\r\n C++ compilation of rule '//tensorflow/core/kernels:snapshot_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/snapshot_op_gpu/snapshot_op_gpu.cu.pic.d ... (remaining 259 argument(s) skipped)\r\nexternal/com_google_absl/absl/functional/function_ref.h:100:29: error: parameter packs not expanded with '...':\r\n   template <typename F, typename = EnableIfCompatible<const F&>>\r\n\r\n```", "@watermellon2018 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\n\r\n\r\nThanks!", "FYR, I met this problem on TensorFlow master branch after a5ad7ef2 on nvidia Jetson Xavier NX running [JetPack 4.5.1](https://developer.nvidia.com/embedded/jetpack) (Ubuntu 18.04.5, CUDA 10.2, CuDNN 8.0, TensorRT 7.1.3). So this could be a master + cuda 10.2 question after a5ad7ef2.", "-  before a5ad7ef: no such problem\r\n- (a5ad7ef, e589241]: comment out the patch as suggested by the comment should work\r\n```diff\r\ndiff --git a/third_party/absl/workspace.bzl b/third_party/absl/workspace.bzl\r\nindex e1987f475c3..25834eac99e 100644\r\n--- a/third_party/absl/workspace.bzl\r\n+++ b/third_party/absl/workspace.bzl\r\n@@ -15,7 +15,7 @@ def repo(name):\r\n         build_file = \"//third_party/absl:com_google_absl.BUILD\",\r\n         # TODO: Remove the patch when https://github.com/abseil/abseil-cpp/issues/326 is resolved\r\n         # and when TensorFlow is build against CUDA 10.2\r\n-        patch_file = \"//third_party/absl:com_google_absl_fix_mac_and_nvcc_build.patch\",\r\n+        # patch_file = \"//third_party/absl:com_google_absl_fix_mac_and_nvcc_build.patch\",\r\n         strip_prefix = \"abseil-cpp-{commit}\".format(commit = ABSL_COMMIT),\r\n         urls = [\r\n             \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/{commit}.tar.gz\".format(commit = ABSL_COMMIT),\r\n\r\n```\r\nsomewhere after e589241: something is wrong. I don't know how to make it work yet. Either commenting out the patch or not doesn't work for me.", "`git bisect` told me the culprit is 07665aa311d6a157f18d36489de67f4a258811a0. A quick fix is reverting 07665aa3.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48468\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48468\">No</a>\n"]}, {"number": 48467, "title": "tf.math.acos should accept only float/double inputs", "body": "OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version: 2.4.1\r\nPython version: 3.7\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\n\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/acos\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Parameters defined\r\n\r\nThe input tensor `x` can not be bfloat16, half, uint8, int8, int16, int32, int64, complex64, complex128, string. In fact, only float32 and float64 is supported now.\r\n\r\n### Usage example\r\n\r\n```\r\nimport tensorflow as tf\r\na = tf.constant([1.0], dtype = tf.half)\r\ntf.math.acos(a)\r\n```\r\n\r\nOutput:\r\nNotFoundError: Could not find device for node: {{node Acos}} = Acos[T=DT_INT8]\r\nAll kernels registered for op Acos:\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n [Op:Acos]", "comments": ["@lugalUrim ,\r\n\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n\r\nThanks!", "Updated. Thanks. @tilakrayal ", "I am able to replicate the issue on tf 2.4 and nightly but the issue is not reproducible on tf 2.3, please find the [gist](https://colab.research.google.com/gist/tilakrayal/b4a87b4a6a7d459e9cc39cb3ecc4756c/48467.ipynb) here.", "This is fixed with TF 2.6 please refer colab [gist](https://colab.research.google.com/gist/ymodak/7336b5b143ee04f7c8e88d50eb51e606/untitled3.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48467\">No</a>\n"]}, {"number": 48466, "title": "keras.layers.Dense should not have float units", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version:  N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ntf.keras.layers.Dense accepts a floating `units` and successfully inits.\r\n\r\n**Describe the expected behavior**\r\nExpect 'ValueError' because 'units' should be a positive integer.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nlayer = tf.keras.layers.Dense(units=3.3)\r\nlayer(tf.ones([1, 3]))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@lugalUrim \r\nDon't worry,  it floors to an integer.\r\n```python\r\nlayer.get_config()\r\n\r\n #{'activation': 'linear',\r\n #'activity_regularizer': None,\r\n # 'bias_constraint': None,\r\n #'bias_initializer': {'class_name': 'Zeros', 'config': {}},\r\n #'bias_regularizer': None,\r\n #'dtype': 'float32',\r\n #'kernel_constraint': None,\r\n #'kernel_initializer': {'class_name': 'GlorotUniform',\r\n #'config': {'seed': None}},\r\n #'kernel_regularizer': None,\r\n #'name': 'dense_1',\r\n #'trainable': True,\r\n #'units': 3,\r\n #'use_bias': True}\r\n```", "Thanks for pointing this out! @AdityaKane2001 \r\nI suppose that it would be better if the document (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) can be changed to make this clear for everyone, because now it says that the `units` should be a positive integer.\r\n", "It needs to be a positive integer, the developers may have just made the provision in case of an typo or something.", "@lugalUrim \r\n\r\nCould you please confirm if the issue is resolved? if yes, please feel free to move this issue to closed status.\r\n\r\n\r\n\r\n", "Thanks. I will close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48466\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48466\">No</a>\n"]}, {"number": 48465, "title": "The `.adapt()` method should be called `.fit()` for TextVectorization", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, to train or fit a text vectorizer in `tf.keras.layers.experimental.preprocessing.TextVectorization`, we have to call a weird `.adapt()` method which no one use this name in standard machine learning. It is more commonly to use the name `.fit()` like in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit). I suggest to name the method as `.fit()` so it will be more familiar to people who just came to tensorflow.\r\n\r\n**Will this change the current api? How?** Yes, there will be a new `.fit()` method which is exactly the `.adapt()` method.\r\n\r\n**Who will benefit with this feature?**\r\nNew beginner to tensorflow who are familiar with other ML framework but just came to tensorflow to use this awesome framework.\r\n\r\n**Any Other info.**\r\n", "comments": ["Thanks for reaching out. `fit()` is already used for model training (via backprop), and the usage of `adapt()` in preprocessing layers is quite different. During the design phase of the preprocessing layers API, we discussed this exact question, and we decided that using a different name for `adapt()` would be clearer than using `fit()`, which would lead to confusion."]}, {"number": 48464, "title": " Int incompatibility error on building TensorFlowLiteC framework", "body": "# System information\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.2.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): github repository\r\n- TensorFlow version: github repository\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: directly from standard terminal\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): Apple clang version 12.0.0 (clang-1200.0.32.29)\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n# Problem description\r\n\r\nI tried to build TensorFlow Lite as suggested here: \r\nBuild TensorFlowLiteC dynamic framework (recommended)\r\nhttps://www.tensorflow.org/lite/guide/build_ios#build_tensorflowlitec_dynamic_framework_recommended\r\nand ended up with int vs int32 incompatibility error\r\n\r\n# Steps\r\n\r\n1. Install Bazel\r\n2. Clone TensoFlow repository from github\r\n3. Run `.configure`\r\n4. Start `bazel build`\r\n\r\nThe build is ending up with `error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'`\r\n\r\nI tried on two different MacBooks with M1 and Intel chips.\r\n\r\nThe issues seem to come from here:\r\nhttps://github.com/google/XNNPACK/issues?q=is%3Aissue+int\r\nbut I can't find any appropriate one.\r\n\r\nCould you suggest why it is happened and how the TensorFlowLiteC dynamic framework can be built?\r\n\r\n# Logs\r\n\r\nLog part:\r\n\r\nexternal/XNNPACK/src/qs8-gemm/gen/1x8c4-minmax-neondot.c:63:18: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n      vacc0x0123 = vdotq_lane_s32(vacc0x0123, vb0123x0123, va0x01234567, 0);\r\n                 ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nFull log:\r\n[tflite_install.log](https://github.com/tensorflow/tensorflow/files/6292923/tflite_install.log)\r\n\r\n", "comments": ["@Maratyszcza @multiverse-tf could you take a look at this?", "> # System information\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.2.2\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n> * TensorFlow installed from (source or binary): github repository\r\n> * TensorFlow version: github repository\r\n> * Python version: 3.8.2\r\n> * Installed using virtualenv? pip? conda?: directly from standard terminal\r\n> * Bazel version (if compiling from source): 3.7.2\r\n> * GCC/Compiler version (if compiling from source): Apple clang version 12.0.0 (clang-1200.0.32.29)\r\n> * CUDA/cuDNN version: n/a\r\n> * GPU model and memory: n/a\r\n> \r\n> # Problem description\r\n> I tried to build TensorFlow Lite as suggested here:\r\n> Build TensorFlowLiteC dynamic framework (recommended)\r\n> https://www.tensorflow.org/lite/guide/build_ios#build_tensorflowlitec_dynamic_framework_recommended\r\n> and ended up with int vs int32 incompatibility error\r\n> \r\n> # Steps\r\n> 1. Install Bazel\r\n> 2. Clone TensoFlow repository from github\r\n> 3. Run `.configure`\r\n> 4. Start `bazel build`\r\n> \r\n> The build is ending up with `error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'`\r\n> \r\n> I tried on two different MacBooks with M1 and Intel chips.\r\n> \r\n> The issues seem to come from here:\r\n> https://github.com/google/XNNPACK/issues?q=is%3Aissue+int\r\n> but I can't find any appropriate one.\r\n> \r\n> Could you suggest why it is happened and how the TensorFlowLiteC dynamic framework can be built?\r\n> \r\n> # Logs\r\n> Log part:\r\n> \r\n> external/XNNPACK/src/qs8-gemm/gen/1x8c4-minmax-neondot.c:63:18: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n> vacc0x0123 = vdotq_lane_s32(vacc0x0123, vb0123x0123, va0x01234567, 0);\r\n> ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> \r\n> Full log:\r\n> [tflite_install.log](https://github.com/tensorflow/tensorflow/files/6292923/tflite_install.log)\r\n\r\nAccording to this [comment](https://github.com/google/XNNPACK/issues/1390#issuecomment-814600788), the issue should have been fixed on the XNNPACK side by this commit https://github.com/google/XNNPACK/pull/1391/commits/60fc61373f21f0ad3164cc719de464f4b787dc04. Could you help with the verification? Thx!\r\n\r\nBtw, you need to change https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzl to update the XNNPACK to the proper commit for the testing.", "@multiverse-tf thank you for the tip with updating of `workspace2.bzl`. I am compiling tensorflow for the first time, so it was not so obvious for me.\r\n\r\nSo what I did: \r\n\r\n1. I retrieved the commit ID from https://github.com/google/XNNPACK/commit/60fc61373f21f0ad3164cc719de464f4b787dc04\r\n2. It is `60fc61373f21f0ad3164cc719de464f4b787dc04`\r\n3. I replaced commit ID in 3 places: `strip_prefix` and two `urls`\r\n4. I got and updated `sha256` based on the second URL\r\n\r\nThe final part looks like this, starting from line 121:\r\n``` bzl\r\n# To update any of the dependencies bellow:\r\n# a) update URL and strip_prefix to the new git commit hash\r\n# b) get the sha256 hash of the commit by running:\r\n#    curl -L <url> | sha256sum\r\n# and update the sha256 with the result.\r\ntf_http_archive(\r\n    name = \"XNNPACK\",\r\n    sha256 = \"27736d765efbac5a8077862515b7212bd9a5cfa281ab06f4c0a27c3fc445c58e\",\r\n    strip_prefix = \"XNNPACK-60fc61373f21f0ad3164cc719de464f4b787dc04\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/60fc61373f21f0ad3164cc719de464f4b787dc04.zip\",\r\n        \"https://github.com/google/XNNPACK/archive/60fc61373f21f0ad3164cc719de464f4b787dc04.zip\",\r\n    ],\r\n)\r\n```\r\n\r\nI also noticed, that from these two URLs only the second one is operational.\r\nFor the first URL I see:\r\n<img width=\"987\" alt=\"Screenshot 2021-04-12 at 19 09 08\" src=\"https://user-images.githubusercontent.com/38891920/114433859-99217700-9bc2-11eb-9ff5-5b7a9cb9cac8.png\">\r\n\r\nBefore starting the build I also cleared the folder `/private/var/tmp/_bazel_user`\r\n\r\nAnyway the build failed with almost the same results.\r\n\r\nHere is the warning regarding missing commit:\r\n<img width=\"721\" alt=\"Screenshot 2021-04-12 at 19 39 16\" src=\"https://user-images.githubusercontent.com/38891920/114437408-d0922280-9bc6-11eb-978d-f313292ebb96.png\">\r\n\r\nFull log:\r\n[tflite_install2.log](https://github.com/tensorflow/tensorflow/files/6298726/tflite_install2.log)\r\n\r\nDo you have any advice how to proceed further?", "The error is due to compiler lacking `vdotq_lane_s32` instructions. If you're building for iOS, please try a newer version of XCode. If you're building for Android, please try Android NDK r19 or later.", "@Maratyszcza thank you for the advice!\r\n\r\nI am building iOS version and currently using the latest stable version of XCode:\r\n\r\n```\r\n% xcodebuild -version\r\nXcode 12.4\r\nBuild version 12D4e\r\n```\r\n<img width=\"525\" alt=\"Screenshot 2021-04-12 at 22 41 45\" src=\"https://user-images.githubusercontent.com/38891920/114459036-48207b80-9be0-11eb-8040-73edc8ee79a6.png\">\r\n\r\nhttps://en.wikipedia.org/wiki/Xcode\r\n<img width=\"325\" alt=\"Screenshot 2021-04-12 at 22 39 52\" src=\"https://user-images.githubusercontent.com/38891920/114458851-0bed1b00-9be0-11eb-9b8e-ef573e1e8275.png\">\r\n\r\nWould you suggest to use beta 12.5 beta 3 (12E5244e)?\r\n\r\nFor me it is also OK to not have the latest version of TensorFlow. What do you think if I get the previous release, could it be built using the stable version of XCode? Do you have any certain release in mind? \r\n", "From the report in google/XNNPACK#1402 it looks like Apple toolchain for AArch32 doesn't support neon-dot intrinsics at all. I'm working on removing these microkernels from iOS ARMv7 builds.", "99db61328e4d63ba7c1a35827290b504bebcccb6 is supposed to fix this issue", "@Maratyszcza thank you!\r\n\r\nI made `git checkout 99db61328e4d63ba7c1a35827290b504bebcccb6` and then repeated the steps as described [on the page](https://www.tensorflow.org/lite/guide/build_ios#build_tensorflowlitec_dynamic_framework_recommended).\r\n\r\n```\r\nbazel build --config=ios_fat -c opt \\\r\n  //tensorflow/lite/ios:TensorFlowLiteC_framework\r\n```\r\nThe build has been completed without errors.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48464\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48464\">No</a>\n"]}, {"number": 48463, "title": "Error: Invalid argument: Quantized tensors must have non-zero scales", "body": "Hi, \r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): Nvidia docker image (TF1.15)\r\n\r\n\r\n### 2. Code\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/6292072/model.zip)\r\n\r\n\r\n**Transformation from Checkpoints (used scripts from the Object detection API, see model.zip)**\r\n\r\n```\r\npython export_tflite_ssd_graph.py --pipeline_config_path=/Studienarbeit/tensorflow_od/models/Variation79/mobilnetv2_variation79.config --trained_checkpoint_prefix=/Studienarbeit/tensorflow_od/models/Variation79/model_checkpoints/model.ckpt-176720 --output_directory=exported-models/Variation79    --add_postprocessing_op=true\r\n```\r\n\r\n```\r\ntflite_convert --graph_def_file=/Studienarbeit/tensorflow_od/exported-models/Variation79/tflite_graph.pb  --output_file=/Studienarbeit/tensorflow_od/exported-models/Variation79/detect.tflite --input_shapes=1,360,640,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8  --mean_values=128 --std_dev_values=127 --allow_custom_ops\r\n```\r\n\r\n### 3. Failure after conversion\r\nI'm trying to convert a QAT trained Model from the TF1 Object Detection API (MobileNetV2-SSDLite) into a working model for the USB Edge TPU. It is working on some models fine but on others I get the following error when I try to convert the .tflite model into a .tflite model for the TPU.  I used the Command Line tool.\r\n\r\n```\r\nEdge TPU Compiler version 15.0.340273435\r\nloc(\"BoxPredictor_4/BoxEncodingPredictor/Conv2D_bias\"): error: Invalid argument: Quantized tensors must have non-zero scales\r\nerror: could not translate function : Quantized tensors must have non-zero scales\r\n```\r\nI can see in Netron that the Graph contains for the Bias no quantization term. \r\nIn this issue #https://github.com/tensorflow/tensorflow/issues/46334  they found a solution or a work around with the option _(converter._experimental_new_quantizer=True)_. \r\nUnfortunatly it is just working with newer tf2 versions but i use tf1.15. \r\n\r\n**Question:**\r\n\r\n- Is there a other solution to solve the error?\r\n- Can I use models trained with tf1.15 and convert them in e.g. tf2.5.0 with the option _(converter._experimental_new_quantizer=True)_?\r\n\r\n### 5. Extra Information\r\n\r\nHere you can see that the quantization conversion term is missing for the Bias. For alle the other Conv2D there are quantization terms for the BIAS values.\r\n![error_netron](https://user-images.githubusercontent.com/74291692/114309180-a6693380-9ae6-11eb-937f-3a3a62287b14.png)\r\n\r\nHere how it should look like.\r\n![correct_bias](https://user-images.githubusercontent.com/74291692/114309669-43789c00-9ae8-11eb-885d-66e1e7389d70.png)\r\n", "comments": ["@Xhark could you triage this issue?", "> Can I use models trained with tf1.15 and convert them in e.g. tf2.5.0 with the option _(converter.experimental_new_quantizer=True)?\r\n\r\nYes, you can decouple the training jobs and conversion jobs. If the model is converted with the recent TF version, the converted model should be executed with the recent TF version.", "- To use TF 2.x converter, you may have to load your model with TF 2.x Object Detection API and export it to saved model format using [export_tflite_graph_tf2.py](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py)\r\n\r\n- You still have to do a lot of steps to get actual quantized model for EdgeTPU. more context is [here](https://github.com/tensorflow/models/issues/9371).\r\n", "Thank's for the fast reply. \r\nI worked on it last evening and this is my result.\r\nYou can convert (mostly) TF 1.15 QAT trained models with TF 2.4.0. I use a docker image from [nvidia](https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) because of the GPU support.\r\n\r\n1. \r\nAt first I converte in my docker image for the model training (nvidia container with TF1.15 and Object Detection API installed) the model checkpoint with the _export_tflite_ssd_graph.py_ in a TFLite suitable frozen_graph format (tflite_graph.pb). The file is located inside the Object Detection API.\r\n\r\n2.\r\nAfter this step I use the image with TF2.4.0 for the conversion to a fully quantized model. For the conversion I this python script which I created based on this [information ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#checkpoints).\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Convert the model\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('/Studienarbeit/tensorflow_od/exported-models/Variation74/tflite_graph.pb',\r\ninput_arrays=['normalized_input_image_tensor'],\r\ninput_shapes={'normalized_input_image_tensor' : [1,300,300,3]},\t\t#[1,360,640,3]\r\noutput_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'],\r\n) # \r\n\r\nconverter.quantized_input_stats = {'normalized_input_image_tensor' : (128, 127)}  # mean, std_dev (input range is [, ])\r\n#converter.inference_type = tf.int8 # conversion possible, also to an edgetpu.tflite model, but error at inference\r\nconverter.inference_input_type=tf.uint8 \r\nconverter.inference_output_type=tf.uint8 # optional\r\nconverter._experimental_new_quantizer = True\r\nconverter.allow_custom_ops = True\r\ntflite_quant_model = converter.convert()\r\nwith open('/Studienarbeit/tensorflow_od/exported-models/Variation74/test.tflite', 'wb') as w:\r\n\tw.write(tflite_quant_model)\r\nprint(\"Full Integer Quantization complete! - test.tflite\")\r\n```\r\nThe test.tflite file isn't completly identically to the models converted with the TFLite Command Line Tool in TF1.15 (at least the Netron Graph looks a liitle bit different). But the modell is fully quantized (only the NMS part is not but this is espected and ok).\r\n\r\n3.\r\nNow it is possible to use the Edge TPU Compiler to convert the test.tflite into a working Graph for the EDGE TPU (tested with different Models and different Image resolution). The inference speed is the same. The only difference is that know the model contains two extra Dequantize Operations a the end (see images down below.)\r\n\r\nAs prove I add some Images down below and the model files as [**model files as zip**](https://github.com/tensorflow/tensorflow/files/6294854/model_new.zip).\r\n\r\n\r\n**Old .tflite graph with error ** (TensorFlow Lite v3, TOCO Converted, runtime:1.5.0)\r\n\r\n![detect tflite](https://user-images.githubusercontent.com/74291692/114359611-710b2700-9b74-11eb-9d10-153d15515b21.png)\r\n\r\n**New .tflite graph working** (TensorFlow Lite v3, MLIR Converted., runtime:1.14.0)\r\n\r\n![test tflite](https://user-images.githubusercontent.com/74291692/114359743-926c1300-9b74-11eb-88da-d686db3dfae8.png)\r\n\r\n**Final graph Edge TPU (it is working)**\r\n![test_edgetpu tflite](https://user-images.githubusercontent.com/74291692/114359791-9bf57b00-9b74-11eb-8750-1d490ffe12dd.png)\r\n\r\n----------------------------------------------------------------------------------------------------------\r\n**Problem:**\r\n\r\nUnfortunatly I get still an error for one QAT trained model (InceptionV2-SSD with 640x360 resolution). The conversion with TFlite is working but the Conversione with the TPU Compiler ist just aborting without a clear error code (Edge TPU Compiler version 15.0.340273435; Internal compiler error. Aborting!). \r\nRight now I'm not sure if the problem is the Compiler or the .tflite model.", "Hi @DemolationPeter, the new quantizer was indeed experimental in the TF 2.4.0, but enabled by default on TF 2.5.0 and works well by default. Can you try once again with TF 2.5.0 or nightly? Thanks!", "Hi @teijeong, now I tried it with the TF2.5.0 and used the offical docker image (tensorflow/tensorflow:2.5.0rc0-gpu). The conversion is working fine for all previous working models with the difference that I can delete _converter.experimental_new_quantizer = True_. (as expected).\r\nThe conversion for my InceptionV2-SSD model is working (I get again a .tflite file) and the Graph looks fine in _netron_ to me. ([model.zip](https://github.com/tensorflow/tensorflow/files/6303555/model.zip)\r\nUnfortunatly the Edge TPU Compiler is still aborting the conversion for this specific model..\r\nMaybe there is a problem with the Edge TPU Compiler?\r\nRight know I'm retraining the model maybe this will solve the issue (you never know). \r\n\r\n**Update:**\r\nUnfortuantly retraining the Model isn`t solving the Problem. I will also open an issue for the Edge-TPU Compiler.  Unfortunatly you don`t get an clear error message form the Edge-TPU Compiler. .\r\n\r\nClosing the issue due to error in EdgeTPU Compiler."]}, {"number": 48462, "title": "Tensorflow error messages are verbose, frustrating and hard to understand", "body": "I run into weird and verbose errors that are hard to understand all the time, and it always takes me tons of time to debug into it, however actually, even debugging is also frustrating. \r\n\r\nI don't know if other people feel the same.\r\n\r\nI'm not actually talking about some specific type of error messages and it is a common issue which might be blamed to the computation graph tensorflow builds and causes that `model.fit` could lead to tons of types of errors.\r\n\r\nI hesitated for a long time, thinking about whether I should submit it here as an issue.\r\n\r\n### Expectation\r\n\r\n- The error stacks should not always from `model.fit`\r\n- `ValueError: in user code:` should list real **user code** instead of verbose and detailed implementation of keras. There might be some mechanism to track the execution of graph flow and show users the root cause.", "comments": ["@kaelzhang ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48461, "title": "pipenv shell not working", "body": "I have been trying to call \"pipenv shell\" but it gives this error:|\r\n                                                                                                 \\/\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Scripts\\pipenv.exe\\__main__.py\", line 7, in <module>\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\pipenv\\vendor\\click\\decorators.py\", line 73, in new_func\r\n    return ctx.invoke(f, obj, *args, **kwargs)\r\n  File \"C:\\Users\\yaseen\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pipenv\\cli\\command.py\", line 428, in shell\r\n    do_shell(\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pipenv\\core.py\", line 2356, in do_shell\r\n    ensure_project(\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pipenv\\core.py\", line 576, in ensure_project\r\n    ensure_virtualenv(\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pipenv\\core.py\", line 498, in ensure_virtualenv\r\n    python = ensure_python(three=three, python=python)\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pipenv\\core.py\", line 388, in ensure_python\r\n    path_to_python = find_a_system_python(python)\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pipenv\\core.py\", line 350, in find_a_system_python\r\n    return next(iter(finder.find_all_python_versions()), None)\r\n  File \"c:\\users\\yaseen\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pipenv\\vendor\\pythonfinder\\pythonfinder.py\", line 328, in find_all_python_versions\r\n    path_list = sorted(versions, key=version_sort, reverse=True)\r\nAttributeError: 'NoneType' object has no attribute 'version_sort'", "comments": ["This is not a TF error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48461\">No</a>\n", "@TaahaKhan You may want to post this issue on https://github.com/pypa/pipenv/issues repository if still need help, Since the error you are facing is not TF related."]}, {"number": 48460, "title": "Keras models latency grows too fast after converting to TFLite", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: custom code.\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: IOS 14.4.2, Android 11.\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: iPad 5, Pixel 5.\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**: 2.4.1\r\n-   **Python version**: 3.7.10\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nIn my code, I use the default keras.applications.MobileNetV2 model and then add some layers on top. The latency of MobileNetV2 in Google Colab (GPU) is about 0.046 s, with my layers it increases to 0.048 s (about 4%). Then I convert both models to TFLite and check on my smartphone's GPU. The latency of MobileNetV2 is about 0.065 s, but when layers are added, it increases to 0.11 s (about 40% !!!). What could be the reason for this behavior?\r\n\r\n### Source code / logs\r\nColab Notebook to reproduce: https://colab.research.google.com/drive/18qryA4hPu-uCEyFggoyWRkW_ouT8024A?usp=sharing\r\n", "comments": ["What kind of layers were newly introduced? I suspect that there might be not unsupported ops in the newly attached layer from GPU delegate.", "> What kind of layers were newly introduced? I suspect that there might be not unsupported ops in the newly attached layer from GPU delegate.\r\n\r\nI applied keras.layers.experimental.preprocessing.Resizing and keras.layers.Conv2D, But I get the same results with keras.layers.Conv2D only.\r\nHere is the simple code snippet: https://colab.research.google.com/drive/18qryA4hPu-uCEyFggoyWRkW_ouT8024A?usp=sharing", "Did you resolve the problem?"]}, {"number": 48459, "title": "fatal error: tensorflow/core/framework/types.pb.h: No such file or directory", "body": "\r\n**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 20.04 LTS):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version: r2.4\r\n- Python version: 3.8, but I did not build for it\r\n- Bazel version (if compiling from source): bazel release 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc-7\r\n- CUDA/cuDNN version: CUDA Compute: 7.5, CUDA Toolkit 11.1, cuDNN: 8.1.0.77\r\n- GPU model and memory: RTX 2060, 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\ntensorflow/core/framework/types.pb.h is not found in bazel-bin directory.\r\nQuoting this issue: #44602\r\nCMake build tthrows this following error:\r\nbuild] In file included from /home/aswath/Downloads/tensorflow/tensorflow/core/framework/tensor.h:24,\r\n[build]                  from /home/aswath/Downloads/tensorflow/tensorflow/cc/framework/ops.h:21,\r\n[build]                  from /home/aswath/Downloads/tensorflow/tensorflow/cc/client/client_session.h:24,\r\n[build]                  from /home/aswath/Downloads/tfcc/tfcc.cpp:1:\r\n[build] /home/aswath/Downloads/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory\r\n**[build]    22 | #include \"tensorflow/core/framework/types.pb.h\"**\r\n\r\nCMakeLists.txt:\r\n```\r\ninclude_directories(\r\n~/Downloads/tensorflow/\r\n~/Downloads/tensorflow/tensorflow\r\n~/Downloads/tensorflow/third_party\r\n~/Downloads/tensorflow/third_party/**abseil-cpp-master** \r\n${OpenCV_INCLUDE_DIRS}\r\n${EIGEN3_INCLUDE_DIR}\r\n)\r\n\r\nset(LIBS\r\n~/Downloads/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so.2\r\n~/Downloads/tensorflow/bazel-bin/tensorflow/libtensorflow_framework.so.2.4.1\r\n${OpenCV_LIBS}\r\n)\r\n\r\nadd_executable(tfcc tfcc.cpp)\r\n\r\ntarget_link_libraries(tfcc\r\n${LIBS}\r\n)\r\n```\r\n**abseil-cpp-master:**\r\nI had this error previously: cannot open source file \"absl/strings/string_view.h\" (dependency of \"tensorflow/cc/client/client_session.h\")C/C++(1696)\r\nI solved this by cloning this repo into \"third-party\" folder: https://github.com/abseil/abseil-cpp\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n~/Downloads/tensorflow$ git clone https://github.com/tensorflow/tensorflow.git\r\n~/Downloads/tensorflow$ git checkout r2.4\r\n~/Downloads/tensorflow$ ./configure \r\n~/Downloads/tensorflow$ bazel build -c opt --config=monolithic --jobs=1 //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\n**Any other info / logs**\r\n**1)Configuring with bazel**\r\n```\r\n~/Downloads/tensorflow$ ./configure \r\n\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.8/dist-packages\r\n  /home/aswath/slam_ws/devel/lib/python3/dist-packages\r\n  /home/aswath/Capstone_Project/catkin_ws/devel/lib/python3/dist-packages\r\n  /usr/lib/python3/dist-packages\r\n  /opt/ros/noetic/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]\r\n/usr/lib/python3/dist-packages\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.1 in:\r\n    /usr/local/cuda-11.1/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.1/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/local/cuda-11.1/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.1/targets/x86_64-linux/include\r\nFound TensorRT 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include/x86_64-linux-gnu\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5]: 7.5\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-7\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN support for Aarch64.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\n**2)Building tensorflow_cc with Bazel**\r\n\r\n```\r\naswath@aswath:~/Downloads/tensorflow$ bazel build -c opt --config=monolithic --jobs=1 //tensorflow:libtensorflow_cc.so\r\n\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=166\r\nINFO: Reading rc options for 'build' from /home/aswath/Downloads/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/aswath/Downloads/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/aswath/Downloads/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --config=tensorrt --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:/home/aswath/ORB_SLAM2_CUDA/Examples/ROS/ORB_SLAM2_CUDA/build/devel/lib:/home/aswath/slam_ws/devel/lib:/home/aswath/Capstone_Project/catkin_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/aswath/Downloads/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:tensorrt in file /home/aswath/Downloads/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file /home/aswath/Downloads/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:monolithic in file /home/aswath/Downloads/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /home/aswath/Downloads/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Analyzed target //tensorflow:libtensorflow_cc.so (224 packages loaded, 22536 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow:libtensorflow_cc.so up-to-date:\r\n  bazel-bin/tensorflow/libtensorflow_cc.so\r\n**\r\nINFO: Elapsed time: 29923.294s, Critical Path: 161.20s\r\nINFO: 16233 processes: 16233 local.\r\nINFO: Build completed successfully, 24186 total actions\r\n**\r\n```\r\n\r\n**3) I listed out the locations of types.pb.h:**\r\n```\r\naswath@aswath~$ locate tensorflow/core/framework/types.pb.h\r\n\r\n/home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/framework/types.pb.h\r\n/home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/framework/types.pb.h\r\n/home/aswath/.cache/bazel/_bazel_aswath/39c920a04757f746432c7b023a8d9b9f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/framework/types.pb.h\r\n/home/aswath/.cache/bazel/_bazel_aswath/39c920a04757f746432c7b023a8d9b9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/framework/types.pb.h\r\n/home/aswath/.cache/bazel/_bazel_aswath/39c920a04757f746432c7b023a8d9b9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/include/tensorflow/core/framework/types.pb.h\r\n/home/aswath/anaconda3/envs/rosconda/lib/python3.8/site-packages/tensorflow/include/tensorflow/core/framework/types.pb.h\r\n```\r\n\r\n**4) As you can see above, types.pb.h is not in bazel-bin directory, please tell me where I have gone wrong. Thank you**\r\nBazel-bin contains the .so files:\r\n```\r\nbazel-bin/tensorflow$ ls\r\nc  cc  compiler  core  libtensorflow_cc.so  libtensorflow_cc.so.2  libtensorflow_cc.so.2.4.1  libtensorflow_cc.so.2.4.1-2.params\r\n```\r\n[bazel_tensorflowcc_build_logs.txt](https://github.com/tensorflow/tensorflow/files/6291200/bazel_tensorflowcc_build_logs.txt)\r\n\r\n", "comments": ["@aswath8 ,\r\n\r\nTensorFlow v2.4 is built and tested against CUDA 11.0 and cuDNN 8. Could you please update the CUDA and cuDNN packages on your machine to compatible versions and check if you are facing same issue.For more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nAnd also please refer [#46837](https://github.com/tensorflow/tensorflow/issues/46837) for similar issue.It helps.\r\n\r\nThanks!", "Okay, [Resolution](https://github.com/tensorflow/tensorflow/issues/46837#issuecomment-773536957) that is mentioned here works. Also, it works with my CUDA Toolkit 11.1 and cuDNN 8.1.0.77", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48459\">No</a>\n"]}, {"number": 48458, "title": "Broken link at tf.keras.callbacks.TensorBoard in docs page", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\nURL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\r\n\r\n## Description of issue (what needs changing): \r\nThe link needs to be changed or removed\r\n\r\n### Submit a pull request?\r\nYes, here it is #48457 \r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["#48714 does not exist. Was that a typo?", "It was marked stale and closed abruptly. But the error is the link under `embeddings.metaData` which doesn't exist and throws a 404 error", "Oh ok wait... I'll edit the comment and tag the issue", "It's #48174 sorry ma bad... Thanks @mihaimaruseac for pointing out", "@alphaX86,\r\nThank you for providing the [reference link](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard).\r\n\r\n#48174 is not a Pull Request but is an issue. I will submit a fix to correct the broken link. Can you please let me know if I can close this issue as it is already being tracked in #48174? Thanks!", "Alright! You can close this and I'll reference this in my PR. Thanks @rmothukuru ", "@alphaX86,\r\nYou mean to say you have already created a PR for this change? If so, can you please share the PR number? Thanks! ", "Yes I already created a PR based on #48174... So here it is... PR #48457 @rmothukuru ", "@alphaX86,\r\nThank you for the confirmation. Closing this issue. Thanks! "]}]