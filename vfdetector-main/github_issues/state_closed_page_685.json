[{"number": 33044, "title": "Detection and get position in TensorFlow android studio", "body": "\r\n", "comments": []}, {"number": 33043, "title": "Dumping XLA compiled results in specific directory not working", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : CentOS 7.6 x86_64\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): r1.12.3\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): gcc 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.5\r\n- GPU model and memory: Tesla V100-PCIE-32GB / Titan Xp 12GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n- XLA compiled model files are not saved in the desired directory, even though I declared the xla dump directory in an explicit manner\r\n- According to this link (https://www.tensorflow.org/xla), a compiled model must be saved as `module_XXXX.ptx` or something, which is not visible.\r\n- Meanwhile, I only can see the graph descriptions about the model under /tmp, even though I did already declared a different directory far from /tmp.\r\n```\r\n$ ls /tmp\r\nbefore_mark_for_compilation_1.pbtxt\r\nbefore_mark_for_compilation_2.pbtxt\r\nbefore_mark_for_compilation_3.pbtxt\r\nbefore_mark_for_compilation_4.pbtxt\r\nbefore_mark_for_compilation_5.pbtxt\r\nbefore_mark_for_compilation_6.pbtxt\r\nbefore_mark_for_compilation.pbtxt\r\nmark_for_compilation_1.pbtxt\r\nmark_for_compilation_2.pbtxt\r\nmark_for_compilation_3.pbtxt\r\nmark_for_compilation_4.pbtxt\r\nmark_for_compilation_5.pbtxt\r\nmark_for_compilation_6.pbtxt\r\nmark_for_compilation.pbtxt\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\n# XLA compiled module files may be saved in /somewhere/xladump\r\n$ ls /somewhere/xladump\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nTF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_clustering_debug\" TF_DUMP_GRAPH_PREFIX=/somewhere/xladump XLA_FLAGS=\"--dump_hlo_as_text --xla_dump_to=/somewhere/xladump\" PYTHONDONTWRITEBYTECODE=1 time python blahblah.py\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[Build Flags]\r\n```\r\n- Build with Ignite? Y\r\n- Build with XLA JIT? Y\r\n...\r\n```\r\n\r\n[Runtime Log]\r\n```\r\n2019-10-04 15:10:36.868003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-04 15:10:36.868683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.75GiB\r\n2019-10-04 15:10:36.868705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-10-04 15:10:37.195028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-04 15:10:37.195073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2019-10-04 15:10:37.195082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2019-10-04 15:10:37.195174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11367 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\n2019-10-04 15:10:37.784171: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation.pbtxt\r\n2019-10-04 15:10:37.794203: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation.pbtxt\r\n2019-10-04 15:10:37.847682: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f237c001c70 executing computations on platform CUDA. Devices:\r\n2019-10-04 15:10:37.847749: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2019-10-04 15:10:37.903711: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:402] *** WARNING *** You are using ptxas 10.0.145, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\n2019-10-04 15:10:40.008334: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_1.pbtxt\r\n2019-10-04 15:10:40.009382: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_1.pbtxt\r\n2019-10-04 15:10:40.548377: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_2.pbtxt\r\n2019-10-04 15:10:40.549325: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_2.pbtxt\r\n2019-10-04 15:10:40.783753: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_3.pbtxt\r\n2019-10-04 15:10:40.784706: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_3.pbtxt\r\n[I] TF_XLA_FLAGS=--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_clustering_debug\r\n[I] XLA_FLAGS=--dump_hlo_as_text --xla_dump_to=sponge\r\n[I] TF_DUMP_GRAPH_PREFIX=sponge\r\n```", "comments": ["My guess is that XLA is not compiling anything in your model.\r\n\r\nThe XLA cluster nodes should be marked as `_XlaCluster` in the mark_for_compilation_1.pbtxt graphs, are you seeing any?", "Dear @sanjoy,\r\nSome parameters containing XLA cluster found in multiple mark_for_compilation_*.pbtxt graphs.\r\nFurthermore, in /tmp, I could find multiple outputs from tensorflow, such as...\r\n```\r\nbefore_mark_for_compilation_10.pbtxt  ccY1v2Nx.s\r\nbefore_mark_for_compilation_11.pbtxt  ccY3BvQB.s\r\nbefore_mark_for_compilation_12.pbtxt  ccYOO5oC.s\r\nbefore_mark_for_compilation_13.pbtxt  ccYzOwnP.s\r\nbefore_mark_for_compilation_14.pbtxt  cczFaENP.s\r\n...\r\nbefore_mark_for_compilation_28.pbtxt  tmpxft_00004a65_00000000\r\nbefore_mark_for_compilation_29.pbtxt  tmpxft_00004a65_00000000-0\r\nbefore_mark_for_compilation_2.pbtxt   tmpxft_00004a65_00000000-1.cpp\r\nbefore_mark_for_compilation_30.pbtxt  tmpxft_00004a65_00000000-2_cwise_op_gpu_xdivy.cu.fatbin.c\r\nbefore_mark_for_compilation_31.pbtxt  tmpxft_00004a65_00000000-3_cwise_op_gpu_xdivy.cu.module_id\r\nbefore_mark_for_compilation_32.pbtxt  tmpxft_00004a65_00000000-4_cwise_op_gpu_xdivy.cu.cpp4.ii\r\nbefore_mark_for_compilation_33.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.c\r\nbefore_mark_for_compilation_34.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.cpp\r\nbefore_mark_for_compilation_35.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.gpu\r\nbefore_mark_for_compilation_36.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.stub.c\r\nbefore_mark_for_compilation_37.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.ptx\r\nbefore_mark_for_compilation_38.pbtxt  tmpxft_00004a65_00000000-6_cwise_op_gpu_xdivy.cu.cpp1.ii\r\nbefore_mark_for_compilation_39.pbtxt  tmpxft_00004a65_00000000-7_cwise_op_gpu_xdivy.cu.sm_70.cubin\r\n```\r\nI believe xla optimization is working by all means and outputs are exported in `/tmp`,\r\nwhich is not expected behavior when I declare the flags such as `--xla_dump_to=/somewhere/xladump`.\r\nMeanwhile, I am revising my runtime code now to initiate an explicit XLA compiling as well as avoiding AUTO_JIT.", "After a deep survey on the source code of r1.12.3, especially a part of 'compiler',\r\nI found a valid configuration for the explicit target directory selection.\r\n\r\nFor further reference, `--tf_dump_graph_prefix=/path/to/somewhere` must be declared,\r\nin part of XLA environment variables as `TF_XLA_FLAGS`.\r\n(e.g. `TF_XLA_FLAGS=\"--something --tf_dump_graph_prefix=/path/to/somewhere\"`)\r\n\r\n(This hint was found in [tensorflow/compiler/tf2xla/dump_graph.h](https://github.com/tensorflow/tensorflow/blob/5b900cfe4b3b848f577315a0dde09a729f770e95/tensorflow/compiler/tf2xla/dump_graph.h) of commit 5b900cfe4b3b848f577315a0dde09a729f770e95 - r1.12.3)\r\n\r\nAs a result, the documentation of XLA usage is far different from actual behavior of the source code.\r\nIn other words, using env `TF_DUMP_GRAPH_PREFIX` does NOTING in tf-r1.12.3 described as below.\r\n```\r\n$ TF_DUMP_GRAPH_PREFIX=/tmp/generated \\\r\n  TF_XLA_FLAGS=\"--tf_xla_clustering_debug --tf_xla_auto_jit=2\" \\\r\n  XLA_FLAGS=\"--xla_dump_hlo_as_text --xla_dump_to=/tmp/generated\" \\\r\n    my/tensorflow/program\"\r\n```\r\n\r\nAs a result, here's some log from tensorflow runtime,\r\n```\r\n...\r\n2019-10-09 17:23:37.144396: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /path/to/somewhere/mark_for_compilation_57.pbtxt\r\n...\r\n```\r\nas well as directory listing under `/path/to/somewhere`.\r\n```\r\n$ ls /path/to/somewhere/\r\nbefore_mark_for_compilation_10.pbtxt  mark_for_compilation_10.pbtxt\r\nbefore_mark_for_compilation_11.pbtxt  mark_for_compilation_11.pbtxt\r\n...\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33043\">No</a>\n"]}, {"number": 33042, "title": "I have learned the dark secret to compile on Windows", "body": "For all my fellow cybernauts, the dark secrets revealed. To tell it what compiler to use, requires ancient spells of dark magic. You must call into vcvarsall.bat of olde, and then you can tell it the vcvars for which compiler you wish it to choose. Now, go forth and compile.", "comments": []}, {"number": 33041, "title": "EdgeTPU library is not updating for coral dev board ", "body": "* Doc you were trying to follow: https://coral.withgoogle.com/news/updates-07-2019/\r\n* Your host OS: Ubuntu 16.04\r\nPython3 version: 3.5.3\r\n\r\n# What I ran\r\n\r\nI executed the commands as mentioned in the post as mentioned above :\r\n\r\nsudo apt-get update\r\n\r\nsudo apt-get dist-upgrade\r\n\r\n\r\n\r\nthis automatically updates the edgetpu library along with other Mendel system software.\r\n\r\n# What the docs said should happen:\r\n\r\nAs per the post it should have updated the library but after updates were done the edgetpu library version is still been shown as 1.2.0\r\n\r\nBut our expectations were to be updated to newest 2.11.1 version\r\n\r\n# What actually happened\r\n\r\nEdgeTPU library version is still been shown as 1.2.0\r\n\r\n\r\nKindly help in resolving the issue.\r\n", "comments": ["Edge TPU Python library is [deprecated](https://coral.ai/news/updates-11-2020/) and [PyCoral](https://coral.ai/software/#pycoral-api) is the new library for Python development with the Edge TPU.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33041\">No</a>\n"]}, {"number": 33040, "title": "Illegal Instruction on importing tensorflow", "body": "Tensorflow version = 1.14\r\nOS : Kali Linux Rolling (2018.2)\r\nOutput on cat /proc/cpuinfo : \r\nprocessor : 0\r\nvendor_id : GenuineIntel\r\ncpu family : 6\r\nmodel : 23\r\nmodel name : Intel(R) Core(TM)2 Duo CPU E7500 @ 2.93GHz\r\nstepping : 10\r\nmicrocode : 0xa0b\r\ncpu MHz : 2546.724\r\ncache size : 3072 KB\r\nphysical id : 0\r\nsiblings : 2\r\ncore id : 0\r\ncpu cores : 2\r\napicid : 0\r\ninitial apicid : 0\r\nfpu : yes\r\nfpu_exception : yes\r\ncpuid level : 13\r\nwp : yes\r\nflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nbugs : cpu_meltdown spectre_v1 spectre_v2\r\nbogomips : 5850.33\r\nclflush size : 64\r\ncache_alignment : 64\r\naddress sizes : 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor : 1\r\nvendor_id : GenuineIntel\r\ncpu family : 6\r\nmodel : 23\r\nmodel name : Intel(R) Core(TM)2 Duo CPU E7500 @ 2.93GHz\r\nstepping : 10\r\nmicrocode : 0xa0b\r\ncpu MHz : 2231.318\r\ncache size : 3072 KB\r\nphysical id : 0\r\nsiblings : 2\r\ncore id : 1\r\ncpu cores : 2\r\napicid : 1\r\ninitial apicid : 1\r\nfpu : yes\r\nfpu_exception : yes\r\ncpuid level : 13\r\nwp : yes\r\nflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nbugs : cpu_meltdown spectre_v1 spectre_v2\r\nbogomips : 5850.33\r\nclflush size : 64\r\ncache_alignment : 64\r\naddress sizes : 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\n", "comments": ["@Xavier449446, Please provide the error log of `import Tensorflow`. Thanks!", "![Screenshot from 2019-10-04 09-26-33](https://user-images.githubusercontent.com/47718724/66202206-c7b3bd00-e6c2-11e9-8a78-83d895963221.png)\r\nThis is all what I get as error and then the python stops.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33040\">No</a>\n", "@Xavier449446, Does your CPU supports AVX instruction set or not.\r\nPlease refer this [link](https://superuser.com/questions/1251865/is-there-a-way-to-tell-if-my-hardware-supports-specific-instructions) to know the CPU instructions set. Thanks!", "Based on the cpuinfo provided, your CPU doesn't support AVX. See #19584", "@Xavier449446, Try to build the Tensorflow from source. Please follow the instructions mentioned in the [website](https://www.tensorflow.org/install/source). Thanks!", "Closing as issue has been identified to be AVX support lacking", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33040\">No</a>\n"]}, {"number": 33039, "title": "TF 2.0:  ValueError: Lengths of branch outputs of cond must match.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: 10/7.4\r\n- GPU model and memory: gtx1050ti / 12GB\r\n\r\n**Describe the current behavior**\r\nI have a error when I use if cond in tf 2.0.\r\nThe error code is as follows:\r\n\r\n+ Markdown format does not work properly. sry....\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-51-52716284752e> in <module>\r\n      2 multiboxloss = MultiboxLoss(8)\r\n      3 \r\n----> 4 model.compile(optimizer=tf.keras.optimizers.Nadam(lr = 0.001), loss = multiboxloss.comute_loss, metrics=['acc'])\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    371 \r\n    372       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 373       self._compile_weights_loss_and_weighted_metrics()\r\n    374 \r\n    375       # Functions for train, test and predict will\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1651       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1652       #                   layer losses.\r\n-> 1653       self.total_loss = self._prepare_total_loss(masks)\r\n   1654 \r\n   1655   def _prepare_skip_target_masks(self):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _prepare_total_loss(self, masks)\r\n   1711 \r\n   1712           if hasattr(loss_fn, 'reduction'):\r\n-> 1713             per_sample_losses = loss_fn.call(y_true, y_pred)\r\n   1714             weighted_losses = losses_utils.compute_weighted_loss(\r\n   1715                 per_sample_losses,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py in call(self, y_true, y_pred)\r\n    219       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\r\n    220           y_pred, y_true)\r\n--> 221     return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    222 \r\n    223   def get_config(self):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in bound_method_wrapper(*args, **kwargs)\r\n   2656     # However, the replacer is still responsible for attaching self properly.\r\n   2657     # TODO(mdan): Is it possible to do it here instead?\r\n-> 2658     return wrapped_fn(*args, **kwargs)\r\n   2659   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)\r\n   2660 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\r\n    903           except Exception as e:  # pylint:disable=broad-except\r\n    904             if hasattr(e, \"ag_error_metadata\"):\r\n--> 905               raise e.ag_error_metadata.to_exception(e)\r\n    906             else:\r\n    907               raise\r\n\r\nValueError: in converted code:\r\n\r\n    <ipython-input-31-45935cecbe16>:100 comute_loss  *\r\n        pos_list, neg_list, t_gtl, t_gtb = matcher.matching(y_pred[i, :, 4:8], y_pred[i, :, :4], actual_list, i)\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py:457 __call__\r\n        result = self._call(*args, **kwds)\r\n    <ipython-input-50-83f8d8829ac3>:94 matching  *\r\n        if(jacc_thred == 1):\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\operators\\control_flow.py:893 if_stmt\r\n        basic_symbol_names, composite_symbol_names)\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\operators\\control_flow.py:931 tf_if_stmt\r\n        error_checking_orelse)\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py:507 new_func\r\n        return func(*args, **kwargs)\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py:1174 cond\r\n        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py:101 cond_v2\r\n        name=scope)\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py:216 _build_cond\r\n        _check_same_outputs(_COND, [true_graph, false_graph])\r\n    C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py:736 _check_same_outputs\r\n        len_b=len(graphs[b].outputs)))\r\n\r\n    ValueError: Lengths of branch outputs of cond must match.\r\n    len(graphs[0].outputs): 3\r\n    len(graphs[1].outputs): 2```\r\n\r\n---------------------------------------------------------------------------\r\n# **Describe the expected behavior**\r\nI want to make the if statement in my code work. The if statement is as follows:\r\n\r\n\r\n# compute jaccard for each default box\r\nfor gt_list in actual_list:\r\n    gt_label = gt_list[1]\r\n    gt_box = gt_list[0]\r\n            \r\n    for i in range(len(matches)):\r\n         # self.default_boxes[batch_size, i].shape --> (4, )\r\n         jacc = jaccard(gt_box[0], self.default_boxes[batch_size, i])\r\n         jacc_thred = tf.where(0.5 <= jacc, 1, 0)\r\n         if(jacc_thred == 1):\r\n                  matches[i] = 4    # <-- **Error part!**\r\n                  self.pos += 1\r\n                  matched.append(gt_label)\r\n", "comments": ["@airplane2230, \r\nIn order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thanks!\r\n", "Thank you for your answer!\r\n\r\nI refered this git-page, SSD implementation and am changing to tf 2.0!\r\n[SSD300, arabian9ts](https://github.com/arabian9ts/SSD300)\r\n\r\nAnd, This is my full code in only this class.\r\n<hr>\r\n```python\r\nimport numpy as np<br>\r\nfrom policy import * <br>\r\nclass Matcher:<br>\r\n    def __init__(self, num_boxes, default_boxes):<br>\r\n        \"\"\"\r\n                initializer require feature-map shapes and default boxes<br>\r\n                Args:<br>\r\n                    fmap_shapes: feature-map's shape<br>\r\n                    default_boxes: generated default boxes<br>\r\n                \"\"\"<br>\r\n        self.num_boxes = num_boxes<br>\r\n        self.default_boxes = default_boxes<br>\r\n    \r\n    @tf.function\r\n    def extract_highest_indicies(self, pred_confs, max_length):\r\n        \"\"\"\r\n                extract specific indicies, that is, have most high loss_confs.\r\n                Args:\r\n                    pred_confs: predicated confidences\r\n                    max_length: max length of extracted indicies (in here, pos*3)\r\n                Returns:\r\n                    extracted indicies of boxes (confidences).\r\n        \"\"\"\r\n        print('extract_highest_indicies========================')\r\n        print(pred_confs.shape)\r\n        loss_confs = []\r\n\r\n        # pred_confs : y_pred[i, :, 4:8]\r\n        for pred_conf in pred_confs:\r\n            print(pred_conf.__class__)\r\n            pred = tf.divide(tf.math.exp(pred_conf), (tf.reduce_sum(tf.math.exp(pred_conf)) + 1e-5))\r\n            loss_confs.append(tf.argmax(pred))\r\n\r\n        size = tf.math.minimum(len(loss_confs), max_length)\r\n        indicies = np.argpartition(loss_confs, -size)[-size:] #TODO\r\n\r\n        return indicies\r\n        \r\n    @tf.function\r\n    def matching(self, pred_confs, pred_locs, actual_list, batch_size):\r\n        \"\"\"\r\n                match default boxes and bouding boxes.\r\n                matching computes pos and neg count for the computation of loss.\r\n                now, the most noting point is that it is not important that\r\n                whether class label is correctly predicted.\r\n                class label loss is evaled by loss_conf\r\n                matches variable have some Box instance and most of None.\r\n                if jaccard >= 0.5, that matches box has Box(gt_loc, gt_label).\r\n                then, sort by pred_confs loss and extract 3*pos boxes, which they\r\n                have Box([], classes) => background.\r\n                when compute losses, we need transformed ground truth labels and locations\r\n                because each box has self confidence and location.\r\n                so, we should prepare expanded labels and locations whose size is as same as len(matches).\r\n                Args:\r\n                    pred_confs: predicated confidences\r\n                    pred_locs: predicated locations\r\n                    actual_labels: answer class labels\r\n                    actual_locs: answer box locations\r\n                Returns:\r\n                    postive_list: if pos -> 1 else -> 0\r\n                    negative_list: if neg and label is not classes(not unknown class) 1 else 0\r\n                    expanded_gt_labels: gt_label if pos else classes\r\n                    expanded_gt_locs: gt_locs if pos else [0, 0, 0, 0]\r\n        \"\"\"\r\n        self.pos = 0\r\n        self.neg = 0\r\n        pos_list = []\r\n        neg_list = []\r\n        expanded_gt_labels = []\r\n        expanded_gt_locs = []\r\n        \r\n        matches = []\r\n        matched = []\r\n        \r\n        print('In Matcher1!===============================')\r\n        for _ in range(self.num_boxes):\r\n            matches.append(None) # len is 938\r\n        \r\n        for gt_list in actual_list:\r\n            gt_label = gt_list[1]\r\n            gt_box = gt_list[0]\r\n            \r\n            for i in range(len(matches)):\r\n\r\n              # If this part is missing, an error occurs. I made it temporarily.\r\n                matches[i] = 999 # tricks!\r\n\r\n                # self.default_boxes[batch_size, i].shape --> (4, )\r\n                jacc = jaccard(gt_box[0], self.default_boxes[batch_size, i])\r\n                jacc_thred = tf.where(0.5 <= jacc, 1, 0)\r\n\r\n             ** problem part! **\r\n                if(jacc_thred == 1):\r\n                    matches[i] =  4 <<\r\n                    self.pos += 1\r\n                    matched.append(gt_label)\r\n                    \r\n        print('In Matcher2!===============================')\r\n\r\n\r\n        neg_pos = 5\r\n\r\n        indicies = self.extract_highest_indicies(pred_confs, self.pos * neg_pos)\r\n\r\n        for i in indicies:\r\n            if neg > pos * neg_pos:\r\n                break\r\n            if(matches[i] is None and classes-1 != np.argmax(pred_confs[i])):\r\n                matches[i] = 1\r\n                neg += 1\r\n\r\n        for box in matches:\r\n            if box is None:\r\n                pos_list.append(0)\r\n                neg_list.append(0)\r\n                expanded_gt_labels.append(classes - 1)\r\n                expanded_gt_locs.append([0] * 4)\r\n            elif(0 == len(box.loc)):\r\n                pos_list.append(0)\r\n                neg_list.append(1)\r\n                expanded_gt_labels.append(classes - 1)\r\n                expanded_gt_locs.append([0] * 4)\r\n            else:\r\n                pos_list.append(1)\r\n                neg_list.append(0)\r\n                expanded_gt_labels.append(box.index)\r\n                expanded_gt_locs.append(box.loc)\r\n\r\n        return pos_list, neg_list, expanded_gt_labels, expanded_gt_locs\r\n```", "@airplane2230, I tried replicating the issue on Colab but the code is throwing different error. Please take a look at the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/fc6df853c58d9df38e4a7d2718c741fb/untitled185.ipynb) and provide the more information to reproduce the issue.Thanks!", "@airplane2230, Did you get a chance to look at attached gist. Thanks!", "Oh, I'm super Sorry..... \r\nI've tried many things, but I haven't solved them yet. \r\nBefore solving this error, I encountered another error and am currently working on fixing it. \r\nI tried to sort out the code and give you an answer, but it didn't. \r\nIf I solve my previous error and meet this issue again, Will I reopen it?\r\nThanks!", "@airplane2230, Are you happy to close this issue now. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "For anyone who runs into this in the future, the problem for me was that one branch was outputting a variable with `None` and the other branch was outputting a tensor, so autograph was getting a different number of tracked objects from each branch."]}, {"number": 33038, "title": "TF 2.0: Illegal instruction (core dumped)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: pip (in conda environment)\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: 10 / 7\r\n- GPU model and memory: GeForce Titan X, 12GB\r\n\r\n**Describe the problem**\r\n\r\nAfter installing TF2 through pip I can't launch it and python exits.\r\n\r\nFrom what I gather [here](https://github.com/tensorflow/tensorflow/issues/30114), my cpu would need AVX instructions and in fact they don't show in my cpuinfo.\r\nHowever, on the same machine I can smoothly run Tensorflow 1.14 (both cpu and gpu). Any idea on what's going on? Is AVX actually the problem?\r\n\r\nStrangely, on [this chart](https://www.intel.com/content/dam/support/us/en/documents/processors/core/intel-core-i7-comparison-chart.pdf) from Intel, it states that my processor (Intel Core i7 CPU 960) supports AVX.\r\n\r\nI have also tried building from source but Bazel fails to build the package. I don't know if this could be related.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ python\r\n>> import tensorflow as tf\r\nIllegal instruction (core dumped)\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n$ cat /proc/cpuinfo\r\n\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.506\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 0\r\ncpu cores\t: 4\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 1\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.503\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 1\r\ncpu cores\t: 4\r\napicid\t\t: 2\r\ninitial apicid\t: 2\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 2\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.504\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 2\r\ncpu cores\t: 4\r\napicid\t\t: 4\r\ninitial apicid\t: 4\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 3\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.503\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 3\r\ncpu cores\t: 4\r\napicid\t\t: 6\r\ninitial apicid\t: 6\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 4\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.512\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 0\r\ncpu cores\t: 4\r\napicid\t\t: 1\r\ninitial apicid\t: 1\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 5\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.506\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 1\r\ncpu cores\t: 4\r\napicid\t\t: 3\r\ninitial apicid\t: 3\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 6\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.501\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 2\r\ncpu cores\t: 4\r\napicid\t\t: 5\r\ninitial apicid\t: 5\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 7\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x11\r\ncpu MHz\t\t: 3373.507\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 3\r\ncpu cores\t: 4\r\napicid\t\t: 7\r\ninitial apicid\t: 7\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6477.12\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\n```\r\n", "comments": ["No AVX support, please check #19584 or build from source"]}, {"number": 33037, "title": "radon and inverse radon transform, Projection and back_projection function in CT", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  tf 1.14\r\n- Are you willing to contribute it (Yes/No):  no\r\n   When training a CT-reconstructed model, the projection and back_projection functions are usually used. However, to my knowledge, there is no api  to implement the above functions.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n  Like in the scikit-image package, the projection function is the radon transform (skimage.transform.radon) while the back-projection is the dual operator of projection.\r\n\r\n**Will this change the current api? How?**\r\nWill add two maybe three apis.\r\n\r\n**Who will benefit with this feature?**\r\nanyone uses these functions to train CT-reconstruction moedl.\r\n\r\n**Any Other info.**\r\n", "comments": ["I would like to see this feature, though in latest stable releases, should I open another issue or can we add it here?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Thank you. I have implemented these functions using tensorflow 2. Please close the issue.", "Hi, has this feature been added as a tensorflow layer?"]}, {"number": 33036, "title": "TF 2.0 How to set a different learning rate for each layer?", "body": "## Description of issue (what needs changing):\r\nHi, I would like to ask a question about how to set different learning rates for several layers. In my implementation, I would like to set one learning rate for some layers while the other learning rate for some other layers. Is there some example codes? Thanks in advance.\r\n", "comments": ["Maybe you can refer to this article and use tf.keras to implement it. \r\nThis is a Chinese blogger's article : https://spaces.ac.cn/archives/6418", "@yw155, Please provide the Tensorflow version and also provide the code snippet. \r\nDid you try the @zhen8838's suggestion. Thanks!", "Hi @gadagashwini, I would like to implement this function in TensorFlow 2.0. I am still not clear how to implement it. \r\n\r\nThe function should be able to implement the layer-wise learning rate as follows:\r\nlayer {\r\n  name: \"conv1\"\r\n  type: \"Convolution\"\r\n  bottom: \"input\"\r\n  top: \"conv1\"\r\n  param {\r\n    lr_mult: 1.0\r\n    decay_mult: 1.0\r\n  }\r\n  param {\r\n    lr_mult: 2.0\r\n    decay_mult: 0.0\r\n  }\r\n  convolution_param {\r\n    num_output: 128\r\n    pad: 1\r\n    kernel_size: 3\r\n    weight_filler {\r\n      type: \"gaussian\"\r\n      std: 0.01\r\n    }\r\n    bias_filler {\r\n      type: \"constant\"\r\n    }\r\n  }\r\n}\r\nlayer {\r\n  name: \"conv2\"\r\n  type: \"Convolution\"\r\n  bottom: \"conv1\"\r\n  top: \"conv2\"\r\n  param {\r\n    lr_mult: 4.0\r\n    decay_mult: 1.0\r\n  }\r\n  param {\r\n    lr_mult: 8.0\r\n    decay_mult: 0.0\r\n  }\r\n  convolution_param {\r\n    num_output: 128\r\n    pad: 1\r\n    kernel_size: 3\r\n    weight_filler {\r\n      type: \"gaussian\"\r\n      std: 0.01\r\n    }\r\n    bias_filler {\r\n      type: \"constant\"\r\n    }\r\n  }\r\n}", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 33035, "title": "tf.saved_model.save() unable to save model with sparse input.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu (Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  fails on 1.14.0, 1.15.0rc2, 2.0.0\r\n- Python version: python 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nSee this gist for reproduction \r\nhttps://colab.research.google.com/gist/yzhuang/3d4c511b247988fdafbd083785d624c1/model-saving-fails-with-sparse-input-tensor.ipynb\r\n\r\nIf a model has sparse input, they seem to be converted to dense tensors when `saved_model.save()` is called.  This causes model saving to fail.\r\n\r\n**Describe the expected behavior**\r\n\r\nIf a model works during model.fit(), I expect the same model to also work for `saved_model.save()`.\r\n\r\n**Code to reproduce the issue**\r\nColab gist:\r\nhttps://colab.research.google.com/gist/yzhuang/3d4c511b247988fdafbd083785d624c1/model-saving-fails-with-sparse-input-tensor.ipynb\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@yzhuang ,\r\nThank you for reporting the issue, error also replicating for TF version-[2.0rc2](https://colab.sandbox.google.com/gist/oanush/d8dc1f89e88530a4ab3c83cdf81adc15/model-saving-fails-with-sparse-input-tensor.ipynb).Thanks!", "@yzhuang,\r\nThe error is because of the below line of code.\r\n\r\n`sparse_tensor = tf.sparse.SparseTensor(indices=sparse_tensor.indices, values=sparse_tensor.values, dense_shape=[2, 2])` .\r\n\r\nIt works fine if you replace that line with below line:\r\n\r\n`sparse_tensor = tf.sparse.SparseTensor(indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),values=tf.constant([1.0, 1.0], dtype=tf.float32),dense_shape=[2, 2])`.", "Hi @rmothukuru \r\n\r\nI am reproducing this bug specifically: \"During tf.saved_model.save(), while calling a model's call() method, all SparseTensor inputs are converted to dense tensors.\"  That's why the above code fails. \r\n\r\nWhile the suggested one line change fixes the crash on that line, it no longer uses the input to the call() method on that line, and hence no longer reproduce this bug on that line.  I updated the repro colab to simply remove that line, and it'll crash later. See:\r\nhttps://colab.research.google.com/gist/yzhuang/3d4c511b247988fdafbd083785d624c1/model-saving-fails-with-sparse-input-tensor.ipynb\r\n\r\nLet me describe the problem in another way:\r\nfocus on the input to the call() method: `sparse_tensor`.  Its type is a `tf.SparseTensor` during `model.fit()`, but its type is a dense `Tensor` during `tf.saved_model.save()`.\r\n", "Could reproduce this bug with TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/807a0c0d20281b818a2abeafeeae7a6b/model-saving-fails-with-sparse-input-tensor.ipynb).", "Is there a workaround to still save a trained model? My project absolutely needs sparse input tensors, and if I am not able to save them, that will be a major blocker. \r\nThanks,", "Hi guys, is there any update on this?", "This is fixed with latest tf-nightly version '2.2.0-dev20200302'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33035\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33035\">No</a>\n"]}, {"number": 33034, "title": "Per-operation random seeds don't work in tensorflow 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): anaconda\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: Python 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nMinimal test script:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.random.uniform([1], seed=1)\r\ny = tf.random.uniform([1], seed=1)\r\nprint(\"x == y?\", bool(tf.reduce_all(x == y)))\r\nprint(x)\r\nprint(y)\r\n```\r\n\r\nOutput:\r\n```\r\nx == y? False\r\ntf.Tensor([0.2390374], shape=(1,), dtype=float32)\r\ntf.Tensor([0.22267115], shape=(1,), dtype=float32)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe script should print `x == y? True`.\r\n\r\nAccording to the [docs](https://www.tensorflow.org/api_docs/python/tf/compat/v1/set_random_seed) on `tf.compat.v1.set_random_seed`:\r\n>  If the graph-level seed is not set, but the operation seed is set: A default graph-level seed and the specified operation seed are used to determine the random sequence.\r\n\r\nThis does work correctly when not executing eagerly.", "comments": ["@kazimuth,\r\nI could replicate the issue with TF 2.0.0.rc2. Please see the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/847114c631edb377542a001d0c6714e5/untitled174.ipynb). Thanks!", "@kazimuth,\r\nThank you for the details and the reference. I think the behavior is expected, as per [the point](https://www.tensorflow.org/api_docs/python/tf/compat/v1/set_random_seed), \r\n\r\n> If the graph-level seed is not set, but the operation seed is set: A default graph-level seed and the specified operation seed are used to determine the random sequence. \r\n\r\nIt is because, in Eager Execution, there will not be Graph Level Seed, as there will be no graph at all. So, they will be two independent Random Tensor Operations.\r\n\r\nPlease let me know your opinion about the same. Thanks.", "I mean, that makes sense, but it's not really intuitive; it seems like the implementation details are getting in the way of the expected interface. Ideally it would be possible to set global and per-operation seeds in eager mode, whether or not there's a graph in the background.\r\n\r\nIn particular, as far as I can tell I can't write code with reproducible randomness right now if I'm running in eager mode; that makes reproducing experiments and writing tests harder.", "The behavior is caused by the facts that eager runtime caches and reuses a kernel for an op if the op is called multiple times with the same arguments, and that `tf.random.uniform` is a stateful op. In the line `x = ...`, a `tf.random.uniform` kernel is used and its internal state changed; in the line `y = ...`, the same kernel is used again but its internal state has been changed. The two `tf.random.uniform` will only produce identical numbers in eager mode if you put `tf.random.set_seed(...)` before each of them. The updated docstring for `set_seed` (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/random_seed.py#L191) explains those caveats. Alternatively you can use stateless random ops (https://www.tensorflow.org/api_docs/python/tf/random/stateless_uniform) or the new stateful random ops (https://www.tensorflow.org/api_docs/python/tf/random/experimental/Generator).", "Ohh, I see, so this is intentional behavior. If I rerun my test script, I get the same tensors every run, good. Okay, in that case, this isn't a bug. Thanks for the clarification!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33034\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33034\">No</a>\n", "In Tensorflow 2.0 you can set random seed like this :\r\n\r\n\r\n    import tensorflow as tf\r\n    tf.random.set_seed(221)\r\n\r\n\r\n    from tensorflow import keras\r\n    from tensorflow.keras import layers\r\n    \r\n    \r\n    model = keras.Sequential( [ \r\n    layers.Dense(2,name = 'one'),\r\n    layers.Dense(3,activation = 'sigmoid', name = 'two'),\r\n    layers.Dense(2,name = 'three')])\r\n    \r\n    x = tf.random.uniform((12,12))\r\n    model(x)", "> In Tensorflow 2.0 you can set random seed like this :\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> tf.random.set_seed(221)\r\n> \r\n> \r\n> from tensorflow import keras\r\n> from tensorflow.keras import layers\r\n> \r\n> \r\n> model = keras.Sequential( [ \r\n> layers.Dense(2,name = 'one'),\r\n> layers.Dense(3,activation = 'sigmoid', name = 'two'),\r\n> layers.Dense(2,name = 'three')])\r\n> \r\n> x = tf.random.uniform((12,12))\r\n> model(x)\r\n> ```\r\n\r\nAre you sure  \"tf.random.set_seed(221)\" works in Tensorflow 2.0? One of my systems has Tensorflow 2.0 and the deep learning there pipeline always returns a different result. The same code in Tensorflow 2.2 always returns the same result."]}, {"number": 33033, "title": "Update documentation for TFLiteConverterV2", "body": "- fixed a minor doc issue\r\n- added to docs, that currently `from_concrete_functions ` support only one-by-one conversion, not multiple. That was missing part, even that it's in code - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L400 and tutorials ", "comments": ["Sorry. I have no idea what the heck IAM doing.. I have to admit this whole\nset up on GitHub if stupid as hell.  I Don't see any logic in how it\nfunctions. Hello world.. That's what they say right. Gitoutside.\n\nOn Fri, Oct 4, 2019, 8:14 AM Nupur Garg <notifications@github.com> wrote:\n\n> *@gargn* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/lite/python/lite.py\n> <https://github.com/tensorflow/tensorflow/pull/33033#discussion_r331550678>\n> :\n>\n> > @@ -317,7 +317,8 @@ def from_concrete_functions(cls, funcs):\n>\n>      Args:\n>        funcs: List of TensorFlow ConcreteFunctions. The list should not contain\n> -        duplicate elements.\n> +        duplicate elements. Currently converter can only convert a single\n> +        ConcreteFunction. Converting multiple functions is under development.\"\n>\n>\n> nit: Remove the quotation mark at the end.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/33033?email_source=notifications&email_token=AMHXS5PL4NHMWXQYVB2L3UTQM5MWXA5CNFSM4I5ITNJKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCG6BMUQ#pullrequestreview-297539154>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMHXS5M5QEDE5ZHD33EOYVDQM5MWXANCNFSM4I5ITNJA>\n> .\n>\n"]}, {"number": 33032, "title": "Improve TF-TRT log", "body": "This does some minor changes in TF-TRT logs plus printing TensorRT networks in the log.", "comments": []}, {"number": 33031, "title": "Model.evaluate diverging from fit on Unet based model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Colab (K80 ?)\r\n\r\n**Describe the current behavior**\r\nthe model.evaluate is providing totally different results from the model.fit method.\r\n\r\nTo demonstrate this, i've don't this on colab :\r\n1/ Create a notebook with GPU setup\r\n2/ create a dataset with only one image\r\n3/ feed a training loop (tf.keras.model.fit on 100 epochs) with the image embedded as a tf.Data.Dataset.\r\nAs there are only one image, i got a **loss of 0.0571** and a binary **accuracy close to 1**\r\n4/ the, i run the model.evaluate method and i got something totally different :  \r\n**loss of 0.3667** and a binary **accuracy of 0.8373**\r\n5/ a run the fit method again for 1 epoch and got results close to step 3 :  \r\n**loss of 0.0558** and a binary **accuracy close to 1**\r\n\r\n**Describe the expected behavior**\r\nThe evaluate method must provide results close to the fit method, especially after 100 epochs\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom google.colab import drive\r\n\r\n#get a image as input data for model : tensorflow logo\r\n!curl https://avatars0.githubusercontent.com/u/15658638?s=256 --output tensor_logo.png\r\n\r\ndef train_input_fn():\r\n\r\n  def _process_string_image(img_path):\r\n    raw_image = tf.read_file(img_path)\r\n    image_decoded = tf.image.decode_png(raw_image, channels=3)\r\n    image_decoded = tf.cast(image_decoded, tf.float32)/255.\r\n    dummy_labels = tf.round(image_decoded)\r\n    return image_decoded, dummy_labels\r\n\r\n  list_files = tf.data.Dataset.from_tensor_slices(['/content/tensor_logo.png'])\r\n  files = list_files.map(_process_string_image)\r\n  files = files.repeat(100)\r\n  files = files.batch(1, drop_remainder=True)\r\n  return files\r\n\r\n#basic check to compare train_input_fn_dummy_data, train_input_fn_from_tf_records\r\n#can't be run after TPU initialisation\r\nwith tf.Session() as sess:\r\n  batch = train_input_fn().make_one_shot_iterator().get_next()\r\n  item = sess.run(batch)\r\n  print('shape of first item :', item[0].shape, item[1].shape)\r\n  plt.imshow(item[0][0])\r\n  plt.show()\r\n  plt.imshow(item[1][0])\r\n\r\ndef Unet_Encoder(inp, layer_nb, activation = 'relu', padding = 'same', batchnorm = True, is_pool = True):\r\n    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f\"conv_1_L{layer_nb}\") (inp)\r\n    if batchnorm:\r\n        inp = tf.keras.layers.BatchNormalization(name = f\"bn_1_L{layer_nb}\")(inp)\r\n    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f\"conv_2_L{layer_nb}\") (inp)\r\n    if batchnorm:\r\n        inp = tf.keras.layers.BatchNormalization(name = f\"bn_2_L{layer_nb}\")(inp)\r\n        \r\n    if is_pool:\r\n        return inp, tf.keras.layers.MaxPooling2D((2, 2), name = f\"maxpool_L{layer_nb}\") (inp)\r\n    \r\n    return inp\r\n\r\ndef Unet_Decoder(inp, attention, layer_nb, activation = 'relu', padding = 'same', batchnorm = True):\r\n    inp = tf.keras.layers.Conv2DTranspose(8 * 2**layer_nb, (2, 2), strides=(2, 2), padding=padding, name = f\"convT_L{layer_nb}\") (inp)\r\n    if batchnorm:\r\n        inp = tf.keras.layers.BatchNormalization()(inp)\r\n    inp = tf.keras.layers.concatenate([inp, attention])\r\n    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f\"uconv_1_L{layer_nb}\") (inp)\r\n    if batchnorm:\r\n        inp = tf.keras.layers.BatchNormalization(name = f\"ubn_1_L{layer_nb}\")(inp)\r\n    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f\"uconv_2_L{layer_nb}\") (inp)\r\n    if batchnorm:\r\n        inp = tf.keras.layers.BatchNormalization(name = f\"ubn_2_L{layer_nb}\")(inp)\r\n    \r\n    return inp\r\n\r\ndef Build_Unet(input_shape, depth, activation = 'relu', padding = 'same', batchnorm = True):\r\n    inputs = tf.keras.layers.Input(input_shape)\r\n    conv = [None for x in range(depth)]\r\n    \r\n    ###### encoding part ######\r\n    last_pool = inputs\r\n    for i in range(0, depth):\r\n        conv[i], last_pool = Unet_Encoder(last_pool, i, activation, padding, batchnorm)\r\n    \r\n    ###### bottom layer ###### \r\n    uconv = Unet_Encoder(last_pool, depth, activation, padding, batchnorm, is_pool=False)\r\n    \r\n    ###### encoding part ######\r\n    for i in range(depth-1, -1, -1):\r\n        uconv = Unet_Decoder(uconv, conv[i], i, activation, padding, batchnorm)\r\n    \r\n    outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid') (uconv)\r\n    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\r\n\r\n    return model\r\n\r\n#build model and compile\r\nmodel = Build_Unet((None, None, 3), 4)\r\n\r\nadam = tf.keras.optimizers.Adam(learning_rate=0.005)\r\nmodel.compile(adam, loss='binary_crossentropy', metrics=['binary_accuracy'])\r\n\r\nmodel.summary()\r\n\r\n# fit the model\r\nmodel.fit(train_input_fn(), epochs= 100, steps_per_epoch=1)\r\n\r\nmodel.evaluate(train_input_fn(), steps=1)\r\n\r\nmodel.fit(train_input_fn(), epochs=1, steps_per_epoch=1)\r\n```\r\n\r\n\r\nIt seems that the more epochs i run, the more the results are different from train and fit methods.\r\nWith a really simple 'dummy' models like this one, i don't have the issue : \r\n```\r\ninputs = tf.keras.layers.Input(shape=(256, 256, 3))\r\noutputs = tf.keras.layers.Conv2D(6,(2,2),padding='same')(inputs)\r\noutputs = tf.keras.layers.MaxPool2D()(outputs)\r\noutputs = tf.keras.layers.Conv2D(12,(2,2),padding='same')(outputs)\r\noutputs = tf.keras.layers.Conv2DTranspose(6,(4,4),(2,2),padding='same')(outputs)\r\noutputs = tf.keras.layers.Conv2D(3,(2,2),padding='same', activation='sigmoid')(outputs)\r\n```", "comments": ["I have tried on colab with TF version 1.14,1.15.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2cba665f3b22aa25bb1c8343793fc660/untitled243.ipynb). Thanks!", "i've added a check by calling the model.predict : (just add this code after the one provided)\r\n```\r\n#getting the generated truth\r\nwith tf.Session() as sess:\r\n  item = train_input_fn().make_one_shot_iterator().get_next()\r\n  record = sess.run(item)\r\n  print(record[1].shape)\r\n\r\n#getting the prediction from model\r\npreds = model.predict(train_input_fn(), steps=1)\r\nprint(preds.shape)\r\n#calculate the binary accuracy\r\nprint(np.mean(record[1].reshape(-1) == np.round(preds).reshape(-1)))\r\n```\r\n\r\nthe last line provide a result of 0.862284\r\n\r\nWith this, i have the feeling that the issue is within the fit method and the loss calculation (can be potentially link with a specific layer... i can't figure it out)", "@anhmeow I think this is expected as you are using BatchNorm. There are some layers (BatchNorm, Dropout) that are enabled during training and disabled while evaluation. If you disable `BatchNorm` and run the training (`model.fit`) and Evaluation (`model.evaluate`), the results are very very close (upto 3rd ot 4th decimal). Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/735dcb2ba93dbbfd0574214765eabffd/tf33031.ipynb). Please check the colab gist and let us know what you think. My gist is with `TF1.15.0rc2`, however you can try with `TF1.14` also.\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Thanks for this feedback, indeed, without batchnorm, i have 'almost' the same results even with TF 1.14.\r\nI will look deeper in BatchNorm but the issue is now solved for me !\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33031\">No</a>\n"]}, {"number": 33030, "title": "Memory leak on TF 2.0 with model.predict or/and model.fit with keras", "body": "**System information**\r\n- OS Platform:\r\n\r\n> System Version: macOS 10.14.6 (18G103)\r\n> Kernel Version: Darwin 18.7.0\r\n\r\n- TensorFlow installed from binary using `pip install tensorflow`\r\n\r\n- Python version:\r\n\r\n```python\r\npython -V                                                                                                                                                                                                      \r\nPython 3.7.3\r\n```\r\n\r\n- GPU model and memory: No GPU\r\n\r\n- TensorFlow version\r\n\r\n\r\n```python\r\npython -c \"import tensorflow as tf; print(tf.version.VERSION)\"                                                                                                                                               \r\n2.0.0\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nWhile running using tensorflow 1.14 or theano backends this code works fine.\r\nAfter upgraded to tensorflow 2.0.0 it stops working and memory usage increasing without finish the program.\r\n\r\n**Describe the expected behavior**\r\nUsing theano I get 28 seconds by iteration.\r\nUsing tensorflow 2.0.0 I expect same behavior (or better). \r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport gym\r\nimport numpy as np\r\nimport matplotlib.pylab as plt\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nenv = gym.make('NChain-v0')\r\n\r\n\r\ndef q_learning_keras(env, num_episodes=1000):\r\n    # create the keras model\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.InputLayer(batch_input_shape=(1, 5)))\r\n    model.add(layers.Dense(10, activation='sigmoid'))\r\n    model.add(layers.Dense(2, activation='linear'))\r\n    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\n    # now execute the q learning\r\n    y = 0.95\r\n    eps = 0.5\r\n    decay_factor = 0.999\r\n    r_avg_list = []\r\n    for i in range(num_episodes):\r\n        s = env.reset()\r\n        eps *= decay_factor\r\n        if i % 100 == 0:\r\n            print(\"Episode {} of {}\".format(i + 1, num_episodes))\r\n        done = False\r\n        r_sum = 0\r\n        while not done:\r\n            if np.random.random() < eps:\r\n                a = np.random.randint(0, 2)\r\n            else:\r\n                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\r\n            new_s, r, done, _ = env.step(a)\r\n            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\r\n            target_vec = model.predict(np.identity(5)[s:s + 1])[0]\r\n            target_vec[a] = target\r\n            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\r\n            s = new_s\r\n            r_sum += r\r\n        r_avg_list.append(r_sum / 1000)\r\n    plt.plot(r_avg_list)\r\n    plt.ylabel('Average reward per game')\r\n    plt.xlabel('Number of games')\r\n    plt.show()\r\n    for i in range(5):\r\n        print(\"State {} - action {}\".format(i, model.predict(np.identity(5)[i:i + 1])))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    q_learning_keras(env)\r\n```\r\n", "comments": ["I am able to reproduce the issue in colab using TF 2.0.0-rc2. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/ca9f659a22917431c694527e76decee0/untitled244.ipynb) .Thanks!", "I have that same problem", "Same problem here as well. No issues with 1.14 but suddenly appeared when I installed 2.0.", "Got OOM with 2.0, no issues with 1.14", "Adding @robieta who is an expert", "I reported the same issue here with an even simpler test case on 2.0.0\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/32500", "It didn't happen in tensorflow 2.0 alpha, but in 2.0.\r\n\r\n`!pip install tensorflow-gpu==2.0.0` : got memory leak\r\n`!pip install tensorflow-gpu==2.0.0-alpha` : everything's fine", "I am using `tensorflow-gpu==2.0.0` \r\nI spent 1 full day in checking my code for memory leaks and eventually I spotted the leak with -\r\n`tf.model.predict(...)`\r\n\r\nBy the way, I used [memory_profiler](https://pypi.org/project/memory-profiler/) for profiling my code.  \r\nThank God ! I am not alone with this issue.", "Welcome!", "Also experiencing this issue.\r\nIt seems some tensors are being kept in GPU memory.\r\nFollowing.", "Hi guys! Any progress? Maybe a temporary fix?", "Hi, there were a couple of recent fixes related to this https://github.com/tensorflow/tensorflow/commit/082415b2ff49bfb8890f7d5361585bac04749add https://github.com/tensorflow/tensorflow/commit/c2fc448fe253bc59d3f0417d7d08e16d53f2a856 ", "@kkimdev Great! I think my problem is probably due to a leak in `Dataset.map` because it does not leak when I use Keras Sequence generators.", "can i get the tensorflow whici is fixed the memory leak from conda?", "Will the fixes be added to the current release any time soon?", "Thanks for checking in! We're still verifying the fix solves the issue and should have updates soon.", "Same issue here - traced back to model.predict()\r\nIf I replace that inference with a static value, the leak goes away so it must be in there.\r\n\r\nI tried to just blast away the model and copy the weights over, but the leak persists even with the model deleted. \r\n\r\n@rchao Thanks for working on this. Any updates are duly appreciated!", "My workaround for the memory leak in `dataset.map` is not to use `tf.data` API - stick with Keras Sequence data generators. And a workaround for the leak in `model.predict()` - call `model.predict_on_batch()`, which does not have a leak.", "I am also facing this problem where I am using a TF model and running in a loop for cross-validation. In each iteration of the loop the memory used grows although it is the same model and it shouldn't. As a result, after few iterations, the code returns segmentation fault. Looking for a fix on this or a workaround. I am using model.fit(). This did not happen in TF1.14. (tf-gpu)\r\n\r\nAttached the memory profile for same code using different TF versions. The drop and rise occurs at each epoch:\r\n![tf1 14_memory_profile](https://user-images.githubusercontent.com/6624314/69166778-c33a4b00-0b2e-11ea-8491-1e0e400bcd93.png)\r\n![tf2_memory_profile](https://user-images.githubusercontent.com/6624314/69166792-c7666880-0b2e-11ea-8da7-c9ce665a72c0.png)\r\n\r\n", "Thanks for the updates! I verified locally that the memory issue has been mitigated with TF nightly. Can you try `!pip install tf-nightly` and see if it resolves your issue?", "I am using tf-nightly-gpu, and I still face the same issue:\r\n\r\n`Name: tf-nightly-gpu\r\nVersion: 2.1.0.dev20191119\r\n`\r\nThe memory profile still keeps increasing (image below) with each training fold which was not observed in TF1.X\r\n![tfnightly2 x_memleak](https://user-images.githubusercontent.com/6624314/69220250-7cd90080-0baf-11ea-9184-80c4b5aa9b25.png)\r\n\r\nI haven't tested with tf-nightly (non-gpu) version yet\r\n\r\n**UPDATE**\r\nAlso the same outcome with tf-nightly.", "Thanks @SivamPillai for the update. We'll continue to investigate into this.", "@rchao nightly fixed it for me", "Thanks @birdmw! Can you share with us the version of TF you're using?", "I forget. It's whatever the latest nightly was on Saturday the 16th this\nlast weekend.\n\nOn Thu, Nov 21, 2019, 12:49 PM Rick Chao <notifications@github.com> wrote:\n\n> Thanks @birdmw <https://github.com/birdmw>! Can you share with us the\n> version of TF you're using?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33030?email_source=notifications&email_token=AAWKJTBCQLD4XH5XY2JWOELQU3X47A5CNFSM4I5HVD6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE3TIHA#issuecomment-557265948>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAWKJTGINVQREMQ5WTSML7LQU3X47ANCNFSM4I5HVD6A>\n> .\n>\n", "Yes if you can tell me the commit version or something I can experiment and share the results here. Thanks", "this should be fixed in `2.1.0-rc0`", "Closing the issue as it's confirmed fixed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33030\">No</a>\n", "Unfortunately, it is not fully fixed for me in `2.1.0-rc0`. The memory usage for the code posted by @ipsec has greatly improved but it works almost 10x slower with `2.1.0-rc0` than with `1.14.0`. @rchao, should I post more details here or open a new issue?", "Thanks for reporting @gdudziuk. Would you mind opening a new issue describing what have been fixed and what have not?", "I am not able to confirm if this works as suggested since I have modified the approach to delete the model after each iteration and reload it again. This way I was able to address the memory issue and since my model was not too large this was manageable. But I do agree that the operations in 2.x do seem to be slower than equivalent operations in TF 1.x. Is there any way you are aware that we can optimize the code to help the performance (Some standard guidelines for example). I am working on getting some benchmarks... May be I will raise a new issue based on that. Thanks", "No problem, @rchao. I have described the behavior with `2.1.0-rc*` in the new issue #35124.", "I'm using tensorflow-gpu 2.1.0 and problem is still not solved.\r\nHave anybody faced same issue?", "For me the memory leak is fixed with TF 2.1.0. I get low and stable memory usage for the above test code. There is a speed problem however, but this is a separate issue (cf. #35124).", "For me the memory leak is not fixed in tensorflow 2.1.0, Windows 10 and Python 3.7.6 (64 bits). After using tf.keras.backend.clear_session(), the memory leak is fixed as a workaround.", "> For me the memory leak is not fixed in tensorflow 2.1.0, Windows 10 and Python 3.7.6 (64 bits). After using tf.keras.backend.clear_session(), the memory leak is fixed as a workaround.\r\n\r\nWhere exactly did you use it?\r\nAfter tensorflow import?", "> For me the memory leak is not fixed in tensorflow 2.1.0, Windows 10 and Python 3.7.6 (64 bits). After using tf.keras.backend.clear_session(), the memory leak is fixed as a workaround.\r\n\r\nNever mind. I just added it after import tensorflow and my issue was solved.\r\nThanks!", "Thank you for the important hint/workaround with the clear_session().\r\n\r\nIn my case this is at least a good workaround till the memory leak will be fixed.\r\n\r\nI tried TensorFlow 2.0 and observed the memory leak. Then I installed TensorFlow 2.1 since I hoped that it would be fixed there. Unfortunately still not yet!", "> Thank you for the important hint/workaround with the clear_session().\r\n> \r\n> In my case this is at least a good workaround till the memory leak will be fixed.\r\n> \r\n> I tried TensorFlow 2.0 and observed the memory leak. Then I installed TensorFlow 2.1 since I hoped that it would be fixed there. Unfortunately still not yet!\r\n\r\nme too", "> For me the memory leak is not fixed in tensorflow 2.1.0, Windows 10 and Python 3.7.6 (64 bits). After using tf.keras.backend.clear_session(), the memory leak is fixed as a workaround.\r\n\r\nHow is tf.keras.backend.clear_session() used?  I am manually iterating over epochs, calling\r\nfit_generator() for each epoch.  Can I call clear_session() after each call to fit_generator(), or\r\nwill it loose the training obtained for each epoch if I do that?  (i.e., what exactly does clear_session\r\ndo?)", "The memory leak occurs for me on TF 2.1.0 on Ubuntu 18.04 with Python 3.6.10. The memory leak occurs in fit_generator. I'm using generators for both training and validation, it seems that the validation generator generates much more batches than needed. It starts filling up the cache memory until it crashes. So far, none of the workarounds work for me including clear_session(). I didn't have any problems using generators in TF 1.13.1 (no leak) but I need to use Tensorflow-addons that works only for > TF 2.0. \r\nI implemented a Sequence but still leak continues.\r\nDoes anyone still have this issue or any more workarounds?  I think about using train_on_batch instead of generators. ", "Hi dogacbasaran,\r\n\r\n> So far, none of the workarounds work for me including clear_session(). \r\n\r\nI can confirm, that even calling clear_session() of the Keras-backend did not solve the memory leak for me neither. Only restarting the Python-Kernel worked for me.\r\n\r\nCurrently the memory leak in TF 2.0 and TF 2.1 (I run it under Ubuntu 18.04) limits how many epochs I can train with the Keras backend fit() method.\r\n\r\nI hope the memory leak will be fixed in the next TF version.\r\n\r\nWith TF 1.x + Keras as a separate library I had never such memory leak problems.", "If something is wrong, why don't you just open another issue? This one is already closed so probably Tensorflow team will not react to your comments here.\r\n\r\n> I hope the memory leak will be fixed in the next TF version.\r\n\r\nChances are that it will not be fixed if nobody reports it.\r\n\r\n> For me the memory leak is not fixed in tensorflow 2.1.0, Windows 10 and Python 3.7.6 (64 bits). After using tf.keras.backend.clear_session(), the memory leak is fixed as a workaround.\r\n\r\n@taborda11, it's nice that this works, but as you note, it sounds more like a workaround than a real fix. According to the documentation, `clear_session` \"Destroys the current TF graph and creates a new one\". Sounds like an overkill likely to impact the performance if you to do it several times, e.g. in a loop. I don't know your code and what you are trying to achieve but maybe it _should_ work without `clear_session`? If so, then it is definitely and good matter to report as an issue.\r\n\r\n> The memory leak occurs for me on TF 2.1.0 on Ubuntu 18.04 with Python 3.6.10. The memory leak occurs in fit_generator.\r\n\r\n@dogacbasaran, according to the documentation of TF 2.1.0 concerning `fit_generator`: \"THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators.\" So, have you tried `Model.fit`?\r\n\r\n> I implemented a Sequence but still leak continues.\r\n> Does anyone still have this issue or any more workarounds? I think about using train_on_batch instead of generators.\r\n\r\nAnother option are Datasets (`tf.data.Dataset`). I'm having the impression that Datasets are now the suggested way of providing data to models in Tensorflow. For example, [this guide](https://www.tensorflow.org/guide/effective_tf2) states that: \"When iterating over training data that fits in memory, feel free to use regular Python iteration. Otherwise, `tf.data.Dataset` is the best way to stream training data from disk.\"", "@gdudziuk I still use fit_generator because I need to use a generator for validation as well but Model.fit does not allow it. It expects a validation set. It is also not possible to input a generator for training data and use a part of it as validation. ", "Oh, fact. That's an inconsistency in Model.fit. And what about switching to datasets? You say you have to use generators, but generators may be wrapped in datasets and then passed to Model.fit - see `tf.data.Dataset.from_generator`. ", "@gdudziuk it seems that fit_generator uses tf.data.Dataset. This might be the reason for the memory leak. I implemented train_on_batch method and I still observe the memory leak. I generate the batches manually and feed into train_on_batch, and the cache memory continues to increase as I feed the batches to the train_on_batch method.  ", "I have just run a test with TF 2.1.0 and some simple test model. I have generated some random synthetic data as Numpy arrays, then wrapped those data into generators, then wrapped those generators into tf.data.Datasets and then run a training with the use of Model.fit for 100 epochs. In the run of training, I have observed no significant increase in the memory usage associated with the test script. So there has been no obvious memory leak.\r\n\r\nOk, the memory usage increased ~5 MB between the epoch 1 and the epoch 100, but this is not much relative to the data size (~120 MB in my test) and the general usage of RAM when running a Tensorflow training (over ~1.5 GB in my test). It could be a small memory leak but also it could be an effect of normal operation.\r\n\r\nI'm using Kubuntu 18.04 and Python 3.6.9 so it seems that our environments are fairly similar. I'm installing Tensorflow via pip.", "I've been struggling with apparent memory leak using tf 2.1.  \r\nMy code is training on variable length segments.  These are grouped into\r\nequal length batches and provided by a generator to model.fit.\r\n\r\nToday I tested tf-nightly\r\ntf_nightly-2.2.0.dev20200303\r\n\r\nIt appears memory usage is a lot better.", "I was having this issue using a custom keras Iterator class in tf 2.1.\r\n\r\nI solved it by turning my custom keras Iterator class into a generic class with a callable generator method (that yields batches) and using it to create a tf Dataset with tf.data.Dataset.flow_from_generator() . \r\nUsing tf Dataset got rid of the memory leak.\r\n\r\nFor some reason keras generators (e.g. Sequence, Iterator) aren't being garbage collected in .fit() .\r\n", "@emuccino could you elaborate on your solution/provide example code? Conversion to a data set with tf.data.Dataset.from_generator() lead to poor performance (although the memory leak is fixed then as well). ", "> For me the memory leak is not fixed in tensorflow 2.1.0, Windows 10 and Python 3.7.6 (64 bits). After using tf.keras.backend.clear_session(), the memory leak is fixed as a workaround.\r\n\r\nThank you so much! That totally solved my problem in tf 2.2.0! ", "> > For me the memory leak is not fixed in tensorflow 2.1.0, Windows 10 and Python 3.7.6 (64 bits). After using tf.keras.backend.clear_session(), the memory leak is fixed as a workaround.\n> \n> \n> \n> Thank you so much! That totally solved my problem in tf 2.2.0! \n\nHello! You said that tf 2.2.0 with using tf.keras.backend.clear_session() solve your problem.\nHow did you solve it? Is it call the tf.keras.backend.clear_session() once after import TensorFlow? Or every single step use it?Would you mind provide some code example or information?", "I'm seeing the same issue where the batches produced by the generator aren't being freed up on the CPU after the epoch. This is clear because the memory and time grow linearly with each epoch (see screenshot). Neither `tf.data.Dataset.from_generator` nor `tf.keras.backend.clear_session()` solve the problem. I've also tried `import gc; gc.collect()` within the generator loop to no avail. And I've checked that manually iterating through the generator outside of `fit` does not result in increasing memory and time, and so it seems like something inside of `fit` is clinging onto old data. Using TF 2.2.0, Python 3.7.7, and Ubuntu 18.04.\r\n\r\n![memory-leak](https://user-images.githubusercontent.com/5420057/87240257-f6935800-c3e5-11ea-8b75-e56a847430a0.png)", "@arvoelke I think this issue wasn't solve yet, the problem of tf.data.Dataset.from_generator was mentioned in this issue [#37653](https://github.com/tensorflow/tensorflow/issues/37653)", "How do I get off this thread it's blowing up my inbox\n\nOn Sat, Jul 11, 2020, 11:24 PM Leow Chee Siang <notifications@github.com>\nwrote:\n\n> @arvoelke <https://github.com/arvoelke> I think this issue wasn't solve\n> yet, the problem of tf.data.Dataset.from_generator was mentioned in this\n> issue #37653 <https://github.com/tensorflow/tensorflow/issues/37653>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33030#issuecomment-657181160>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAWKJTBZREABGK3ND4SQEIDR3FJLBANCNFSM4I5HVD6A>\n> .\n>\n", "Hi, I am still facing the same issue. I have tested on TensorFlow 2.0, 2.2 and 2.3, with Windows (python 3.7) and Ubuntu (python 3.8) and there is a memory leak using model.predict(). \r\n\r\nI have built an API and my model is on the server side. Every time I call my prediction function using a simple request, the instance memory grows up.\r\n\r\nUsing tf.keras.backend.clear_session() didn't solve the issue.", "> Hi, I am still facing the same issue. I have tested on TensorFlow 2.0, 2.2 and 2.3, with Windows (python 3.7) and Ubuntu (python 3.8) and there is a memory leak using model.predict().\r\n> \r\n> I have built an API and my model is on the server side. Every time I call my prediction function using a simple request, the instance memory grows up.\r\n> \r\n> Using tf.keras.backend.clear_session() didn't solve the issue.\r\n\r\nI hit the same issue, but if I found that using model(input) rather than model.predict(input) is a workaround and produces correct result. I verified this by converting the model to TensorRT and comparing results.", "> Hi, I am still facing the same issue. I have tested on TensorFlow 2.0, 2.2 and 2.3, with Windows (python 3.7) and Ubuntu (python 3.8) and there is a memory leak using model.predict().\r\n> \r\n> I have built an API and my model is on the server side. Every time I call my prediction function using a simple request, the instance memory grows up.\r\n> \r\n> Using tf.keras.backend.clear_session() didn't solve the issue.\r\n\r\ntake me back to tensorflow 1x please", "I have similar issue. My code is killed after several epochs of training. Using htop to check the memory. Found that during evaluation step, the memory usage goes unbounded until it crashes. Disabling EarlyStopping callback resolved my issue (for now). \r\n\r\nBTW, I am using default model.fit, with customized loss function.\r\n\r\ntf version: 2.3", "Have the same problem with predict, training works fine.", "We had the same issue with tensorflow 2.3.  The memory usage kept growing during model fitting.  Disabling EarlyStopping did not have any impact for us.", "> We had the same issue with tensorflow 2.3.  The memory usage kept growing during model fitting.  Disabling EarlyStopping did not have any impact for us.\n\nAre there any other callbacks you are using? I tried to disable them one by one and in my case it was Earlystopping.", "> > We had the same issue with tensorflow 2.3.  The memory usage kept growing during model fitting.  Disabling EarlyStopping did not have any impact for us.\r\n> \r\n> Are there any other callbacks you are using? I tried to disable them one by one and in my case it was Earlystopping.\r\n\r\nYes, we are using others.  Thanks for the hint, we'll investigate when we get a chance.", "> > > We had the same issue with tensorflow 2.3.  The memory usage kept growing during model fitting.  Disabling EarlyStopping did not have any impact for us.\r\n> > \r\n> > \r\n> > Are there any other callbacks you are using? I tried to disable them one by one and in my case it was Earlystopping.\r\n> \r\n> Yes, we are using others. Thanks for the hint, we'll investigate when we get a chance.\r\n\r\nWe turned off all callbacks but still had the memory leak issue.", "I can confirm that this issue persists with model.train_on_batch. The situation is more severe without a GPU. I am using the latest version 2.3.1 with git version 2.3.0-54-gfcc4b966f1. And tf.keras.backend.clear_session() doesn't work for me.\r\nOne workaround is to write everything using custom training loop. But this is not desired.", "This issue is NOT closed! ", "Found a workaround, del together with gc.collect() working fine for me.:\r\n\r\n           `images_list = np.vstack(images_to_load)\r\n            pred_result = self.model.predict(images_list, batch_size=10)\r\n\r\n            del(pred_result)\r\n            gc.collect()`", "train_on_batch still has a memory leak... del and gc.collect() does not work", "It's been 2 weeks that I am having this problem too. My memory usage double while my model is fitting.\r\n\r\nBefore doing `gc.collect()` as suggested by @heilwood you can use `tf.keras.backend.clear_session()`.\r\nAnyway, this is just a workaround not a fix !\r\n", "I can't get a work around to be successful because I'm using combined models for GANs. This is very annoying. \r\n", "> I can't get a work around to be successful because I'm using combined models for GANs. This is very annoying.\r\n\r\n@Emile0205   Hi, after 4 months, have you solved the memory problem?", "https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\nThis works well \ud83d\udc4c", "> https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n> \r\n> This works well \ud83d\udc4c\r\n\r\n@Emile0205 So you have totally abandoned the high level fit() and predict() ?", "> > https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n> > This works well \ud83d\udc4c\r\n> \r\n> @Emile0205 So you have totally abandoned the high level fit() and predict() ?\r\n\r\nYou can actually overload train_step, test_step and predict_step, which will be called by fit and predict.", "Unfortunately yea. There\u2019s definitely a memory leak (at least for combined models) in these functions. I\u2019m assuming it isn\u2019t being addressed cus everyone using complicated/combined models are using custom training loops", "> Unfortunately yea. There\u2019s definitely a memory leak (at least for combined models) in these functions. I\u2019m assuming it isn\u2019t being addressed cus everyone using complicated/combined models are using custom training loops\r\n\r\nyeap, firstly I assumed it was the tf.dataset, but after a dataset memory test, I realized it was these high level apis that caused the memory leak", "> > > https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n> > > This works well \ud83d\udc4c\r\n> > \r\n> > \r\n> > @Emile0205 So you have totally abandoned the high level fit() and predict() ?\r\n> \r\n> You can actually overload train_step, test_step and predict_step, which will be called by fit and predict.\r\n\r\n@vermouth1992 Hi, I have tried only override the train_step(), but memory leak still exists if we call fit()", "Truth be told, all Keras functions gave me issues. Check out my code at \n\nhttps://github.com/Emile0205/neural_LACU\n\nfor a simple Tensorflow 2.2 neural network. I extended this same code for my GAN. \n\nHope that helps \nGood luck ", "> Truth be told, all Keras functions gave me issues. Check out my code at\r\n> \r\n> https://github.com/Emile0205/neural_LACU\r\n> \r\n> for a simple Tensorflow 2.2 neural network. I extended this same code for my GAN.\r\n> \r\n> Hope that helps\r\n> Good luck\r\n\r\n@Emile0205 Sad story! I try custom training loop instead of model.fit(), but memory leak still exists. ", "How do I get off this email thread?\n\nOn Wed, Mar 10, 2021, 12:34 AM Jayce <notifications@github.com> wrote:\n\n> Truth be told, all Keras functions gave me issues. Check out my code at\n>\n> https://github.com/Emile0205/neural_LACU\n>\n> for a simple Tensorflow 2.2 neural network. I extended this same code for\n> my GAN.\n>\n> Hope that helps\n> Good luck\n>\n> @Emile0205 <https://github.com/Emile0205> Sad story! I try custom\n> training loop instead of model.fit(), but memory leak still exists.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33030#issuecomment-795084723>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAWKJTAYJ5WHKFNMB4BEEFDTC4VJHANCNFSM4I5HVD6A>\n> .\n>\n", "https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237", "For those looking for a reliable workaround for leaks that can't be solved directly, I created this package: `scriptifier`\r\n\r\nIt's a decorator that seamlessly runs any function as a separate script, it automatically generates the script and takes care of passing the arguments and returns (as long as they are pickleable or keras models or lists of keras models...) (for documentation see: [github](https://github.com/ben981/scriptifier)\r\n\r\nWhen the function the function ends, it returns the results and 100% of the memory allocated by that function is freed.\r\n\r\nIt installs via `pip install scriptifier`\r\n\r\nIt should look like this\r\n\r\n```\r\nfrom scriptifier import scriptifier\r\n\r\ndef func_1(in):\r\n    ...\r\n    model.fit()\r\n    ...\r\n    return out\r\n\r\nscriptified_func_1 = scriptifier.run_as_script(func_1)\r\nout = scriptified_func_1(in)\r\n```"]}, {"number": 33029, "title": "[INTEL MKL] Changing import orders to fix bazel test failures with (--config=mkl) ", "body": "Otherwise, there will be \"libiomp5.so\" No such file or directory failure. ", "comments": ["@caisq  Yes, for now there is another way to fix (while we work on figuring out a more elegant fix). \r\nWe want to modify ./configure.py so that it appends the following line to the .tf_configure.bazel_rc \r\n\r\nwrite_to_bazelrc('test:mkl --action_env=LD_LIBRARY_PATH=\"%s\"' % (os.getcwd()+\"/bazel-out/k8-opt/bin/_solib_k8/_U\\@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib\")) \r\n\r\nWe wanted to expose the LD_LIBRARY_PATH containing the libiomp5.so to bazel test command. \r\n\r\nIf the above change is ok, I will create the PR for review.  Otherwise, we will try a third approach. Thanks!", "The commit that introduced/affected the libiomp5 library loading issue was https://github.com/tensorflow/tensorflow/commit/98f22653431e67e77e5889198357c152a8a0fae7 \r\nwhere pybind_extension was changed to tf_python_bind_extension.  Do you have any suggestions how to overcome the side-effect of library loading issue? This did not affect \"bazel build --config=mkl\", but only \"bazel test --config=mkl\".  \r\nCurrently our workarounds are to explicitly pass to bazel in one way or another an action_env that has LD_LIBRARY_PATH set to the libimop5.so file.  \r\nThanks for any suggestions in advance!", "cc @av8ramit For thoughts. Please see @wei-v-wang 's comments above regarding the build issues the recent pybind11 changes has caused to the INTEL MKL build.", "cc @penpornk ", "Thanks @caisq \r\nIn the meantime, do you like the following fix better? \r\nWe want to modify ./configure.py so that it appends the following line to the .tf_configure.bazel_rc\r\n\r\nwrite_to_bazelrc('test:mkl --action_env=LD_LIBRARY_PATH=\"%s\"' % (os.getcwd()+\"/bazel-out/k8-opt/bin/_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib\"))\r\n\r\nThanks!", "I think fixing the linking options is better. \r\n@gunan, what do you think about the proposed solution in https://github.com/tensorflow/tensorflow/pull/33029#issuecomment-538155728? ", "Adding @penpornk to Rewiewer list, as she's working with the MKL group more closely.\r\n\r\nI'm just providing drive-by comments to code relating to python/debug and core/debug. I don't have informed opinions about the build infrastructure.", "Thanks everyone for your review. Our team figured out an elegant fix. Thanks @agramesh1 and @mahmoud-abuzaina ! \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/33058 deprecates this PR. Hence closing. "]}, {"number": 33028, "title": "tf keras progbar callback is not displayed during keras.fit() when verbose=0.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary (conda install tensorflow-gpu==1.14.0)\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Python 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: nvidia 1080, 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAttempting to use progbar as a callback to model.fit() fails.\r\n**Describe the expected behavior**\r\nIf runing model.fit() with progbar callback and verbosity=0 should be identical to verbosity=1 and no progbar callback\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ncurrent output:\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport tensorflow as tf\r\n\r\nprint('tf_version:', tf.__version__, 'gpu available:', tf.test.is_gpu_available())\r\nmodel = tf.keras.applications.ResNet50()\r\n\r\n\r\nprint('compiling model')\r\nmodel.compile(optimizer='SGD', loss=tf.keras.losses.categorical_crossentropy)\r\n\r\nprint('running fit function')\r\nx = tf.data.Dataset.from_tensors(tf.zeros([16]+model.input.shape.as_list()[1:]))\r\ny = tf.data.Dataset.from_tensors(tf.zeros([16]+model.output.shape.as_list()[1:]))\r\nprint('x:', x, '\\ny', y)\r\nmodel.fit(tf.data.Dataset.zip((x,y)).repeat().shuffle(buffer_size=1),\r\n          steps_per_epoch=10,\r\n          verbose=0,\r\n          callbacks=[tf.keras.callbacks.ProgbarLogger('steps')])\r\nprint('done')\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n*running the code above recreates the problem.*\r\n```\r\ntf_version: 1.14.0 gpu available: True\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1003 10:20:49.769316 140195608012544 deprecation.py:506] From /home/hackerman/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\ncompiling model\r\nrunning fit function\r\nx: <DatasetV1Adapter shapes: (16, 224, 224, 3), types: tf.float32> \r\ny <DatasetV1Adapter shapes: (16, 1000), types: tf.float32>\r\ndone\r\n```\r\n*expected output (reproduced by commenting out callback, and setting verbose to 1):*\r\n```\r\ntf_version: 1.14.0 gpu available: True\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1003 10:21:47.987383 140242665334528 deprecation.py:506] From /home/hackerman/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\ncompiling model\r\nrunning fit function\r\nx: <DatasetV1Adapter shapes: (16, 224, 224, 3), types: tf.float32> \r\ny <DatasetV1Adapter shapes: (16, 1000), types: tf.float32>\r\n10/10 [==============================] - 6s 592ms/step - loss: 0.0000e+00\r\ndone\r\n```\r\n*bonus bug (if keeping callback and setting verbosity to 1):*\r\n```\r\ntf_version: 1.14.0 gpu available: True\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1003 10:23:10.488721 140225721059072 deprecation.py:506] From /home/hackerman/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\ncompiling model\r\nrunning fit function\r\nx: <DatasetV1Adapter shapes: (16, 224, 224, 3), types: tf.float32> \r\ny <DatasetV1Adapter shapes: (16, 1000), types: tf.float32>\r\n10/10 [==============================] - 6s 597ms/step - loss: 0.0000e+00\r\n10/10 [==============================] - 6s 597ms/step - loss: 0.0000e+00\r\ndone\r\n```\r\n", "comments": ["Was able to reproduce this issue. Here's my github [gist](https://colab.sandbox.google.com/gist/gowthamkpr/7396f9add006cd8fe1f0ce1a943e643e/untitled167.ipynb)", "@hollowgalaxy `verbose=0` is intended to suppress all output", "@omalleyt12 If both the call back is provided and verbose is set to 1, the progress is displayed twice. Isn't it a bug?\r\n\r\n```\r\ntf_version: 1.14.0 gpu available: True\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1003 10:23:10.488721 140225721059072 deprecation.py:506] From /home/hackerman/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\ncompiling model\r\nrunning fit function\r\nx: <DatasetV1Adapter shapes: (16, 224, 224, 3), types: tf.float32> \r\ny <DatasetV1Adapter shapes: (16, 1000), types: tf.float32>\r\n10/10 [==============================] - 6s 597ms/step - loss: 0.0000e+00\r\n10/10 [==============================] - 6s 597ms/step - loss: 0.0000e+00\r\ndone\r\n\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33028\">No</a>\n", "@gowthamkpr Hm I see what you're saying. \r\n\r\nThe ProgBarLogger callback is automatically added when you set verbose != 0. Users shouldn't need to add this to their `fit` call manually. Arguably we could check to see if a ProgBarLogger callback already exists before automatically adding, but as this doesn't crash anything and just duplicates the outputs I'm OK with leaving it like this for now. Please feel free to submit a PR though!", "> `verbose=0` is intended to suppress all output\r\n\r\n@omalleyt12  thats not true. I can print stuff while `verbose=0` with a custom callback. The bug, I'm pointing out is that using `tf.keras.callbacks.ProgbarLogger` with `verbose=0` doesn't get printed.\r\n``` \r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport tensorflow as tf\r\n\r\nprint('tf_version:', tf.__version__, 'gpu available:', tf.test.is_gpu_available())\r\nimport datetime\r\n\r\n# copied from https://www.tensorflow.org/guide/keras/custom_callback\r\nclass MyCustomCallback(tf.keras.callbacks.Callback):\r\n\r\n  def on_train_batch_begin(self, batch, logs=None):\r\n    print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\r\n\r\n  def on_train_batch_end(self, batch, logs=None):\r\n    print('Training: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\r\n\r\n  def on_test_batch_begin(self, batch, logs=None):\r\n    print('Evaluating: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\r\n\r\n  def on_test_batch_end(self, batch, logs=None):\r\n    print('Evaluating: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\r\n    \r\n\r\nmodel = tf.keras.applications.ResNet50()    \r\nprint('compiling model')\r\nmodel.compile(optimizer='SGD', loss=tf.keras.losses.categorical_crossentropy)\r\n\r\nprint('running fit function')\r\nx = tf.data.Dataset.from_tensors(tf.zeros([1]+model.input.shape.as_list()[1:]))\r\ny = tf.data.Dataset.from_tensors(tf.zeros([1]+model.output.shape.as_list()[1:]))\r\nprint('x:', x, '\\ny', y)\r\nmodel.fit(tf.data.Dataset.zip((x,y)).repeat().shuffle(buffer_size=1),\r\n          steps_per_epoch=2,\r\n          verbose=0,\r\n          callbacks=[MyCustomCallback()])\r\nprint('done')\r\n```\r\nreturns the following \r\n```\r\ntf_version: 1.14.0 gpu available: False\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\r\n102858752/102853048 [==============================] - 1s 0us/step\r\ncompiling model\r\nrunning fit function\r\nx: <DatasetV1Adapter shapes: (1, 224, 224, 3), types: tf.float32> \r\ny <DatasetV1Adapter shapes: (1, 1000), types: tf.float32>\r\nTraining: batch 0 begins at 22:45:37.495002\r\nTraining: batch 0 ends at 22:45:43.157376\r\nTraining: batch 1 begins at 22:45:43.158103\r\nTraining: batch 1 ends at 22:45:44.163262\r\ndone\r\n```\r\n", "In addition, how can one use `tf.keras.callbacks.ProgbarLogger` when the goal is to override it?", "@hollowgalaxy You could set verbose=0 in `fit` and then override `on_train_begin` to ignore the `verbose` setting in your overridden version:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/58b283fdc9428774f555dafb8ac347ae06bce264/tensorflow/python/keras/callbacks.py#L739"]}, {"number": 33027, "title": "tf.keras.layers.BatchNormalization produces wrong results when original feature scale is moderately small.", "body": "It looks like BatchNormalization fails to scale features up if the original scale is too low ... or I am doing something brain dead here. \r\n\r\nIf this is actually a constraint on the scale param, it should be very explicitly documented. \r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0rc2\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\nx = np.random.randn(1000, 1).astype(np.float32)\r\n\r\nx = x * 0.001\r\n\r\nbn = tf.keras.layers.BatchNormalization()\r\n\r\n@tf.function\r\ndef _run_one(x):\r\n    bn(x, training=True)\r\n\r\ndef run(x):\r\n    for i in range(10000):\r\n        _run_one(x)\r\n\r\ntf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))\r\nrun(x)\r\ntf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))\r\n\r\n[0.000992934452] [-2.47328098e-05]\r\n[0.0313995518] [-2.12341544e-09]\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\nx = np.random.randn(1000, 1).astype(np.float32)\r\n\r\n# x = x * 0.001\r\nx = x * 2\r\n\r\nbn = tf.keras.layers.BatchNormalization()\r\n\r\n@tf.function\r\ndef _run_one(x):\r\n    bn(x, training=True)\r\n\r\ndef run(x):\r\n    for i in range(10000):\r\n        _run_one(x)\r\n\r\ntf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))\r\nrun(x)\r\ntf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))\r\n\r\n\r\n[2.03433657] [0.100093901]\r\n[0.999882281] [1.81198118e-07]\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee above.\r\n\r\n\r\n**Other info / logs**\r\n", "comments": ["I have tried on colab with TF version 2.0.0rc2 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9dd7984692101e09e5c9b816b1a3643a/untitled245.ipynb).Thanks!", "This is not a bug but expected.\r\nUsing `BatchNormalization(epsilon=0.0)` will make it print your expected output.", "> This is not a bug but expected.\r\n> Using `BatchNormalization(epsilon=0.0)` will make it print your expected output.\r\n\r\n\r\nSeems correct.\r\n\r\n    epsilon=0.001,\r\n\r\nThis default is way too low and we should get a warning added to the docs ... the default should be much, much lower.", "@cottrell Would you like to create any PR to update the docs? Thanks!", "Seems like the incentives are aligned to leave it as is. But let's leave this issue here for now. I might follow up with some digging on the source of this bad-value attack.", "PR something but I likely have to read more about how to do this. Am hoping the PR triggers some tests that I can then chase later. Let me know if this is bad.\r\n", "Closing as intended behavior.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33027\">No</a>\n"]}, {"number": 33026, "title": "Why is CUDA 10.1 not supported & strange error message?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install tensorflow           (2.0 version)\r\n- TensorFlow version (use command below): Tensorflow v2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia Quadro M1000M\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to use Tensorflow 2.0 with CUDA 10.1 I run into some errors. Previously I haven't seen these errors with older versions of keras/tensorflow.\r\n\r\n![image](https://user-images.githubusercontent.com/2350015/66144475-32082700-e609-11e9-8fa6-cb88e3692f53.png)\r\n\r\nCode:\r\n\r\n```\r\ntf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\r\n```\r\n\r\nError:\r\n```\r\n\r\nTensorflow v2.0.0\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-4-ee93c3bb3dbc> in <module>\r\n      4 print(f'Tensorflow v{tf.__version__}')\r\n      5 \r\n----> 6 tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\r\n\r\nC:\\ProgramData\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\test_util.py in is_gpu_available(cuda_only, min_cuda_compute_capability)\r\n   1430 \r\n   1431   try:\r\n-> 1432     for local_device in device_lib.list_local_devices():\r\n   1433       if local_device.device_type == \"GPU\":\r\n   1434         if (min_cuda_compute_capability is None or\r\n\r\nC:\\ProgramData\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\client\\device_lib.py in list_local_devices(session_config)\r\n     39   return [\r\n     40       _convert(s)\r\n---> 41       for s in pywrap_tensorflow.list_devices(session_config=session_config)\r\n     42   ]\r\n\r\nC:\\ProgramData\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py in list_devices(session_config)\r\n   2247     return ListDevicesWithSessionConfig(session_config.SerializeToString())\r\n   2248   else:\r\n-> 2249     return ListDevices()\r\n   2250 \r\n   2251 \r\n\r\nInternalError: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect a minor release (10.0 to 10.1) which has been out for almost 2 years to work correctly.\r\n\r\nFurthermore the error doesn't specify anything regarding CUDA 10.1 not being supported. If this would be the case then that's the error I would expect.\r\n\r\n**Code to reproduce the issue**\r\nInstall the latest tensorflow with `pip install tensorflow` into a Conda environment running python 3.7 with CUDA 10.1 installed. Then run the following command:\r\n```\r\ntf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\r\n```\r\n\r\n**Other info / logs**\r\nN/A", "comments": ["@devedse, Please take a look at the similar issue [#32381](https://github.com/tensorflow/tensorflow/issues/32381). Thanks!", "@gadagashwini, yeah I saw that. It doesn't really answer the question of why though", "@devedse,\r\nTF 2.0 supports cuda 10.0 Please switch to cuda 10.0 and update cuda paths. Did you try with CUDA 10.0. Thanks!", "Yes I'm currently trying to install CUDA 10.0, it's somewhat annoying that other frameworks support the latest CUDA 10.1. Is upgrading to CUDA 10.1 on the backlog? And if so, when do you expect it to be implemented?\r\n\r\nCUDA 10.1 has been out for a long while so I would hope the newest Tensorflow could also get support for this.", "@devedse, If you want to use cuda 10.1 then you must build the whole tensorflow from source on Windows. Thanks!", "To me that sounds like it would be very easy to upgrade the dependencies to CUDA 10.1. So coming back to my question, why is TF2 targeting an old version of CUDA?", "I'd like to add a little color on why CUDA 10.1 might be important in the context of TF 2.0 Windows builds.\r\n\r\nWe're facing the same issue as in https://github.com/tensorflow/tensorflow/issues/27706 and https://github.com/tensorflow/tensorflow/issues/27576 where it's confirmed to be a known issue with nvidia cuda 10.0 that is now fixed in 10.1 and there are no plans for backporting it to 10.0.\r\n\r\nOur case is the same, Windows build env with VC 2017 Build tools and TF 2.0 fails with the error:  `nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)`\r\n\r\nAt this point we haven't found a way to build TF 2.0 with CUDA/GPU support in Windows using CUDA 10.0 (getting this ACCESS_VIOLATION error).\r\n\r\nIt would be good to know if the decision to hold off with CUDA 10.1 is related to the extra release engineering work or specific CUDA 10.1 compatibility issues that won't come out during the build.", "Can one of the tensorflow project maintainers please finally answer a simple question which seems to pop up repeatedly on this very issue tracker:\r\n\r\nQuestion: **When will prebuilt tensorflow packages for CUDA 10.1 be available?**\r\n\r\nAlternative version of the question: **When will building against CUDA 10.1 be supported?**\r\n\r\nValid answers, in order of preference:\r\n\r\n - a specific date or release version\r\n - a rough time estimate\r\n - \"never\" with clarification as to what seems to be the issue\r\n\r\nInvalid answers::\r\n\r\n - \"build it yourself if you need it\"\r\n - \"downgrade to 10.0\"\r\n\r\nThanks in advance.", "Building against cuda 10.1 is already supported. I am running continuous builds already and they are running just fine. Occasionally there has been issues, but we have fixed them.\r\n\r\nWe do not build the main packages with latest cuda version support  as soon as they are out, because we want our prebuilt packages to \"run on as many machines as possible, while still being performant\". It is a subjective and difficult line. And it is the kind of a choice which, whatever we pick, someone will be unhappy.\r\n\r\nThe reason new CUDA versions do not work with most machines is, every new CUDA version requires a driver version that is \"too new\" on majority of linux distributions. My current linux distro still has not blessed drivers that work with cuda 10.1. So I had to uninstall drivers that came with the system and jump through hoops to get the newer drivers. And as we realized it was difficult, we made the choice to not upgrade.\r\n\r\nThe above is the justification for not having the CUDA 10.1 support for the main TF releases yet. I am sorry if it is not a satisfactory answer. We are evaluating if we can upgrade to 10.1 for 2.1, but we are still just exploring.\r\n\r\nFinally, as always, TF has details building from sources instructions for all operating systems. I know that it is difficult, but if you have a use case that cannot do without cuda 10.1, or cuda 9, or 9.2, our official answer is always going to be \"you will need to build those packages yourself\", however unpopular it is.", "@gunan Thanks for the response.\r\n\r\nYou say \"We do not build the main packages with latest cuda version support _as soon as they are out_\", but CUDA 10.1 was out on **February 27th, 2019**.\r\n\r\nIn the meantime there were two binary compatible releases (10.1 update 1 and 10.1 update 2 released in August) which brought some important bugfixes and changes, including how CUDA DLLs are named, which compilers and systems are supported, performance improvements and proper support for RTX cards, and for Windows platform also nvJPEG library which wasn't available until 10.1 update 2 and which is quite a boost for any batch image processing for ML.\r\n\r\nAlthough my experience with installing or updating NVIDIA drivers under Linux was always problem-free I understand that you may personally find it inconvenient or even intimidating to update drivers, but a big part of CUDA runtime support is in the driver itself and that's why it has to change in sync with it -- if you want new features, better performance and support for new hardware then updating the drivers is the only way forward.\r\n\r\nSome people use CUDA for other things, not just for TF and some of the features only exist or work properly in the latest version so it is not really easy to \"just downgrade to 10.0\".\r\n\r\nAs for building from source, I did try that few versions ago on Windows and half way through bazel barfed a bunch of indecipherable errors. Not having a clue how the whole build system works and how to even begin to untangle it I gave up, hoping that the main TF package will one day catch up. Sadly, that day seems to remain forever in the future.\r\n\r\nFinally, I hope I am not coming across as confrontational or ungrateful, but if the choice is between the TF devs updating drivers to make a new CUDA 10.1 based release and everyone else having to learn the intimate details of the TF's bazel build system to be able to roll their own to me the former wins and the latter is quite unrealistic to expect even though it may be your official response.\r\n\r\nThank you for your time.", "I understand your concern. But as I mentioned before, this is a subjective decision, which will result in a lot of people being unhappy whatever position we choose.\r\nAs majority of our github issues show that a lot of people have problems installing CUDA/nvidia drivers, we choose to update is as little as possible.\r\n\r\nI am closing this issue, as the build on windows is now working, and the discussion is diverging.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33026\">No</a>\n", ":( still an issue and not ideal.. \r\n\r\nCuda 10.2 is already out and even Pytorch already supports Cuda 10.1\r\n\r\nconsidering moving to Pytorch..", "Huggingface is looking strong. They use Pytorch on CUDA 10.1 - we are thinking of switching over too.", "@birdmw @kooscode @levicki Please see https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc1\r\n\r\n`pip package is built with CUDA 10.1 and cuDNN 7.6.`", "[UPDATE] - Installing NIGHTLY version has solved this for me.\r\n\r\n@ahtik - I tried installing to Windows 10 per your link as `pip install tensorflow-gpu==2.1.0-rc1`. However, I received the following: `ERROR: Could not find a version that satisfies the requirement tensorflow-gpu-estimator<2.2.0,>=2.1.0rc0 (from tensorflow-gpu==2.1.0-rc1)` - do you have any suggestion to overcome this issue?\r\n\r\nI can pip install `tensorflow-estimator==2.1.0rc0` but not `tensorflow-gpu-estimator==2.1.0rc0` and in fact I cannot find `tensorflow-gpu-estimator` anywhere on line. However, after installing `tensorflow-estimator-2.1.0rc0` the error installing `pip install tensorflow-gpu==2.1.0-rc1` persists. ", "@ahtik Too late for me, I am already on CUDA 10.2."]}, {"number": 33025, "title": "makes possible to move stderr on run_shell at configure.py and non show traceback message when find site packages", "body": "this PR makes possible move stderr to some file and prevents that the bellow message is shown when python fails to use `site.getsitepackages()` using virtual environment.\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'site' has no attribute 'getsitepackages'\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33025) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33025) for more info**.\n\n<!-- ok -->", "@wallysslima  Could you please address Ubuntu Sanity errors? Thanks!", "Sure @gbaned. For now, I updated wallysslima:master from tensorflow:master."]}, {"number": 33024, "title": "Performance: Training is much slower in TF v2.0.0 VS v1.14.0 when using `Tf.Keras` and `model.fit_generator` ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n\r\n**System information**\r\n**NOTE**: I have provided Google Colab' notebooks to reproduce the slowness. \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): sortof, but is a basically and MNIST example.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab and Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10 or Google Colab\r\n- GPU model and memory: 1080 Ti, or Google Colab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nIt happens on the standard Colab GPU instance\r\n\r\n**Describe the current behavior**\r\n**Version 2.0.0** is SLOW compared to the identical code running **v1.14.0**. \r\nThe code I have used to demonstrate it is very simple, and very similar to most existing Keras examples. \r\nA Larger NN on MNIST is going from `10s` per epoch to `~20s`  and that is a very major slowdown.\r\n\r\n**Describe the expected behavior**\r\nA new version should have similar or better performance than the previous version. \r\nIf user error or a new limitation/feature is causing the problem, it should be warned about in Update Notes/Quick Start. This code was perfectly normal in TF 1.X\r\n\r\n**Code to reproduce the issue**\r\nSee this (GPU) Colab Notebook example with MNIST Data:\r\nhttps://colab.research.google.com/gist/Raukk/f0927a5e2a357f2d80c9aeef1202e6ee/example_slow_tf2.ipynb\r\n\r\nSee this (GPU) Colab Notebook example with numpy random for Data:\r\nhttps://colab.research.google.com/gist/Raukk/518d3d21e08ad02089429529bd6c67d4/simplified_example_slow_tf2.ipynb\r\n\r\nSee this (GPU) Colab Notebook example using standard Conv2D (not DepthwiseConv2D):\r\nhttps://colab.research.google.com/gist/Raukk/4f102e192f47a6dc144b890925b652f8/standardconv_example_slow_tf2.ipynb\r\n\r\n\r\n**Please notify me if you cannot access any of these notebooks, or if they do not run, or don't sufficiently reproduce the issue.**\r\n\r\n\r\n**Other info / logs**\r\nEach example above starts with a **TLDR;** that gives a very basic summary of results. \r\n\r\n\r\nThank you!", "comments": ["Can confirm from my own experience with it. I had a similar issue with my own project when switching to TF2 (stable. I waited for the official release a few days ago), with a 2x to 3x decrease in training time for the same data and code, as compared to TF1. After some Google searching and reading, I then proceeded to implement the code using tf.data.Dataset.from_generator(), instead, which allows me to use model.fit().\r\n\r\n**Unfortunately there was 0 performance benefit either way.**\r\n\r\nAs for some pseudocode (posting here just in case someone can point out something fundamentally wrong with my setup), my fit_generator version of my code went something like this below. All my code uses the internal tf.keras instead of the external one:\r\n\r\n    def datagen(args):\r\n        while True:\r\n            #some code here to load and manipulate data into x and y. Mostly numpy functions\r\n            yield x,y\r\n\r\n    #some code here to create and compile model \r\n\r\n    model.fit_generator(datagen(args), . . . )\r\n\r\n\r\n\r\nFor the pseudocode using tf.data.Dataset.from_generator():\r\n\r\n    from tensorflow.compat.v2.data import Dataset\r\n\r\n    def datagen(args):\r\n        while True:\r\n            #some code here to load and manipulate data into x and y. Mostly numpy functions\r\n            yield x,y\r\n\r\n    #some code here to create and compile model \r\n\r\n    train_data = Dataset.from_generator(generator=lambda: datagen(args), . . . )\r\n    model.fit(train_data , . . . )", "Have also experienced a large (4x) increase in training times for Keras models when using fit_generator  after upgrading to TensorFlow 2.0. \r\n\r\nExecution times became comparable to TF 1.14 when disabling eager execution by running:\r\n`tf.compat.v1.disable_eager_execution()`.\r\n\r\n", "@Szubie Thanks, that does seem to improve the performance back to standard levels, or at least close enough. \r\n\r\n\r\nI wonder if there is a better fix for this issue?\r\n\r\n\r\n`tf.compat.v1.disable_eager_execution()` seems like a workaround more than a fix, especially since I doubt V1 compatibility going to be maintained forever.  \r\n\r\n", "@Szubie Thanks for you input!\r\n\r\nUnfortunately running tf.compat.v1.disable_eager_execution() does not seem to to work any better for me, as compared to TF1.10 that I am using. Running 1 epoch of 100 update iterations takes, on average, 43 seconds with TF2, but only 20 seconds with TF1.", "I have the same issue. I just installed Tensorflow-gpu 2.0 and modified my Keras code to use the \"native\" Keras module in Tensorflow.\r\nMy model that used to train in 12 sec/epoch with TF 1.14.0 now takes 83 sec/epoch.\r\nAnd my RTX 2070 GPU is used at only 3% of its power !\r\nI also used a fit_generator for the training.\r\n\r\nI am on Windows 10 x64, CUDA 10.0\r\nCPU Core i9 9900K\r\n32 GB RAM for the CPU, 8GB RAM for the GPU\r\n1TB NVME SSD \r\n\r\nTF-GPU 2.0 as it is is just unusable, I roll back to 1.14\r\nHope that will be fixed soon.", "Can you test the 1.15 release candidate and tell us if you still see the slowdown, as we're trying to identify the root cause?", "@karmel, @robieta, this looks like a problem with plain numpy input and fit_generator, both CPU and GPU. Can you take a look?", "First of all, thank you for the wonderful repro. I can't tell you how much easier it makes all of this.\r\n\r\nIt looks like `fit_generator` is incorrectly falling back to the eager path, which is why training is slower. I will look into why, but in the mean time can you try using `model.fit`? It actually also supports generators (we plan to deprecate the fit_generator endpoint at some point as it is now obsolete), and in my testing is actually faster than the 1.14 baseline.", "@robieta I had also attempted model.fit() without any improvement to performance. It may have to do with my current implementation, so I'm pasting some pseudo code below. I am hoping there is something fundamentally wrong with it (I'm thinking its the usage of lambda):\r\n\r\nFor the pseudocode using tf.data.Dataset.from_generator():\r\n\r\n    from tensorflow.compat.v2.data import Dataset\r\n\r\n    def datagen(args):\r\n        while True:\r\n            #some code here to load and manipulate data into x and y. Mostly numpy functions\r\n            yield x,y\r\n\r\n    #some code here to create and compile model \r\n    \r\n    #I'm thinking the performance issue here is in using lambda. However, without this I get a \r\n    #\"'generator' must be callable\" error\r\n\r\n    train_data = Dataset.from_generator(generator=lambda: datagen(args), . . . )\r\n    model.fit(train_data , . . . )", "@mihaimaruseac I just tested my code on TF version 1.15.0rc2. It seems to be equally as slow as TF2.0, I hope this helps with your debugging!\r\n\r\nEDIT: TF1.15.0rc2 seems to be faster by a few seconds (35-40s per epoch) as compared to TF2.0 (40-45s per epochs).", "It sounds like @Raukk and @Szubie (and maybe @Marmotte06) are hitting the issue I described above with `fit_generator` running eagerly, while @DanMinhNguyen's issue is likely different. A simple way to check is to pass the generator function directly into `Model.fit`. It will also call `Dataset.from_generator` under the hood, but with best practice optimizations like prefetching. I would also suggest that you change your datagen to:\r\n```\r\ndef datagen(args):\r\n    while True:\r\n        #some code here to load and manipulate data into x and y. Mostly numpy functions\r\n        while True:  # Rule out that the generator itself is the bottleneck by repeating one batch.\r\n            yield x,y\r\n```\r\nIf that doesn't help you'll need to create a colab which demonstrates the difference in `Model.fit` between 1.14 and 1.15 / 2.0", "@robieta Thanks for the comment! I'll try it out later this evening or tomorrow and get back to you. If I'm still having issues, will make a colab to share and demonstrate it. Currently I was using some internal data which I cannot share, hence the pseudocode.", "@robieta So I just ran my code by passing the generator function directly into model.fit(), and that seemed to fix the issue completely!\r\n\r\nBasically my pseudo code now looks like this:\r\n\r\n    def datagen(args):\r\n        while True:\r\n            #some code here to load and manipulate data into x and y. Mostly numpy functions\r\n            yield x,y\r\n\r\n    #some code here to create and compile model \r\n\r\n    model.fit(datagen(args) , . . . )\r\n\r\nSo basically what I learned is:\r\n1) don't use model.fit_generator() anymore\r\n2) don't call Dataset.from_generator() separately\r\n3) just use model.fit() and pass the generator directly into it.\r\n\r\nThanks so much for everything!\r\n\r\n(As a side note for anyone else reading: the validation_data argument in model.fit() can also take a generator directly as input)", "Excellent! I'm planning on just aliasing `(fit / evaluate / predict)_generator` to `(fit / evaluate / predict)`, as those methods are now strictly superior.", "I have ran the Colab's I posted on version `1.15.0-rc2` and got the exact same results as on version `1.14.0` for performance.\r\n\r\nOn version `2.0.0` I get comparable performance when I use `model.fit(` instead of `model.fit_generator(` OR if I use `tf.compat.v1.disable_eager_execution()`. \r\nSwitching to `model.fit(` is the solution I will use for all my code. \r\n\r\nI vote that `.fit_generator(` become an Alias for `.fit(` because that would resolve the performance issues without breaking any existing examples (and allowing those examples to work on all versions of TF). I'm sure `.fit_generator(` will go away eventually, but since TF 2.0 just released, I'd want to keep it backwards compatible (especially for writing examples).", "Hello everyone!\r\n\r\nI have the same problem and was looking on a solution. I was also using fit_generator but on a Sequence class. The proposed change to simply use fit method is giving me errors, is there some workaround in sequence class case? I'm using tf 2.0.0\r\n\r\n`\r\nclass LSTMSequence(Sequence):\r\n\r\n    def __init__(self, x, subnet_x, y, batch_size):\r\n        self.batch_size = batch_size\r\n        self.x, self.subnet_x, self.y = x, subnet_x, y\r\n\r\n    def __len__(self):\r\n        return math.ceil(len(self.x) / self.batch_size)\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x = list()\r\n        # x\r\n        batch_x.append(np.array(pad_sequences(self.x[idx * self.batch_size:(idx + 1) *\r\n                                                                           self.batch_size],\r\n                                              padding='post', dtype='float32')))\r\n        # subnet x\r\n        for subnet_x_data in self.subnet_x:\r\n            batch_x.append(np.array(pad_sequences(subnet_x_data[idx * self.batch_size:(idx + 1) *\r\n                                                                                      self.batch_size],\r\n                                                  padding='post', dtype='int16')))\r\n        # y\r\n        batch_y = np.array(self.y[idx * self.batch_size:(idx + 1) *\r\n                                                        self.batch_size])\r\n        return batch_x, batch_y\r\n`\r\n\r\nand errors i have when use fit instead of fit_generator:\r\n\r\n`\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 111, in <module>\r\n    model.fit(sequence, epochs=NUM_EPOCHS, verbose=2, validation_data=val_sequence,\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 224, in fit\r\n    distribution_strategy=strategy)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 547, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 606, in _process_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 613, in __init__\r\n    output_shapes=nested_shape)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 540, in from_generator\r\n    output_types, tensor_shape.as_shape, output_shapes)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\nest.py\", line 471, in map_structure_up_to\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\nest.py\", line 471, in <listcomp>\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 1216, in as_shape\r\n    return TensorShape(shape)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 776, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 776, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 718, in as_dimension\r\n    return Dimension(value)\r\n  File \"C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 193, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'`", "@max1mn If you replace `return batch_x, batch_y` with `return tuple(batch_x), batch_y` your code should work. This stems from a historic decision in tf.data about how to treat lists. I will make fit robust to this, but adding that `tuple()` will immediately unblock you. Sorry for the inconvenience.", "@robieta Thank you very much, the code passes with than change. However, another problem appeared - it seemes the model is running on cpu only now. The training time is 254s per epoch, with 2.0 fit generator (and gpu) it was about 70s and with 1.14 gpu it was 20s. There are warnings, dont know whether they are related. Anyway, simply aliasing fit_generator to fit can break using gpu as in my case\r\n\r\n`\r\nTrain for 678 steps, validate for 42 steps\r\nEpoch 1/150\r\n\r\n2019-10-08 11:05:39.314124: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_6231_7688' and '__inference___backward_cudnn_lstm_with_fallback_6231_7688_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8683' both implement 'lstm_1489aaa8-07c9-4313-8db7-7c40df79c8a8' but their signatures do not match.\r\n2019-10-08 11:09:42.401028: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_10665' and '__inference_cudnn_lstm_with_fallback_10665_specialized_for_model_concatenate_lstm2_StatefulPartitionedCall_at___inference_distributed_function_12157' both implement 'lstm_6f710079-b289-4f03-b16c-816fc6d27388' but their signatures do not match.\r\n\r\n678/678 - 254s - loss: 4.6264 - val_loss: 0.6066\r\n`", "@max1mn I think that is a separate issue. Can you create a new issue with a minimal repro and cc @qlzh727? (The error you're seeing is in the part of the LSTM that tries to use CuDNN if applicable.)", "The warning message above actually means the function has been optimized for cudnn backend. I have suppress this warning in some previous change, but might not be in 2.0. You can ignore it for the moment.", "> Excellent! I'm planning on just aliasing `(fit / evaluate / predict)_generator` to `(fit / evaluate / predict)`, as those methods are now strictly superior.\r\n\r\nI've also encountered this performance issue:\r\n\r\n|      |    fit    |   fit_generator  |\r\n| --- | ----- |  ------------ |\r\n| tf1 |     25s    |       13s       |\r\n| tf2 |     4s       |       28s      |\r\n\r\nAfter using `tf.compat.v1.disable_eager_execution()`, the training time of fit_generator in tf2 reduces to 14s. It's comparable to tf1 but **still 3x slower** than fit in tf2.\r\n`model.fit(x=sequence, ...)` also completes the training in 14s but it seems to **load all data into memory** and log \"Filling up shuffle buffer (this may take a while)\" if I set `shuffle=True`.\r\nAny ideas?", "I had a commit that aliased fit_generator to fit, but I had to roll it back as it broke some use cases. I'm rolling it forward today now that those issues are resolved. Part of that fix includes better handling of sequences (make sure you use the Sequence class in `tf.keras`, not `keras-team/keras`) which will not pull all of the data into memory when shuffling.", "> > Excellent! I'm planning on just aliasing `(fit / evaluate / predict)_generator` to `(fit / evaluate / predict)`, as those methods are now strictly superior.\r\n> \r\n> I've also encountered this performance issue:\r\n> \tfit \tfit_generator\r\n> tf1 \t25s \t13s\r\n> tf2 \t4s \t28s\r\n> \r\n> After using `tf.compat.v1.disable_eager_execution()`, the training time of fit_generator in tf2 reduces to 14s. It's comparable to tf1 but **still 3x slower** than fit in tf2.\r\n> `model.fit(x=sequence, ...)` also completes the training in 14s but it seems to **load all data into memory** and log \"Filling up shuffle buffer (this may take a while)\" if I set `shuffle=True`.\r\n> Any ideas?\r\n\r\nI see the same performance. My question: is there a way to set buffer size and not load all data when calling `.fit()` on Sequence right now? @robieta Thank you!", "@ychervonyi Are you using the latest tf-nightly? https://github.com/tensorflow/tensorflow/commit/ac20030c96d37e980333b604402ef6dba48ef5e2 is the relevant change, and should be in `tf-nightly==2.1.0.dev20191109` Feel free to post a repro colab if you're seeing Sequence shuffling handled inefficiently.", "> @ychervonyi Are you using the latest tf-nightly? [ac20030](https://github.com/tensorflow/tensorflow/commit/ac20030c96d37e980333b604402ef6dba48ef5e2) is the relevant change, and should be in `tf-nightly==2.1.0.dev20191109` Feel free to post a repro colab if you're seeing Sequence shuffling handled inefficiently.\r\n\r\nI'm still using tensorflow-gpu, whose latest version is 2.0.0. When I use `model.fit(x=generator, shuffle=False, workers=8, ...)`, it seems that there is still only one worker whether I set `multiprocessing=True` or not. Could you please verify this behavior?", "@Seterplus Could you please try with tensorflow-gpu==2.1.0rc0, this has the fix [ac20030](https://github.com/tensorflow/tensorflow/commit/ac20030c96d37e980333b604402ef6dba48ef5e2)\r\nthat @robieta mentioned above.", "> First of all, thank you for the wonderful repro. I can't tell you how much easier it makes all of this.\r\n> \r\n> It looks like `fit_generator` is incorrectly falling back to the eager path, which is why training is slower. I will look into why, but in the mean time can you try using `model.fit`? It actually also supports generators (we plan to deprecate the fit_generator endpoint at some point as it is now obsolete), and in my testing is actually faster than the 1.14 baseline.\r\n\r\n\r\n\r\n> @max1mn If you replace `return batch_x, batch_y` with `return tuple(batch_x), batch_y` your code should work. This stems from a historic decision in tf.data about how to treat lists. I will make fit robust to this, but adding that `tuple()` will immediately unblock you. Sorry for the inconvenience.\r\n\r\nI have similar issue, when I tried fit instead of fit_generato, and when I tried tuple thing the error disappeared, but I got a different issue, my dataset is made of 300000 images, and it appears that now keras is trying to load all the images to memory before start training, and obviously it does not work, any walk around this problem?\r\n\r\nmy data generator is feeding a multi input model, that receives 2 images and two element numerical vector", "@Dr-Gandalf what is your exact version of TensorFlow? And can you provide a minimal repro of a case where keras is loading too much into memory?", "@robieta I am using tf 2.0.0 inside a docker from docker hub, \"tensorflow/tensorflow:latest-gpu-py3-jupyter\".\r\n\r\nthese are my imports\r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate, Dropout, GlobalAveragePooling2D\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.preprocessing.image import Iterator\r\nfrom tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom matplotlib import pyplot as plt\r\nfrom tensorflow.keras.utils import Sequence\r\nimport tensorflow as tf\r\nfrom datetime import datetime\r\nimport io\r\nfrom sklearn.metrics import roc_curve,roc_auc_score\r\n\r\n#this is the return line of my generator:\r\n\r\nreturn (X1i[0], X2i[0], X3i[0]), X1i[1]\r\n\r\n#this is the training line I am using, that it used to work fine with fit_generator:\r\n\r\nT_history = classification_model.fit(trainGenerator, steps_per_epoch=steps_per_epoch,\r\n                                              validation_data=validation_generator,\r\n                                              validation_steps=validation_steps, \r\n                                              callbacks=callbacks_list,\r\n                                              epochs=6,\r\n                                              use_multiprocessing=True,\r\n                                              workers=8,\r\n                                              max_queue_size=50)\r\n```\r\nthe console message I am getting is :\r\n\r\n`Filling up shuffle buffer (this may take a while): 10 of 5802`\r\n\r\n", "Ah, ok I see what is happening. `fit` takes a `shuffle` argument which defaults to True. (Since that is generally what is desired for arrays or Datasets.) However it doesn't really make sense for generators. In order to provide shuffling tf.keras currently drains the entire generator so it can shuffle the batches, whereas it should just drop the shuffle arg and use the elements as they are yielded. I will fix this and make sure it makes it into TF 2.1.", "thanks for your time, you are very kind :)", "@robieta we have to make sure that this behavior (shuffle is true except\nfor generators) makes it into the documentation as well.\n", "@Dr-Gandalf No, no, thank you. This is an important performance detail and I'm very happy that it's now going to make it into 2.1. Thanks for reporting.", "FYI this is fixed by https://github.com/tensorflow/tensorflow/commit/7d533c27d4a36a6da5290037d34c36b45983da02, and will be cherrypicked into TF 2.1", "I'm going to go ahead and close this since the appropriate fixes are now in tf.", "I also notice that when replacing the fit_generator for fit and even when using use_multiprocessing and workers I do not observe multi-threading.", "Hello there!\r\n\r\nI was using TF 1.14.0 along with Keras 2.3.1. But then, to use [keract](https://github.com/philipperemy/keract), I moved to TF 2.0.0. \r\nEverything works fine so far, except for the training time. Training is extremely slow compared to when I was using TF 1.14.0.\r\nHere, I found a discussion on fit() function, but how about train_on_batch (I stick with this func.).\r\n@robieta any considerations", "@farhodfm TF 2.0 has known issues with both `fit_generator` and `train_on_batch`. Can you try `tf-nightly` or `tensorflow==2.1.0rc2`?", "@robieta thanks for the reply!\r\n\r\nLet me try to upgrade to `tensorflow==2.1.0rc2`, and do checking on training time using `train_on_batch`.\r\nI will inform you asap.", "> Ah, ok I see what is happening. `fit` takes a `shuffle` argument which defaults to True. (Since that is generally what is desired for arrays or Datasets.) However it doesn't really make sense for generators. In order to provide shuffling tf.keras currently drains the entire generator so it can shuffle the batches, whereas it should just drop the shuffle arg and use the elements as they are yielded. I will fix this and make sure it makes it into TF 2.1.\r\n\r\n@robieta Is there a known workaround for this in TF 2.0? I am using it with keras-tuner, so I have limited control over the training code and I cannot easily upgrade to TF 2.1 (driver requirements collide with me not having admin rights on the machine). \r\n", "@phiwei I would be skeptical that the change would be back ported since it is a non-trivial behavior change, and putting it into a point release could cause other models in 2.0 to silently train differently. (Which is why point releases are generally reserved for absolutely critical fixes.) Although I am no longer a member of the TensorFlow team, so that is pure speculation on my part.", "I have the same issue while using TF 2.0. I stick with model.fit and model.predict. the performace is very slow and after disabling the eager execution using `tf.compat.v1.disable_eager_execution()`, it will be 3x - 4x faster. any updates on this issue?", "@shtse8 Thanks for the tips. I am using TF 2.0.0 on a virtual machine and I can confirm that **`disabling the eager execution`** does resolve the sluggishness and dispel the annoying message \"Filling up shuffle buffer ...\" Based on my experiments, the performance of TF 2.0.0 on a GCP virtual machine with Tesla T4 and 4 vCPUs is on par with the performance on google Colab where TF is of version 2.3.0 with Tesla T4 and 2 CPUs."]}, {"number": 33023, "title": "optimize gfile for sequential reading", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12 and 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current gfile only contain one buffer and most of behave is sequential reading, I think it is worthwhile to let gfile to contain double buffers to overlap reading and processing completely. I will make sure there is no performance regression even the handle is used for random access\uff08seek or other interface\uff09. The feature will be very useful especially when use tensorflow in cloud environment and IO has a very long latency.\r\n\r\n**Will this change the current api? How?**\r\nI maybe will introduce additonal optional argument to let user to tell if the handle will be used as sequential reading or random reading.\r\n\r\n**Who will benefit with this feature?**\r\nanyone who will use the gfile, and most of behave is sequential reading, and the read latency is a little long.\r\n\r\n**Any Other info.**\r\nI will prepare the change and send CR", "comments": ["Please tag me in any PRs related to this.", "> Please tag me in any PRs related to this.\r\n\r\nso you agree it is worthwhile to develop. Sure, i will tag you in the related PRs ", "hey guys, the PR has been submitted. https://github.com/tensorflow/tensorflow/pull/35670", "@ustcwlin,\r\nCan you please confirm if we can close this issue with respect to [this comment](https://github.com/tensorflow/tensorflow/pull/35670#issuecomment-597154748) from Mihai in #35670, and also that not many developers in the community are interested in this feature? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33022, "title": "No module named `tensorflow.data`", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.3 using the official tf docker image (local: macOS High Sierra v10.13.6)\r\n- TensorFlow installed from: Official docker image\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n`'import tensorflow.data'` yields `ModuleNotFoundError: No module named 'tensorflow.data'`.\r\n\r\n**Describe the expected behavior**\r\nImport the submodule as rc0 did.\r\n\r\n**Code to reproduce the issue**\r\nFor the release candidate,\r\n`docker run tensorflow/tensorflow:2.0.0rc0-gpu-py3 python -c 'import tensorflow.data'` works fine.\r\n`docker run tensorflow/tensorflow:2.0.0-gpu-py3 python -c 'import tensorflow.data'` yields `ModuleNotFoundError: No module named 'tensorflow.data'`.", "comments": ["Expected behavior. The only supported way is to `import tensorflow as tf` and access the rest of the API via `tf`'s attributes. All other ways of importing might break.\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0'\r\n>>> tf.data\r\n<module 'tensorflow_core._api.v2.data' from '/tmp/20/1/lib/python3.6/site-packages/tensorflow_core/_api/v2/data/__init__.py'>\r\n>>> import tensorflow.data\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.data'\r\n>>> \r\n```"]}, {"number": 33021, "title": "fix msvc 16.3.0 + cuda 10.1.243 build break", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.0.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): msvc 16.3.0\r\n- CUDA/cuDNN version: 10.1.243 / 7.6.4.38\r\n- GPU model and memory: NVIDIA GeForce RTX 2080 TI 11GB / driver 426.00\r\n\r\n**Any other info / logs**\r\n```\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)1> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)2> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)3> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)4> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)5> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)6> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)7> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)1> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)2> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)3> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)4> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)5> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)6> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<float> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<float> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)7> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<float> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)1> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)2> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)3> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)4> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)5> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)6> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , int, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)7> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)1> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)2> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)3> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)4> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)5> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)6> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: calling a __host__ function(\"std::operator -<double> \") from a __global__ function(\"tensorflow::ScatterNdOpKernel<    ::std::complex<double> , long long, ( ::tensorflow::scatter_nd_op::UpdateOp)2, (int)7> \") is not allowed\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -<double> \" is undefined in device code\r\n\r\n```", "comments": []}, {"number": 33020, "title": "Silent end of the estimator's train_and_evaluate method but exit code != 0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): light adaptation in the estimator paradigm of the TensorFlow 2.0 [BERT pretraining code](https://github.com/tensorflow/models/tree/master/official/nlp)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: only CPU for now\r\n\r\n**Current behavior**\r\nRunning a BERT pretraining (but I don't think it is tied to my issue) in the TensorFlow estimator paradigm (using `train_and_evaluate` method), everything sounds to be ok, one checkpoint is written and script terminate without any apparent error. However, regarding at the final exit code, we realize that it is not 0 but the very generic Windows one `-1073741819 (0xC0000005)`.\r\n\r\n**Describe the expected behavior**\r\nIf every thing really went well, the expected error code is logically zero.\r\n\r\n**Other info / logs**\r\nJust before crashing silently, in the cpp logs we have:\r\n```\r\n2019-10-01 13:13:14.853411: I .\\tensorflow/core/common_runtime/executor.h:229] ExecutorBarrier finished with bad status: Invalid argument: NewRandomAccessFile failed to Create/Open: \r\n\ufffd\u0006\r\n\ufffd\u0001\r\n\tinput_ids\u0012\ufffd\u0001\u001a\ufffd\u0001\r\n\ufffd\u0001e\ufffd\u0014\ufffd\r\n\ufffd\b\ufffd\ufffd\u0001\ufffd\b\ufffd\u0007\ufffd\u0007\ufffd:\ufffd\u0007\ufffd\ufffd\u0001\ufffd\u0007g\ufffd\u0011\ufffd\u0007\ufffd\u001e\ufffd\u0007\ufffd\u0001\ufffd\u0007gg\ufffd\u0010gf\ufffd\u0013\ufffd\b\ufffd\b\ufffd\b\ufffd'g\ufffd1\ufffd \ufffdl\ufffd\u000f\ufffd0g\ufffd\u0010\ufffd\ufffd\u0001\ufffd\b\ufffd\u0007\ufffd\u0012g\ufffdHg\ufffd\b\ufffdE\ufffd\u000f\ufffd\u000f\ufffd\u0015\ufffd\u0017\ufffd\u000f\ufffd\u000f\ufffd\u0010\ufffd\u0010\ufffd\u0018\ufffd\u000f\ufffd\u000f\ufffd\u0013\ufffd\u000f\ufffdu\ufffdx\ufffd\b\ufffd\u0013\ufffd\u0014\ufffd5\ufffd\b\ufffd\u0007\ufffd\u0010\ufffd\b\ufffd\b\ufffd\u0015\ufffd\b\ufffd\u0007\ufffd4\ufffd\b\ufffd\ufffd\u0001\ufffd\u0013\ufffd\u000fg\ufffdx\ufffd\b\ufffd\u0007\ufffd\bf\r\n\r\n```\r\nPlease find attached two log files of my failing scripts: one with only python logs and another one with cpp log level set to maximum verbosity.\r\n\r\n[logs.zip](https://github.com/tensorflow/tensorflow/files/3686288/logs.zip)\r\n", "comments": ["@LoicDagnas ,\r\nThank you for reporting the issue, Can you share a simple and standalone code to reproduce the issue reported here? ", "@oanush here is one, it not really light but most of the codes comes from [TensorFlow code](https://github.com/tensorflow/models/tree/master/official/nlp).\r\n[bert_pretraining_test.zip](https://github.com/tensorflow/tensorflow/files/3690007/bert_pretraining_test.zip)\r\n\r\n", "@oanush do you have any news ? Thank you :) ", "@LoicDagnas Can you try running it in google colab and let me know if the issue still persists. I guess the error is occuring due to the windows platform. Thanks!", "Hello @gowthamkpr,\r\nThank you for helping me!\r\n\r\nActually I have, more or less, identified the problem. It sounds to come from my use of `Dataset.interleave`. Here is a new minimal example which fails both in Windows and google Colab (not silently this time):\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf_records = [\r\n        '/content/drive/My Drive/models/bert_pretraining/input_files/bert_input1.tfrecord',\r\n        '/content/drive/My Drive/models/bert_pretraining/input_files/bert_input1.tfrecord']\r\n\r\ndataset = tf.data.TFRecordDataset(tf_records)\r\n\r\nnum_cpu_threads = 4\r\ncycle_length = min(num_cpu_threads, len(tf_records))\r\ndataset = dataset.interleave(\r\n    tf.data.TFRecordDataset,\r\n    cycle_length=cycle_length,\r\n    num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\nfor raw_record in dataset:\r\n    example = tf.train.Example()\r\n    example.ParseFromString(raw_record.numpy())\r\n```\r\nwith Stack Trace:\r\n```\r\n2019-10-15 11:57:30.322354: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at iterator_ops.cc:929 : Invalid argument: NewRandomAccessFile failed to Create/Open: \r\n,\r\n\u0017\r\n\u0007feature\u0012\f\u0012\r\n\r\nff\ufffd?\ufffd\ufffd\ufffd@\r\n\u0011\r\n\u0005label\r\n\u0006\r\n\u0004true : The filename, directory name, or volume label syntax is incorrect.\r\n; Unknown error\r\n\r\n\r\nRan 1 test in 1.568s\r\n\r\nFAILED (errors=1)\r\n\r\nError\r\nTraceback (most recent call last):\r\n  File \"C:\\dev\\ice\\helium.ice\\python\\language\\windows\\python3.6\\Lib\\unittest\\case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"C:\\dev\\ice\\helium.ice\\python\\language\\windows\\python3.6\\Lib\\unittest\\case.py\", line 605, in run\r\n    testMethod()\r\n  File \"C:\\dev\\ice\\helium.ml\\tensorflow\\sinequa-tensorflow\\tests\\data\\test_datasets.py\", line 46, in test_raw_loading_bis\r\n    for raw_record in dataset:\r\n  File \"C:\\dev\\ice\\helium.ml\\tensorflow\\virtualenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 622, in __next__\r\n    return self.next()\r\n  File \"C:\\dev\\ice\\helium.ml\\tensorflow\\virtualenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 666, in next\r\n    return self._next_internal()\r\n  File \"C:\\dev\\ice\\helium.ml\\tensorflow\\virtualenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 651, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"C:\\dev\\ice\\helium.ml\\tensorflow\\virtualenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\", line 2659, in iterator_get_next_sync\r\n    \"output_types\", output_types, \"output_shapes\", output_shapes)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xa6 in position 64: invalid start byte\r\n```\r\n\r\nPlease not that commenting the interleave part, everything is ok.", "Can I have the gist of the colab notebook that you have used to identify this issue @LoicDagnas. Thanks!", "@gowthamkpr \r\nhere is the gist https://colab.research.google.com/gist/LoicDagnas/22e522fd0355e4c337a5dae325476703/tensorflow_interleave_bug.ipynb\r\n\r\nThe data I used are the two tf records in this zip:\r\n[tfrecords.zip](https://github.com/tensorflow/tensorflow/files/3749465/tfrecords.zip)\r\n\r\n", "@LoicDagnas  I don't think the issue is with `Dataset.interleave`. I think the issue is with reading the tfrecords back using Tensorflow. Can you please go through this [tutorial](https://towardsdatascience.com/practical-coding-in-tensorflow-2-0-fafd2d3863f6) on reading back tfrecords in Tensorflow 2.0 and let me know if it helps!", "@gowthamkpr yessss, it helps much. I made a big mistake reading twice the dataset with `tf.data.TFRecordDataset`. I close this issue, however the only weird thing remaining is the silent failure.\r\n\r\nThank you."]}, {"number": 33019, "title": "Implement reference kernel and test for concatenation into TFLu - Uin\u2026", "body": "This patch adds the support for concatenation (Uint8\\Int8) along with few tests in TensorFlow Lite micro. The reference kernel integrated to validate this function is the same one used in TensorFlow Lite.\r\n\r\nThe following 22 tests have been added to validate the functionality:\r\n\r\nConcatTestTwoInputsFourDimensionalAxes0UInt8\r\nConcatTestTwoInputsFourDimensionalAxes1UInt8\r\nConcatTestTwoInputsFourDimensionalAxes2UInt8\r\nConcatTestTwoInputsFourDimensionalAxes3UInt8\r\nConcatTestTwoInputsFourDimensionalAxesNegativeUInt8\r\nConcatTestOneInputThreeDimensionalUInt8\r\nConcatTestTwoInputsThreeDimensionalAxes0UInt8\r\nConcatTestTwoInputsThreeDimensionalAxes1UInt8\r\nConcatTestTwoInputsThreeDimensionalAxes2UInt8\r\nConcatTestThreeInputsThreeDimensionalAxes2UInt8\r\nConcatTestOneInputFourDimensionalInt8\r\nConcatTestTwoInputsFourDimensionalAxes0Int8\r\nConcatTestTwoInputsFourDimensionalAxes1Int8\r\nConcatTestTwoInputsFourDimensionalAxes2Int8\r\nConcatTestTwoInputsFourDimensionalAxes3Int8\r\nConcatTestTwoInputsFourDimensionalAxesNegativeInt8\r\nConcatTestOneInputThreeDimensionalInt8\r\nConcatTestTwoInputsThreeDimensionalAxes0Int8\r\nConcatTestTwoInputsThreeDimensionalAxes1Int8\r\nConcatTestTwoInputsThreeDimensionalAxes2Int8\r\nConcatTestThreeInputsThreeDimensionalAxes2Int8\r\n", "comments": ["Could you please run clang-format on concatenation.cc and concatenation_test.cc (as described here: https://www.tensorflow.org/community/contribute/code_style#c_coding_style) in order to fix the code formatting?", "Done. Thanks for reminding to run the code formatting script", "I updated the review because I forgot to add the concatenation test in BUILD in order to build with Bazel", "@gmiodice Can you please resolve conflicts? Thanks!", "Just fixed the conflicts. Thanks ;)", "Hi, let me know if I need to tweak something else in the code. Thanks a lot for your time and help.", "Hi, is this patch ready to merge?", "Hi Gian Marco,\r\n\r\nThanks for the PR. \r\nWe already had an implementation internally and it recently landed here: https://github.com/tensorflow/tensorflow/commit/891e3dc6219bc886e5dbd22b268cd097ab2effcd\r\n\r\nI will still work on merging most of this PR, as you included many tests.", "Hi @fredrec,\r\n\r\nno problem at all and thanks for letting me know. Since you are going to extend the test for concatenation, should I drop this PR then?\r\n\r\nThanks,\r\nGian Marco", "@fredrec Any update on this PR, please. ", "Closing this since the main op has been implemented, and we'll try to adopt some of the tests in later changes."]}, {"number": 33018, "title": "UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?   'Discrepancy between trainable weights and collected trainable'", "body": "Please Check my process because I am trying to solve this issue from very long time. I tried every thing on google but it didn't help.\r\n\r\n1) build_generator() and build_discriminator() are 2 functions\r\n2)inputs = Input(shape=(MAX_SEQUENCE_LENGTH,lattice))\r\ndis_model=build_discriminator(inputs)\r\ndis_model.summary()\r\ndis_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9))\r\nn_disc_trainable = len(dis_model.trainable_weights)\r\nprint(\"Discriminator\",n_disc_trainable)\r\n\r\n3) gen_model=build_generator()\r\ngen_model.trainable = False\r\ngen_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),)\r\nn_gen_trainable = len(gen_model.trainable_weights)\r\n\r\n4)update_data = np_utils.to_categorical(data[0:10],lattice)\r\nreal_samples = Input(shape=update_data.shape[1:])\r\ngenerator_input_for_discriminator = Input(shape=(seed,))\r\ngenerated_samples_for_discriminator = gen_model(generator_input_for_discriminator)\r\ndiscriminator_output_from_generator = dis_model(generated_samples_for_discriminator)\r\ndiscriminator_output_from_real_samples = dis_model(real_samples)\r\n\r\naveraged_samples = RandomWeightedAverage()([real_samples,generated_samples_for_discriminator])\r\naveraged_samples_out = dis_model(averaged_samples)\r\n\r\n\r\npartial_gp_loss = partial(gradient_penalty_loss,\r\n                          averaged_samples=averaged_samples,\r\n                          gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\r\n# Functions need names or Keras will throw an error\r\npartial_gp_loss.__name__ = 'gradient_penalty'\r\n\r\ndiscriminator_model = Model(inputs=[real_samples,\r\n                                    generator_input_for_discriminator],\r\n                            outputs=[discriminator_output_from_real_samples,\r\n                                     discriminator_output_from_generator,\r\naveraged_samples_out])\r\n\r\ndiscriminator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\r\n                            loss=[Wasserstein_loss,\r\n                                  Wasserstein_loss,\r\npartial_gp_loss],metrics=['accuracy'])\r\ndiscriminator_model.summary()\r\n\r\n\r\n5)# PREPARATION FOR GENERATOR MODEL\r\ndis_model.trainable = False\r\ndis_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss=Wasserstein_loss,metrics=['accuracy'])\r\n\r\ngen_model.trainable = True\r\ngen_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss=Wasserstein_loss,metrics=['accuracy'])\r\n\r\n# COMPILE AND INITIALIZE GENERATOR MODEL\r\ngenerator_input = Input(shape=(seed,))\r\ngenerator_layers = gen_model(generator_input)\r\ndiscriminator_layers_for_generator = dis_model(generator_layers)\r\ngenerator_model = Model(inputs=[generator_input],\r\n                        outputs=[discriminator_layers_for_generator])\r\n\r\ngenerator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\r\nloss=Wasserstein_loss,metrics=['accuracy'])\r\ngenerator_model.summary()\r\n\r\nprint(\"BUILD DISCRIMINATOR MODEL TRAINABLE STATUS\",dis_model.trainable)\r\ndis_model.trainable = True\r\ndis_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss=Wasserstein_loss,metrics=['accuracy'])\r\n\r\npositive_y = np.ones((batch_size, 1), dtype=np.float32)\r\nnegative_y = -positive_y\r\ndummy_y = np.zeros((batch_size, 1), dtype=np.float32)\r\n\r\n6) Then a training loop  ( Train on batch - both discriminator_model and generator_model)\r\n\r\n", "comments": ["@Palak-15, Please provide the information asked in the [Template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "@gadagashwini,\r\nPlease find the detail and this is not hardware issue for your kind information. There is no need of that template.\r\n**System information**\r\n- I already provided you my code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow version (use command below): Tried all version \r\n**Describe the current behavior**- Problem in wgan, weights are different. \r\n\r\n**Describe the expected behavior**- They should be same.\r\n\r\n**Code to reproduce the issue**- I have already given\r\n**Other info / logs**- Already given\r\n", "@Palak-15, Please provide the intended code snippet to reproduce the issue. Thanks! ", "@gadagashwini ,\r\n\r\nExcuse me. I already provided it, main code. are you new on github or on tensorflow?", "@Palak-15, I saw the above code and i tried replicating the reported issue but looks like the code is not properly intended. Please provide the complete standalone code to reproduce the issue. Thanks!"]}, {"number": 33017, "title": "Fix a comment error in cost estimator", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33017) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33017) for more info**.\n\n<!-- ok -->", "@rmlarsen just a small fix,  take a look at it?"]}, {"number": 33016, "title": "Xla and memory allocation", "body": "- TensorFlow version 1.14\r\n- Python version: 2.7\r\n\r\nI am defining this kind of graph:\r\n\r\n    product_list = [(5, 6), ...] # list of pairs of integers\r\n    data = tf.compat.v1.placeholder(dtype=tf.float64,\r\n                                           name='input_tensor', shape=[1, 1000])\r\n    x_i = tf.gather(data, [i[0] for i in product_list], axis=1)\r\n    x_j = tf.gather(data, [i[1] for i in product_list], axis=1)\r\n    res = x_i * x_j\r\n\r\n    res = tf.concat([data, res], axis=-1)\r\n\r\nand using `AOT` to compile an `so`.\r\nThen calling the compiled function seems to take a lot of time.\r\nSo my question is : is there memory allocation at every call?\r\nIf yes, how can I avoid it by telling Tensorflow to pre-allocate the memory (as all the shapes are known in compile time) \r\nFor example are there any commands that result in a new memory allocation at every call? (`tf.stack`, `tf.concat` etc..)\r\n\r\n", "comments": ["@volvador, If you are using GPU, can you please try using GPU memory allocation mentioned in [this link](https://www.tensorflow.org/guide/gpu#manual_device_placement) and let us know if the issue still persists. Thanks!", "@gadagashwini I am using CPU", "@volvador, Please provide the complete code to reproduce the reported issue. Thanks!", "@volvador, Any update on the reproducible code. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33015, "title": "Slight change in README", "body": "Added slight change to make README more understandable for non-ML folks.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33015) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33015) for more info**.\n\n<!-- ok -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac @chanshah"]}]