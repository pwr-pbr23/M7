[{"number": 16903, "title": "typo fix", "body": "Copied the new description from docstring on line 37. Used the phrase \"spectogram timeslice\" rather than \"frequency window\" for consistency with the tooltip on ```--window_size_ms```.", "comments": []}, {"number": 16902, "title": "Add reduction parameter to mean_pairwise_squared_error loss", "body": "- add reduction parameter to `tf.losses.mean_pairwise_squared_error` to make it consistent with all of the other loss functions\r\n- increased clarify of the documentation for the function\r\n- use the `axis` parameter instead of `reduction_indices` for `math_ops.reduce_sum()` because `reduction_indices` is deprecated", "comments": ["Actually, would you mind redoing this on top of master? We're not taking changes to the 1.6 branch. Closing this out, but please send another PR."]}, {"number": 16901, "title": "Fix the missing Windows cuDNN reference.", "body": "", "comments": ["Should be merged to master, 1.5 and 1.6, too"]}, {"number": 16900, "title": "Restoring a model trained with tf.estimator and feeding input through feed_dict", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.5\r\n- **Python version**: \r\n3.6.2\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n- **GPU model and memory**:\r\nNVIDIA 1050 4 GB\r\n\r\nI trained a resnet with tf.estimator, the model was saved during the training process. The saved files consist of `.data`, `.index` and `.meta`. I'd like to load this model back and get predictions for new images. The data was fed to the model during training using `tf.data.Dataset`.  I have closely followed the resnet implementation given [here][1].\r\n\r\nI would like to restore the model and feed inputs to the nodes using a feed_dict. \r\n**My attempt**\r\n      #rebuild input pipeline\r\n      images, labels = input_fn(data_dir, batch_size=32, num_epochs=1)\r\n        \r\n      #rebuild graph\r\n      prediction= imagenet_model_fn(images,labels,{'batch_size':32,'data_format':'channels_first','resnet_size':18},mode = tf.estimator.ModeKeys.EVAL).predictions \r\n    \r\n      saver  = tf.train.Saver()\r\n      with tf.Session() as sess:\r\n        ckpt = tf.train.get_checkpoint_state(r'./model')\r\n        saver.restore(sess, ckpt.model_checkpoint_path)\r\n        while True:\r\n        try:\r\n            pred,im= sess.run([prediction,images])\r\n            print(pred)\r\n        except tf.errors.OutOfRangeError:\r\n          break\r\n\r\nI fed a dataset which was evaluated on the same model using `classifier.evaluate`, but the above method gives wrong predictions. The model gives same class and probability, 1.0, for all images.\r\n\r\nThe code I used for training and building the model is as below:\r\n\r\nSpecification for parsing the dataset:\r\n\r\n    def parse_record(raw_record, is_training):\r\n      keys_to_features = {\r\n          'image/encoded':\r\n              tf.FixedLenFeature((), tf.string, default_value=''),\r\n          'image/class/label':\r\n              tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\r\n      }\r\n      parsed = tf.parse_single_example(raw_record, keys_to_features)\r\n      image = tf.image.decode_image(\r\n          tf.reshape(parsed['image/encoded'], shape=[]),3)\r\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\r\n      label = tf.cast(\r\n          tf.reshape(parsed['image/class/label'], shape=[]),\r\n          dtype=tf.int32)\r\n      return image, tf.one_hot(label,2)\r\n\r\nThe following function parses the data and creates batches for training\r\n\r\n    def input_fn(is_training, data_dir, batch_size, num_epochs=1):\r\n      dataset = tf.data.Dataset.from_tensor_slices(\r\n          filenames(is_training, data_dir))\r\n      if is_training:\r\n         dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)\r\n      dataset = dataset.flat_map(tf.data.TFRecordDataset)\r\n      dataset = dataset.map(lambda value: parse_record(value, is_training),\r\n                            num_parallel_calls=5)\r\n      dataset = dataset.prefetch(batch_size)\r\n      if is_training:\r\n          dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\r\n      dataset = dataset.repeat(num_epochs)\r\n      dataset = dataset.batch(batch_size)\r\n    \r\n      iterator = dataset.make_one_shot_iterator()\r\n      images, labels = iterator.get_next()\r\n      return images, labels\r\n\r\nA classifier is created as below for training on train set and evaluation on validation set\r\n\r\n    classifier = tf.estimator.Estimator(\r\n          model_fn=model_function, model_dir=flags.model_dir, config=run_config,\r\n          params={\r\n              'resnet_size': flags.resnet_size,\r\n              'data_format': flags.data_format,\r\n              'batch_size': flags.batch_size,\r\n          })\r\n    \r\n        #Training cycle\r\n         classifier.train(\r\n             input_fn=lambda: input_function(\r\n                 training_phase=True, flags.data_dir, flags.batch_size, flags.epochs_per_eval),\r\n             hooks=[logging_hook])\r\n        # Evaluate the model \r\n        eval_results = classifier.evaluate(input_fn=lambda: input_function(\r\n            training_phase=False, flags.data_dir, flags.batch_size))\r\n\r\nThis is how I tried to load and get predictions from the model.\r\nI'd like to feed images through a `feed_dict` so that I can see the model's performance on individual images.\r\nWhat is the right way to restore a saved model and perform inference on it. I want to feed images directly without using `tf.data.Dataset`.  I had opened a [question ](https://stackoverflow.com/questions/48679622/restoring-a-model-trained-with-tf-estimator-and-feeding-input-through-feed-dict)on stackoverflow, but didn't get any response. I'm wondering if there really is a feature that can help with restoring a model trained with `tf.estimator` and feeding images through a `feed_dict`. \r\n\r\n\r\n  [1]: https://github.com/deepaksuresh/models/blob/master/official/resnet/resnet.py\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", " I had opened a [question ](https://stackoverflow.com/questions/48679622/restoring-a-model-trained-with-tf-estimator-and-feeding-input-through-feed-dict)on stackoverflow, but didn't get any response.", "I was facing many issues while restoring and saving , finally here is working Ipython notebook with data\r\nhttps://github.com/monk1337/DNNClassifier-example"]}, {"number": 16899, "title": "TF 1.5.0 Java API broken in Ubuntu 14.04: `GLIBCXX_3.4.20' not found", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nThe 1.5.0 Java API fails at runtime on Ubuntu 14.04 (after working okay for 1.3/1.4) on the hello world example. I originally noticed this behavior in CentOS 7; I'm reporting this as a bug for Ubuntu 14.04 because it is officially supported in the [docs](https://www.tensorflow.org/install/install_java#supported_platforms) and the error appears to be the same.\r\n\r\nPossibly related to #15777, in which case the solution may just be to build `libtensorflow_jni.so` on Ubuntu 14. Thanks in advance for looking into this!\r\n\r\n### Source code / logs\r\n1. (Optionally) use docker container with Java 8 and Maven: \r\n`docker pull goyalzz/ubuntu-java-8-maven-docker-image`\r\n`docker run -it goyalzz/ubuntu-java-8-maven-docker-image:latest bash`\r\n2. Follow instructions in [official Java API Maven example](https://www.tensorflow.org/install/install_java#example).\r\n3. Upon Step 3 of above (` mvn -q compile exec:java`), observe the following error:\r\n\r\n```\r\n[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:java (default-cli) on project hellotf: An exception occured while executing the Java class. /tmp/tensorflow_native_libraries-1518205710861-0/libtensorflow_jni.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /tmp/tensorflow_native_libraries-1518205710861-0/libtensorflow_jni.so) -> [Help 1]\r\n[ERROR] \r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\n```", "comments": ["@gunan @av8ramit - Are the C (and JNI) libraries for 1.5 supposed to be built in a way that is compatible with the older glibc of Ubuntu 14.04?", "@asimshankar those are not building currently with 14.04 support. We are working on migrating everything asap, but right now only the linux python builds have it.", "Thanks @av8ramit : Is it correct to say that for 1.5, the minimum requirement will be Ubuntu 16.04 (and should I update the website)? Do we plan to return to Ubuntu 14.04 for 1.6 or 1.7?", "Yes, let's update the minimum requirement if that's okay. @gunan ?", "I would appreciate it if you would reconsider for the same reasons it was reconsidered in #15777.  We are happily using the java API on CentOS 7.  This version upgrade to TF effectively means we need to rebuild these artifacts and deploy the associated (non-trivial) build chain.", "We can try looking into building this with ubuntu 14.04\r\nWe can work together on Monday to come up with a solution.", "I have the exact same problem with TF Java 1.5.0 on Red Hat Enterprise Linux 7 and CentOS 7. TF 1.4.0 Java worked fine before.\r\n\r\n```\r\n$ cat /etc/redhat-release \r\nCentOS Linux release 7.4.1708 (Core) \r\n$ uname -a \r\nLinux adam 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 20:13:58 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n$ rpm -qf /lib64/libstdc++.so.6\r\nlibstdc++-4.8.5-16.el7_4.1.x86_64\r\n$ file /lib64/libstdc++.so.6.0.19 \r\n/lib64/libstdc++.so.6.0.19: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, BuildID[sha1]=ecd3167b42e7d9035fe94a5276cf1d21dd00a462, stripped\r\n$ mvn -q compile exec:java\r\n[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:java (default-cli) on project hellotf: An exception occured while executing the Java class. /tmp/tensorflow_native_libraries-1518364937142-0/libtensorflow_jni.so: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /tmp/tensorflow_native_libraries-1518364937142-0/libtensorflow_jni.so) -> [Help 1]\r\n[ERROR] \r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\n```\r\n\r\nUnfortunately, this is the newest version of RHEL/CentOS (there is no 8 yet, not even beta).\r\n\r\nPython TF 1.5.0 (tensorflow-1.5.0-cp27-cp27mu-manylinux1_x86_64.whl) works just fine on RHEL/CentOS7, so this seems specific to TF Java.\r\n", "Thanks, @gunan! It would definitely be more convenient if we could maintain off-the-shelf compatibility with older OSs and versions of glibc instead of changing the minimum requirement.", "1.6.0-rc1 will not support 14.04, but we will try to support it in 1.6.0 official.", "Thanks for considering to support this in 1.6.0. Just as @krm1312, we would like to avoid building TF Java ourselves, which indeed needs another toolchain.\r\n\r\nIt seems sensible to me that TF Java would support the same platforms as TF Python, and TF Python 1.5 does support 14.04 (and CentOS 7) (not that I'm asking for a 1.5.1 or so, 1.6 is ok).", "That's great @av8ramit.  Thank you.", "I have the same problem with a RHEL 7.4 server and all updates installed. There is no newer version of RHEL available. Please provide binaries that can be used with this system. ", "Please see https://github.com/tensorflow/tensorflow/issues/16899#issuecomment-365441309", "(FYI 1.6.0 was released a few days ago)", "Seems to have fixed it on RHEL 7.4. Thanks again guys!"]}, {"number": 16898, "title": "Tensorflow build with MKL-DNN produces garbage results", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (running a keras example)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.6.0-rc0-0-gaaf367e, 1.6.0-rc0\r\n- **Python version**: 3.6.4 (Anaconda)\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:  see below\r\n\r\n### Describe the problem\r\nI have built Tensorflow 1.6.0-rc0 from source in Ubuntu 16.04 with MKL-DNN support following [this guide][1]. The build proceeds without any problem. Testing it with keras 2.1.3 on a simple convnet from [this example][2] \"as is\" is two times slower than with the non-MKL build.\r\n\r\nNow, tuning the MKL parameters as recommended in [the guide][1] leads to almost 2 times speedup over the non-MKL build. But produces complete nonsense in terms of accuracy (and loss):\r\n\r\n[![Erroneous results from Tensorflow MKL-DNN build][3]][3]\r\n\r\nThis comes with no errors or warnings from the console. The CPU is an i7-4790K.\r\n\r\nFor reference, results obtained from a run without tuning the MKL parameters are as expected:\r\n\r\n[![Correct results from Tensorflow][4]][4]\r\n\r\n  [1]: https://www.tensorflow.org/performance/performance_guide#tensorflow_with_intel_mkl_dnn\r\n  [2]: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.2-using-convnets-with-small-datasets.ipynb\r\n  [3]: https://i.stack.imgur.com/7EUmX.png\r\n  [4]: https://i.stack.imgur.com/8s8jn.png\r\n\r\n### Source code / logs\r\nThe MKL parameters were tuned as follows:\r\n~~~\r\nfrom keras import backend as K\r\nK.set_session(K.tf.Session(config=K.tf.ConfigProto(inter_op_parallelism_threads=1)))\r\nos.environ[\"KMP_BLOCKTIME\"] = \"0\"\r\nos.environ[\"KMP_AFFINITY\"] = \"granularity=fine,verbose,compact,1,0\"\r\n~~~\r\nand following settings were printed in the console:\r\n~~~\r\nOMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}\r\nOMP: Info #156: KMP_AFFINITY: 8 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\r\nOMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5323 thread 0 bound to OS proc set {0}\r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5334 thread 1 bound to OS proc set {1}\r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5335 thread 2 bound to OS proc set {2}\r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5336 thread 3 bound to OS proc set {3}\r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5337 thread 4 bound to OS proc set {4}\r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5338 thread 5 bound to OS proc set {5}\r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5339 thread 6 bound to OS proc set {6}\r\nOMP: Info #247: KMP_AFFINITY: pid 5271 tid 5340 thread 7 bound to OS proc set {7}\r\n~~~", "comments": ["This question can be better answered at Intel Developer Zone. Just post it in the forum.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks Ilya @ikizhvatov for reporting! Sometimes we do encounter regressions. While we work on reproducing and fixing the issue, can you please try a relatively older commit that we have verified to our best knowledge about its functionality? Please try this public TF commit (rebuild tensorflow and re-run, thank you!): https://github.com/tensorflow/tensorflow/commit/abccb5d3cb45da0d8703b526776883df2f575c87", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks Wei @wei-v-wang for looking into it. With the commit you specified ([abccb5d](https://github.com/tensorflow/tensorflow/commit/abccb5d3cb45da0d8703b526776883df2f575c87)), I still observe the problem. \r\n\r\nHowever, a very recent commit I tried ([dfbacef](https://github.com/tensorflow/tensorflow/commit/dfbacef32d1fdafe71caad4c99ce8c4b648e6397)) does not have the problem any more. The speedup achieved by tuning the MKL parameters (as described in [the guide](https://www.tensorflow.org/performance/performance_guide#tensorflow_with_intel_mkl_dnn)) comes with plausible numerical output. For reference, in my original test case I observe the following timings with this commit:\r\n- without any MKL parameter tuning: 45 s/epoch (consistent with the non-MKL build)\r\n- with MKL parameter tuning as in the guide: 26 s/epoch\r\n- with alternative MKL parameter tuning [as proposed on Stackoverflow](https://stackoverflow.com/questions/48662799/tensorflow-mkl-dnn-build-in-ubuntu-silently-produces-erroneus-results): 35 s /epoch", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ikizhvatov Sorry for the delay. Thank you very much for trying our recent commits! \r\nYes, we keep improving our code quality and also we are glad to see that the MKL parameter tuning resulted in ~2x for your workload. \r\n", "@ikizhvatov Since the bug associated with this issue is fixed, could you please help close this issue? Thank you!"]}, {"number": 16897, "title": "Improve formatting of Tensor shapes in tf.losses", "body": "Updating the documentation of Tensor shapes in `tf.losses` to match the documentation guide at \r\nhttps://www.tensorflow.org/community/documentation", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "is this going to be approved?", "Thanks for the PR! Can you please send it on top of master? Closing this out, but we'd be happy to take this change if done on the master branch."]}, {"number": 16896, "title": "dataset_ops.py batch() checks type immediately", "body": "Currently if you inadvertently pass a non integer value for batch_size you get a strangely cryptic error with a long trace:\r\n```\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 734, in batch\r\n    return BatchDataset(self, batch_size)\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1392, in __init__\r\n    batch_size, dtype=dtypes.int64, name=\"batch_size\")\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 932, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1022, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 233, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 212, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 413, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 328, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int64, got 'dataset_reader.py' of type 'str' instead.\r\n```\r\n\r\nThis PR hopes to clear that up by checking the type right at the call site.\r\n\r\nIf I made a mistake in the preferred approach to such a check I'd appreciate advice, I did some searching and I couldn't find obvious examples that can accept both integers and scalar tensors, there are always slight variations and acceptable imports depend on the code location. ", "comments": ["Thanks for the contribution. Unfortunately I can't accept it because it would break backwards compatibility: types other than `tf.Tensor` and `int` are convertible into the `tf.int64` argument for `batch_size` and this would incorrectly reject them.\r\n\r\nI would be in favor of improving the argument handling for `tf.data` APIs in general, so if you come up with another way to do this I'd be happy to review another PR."]}, {"number": 16895, "title": "Variables disappearing when freezing graph with (fused) BatchNorm", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I'm using an inception_resnet_v2 from the slim model zoo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0rc0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: 1080, 8GB\r\n- **Exact command to reproduce**: - \r\n\r\n### Describe the problem\r\nThere seem to be a bug in handling fused BatchNorm's variables when freezing a graph.\r\nBoth `moving_mean` and `moving_average` disappear from the graph after `inference_graph = extract_sub_graph(input_graph_def, output_node_names)` in `graph_util.convert_variables_to_constants` and this breaks afterwards the `optimize_for_inference` script, that searches for those two variables-turned-const.\r\n\r\nFrom inspecting the input and output GraphDef, what I think is the culprit is the definition of the FusedBatchNorm node:\r\n\r\n<details>\r\n  <summary>Node GraphDef</summary>\r\n\r\n```\r\nnode {\r\n  name: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/FusedBatchNorm\"\r\n  op: \"FusedBatchNorm\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/Conv2D\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta/read\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_1\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_2\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n          dim {\r\n            size: 149\r\n          }\r\n          dim {\r\n            size: 149\r\n          }\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"data_format\"\r\n    value {\r\n      s: \"NHWC\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"epsilon\"\r\n    value {\r\n      f: 0.001\r\n    }\r\n  }\r\n  attr {\r\n    key: \"is_training\"\r\n    value {\r\n      b: true\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n</details>\r\n\r\nThe `input`s don't mention neither the mean or average, my suspicion is that `extract_sub_graph` does not detect them as part of the graph to be kept and strips the nodes.\r\n\r\n", "comments": ["After reading issue #10857, I figured out the source of all evil seems to be the graph having been generated with `is_training=True`. Since the attribute is available in `FusedBatchNorm`, would it be possible to have `freeze_graph.py` emit a warning when freezing a FusedBatchNorm with `is_training=True`? Otherwise the script will silently work but the generated `.pb` file is broken.", "@bignamehyp Would it be possible to have a comment about the reasons for closing the issue?"]}, {"number": 16894, "title": "`control_dependencies` unexpected behavior with tensorflow>=1.3", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: from binary\r\n- **TensorFlow version (use command below)**: 1.1.0, 1.3.0, 1.5.0\r\n- **Python version**:  2.7.6\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1~14.04.1) for compiling custom ops.\r\n- **CUDA/cuDNN version**: do not matter\r\n- **GPU model and memory**: do not matter \r\n- **Exact command to reproduce**: As following\r\n\r\n### Describe the problem\r\n\r\nFor the code below: (the complete sample follows later)\r\n```python\r\nfirst_op = custom_ops_module.custom_first(features_v)\r\nwith tf.control_dependencies([first_op]):\r\n    second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])\r\n\r\nfor i in range(5):\r\n    sess.run(second_op)\r\n```\r\nAccording to [the doc of control dependencies](https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies), `second_op` will only run after `first_op` has executed.\r\nHowever, this is not true for tensorflow 1.3+ when the ops are customized cpp codes.\r\n\r\n* tensorflow 1.1\r\nThe code runs exactly what I want: a `first_op`-`second_op` loop.\r\n* tensorflow 1.3\r\n`second_op` **runs first**, followed the `first_op`-`second_op` loop.\r\n* tensorflow 1.5\r\n`second_op` runs **first and only once globally**, then `first_op` runs again and again...\r\n\r\n### Source code / logs\r\n\r\nTo reproduce, a customized op is need (C++ source and compile script attached). The Python part loads the compiled `custom_ops.so` and behaves as described above, for TensorFlow 1.1, 1.3 and 1.5, each with log provided.\r\n\r\n```python\r\n#!/usr/bin/env python2\r\n# -*- coding: utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\nimport tensorflow as tf\r\ncustom_ops_module = tf.load_op_library('./custom_ops.so')\r\n\r\ndef run():\r\n    features_v = tf.Variable(\r\n            tf.zeros(shape=[480, 256]), \r\n            name='features_v',\r\n            trainable=False,\r\n            collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n    first_op = custom_ops_module.custom_first(features_v)\r\n    with tf.control_dependencies([first_op]):\r\n        second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])\r\n\r\n    sess_config = tf.ConfigProto()\r\n    sess_config.allow_soft_placement = True \r\n\r\n    sess = tf.Session(config=sess_config)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n    print('init vars done')\r\n\r\n    print('start loop')\r\n    for i in range(5):\r\n        print('loop: ', i)\r\n        sess.run(second_op)\r\n\r\nif __name__ == '__main__':\r\n    print(\"tensorflow\", tf.__version__)\r\n    run()\r\n\r\n```\r\n\r\nThe custom ops\r\n```cpp\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\n#include <glog/logging.h>\r\n\r\n///////////////////////////////////////// NameSpace ////////////////////////////////////////////\r\n\r\nusing tensorflow::DEVICE_CPU;\r\nusing tensorflow::Tensor;\r\n\r\n///////////////////////////////////////// CustomFirstOp ////////////////////////////////////////\r\nREGISTER_OP(\"CustomFirst\")\r\n    .Input(\"features: float32\")\r\n    ;\r\nclass CustomFirstOp : public tensorflow::OpKernel {\r\n public:\r\n  explicit CustomFirstOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}\r\n\r\n  void Compute(tensorflow::OpKernelContext* context) override {\r\n      const Tensor& input_tensor = context->input(0);\r\n      LOG(INFO) << \"FirstOp enter with \" << input_tensor.DebugString(); \r\n      LOG(INFO) << \"FirstOp leave\";\r\n  }\r\n};\r\nREGISTER_KERNEL_BUILDER(Name(\"CustomFirst\").Device(DEVICE_CPU), CustomFirstOp);\r\n///////////////////////////////////////// CustomSecondOp ///////////////////////////////////////\r\nREGISTER_OP(\"CustomSecond\")\r\n    .Output(\"images: uint8\")\r\n    ;\r\nclass CustomSecondOp : public tensorflow::OpKernel {\r\n public:\r\n  explicit CustomSecondOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}\r\n\r\n  void Compute(tensorflow::OpKernelContext* context) override {\r\n      LOG(INFO) << \"SecondOp enter\";\r\n      tensorflow::Tensor* output_tensor = nullptr;\r\n      OP_REQUIRES_OK(context, context->allocate_output(0, tensorflow::TensorShape({480, 220, 220, 1}), &output_tensor));\r\n      auto output_tensor_buffer = output_tensor->shaped<tensorflow::uint8, 4>({480, 220, 220, 1});\r\n      LOG(INFO) << \"SecondOp leave with \" << output_tensor->DebugString();\r\n  }\r\n};\r\nREGISTER_KERNEL_BUILDER(Name(\"CustomSecond\").Device(DEVICE_CPU), CustomSecondOp);\r\n//////////////////////////////////////// Op End //////////////////////////////////////////////////////\r\n\r\n```\r\nCompile scripts for tensorflow 1.1 and 1.3\r\n```\r\n#!/bin/bash\r\nTF_INC=( $(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') )\r\ng++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC -I $TF_INC -lglog\r\n```\r\nCompile scripts for tensorflow 1.5\r\n```\r\n#!/bin/bash\r\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\ng++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -lglog\r\n```\r\nLog with tensorflow 1.1\r\n```\r\ntensorflow 1.1.0\r\ninit vars done\r\nstart loop\r\nloop:  0\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0210 00:04:19.464854 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:04:19.464900 31270 custom_ops.cc:23] FirstOp leave\r\nI0210 00:04:19.464946 31268 custom_ops.cc:36] SecondOp enter\r\nI0210 00:04:19.486101 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  1\r\nI0210 00:04:19.507359 31266 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:04:19.507411 31266 custom_ops.cc:23] FirstOp leave\r\nI0210 00:04:19.507489 31269 custom_ops.cc:36] SecondOp enter\r\nI0210 00:04:19.507637 31269 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  2\r\nI0210 00:04:19.547608 31269 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:04:19.547644 31269 custom_ops.cc:23] FirstOp leave\r\nI0210 00:04:19.547683 31268 custom_ops.cc:36] SecondOp enter\r\nI0210 00:04:19.547745 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  3\r\nI0210 00:04:19.579887 31267 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:04:19.579923 31267 custom_ops.cc:23] FirstOp leave\r\nI0210 00:04:19.579952 31270 custom_ops.cc:36] SecondOp enter\r\nI0210 00:04:19.580021 31270 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  4\r\nI0210 00:04:19.874367 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:04:19.874406 31270 custom_ops.cc:23] FirstOp leave\r\nI0210 00:04:19.874454 31267 custom_ops.cc:36] SecondOp enter\r\nI0210 00:04:19.874563 31267 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\n```\r\nLog with tensorflow 1.3\r\n```\r\ntensorflow 1.3.0\r\ninit vars done\r\nstart loop\r\nloop:  0\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0210 00:03:34.893167 28884 custom_ops.cc:36] SecondOp enter\r\nI0210 00:03:34.893406 28884 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nI0210 00:03:35.004695 29283 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:03:35.004729 29283 custom_ops.cc:23] FirstOp leave\r\nI0210 00:03:35.004763 29283 custom_ops.cc:36] SecondOp enter\r\nI0210 00:03:35.025548 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  1\r\nI0210 00:03:35.031692 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:03:35.031735 29281 custom_ops.cc:23] FirstOp leave\r\nI0210 00:03:35.031818 29280 custom_ops.cc:36] SecondOp enter\r\nI0210 00:03:35.051908 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  2\r\nI0210 00:03:35.056428 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:03:35.056473 29281 custom_ops.cc:23] FirstOp leave\r\nI0210 00:03:35.056516 29280 custom_ops.cc:36] SecondOp enter\r\nI0210 00:03:35.056560 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  3\r\nI0210 00:03:35.061014 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:03:35.061050 29281 custom_ops.cc:23] FirstOp leave\r\nI0210 00:03:35.061085 29283 custom_ops.cc:36] SecondOp enter\r\nI0210 00:03:35.061139 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nloop:  4\r\nI0210 00:03:35.065387 29282 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:03:35.065429 29282 custom_ops.cc:23] FirstOp leave\r\nI0210 00:03:35.065500 29281 custom_ops.cc:36] SecondOp enter\r\nI0210 00:03:35.065654 29281 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\n```\r\nLog with tensorflow 1.5\r\n```\r\ntensorflow 1.5.0\r\ninit vars done\r\nstart loop\r\nloop:  0\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0210 00:39:03.166337 27046 custom_ops.cc:36] SecondOp enter\r\nI0210 00:39:03.166424 27046 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>\r\nI0210 00:39:03.868782 27965 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:39:03.868820 27965 custom_ops.cc:23] FirstOp leave\r\nloop:  1\r\nI0210 00:39:03.879077 27964 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:39:03.879094 27964 custom_ops.cc:23] FirstOp leave\r\nloop:  2\r\nI0210 00:39:03.893448 27962 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:39:03.893474 27962 custom_ops.cc:23] FirstOp leave\r\nloop:  3\r\nI0210 00:39:03.906409 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:39:03.906440 27963 custom_ops.cc:23] FirstOp leave\r\nloop:  4\r\nI0210 00:39:03.917815 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>\r\nI0210 00:39:03.917836 27963 custom_ops.cc:23] FirstOp leave\r\n```", "comments": ["Recent versions of TensorFlow have applied more aggressive constant folding. Both of your custom op implementations are stateless (ignoring the side effect of their `LOG` messages), so they may be subject to constant folding. To inhibit constant folding for each op add `.SetIsStateful()` to the end of the `REGISTER_OP()` builder.", "Thanks\uff0cthis solved my problem."]}, {"number": 16893, "title": "Building TensorFlow v1.5.0 from sources does not detect bazel v0.10.0 correctly", "body": "It looks like build script uses strings for version comparison, i.e. tuple `(\"0\", \"10\", \"0\") < (\"0\", \"4\", \"5\")`, hence Bazel version 0.10.0 is considered less then required version 0.4.5:\r\n```\r\nuser@server:~/tensorflow-1.5.0$ bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nERROR: /home/user/tensorflow-1.5.0/WORKSPACE:15:1: Traceback (most recent call last):\r\n        File \"/home/user/tensorflow-1.5.0/WORKSPACE\", line 15\r\n                closure_repositories()\r\n        File \"/home/user/.cache/bazel/_bazel_user/999dc7eb04d16a3dd0103e4cb7e7e45c/external/io_bazel_rules_closure/closure/repositories.bzl\", line 69, in closure_repositories\r\n                _check_bazel_version(\"Closure Rules\", \"0.4.5\")\r\n        File \"/home/user/.cache/bazel/_bazel_user/999dc7eb04d16a3dd0103e4cb7e7e45c/external/io_bazel_rules_closure/closure/repositories.bzl\", line 172, in _check_bazel_version\r\n                fail((\"%s requires Bazel >=%s but was...)))\r\nClosure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)\r\n```\r\nOS Platform and Distribution: SuSE Linux 12.3\r\nTensorFlow installed from: [this source](https://github.com/tensorflow/tensorflow/archive/v1.5.0.zip)\r\nTensorFlow version: 1.5.0\r\nBazel version: 0.10.0, compiled from [this source](https://github.com/bazelbuild/bazel/releases/download/0.10.0/bazel-0.10.0-dist.zip)\r\nCUDA/cuDNN version: N/A / was not enabled\r\nGPU model and memory: N/A / was not enabled\r\nExact command to reproduce: `bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Most of the information is visible from the log, however I've updated the ticket to make it complete.", "Thanks for the feedback. Please sync to the latest build script. We fixed this issue.\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I confirm that Tensorflow from master now builds OK with Bazel v0.10.0."]}, {"number": 16892, "title": "Added detailed discussion of non-strict semantics", "body": "", "comments": ["Failures are unrelated."]}, {"number": 16891, "title": "How show detecting image.", "body": "When I start a object_detection_tutorial script via ipython, it's didn't show me any image.  My console:\r\n![image](https://user-images.githubusercontent.com/17855733/36030750-0f96f458-0db1-11e8-867c-7d545c6e3170.png)\r\nBut something is starting in process because starting some python program, but before it's  open, the program is closed.\r\n![image](https://user-images.githubusercontent.com/17855733/36030857-713182fa-0db1-11e8-9b25-c208d0979d12.png)\r\ncode is here: https://pastebin.com/eh0SWjyU", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "My issue is solved.\r\nMy mistake was that I tried to run it with command `ipython file.py` instead I had to start first `ipython` and then just start with the command `run file.py`\r\nHope it will help someone."]}, {"number": 16890, "title": "Improve shape function of NonMaxSuppression", "body": "In the docs for `tf.image.non_max_suppression`, the shapes of the args `boxes` and `scores` are `[num_boxes, 4]` and `[num_boxes]` respectively.\r\n\r\nThis fix improve the shape function of NonMaxSuppression so that `boxes_shape[0] = scores_shape[0] = num_boxes`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16889, "title": "Problems Getting TensorFlow to behave Deterministically", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes, the code I am working with is company proprietary. If you need an example I will need to try to extract something that I can share. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nMac OS 10.13.2\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\n\r\n- **Python version**: \r\n\r\n3.6.4\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n\r\nRunning on CPU.\r\n\r\n- **GPU model and memory**:\r\n\r\n- **Exact command to reproduce**:\r\n\r\nPlease see description of problem. I don't currently have an example I can give you but I am hoping that the information I am providing is enough to get at least some idea about what is going on.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI am having a major struggle trying to get TensorFlow to behave in a deterministic manner. I started out trying to compare two different versions of my source code, but the numeric differences from one execution to the next were large enough to make comparisons problematic.\r\n\r\nSo I went to trying to run exactly the same code in two different shells. And this is where things got weird.\r\n\r\nI went through the code, and everywhere where TensorFlow is imported, everywhere I did operations using the form \"with graph as default:\", and prior to each call to run I added in a call to     tf.set_random_seed(42) [and yes, I used 42 in all of the calls].\r\n\r\nI also placed breakpoints before anything in my code which was a tf call with 'random' in the name (I even searched the docs to see if there were random calls that didn't have random in the name, I found a couple but I am not using them). In all of the random calls that I am using I added in a seed=42 argument. So as far as I can tell I have covered all of the bases.\r\n\r\nSo what happens? I run the code in one shell, get a set of numbers. Run code in a second shell, get identical numbers. Good. Run the code a few more times, keep getting the same numbers. Better.\r\nRun the code again, get completely different numbers!!! If I keep running the code in the different shells sometimes I get the same numbers from execution to execution. Sometimes I get different numbers. Sometimes it goes on to completely new numbers. Sometimes it goes back to an older set of numbers.\r\n\r\nSo I am getting something that is consistent enough that it feels like I haven't missed any places where I need to set the seed, but not consistent enough to say that it is deterministic.\r\n\r\nI really want to be able to run the code over and over again and get exactly the same numeric result. Is this an unreasonable expectation? \r\n\r\nCan anyone offer an explanation as to why the code would generate exactly the same result (and I'm looking down to 8 decimal places so this is fairly precise) five times in a row, and then switch to a completely different result and then repeat that result multiple times in a row?\r\n\r\nI can certainly try to reduce this into an example I can share (the model I am using is a toy model we use for unit testing). I wanted to see if anyone had any insight into the general problem before I put the time into that.\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI will include two examples of the numeric output I am getting. This data is part of the returned result from Session.run().\r\n\r\nExample A:\r\n\r\nout_node_weights [[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\r\n   9.10322666  9.09527016  9.10811996  9.09029293]\r\n [ 9.34178257  9.34915924  9.34216785  9.34740925  9.32617569  9.34794617\r\n   9.34459496  9.35203457  9.34202194  9.34494209]\r\n [ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\r\n   9.10322666  9.09527016  9.10811996  9.09029293]\r\n [ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\r\n   9.10322666  9.09527016  9.10811996  9.09029293]\r\n [ 9.59115124  9.59841442  9.59205914  9.59892082  9.58935356  9.59801197\r\n   9.59655094  9.5970974   9.5989027   9.59136868]\r\n [ 9.39457417  9.39968395  9.39309788  9.4028101   9.40468502  9.39697838\r\n   9.39671326  9.39805126  9.39696026  9.39565849]]\r\n\r\nExample B:\r\n\r\nout_node_weights [[ 9.04592514  9.05451775  9.05065632  9.05263042  9.06681919  9.05208111\r\n   9.05834484  9.05059719  9.06315136  9.04525948]\r\n [ 9.25311852  9.26151276  9.25516319  9.25806046  9.23816681  9.26067734\r\n   9.25860214  9.26416397  9.25700569  9.25596619]\r\n [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\r\n   9.27913475  9.27859116  9.28033924  9.27534485]\r\n [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\r\n   9.27913475  9.27859116  9.28033924  9.27534485]\r\n [ 9.50198936  9.51004601  9.50419044  9.50922203  9.50067139  9.50993729\r\n   9.50946426  9.5085659   9.51255226  9.50196838]\r\n [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\r\n   9.27913475  9.27859116  9.28033924  9.27534485]]", "comments": ["Do you mind sharing the code you are running here, it is really difficult to help without that information. Thanks!", "Thanks for taking a look. I can try to code up something that replicates the behaviour. Before I do that, can you please confirm whether or not Tensorflow is meant to be able to be used in a way where the numeric results are identical over an indefinite number of executions?\r\n\r\nThe best I have been able to get is within 4-5 decimal places.\r\n\r\nI come from a computer graphics background and am used to being able to rerun very complicated renders or simulations and getting exactly the same result every time if I haven't changed any settings (that is, if I compare the resulting binary files they are identical). While the documentation seems to suggest that Tensorflow can provide this level of consistency, I haven't seen it.\r\n\r\nIf you expect that it is possible to run a graph today and get exactly the same result I got running the same graph last month, then I will try to put together an example which fails to achieve that goal. Then we can figure out if this is me failing to understand something, or a bug in Tensorflow.\r\n\r\nThanks!\r\n", "@rundembear, I believe we do not guarantee this level of determinism with our current code. I would though suggest to use newer versions of tensorflow (I see you are using v1.3), which might give better results. \r\n@ebrevdo do you have any comment on this, I see you are tracking a similar issue#14663.", "@shivaniag Thank you so much! I suspected that might be the case. I know that we are planning on moving to 1.5 or 1.6 in the near future and am happy to try my experiments again once we have done so.", "Have you tried setting your `inter_op_parallelism_threads` to 1? Some core TensorFlow operations on multi-core CPU will be non-deterministic by default, due to parallel execution being enabled by default. \r\n\r\nhttps://stackoverflow.com/questions/41233635/meaning-of-inter-op-parallelism-threads-and-intra-op-parallelism-threads", "It would be great to be able to get a more deterministic behaviour, above all while using GPU training, as this would make hyper-parameters tuning much more useful and doable.\r\nNowadays, using Keras with Tensorflow backend, it can really change quite much from one training to the next. In case that there is already a way to reduce that difference between runs, I would appreciate it if you could provide some documentation.", "> Have you tried setting your `inter_op_parallelism_threads` to 1? Some core TensorFlow operations on multi-core CPU will be non-deterministic by default, due to parallel execution being enabled by default.\r\n> \r\n> https://stackoverflow.com/questions/41233635/meaning-of-inter-op-parallelism-threads-and-intra-op-parallelism-threads\r\n\r\n@yoavz Why would inter-op make cause non-deterministic results instead of intra-op. For intra-op, I expect that you can get different order of arithmetic operations in every run because of how a tensor operation is broken up to be run on different threads resulting in different floating-point error. For inter-op, I don't understand how.", "I understand that this issue, as it started out, is related to determinism when running on a CPU, but you might be interested in the work that I've been doing on determinism in TensorFlow focused on GPUs.\r\n\r\nFor more information, see: https://github.com/NVIDIA/tensorflow-determinism\r\n\r\n@lucav76 The reproducibility on GPUs that you have been wanting should now be available.", "@rundembear We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. refer [link](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16889\">No</a>\n"]}, {"number": 16887, "title": "A problem about gcc 5.5.0. It does not compile TensorFlow.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nSlackware Linux 14.2 64 bit\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n\r\n- **Python version**: \r\n3.6.4\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.5.0\r\n\r\n- **CUDA/cuDNN version**:\r\n9.0/7\r\n\r\n- **GPU model and memory**:\r\n1050Ti / 4Gb\r\n\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n\r\n#### gcc 5.5.0 does not compile TF source code.\r\n\r\nFirst of all, sorry for that I'm NOT using Ubuntu Linux for TF. I know that Ubuntu Linux is the only supported Linux system for TF. Instead, I use an ancient distribution, i.e., Slackware Linux. Recently, I got a security update for Spectre. As a side-effect, I also got an updated gcc (5.3.0 -> 5.5.0).\r\n\r\nI usually compile the TensorFlow source code for optimization. However, it can not be compiled with updated gcc (5.5.0), like this;\r\n\r\n\r\n```\r\nINFO: From Compiling tensorflow/contrib/resampler/kernels/resampler_ops_gpu.cu.cc:\r\n/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9220): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9231): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9244): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9255): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n```\r\n\r\n\r\nSo I googled a little bit, and found the following issue:\r\n\r\n#10220 \r\n\r\nin the middle of the thread, I saw\r\n\r\n> I think the problem here is that gcc-5.5 shipped with avx512*intrin.h headers that switched to using void* and const void* (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=76731) but without switching the builtins to do the same. This is why 5.4 works but 5.5 breaks. \r\n\r\n\r\nso I tracked down the matter that I could see \r\n\r\nhttps://gcc.gnu.org/bugzilla/show_bug.cgi?id=76731\r\n\r\n> All of the scatter/gather intrinsics in avx512intrin.h use int/float/double pointers, which is incorrect. \r\n\r\nSo, at first, I thought this problem is about gcc, but someone suggested that it's maybe related to CUDA.\r\n\r\n> it's the GPU/CUDA code that doesn't support the new compiler so, if you want to build that, your only option is downgrade the compiler, until Nvidia releases a new CUDA sdk.\r\n\r\nfrom https://www.linuxquestions.org/questions/showthread.php?p=5817669#post5817669\r\n\r\nTo wrap up, I was able to compile TF successfully with gcc 5.3.0. With gcc 5.5.0, I get error messages here and there, and yet I do not know what makes this errors. I suspect the combination of gcc and CUDA (and also TensorFlow) does not work well, but I still can not figure out which of them makes this fault.\r\n\r\nThank you for your help.\r\n\r\n\r\nBest regards,\r\nsungjin.\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@gunan can you please take a look? ", "This is not an issue with GCC, it is due to AVX 512. We still do not have official AVX 512 support, but we welcome any contributions. I will close this as a duplicate, as similar issues with compiling in avx512 are reported before, just for housekeeping.\r\nDuplicate of #10220", "Yes. Maybe this is not an issue with GCC. By the way, let me just mention what I tried, for your information;\r\n\r\n1. CPU-only version of TensorFlow (r1.5) can be compiled normally with GCC 5.5. \r\n\r\n2. GPU-accelated version of TensofFlow needs older GCC (I use GCC 5.4) as nvcc, i.e., \r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\nand then I feed gcc-5.4 as nvcc (I maintain this local gcc in my home directory).\r\n\r\n\r\nThank you for your care and support."]}, {"number": 16886, "title": "Please support Cuda 9.1", "body": "I tried cloning the alpha zero chess from https://github.com/Zeta36/chess-alpha-zero and almost got everything to work.  However when I try \"python src/chess_zero/run.py self\" I get the message \r\n\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit \r\n\r\nThe problem is that I have CUDA 9.1 and cannot seem to get CUDA 9.0 on the website.  Would it be hard to make a version of TensorFlow-gpu that works with CUDA 9.1 ?\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10  \r\n \r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled from https://www.tensorflow.org/  \r\n\r\n- **TensorFlow version (use command below)**:\r\n1.5\r\n\r\n- **Python version**:\r\n 3.6.3\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.1\r\n- **GPU model and memory**:\r\nNVIDIA GeForce GTX 1050 Ti  8GB Ram\r\n\r\n- **Exact command to reproduce**:\r\npython src/chess_zero/run.py self\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\ne:\\eDownloads>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.  \r\nI am requesting an update because I have CUDA 9.1 and can't get CUDA 9.0\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["You can get a legacy release of CUDA, e.g., 9.0, from here;\r\n\r\nhttps://developer.nvidia.com/cuda-toolkit-archive", "CC @tfboyd ", "On Mon, Feb 12, 2018 at 4:43 PM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> CC @tfboyd <https://github.com/tfboyd>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16886#issuecomment-365087963>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMt8mNou1lvu8n_BofUBI2StNVjFIWTlks5tUL55gaJpZM4R_d1Q>\n> .\n>\nThank you!\n", "The current plan of record is to move to CUDA 9.2 when it comes out.  CUDA 9.1 did not have any immediate perf improvements and moving drivers again was a concern.  I am not sure if windows is any different but in linux having multiple CUDAs installed is somewhat trivial.  We discussed this plan with NVIDIA before making the decision.  NVIDIA is working on smoothing out the process and revisiting how these releases will work and land.  \r\n\r\nI am sorry that there is near term pain.  ", "Following instructions to install Cuda 9.0 on Ubuntu (apt-get) doesn't work, it will still install 9.1.", "Hi everyone, I know this issue is closed but I hope somebody would answer! I have downloaded cuda 9.0 from the same link as @daisylab suggested (it's actually cuda 9.0.176), now that I have installed it, it shows cuda 9.1 instead of 9.0! and obviously tensorflow doesn't work complaining it can't find 9.0 libs. Can someone help me out please?", "@szm2015 \r\ntry \r\n`sudo apt-get install cuda-9-0` \r\ninstead of \r\n`sudo apt-get install cuda`\r\n\r\nit works for me"]}, {"number": 16885, "title": "Implementation of the unpooling layer in tf.contrib.layers", "body": "This pull request follow the propositions from #2169 \r\nAnd try to reproduce the layer implemented in https://arxiv.org/abs/1505.04366\r\n\r\nI integrated the code as a layer in tf.contrib.layers\r\n\r\nThere is most probably additional changes that are needed.\r\nI saw that there should be a testing unit added but I don't know exactly how it should be done.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "We're promoting class based layers. Adding @fchollet to review.", "Yes, we're promoting class-based layers and we will not add such function layers. Also, if it doesn't have a state (weights), then it is better for it not to be a layer at all.\r\n\r\nIs \"unpooling\" as you define it the same as \"repeat\"? e.g.\r\n\r\n```\r\nA B\r\nC D\r\n\r\nbecomes\r\n\r\nA A B B\r\nA A B B\r\nC C D D\r\nC C D D\r\n```", "It is true that it does not have states. So is there an other place where it would fit better than in layers?\r\nBut I want to point the fact that max_pooling, average_pooling  do not have states and are present in layers.\r\n\r\nNo it is not as repeat.\r\nunpool serve to resize features map after it have been pooled by tf.nn.max_pool_with_argmax.\r\nThe max indices are then used to map the resizing. The other pixels are left at 0. Only the place where the max appeared during the max_pooling will be filled.\r\n\r\nExample taken from the paper \"Learning Deconvolution Network for Semantic Segmentation\"\r\n![screenshot from 2018-02-15 12-43-37](https://user-images.githubusercontent.com/10159876/36239559-fdd4b1e6-124d-11e8-8f55-adf9463491bc.png)\r\n\r\nAn other more numerical example:\r\n\r\n```\r\nimage = tf.placeholder(tf.float32, [None, 4, 4, 1])\r\npooled, ind = tf.nn.max_pool_with_argmax(image,[1,2,2,1],[1,2,2,1],\"SAME\")\r\nunpooled = unpool(pooled, ind, ksize=[1, 2, 2, 1])\r\n```\r\n\r\nimage ->\r\n```\r\n[[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01]\r\n [1.46755891e-01 9.23385948e-02 1.86260211e-01 3.45560727e-01]\r\n [3.96767474e-01 5.38816734e-01 4.19194514e-01 6.85219500e-01]\r\n [2.04452250e-01 8.78117436e-01 2.73875932e-02 6.70467510e-01]]\r\n```\r\n\r\npooled->\r\n```\r\n[[0.7203245  0.34556073]\r\n [0.87811744 0.6852195 ]]\r\n```\r\n\r\nunpooled->\r\n```\r\n[[0.         0.7203245  0.         0.        ]\r\n [0.         0.         0.         0.34556073]\r\n [0.         0.         0.         0.6852195 ]\r\n [0.         0.87811744 0.         0.        ]]\r\n```", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any idea which release will this one make into?  I would like to use it in an autoencoder...", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thank you for your explanation, and sorry for the delayed feedback.\r\n\r\nBased on the following:\r\n\r\n> we're promoting class-based layers and we will not add such function layers\r\n> if it doesn't have a state (weights), then it is better for it not to be a layer at all.\r\n\r\nWe will not add this layer to tf.contrib.\r\n\r\nThanks for the PR regardless.", "This may be a good fit for tf.contrib.nn?", "@fchollet \r\nThat explanation is not enough. It's contradictory.\r\nI'd like to know why max_pooling or average_pooling are present then.\r\n\r\nIf it is just about the class-based implementation it can be modified to work.", "About this solution, we need fix batch size. How can we handle with unfixed batch size for unpooling?\r\n"]}, {"number": 16884, "title": "The output_type of NodeDef should support DT_INT64", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\n\r\nNow we try to export the model to be load in Android and iOS devices. The exported operators includes `tf.argmax` which has the output_type of long(tf.DT_INT64). Once the model is used for inference it throws these error.\r\n\r\n```\r\n2018-02-09 10:45:15.690271: E /Users/tobe/code/tensorflow_template_application/ios_client/RunModelViewController.mm:194] Running model failed: Invalid argument: NodeDef mentions attr 'output_type' not in Op<name=ArgMax; signature=input:T, dimension:Tidx -> output:int64; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; \r\n```\r\n\r\nThe source code about this operator is in https://github.com/tobegit3hub/tensorflow_template_application/blob/master/dense_classifier.py#L422 .\r\n\r\nI'm not really digging into the code why it supports float and int only. It would be great if it can support more output_types so that the models don't less modification for deployment.\r\n\r\n### Source code / logs\r\n\r\nNone\r\n", "comments": ["Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "i had the same error. with tf version 1.4.0...", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 63 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 78 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 93 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 108 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the long delay. I believe this should be addressed if you try defining `__ANDROID_TYPES_FULL__` when you compile (for example with `-copt=-D__ANDROID_TYPES_FULL__` if you're using Bazel). Closing, but please reopen if this doesn't fix it."]}, {"number": 16883, "title": "Feauture Request: Multidimensional RNN", "body": "I would like to contribute a Multidimensional RNN feature in the contrib directory based on the implementation mentioned [here](https://github.com/tensorflow/tensorflow/issues/1453#issuecomment-194976468). \r\n\r\n> Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN. etc. Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results. And maybe feed the result into another RNN.\r\n\r\nBy any chance, is it related to the [main paper](https://arxiv.org/abs/0705.2011) for it?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nome if them are relevant to in my case since this is a feature request.\r\n\r\nHave I written custom code: I will be writing custom code.\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "There used to be `tf.contrib.ndlstm` (which did 2d) but you can still install this:\r\nhttps://github.com/tmbarchive/tfndlstm.\r\n", "Ohhh, I guess I can just make some improvements there instead. Closing this now.", "I wonder if it is based on this paper: http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf"]}, {"number": 16882, "title": "Compilation error when building tfcompile on Windows / Visual Studio 2017", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64bit\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: master / 926fc13f7378d14fa7980963c4fe774e5922e336\r\n- **Python version**: Python 3.6.2 :: Anaconda, Inc.\r\n- **Bazel version (if compiling from source)**: 0.10\r\n- **GCC/Compiler version (if compiling from source)**: Microsoft (R) C/C++ Optimizing Compiler Version 19.12.25835 for x64\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: `bazel build --config=opt //tensorflow/compiler/aot:tfcompile`\r\n\r\n\r\n### Describe the problem\r\nTrying to build tfcompile on Windows, using the instructions at https://github.com/rongjiecomputer/tensorflow-xla-aot-windows and https://github.com/tensorflow/tensorflow/issues/15213 results in a compilation error.\r\n\r\n### Source code / logs\r\n```\r\ntensorflow/compiler/xla/literal_util.cc(237): error C2668: 'xla::Literal::data': ambiguous call to overloaded function\r\n.\\tensorflow/compiler/xla/literal_util.h(851): note: could be 'tensorflow::gtl::MutableArraySlice<tensorflow::uint8> xla::Literal::data<tensorflow::uint8>(const xla::ShapeIndex &)'\r\n.\\tensorflow/compiler/xla/literal_util.h(845): note: or       'tensorflow::gtl::ArraySlice<tensorflow::uint8> xla::Literal::data<tensorflow::uint8>(const xla::ShapeIndex &) const'\r\ntensorflow/compiler/xla/literal_util.cc(237): note: while trying to match the argument list '()'\r\ntensorflow/compiler/xla/literal_util.cc(450): note: see reference to function template instantiation 'tensorflow::Status xla::Literal::CopySliceFromInternal<tensorflow::uint8>(const xla::Literal &,tensorflow::gtl::ArraySlice<tensorflow::int64>,tensorflow::gtl::ArraySlice<tensorflow::int64>,tensorflow::gtl::ArraySlice<tensorflow::int64>)' being compiled\r\ntensorflow/compiler/xla/literal_util.cc(237): error C2672: 'StridedCopy': no matching overloaded function found\r\nTarget //tensorflow/compiler/aot:tfcompile failed to build\r\n```\r\nSee [attached file](https://github.com/tensorflow/tensorflow/files/1709297/bazel.log) for the complete bazel log.", "comments": ["Simple test case:\r\n\r\n```cpp\r\n#include <cstdio>\r\n\r\ntemplate <class T>\r\nstruct A {\r\n  T x;\r\n};\r\n\r\ntemplate <class T>\r\nstruct B {\r\n  T x;\r\n};\r\n\r\nstruct C {\r\n  template <class T>\r\n  A<T> f() {\r\n    puts(\"A C::f() called\");\r\n    return A<T>{1};\r\n  }\r\n  template <class T>\r\n  B<T> f() const {\r\n    puts(\"B C::f const() called\");\r\n    return B<T>{1};\r\n  }\r\n  template <class T>\r\n  void test() {\r\n    auto lambda = [&]() {\r\n      acceptA(f<T>());\r\n    };\r\n    lambda();\r\n  }\r\n};\r\n\r\ntemplate <class T>\r\nT acceptA(const A<T>& a) {\r\n  return a.x;\r\n}\r\n\r\nint main() {\r\n  C c1;\r\n  c1.test<int>();\r\n}\r\n```\r\n\r\nDue to MSVC's buggy template parsing, `acceptA(f<T>())` will fail. Changing it to `acceptA(this->f<T>())` will make it work for MSVC. GCC and Clang will happily accept the extra `this->`. PR coming soon.", "@thefiddler Can you check if #16904 fixes the issue?", "I can confirm #16904 fixes this specific compilation issue.", "Nagging Assignee @sanjoy: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16881, "title": "Enable validate_args for all distributions based on a global parameter", "body": "The probability distributions in Tensorflow have a `validate_args` argument that is initially set to `False`. This PR adds a feature that initializes the value of `validate_args` to the value of a global flag `validate_args_default`, which can be set from the user program\r\n\r\nRationale: Currently, when the user sets up the model incorrectly e.g. wrong parameter values are provided for the distributions, or data values don't match the support for the distribution, the program silently fails and produces nans in the output (or worse, wrong values). The user has no way to debug this easily because `validate_args` is `False` by default, and manually adding `validate_args=True` while initializing each distribution can be tedious for the user.\r\n\r\nResolving: #16839 ", "comments": ["@jvdillon can you review this?", "Hello and thanks for your contribution. Without detailed knowledge of your specific use case, I am unfortunately not comfortable with setting validate_args from a global.  Is there any chance you can just pass this arg into every instance of your call to distributions?", "I believe the intended use-case is to have a debugging flag which can be\nflipped by the user at the top level without going into every library they\ncall which uses distributions to find which call was wrong. This makes\nsense to me.\n\nOn Mon, Feb 12, 2018 at 9:31 AM, Joshua V. Dillon <notifications@github.com>\nwrote:\n\n> Hello and thanks for your contribution. Without detailed knowledge of your\n> specific use case, I am unfortunately not comfortable with setting\n> validate_args from a global. Is there any chance you can just pass this arg\n> into every instance of your call to distributions?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16881#issuecomment-364998004>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxaw3ZVLTW0Q8abVoFg2I2vSdUEcGks5tUHV-gaJpZM4R_NJn>\n> .\n>\n\n\n\n-- \n - Alex\n", "@jvdillon, it's like @alextp said: we don't want to have to change every single call in the library to have this check. In particular, we are using [edward probablistic programming language](http://edwardlib.org/). We considered to add a global check to edward, but the developers there [suggested](https://github.com/blei-lab/edward/issues/846) that more libraries will benefit from this fix if it's done in tensorflow directly.", "Sounds good. Let me discuss this with some of the other authors of distributions and see what they think. Ill report back. (Note: Dustin--author of Edward--has joined Bayesflow so Ill ask him.)", "A scoped version of something like this was suggested in `tf.contrib.slim`: a decorator called `argscope` which allowed properly decorated callables executed within an argscope context to take default values:\r\n\r\n```\r\nwith argscope({MyClassInitializer: {'validate_args': True}}):\r\n  ... create my classes\r\n```\r\n\r\nunfortuantely this doesn't align well with good programming practice of explicit knowledge about what's happening just by reading the code.  in this case, someone may enable validate_args, and this could create a huge performance cliff in the TF graph.  but you can't tell from reading the code where it's happening.\r\n\r\nfor this reason, argscope was rejected from ever being merged into TF core.\r\n\r\nIf you want to add something like this (a \"global\" arg that can be used by any TF internal library to check if it should add expensive checks), you would want to write a design proposal and have TF core team review it formally.", "(in the meantime it's a good idea to expose the `validate_args` argument all the way up the stack).  anything else is essentially dependency injection.", "Nagging Assignee @alextp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "While we appreciate the sometimes inconvenience of the current design, we're reluctant to introduce a global mechanism for setting validate_args.  This means we can't accept this PR.\r\n\r\nIf you have one single module or IPython notebook, we recommend you follow our style guide's recommendation.\r\n- use a \"file constant\", VALIDATE_ARGS, set at the top.\r\n- use a file constant _validate_args, and the `global` operator\r\n\r\nIf you have multiple modules do one of:\r\n- create a module, called config.py, and set validate_args = True/False inside it.  Import config from other programs\r\n- create a config file, config.yml, and set validate_args inside it.  Read it into an object that you pass around."]}, {"number": 16880, "title": "Update bazel version in docker", "body": "", "comments": []}, {"number": 16879, "title": "[DO NOT MERGE] Testing: Revert \"Dropping the Microsoft from the Visual Studio\"", "body": "Reverts tensorflow/tensorflow#16848", "comments": []}, {"number": 16878, "title": "Adding the CMAKE_GENERATOR  line to all external cmake files.", "body": "", "comments": ["The failure looks related. Could you take a  look?", "I grepped the logs and there are some suspicious architecture-related warnings:\r\n\r\n```\r\n$ grep \"library machine type\" 5ea42dd3-build.log  | head\r\n    41>zlib\\install\\lib\\zlibstatic.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>gif\\install\\lib\\giflib.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>png\\install\\lib\\libpng12_static.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>jpeg\\install\\lib\\libjpeg.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>lmdb\\install\\lib\\lmdb.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>jsoncpp\\src\\jsoncpp\\src\\lib_json\\Release\\jsoncpp.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>farmhash\\install\\lib\\farmhash.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>fft2d\\\\src\\lib\\fft2d.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>highwayhash\\install\\lib\\highwayhash.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n    41>nsync\\install\\lib\\nsync.lib : warning LNK4272: library machine type 'X86' conflicts with target machine type 'x64' [T:\\src\\github\\tensorflow\\cmake_build\\proto_text.vcxproj]\r\n```\r\n\r\n...so for some reason it looks like the build is defaulting to making 32-bit builds for some of the external projects. I guess the spirit of the 90s is alive in CMake.\r\n\r\nNotably `protobuf.cmake` doesn't display these errors, as it has an explicit directive to set the build platform to `${CMAKE_GENERATOR_PLATFORM}` in`protobuf.cmake`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/926fc13f7378d14fa7980963c4fe774e5922e336/tensorflow/contrib/cmake/external/protobuf.cmake#L38-L49\r\n\r\n...so perhaps what's needed is to replicate *that* structure (effectively setting the `-A`, `-G`, and `-T` flags to match the parent) in each of the externals. Perhaps setting one but not the others leads to it using bad defaults?", "@av8ramit Are we really merging this into 1.5? Is this still current?", "@gunan feel free to reopen if you plan on using this."]}, {"number": 16876, "title": "Fix warning about keep_dims. keep_dims -> keepdims for tf.reduce_sum().", "body": "Fix warning about keep_dims. keep_dims -> keepdims for tf.reduce_sum().", "comments": ["There are some test failures. Mind taking a look?", "Have fixed the test failures."]}, {"number": 16875, "title": "Update Dockerfile to install Python 3.6", "body": "The Python 3 containers at gcr.io/tensorflow/tensorflow generated by [tensorflow/tools/docker](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) are currently built using Python 3.5.\r\n\r\nIt would be nice to update them to Python 3.6 instead, which I think would be as simple as adding `sudo apt-get install python3.6`.\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Unfortunately, it is not that simple.\r\nWe are staying with the earliest ubuntu version nvidia docker supports, which is ubuntu 16.04.\r\nOn ubuntu 16, python3.6 is not available.\r\nSince we also had problems with 3rd party python distros before, until we upgrade the docker container to a newer version of ubuntu we will stay with python 3.5.\r\n\r\nJust to check, is this breaking you by not having python3.6? I just would like to see if there is a specific usecase you had in mind?", "No, it was just \"nice to have\". Thanks for checking (and the explanation).\n\nOn Mon, Feb 12, 2018 at 2:46 PM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> Closed #16875 <https://github.com/tensorflow/tensorflow/issues/16875>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16875#event-1470537485>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABKS1i2mmHeiaUrU9Xbv9wcjH2uWkizIks5tUL9YgaJpZM4R-_J2>\n> .\n>\n", "My specific use case for wanting Python 3.6 is because our codebase extensively uses [PEP 526 Variable Annotations](https://www.python.org/dev/peps/pep-0526) and Python 3.5.2 in the current docker images chokes on the syntax.  I'd like to be able to use the official TF docker images for running continuous integration tests..", "@gunan Hi, our codebase uses [PEP 498: Formatted string literals](https://docs.python.org/3/whatsnew/3.6.html#pep-498-formatted-string-literals), which is only supported by py36 and newer.", "+1", "I think it may be time to reevaluate. Our builds have to use ubuntu 14 anyway, and nvidia does not provide such dockerfiles. We will need to split our docker images and CI setup anyway.", "@gunan I agree. This will only cause more grievance with time. With an unpleasant workaround being using virtualenv  inside a container... which defeats the point. ", "Is there any way the community can help move this along? Maybe an experimental release can be created? This feature would be very useful.", "Joining the hype train. Would love to see the TensorFlow docker containers move to Python 3.6 or 3.7. Both carry performance improvements that I'd like to get the benefits from.", "any updates here?", "Hello, any update for this? I use the nvidia-docker provided for Nvidia GPU Cloud docker image. It also has python3.5\r\n\r\nMy codes lots of f\"string {variable}\" literals, which only work on python 3.6 or more. \r\nIs it possible to update the version of python without interfering existing packages like cuda or tensorflow-gpu?\r\n\r\nThanks", "> I think it may be time to reevaluate. Our builds have to use ubuntu 14 anyway, and nvidia does not provide such dockerfiles. We will need to split our docker images and CI setup anyway.\r\n\r\n@gunan in july you played with the idea of reevaluating the choice of sticking to 3.5. Any news on that process? Did you decide to keep Ubuntu 16.04, or is there movement in that regard?", "After discussing with NVIDIA they agreed to reintroduce support for ubuntu 14.\r\nAs Debian stable and Centos 7 compatibility would be lost with newer ubuntu versions, our CI stayed with ubuntu 14.\r\nI think our dockerfiles have been updated to ubuntu 16. Not sure about the python version on them.", "For anyone encountered the `f\"string {variable}\"` literals issue: A hacky workaround, that works on earlier versions of python is to change those expressions to `\"string {variable}\".format(**locals())` ", "@gunan Can confirm that tf dockerfiles are still python 3.5.2. Now that TF 1.13 is released with support for python 3.7, maybe it is worth it to look into this again? Python 3.5 will reach end of life on 2020-09-13, which isn't that far out. I'd like to move on sooner rather than later."]}, {"number": 16874, "title": "Revert \"Fix missing . (#16460)\"", "body": "This reverts commit 5e23338fec26f0c5ad588742b8df80e5bf1a940d.", "comments": []}, {"number": 16873, "title": "Undefined symbol when compiling a custom op", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: https://github.com/mozilla/DeepSpeech/blob/master/native_client/beam_search.cc\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430 1.5.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.1/7\r\n- **GPU model and memory**: TitanX 12Gb x 2\r\n- **Exact command to reproduce**:\r\nMakefile:\r\n```\r\nCXX := g++\r\nCFLAGS := -Wall -O3 -std=c++11\r\n\r\n# KenLM flags.\r\nSRC_DIR := kenlm\r\nOBJ_DIR := kenlm\r\n\r\nSRC_FILES := $(filter-out $(wildcard $(SRC_DIR)/util/*main*) $(wildcard $(SRC_DIR)/util/*test*), $(wildcard $(SRC_DIR)/util/*.cc))\r\nSRC_FILES := $(SRC_FILES) $(filter-out $(wildcard $(SRC_DIR)/lm/*main*) $(wildcard $(SRC_DIR)/lm/*test*), $(wildcard $(SRC_DIR)/lm/*.cc))\r\nSRC_FILES := $(SRC_FILES) $(filter-out $(wildcard $(SRC_DIR)/util/double-conversion/*main*) $(wildcard $(SRC_DIR)/util/double-conversion/*test*), $(wildcard $(SRC_DIR)/util/double-conversion/*.cc))\r\n\r\nOBJ_FILES := $(patsubst $(SRC_DIR)/%.cc,$(OBJ_DIR)/%.o,$(SRC_FILES))\r\n\r\nKENLM_CFLAGS := -I$(shell pwd)/kenlm -DNDEBUG -DKENLM_MAX_ORDER=6\r\nKENLM_LFLAGS := -L$(shell pwd)/kenlm/build/lib -lkenlm -lkenlm_builder -lkenlm_filter -lkenlm_interpolate -lkenlm_util -lz -lbz2 -llzma\r\n\r\n# TensorFlow flags.\r\nTF_CFLAGS := -I$(shell pwd)/tensorflow\r\n\r\nTF_CFLAGS := $(TF_CFLAGS) $(shell python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))')\r\nTF_LFLAGS := $(shell python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))')\r\n\r\n.PHONY: clean ctc_decoder.so generate_trie\r\n\r\ndefault: clean ctc_decoder.so generate_trie\r\n\r\nclean:\r\n\trm -f $(OBJ_FILES)\r\n\trm -f ctc_decoder.so\r\n\trm -f generate_trie\r\n\r\nctc_decoder.so: $(OBJ_FILES)\r\n\t$(CXX) $(CFLAGS) $(KENLM_CFLAGS) $(TF_CFLAGS) -fPIC -shared -o $@ beam_search.cc $^ $(TF_LFLAGS)\r\n\r\n$(OBJ_DIR)/%.o: $(SRC_DIR)/%.cc\r\n\t$(CXX) $(CFLAGS) $(KENLM_CFLAGS) -fPIC -c -o $@ $<\r\n\r\ngenerate_trie:\r\n\t$(CXX) $(CFLAGS) $(KENLM_CFLAGS) generate_trie.cc -o $@ $(KENLM_LFLAGS)\r\n\r\n```\r\n\r\nPython:\r\n```\r\nimport tensorflow as tf\r\nctc_module = tf.load_op_library('/home/thomas/projects/deepspeech2/deepspeech2/ctc/ctc_decoder.so')\r\n```\r\n### Describe the problem\r\nI compiled a custom op (mozilla's ctc decoder with lm) using the make file I pasted above and it compiles without a problem but when I try to load it I get an undefined symbol error.\r\n\r\n### Source code / logs\r\n```\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> ctc_module = tf.load_op_library('/home/thomas/projects/deepspeech2/deepspeech2/ctc/ctc-decoder.so')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/thomas/projects/deepspeech2/deepspeech2/ctc/ctc-decoder.so: undefined symbol: _ZN10tensorflow15OpKernelContext10CtxFailureEPKciRKNS_6StatusE\r\n```\r\n", "comments": ["I resolved the problem. This is not a TensorFlow issue.", "I am having similar problem with my op. Can you share how you fixed your link command? Thanks.", "Same here.", "could any tell me how to solve this issue? Same error."]}, {"number": 16872, "title": "Tensorflow 1.5 tf.dynamic_partition on GPU calculates wrong gradient shapes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNone\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\npip install tensorflow-gpu==1.5.0\r\n\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\nv1.5.0-0-g37aa430d84 1.5.0\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0/7_7.0.5.15-1\r\n- **GPU model and memory**:\r\nGTX 1080 ti\r\n- **Exact command to reproduce**:\r\nSee source code below\r\n\r\n\r\n### Describe the problem\r\nUsing Tensorflow 1.5 with GPU Cuda 9.0 loss can not back propagate through tf.dynamic_partition.  This happens only for very specific partition lists and I have not figured out what exactly causes it based on the partition lists.  Attached is a partition list that does have the issue.\r\n\r\n[membership.json.txt](https://github.com/tensorflow/tensorflow/files/1708232/membership.json.txt)\r\n\r\n\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport json\r\n\r\n\r\ndef main(session):\r\n  partitions_np = np.array(json.load(open('membership.json')))\r\n  data_np = np.random.random(size=(len(partitions_np), 1))\r\n  num_partitions = int(np.max(partitions_np)) + 1\r\n  labels_np = np.zeros(shape=(num_partitions,))\r\n\r\n  data = tf.placeholder(tf.float32, shape=data_np.shape)\r\n  partitions = tf.placeholder(tf.int32, shape=partitions_np.shape)\r\n  labels = tf.placeholder(tf.float32, shape=labels_np.shape)\r\n  feed_dict = {\r\n    data: data_np,\r\n    partitions: partitions_np,\r\n    labels: labels_np\r\n  }\r\n\r\n  data = tf.layers.Dense(1)(data)\r\n  activated_par = tf.dynamic_partition(data, partitions, num_partitions)\r\n\r\n  sparse_reps = [\r\n    tf.reduce_mean(activated, 0, keepdims=True)\r\n    for activated in activated_par\r\n  ]\r\n\r\n  output = tf.reduce_sum(sparse_reps, axis=1)\r\n  loss = tf.reduce_sum(output - labels)\r\n  session.run(tf.global_variables_initializer())\r\n\r\n  momentum_train_op = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.1).minimize(loss=loss)\r\n  adam_train_op = tf.train.AdamOptimizer().minimize(loss=loss)\r\n  session.run(tf.global_variables_initializer())\r\n  print(\"Attempting to run momentum optimizer\")\r\n  try:\r\n    session.run(momentum_train_op, feed_dict=feed_dict)\r\n  except Exception as e:\r\n    print(e)\r\n\r\n  print(\"Attempting to run adam optimizer\")\r\n  try:\r\n    session.run(adam_train_op, feed_dict=feed_dict)\r\n  except Exception as e:\r\n    print(e)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  config = tf.ConfigProto(\r\n    # If you lock to CPU it will not throw exception\r\n    # device_count={'GPU': 0}\r\n  )\r\n  with tf.Session(config=config) as session:\r\n    main(session)\r\n```\r\n\r\nOutput\r\n```\r\nAttempting to run momentum optimizer\r\ndata[255].shape = [41,1] does not start with indices[255].shape = [28]\r\n\t [[Node: gradients/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/DynamicPartition_grad/DynamicPartition/_2057, gradients/DynamicPartition_grad/DynamicPartition/_2059, gradients/DynamicPartition_grad/DynamicPartition/_2061, gradients/DynamicPartition_grad/DynamicPartition/_2063, gradients/DynamicPartition_grad/DynamicPartition/_2065, gradients/DynamicPartition_grad/DynamicPartition/_2067, gradients/DynamicPartition_grad/DynamicPartition/_2069, gradients/DynamicPartition_grad/DynamicPartition/_2071, gradients/DynamicPartition_grad/DynamicPartition/_2073, gradients/DynamicPartition_grad/DynamicPartition/_2075, gradients/DynamicPartition_grad/DynamicPartition/_2077, gradients/DynamicPartition_grad/DynamicPartition/_2079, gradients/DynamicPartition_grad/DynamicPartition/_2081, gradients/DynamicPartition_grad/DynamicPartition/_2083, gradients/DynamicPartition_grad/DynamicPartition/_2085, gradients/DynamicPartition_grad/DynamicPartition/_2087, gradients/DynamicPartition_grad/DynamicPartition/_2089, gradients/DynamicPartition_grad/DynamicPartition/_2091, gradients/DynamicPartition_grad/DynamicPartition/_2093, gradients/DynamicPartition_grad/DynamicPartition/_2095, gradients/DynamicPartition_grad/DynamicPartition/_2097, gradients/DynamicPartition_grad/DynamicPartition/_2099, gradients/DynamicPartition_grad/DynamicPartition/_2101, gradients/DynamicPartition_grad/DynamicPartition/_2103, gradients/DynamicPartition_grad/DynamicPartition/_2105, gradients/DynamicPartition_grad/DynamicPartition/_2107, gradients/DynamicPartition_grad/DynamicPartition/_2109, gradients/DynamicPartition_grad/DynamicPartition/_2111, gradients/DynamicPartition_grad/DynamicPartition/_2113, gradients/DynamicPartition_grad/DynamicPartition/_2115, gradients/DynamicPartition_grad/DynamicPartition/_2117, gradients/DynamicPartition_grad/DynamicPartition/_2119, gradients/DynamicPartition_grad/DynamicPartition/_2121, gradients/DynamicPartition_grad/DynamicPartition/_2123, gradients/DynamicPartition_grad/DynamicPartition/_2125, gradients/DynamicPartition_grad/DynamicPartition/_2127, gradients/DynamicPartition_grad/DynamicPartition/_2129, gradients/DynamicPartition_grad/DynamicPartition/_2131, gradients/DynamicPartition_grad/DynamicPartition/_2133, gradients/DynamicPartition_grad/DynamicPartition/_2135, gradients/DynamicPartition_grad/DynamicPartition/_2137, gradients/DynamicPartition_grad/DynamicPartition/_2139, gradients/DynamicPartition_grad/DynamicPartition/_2141, gradients/DynamicPartition_grad/DynamicPartition/_2143, gradients/DynamicPartition_grad/DynamicPartition/_2145, gradients/DynamicPartition_grad/DynamicPartition/_2147, gradients/DynamicPartition_grad/DynamicPartition/_2149, gradients/DynamicPartition_grad/DynamicPartition/_2151, gradients/DynamicPartition_grad/DynamicPartition/_2153, gradients/DynamicPartition_grad/DynamicPartition/_2155, gradients/DynamicPartition_grad/DynamicPartition/_2157, gradients/DynamicPartition_grad/DynamicPartition/_2159, gradients/DynamicPartition_grad/DynamicPartition/_2161, gradients/DynamicPartition_grad/DynamicPartition/_2163, gradients/DynamicPartition_grad/DynamicPartition/_2165, gradients/DynamicPartition_grad/DynamicPartition/_2167, gradients/DynamicPartition_grad/DynamicPartition/_2169, gradients/DynamicPartition_grad/DynamicPartition/_2171, gradients/DynamicPartition_grad/DynamicPartition/_2173, gradients/DynamicPartition_grad/DynamicPartition/_2175, gradients/DynamicPartition_grad/DynamicPartition/_2177, gradients/DynamicPartition_grad/DynamicPartition/_2179, gradients/DynamicPartition_grad/DynamicPartition/_2181, gradients/DynamicPartition_grad/DynamicPartition/_2183, gradients/DynamicPartition_grad/DynamicPartition/_2185, gradients/DynamicPartition_grad/DynamicPartition/_2187, gradients/DynamicPartition_grad/DynamicPartition/_2189, gradients/DynamicPartition_grad/DynamicPartition/_2191, gradients/DynamicPartition_grad/DynamicPartition/_2193, gradients/DynamicPartition_grad/DynamicPartition/_2195, gradients/DynamicPartition_grad/DynamicPartition/_2197, gradients/DynamicPartition_grad/DynamicPartition/_2199, gradients/DynamicPartition_grad/DynamicPartition/_2201, gradients/DynamicPartition_grad/DynamicPartition/_2203, gradients/DynamicPartition_grad/DynamicPartition/_2205, gradients/DynamicPartition_grad/DynamicPartition/_2207, gradients/DynamicPartition_grad/DynamicPartition/_2209, gradients/DynamicPartition_grad/DynamicPartition/_2211, gradients/DynamicPartition_grad/DynamicPartition/_2213, gradients/DynamicPartition_grad/DynamicPartition/_2215, gradients/DynamicPartition_grad/DynamicPartition/_2217, gradients/DynamicPartition_grad/DynamicPartition/_2219, gradients/DynamicPartition_grad/DynamicPartition/_2221, gradients/DynamicPartition_grad/DynamicPartition/_2223, gradients/DynamicPartition_grad/DynamicPartition/_2225, gradients/DynamicPartition_grad/DynamicPartition/_2227, gradients/DynamicPartition_grad/DynamicPartition/_2229, gradients/DynamicPartition_grad/DynamicPartition/_2231, gradients/DynamicPartition_grad/DynamicPartition/_2233, gradients/DynamicPartition_grad/DynamicPartition/_2235, gradients/DynamicPartition_grad/DynamicPartition/_2237, gradients/DynamicPartition_grad/DynamicPartition/_2239, gradients/DynamicPartition_grad/DynamicPartition/_2241, gradients/DynamicPartition_grad/DynamicPartition/_2243, gradients/DynamicPartition_grad/DynamicPartition/_2245, gradients/DynamicPartition_grad/DynamicPartition/_2247, gradients/DynamicPartition_grad/DynamicPartition/_2249, gradients/DynamicPartition_grad/DynamicPartition/_2251, gradients/DynamicPartition_grad/DynamicPartition/_2253, gradients/DynamicPartition_grad/DynamicPartition/_2255, gradients/DynamicPartition_grad/DynamicPartition/_2257, gradients/DynamicPartition_grad/DynamicPartition/_2259, gradients/DynamicPartition_grad/DynamicPartition/_2261, gradients/DynamicPartition_grad/DynamicPartition/_2263, gradients/DynamicPartition_grad/DynamicPartition/_2265, gradients/DynamicPartition_grad/DynamicPartition/_2267, gradients/DynamicPartition_grad/DynamicPartition/_2269, gradients/DynamicPartition_grad/DynamicPartition/_2271, gradients/DynamicPartition_grad/DynamicPartition/_2273, gradients/DynamicPartition_grad/DynamicPartition/_2275, gradients/DynamicPartition_grad/DynamicPartition/_2277, gradients/DynamicPartition_grad/DynamicPartition/_2279, gradients/DynamicPartition_grad/DynamicPartition/_2281, gradients/DynamicPartition_grad/DynamicPartition/_2283, gradients/DynamicPartition_grad/DynamicPartition/_2285, gradients/DynamicPartition_grad/DynamicPartition/_2287, gradients/DynamicPartition_grad/DynamicPartition/_2289, gradients/DynamicPartition_grad/DynamicPartition/_2291, gradients/DynamicPartition_grad/DynamicPartition/_2293, gradients/DynamicPartition_grad/DynamicPartition/_2295, gradients/DynamicPartition_grad/DynamicPartition/_2297, gradients/DynamicPartition_grad/DynamicPartition/_2299, gradients/DynamicPartition_grad/DynamicPartition/_2301, gradients/DynamicPartition_grad/DynamicPartition/_2303, gradients/DynamicPartition_grad/DynamicPartition/_2305, gradients/DynamicPartition_grad/DynamicPartition/_2307, gradients/DynamicPartition_grad/DynamicPartition/_2309, gradients/DynamicPartition_grad/DynamicPartition/_2311, gradients/DynamicPartition_grad/DynamicPartition/_2313, gradients/DynamicPartition_grad/DynamicPartition/_2315, gradients/DynamicPartition_grad/DynamicPartition/_2317, gradients/DynamicPartition_grad/DynamicPartition/_2319, gradients/DynamicPartition_grad/DynamicPartition/_2321, gradients/DynamicPartition_grad/DynamicPartition/_2323, gradients/DynamicPartition_grad/DynamicPartition/_2325, gradients/DynamicPartition_grad/DynamicPartition/_2327, gradients/DynamicPartition_grad/DynamicPartition/_2329, gradients/DynamicPartition_grad/DynamicPartition/_2331, gradients/DynamicPartition_grad/DynamicPartition/_2333, gradients/DynamicPartition_grad/DynamicPartition/_2335, gradients/DynamicPartition_grad/DynamicPartition/_2337, gradients/DynamicPartition_grad/DynamicPartition/_2339, gradients/DynamicPartition_grad/DynamicPartition/_2341, gradients/DynamicPartition_grad/DynamicPartition/_2343, gradients/DynamicPartition_grad/DynamicPartition/_2345, gradients/DynamicPartition_grad/DynamicPartition/_2347, gradients/DynamicPartition_grad/DynamicPartition/_2349, gradients/DynamicPartition_grad/DynamicPartition/_2351, gradients/DynamicPartition_grad/DynamicPartition/_2353, gradients/DynamicPartition_grad/DynamicPartition/_2355, gradients/DynamicPartition_grad/DynamicPartition/_2357, gradients/DynamicPartition_grad/DynamicPartition/_2359, gradients/DynamicPartition_grad/DynamicPartition/_2361, gradients/DynamicPartition_grad/DynamicPartition/_2363, gradients/DynamicPartition_grad/DynamicPartition/_2365, gradients/DynamicPartition_grad/DynamicPartition/_2367, gradients/DynamicPartition_grad/DynamicPartition/_2369, gradients/DynamicPartition_grad/DynamicPartition/_2371, gradients/DynamicPartition_grad/DynamicPartition/_2373, gradients/DynamicPartition_grad/DynamicPartition/_2375, gradients/DynamicPartition_grad/DynamicPartition/_2377, gradients/DynamicPartition_grad/DynamicPartition/_2379, gradients/DynamicPartition_grad/DynamicPartition/_2381, gradients/DynamicPartition_grad/DynamicPartition/_2383, gradients/DynamicPartition_grad/DynamicPartition/_2385, gradients/DynamicPartition_grad/DynamicPartition/_2387, gradients/DynamicPartition_grad/DynamicPartition/_2389, gradients/DynamicPartition_grad/DynamicPartition/_2391, gradients/DynamicPartition_grad/DynamicPartition/_2393, gradients/DynamicPartition_grad/DynamicPartition/_2395, gradients/DynamicPartition_grad/DynamicPartition/_2397, gradients/DynamicPartition_grad/DynamicPartition/_2399, gradients/DynamicPartition_grad/DynamicPartition/_2401, gradients/DynamicPartition_grad/DynamicPartition/_2403, gradients/DynamicPartition_grad/DynamicPartition/_2405, gradients/DynamicPartition_grad/DynamicPartition/_2407, gradients/DynamicPartition_grad/DynamicPartition/_2409, gradients/DynamicPartition_grad/DynamicPartition/_2411, gradients/DynamicPartition_grad/DynamicPartition/_2413, gradients/DynamicPartition_grad/DynamicPartition/_2415, gradients/DynamicPartition_grad/DynamicPartition/_2417, gradients/DynamicPartition_grad/DynamicPartition/_2419, gradients/DynamicPartition_grad/DynamicPartition/_2421, gradients/DynamicPartition_grad/DynamicPartition/_2423, gradients/DynamicPartition_grad/DynamicPartition/_2425, gradients/DynamicPartition_grad/DynamicPartition/_2427, gradients/DynamicPartition_grad/DynamicPartition/_2429, gradients/DynamicPartition_grad/DynamicPartition/_2431, gradients/DynamicPartition_grad/DynamicPartition/_2433, gradients/DynamicPartition_grad/DynamicPartition/_2435, gradients/DynamicPartition_grad/DynamicPartition/_2437, gradients/DynamicPartition_grad/DynamicPartition/_2439, gradients/DynamicPartition_grad/DynamicPartition/_2441, gradients/DynamicPartition_grad/DynamicPartition/_2443, gradients/DynamicPartition_grad/DynamicPartition/_2445, gradients/DynamicPartition_grad/DynamicPartition/_2447, gradients/DynamicPartition_grad/DynamicPartition/_2449, gradients/DynamicPartition_grad/DynamicPartition/_2451, gradients/DynamicPartition_grad/DynamicPartition/_2453, gradients/DynamicPartition_grad/DynamicPartition/_2455, gradients/DynamicPartition_grad/DynamicPartition/_2457, gradients/DynamicPartition_grad/DynamicPartition/_2459, gradients/DynamicPartition_grad/DynamicPartition/_2461, gradients/DynamicPartition_grad/DynamicPartition/_2463, gradients/DynamicPartition_grad/DynamicPartition/_2465, gradients/DynamicPartition_grad/DynamicPartition/_2467, gradients/DynamicPartition_grad/DynamicPartition/_2469, gradients/DynamicPartition_grad/DynamicPartition/_2471, gradients/DynamicPartition_grad/DynamicPartition/_2473, gradients/DynamicPartition_grad/DynamicPartition/_2475, gradients/DynamicPartition_grad/DynamicPartition/_2477, gradients/DynamicPartition_grad/DynamicPartition/_2479, gradients/DynamicPartition_grad/DynamicPartition/_2481, gradients/DynamicPartition_grad/DynamicPartition/_2483, gradients/DynamicPartition_grad/DynamicPartition/_2485, gradients/DynamicPartition_grad/DynamicPartition/_2487, gradients/DynamicPartition_grad/DynamicPartition/_2489, gradients/DynamicPartition_grad/DynamicPartition/_2491, gradients/DynamicPartition_grad/DynamicPartition/_2493, gradients/DynamicPartition_grad/DynamicPartition/_2495, gradients/DynamicPartition_grad/DynamicPartition/_2497, gradients/DynamicPartition_grad/DynamicPartition/_2499, gradients/DynamicPartition_grad/DynamicPartition/_2501, gradients/DynamicPartition_grad/DynamicPartition/_2503, gradients/DynamicPartition_grad/DynamicPartition/_2505, gradients/DynamicPartition_grad/DynamicPartition/_2507, gradients/DynamicPartition_grad/DynamicPartition/_2509, gradients/DynamicPartition_grad/DynamicPartition/_2511, gradients/DynamicPartition_grad/DynamicPartition/_2513, gradients/DynamicPartition_grad/DynamicPartition/_2515, gradients/DynamicPartition_grad/DynamicPartition/_2517, gradients/DynamicPartition_grad/DynamicPartition/_2519, gradients/DynamicPartition_grad/DynamicPartition/_2521, gradients/DynamicPartition_grad/DynamicPartition/_2523, gradients/DynamicPartition_grad/DynamicPartition/_2525, gradients/DynamicPartition_grad/DynamicPartition/_2527, gradients/DynamicPartition_grad/DynamicPartition/_2529, gradients/DynamicPartition_grad/DynamicPartition/_2531, gradients/DynamicPartition_grad/DynamicPartition/_2533, gradients/DynamicPartition_grad/DynamicPartition/_2535, gradients/DynamicPartition_grad/DynamicPartition/_2537, gradients/DynamicPartition_grad/DynamicPartition/_2539, gradients/DynamicPartition_grad/DynamicPartition/_2541, gradients/DynamicPartition_grad/DynamicPartition/_2543, gradients/DynamicPartition_grad/DynamicPartition/_2545, gradients/DynamicPartition_grad/DynamicPartition/_2547, gradients/DynamicPartition_grad/DynamicPartition/_2549, gradients/DynamicPartition_grad/DynamicPartition/_2551, gradients/DynamicPartition_grad/DynamicPartition/_2553, gradients/DynamicPartition_grad/DynamicPartition/_2555, gradients/DynamicPartition_grad/DynamicPartition/_2557, gradients/DynamicPartition_grad/DynamicPartition/_2559, gradients/DynamicPartition_grad/DynamicPartition/_2561, gradients/DynamicPartition_grad/DynamicPartition/_2563, gradients/DynamicPartition_grad/DynamicPartition/_2565, gradients/DynamicPartition_grad/DynamicPartition/_2567, gradients/Mean_grad/truediv, gradients/Mean_1_grad/truediv, gradients/Mean_2_grad/truediv, gradients/Mean_3_grad/truediv, gradients/Mean_4_grad/truediv, gradients/Mean_5_grad/truediv, gradients/Mean_6_grad/truediv, gradients/Mean_7_grad/truediv, gradients/Mean_8_grad/truediv, gradients/Mean_9_grad/truediv, gradients/Mean_10_grad/truediv, gradients/Mean_11_grad/truediv, gradients/Mean_12_grad/truediv, gradients/Mean_13_grad/truediv, gradients/Mean_14_grad/truediv, gradients/Mean_15_grad/truediv, gradients/Mean_16_grad/truediv, gradients/Mean_17_grad/truediv, gradients/Mean_18_grad/truediv, gradients/Mean_19_grad/truediv, gradients/Mean_20_grad/truediv, gradients/Mean_21_grad/truediv, gradients/Mean_22_grad/truediv, gradients/Mean_23_grad/truediv, gradients/Mean_24_grad/truediv, gradients/Mean_25_grad/truediv, gradients/Mean_26_grad/truediv, gradients/Mean_27_grad/truediv, gradients/Mean_28_grad/truediv, gradients/Mean_29_grad/truediv, gradients/Mean_30_grad/truediv, gradients/Mean_31_grad/truediv, gradients/Mean_32_grad/truediv, gradients/Mean_33_grad/truediv, gradients/Mean_34_grad/truediv, gradients/Mean_35_grad/truediv, gradients/Mean_36_grad/truediv, gradients/Mean_37_grad/truediv, gradients/Mean_38_grad/truediv, gradients/Mean_39_grad/truediv, gradients/Mean_40_grad/truediv, gradients/Mean_41_grad/truediv, gradients/Mean_42_grad/truediv, gradients/Mean_43_grad/truediv, gradients/Mean_44_grad/truediv, gradients/Mean_45_grad/truediv, gradients/Mean_46_grad/truediv, gradients/Mean_47_grad/truediv, gradients/Mean_48_grad/truediv, gradients/Mean_49_grad/truediv, gradients/Mean_50_grad/truediv, gradients/Mean_51_grad/truediv, gradients/Mean_52_grad/truediv, gradients/Mean_53_grad/truediv, gradients/Mean_54_grad/truediv, gradients/Mean_55_grad/truediv, gradients/Mean_56_grad/truediv, gradients/Mean_57_grad/truediv, gradients/Mean_58_grad/truediv, gradients/Mean_59_grad/truediv, gradients/Mean_60_grad/truediv, gradients/Mean_61_grad/truediv, gradients/Mean_62_grad/truediv, gradients/Mean_63_grad/truediv, gradients/Mean_64_grad/truediv, gradients/Mean_65_grad/truediv, gradients/Mean_66_grad/truediv, gradients/Mean_67_grad/truediv, gradients/Mean_68_grad/truediv, gradients/Mean_69_grad/truediv, gradients/Mean_70_grad/truediv, gradients/Mean_71_grad/truediv, gradients/Mean_72_grad/truediv, gradients/Mean_73_grad/truediv, gradients/Mean_74_grad/truediv, gradients/Mean_75_grad/truediv, gradients/Mean_76_grad/truediv, gradients/Mean_77_grad/truediv, gradients/Mean_78_grad/truediv, gradients/Mean_79_grad/truediv, gradients/Mean_80_grad/truediv, gradients/Mean_81_grad/truediv, gradients/Mean_82_grad/truediv, gradients/Mean_83_grad/truediv, gradients/Mean_84_grad/truediv, gradients/Mean_85_grad/truediv, gradients/Mean_86_grad/truediv, gradients/Mean_87_grad/truediv, gradients/Mean_88_grad/truediv, gradients/Mean_89_grad/truediv, gradients/Mean_90_grad/truediv, gradients/Mean_91_grad/truediv, gradients/Mean_92_grad/truediv, gradients/Mean_93_grad/truediv, gradients/Mean_94_grad/truediv, gradients/Mean_95_grad/truediv, gradients/Mean_96_grad/truediv, gradients/Mean_97_grad/truediv, gradients/Mean_98_grad/truediv, gradients/Mean_99_grad/truediv, gradients/Mean_100_grad/truediv, gradients/Mean_101_grad/truediv, gradients/Mean_102_grad/truediv, gradients/Mean_103_grad/truediv, gradients/Mean_104_grad/truediv, gradients/Mean_105_grad/truediv, gradients/Mean_106_grad/truediv, gradients/Mean_107_grad/truediv, gradients/Mean_108_grad/truediv, gradients/Mean_109_grad/truediv, gradients/Mean_110_grad/truediv, gradients/Mean_111_grad/truediv, gradients/Mean_112_grad/truediv, gradients/Mean_113_grad/truediv, gradients/Mean_114_grad/truediv, gradients/Mean_115_grad/truediv, gradients/Mean_116_grad/truediv, gradients/Mean_117_grad/truediv, gradients/Mean_118_grad/truediv, gradients/Mean_119_grad/truediv, gradients/Mean_120_grad/truediv, gradients/Mean_121_grad/truediv, gradients/Mean_122_grad/truediv, gradients/Mean_123_grad/truediv, gradients/Mean_124_grad/truediv, gradients/Mean_125_grad/truediv, gradients/Mean_126_grad/truediv, gradients/Mean_127_grad/truediv, gradients/Mean_128_grad/truediv, gradients/Mean_129_grad/truediv, gradients/Mean_130_grad/truediv, gradients/Mean_131_grad/truediv, gradients/Mean_132_grad/truediv, gradients/Mean_133_grad/truediv, gradients/Mean_134_grad/truediv, gradients/Mean_135_grad/truediv, gradients/Mean_136_grad/truediv, gradients/Mean_137_grad/truediv, gradients/Mean_138_grad/truediv, gradients/Mean_139_grad/truediv, gradients/Mean_140_grad/truediv, gradients/Mean_141_grad/truediv, gradients/Mean_142_grad/truediv, gradients/Mean_143_grad/truediv, gradients/Mean_144_grad/truediv, gradients/Mean_145_grad/truediv, gradients/Mean_146_grad/truediv, gradients/Mean_147_grad/truediv, gradients/Mean_148_grad/truediv, gradients/Mean_149_grad/truediv, gradients/Mean_150_grad/truediv, gradients/Mean_151_grad/truediv, gradients/Mean_152_grad/truediv, gradients/Mean_153_grad/truediv, gradients/Mean_154_grad/truediv, gradients/Mean_155_grad/truediv, gradients/Mean_156_grad/truediv, gradients/Mean_157_grad/truediv, gradients/Mean_158_grad/truediv, gradients/Mean_159_grad/truediv, gradients/Mean_160_grad/truediv, gradients/Mean_161_grad/truediv, gradients/Mean_162_grad/truediv, gradients/Mean_163_grad/truediv, gradients/Mean_164_grad/truediv, gradients/Mean_165_grad/truediv, gradients/Mean_166_grad/truediv, gradients/Mean_167_grad/truediv, gradients/Mean_168_grad/truediv, gradients/Mean_169_grad/truediv, gradients/Mean_170_grad/truediv, gradients/Mean_171_grad/truediv, gradients/Mean_172_grad/truediv, gradients/Mean_173_grad/truediv, gradients/Mean_174_grad/truediv, gradients/Mean_175_grad/truediv, gradients/Mean_176_grad/truediv, gradients/Mean_177_grad/truediv, gradients/Mean_178_grad/truediv, gradients/Mean_179_grad/truediv, gradients/Mean_180_grad/truediv, gradients/Mean_181_grad/truediv, gradients/Mean_182_grad/truediv, gradients/Mean_183_grad/truediv, gradients/Mean_184_grad/truediv, gradients/Mean_185_grad/truediv, gradients/Mean_186_grad/truediv, gradients/Mean_187_grad/truediv, gradients/Mean_188_grad/truediv, gradients/Mean_189_grad/truediv, gradients/Mean_190_grad/truediv, gradients/Mean_191_grad/truediv, gradients/Mean_192_grad/truediv, gradients/Mean_193_grad/truediv, gradients/Mean_194_grad/truediv, gradients/Mean_195_grad/truediv, gradients/Mean_196_grad/truediv, gradients/Mean_197_grad/truediv, gradients/Mean_198_grad/truediv, gradients/Mean_199_grad/truediv, gradients/Mean_200_grad/truediv, gradients/Mean_201_grad/truediv, gradients/Mean_202_grad/truediv, gradients/Mean_203_grad/truediv, gradients/Mean_204_grad/truediv, gradients/Mean_205_grad/truediv, gradients/Mean_206_grad/truediv, gradients/Mean_207_grad/truediv, gradients/Mean_208_grad/truediv, gradients/Mean_209_grad/truediv, gradients/Mean_210_grad/truediv, gradients/Mean_211_grad/truediv, gradients/Mean_212_grad/truediv, gradients/Mean_213_grad/truediv, gradients/Mean_214_grad/truediv, gradients/Mean_215_grad/truediv, gradients/Mean_216_grad/truediv, gradients/Mean_217_grad/truediv, gradients/Mean_218_grad/truediv, gradients/Mean_219_grad/truediv, gradients/Mean_220_grad/truediv, gradients/Mean_221_grad/truediv, gradients/Mean_222_grad/truediv, gradients/Mean_223_grad/truediv, gradients/Mean_224_grad/truediv, gradients/Mean_225_grad/truediv, gradients/Mean_226_grad/truediv, gradients/Mean_227_grad/truediv, gradients/Mean_228_grad/truediv, gradients/Mean_229_grad/truediv, gradients/Mean_230_grad/truediv, gradients/Mean_231_grad/truediv, gradients/Mean_232_grad/truediv, gradients/Mean_233_grad/truediv, gradients/Mean_234_grad/truediv, gradients/Mean_235_grad/truediv, gradients/Mean_236_grad/truediv, gradients/Mean_237_grad/truediv, gradients/Mean_238_grad/truediv, gradients/Mean_239_grad/truediv, gradients/Mean_240_grad/truediv, gradients/Mean_241_grad/truediv, gradients/Mean_242_grad/truediv, gradients/Mean_243_grad/truediv, gradients/Mean_244_grad/truediv, gradients/Mean_245_grad/truediv, gradients/Mean_246_grad/truediv, gradients/Mean_247_grad/truediv, gradients/Mean_248_grad/truediv, gradients/Mean_249_grad/truediv, gradients/Mean_250_grad/truediv, gradients/Mean_251_grad/truediv, gradients/Mean_252_grad/truediv, gradients/Mean_253_grad/truediv, gradients/Mean_254_grad/truediv, gradients/Mean_255_grad/truediv)]]\r\n\r\nCaused by op 'gradients/DynamicPartition_grad/DynamicStitch', defined at:\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 55, in <module>\r\n    main(session)\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 33, in main\r\n    momentum_train_op = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.1).minimize(loss=loss)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 355, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_grad.py\", line 42, in _DynamicPartitionGrads\r\n    reconstructed = data_flow_ops.dynamic_stitch(partitioned_indices, grads)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 683, in dynamic_stitch\r\n    \"DynamicStitch\", indices=indices, data=data, name=name)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'DynamicPartition', defined at:\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 55, in <module>\r\n    main(session)\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 22, in main\r\n    activated_par = tf.dynamic_partition(data, partitions, num_partitions)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 573, in dynamic_partition\r\n    num_partitions=num_partitions, name=name)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): data[255].shape = [41,1] does not start with indices[255].shape = [28]\r\n\t [[Node: gradients/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/DynamicPartition_grad/DynamicPartition/_2057, gradients/DynamicPartition_grad/DynamicPartition/_2059, gradients/DynamicPartition_grad/DynamicPartition/_2061, gradients/DynamicPartition_grad/DynamicPartition/_2063, gradients/DynamicPartition_grad/DynamicPartition/_2065, gradients/DynamicPartition_grad/DynamicPartition/_2067, gradients/DynamicPartition_grad/DynamicPartition/_2069, gradients/DynamicPartition_grad/DynamicPartition/_2071, gradients/DynamicPartition_grad/DynamicPartition/_2073, gradients/DynamicPartition_grad/DynamicPartition/_2075, gradients/DynamicPartition_grad/DynamicPartition/_2077, gradients/DynamicPartition_grad/DynamicPartition/_2079, gradients/DynamicPartition_grad/DynamicPartition/_2081, gradients/DynamicPartition_grad/DynamicPartition/_2083, gradients/DynamicPartition_grad/DynamicPartition/_2085, gradients/DynamicPartition_grad/DynamicPartition/_2087, gradients/DynamicPartition_grad/DynamicPartition/_2089, gradients/DynamicPartition_grad/DynamicPartition/_2091, gradients/DynamicPartition_grad/DynamicPartition/_2093, gradients/DynamicPartition_grad/DynamicPartition/_2095, gradients/DynamicPartition_grad/DynamicPartition/_2097, gradients/DynamicPartition_grad/DynamicPartition/_2099, gradients/DynamicPartition_grad/DynamicPartition/_2101, gradients/DynamicPartition_grad/DynamicPartition/_2103, gradients/DynamicPartition_grad/DynamicPartition/_2105, gradients/DynamicPartition_grad/DynamicPartition/_2107, gradients/DynamicPartition_grad/DynamicPartition/_2109, gradients/DynamicPartition_grad/DynamicPartition/_2111, gradients/DynamicPartition_grad/DynamicPartition/_2113, gradients/DynamicPartition_grad/DynamicPartition/_2115, gradients/DynamicPartition_grad/DynamicPartition/_2117, gradients/DynamicPartition_grad/DynamicPartition/_2119, gradients/DynamicPartition_grad/DynamicPartition/_2121, gradients/DynamicPartition_grad/DynamicPartition/_2123, gradients/DynamicPartition_grad/DynamicPartition/_2125, gradients/DynamicPartition_grad/DynamicPartition/_2127, gradients/DynamicPartition_grad/DynamicPartition/_2129, gradients/DynamicPartition_grad/DynamicPartition/_2131, gradients/DynamicPartition_grad/DynamicPartition/_2133, gradients/DynamicPartition_grad/DynamicPartition/_2135, gradients/DynamicPartition_grad/DynamicPartition/_2137, gradients/DynamicPartition_grad/DynamicPartition/_2139, gradients/DynamicPartition_grad/DynamicPartition/_2141, gradients/DynamicPartition_grad/DynamicPartition/_2143, gradients/DynamicPartition_grad/DynamicPartition/_2145, gradients/DynamicPartition_grad/DynamicPartition/_2147, gradients/DynamicPartition_grad/DynamicPartition/_2149, gradients/DynamicPartition_grad/DynamicPartition/_2151, gradients/DynamicPartition_grad/DynamicPartition/_2153, gradients/DynamicPartition_grad/DynamicPartition/_2155, gradients/DynamicPartition_grad/DynamicPartition/_2157, gradients/DynamicPartition_grad/DynamicPartition/_2159, gradients/DynamicPartition_grad/DynamicPartition/_2161, gradients/DynamicPartition_grad/DynamicPartition/_2163, gradients/DynamicPartition_grad/DynamicPartition/_2165, gradients/DynamicPartition_grad/DynamicPartition/_2167, gradients/DynamicPartition_grad/DynamicPartition/_2169, gradients/DynamicPartition_grad/DynamicPartition/_2171, gradients/DynamicPartition_grad/DynamicPartition/_2173, gradients/DynamicPartition_grad/DynamicPartition/_2175, gradients/DynamicPartition_grad/DynamicPartition/_2177, gradients/DynamicPartition_grad/DynamicPartition/_2179, gradients/DynamicPartition_grad/DynamicPartition/_2181, gradients/DynamicPartition_grad/DynamicPartition/_2183, gradients/DynamicPartition_grad/DynamicPartition/_2185, gradients/DynamicPartition_grad/DynamicPartition/_2187, gradients/DynamicPartition_grad/DynamicPartition/_2189, gradients/DynamicPartition_grad/DynamicPartition/_2191, gradients/DynamicPartition_grad/DynamicPartition/_2193, gradients/DynamicPartition_grad/DynamicPartition/_2195, gradients/DynamicPartition_grad/DynamicPartition/_2197, gradients/DynamicPartition_grad/DynamicPartition/_2199, gradients/DynamicPartition_grad/DynamicPartition/_2201, gradients/DynamicPartition_grad/DynamicPartition/_2203, gradients/DynamicPartition_grad/DynamicPartition/_2205, gradients/DynamicPartition_grad/DynamicPartition/_2207, gradients/DynamicPartition_grad/DynamicPartition/_2209, gradients/DynamicPartition_grad/DynamicPartition/_2211, gradients/DynamicPartition_grad/DynamicPartition/_2213, gradients/DynamicPartition_grad/DynamicPartition/_2215, gradients/DynamicPartition_grad/DynamicPartition/_2217, gradients/DynamicPartition_grad/DynamicPartition/_2219, gradients/DynamicPartition_grad/DynamicPartition/_2221, gradients/DynamicPartition_grad/DynamicPartition/_2223, gradients/DynamicPartition_grad/DynamicPartition/_2225, gradients/DynamicPartition_grad/DynamicPartition/_2227, gradients/DynamicPartition_grad/DynamicPartition/_2229, gradients/DynamicPartition_grad/DynamicPartition/_2231, gradients/DynamicPartition_grad/DynamicPartition/_2233, gradients/DynamicPartition_grad/DynamicPartition/_2235, gradients/DynamicPartition_grad/DynamicPartition/_2237, gradients/DynamicPartition_grad/DynamicPartition/_2239, gradients/DynamicPartition_grad/DynamicPartition/_2241, gradients/DynamicPartition_grad/DynamicPartition/_2243, gradients/DynamicPartition_grad/DynamicPartition/_2245, gradients/DynamicPartition_grad/DynamicPartition/_2247, gradients/DynamicPartition_grad/DynamicPartition/_2249, gradients/DynamicPartition_grad/DynamicPartition/_2251, gradients/DynamicPartition_grad/DynamicPartition/_2253, gradients/DynamicPartition_grad/DynamicPartition/_2255, gradients/DynamicPartition_grad/DynamicPartition/_2257, gradients/DynamicPartition_grad/DynamicPartition/_2259, gradients/DynamicPartition_grad/DynamicPartition/_2261, gradients/DynamicPartition_grad/DynamicPartition/_2263, gradients/DynamicPartition_grad/DynamicPartition/_2265, gradients/DynamicPartition_grad/DynamicPartition/_2267, gradients/DynamicPartition_grad/DynamicPartition/_2269, gradients/DynamicPartition_grad/DynamicPartition/_2271, gradients/DynamicPartition_grad/DynamicPartition/_2273, gradients/DynamicPartition_grad/DynamicPartition/_2275, gradients/DynamicPartition_grad/DynamicPartition/_2277, gradients/DynamicPartition_grad/DynamicPartition/_2279, gradients/DynamicPartition_grad/DynamicPartition/_2281, gradients/DynamicPartition_grad/DynamicPartition/_2283, gradients/DynamicPartition_grad/DynamicPartition/_2285, gradients/DynamicPartition_grad/DynamicPartition/_2287, gradients/DynamicPartition_grad/DynamicPartition/_2289, gradients/DynamicPartition_grad/DynamicPartition/_2291, gradients/DynamicPartition_grad/DynamicPartition/_2293, gradients/DynamicPartition_grad/DynamicPartition/_2295, gradients/DynamicPartition_grad/DynamicPartition/_2297, gradients/DynamicPartition_grad/DynamicPartition/_2299, gradients/DynamicPartition_grad/DynamicPartition/_2301, gradients/DynamicPartition_grad/DynamicPartition/_2303, gradients/DynamicPartition_grad/DynamicPartition/_2305, gradients/DynamicPartition_grad/DynamicPartition/_2307, gradients/DynamicPartition_grad/DynamicPartition/_2309, gradients/DynamicPartition_grad/DynamicPartition/_2311, gradients/DynamicPartition_grad/DynamicPartition/_2313, gradients/DynamicPartition_grad/DynamicPartition/_2315, gradients/DynamicPartition_grad/DynamicPartition/_2317, gradients/DynamicPartition_grad/DynamicPartition/_2319, gradients/DynamicPartition_grad/DynamicPartition/_2321, gradients/DynamicPartition_grad/DynamicPartition/_2323, gradients/DynamicPartition_grad/DynamicPartition/_2325, gradients/DynamicPartition_grad/DynamicPartition/_2327, gradients/DynamicPartition_grad/DynamicPartition/_2329, gradients/DynamicPartition_grad/DynamicPartition/_2331, gradients/DynamicPartition_grad/DynamicPartition/_2333, gradients/DynamicPartition_grad/DynamicPartition/_2335, gradients/DynamicPartition_grad/DynamicPartition/_2337, gradients/DynamicPartition_grad/DynamicPartition/_2339, gradients/DynamicPartition_grad/DynamicPartition/_2341, gradients/DynamicPartition_grad/DynamicPartition/_2343, gradients/DynamicPartition_grad/DynamicPartition/_2345, gradients/DynamicPartition_grad/DynamicPartition/_2347, gradients/DynamicPartition_grad/DynamicPartition/_2349, gradients/DynamicPartition_grad/DynamicPartition/_2351, gradients/DynamicPartition_grad/DynamicPartition/_2353, gradients/DynamicPartition_grad/DynamicPartition/_2355, gradients/DynamicPartition_grad/DynamicPartition/_2357, gradients/DynamicPartition_grad/DynamicPartition/_2359, gradients/DynamicPartition_grad/DynamicPartition/_2361, gradients/DynamicPartition_grad/DynamicPartition/_2363, gradients/DynamicPartition_grad/DynamicPartition/_2365, gradients/DynamicPartition_grad/DynamicPartition/_2367, gradients/DynamicPartition_grad/DynamicPartition/_2369, gradients/DynamicPartition_grad/DynamicPartition/_2371, gradients/DynamicPartition_grad/DynamicPartition/_2373, gradients/DynamicPartition_grad/DynamicPartition/_2375, gradients/DynamicPartition_grad/DynamicPartition/_2377, gradients/DynamicPartition_grad/DynamicPartition/_2379, gradients/DynamicPartition_grad/DynamicPartition/_2381, gradients/DynamicPartition_grad/DynamicPartition/_2383, gradients/DynamicPartition_grad/DynamicPartition/_2385, gradients/DynamicPartition_grad/DynamicPartition/_2387, gradients/DynamicPartition_grad/DynamicPartition/_2389, gradients/DynamicPartition_grad/DynamicPartition/_2391, gradients/DynamicPartition_grad/DynamicPartition/_2393, gradients/DynamicPartition_grad/DynamicPartition/_2395, gradients/DynamicPartition_grad/DynamicPartition/_2397, gradients/DynamicPartition_grad/DynamicPartition/_2399, gradients/DynamicPartition_grad/DynamicPartition/_2401, gradients/DynamicPartition_grad/DynamicPartition/_2403, gradients/DynamicPartition_grad/DynamicPartition/_2405, gradients/DynamicPartition_grad/DynamicPartition/_2407, gradients/DynamicPartition_grad/DynamicPartition/_2409, gradients/DynamicPartition_grad/DynamicPartition/_2411, gradients/DynamicPartition_grad/DynamicPartition/_2413, gradients/DynamicPartition_grad/DynamicPartition/_2415, gradients/DynamicPartition_grad/DynamicPartition/_2417, gradients/DynamicPartition_grad/DynamicPartition/_2419, gradients/DynamicPartition_grad/DynamicPartition/_2421, gradients/DynamicPartition_grad/DynamicPartition/_2423, gradients/DynamicPartition_grad/DynamicPartition/_2425, gradients/DynamicPartition_grad/DynamicPartition/_2427, gradients/DynamicPartition_grad/DynamicPartition/_2429, gradients/DynamicPartition_grad/DynamicPartition/_2431, gradients/DynamicPartition_grad/DynamicPartition/_2433, gradients/DynamicPartition_grad/DynamicPartition/_2435, gradients/DynamicPartition_grad/DynamicPartition/_2437, gradients/DynamicPartition_grad/DynamicPartition/_2439, gradients/DynamicPartition_grad/DynamicPartition/_2441, gradients/DynamicPartition_grad/DynamicPartition/_2443, gradients/DynamicPartition_grad/DynamicPartition/_2445, gradients/DynamicPartition_grad/DynamicPartition/_2447, gradients/DynamicPartition_grad/DynamicPartition/_2449, gradients/DynamicPartition_grad/DynamicPartition/_2451, gradients/DynamicPartition_grad/DynamicPartition/_2453, gradients/DynamicPartition_grad/DynamicPartition/_2455, gradients/DynamicPartition_grad/DynamicPartition/_2457, gradients/DynamicPartition_grad/DynamicPartition/_2459, gradients/DynamicPartition_grad/DynamicPartition/_2461, gradients/DynamicPartition_grad/DynamicPartition/_2463, gradients/DynamicPartition_grad/DynamicPartition/_2465, gradients/DynamicPartition_grad/DynamicPartition/_2467, gradients/DynamicPartition_grad/DynamicPartition/_2469, gradients/DynamicPartition_grad/DynamicPartition/_2471, gradients/DynamicPartition_grad/DynamicPartition/_2473, gradients/DynamicPartition_grad/DynamicPartition/_2475, gradients/DynamicPartition_grad/DynamicPartition/_2477, gradients/DynamicPartition_grad/DynamicPartition/_2479, gradients/DynamicPartition_grad/DynamicPartition/_2481, gradients/DynamicPartition_grad/DynamicPartition/_2483, gradients/DynamicPartition_grad/DynamicPartition/_2485, gradients/DynamicPartition_grad/DynamicPartition/_2487, gradients/DynamicPartition_grad/DynamicPartition/_2489, gradients/DynamicPartition_grad/DynamicPartition/_2491, gradients/DynamicPartition_grad/DynamicPartition/_2493, gradients/DynamicPartition_grad/DynamicPartition/_2495, gradients/DynamicPartition_grad/DynamicPartition/_2497, gradients/DynamicPartition_grad/DynamicPartition/_2499, gradients/DynamicPartition_grad/DynamicPartition/_2501, gradients/DynamicPartition_grad/DynamicPartition/_2503, gradients/DynamicPartition_grad/DynamicPartition/_2505, gradients/DynamicPartition_grad/DynamicPartition/_2507, gradients/DynamicPartition_grad/DynamicPartition/_2509, gradients/DynamicPartition_grad/DynamicPartition/_2511, gradients/DynamicPartition_grad/DynamicPartition/_2513, gradients/DynamicPartition_grad/DynamicPartition/_2515, gradients/DynamicPartition_grad/DynamicPartition/_2517, gradients/DynamicPartition_grad/DynamicPartition/_2519, gradients/DynamicPartition_grad/DynamicPartition/_2521, gradients/DynamicPartition_grad/DynamicPartition/_2523, gradients/DynamicPartition_grad/DynamicPartition/_2525, gradients/DynamicPartition_grad/DynamicPartition/_2527, gradients/DynamicPartition_grad/DynamicPartition/_2529, gradients/DynamicPartition_grad/DynamicPartition/_2531, gradients/DynamicPartition_grad/DynamicPartition/_2533, gradients/DynamicPartition_grad/DynamicPartition/_2535, gradients/DynamicPartition_grad/DynamicPartition/_2537, gradients/DynamicPartition_grad/DynamicPartition/_2539, gradients/DynamicPartition_grad/DynamicPartition/_2541, gradients/DynamicPartition_grad/DynamicPartition/_2543, gradients/DynamicPartition_grad/DynamicPartition/_2545, gradients/DynamicPartition_grad/DynamicPartition/_2547, gradients/DynamicPartition_grad/DynamicPartition/_2549, gradients/DynamicPartition_grad/DynamicPartition/_2551, gradients/DynamicPartition_grad/DynamicPartition/_2553, gradients/DynamicPartition_grad/DynamicPartition/_2555, gradients/DynamicPartition_grad/DynamicPartition/_2557, gradients/DynamicPartition_grad/DynamicPartition/_2559, gradients/DynamicPartition_grad/DynamicPartition/_2561, gradients/DynamicPartition_grad/DynamicPartition/_2563, gradients/DynamicPartition_grad/DynamicPartition/_2565, gradients/DynamicPartition_grad/DynamicPartition/_2567, gradients/Mean_grad/truediv, gradients/Mean_1_grad/truediv, gradients/Mean_2_grad/truediv, gradients/Mean_3_grad/truediv, gradients/Mean_4_grad/truediv, gradients/Mean_5_grad/truediv, gradients/Mean_6_grad/truediv, gradients/Mean_7_grad/truediv, gradients/Mean_8_grad/truediv, gradients/Mean_9_grad/truediv, gradients/Mean_10_grad/truediv, gradients/Mean_11_grad/truediv, gradients/Mean_12_grad/truediv, gradients/Mean_13_grad/truediv, gradients/Mean_14_grad/truediv, gradients/Mean_15_grad/truediv, gradients/Mean_16_grad/truediv, gradients/Mean_17_grad/truediv, gradients/Mean_18_grad/truediv, gradients/Mean_19_grad/truediv, gradients/Mean_20_grad/truediv, gradients/Mean_21_grad/truediv, gradients/Mean_22_grad/truediv, gradients/Mean_23_grad/truediv, gradients/Mean_24_grad/truediv, gradients/Mean_25_grad/truediv, gradients/Mean_26_grad/truediv, gradients/Mean_27_grad/truediv, gradients/Mean_28_grad/truediv, gradients/Mean_29_grad/truediv, gradients/Mean_30_grad/truediv, gradients/Mean_31_grad/truediv, gradients/Mean_32_grad/truediv, gradients/Mean_33_grad/truediv, gradients/Mean_34_grad/truediv, gradients/Mean_35_grad/truediv, gradients/Mean_36_grad/truediv, gradients/Mean_37_grad/truediv, gradients/Mean_38_grad/truediv, gradients/Mean_39_grad/truediv, gradients/Mean_40_grad/truediv, gradients/Mean_41_grad/truediv, gradients/Mean_42_grad/truediv, gradients/Mean_43_grad/truediv, gradients/Mean_44_grad/truediv, gradients/Mean_45_grad/truediv, gradients/Mean_46_grad/truediv, gradients/Mean_47_grad/truediv, gradients/Mean_48_grad/truediv, gradients/Mean_49_grad/truediv, gradients/Mean_50_grad/truediv, gradients/Mean_51_grad/truediv, gradients/Mean_52_grad/truediv, gradients/Mean_53_grad/truediv, gradients/Mean_54_grad/truediv, gradients/Mean_55_grad/truediv, gradients/Mean_56_grad/truediv, gradients/Mean_57_grad/truediv, gradients/Mean_58_grad/truediv, gradients/Mean_59_grad/truediv, gradients/Mean_60_grad/truediv, gradients/Mean_61_grad/truediv, gradients/Mean_62_grad/truediv, gradients/Mean_63_grad/truediv, gradients/Mean_64_grad/truediv, gradients/Mean_65_grad/truediv, gradients/Mean_66_grad/truediv, gradients/Mean_67_grad/truediv, gradients/Mean_68_grad/truediv, gradients/Mean_69_grad/truediv, gradients/Mean_70_grad/truediv, gradients/Mean_71_grad/truediv, gradients/Mean_72_grad/truediv, gradients/Mean_73_grad/truediv, gradients/Mean_74_grad/truediv, gradients/Mean_75_grad/truediv, gradients/Mean_76_grad/truediv, gradients/Mean_77_grad/truediv, gradients/Mean_78_grad/truediv, gradients/Mean_79_grad/truediv, gradients/Mean_80_grad/truediv, gradients/Mean_81_grad/truediv, gradients/Mean_82_grad/truediv, gradients/Mean_83_grad/truediv, gradients/Mean_84_grad/truediv, gradients/Mean_85_grad/truediv, gradients/Mean_86_grad/truediv, gradients/Mean_87_grad/truediv, gradients/Mean_88_grad/truediv, gradients/Mean_89_grad/truediv, gradients/Mean_90_grad/truediv, gradients/Mean_91_grad/truediv, gradients/Mean_92_grad/truediv, gradients/Mean_93_grad/truediv, gradients/Mean_94_grad/truediv, gradients/Mean_95_grad/truediv, gradients/Mean_96_grad/truediv, gradients/Mean_97_grad/truediv, gradients/Mean_98_grad/truediv, gradients/Mean_99_grad/truediv, gradients/Mean_100_grad/truediv, gradients/Mean_101_grad/truediv, gradients/Mean_102_grad/truediv, gradients/Mean_103_grad/truediv, gradients/Mean_104_grad/truediv, gradients/Mean_105_grad/truediv, gradients/Mean_106_grad/truediv, gradients/Mean_107_grad/truediv, gradients/Mean_108_grad/truediv, gradients/Mean_109_grad/truediv, gradients/Mean_110_grad/truediv, gradients/Mean_111_grad/truediv, gradients/Mean_112_grad/truediv, gradients/Mean_113_grad/truediv, gradients/Mean_114_grad/truediv, gradients/Mean_115_grad/truediv, gradients/Mean_116_grad/truediv, gradients/Mean_117_grad/truediv, gradients/Mean_118_grad/truediv, gradients/Mean_119_grad/truediv, gradients/Mean_120_grad/truediv, gradients/Mean_121_grad/truediv, gradients/Mean_122_grad/truediv, gradients/Mean_123_grad/truediv, gradients/Mean_124_grad/truediv, gradients/Mean_125_grad/truediv, gradients/Mean_126_grad/truediv, gradients/Mean_127_grad/truediv, gradients/Mean_128_grad/truediv, gradients/Mean_129_grad/truediv, gradients/Mean_130_grad/truediv, gradients/Mean_131_grad/truediv, gradients/Mean_132_grad/truediv, gradients/Mean_133_grad/truediv, gradients/Mean_134_grad/truediv, gradients/Mean_135_grad/truediv, gradients/Mean_136_grad/truediv, gradients/Mean_137_grad/truediv, gradients/Mean_138_grad/truediv, gradients/Mean_139_grad/truediv, gradients/Mean_140_grad/truediv, gradients/Mean_141_grad/truediv, gradients/Mean_142_grad/truediv, gradients/Mean_143_grad/truediv, gradients/Mean_144_grad/truediv, gradients/Mean_145_grad/truediv, gradients/Mean_146_grad/truediv, gradients/Mean_147_grad/truediv, gradients/Mean_148_grad/truediv, gradients/Mean_149_grad/truediv, gradients/Mean_150_grad/truediv, gradients/Mean_151_grad/truediv, gradients/Mean_152_grad/truediv, gradients/Mean_153_grad/truediv, gradients/Mean_154_grad/truediv, gradients/Mean_155_grad/truediv, gradients/Mean_156_grad/truediv, gradients/Mean_157_grad/truediv, gradients/Mean_158_grad/truediv, gradients/Mean_159_grad/truediv, gradients/Mean_160_grad/truediv, gradients/Mean_161_grad/truediv, gradients/Mean_162_grad/truediv, gradients/Mean_163_grad/truediv, gradients/Mean_164_grad/truediv, gradients/Mean_165_grad/truediv, gradients/Mean_166_grad/truediv, gradients/Mean_167_grad/truediv, gradients/Mean_168_grad/truediv, gradients/Mean_169_grad/truediv, gradients/Mean_170_grad/truediv, gradients/Mean_171_grad/truediv, gradients/Mean_172_grad/truediv, gradients/Mean_173_grad/truediv, gradients/Mean_174_grad/truediv, gradients/Mean_175_grad/truediv, gradients/Mean_176_grad/truediv, gradients/Mean_177_grad/truediv, gradients/Mean_178_grad/truediv, gradients/Mean_179_grad/truediv, gradients/Mean_180_grad/truediv, gradients/Mean_181_grad/truediv, gradients/Mean_182_grad/truediv, gradients/Mean_183_grad/truediv, gradients/Mean_184_grad/truediv, gradients/Mean_185_grad/truediv, gradients/Mean_186_grad/truediv, gradients/Mean_187_grad/truediv, gradients/Mean_188_grad/truediv, gradients/Mean_189_grad/truediv, gradients/Mean_190_grad/truediv, gradients/Mean_191_grad/truediv, gradients/Mean_192_grad/truediv, gradients/Mean_193_grad/truediv, gradients/Mean_194_grad/truediv, gradients/Mean_195_grad/truediv, gradients/Mean_196_grad/truediv, gradients/Mean_197_grad/truediv, gradients/Mean_198_grad/truediv, gradients/Mean_199_grad/truediv, gradients/Mean_200_grad/truediv, gradients/Mean_201_grad/truediv, gradients/Mean_202_grad/truediv, gradients/Mean_203_grad/truediv, gradients/Mean_204_grad/truediv, gradients/Mean_205_grad/truediv, gradients/Mean_206_grad/truediv, gradients/Mean_207_grad/truediv, gradients/Mean_208_grad/truediv, gradients/Mean_209_grad/truediv, gradients/Mean_210_grad/truediv, gradients/Mean_211_grad/truediv, gradients/Mean_212_grad/truediv, gradients/Mean_213_grad/truediv, gradients/Mean_214_grad/truediv, gradients/Mean_215_grad/truediv, gradients/Mean_216_grad/truediv, gradients/Mean_217_grad/truediv, gradients/Mean_218_grad/truediv, gradients/Mean_219_grad/truediv, gradients/Mean_220_grad/truediv, gradients/Mean_221_grad/truediv, gradients/Mean_222_grad/truediv, gradients/Mean_223_grad/truediv, gradients/Mean_224_grad/truediv, gradients/Mean_225_grad/truediv, gradients/Mean_226_grad/truediv, gradients/Mean_227_grad/truediv, gradients/Mean_228_grad/truediv, gradients/Mean_229_grad/truediv, gradients/Mean_230_grad/truediv, gradients/Mean_231_grad/truediv, gradients/Mean_232_grad/truediv, gradients/Mean_233_grad/truediv, gradients/Mean_234_grad/truediv, gradients/Mean_235_grad/truediv, gradients/Mean_236_grad/truediv, gradients/Mean_237_grad/truediv, gradients/Mean_238_grad/truediv, gradients/Mean_239_grad/truediv, gradients/Mean_240_grad/truediv, gradients/Mean_241_grad/truediv, gradients/Mean_242_grad/truediv, gradients/Mean_243_grad/truediv, gradients/Mean_244_grad/truediv, gradients/Mean_245_grad/truediv, gradients/Mean_246_grad/truediv, gradients/Mean_247_grad/truediv, gradients/Mean_248_grad/truediv, gradients/Mean_249_grad/truediv, gradients/Mean_250_grad/truediv, gradients/Mean_251_grad/truediv, gradients/Mean_252_grad/truediv, gradients/Mean_253_grad/truediv, gradients/Mean_254_grad/truediv, gradients/Mean_255_grad/truediv)]]\r\n\r\nAttempting to run adam optimizer\r\ndata[255].shape = [41,1] does not start with indices[255].shape = [28]\r\n\t [[Node: gradients_1/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_1/DynamicPartition_grad/DynamicPartition/_4625, gradients_1/DynamicPartition_grad/DynamicPartition/_4627, gradients_1/DynamicPartition_grad/DynamicPartition/_4629, gradients_1/DynamicPartition_grad/DynamicPartition/_4631, gradients_1/DynamicPartition_grad/DynamicPartition/_4633, gradients_1/DynamicPartition_grad/DynamicPartition/_4635, gradients_1/DynamicPartition_grad/DynamicPartition/_4637, gradients_1/DynamicPartition_grad/DynamicPartition/_4639, gradients_1/DynamicPartition_grad/DynamicPartition/_4641, gradients_1/DynamicPartition_grad/DynamicPartition/_4643, gradients_1/DynamicPartition_grad/DynamicPartition/_4645, gradients_1/DynamicPartition_grad/DynamicPartition/_4647, gradients_1/DynamicPartition_grad/DynamicPartition/_4649, gradients_1/DynamicPartition_grad/DynamicPartition/_4651, gradients_1/DynamicPartition_grad/DynamicPartition/_4653, gradients_1/DynamicPartition_grad/DynamicPartition/_4655, gradients_1/DynamicPartition_grad/DynamicPartition/_4657, gradients_1/DynamicPartition_grad/DynamicPartition/_4659, gradients_1/DynamicPartition_grad/DynamicPartition/_4661, gradients_1/DynamicPartition_grad/DynamicPartition/_4663, gradients_1/DynamicPartition_grad/DynamicPartition/_4665, gradients_1/DynamicPartition_grad/DynamicPartition/_4667, gradients_1/DynamicPartition_grad/DynamicPartition/_4669, gradients_1/DynamicPartition_grad/DynamicPartition/_4671, gradients_1/DynamicPartition_grad/DynamicPartition/_4673, gradients_1/DynamicPartition_grad/DynamicPartition/_4675, gradients_1/DynamicPartition_grad/DynamicPartition/_4677, gradients_1/DynamicPartition_grad/DynamicPartition/_4679, gradients_1/DynamicPartition_grad/DynamicPartition/_4681, gradients_1/DynamicPartition_grad/DynamicPartition/_4683, gradients_1/DynamicPartition_grad/DynamicPartition/_4685, gradients_1/DynamicPartition_grad/DynamicPartition/_4687, gradients_1/DynamicPartition_grad/DynamicPartition/_4689, gradients_1/DynamicPartition_grad/DynamicPartition/_4691, gradients_1/DynamicPartition_grad/DynamicPartition/_4693, gradients_1/DynamicPartition_grad/DynamicPartition/_4695, gradients_1/DynamicPartition_grad/DynamicPartition/_4697, gradients_1/DynamicPartition_grad/DynamicPartition/_4699, gradients_1/DynamicPartition_grad/DynamicPartition/_4701, gradients_1/DynamicPartition_grad/DynamicPartition/_4703, gradients_1/DynamicPartition_grad/DynamicPartition/_4705, gradients_1/DynamicPartition_grad/DynamicPartition/_4707, gradients_1/DynamicPartition_grad/DynamicPartition/_4709, gradients_1/DynamicPartition_grad/DynamicPartition/_4711, gradients_1/DynamicPartition_grad/DynamicPartition/_4713, gradients_1/DynamicPartition_grad/DynamicPartition/_4715, gradients_1/DynamicPartition_grad/DynamicPartition/_4717, gradients_1/DynamicPartition_grad/DynamicPartition/_4719, gradients_1/DynamicPartition_grad/DynamicPartition/_4721, gradients_1/DynamicPartition_grad/DynamicPartition/_4723, gradients_1/DynamicPartition_grad/DynamicPartition/_4725, gradients_1/DynamicPartition_grad/DynamicPartition/_4727, gradients_1/DynamicPartition_grad/DynamicPartition/_4729, gradients_1/DynamicPartition_grad/DynamicPartition/_4731, gradients_1/DynamicPartition_grad/DynamicPartition/_4733, gradients_1/DynamicPartition_grad/DynamicPartition/_4735, gradients_1/DynamicPartition_grad/DynamicPartition/_4737, gradients_1/DynamicPartition_grad/DynamicPartition/_4739, gradients_1/DynamicPartition_grad/DynamicPartition/_4741, gradients_1/DynamicPartition_grad/DynamicPartition/_4743, gradients_1/DynamicPartition_grad/DynamicPartition/_4745, gradients_1/DynamicPartition_grad/DynamicPartition/_4747, gradients_1/DynamicPartition_grad/DynamicPartition/_4749, gradients_1/DynamicPartition_grad/DynamicPartition/_4751, gradients_1/DynamicPartition_grad/DynamicPartition/_4753, gradients_1/DynamicPartition_grad/DynamicPartition/_4755, gradients_1/DynamicPartition_grad/DynamicPartition/_4757, gradients_1/DynamicPartition_grad/DynamicPartition/_4759, gradients_1/DynamicPartition_grad/DynamicPartition/_4761, gradients_1/DynamicPartition_grad/DynamicPartition/_4763, gradients_1/DynamicPartition_grad/DynamicPartition/_4765, gradients_1/DynamicPartition_grad/DynamicPartition/_4767, gradients_1/DynamicPartition_grad/DynamicPartition/_4769, gradients_1/DynamicPartition_grad/DynamicPartition/_4771, gradients_1/DynamicPartition_grad/DynamicPartition/_4773, gradients_1/DynamicPartition_grad/DynamicPartition/_4775, gradients_1/DynamicPartition_grad/DynamicPartition/_4777, gradients_1/DynamicPartition_grad/DynamicPartition/_4779, gradients_1/DynamicPartition_grad/DynamicPartition/_4781, gradients_1/DynamicPartition_grad/DynamicPartition/_4783, gradients_1/DynamicPartition_grad/DynamicPartition/_4785, gradients_1/DynamicPartition_grad/DynamicPartition/_4787, gradients_1/DynamicPartition_grad/DynamicPartition/_4789, gradients_1/DynamicPartition_grad/DynamicPartition/_4791, gradients_1/DynamicPartition_grad/DynamicPartition/_4793, gradients_1/DynamicPartition_grad/DynamicPartition/_4795, gradients_1/DynamicPartition_grad/DynamicPartition/_4797, gradients_1/DynamicPartition_grad/DynamicPartition/_4799, gradients_1/DynamicPartition_grad/DynamicPartition/_4801, gradients_1/DynamicPartition_grad/DynamicPartition/_4803, gradients_1/DynamicPartition_grad/DynamicPartition/_4805, gradients_1/DynamicPartition_grad/DynamicPartition/_4807, gradients_1/DynamicPartition_grad/DynamicPartition/_4809, gradients_1/DynamicPartition_grad/DynamicPartition/_4811, gradients_1/DynamicPartition_grad/DynamicPartition/_4813, gradients_1/DynamicPartition_grad/DynamicPartition/_4815, gradients_1/DynamicPartition_grad/DynamicPartition/_4817, gradients_1/DynamicPartition_grad/DynamicPartition/_4819, gradients_1/DynamicPartition_grad/DynamicPartition/_4821, gradients_1/DynamicPartition_grad/DynamicPartition/_4823, gradients_1/DynamicPartition_grad/DynamicPartition/_4825, gradients_1/DynamicPartition_grad/DynamicPartition/_4827, gradients_1/DynamicPartition_grad/DynamicPartition/_4829, gradients_1/DynamicPartition_grad/DynamicPartition/_4831, gradients_1/DynamicPartition_grad/DynamicPartition/_4833, gradients_1/DynamicPartition_grad/DynamicPartition/_4835, gradients_1/DynamicPartition_grad/DynamicPartition/_4837, gradients_1/DynamicPartition_grad/DynamicPartition/_4839, gradients_1/DynamicPartition_grad/DynamicPartition/_4841, gradients_1/DynamicPartition_grad/DynamicPartition/_4843, gradients_1/DynamicPartition_grad/DynamicPartition/_4845, gradients_1/DynamicPartition_grad/DynamicPartition/_4847, gradients_1/DynamicPartition_grad/DynamicPartition/_4849, gradients_1/DynamicPartition_grad/DynamicPartition/_4851, gradients_1/DynamicPartition_grad/DynamicPartition/_4853, gradients_1/DynamicPartition_grad/DynamicPartition/_4855, gradients_1/DynamicPartition_grad/DynamicPartition/_4857, gradients_1/DynamicPartition_grad/DynamicPartition/_4859, gradients_1/DynamicPartition_grad/DynamicPartition/_4861, gradients_1/DynamicPartition_grad/DynamicPartition/_4863, gradients_1/DynamicPartition_grad/DynamicPartition/_4865, gradients_1/DynamicPartition_grad/DynamicPartition/_4867, gradients_1/DynamicPartition_grad/DynamicPartition/_4869, gradients_1/DynamicPartition_grad/DynamicPartition/_4871, gradients_1/DynamicPartition_grad/DynamicPartition/_4873, gradients_1/DynamicPartition_grad/DynamicPartition/_4875, gradients_1/DynamicPartition_grad/DynamicPartition/_4877, gradients_1/DynamicPartition_grad/DynamicPartition/_4879, gradients_1/DynamicPartition_grad/DynamicPartition/_4881, gradients_1/DynamicPartition_grad/DynamicPartition/_4883, gradients_1/DynamicPartition_grad/DynamicPartition/_4885, gradients_1/DynamicPartition_grad/DynamicPartition/_4887, gradients_1/DynamicPartition_grad/DynamicPartition/_4889, gradients_1/DynamicPartition_grad/DynamicPartition/_4891, gradients_1/DynamicPartition_grad/DynamicPartition/_4893, gradients_1/DynamicPartition_grad/DynamicPartition/_4895, gradients_1/DynamicPartition_grad/DynamicPartition/_4897, gradients_1/DynamicPartition_grad/DynamicPartition/_4899, gradients_1/DynamicPartition_grad/DynamicPartition/_4901, gradients_1/DynamicPartition_grad/DynamicPartition/_4903, gradients_1/DynamicPartition_grad/DynamicPartition/_4905, gradients_1/DynamicPartition_grad/DynamicPartition/_4907, gradients_1/DynamicPartition_grad/DynamicPartition/_4909, gradients_1/DynamicPartition_grad/DynamicPartition/_4911, gradients_1/DynamicPartition_grad/DynamicPartition/_4913, gradients_1/DynamicPartition_grad/DynamicPartition/_4915, gradients_1/DynamicPartition_grad/DynamicPartition/_4917, gradients_1/DynamicPartition_grad/DynamicPartition/_4919, gradients_1/DynamicPartition_grad/DynamicPartition/_4921, gradients_1/DynamicPartition_grad/DynamicPartition/_4923, gradients_1/DynamicPartition_grad/DynamicPartition/_4925, gradients_1/DynamicPartition_grad/DynamicPartition/_4927, gradients_1/DynamicPartition_grad/DynamicPartition/_4929, gradients_1/DynamicPartition_grad/DynamicPartition/_4931, gradients_1/DynamicPartition_grad/DynamicPartition/_4933, gradients_1/DynamicPartition_grad/DynamicPartition/_4935, gradients_1/DynamicPartition_grad/DynamicPartition/_4937, gradients_1/DynamicPartition_grad/DynamicPartition/_4939, gradients_1/DynamicPartition_grad/DynamicPartition/_4941, gradients_1/DynamicPartition_grad/DynamicPartition/_4943, gradients_1/DynamicPartition_grad/DynamicPartition/_4945, gradients_1/DynamicPartition_grad/DynamicPartition/_4947, gradients_1/DynamicPartition_grad/DynamicPartition/_4949, gradients_1/DynamicPartition_grad/DynamicPartition/_4951, gradients_1/DynamicPartition_grad/DynamicPartition/_4953, gradients_1/DynamicPartition_grad/DynamicPartition/_4955, gradients_1/DynamicPartition_grad/DynamicPartition/_4957, gradients_1/DynamicPartition_grad/DynamicPartition/_4959, gradients_1/DynamicPartition_grad/DynamicPartition/_4961, gradients_1/DynamicPartition_grad/DynamicPartition/_4963, gradients_1/DynamicPartition_grad/DynamicPartition/_4965, gradients_1/DynamicPartition_grad/DynamicPartition/_4967, gradients_1/DynamicPartition_grad/DynamicPartition/_4969, gradients_1/DynamicPartition_grad/DynamicPartition/_4971, gradients_1/DynamicPartition_grad/DynamicPartition/_4973, gradients_1/DynamicPartition_grad/DynamicPartition/_4975, gradients_1/DynamicPartition_grad/DynamicPartition/_4977, gradients_1/DynamicPartition_grad/DynamicPartition/_4979, gradients_1/DynamicPartition_grad/DynamicPartition/_4981, gradients_1/DynamicPartition_grad/DynamicPartition/_4983, gradients_1/DynamicPartition_grad/DynamicPartition/_4985, gradients_1/DynamicPartition_grad/DynamicPartition/_4987, gradients_1/DynamicPartition_grad/DynamicPartition/_4989, gradients_1/DynamicPartition_grad/DynamicPartition/_4991, gradients_1/DynamicPartition_grad/DynamicPartition/_4993, gradients_1/DynamicPartition_grad/DynamicPartition/_4995, gradients_1/DynamicPartition_grad/DynamicPartition/_4997, gradients_1/DynamicPartition_grad/DynamicPartition/_4999, gradients_1/DynamicPartition_grad/DynamicPartition/_5001, gradients_1/DynamicPartition_grad/DynamicPartition/_5003, gradients_1/DynamicPartition_grad/DynamicPartition/_5005, gradients_1/DynamicPartition_grad/DynamicPartition/_5007, gradients_1/DynamicPartition_grad/DynamicPartition/_5009, gradients_1/DynamicPartition_grad/DynamicPartition/_5011, gradients_1/DynamicPartition_grad/DynamicPartition/_5013, gradients_1/DynamicPartition_grad/DynamicPartition/_5015, gradients_1/DynamicPartition_grad/DynamicPartition/_5017, gradients_1/DynamicPartition_grad/DynamicPartition/_5019, gradients_1/DynamicPartition_grad/DynamicPartition/_5021, gradients_1/DynamicPartition_grad/DynamicPartition/_5023, gradients_1/DynamicPartition_grad/DynamicPartition/_5025, gradients_1/DynamicPartition_grad/DynamicPartition/_5027, gradients_1/DynamicPartition_grad/DynamicPartition/_5029, gradients_1/DynamicPartition_grad/DynamicPartition/_5031, gradients_1/DynamicPartition_grad/DynamicPartition/_5033, gradients_1/DynamicPartition_grad/DynamicPartition/_5035, gradients_1/DynamicPartition_grad/DynamicPartition/_5037, gradients_1/DynamicPartition_grad/DynamicPartition/_5039, gradients_1/DynamicPartition_grad/DynamicPartition/_5041, gradients_1/DynamicPartition_grad/DynamicPartition/_5043, gradients_1/DynamicPartition_grad/DynamicPartition/_5045, gradients_1/DynamicPartition_grad/DynamicPartition/_5047, gradients_1/DynamicPartition_grad/DynamicPartition/_5049, gradients_1/DynamicPartition_grad/DynamicPartition/_5051, gradients_1/DynamicPartition_grad/DynamicPartition/_5053, gradients_1/DynamicPartition_grad/DynamicPartition/_5055, gradients_1/DynamicPartition_grad/DynamicPartition/_5057, gradients_1/DynamicPartition_grad/DynamicPartition/_5059, gradients_1/DynamicPartition_grad/DynamicPartition/_5061, gradients_1/DynamicPartition_grad/DynamicPartition/_5063, gradients_1/DynamicPartition_grad/DynamicPartition/_5065, gradients_1/DynamicPartition_grad/DynamicPartition/_5067, gradients_1/DynamicPartition_grad/DynamicPartition/_5069, gradients_1/DynamicPartition_grad/DynamicPartition/_5071, gradients_1/DynamicPartition_grad/DynamicPartition/_5073, gradients_1/DynamicPartition_grad/DynamicPartition/_5075, gradients_1/DynamicPartition_grad/DynamicPartition/_5077, gradients_1/DynamicPartition_grad/DynamicPartition/_5079, gradients_1/DynamicPartition_grad/DynamicPartition/_5081, gradients_1/DynamicPartition_grad/DynamicPartition/_5083, gradients_1/DynamicPartition_grad/DynamicPartition/_5085, gradients_1/DynamicPartition_grad/DynamicPartition/_5087, gradients_1/DynamicPartition_grad/DynamicPartition/_5089, gradients_1/DynamicPartition_grad/DynamicPartition/_5091, gradients_1/DynamicPartition_grad/DynamicPartition/_5093, gradients_1/DynamicPartition_grad/DynamicPartition/_5095, gradients_1/DynamicPartition_grad/DynamicPartition/_5097, gradients_1/DynamicPartition_grad/DynamicPartition/_5099, gradients_1/DynamicPartition_grad/DynamicPartition/_5101, gradients_1/DynamicPartition_grad/DynamicPartition/_5103, gradients_1/DynamicPartition_grad/DynamicPartition/_5105, gradients_1/DynamicPartition_grad/DynamicPartition/_5107, gradients_1/DynamicPartition_grad/DynamicPartition/_5109, gradients_1/DynamicPartition_grad/DynamicPartition/_5111, gradients_1/DynamicPartition_grad/DynamicPartition/_5113, gradients_1/DynamicPartition_grad/DynamicPartition/_5115, gradients_1/DynamicPartition_grad/DynamicPartition/_5117, gradients_1/DynamicPartition_grad/DynamicPartition/_5119, gradients_1/DynamicPartition_grad/DynamicPartition/_5121, gradients_1/DynamicPartition_grad/DynamicPartition/_5123, gradients_1/DynamicPartition_grad/DynamicPartition/_5125, gradients_1/DynamicPartition_grad/DynamicPartition/_5127, gradients_1/DynamicPartition_grad/DynamicPartition/_5129, gradients_1/DynamicPartition_grad/DynamicPartition/_5131, gradients_1/DynamicPartition_grad/DynamicPartition/_5133, gradients_1/DynamicPartition_grad/DynamicPartition/_5135, gradients_1/Mean_grad/truediv, gradients_1/Mean_1_grad/truediv, gradients_1/Mean_2_grad/truediv, gradients_1/Mean_3_grad/truediv, gradients_1/Mean_4_grad/truediv, gradients_1/Mean_5_grad/truediv, gradients_1/Mean_6_grad/truediv, gradients_1/Mean_7_grad/truediv, gradients_1/Mean_8_grad/truediv, gradients_1/Mean_9_grad/truediv, gradients_1/Mean_10_grad/truediv, gradients_1/Mean_11_grad/truediv, gradients_1/Mean_12_grad/truediv, gradients_1/Mean_13_grad/truediv, gradients_1/Mean_14_grad/truediv, gradients_1/Mean_15_grad/truediv, gradients_1/Mean_16_grad/truediv, gradients_1/Mean_17_grad/truediv, gradients_1/Mean_18_grad/truediv, gradients_1/Mean_19_grad/truediv, gradients_1/Mean_20_grad/truediv, gradients_1/Mean_21_grad/truediv, gradients_1/Mean_22_grad/truediv, gradients_1/Mean_23_grad/truediv, gradients_1/Mean_24_grad/truediv, gradients_1/Mean_25_grad/truediv, gradients_1/Mean_26_grad/truediv, gradients_1/Mean_27_grad/truediv, gradients_1/Mean_28_grad/truediv, gradients_1/Mean_29_grad/truediv, gradients_1/Mean_30_grad/truediv, gradients_1/Mean_31_grad/truediv, gradients_1/Mean_32_grad/truediv, gradients_1/Mean_33_grad/truediv, gradients_1/Mean_34_grad/truediv, gradients_1/Mean_35_grad/truediv, gradients_1/Mean_36_grad/truediv, gradients_1/Mean_37_grad/truediv, gradients_1/Mean_38_grad/truediv, gradients_1/Mean_39_grad/truediv, gradients_1/Mean_40_grad/truediv, gradients_1/Mean_41_grad/truediv, gradients_1/Mean_42_grad/truediv, gradients_1/Mean_43_grad/truediv, gradients_1/Mean_44_grad/truediv, gradients_1/Mean_45_grad/truediv, gradients_1/Mean_46_grad/truediv, gradients_1/Mean_47_grad/truediv, gradients_1/Mean_48_grad/truediv, gradients_1/Mean_49_grad/truediv, gradients_1/Mean_50_grad/truediv, gradients_1/Mean_51_grad/truediv, gradients_1/Mean_52_grad/truediv, gradients_1/Mean_53_grad/truediv, gradients_1/Mean_54_grad/truediv, gradients_1/Mean_55_grad/truediv, gradients_1/Mean_56_grad/truediv, gradients_1/Mean_57_grad/truediv, gradients_1/Mean_58_grad/truediv, gradients_1/Mean_59_grad/truediv, gradients_1/Mean_60_grad/truediv, gradients_1/Mean_61_grad/truediv, gradients_1/Mean_62_grad/truediv, gradients_1/Mean_63_grad/truediv, gradients_1/Mean_64_grad/truediv, gradients_1/Mean_65_grad/truediv, gradients_1/Mean_66_grad/truediv, gradients_1/Mean_67_grad/truediv, gradients_1/Mean_68_grad/truediv, gradients_1/Mean_69_grad/truediv, gradients_1/Mean_70_grad/truediv, gradients_1/Mean_71_grad/truediv, gradients_1/Mean_72_grad/truediv, gradients_1/Mean_73_grad/truediv, gradients_1/Mean_74_grad/truediv, gradients_1/Mean_75_grad/truediv, gradients_1/Mean_76_grad/truediv, gradients_1/Mean_77_grad/truediv, gradients_1/Mean_78_grad/truediv, gradients_1/Mean_79_grad/truediv, gradients_1/Mean_80_grad/truediv, gradients_1/Mean_81_grad/truediv, gradients_1/Mean_82_grad/truediv, gradients_1/Mean_83_grad/truediv, gradients_1/Mean_84_grad/truediv, gradients_1/Mean_85_grad/truediv, gradients_1/Mean_86_grad/truediv, gradients_1/Mean_87_grad/truediv, gradients_1/Mean_88_grad/truediv, gradients_1/Mean_89_grad/truediv, gradients_1/Mean_90_grad/truediv, gradients_1/Mean_91_grad/truediv, gradients_1/Mean_92_grad/truediv, gradients_1/Mean_93_grad/truediv, gradients_1/Mean_94_grad/truediv, gradients_1/Mean_95_grad/truediv, gradients_1/Mean_96_grad/truediv, gradients_1/Mean_97_grad/truediv, gradients_1/Mean_98_grad/truediv, gradients_1/Mean_99_grad/truediv, gradients_1/Mean_100_grad/truediv, gradients_1/Mean_101_grad/truediv, gradients_1/Mean_102_grad/truediv, gradients_1/Mean_103_grad/truediv, gradients_1/Mean_104_grad/truediv, gradients_1/Mean_105_grad/truediv, gradients_1/Mean_106_grad/truediv, gradients_1/Mean_107_grad/truediv, gradients_1/Mean_108_grad/truediv, gradients_1/Mean_109_grad/truediv, gradients_1/Mean_110_grad/truediv, gradients_1/Mean_111_grad/truediv, gradients_1/Mean_112_grad/truediv, gradients_1/Mean_113_grad/truediv, gradients_1/Mean_114_grad/truediv, gradients_1/Mean_115_grad/truediv, gradients_1/Mean_116_grad/truediv, gradients_1/Mean_117_grad/truediv, gradients_1/Mean_118_grad/truediv, gradients_1/Mean_119_grad/truediv, gradients_1/Mean_120_grad/truediv, gradients_1/Mean_121_grad/truediv, gradients_1/Mean_122_grad/truediv, gradients_1/Mean_123_grad/truediv, gradients_1/Mean_124_grad/truediv, gradients_1/Mean_125_grad/truediv, gradients_1/Mean_126_grad/truediv, gradients_1/Mean_127_grad/truediv, gradients_1/Mean_128_grad/truediv, gradients_1/Mean_129_grad/truediv, gradients_1/Mean_130_grad/truediv, gradients_1/Mean_131_grad/truediv, gradients_1/Mean_132_grad/truediv, gradients_1/Mean_133_grad/truediv, gradients_1/Mean_134_grad/truediv, gradients_1/Mean_135_grad/truediv, gradients_1/Mean_136_grad/truediv, gradients_1/Mean_137_grad/truediv, gradients_1/Mean_138_grad/truediv, gradients_1/Mean_139_grad/truediv, gradients_1/Mean_140_grad/truediv, gradients_1/Mean_141_grad/truediv, gradients_1/Mean_142_grad/truediv, gradients_1/Mean_143_grad/truediv, gradients_1/Mean_144_grad/truediv, gradients_1/Mean_145_grad/truediv, gradients_1/Mean_146_grad/truediv, gradients_1/Mean_147_grad/truediv, gradients_1/Mean_148_grad/truediv, gradients_1/Mean_149_grad/truediv, gradients_1/Mean_150_grad/truediv, gradients_1/Mean_151_grad/truediv, gradients_1/Mean_152_grad/truediv, gradients_1/Mean_153_grad/truediv, gradients_1/Mean_154_grad/truediv, gradients_1/Mean_155_grad/truediv, gradients_1/Mean_156_grad/truediv, gradients_1/Mean_157_grad/truediv, gradients_1/Mean_158_grad/truediv, gradients_1/Mean_159_grad/truediv, gradients_1/Mean_160_grad/truediv, gradients_1/Mean_161_grad/truediv, gradients_1/Mean_162_grad/truediv, gradients_1/Mean_163_grad/truediv, gradients_1/Mean_164_grad/truediv, gradients_1/Mean_165_grad/truediv, gradients_1/Mean_166_grad/truediv, gradients_1/Mean_167_grad/truediv, gradients_1/Mean_168_grad/truediv, gradients_1/Mean_169_grad/truediv, gradients_1/Mean_170_grad/truediv, gradients_1/Mean_171_grad/truediv, gradients_1/Mean_172_grad/truediv, gradients_1/Mean_173_grad/truediv, gradients_1/Mean_174_grad/truediv, gradients_1/Mean_175_grad/truediv, gradients_1/Mean_176_grad/truediv, gradients_1/Mean_177_grad/truediv, gradients_1/Mean_178_grad/truediv, gradients_1/Mean_179_grad/truediv, gradients_1/Mean_180_grad/truediv, gradients_1/Mean_181_grad/truediv, gradients_1/Mean_182_grad/truediv, gradients_1/Mean_183_grad/truediv, gradients_1/Mean_184_grad/truediv, gradients_1/Mean_185_grad/truediv, gradients_1/Mean_186_grad/truediv, gradients_1/Mean_187_grad/truediv, gradients_1/Mean_188_grad/truediv, gradients_1/Mean_189_grad/truediv, gradients_1/Mean_190_grad/truediv, gradients_1/Mean_191_grad/truediv, gradients_1/Mean_192_grad/truediv, gradients_1/Mean_193_grad/truediv, gradients_1/Mean_194_grad/truediv, gradients_1/Mean_195_grad/truediv, gradients_1/Mean_196_grad/truediv, gradients_1/Mean_197_grad/truediv, gradients_1/Mean_198_grad/truediv, gradients_1/Mean_199_grad/truediv, gradients_1/Mean_200_grad/truediv, gradients_1/Mean_201_grad/truediv, gradients_1/Mean_202_grad/truediv, gradients_1/Mean_203_grad/truediv, gradients_1/Mean_204_grad/truediv, gradients_1/Mean_205_grad/truediv, gradients_1/Mean_206_grad/truediv, gradients_1/Mean_207_grad/truediv, gradients_1/Mean_208_grad/truediv, gradients_1/Mean_209_grad/truediv, gradients_1/Mean_210_grad/truediv, gradients_1/Mean_211_grad/truediv, gradients_1/Mean_212_grad/truediv, gradients_1/Mean_213_grad/truediv, gradients_1/Mean_214_grad/truediv, gradients_1/Mean_215_grad/truediv, gradients_1/Mean_216_grad/truediv, gradients_1/Mean_217_grad/truediv, gradients_1/Mean_218_grad/truediv, gradients_1/Mean_219_grad/truediv, gradients_1/Mean_220_grad/truediv, gradients_1/Mean_221_grad/truediv, gradients_1/Mean_222_grad/truediv, gradients_1/Mean_223_grad/truediv, gradients_1/Mean_224_grad/truediv, gradients_1/Mean_225_grad/truediv, gradients_1/Mean_226_grad/truediv, gradients_1/Mean_227_grad/truediv, gradients_1/Mean_228_grad/truediv, gradients_1/Mean_229_grad/truediv, gradients_1/Mean_230_grad/truediv, gradients_1/Mean_231_grad/truediv, gradients_1/Mean_232_grad/truediv, gradients_1/Mean_233_grad/truediv, gradients_1/Mean_234_grad/truediv, gradients_1/Mean_235_grad/truediv, gradients_1/Mean_236_grad/truediv, gradients_1/Mean_237_grad/truediv, gradients_1/Mean_238_grad/truediv, gradients_1/Mean_239_grad/truediv, gradients_1/Mean_240_grad/truediv, gradients_1/Mean_241_grad/truediv, gradients_1/Mean_242_grad/truediv, gradients_1/Mean_243_grad/truediv, gradients_1/Mean_244_grad/truediv, gradients_1/Mean_245_grad/truediv, gradients_1/Mean_246_grad/truediv, gradients_1/Mean_247_grad/truediv, gradients_1/Mean_248_grad/truediv, gradients_1/Mean_249_grad/truediv, gradients_1/Mean_250_grad/truediv, gradients_1/Mean_251_grad/truediv, gradients_1/Mean_252_grad/truediv, gradients_1/Mean_253_grad/truediv, gradients_1/Mean_254_grad/truediv, gradients_1/Mean_255_grad/truediv)]]\r\n\r\nCaused by op 'gradients_1/DynamicPartition_grad/DynamicStitch', defined at:\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 55, in <module>\r\n    main(session)\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 34, in main\r\n    adam_train_op = tf.train.AdamOptimizer().minimize(loss=loss)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 355, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_grad.py\", line 42, in _DynamicPartitionGrads\r\n    reconstructed = data_flow_ops.dynamic_stitch(partitioned_indices, grads)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 683, in dynamic_stitch\r\n    \"DynamicStitch\", indices=indices, data=data, name=name)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'DynamicPartition', defined at:\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 55, in <module>\r\n    main(session)\r\n  File \"/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py\", line 22, in main\r\n    activated_par = tf.dynamic_partition(data, partitions, num_partitions)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 573, in dynamic_partition\r\n    num_partitions=num_partitions, name=name)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): data[255].shape = [41,1] does not start with indices[255].shape = [28]\r\n\t [[Node: gradients_1/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_1/DynamicPartition_grad/DynamicPartition/_4625, gradients_1/DynamicPartition_grad/DynamicPartition/_4627, gradients_1/DynamicPartition_grad/DynamicPartition/_4629, gradients_1/DynamicPartition_grad/DynamicPartition/_4631, gradients_1/DynamicPartition_grad/DynamicPartition/_4633, gradients_1/DynamicPartition_grad/DynamicPartition/_4635, gradients_1/DynamicPartition_grad/DynamicPartition/_4637, gradients_1/DynamicPartition_grad/DynamicPartition/_4639, gradients_1/DynamicPartition_grad/DynamicPartition/_4641, gradients_1/DynamicPartition_grad/DynamicPartition/_4643, gradients_1/DynamicPartition_grad/DynamicPartition/_4645, gradients_1/DynamicPartition_grad/DynamicPartition/_4647, gradients_1/DynamicPartition_grad/DynamicPartition/_4649, gradients_1/DynamicPartition_grad/DynamicPartition/_4651, gradients_1/DynamicPartition_grad/DynamicPartition/_4653, gradients_1/DynamicPartition_grad/DynamicPartition/_4655, gradients_1/DynamicPartition_grad/DynamicPartition/_4657, gradients_1/DynamicPartition_grad/DynamicPartition/_4659, gradients_1/DynamicPartition_grad/DynamicPartition/_4661, gradients_1/DynamicPartition_grad/DynamicPartition/_4663, gradients_1/DynamicPartition_grad/DynamicPartition/_4665, gradients_1/DynamicPartition_grad/DynamicPartition/_4667, gradients_1/DynamicPartition_grad/DynamicPartition/_4669, gradients_1/DynamicPartition_grad/DynamicPartition/_4671, gradients_1/DynamicPartition_grad/DynamicPartition/_4673, gradients_1/DynamicPartition_grad/DynamicPartition/_4675, gradients_1/DynamicPartition_grad/DynamicPartition/_4677, gradients_1/DynamicPartition_grad/DynamicPartition/_4679, gradients_1/DynamicPartition_grad/DynamicPartition/_4681, gradients_1/DynamicPartition_grad/DynamicPartition/_4683, gradients_1/DynamicPartition_grad/DynamicPartition/_4685, gradients_1/DynamicPartition_grad/DynamicPartition/_4687, gradients_1/DynamicPartition_grad/DynamicPartition/_4689, gradients_1/DynamicPartition_grad/DynamicPartition/_4691, gradients_1/DynamicPartition_grad/DynamicPartition/_4693, gradients_1/DynamicPartition_grad/DynamicPartition/_4695, gradients_1/DynamicPartition_grad/DynamicPartition/_4697, gradients_1/DynamicPartition_grad/DynamicPartition/_4699, gradients_1/DynamicPartition_grad/DynamicPartition/_4701, gradients_1/DynamicPartition_grad/DynamicPartition/_4703, gradients_1/DynamicPartition_grad/DynamicPartition/_4705, gradients_1/DynamicPartition_grad/DynamicPartition/_4707, gradients_1/DynamicPartition_grad/DynamicPartition/_4709, gradients_1/DynamicPartition_grad/DynamicPartition/_4711, gradients_1/DynamicPartition_grad/DynamicPartition/_4713, gradients_1/DynamicPartition_grad/DynamicPartition/_4715, gradients_1/DynamicPartition_grad/DynamicPartition/_4717, gradients_1/DynamicPartition_grad/DynamicPartition/_4719, gradients_1/DynamicPartition_grad/DynamicPartition/_4721, gradients_1/DynamicPartition_grad/DynamicPartition/_4723, gradients_1/DynamicPartition_grad/DynamicPartition/_4725, gradients_1/DynamicPartition_grad/DynamicPartition/_4727, gradients_1/DynamicPartition_grad/DynamicPartition/_4729, gradients_1/DynamicPartition_grad/DynamicPartition/_4731, gradients_1/DynamicPartition_grad/DynamicPartition/_4733, gradients_1/DynamicPartition_grad/DynamicPartition/_4735, gradients_1/DynamicPartition_grad/DynamicPartition/_4737, gradients_1/DynamicPartition_grad/DynamicPartition/_4739, gradients_1/DynamicPartition_grad/DynamicPartition/_4741, gradients_1/DynamicPartition_grad/DynamicPartition/_4743, gradients_1/DynamicPartition_grad/DynamicPartition/_4745, gradients_1/DynamicPartition_grad/DynamicPartition/_4747, gradients_1/DynamicPartition_grad/DynamicPartition/_4749, gradients_1/DynamicPartition_grad/DynamicPartition/_4751, gradients_1/DynamicPartition_grad/DynamicPartition/_4753, gradients_1/DynamicPartition_grad/DynamicPartition/_4755, gradients_1/DynamicPartition_grad/DynamicPartition/_4757, gradients_1/DynamicPartition_grad/DynamicPartition/_4759, gradients_1/DynamicPartition_grad/DynamicPartition/_4761, gradients_1/DynamicPartition_grad/DynamicPartition/_4763, gradients_1/DynamicPartition_grad/DynamicPartition/_4765, gradients_1/DynamicPartition_grad/DynamicPartition/_4767, gradients_1/DynamicPartition_grad/DynamicPartition/_4769, gradients_1/DynamicPartition_grad/DynamicPartition/_4771, gradients_1/DynamicPartition_grad/DynamicPartition/_4773, gradients_1/DynamicPartition_grad/DynamicPartition/_4775, gradients_1/DynamicPartition_grad/DynamicPartition/_4777, gradients_1/DynamicPartition_grad/DynamicPartition/_4779, gradients_1/DynamicPartition_grad/DynamicPartition/_4781, gradients_1/DynamicPartition_grad/DynamicPartition/_4783, gradients_1/DynamicPartition_grad/DynamicPartition/_4785, gradients_1/DynamicPartition_grad/DynamicPartition/_4787, gradients_1/DynamicPartition_grad/DynamicPartition/_4789, gradients_1/DynamicPartition_grad/DynamicPartition/_4791, gradients_1/DynamicPartition_grad/DynamicPartition/_4793, gradients_1/DynamicPartition_grad/DynamicPartition/_4795, gradients_1/DynamicPartition_grad/DynamicPartition/_4797, gradients_1/DynamicPartition_grad/DynamicPartition/_4799, gradients_1/DynamicPartition_grad/DynamicPartition/_4801, gradients_1/DynamicPartition_grad/DynamicPartition/_4803, gradients_1/DynamicPartition_grad/DynamicPartition/_4805, gradients_1/DynamicPartition_grad/DynamicPartition/_4807, gradients_1/DynamicPartition_grad/DynamicPartition/_4809, gradients_1/DynamicPartition_grad/DynamicPartition/_4811, gradients_1/DynamicPartition_grad/DynamicPartition/_4813, gradients_1/DynamicPartition_grad/DynamicPartition/_4815, gradients_1/DynamicPartition_grad/DynamicPartition/_4817, gradients_1/DynamicPartition_grad/DynamicPartition/_4819, gradients_1/DynamicPartition_grad/DynamicPartition/_4821, gradients_1/DynamicPartition_grad/DynamicPartition/_4823, gradients_1/DynamicPartition_grad/DynamicPartition/_4825, gradients_1/DynamicPartition_grad/DynamicPartition/_4827, gradients_1/DynamicPartition_grad/DynamicPartition/_4829, gradients_1/DynamicPartition_grad/DynamicPartition/_4831, gradients_1/DynamicPartition_grad/DynamicPartition/_4833, gradients_1/DynamicPartition_grad/DynamicPartition/_4835, gradients_1/DynamicPartition_grad/DynamicPartition/_4837, gradients_1/DynamicPartition_grad/DynamicPartition/_4839, gradients_1/DynamicPartition_grad/DynamicPartition/_4841, gradients_1/DynamicPartition_grad/DynamicPartition/_4843, gradients_1/DynamicPartition_grad/DynamicPartition/_4845, gradients_1/DynamicPartition_grad/DynamicPartition/_4847, gradients_1/DynamicPartition_grad/DynamicPartition/_4849, gradients_1/DynamicPartition_grad/DynamicPartition/_4851, gradients_1/DynamicPartition_grad/DynamicPartition/_4853, gradients_1/DynamicPartition_grad/DynamicPartition/_4855, gradients_1/DynamicPartition_grad/DynamicPartition/_4857, gradients_1/DynamicPartition_grad/DynamicPartition/_4859, gradients_1/DynamicPartition_grad/DynamicPartition/_4861, gradients_1/DynamicPartition_grad/DynamicPartition/_4863, gradients_1/DynamicPartition_grad/DynamicPartition/_4865, gradients_1/DynamicPartition_grad/DynamicPartition/_4867, gradients_1/DynamicPartition_grad/DynamicPartition/_4869, gradients_1/DynamicPartition_grad/DynamicPartition/_4871, gradients_1/DynamicPartition_grad/DynamicPartition/_4873, gradients_1/DynamicPartition_grad/DynamicPartition/_4875, gradients_1/DynamicPartition_grad/DynamicPartition/_4877, gradients_1/DynamicPartition_grad/DynamicPartition/_4879, gradients_1/DynamicPartition_grad/DynamicPartition/_4881, gradients_1/DynamicPartition_grad/DynamicPartition/_4883, gradients_1/DynamicPartition_grad/DynamicPartition/_4885, gradients_1/DynamicPartition_grad/DynamicPartition/_4887, gradients_1/DynamicPartition_grad/DynamicPartition/_4889, gradients_1/DynamicPartition_grad/DynamicPartition/_4891, gradients_1/DynamicPartition_grad/DynamicPartition/_4893, gradients_1/DynamicPartition_grad/DynamicPartition/_4895, gradients_1/DynamicPartition_grad/DynamicPartition/_4897, gradients_1/DynamicPartition_grad/DynamicPartition/_4899, gradients_1/DynamicPartition_grad/DynamicPartition/_4901, gradients_1/DynamicPartition_grad/DynamicPartition/_4903, gradients_1/DynamicPartition_grad/DynamicPartition/_4905, gradients_1/DynamicPartition_grad/DynamicPartition/_4907, gradients_1/DynamicPartition_grad/DynamicPartition/_4909, gradients_1/DynamicPartition_grad/DynamicPartition/_4911, gradients_1/DynamicPartition_grad/DynamicPartition/_4913, gradients_1/DynamicPartition_grad/DynamicPartition/_4915, gradients_1/DynamicPartition_grad/DynamicPartition/_4917, gradients_1/DynamicPartition_grad/DynamicPartition/_4919, gradients_1/DynamicPartition_grad/DynamicPartition/_4921, gradients_1/DynamicPartition_grad/DynamicPartition/_4923, gradients_1/DynamicPartition_grad/DynamicPartition/_4925, gradients_1/DynamicPartition_grad/DynamicPartition/_4927, gradients_1/DynamicPartition_grad/DynamicPartition/_4929, gradients_1/DynamicPartition_grad/DynamicPartition/_4931, gradients_1/DynamicPartition_grad/DynamicPartition/_4933, gradients_1/DynamicPartition_grad/DynamicPartition/_4935, gradients_1/DynamicPartition_grad/DynamicPartition/_4937, gradients_1/DynamicPartition_grad/DynamicPartition/_4939, gradients_1/DynamicPartition_grad/DynamicPartition/_4941, gradients_1/DynamicPartition_grad/DynamicPartition/_4943, gradients_1/DynamicPartition_grad/DynamicPartition/_4945, gradients_1/DynamicPartition_grad/DynamicPartition/_4947, gradients_1/DynamicPartition_grad/DynamicPartition/_4949, gradients_1/DynamicPartition_grad/DynamicPartition/_4951, gradients_1/DynamicPartition_grad/DynamicPartition/_4953, gradients_1/DynamicPartition_grad/DynamicPartition/_4955, gradients_1/DynamicPartition_grad/DynamicPartition/_4957, gradients_1/DynamicPartition_grad/DynamicPartition/_4959, gradients_1/DynamicPartition_grad/DynamicPartition/_4961, gradients_1/DynamicPartition_grad/DynamicPartition/_4963, gradients_1/DynamicPartition_grad/DynamicPartition/_4965, gradients_1/DynamicPartition_grad/DynamicPartition/_4967, gradients_1/DynamicPartition_grad/DynamicPartition/_4969, gradients_1/DynamicPartition_grad/DynamicPartition/_4971, gradients_1/DynamicPartition_grad/DynamicPartition/_4973, gradients_1/DynamicPartition_grad/DynamicPartition/_4975, gradients_1/DynamicPartition_grad/DynamicPartition/_4977, gradients_1/DynamicPartition_grad/DynamicPartition/_4979, gradients_1/DynamicPartition_grad/DynamicPartition/_4981, gradients_1/DynamicPartition_grad/DynamicPartition/_4983, gradients_1/DynamicPartition_grad/DynamicPartition/_4985, gradients_1/DynamicPartition_grad/DynamicPartition/_4987, gradients_1/DynamicPartition_grad/DynamicPartition/_4989, gradients_1/DynamicPartition_grad/DynamicPartition/_4991, gradients_1/DynamicPartition_grad/DynamicPartition/_4993, gradients_1/DynamicPartition_grad/DynamicPartition/_4995, gradients_1/DynamicPartition_grad/DynamicPartition/_4997, gradients_1/DynamicPartition_grad/DynamicPartition/_4999, gradients_1/DynamicPartition_grad/DynamicPartition/_5001, gradients_1/DynamicPartition_grad/DynamicPartition/_5003, gradients_1/DynamicPartition_grad/DynamicPartition/_5005, gradients_1/DynamicPartition_grad/DynamicPartition/_5007, gradients_1/DynamicPartition_grad/DynamicPartition/_5009, gradients_1/DynamicPartition_grad/DynamicPartition/_5011, gradients_1/DynamicPartition_grad/DynamicPartition/_5013, gradients_1/DynamicPartition_grad/DynamicPartition/_5015, gradients_1/DynamicPartition_grad/DynamicPartition/_5017, gradients_1/DynamicPartition_grad/DynamicPartition/_5019, gradients_1/DynamicPartition_grad/DynamicPartition/_5021, gradients_1/DynamicPartition_grad/DynamicPartition/_5023, gradients_1/DynamicPartition_grad/DynamicPartition/_5025, gradients_1/DynamicPartition_grad/DynamicPartition/_5027, gradients_1/DynamicPartition_grad/DynamicPartition/_5029, gradients_1/DynamicPartition_grad/DynamicPartition/_5031, gradients_1/DynamicPartition_grad/DynamicPartition/_5033, gradients_1/DynamicPartition_grad/DynamicPartition/_5035, gradients_1/DynamicPartition_grad/DynamicPartition/_5037, gradients_1/DynamicPartition_grad/DynamicPartition/_5039, gradients_1/DynamicPartition_grad/DynamicPartition/_5041, gradients_1/DynamicPartition_grad/DynamicPartition/_5043, gradients_1/DynamicPartition_grad/DynamicPartition/_5045, gradients_1/DynamicPartition_grad/DynamicPartition/_5047, gradients_1/DynamicPartition_grad/DynamicPartition/_5049, gradients_1/DynamicPartition_grad/DynamicPartition/_5051, gradients_1/DynamicPartition_grad/DynamicPartition/_5053, gradients_1/DynamicPartition_grad/DynamicPartition/_5055, gradients_1/DynamicPartition_grad/DynamicPartition/_5057, gradients_1/DynamicPartition_grad/DynamicPartition/_5059, gradients_1/DynamicPartition_grad/DynamicPartition/_5061, gradients_1/DynamicPartition_grad/DynamicPartition/_5063, gradients_1/DynamicPartition_grad/DynamicPartition/_5065, gradients_1/DynamicPartition_grad/DynamicPartition/_5067, gradients_1/DynamicPartition_grad/DynamicPartition/_5069, gradients_1/DynamicPartition_grad/DynamicPartition/_5071, gradients_1/DynamicPartition_grad/DynamicPartition/_5073, gradients_1/DynamicPartition_grad/DynamicPartition/_5075, gradients_1/DynamicPartition_grad/DynamicPartition/_5077, gradients_1/DynamicPartition_grad/DynamicPartition/_5079, gradients_1/DynamicPartition_grad/DynamicPartition/_5081, gradients_1/DynamicPartition_grad/DynamicPartition/_5083, gradients_1/DynamicPartition_grad/DynamicPartition/_5085, gradients_1/DynamicPartition_grad/DynamicPartition/_5087, gradients_1/DynamicPartition_grad/DynamicPartition/_5089, gradients_1/DynamicPartition_grad/DynamicPartition/_5091, gradients_1/DynamicPartition_grad/DynamicPartition/_5093, gradients_1/DynamicPartition_grad/DynamicPartition/_5095, gradients_1/DynamicPartition_grad/DynamicPartition/_5097, gradients_1/DynamicPartition_grad/DynamicPartition/_5099, gradients_1/DynamicPartition_grad/DynamicPartition/_5101, gradients_1/DynamicPartition_grad/DynamicPartition/_5103, gradients_1/DynamicPartition_grad/DynamicPartition/_5105, gradients_1/DynamicPartition_grad/DynamicPartition/_5107, gradients_1/DynamicPartition_grad/DynamicPartition/_5109, gradients_1/DynamicPartition_grad/DynamicPartition/_5111, gradients_1/DynamicPartition_grad/DynamicPartition/_5113, gradients_1/DynamicPartition_grad/DynamicPartition/_5115, gradients_1/DynamicPartition_grad/DynamicPartition/_5117, gradients_1/DynamicPartition_grad/DynamicPartition/_5119, gradients_1/DynamicPartition_grad/DynamicPartition/_5121, gradients_1/DynamicPartition_grad/DynamicPartition/_5123, gradients_1/DynamicPartition_grad/DynamicPartition/_5125, gradients_1/DynamicPartition_grad/DynamicPartition/_5127, gradients_1/DynamicPartition_grad/DynamicPartition/_5129, gradients_1/DynamicPartition_grad/DynamicPartition/_5131, gradients_1/DynamicPartition_grad/DynamicPartition/_5133, gradients_1/DynamicPartition_grad/DynamicPartition/_5135, gradients_1/Mean_grad/truediv, gradients_1/Mean_1_grad/truediv, gradients_1/Mean_2_grad/truediv, gradients_1/Mean_3_grad/truediv, gradients_1/Mean_4_grad/truediv, gradients_1/Mean_5_grad/truediv, gradients_1/Mean_6_grad/truediv, gradients_1/Mean_7_grad/truediv, gradients_1/Mean_8_grad/truediv, gradients_1/Mean_9_grad/truediv, gradients_1/Mean_10_grad/truediv, gradients_1/Mean_11_grad/truediv, gradients_1/Mean_12_grad/truediv, gradients_1/Mean_13_grad/truediv, gradients_1/Mean_14_grad/truediv, gradients_1/Mean_15_grad/truediv, gradients_1/Mean_16_grad/truediv, gradients_1/Mean_17_grad/truediv, gradients_1/Mean_18_grad/truediv, gradients_1/Mean_19_grad/truediv, gradients_1/Mean_20_grad/truediv, gradients_1/Mean_21_grad/truediv, gradients_1/Mean_22_grad/truediv, gradients_1/Mean_23_grad/truediv, gradients_1/Mean_24_grad/truediv, gradients_1/Mean_25_grad/truediv, gradients_1/Mean_26_grad/truediv, gradients_1/Mean_27_grad/truediv, gradients_1/Mean_28_grad/truediv, gradients_1/Mean_29_grad/truediv, gradients_1/Mean_30_grad/truediv, gradients_1/Mean_31_grad/truediv, gradients_1/Mean_32_grad/truediv, gradients_1/Mean_33_grad/truediv, gradients_1/Mean_34_grad/truediv, gradients_1/Mean_35_grad/truediv, gradients_1/Mean_36_grad/truediv, gradients_1/Mean_37_grad/truediv, gradients_1/Mean_38_grad/truediv, gradients_1/Mean_39_grad/truediv, gradients_1/Mean_40_grad/truediv, gradients_1/Mean_41_grad/truediv, gradients_1/Mean_42_grad/truediv, gradients_1/Mean_43_grad/truediv, gradients_1/Mean_44_grad/truediv, gradients_1/Mean_45_grad/truediv, gradients_1/Mean_46_grad/truediv, gradients_1/Mean_47_grad/truediv, gradients_1/Mean_48_grad/truediv, gradients_1/Mean_49_grad/truediv, gradients_1/Mean_50_grad/truediv, gradients_1/Mean_51_grad/truediv, gradients_1/Mean_52_grad/truediv, gradients_1/Mean_53_grad/truediv, gradients_1/Mean_54_grad/truediv, gradients_1/Mean_55_grad/truediv, gradients_1/Mean_56_grad/truediv, gradients_1/Mean_57_grad/truediv, gradients_1/Mean_58_grad/truediv, gradients_1/Mean_59_grad/truediv, gradients_1/Mean_60_grad/truediv, gradients_1/Mean_61_grad/truediv, gradients_1/Mean_62_grad/truediv, gradients_1/Mean_63_grad/truediv, gradients_1/Mean_64_grad/truediv, gradients_1/Mean_65_grad/truediv, gradients_1/Mean_66_grad/truediv, gradients_1/Mean_67_grad/truediv, gradients_1/Mean_68_grad/truediv, gradients_1/Mean_69_grad/truediv, gradients_1/Mean_70_grad/truediv, gradients_1/Mean_71_grad/truediv, gradients_1/Mean_72_grad/truediv, gradients_1/Mean_73_grad/truediv, gradients_1/Mean_74_grad/truediv, gradients_1/Mean_75_grad/truediv, gradients_1/Mean_76_grad/truediv, gradients_1/Mean_77_grad/truediv, gradients_1/Mean_78_grad/truediv, gradients_1/Mean_79_grad/truediv, gradients_1/Mean_80_grad/truediv, gradients_1/Mean_81_grad/truediv, gradients_1/Mean_82_grad/truediv, gradients_1/Mean_83_grad/truediv, gradients_1/Mean_84_grad/truediv, gradients_1/Mean_85_grad/truediv, gradients_1/Mean_86_grad/truediv, gradients_1/Mean_87_grad/truediv, gradients_1/Mean_88_grad/truediv, gradients_1/Mean_89_grad/truediv, gradients_1/Mean_90_grad/truediv, gradients_1/Mean_91_grad/truediv, gradients_1/Mean_92_grad/truediv, gradients_1/Mean_93_grad/truediv, gradients_1/Mean_94_grad/truediv, gradients_1/Mean_95_grad/truediv, gradients_1/Mean_96_grad/truediv, gradients_1/Mean_97_grad/truediv, gradients_1/Mean_98_grad/truediv, gradients_1/Mean_99_grad/truediv, gradients_1/Mean_100_grad/truediv, gradients_1/Mean_101_grad/truediv, gradients_1/Mean_102_grad/truediv, gradients_1/Mean_103_grad/truediv, gradients_1/Mean_104_grad/truediv, gradients_1/Mean_105_grad/truediv, gradients_1/Mean_106_grad/truediv, gradients_1/Mean_107_grad/truediv, gradients_1/Mean_108_grad/truediv, gradients_1/Mean_109_grad/truediv, gradients_1/Mean_110_grad/truediv, gradients_1/Mean_111_grad/truediv, gradients_1/Mean_112_grad/truediv, gradients_1/Mean_113_grad/truediv, gradients_1/Mean_114_grad/truediv, gradients_1/Mean_115_grad/truediv, gradients_1/Mean_116_grad/truediv, gradients_1/Mean_117_grad/truediv, gradients_1/Mean_118_grad/truediv, gradients_1/Mean_119_grad/truediv, gradients_1/Mean_120_grad/truediv, gradients_1/Mean_121_grad/truediv, gradients_1/Mean_122_grad/truediv, gradients_1/Mean_123_grad/truediv, gradients_1/Mean_124_grad/truediv, gradients_1/Mean_125_grad/truediv, gradients_1/Mean_126_grad/truediv, gradients_1/Mean_127_grad/truediv, gradients_1/Mean_128_grad/truediv, gradients_1/Mean_129_grad/truediv, gradients_1/Mean_130_grad/truediv, gradients_1/Mean_131_grad/truediv, gradients_1/Mean_132_grad/truediv, gradients_1/Mean_133_grad/truediv, gradients_1/Mean_134_grad/truediv, gradients_1/Mean_135_grad/truediv, gradients_1/Mean_136_grad/truediv, gradients_1/Mean_137_grad/truediv, gradients_1/Mean_138_grad/truediv, gradients_1/Mean_139_grad/truediv, gradients_1/Mean_140_grad/truediv, gradients_1/Mean_141_grad/truediv, gradients_1/Mean_142_grad/truediv, gradients_1/Mean_143_grad/truediv, gradients_1/Mean_144_grad/truediv, gradients_1/Mean_145_grad/truediv, gradients_1/Mean_146_grad/truediv, gradients_1/Mean_147_grad/truediv, gradients_1/Mean_148_grad/truediv, gradients_1/Mean_149_grad/truediv, gradients_1/Mean_150_grad/truediv, gradients_1/Mean_151_grad/truediv, gradients_1/Mean_152_grad/truediv, gradients_1/Mean_153_grad/truediv, gradients_1/Mean_154_grad/truediv, gradients_1/Mean_155_grad/truediv, gradients_1/Mean_156_grad/truediv, gradients_1/Mean_157_grad/truediv, gradients_1/Mean_158_grad/truediv, gradients_1/Mean_159_grad/truediv, gradients_1/Mean_160_grad/truediv, gradients_1/Mean_161_grad/truediv, gradients_1/Mean_162_grad/truediv, gradients_1/Mean_163_grad/truediv, gradients_1/Mean_164_grad/truediv, gradients_1/Mean_165_grad/truediv, gradients_1/Mean_166_grad/truediv, gradients_1/Mean_167_grad/truediv, gradients_1/Mean_168_grad/truediv, gradients_1/Mean_169_grad/truediv, gradients_1/Mean_170_grad/truediv, gradients_1/Mean_171_grad/truediv, gradients_1/Mean_172_grad/truediv, gradients_1/Mean_173_grad/truediv, gradients_1/Mean_174_grad/truediv, gradients_1/Mean_175_grad/truediv, gradients_1/Mean_176_grad/truediv, gradients_1/Mean_177_grad/truediv, gradients_1/Mean_178_grad/truediv, gradients_1/Mean_179_grad/truediv, gradients_1/Mean_180_grad/truediv, gradients_1/Mean_181_grad/truediv, gradients_1/Mean_182_grad/truediv, gradients_1/Mean_183_grad/truediv, gradients_1/Mean_184_grad/truediv, gradients_1/Mean_185_grad/truediv, gradients_1/Mean_186_grad/truediv, gradients_1/Mean_187_grad/truediv, gradients_1/Mean_188_grad/truediv, gradients_1/Mean_189_grad/truediv, gradients_1/Mean_190_grad/truediv, gradients_1/Mean_191_grad/truediv, gradients_1/Mean_192_grad/truediv, gradients_1/Mean_193_grad/truediv, gradients_1/Mean_194_grad/truediv, gradients_1/Mean_195_grad/truediv, gradients_1/Mean_196_grad/truediv, gradients_1/Mean_197_grad/truediv, gradients_1/Mean_198_grad/truediv, gradients_1/Mean_199_grad/truediv, gradients_1/Mean_200_grad/truediv, gradients_1/Mean_201_grad/truediv, gradients_1/Mean_202_grad/truediv, gradients_1/Mean_203_grad/truediv, gradients_1/Mean_204_grad/truediv, gradients_1/Mean_205_grad/truediv, gradients_1/Mean_206_grad/truediv, gradients_1/Mean_207_grad/truediv, gradients_1/Mean_208_grad/truediv, gradients_1/Mean_209_grad/truediv, gradients_1/Mean_210_grad/truediv, gradients_1/Mean_211_grad/truediv, gradients_1/Mean_212_grad/truediv, gradients_1/Mean_213_grad/truediv, gradients_1/Mean_214_grad/truediv, gradients_1/Mean_215_grad/truediv, gradients_1/Mean_216_grad/truediv, gradients_1/Mean_217_grad/truediv, gradients_1/Mean_218_grad/truediv, gradients_1/Mean_219_grad/truediv, gradients_1/Mean_220_grad/truediv, gradients_1/Mean_221_grad/truediv, gradients_1/Mean_222_grad/truediv, gradients_1/Mean_223_grad/truediv, gradients_1/Mean_224_grad/truediv, gradients_1/Mean_225_grad/truediv, gradients_1/Mean_226_grad/truediv, gradients_1/Mean_227_grad/truediv, gradients_1/Mean_228_grad/truediv, gradients_1/Mean_229_grad/truediv, gradients_1/Mean_230_grad/truediv, gradients_1/Mean_231_grad/truediv, gradients_1/Mean_232_grad/truediv, gradients_1/Mean_233_grad/truediv, gradients_1/Mean_234_grad/truediv, gradients_1/Mean_235_grad/truediv, gradients_1/Mean_236_grad/truediv, gradients_1/Mean_237_grad/truediv, gradients_1/Mean_238_grad/truediv, gradients_1/Mean_239_grad/truediv, gradients_1/Mean_240_grad/truediv, gradients_1/Mean_241_grad/truediv, gradients_1/Mean_242_grad/truediv, gradients_1/Mean_243_grad/truediv, gradients_1/Mean_244_grad/truediv, gradients_1/Mean_245_grad/truediv, gradients_1/Mean_246_grad/truediv, gradients_1/Mean_247_grad/truediv, gradients_1/Mean_248_grad/truediv, gradients_1/Mean_249_grad/truediv, gradients_1/Mean_250_grad/truediv, gradients_1/Mean_251_grad/truediv, gradients_1/Mean_252_grad/truediv, gradients_1/Mean_253_grad/truediv, gradients_1/Mean_254_grad/truediv, gradients_1/Mean_255_grad/truediv)]]\r\n```\r\nNote that indices[255].shape == 28 is correct given the partition list.\r\n", "comments": ["@rmlarsen did you touch this code last?", "This should be fixed by the upgrade to cub 1.8.0", "unassign", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!"]}]