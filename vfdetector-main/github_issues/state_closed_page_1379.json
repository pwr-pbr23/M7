[{"number": 11697, "title": "[OpenCL] Extends matmul_benchmark.py to cover SYCL", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "`googletest.gpu_device_name()` returns _/gpu:0_ but `self._StripGraph(gd)` returns proto that contains _/device:GPU:0_ \r\nThere is inconsistency across the tests - both _/gpu:0_ and _/device:GPU:0_ are used. \r\nIs there intention to unifie them in the future?", "If you want to send a CL to change gpu_device_name to return /device:GPU:0, that would be great!", "Thanks for the pull request @lukeiwanski !\r\nGiven that you have started working on this, would you also consider resolving the check failure as @vrv suggested? Thank you!", "I am looking into this - It might be quite big changeset. Everything points to gpu device creation (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L591) as _gpu_device_name_ (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L61) returns an element from _list_local_devices()_ that trails back to the _ListDevices_  (a list of the devices registered in the session)\r\n\r\nThe dev branch is here: https://github.com/lukeiwanski/tensorflow/tree/dev/gpu_device\r\n", "You might be able to get away with renaming that to '/device:GPU:0' -- I can try this today too and see what breaks.  We've been trying to make sure we canonicalize the device name to the 'new style names' whenever needed, but it's possible there are some places that aren't fixed.", "@vrv I had to make changes in few places :)\r\n\r\nCould you guys have a look if the changes are reasonable?\r\n\r\n", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "There are few tests failing eg: \r\n```\r\n//tensorflow/core/profiler/internal:tfprof_show_test\r\n//tensorflow/core/profiler/internal:tfprof_stats_test\r\n//tensorflow/core/profiler/internal:tfprof_timeline_test\r\n```\r\nThey all reading graph data from ```tensorflow/core/profiler/internal/testdata``` that have gpu in the file instead device:GPU not sure how to approach this.", "@lukeiwanski thanks for updating the datafile! \r\n@tensorflow-jenkins test this please", "@lukeiwanski it looks like there are still some changes in the profiler tests.\r\n@panyx0718 could you help Luke sort this out?", "quote \"googletest.gpu_device_name() returns /gpu:0 but self._StripGraph(gd) returns proto that contains /device:GPU:0\"\r\nwhat's the reason to faover \"/device:GPU:0\" over \"/gpu:0\"? It seems the \"device:\" is redundant and avoiding it allows to keep backward compatibility?", "If you want to fix the tfprof tests, see here.\r\nhttps://github.com/tensorflow/tensorflow/blob/92111fdd1a9baa113264149413105f700f9ee310/tensorflow/core/profiler/internal/tfprof_node.cc#L28\r\n\r\nThere might be one or more places to fix. profiler depends on the device name format to correctly analyze metrics of each operation. Hence, those tests should be kept correct. One solution to keep backward compatibility with old RunMetadata is to support all device naming schemes, instead of just replacing them.", "@panyx0718 the reason for \r\n> what's the reason to faover \"/device:GPU:0\" over \"/gpu:0\"? It seems the \"device:\" is redundant and avoiding it allows to keep backward compatibility?\r\n\r\nsee @vrv comment here: https://github.com/tensorflow/tensorflow/pull/11697#issuecomment-317800870 \r\nIt seems that /device:XXX:x is desired way to report supported devices.. in our effort to add SYCL we have been advised to use /device:SYCL:x \r\nThat inconsitency in the way how devices are reported makes testing generalisation painful.\r\nThe backward compatiblity is intact as you still can register /gpu:0 I believe that the device gets \"canonicalized to new style names\" (https://github.com/tensorflow/tensorflow/pull/11697#issuecomment-318111065)\r\n\r\n> One solution to keep backward compatibility with old RunMetadata is to support all device naming schemes, instead of just replacing them.\r\n\r\nThat's really good point! Will do that!", "Yes, the problem with the old style names like '/gpu:0' is that you don't know ahead of time when parsing a device name whether it even is a device name.  For example, we know the valid names are '/job:foo', '/task:N', etc.  But if you allow '/anything:here', the parsing logic basically has to assume that anything other than 'job', 'task', and 'replica' must be a device, which is error prone.\r\n\r\nSo having 'device' in the name allows us to unambiguously know that the entry is a 'device type'.  We had to backwards-compat the old cpu and gpu names because they were too widely used.", "For internal folks, the related bug about replacing old device names is b/37868888", "@vrv should I do the same for /cpu ? - just double checking :)", "@tensorflow-jenkins test this please", "@panyx0718 / @rmlarsen  / @vrv thanks for the review! :)", "@tensorflow-jenkins test this please", "@yzhwang @vrv can you approve this PR?", "coordinator_test.py failed in Windows Cmake Tests, but it is irrelevant to this PR?", "@lukeiwanski thanks for the contribution. \r\nMerging. The failing coordinator test is unrelated."]}, {"number": 11696, "title": "tf.Print return type", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**:\r\n2.7\r\n- **Exact command to reproduce**: \r\n```\r\na = tf.constant([[1,2],[3,4],[5,6],[7,8]])\r\npartitions = tf.dynamic_partition(a,range(a.shape[0]),a.shape[0])\r\nprint(type(partitions)) # <type 'list'>\r\npartitions = tf.Print(partitions,[partitions],\"partition is:\") # prints the list\r\nprint(type(partitions)) # <class 'tensorflow.python.framework.ops.Tensor'>\r\n```\r\n### Describe the problem\r\n`tf.Print` is supposed to be an identity operation so I assume it should not change type from `list` to `Tensor`. If we are not supposed to pass a list, it should raise an error instead of changing the type.", "comments": ["This is somewhat expected behavior, and it is not unique to tf.Print()\r\n```\r\na = tf.constant([[1,2],[3,4],[5,6],[7,8]])\r\npartitions = tf.dynamic_partition(a,range(a.shape[0]),a.shape[0])\r\n```\r\nThis is a property tf.convert_to_tensor() i.e.\r\n```\r\n>>> tf.convert_to_tensor([tf.constant([1,2]), tf.constant([3,4])])\r\n<tf.Tensor 'packed:0' shape=(2, 2) dtype=int32>\r\n>>> sess.run(tf.convert_to_tensor([tf.constant([1,2]), tf.constant([3,4])]))\r\narray([[1, 2],\r\n       [3, 4]], dtype=int32)\r\n```\r\nThe point is that if you think of tensors of the same shape embedded as lists as being a generalized form of plain python lists it makes sense to convert them into a unified tensor if possible.\r\n\r\ni.e.\r\n```\r\ntf.convert_to_tensor([tf.constant(1), tf.constant(2)]) == tf.constant([1,2])\r\n```\r\nhowever this is an error\r\n```\r\ntf.convert_to_tensor([tf.constant(1), tf.constant([2,3])]) == tf.constant([1,2])\r\n```\r\n\r\nSo in essence this is expected behavior. @ebrevdo, anythign else to add?\r\n", "I'm confused. `tf.Print` is supposed to return the first parameter as is (as it is identity operation), does it really need to convert it to a tensor? I guess not, because it does not operate on it.", "Every op operand that expects a tensor implicitly runs convert_to_tensor... this is why\r\ntf.constant([1,2,3]) works and stuff.  Even identity works like this i.e.\r\n\r\n```\r\nsess.run(tf.identity([tf.constant(3),tf.constant(4)]))\r\narray([3, 4], dtype=int32)\r\n```\r\nThe identity op returns a single tensor and takes a single tensor. There are ops that return multiple tensors but they would need to explicitly be designed to do that. The print op is exactly the same. It doesn't take a list of tensors, it takes a single tensor and returns a single tensor. The Python bindings are trying to be helpful by implcitly making a single tensor out of what you provide. Perhaps convert_to_tensor should not be quite that helpful in the presence of a list of tensor objects vs a pure numpy array or a nested list of lists (where this behavior is expected), but that is how it works now.\r\n", "@vrv, do you have any further insight into this?", "I don't have any further insight into this, I'm not sure what the semantics of Print should be, so I don't know what can or should be changed about it.", "No operation that takes a single tensor, but to which you provide a list, is an identity operation.\r\n\r\nThe fact that this didn't raise an error is a little bit unfortunate; but the automatic merging of lists when a single tensor is expected is very useful in almost all other contexts.\r\n\r\nWe could certainly add an error check of the form:\r\n```if x is a flat list and x has Tensors in it, raise error```\r\nin tf.Print, tf.identity, and tf.placeholder_with_default (specifically because folks expect it to work as identity); though this may be considered an API breakage.  @josh11b wdyt?", "Maybe the simplest thing to do is to not call convert_to_tensor on the wrappers for Print and Identity and tf.placdholder_with_default.... then if you don't give it a variable that is already a Tensor it will be an error. However, it likely will break existing code. We might settle for a warning.", "folks use tf.identity and tf.placeholder_with_default with non-tensors all\nthe time.  that specific approach would break everyone.\n\nOn Tue, Jul 25, 2017 at 10:27 AM, Andrew Selle <notifications@github.com>\nwrote:\n\n> Maybe the simplest thing to do is to not call convert_to_tensor on the\n> wrappers for Print and Identity and tf.placdholder_with_default.... then\n> if you don't give it a variable that is already a Tensor it will be an\n> error. However, it likely will break existing code. We might settle for a\n> warning.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11696#issuecomment-317809235>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim26IQLavcwRuD_a6C0ryn8W-ZkA6ks5sRiWXgaJpZM4Oginm>\n> .\n>\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "I also encountered some error due to dypte conversion.  E.g. `x` is an int32 tensor, then `x = tf.Print(x, [x])` will cause error: `convert from int32 to float32`. \r\nThe other part of my code runs well if I comment this `tf.Print` line.  So I guess there's some hidden operation in `tf.Print()`? But as assumed, `tf.Print()` should do nothing but a `print` operation?"]}, {"number": 11695, "title": "Makefile build fails on OSX: -lprotobuf not found", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo. Using the provided Makefile build system. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS X 10.12.6\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource - r 1.3\r\n\r\n- **TensorFlow version (use command below)**:\r\nr1.3\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.5.2-homebrew\r\n\r\n- **Exact command to reproduce**:\r\n`sh /tensorflow/contrib/makefile/build_all_linux.sh \r\n`\r\n### Describe the problem\r\nI think there is a bug in the make files when building for OSX. Actually, the build finishes ok but the scripts give a linked error when compiling the provided benchmark program. \r\nThe error given is:\r\n\r\n```\r\nld: library not found for -lprotobuf\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n\r\n```\r\n\r\n### Source code / logs\r\nThis is the line where error occurs:\r\n```\r\ngcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -march=native -I. -Itf_build/tensorflow/tensorflow/contrib/makefile/downloads/ -Itf_build/tensorflow/tensorflow/contrib/makefile/downloads/eigen -Itf_build/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -Itf_build/tensorflow/tensorflow/contrib/makefile/gen/proto/ -Itf_build/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -Itf_build/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include \\\r\n\t-o tf_build/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark tf_build/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/util/reporter.o tf_build/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model.o tf_build/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model_main.o \\\r\n\t tf_build/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a -L/usr/local/lib -all_load -lstdc++ -lprotobuf -lz -lm\r\n```\r\n\r\nAs you can see towards the end it links against -lprotobuf, however, it does not give the path to the library (It's missing `-L/some-path/tensorflow/contrib/makefile/gen/protobuf-host/lib`). When I manually compile the program, instead of using the makefile, and pass the library path, it works fine. \r\n\r\nIt is easy to fix by modifying the lines 186 onwards in the Makefile from:\r\n\r\n```\r\nifeq ($(TARGET),OSX)\r\n\tLDFLAGS += -all_load\r\nendif\r\n```\r\nto\r\n```\r\nifeq ($(TARGET),OSX)\r\n\tLDFLAGS += -all_load\r\nifeq ($(HAS_GEN_HOST_PROTOC),true)\r\n\tLIBFLAGS += -L$(MAKEFILE_DIR)/gen/protobuf-host/lib\r\n\texport LD_LIBRARY_PATH=$(MAKEFILE_DIR)/gen/protobuf-host/lib\r\nendif\r\n```\r\nAnother related issue is that I get a whole bunch of warnings from ranlib about some of the built libraries having no symbol. Any comments on that will also be appreciated:\r\n\r\n```\r\n/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(mkl_layout_pass.o) has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(mkl_tfconversion_pass.o) has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(cupti_wrapper.o) has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(android_armv7a_cpu_utils_helper.o) has no symbols\r\n\r\n```\r\n\r\nThanks.", "comments": ["/cc @gunan ", "I get the exact same error:\r\nld: library not found for -lprotobuf\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n\r\nwhen trying to execute the same command:\r\ntensorflow/contrib/makefile/build_all_linux.sh\r\n\r\n Using TensorFlow V 1.1.0 on a mac os 10.12.4\r\n\r\nMaybe @petewarden will have some insights?\r\n", "@petewarden @andrewharp @caisq any thoughts?\r\nOur nightly build is passing, maybe there is a setup issue here?", "I have tried it on two different Macs. I installed all the pre-requisites so I doubt there is a setup issue. Like I said, passing the full path to gen/protobuf lib with -L works. \r\nHaving said that, I am happy to follow any instructions to check for the setup accuracy. \r\n\r\n", "Also, any comment about the no symbol warnings:\r\n\r\n```\r\n/Library/Developer/CommandLineTools/usr/bin/ranlib: file: /fullpath/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(mkl_layout_pass.o) has no symbols\r\n```\r\n\r\nIs it normal?", "Ping!\r\n@petewarden @andrewharp \r\nDo we expect build_all_linux.sh to work on OSX?", "I've got the same error. Should I use build_all_ios.sh on MacOS instead of build_all_linux.sh?", "@petewarden Could you please look into this ?", "@vikrantt Hi, does this issue still exist ?", "> @vikrantt Hi, does this issue still exist ?\r\n\r\nHaven't looked into it lately. ", "Closing this issue, since it's fairly old and doesn't have any recent reports."]}, {"number": 11694, "title": "Add `as_default()` to `MonitoredSession`, `MonitoredTrainingSession` and `SingularMonitoredSession`.", "body": "Equivalent to `tf.Session.as_default()`, allows refetching the session from elsewhere in code using `tf.get_default_session()`.\r\n\r\nNotes:\r\n- (additive) public API change.\r\n- If there is a better way to get the current session when using these `tf.Session` wrappers, this may be unnecessary. I just couldn't find it.\r\n", "comments": ["Can one of the admins verify this patch?", "Hmm, just noticed [this comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/monitored_session.py#L649) saying a `MonitoredSession` cannot be set as the default session. No mention of _why_ though. Perhaps there's some unspecified assumption that this PR would break?", "@darrengarvey I believe that this is because MonitoredTrainingSession isn't a Session, it's something that wraps a Session, so does not implement the interface that as_default() would require.  If it was a subclass of Session, this would be possible, but I think by design it is not.  :("]}, {"number": 11693, "title": "Tensorflow error while building using gpu", "body": "ERROR: /home/overlord/.cache/bazel/_bazel_overlord/38c05a5232666f398e07b83e8b030232/external/lmdb/BUILD.bazel:8:1: C++ compilation of rule '@lmdb//:lmdb' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 33 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/lmdb/midl.c: In function 'MDB_ID* mdb_midl_alloc(int)':\r\nexternal/lmdb/midl.c:105:47: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]\r\n  MDB_IDL ids = malloc((num+2) * sizeof(MDB_ID));\r\n                                               ^\r\nexternal/lmdb/midl.c: In function 'void mdb_midl_shrink(MDB_ID**)':\r\nexternal/lmdb/midl.c:123:58: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]\r\n   (ids = realloc(ids, (MDB_IDL_UM_MAX+2) * sizeof(MDB_ID))))\r\n                                                          ^\r\nexternal/lmdb/midl.c: In function 'int mdb_midl_grow(MDB_ID**, int)':\r\nexternal/lmdb/midl.c:134:54: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]\r\n  idn = realloc(idn, (*idn + num + 2) * sizeof(MDB_ID));\r\n                                                      ^\r\nexternal/lmdb/midl.c: In function 'int mdb_midl_need(MDB_ID**, unsigned int)':\r\nexternal/lmdb/midl.c:148:50: error: invalid conversion from 'void*' to 'MDB_IDL {aka long unsigned int*}' [-fpermissive]\r\n   if (!(ids = realloc(ids-1, num * sizeof(MDB_ID))))\r\n                                                  ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11692, "title": "A bug of tf.reduce_logsumexp with `-inf`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux CentOS 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/5.1.3\r\n- **GPU model and memory**: Tesla K40m, 11439MiB\r\n- **Exact command to reproduce**:  `python -c \"import tensorflow as tf; print tf.Session().run(tf.reduce_logsumexp(float('-inf')))\"`\r\n\r\n### Describe the problem\r\nThe doc of tf.reduce_logsumexp says it\r\n\r\n> Computes log(sum(exp(elements across dimensions of a tensor))).\r\n\r\nHowever, it does not when the tensor is `-inf`.\r\n\r\n### Source code / logs\r\n```\r\npython -c \"import tensorflow as tf; print tf.Session().run(tf.reduce_logsumexp(float('-inf')))\"\r\n```\r\nprints\r\n```\r\nnan\r\n```\r\n--------------------\r\n\r\n```\r\npython -c \"import tensorflow as tf; print tf.Session().run(tf.log(tf.reduce_sum(tf.exp(float('-inf')))))\"\r\n```\r\nprints\r\n```\r\n-inf\r\n```\r\n", "comments": ["The tf.reduce_logsumexp source code currently is:\r\n```python\r\ndef reduce_logsumexp(input_tensor,\r\n                     axis=None,\r\n                     keep_dims=False,\r\n                     name=None,\r\n                     reduction_indices=None):\r\n  \"\"\"TF 1.2.0 source code\"\"\"\r\n  with ops.name_scope(name, \"ReduceLogSumExp\", [input_tensor]) as name:\r\n    my_max = array_ops.stop_gradient(\r\n        reduce_max(\r\n            input_tensor,\r\n            axis=axis,\r\n            reduction_indices=reduction_indices,\r\n            keep_dims=True))\r\n    result = gen_math_ops.log(\r\n        reduce_sum(\r\n            gen_math_ops.exp(input_tensor - my_max),\r\n            axis,\r\n            keep_dims=True,\r\n            reduction_indices=reduction_indices)) + my_max\r\n    if not keep_dims:\r\n      if isinstance(axis, int):\r\n        axis = [axis]\r\n      result = array_ops.squeeze(result, axis)\r\n    return result\r\n```\r\n\r\nI have written my own tf_reduce_logsumexp to fix the bug:\r\n```python\r\ndef tf_reduce_logsumexp(input_tensor,\r\n                        axis=None,\r\n                        keep_dims=False,\r\n                        name=None,\r\n                        reduction_indices=None):\r\n    \"\"\"Fix tf.reduce_logsumexp\"\"\"\r\n    with tf.name_scope(name, \"tf_ReduceLogSumExp\", [input_tensor]) as name:\r\n        raw_max = tf.reduce_max(\r\n            input_tensor,\r\n            axis=axis,\r\n            reduction_indices=reduction_indices,\r\n            keep_dims=True)\r\n        my_max = tf.stop_gradient(\r\n            tf.where(\r\n                tf.is_finite(raw_max),\r\n                raw_max,\r\n                tf.zeros_like(raw_max)))\r\n        result = tf.log(\r\n            tf.reduce_sum(\r\n                tf.exp(input_tensor - my_max),\r\n                axis,\r\n                keep_dims=True,\r\n                reduction_indices=reduction_indices)) + my_max\r\n        if not keep_dims:\r\n            if isinstance(axis, int):\r\n                axis = [axis]\r\n            result = tf.squeeze(result, axis)\r\n        return result\r\n```\r\n", "@fastturtle, could you take a look?", "This looks like a reasonable fix, would you be willing to submit it as a pull-request?", "@aselle Sure, I will try it.\r\n"]}, {"number": 11691, "title": "[Issue 11241] Add checkpoint_convert.py script to package and make _RNN_NAME_REPLACEMENTS public.", "body": "As requested in https://github.com/tensorflow/tensorflow/issues/11241 , this PR adds the script https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/tools/checkpoint_convert.py to the TensorFlow pip package and removes the leading underscore from the name of `_RNN_NAME_REPLACEMENTS` to indicate that this constant can be used by user code.\r\n\r\nTesting done:\r\n* Verified that `checkpoint_convert.py` is not included in the package prior to this change.\r\n* Verified that `checkpoint_convert.py` is included in the package after this change.\r\n* Verified that `checkpoint_convert.py` does not complain about missing imports when invoked from the command line after installing the pip package.\r\n* Ran unit tests in `contrib/rnn` after the change.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "(Tests are failing because of the BUILD file formatting (see [logs](https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/5418/consoleFull)).  \r\n\r\nPlease fix those and you should be good to go. Thanks!\r\nDetails:\r\n\r\n```\r\n=== Sanity check step 3 of 10: do_buildifier (buildifier check) ===\r\n\r\nRunning do_buildifier on 208 files\r\n\r\ntensorflow/contrib/rnn/BUILD # reformat \r\n\r\nbuildifier took 0 s\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n29,30c29,31\r\n<     srcs = [\"__init__.py\"] + glob([\"python/ops/*.py\"]) \r\n<                            + [\"python/tools/checkpoint_convert.py\"],\r\n---\r\n>     srcs = [\"__init__.py\"] + glob([\"python/ops/*.py\"]) + [\r\n>         \"python/tools/checkpoint_convert.py\",\r\n>     ],\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```", "@tensorflow-jenkins test this please"]}, {"number": 11690, "title": "[Feature request] Instance Normalization", "body": "[Instance Normalization](https://arxiv.org/abs/1607.08022) is recently widely used in style transfer and GAN, since it avoid the drawback of batch normalization which brings in-batch correlations. Tensorflow only has quantized version instance norm right now, but a full version is also easy to implement, I am interested in it. \r\nAlso I wonder if it is better to implemented in C++ using eigen or simply add some lines in `nn_impls.py` with the benefit of being able to use `fused_batch_norm`. [Mxnet](https://github.com/dmlc/mxnet/blob/03b7d1402ab8d1142391c601fddbc4081632aa3c/src/operator/instance_norm-inl.h) does the first way while [Pytorch](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/instancenorm.py) use the latter method.", "comments": ["You can use Batch Renormalization in [Keras Contrib](https://github.com/farizrahman4u/keras-contrib).", "@Zardinality, does batch renormalization satisfy your needs?", "@aselle @StefanoD Batch renormalization is not exactly what I want, but I found instance normalization in [the same file](https://github.com/farizrahman4u/keras-contrib/commit/f0bb5becbb0585384890aefc51ffb11a4d832b2b). It is a pity that contrib part of keras isn't contained in tensorflow distribution.", "The implementation in https://github.com/farizrahman4u/keras-contrib is not exactly right.\r\nAccording to the paper, the IN layer should perform differently in training/testing phase.\r\nThere needs a running_mean and a running_var.\r\nThe right Torch implementation for reference is here:\r\nhttps://github.com/junyanz/CycleGAN/blob/master/util/InstanceNormalization.lua\r\n\r\n*My opinion is wrong. see below.*", "@lllyasviel @StefanoD  The instance normalization paper says: \"The key idea is to replace batch normalization layers in the generator architecture with instance normalization layers, and to keep them at test time (as opposed to freeze and simplify them out as done for batch normalization)\". I believe the running_mean and running_var in the lua implementations are being used to hack the batch normalization implementation to do instance normalization. Instance normalization is just contrast normalization and the keras-contrib implementation looks right to me. Please point out specific bugs if you identify them.", "I reviewed the code and finally found I was wrong. I am sorry for that. The keras implementation is good. \ud83d\udc4d \r\nps: maybe use default axis = -1 is better? Many keras users prefer directly writing \"x = InstanceNormalization()(x);\" when the last layer is a channel-last conv2d. In this case, it will take them many time to finally found out that they should add \"axis=3\".\r\nI know that the default axis=None is prepared for dense layer. But the original keras prefers the -1 implementation, right?", "Are the implementations of Instance Normalization and Layer Normalization identical?", "@lllyasviel interestingly enough, axis=None seems to provide better result of axis=-1 (on cycleGAN). I know this is any vague, but I thought it was worth it to put it out there.", " tf 2.0  replace the  api of Instance Normalization ?", "So is Instance Normalization present in TF2.0 ? "]}, {"number": 11688, "title": "Tensorflow v1.2.1 compile error", "body": "I'm, trying to build tensorflow from source. I've followed the tutorial in https://www.tensorflow.org/install/install_sources without success.\r\n\r\nThe only difference is that I'm trying to use OpenCL with SYCL.\r\n\r\n### System information\r\n- **OS Platform and Distribution**: Ubuntu 14.04 64 bits\r\n- **TensorFlow version to be compiled**: v1.2.1\r\n- **Python version**: 2.7.6\r\n- **Bazel version (from repository)**: Build label: 0.5.2\r\n- **GPU model and memory**: Radeon HD 7850\r\n\r\n### Problem\r\nI'm trying to compile tensorflow v1.2.1 but I'm having an error about numpy missing dependecies declaration.\r\nIt seems there is some workaround because headers are there, but I have no clue about how to do it. My bazel knowledge is low.\r\n\r\n**Configuration:**\r\n\r\n```\r\nflener@flener-desktop:~/Downloads/tensorflow-src$ uname -a\r\nLinux flener-desktop 3.11.0-14-generic #21-Ubuntu SMP Tue Nov 12 17:04:55 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux\r\nflener@flener-desktop:~/Downloads/tensorflow-src$ git checkout v1.2.1\r\nHEAD is now at b4957ff... Merge pull request #11156 from av8ramit/1.2.1\r\nflener@flener-desktop:~/Downloads/tensorflow-src$ git clean -fdx\r\nRemoving .bazelrc\r\nRemoving .tf_configure.bazelrc\r\nRemoving bazel-bin\r\nRemoving bazel-genfiles\r\nRemoving bazel-out\r\nRemoving bazel-tensorflow-src\r\nRemoving bazel-testlogs\r\nRemoving tensorflow/tools/git/gen/\r\nRemoving third_party/eigen3/mkl_include\r\nRemoving third_party/mkl/include\r\nRemoving third_party/mkl/libdl.so.2\r\nRemoving third_party/mkl/libiomp5.so\r\nRemoving third_party/mkl/libmklml_intel.so\r\nRemoving third_party/mkl/mkl.config\r\nRemoving third_party/mkl/mklml_lnx_2018.0.20170425.tgz\r\nRemoving third_party/mkl/mklml_lnx_2018.0.20170425/\r\nRemoving tools/python_bin_path.sh\r\nflener@flener-desktop:~/Downloads/tensorflow-src$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\t\r\nDo you wish to build TensorFlow with MKL support? [y/N] y\r\nMKL support will be enabled for TensorFlow\r\nDo you wish to download MKL LIB from the web? [Y/n] y\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] y\r\nVERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] y\r\nOpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] n\r\nNo CUDA support will be enabled for TensorFlow\r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is ]: /usr/bin/g++-4.9\r\nPlease specify which C compiler should be used as the host C compiler. [Default is ]: /usr/bin/gcc-4.9\r\nPlease specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: \r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n\r\n```\r\n\r\n**Build command:**\r\n`flener@flener-desktop:~/Downloads/tensorflow-src$ bazel build --local_resources 4096,4.0,1.0 --verbose_failures -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Error:**\r\n```\r\nERROR: /home/flener/Downloads/tensorflow-src/tensorflow/python/BUILD:158:1: undeclared inclusion(s) in rule '//tensorflow/python:numpy_lib':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/python/lib/core/numpy.cc':\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/arrayobject.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_common.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/numpyconfig.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/_numpyconfig.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_endian.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_cpu.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/utils.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/__multiarray_api.h'\r\n  '/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy/npy_interrupt.h'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n", "comments": ["are you sure you are compiling from the latest master or the release tag? You didn't specify what git sha you are using.\r\n", "aselle, this information is in configuration block: \r\n```\r\nflener@flener-desktop:~/Downloads/tensorflow-src$ git checkout v1.2.1\r\nHEAD is now at b4957ff... Merge pull request #11156 from av8ramit/1.2.1\r\n\r\n```\r\n\r\nfull hash : b4957ffc69e73cf8348db7f381438c3b0ccabd14\r\n", "@jart, could you take a look. It looks like it might be a numpy versioning problem, but I don't really know how to diagnose or suggest a fix.\r\n", "opencl/sycl still does not have official support. I will mark this community support.", "Does this issue persist with the latest tensorflow version ? If so, please open a new issue by providing all the information asked in [this template](https://github.com/tensorflow/tensorflow/issues/new/choose)."]}, {"number": 11687, "title": "Update layers.py", "body": "I have added a check on beta with reference to the following issue raised.\r\nhttps://github.com/tensorflow/tensorflow/issues/11673\r\n\r\nPlease verify and get back.\r\nThank you.", "comments": ["Can one of the admins verify this patch?", "@shreyneil Do you mind adding a unit test for this fix? ", "Sir the following test cases take cover exhaustive cases for the combination of center with zero_debias_moving_mean parameter.\r\nPlease verify.\r\nTest Case 1:\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))\r\nb = tf.contrib.layers.batch_norm(a, center=False, data_format='NCHW',\r\n                                 zero_debias_moving_mean=True)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nTest Case 2:\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))\r\nb = tf.contrib.layers.batch_norm(a, center=False, data_format='NCHW',\r\n                                 zero_debias_moving_mean=False)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nTest Case 3:\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))\r\nb = tf.contrib.layers.batch_norm(a, center=True, data_format='NCHW',\r\n                                 zero_debias_moving_mean=True)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nTest Case 4:\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))\r\nb = tf.contrib.layers.batch_norm(a, center=True, data_format='NCHW',\r\n                                 zero_debias_moving_mean=False)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())", "Would it be worth adding a test to layers_test to validate this fix?", "@shreyneil Please update the unit tests to validate the fix.", "Closing out for now due to being stalled. Feel free to reopen or send a new pull request with the test added. Thanks!"]}, {"number": 11686, "title": "Fix stackoverflow link to avoid redirect warning", "body": "Redirect warning is always shown to go to stackoverflow.\r\n\r\n<img width=\"1081\" alt=\"screen shot 2017-07-23 at 16 50 25\" src=\"https://user-images.githubusercontent.com/1713047/28497657-53ec0e8c-6fc7-11e7-8211-2919b9706dbb.png\">\r\n", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11685, "title": "retrain inception-v3 error", "body": "\r\n", "comments": ["when I retrain inception-v3 on my data set ,running \"bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain\" successfully ,but when I run \" sudo bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/res\",I got this :sudo: bazel-bin/tensorflow/examples/image_retraining/retrain\uff1acommand not found.\r\n      then I went to the tensorflow root and found the file bazel-bin ,I could not open it ,\"\u65e0\u6cd5\u4f7f\u7528\u6b64\u94fe\u63a5\uff0c\u56e0\u4e3a\u5176\u76ee\u6807\u201c/root/.cache/bazel/_bazel_root/942963faa10f2c8f4c9028b108793856/execroot/org_tensorflow/bazel-out/local-opt/bin\u201d\u4e0d\u5b58\u5728\u3002\"\r\n      please help me ,how can I solve this problem?\r\n\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\nI found out the problem,because Ubuntu environment has some problem.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11684, "title": "Can anybody please let me know an error free code of basic CNN tensorflow code.  I am having hard time to resolve  tensorflow coding working, even  the tensorflow tutorial code from tensorflow.org", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Sorry you are having trouble, but if you can't take the time to tell us specifically what problems you are hitting with a concrete example, it's difficult to help you. Please open a new issue and take the time to tell us what commands you are running, what version of TensorFlow you are using. It may be even as simple as your install not being completely working rather than any problem with the code examples. But without any details, we can't really give you much help. Thank you.\r\n", "This is the sample code I tried to execute using TF (python 3.5.2  on Windows 10 64bit)\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\n# Imports\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import learn\r\nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef cnn_model_fn(features, labels, mode):\r\n  \"\"\"Model function for CNN.\"\"\"\r\n  # Input Layer\r\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\r\n  # MNIST images are 28x28 pixels, and have one color channel\r\n  input_layer = tf.reshape(features, [-1, 28, 28, 1])\r\n\r\n  # Convolutional Layer #1\r\n  # Computes 32 features using a 5x5 filter with ReLU activation.\r\n  # Padding is added to preserve width and height.\r\n  # Input Tensor Shape: [batch_size, 28, 28, 1]\r\n  # Output Tensor Shape: [batch_size, 28, 28, 32]\r\n  conv1 = tf.layers.conv2d(inputs=input_layer,filters=32,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\r\n  \r\n  # Pooling Layer #1\r\n  # First max pooling layer with a 2x2 filter and stride of 2\r\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\r\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\r\n  \r\n  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n  \r\n  # Convolutional Layer #2\r\n  # Computes 64 features using a 5x5 filter.\r\n  # Padding is added to preserve width and height.\r\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\r\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\r\n  conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\r\n  \r\n  # Pooling Layer #2\r\n  # Second max pooling layer with a 2x2 filter and stride of 2\r\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\r\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\r\n  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n  \r\n  # Flatten tensor into a batch of vectors\r\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\r\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\r\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n\r\n   # Dense Layer\r\n  # Densely connected layer with 1024 neurons\r\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\r\n  # Output Tensor Shape: [batch_size, 1024]\r\n  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n  \r\n  # Add dropout operation; 0.6 probability that element will be kept\r\n  dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\r\n\r\n  \r\n  # Logits layer\r\n  # Input Tensor Shape: [batch_size, 1024]\r\n  # Output Tensor Shape: [batch_size, 10]\r\n  logits = tf.layers.dense(inputs=dropout, units=10)\r\n  \r\n  \r\n  loss = None\r\n  train_op = None\r\n  \r\n  # Calculate Loss (for both TRAIN and EVAL modes)\r\n  if mode != learn.ModeKeys.INFER:\r\n    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\r\n    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\r\n\t\r\n\t\r\n  # Configure the Training Op (for TRAIN mode)\r\n  if mode == learn.ModeKeys.TRAIN:\r\n    train_op = tf.contrib.layers.optimize_loss(loss=loss,global_step=tf.contrib.framework.get_global_step(),learning_rate=0.001,optimizer=\"SGD\")\r\n\t\r\n\t\r\n\t# Generate Predictions\r\n  predictions = {\r\n      \"classes\": tf.argmax(\r\n          input=logits, axis=1),\r\n      \"probabilities\": tf.nn.softmax(\r\n          logits, name=\"softmax_tensor\")\r\n}\r\n\r\n  # Return a ModelFnOps object\r\n  return model_fn_lib.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)\r\n  \r\ndef main(unused_argv):\r\n  # Load training and eval data\r\n  mnist = learn.datasets.load_dataset(\"mnist\")\r\n  train_data = mnist.train.images  # Returns np.array\r\n  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\n  eval_data = mnist.test.images  # Returns np.array\r\n  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\r\n  \r\n  \r\n  # Create the Estimator\r\n  mnist_classifier = learn.Estimator(model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\r\n  \r\n  \r\n  # Set up logging for predictions\r\n  # Log the values in the \"Softmax\" tensor with label \"probabilities\"\r\n  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\r\n  logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\r\n  \r\n  # Train the model\r\n  mnist_classifier.fit(x=train_data,y=train_labels,batch_size=100,steps=20000,monitors=[logging_hook])\r\n  \r\n  # Configure the accuracy metric for evaluation\r\n  metrics = {\r\n      \"accuracy\":\r\n          learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),\r\n}\r\n\r\n  # Evaluate the model and print results\r\n  eval_results = mnist_classifier.evaluate(x=eval_data, y=eval_labels, metrics=metrics)\r\n  print(eval_results)\r\n  \r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n\r\nHowever , I got so many errors \r\n\r\nExtracting MNIST-data\\train-images-idx3-ubyte.gz\r\nExtracting MNIST-data\\train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data\\t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data\\t10k-labels-idx1-ubyte.gz\r\n**INFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'save_checkpoints_secs': 600, '_is_chief': True, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001959B8C3A58>, 'save_summary_steps': 100, '_evaluation_master': '', 'keep_checkpoint_max': 5, 'tf_random_seed': None, '_environment': 'local', 'tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1\r\n}\r\n, '_master': '', '_num_ps_replicas': 0, 'keep_checkpoint_every_n_hours': 10000, '_task_type': None, 'save_checkpoints_steps': None}\r\nWARNING:tensorflow:From C:\\..\\..\\tensorflow_programs\\Basic_CNN.py:113 in main.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From C:\\..\\..\\tensorflow_programs\\Basic_CNN.py:113 in main.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From C:\\..\\..\\tensorflow_programs\\Basic_CNN.py:113 in main.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nTraceback (most recent call last):\r\n  File \"C:\\..\\..\\tensorflow_programs\\Basic_CNN.py\", line 126, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\Prabir Sinha\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"C:\\..\\..\\tensorflow_programs\\Basic_CNN.py\", line 113, in main\r\n    mnist_classifier.fit(x=train_data,y=train_labels,batch_size=100,steps=20000,monitors=[logging_hook])\r\n  File \"C:\\Users\\Prabir Sinha\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Prabir Sinha\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 355, in fit\r\n    max_steps=max_steps)\r\n  File \"C:\\Users\\Prabir Sinha\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 699, in _train_model\r\n    train_ops = self._get_train_ops(features, labels)\r\n  File \"C:\\Users\\Prabir Sinha\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1052, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"C:\\Users\\Prabir Sinha\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1021, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, mode=mode)\r\n  File \"C:\\..\\..\\tensorflow_programs\\Basic_CNN.py\", line 26, in cnn_model_fn\r\n    conv1 = tf.layers.conv2d(inputs=input_layer,filters=32,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\r\nAttributeError: module 'tensorflow' has no attribute 'layers'**\r\n\r\nCan  anyone put some light how should I  approach to remove this errors .\r\n\r\nThanks \r\nPrabir \r\n"]}, {"number": 11683, "title": "Add a note about tf.contrib.signal to the 1.3 release notes.", "body": "Feel free to close if this is too late (don't know if there'll be an RC1).", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11682, "title": "Missing numpy dependency at installation", "body": "I installed PyTorch on a fresh Python 3.6 installation (`brew install python3`), got an error and found out that numpy wasn't installed. I installed numpy and then things work. Shouldn't it be included among dependencies of the PyTorch package? (with the right version requirement, see #559?)\r\n\r\n```\r\nMinhs-MacBook-Pro:~ cumeo$ pip3 install --user http://download.pytorch.org/whl/torch-0.1.12.post2-cp36-cp36m-macosx_10_7_x86_64.whl\r\nCollecting torch==0.1.12.post2 from http://download.pytorch.org/whl/torch-0.1.12.post2-cp36-cp36m-macosx_10_7_x86_64.whl\r\n  Downloading http://download.pytorch.org/whl/torch-0.1.12.post2-cp36-cp36m-macosx_10_7_x86_64.whl (3.7MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.7MB 1.6MB/s\r\nCollecting pyyaml (from torch==0.1.12.post2)\r\nInstalling collected packages: pyyaml, torch\r\nSuccessfully installed pyyaml-3.12 torch-0.1.12.post2\r\nMinhs-MacBook-Pro:~ cumeo$ python3\r\nPython 3.6.2 (default, Jul 17 2017, 16:44:45)\r\n[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\nImportError: numpy.core.multiarray failed to import\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/cumeo/Library/Python/3.6/lib/python/site-packages/torch/__init__.py\", line 53, in <module>\r\n    from torch._C import *\r\nImportError: numpy.core.multiarray failed to import\r\n>>> import numpy\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'numpy'\r\n>>>\r\nMinhs-MacBook-Pro:~ cumeo$ pip3 install --user numpy\r\nCollecting numpy\r\n  Using cached numpy-1.13.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\r\nInstalling collected packages: numpy\r\nSuccessfully installed numpy-1.13.1\r\nMinhs-MacBook-Pro:~ cumeo$ python3\r\nPython 3.6.2 (default, Jul 17 2017, 16:44:45)\r\n[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>>\r\n```", "comments": ["Perhaps you meant to file this at https://github.com/pytorch/pytorch/issues/new instead of as a TensorFlow issue?", "I'm so sorry :\">"]}, {"number": 11681, "title": "Different GPU memory not all allocated", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nwin10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.1\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\ncuda8.0,cudnn5.5\r\n- **GPU model and memory**:\r\nGTX Titan 6GB, GTX 1080Ti 11GB\r\n\r\n### Describe the problem\r\nNote that tensorflow will allocate tensor/op to all available gpus, but i only got 1080Ti occupied, I wonder if it's a bug or tensorflow doesn't support different gpu models.", "comments": ["Can you clarify what the problem is? Are you saying TensorFlow is only using the 1080 Ti, even though you additionally have a GTX titan? If that is the case, you can use `tf.device(...)` to specify which GPU to use, see the [Using GPUs](https://www.tensorflow.org/tutorials/using_gpu) tutorial.", "@reedwm Thanks for response.\r\n\r\nYes, it only use 1080Ti, but I heared tensorflow can automatically take all detected devices' memory.\r\n\r\nI have tried to use `tf.device(...)`, but it unhandy for me to place it manually. My question may now be does tensorflow automatically use all devices' memory?", "TensorFlow will automatically allocate all the GPUs' memory when it starts. While a TensorFlow is running, the `nvidia-smi` command should show this. But, only the first GPU (in your case, the 1080 Ti) will be used by default. TensorFlow does not automatically place ops on different GPUs so you need to use `tf.device(...)` if you want to use the GTX Titan.", "Ok, I think the neural network layers are ops, so they won't be automatically placed on different GPU. So the best practice is to use same type of GPUs, and get them synchronized parameters, asynchronously trained on different batches."]}, {"number": 11680, "title": "Add new feature description for tfdbg to r1.3 release notes", "body": "", "comments": []}, {"number": 11679, "title": "Error when running imported/restored model that uses feedable iterator (tf.contrib.data)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: Build #242 (Jul 17, 2017 2:25:00 AM)\r\n- **Python version**: Python 3.6.2\r\n- **Bazel version (if compiling from source)**: na\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: 670 gtx 2gb\r\n- **Exact command to reproduce**: na\r\n\r\n### Describe the problem\r\nI can't restore and run checkpoints of models that use feedable iterators, but I can restore and run checkpoints of models that directly use `make_one_shot_iterator()`. Below is code for the feedable iterator version, and below that code for the `make_one_shot_iterator()` version:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.data import Dataset\r\n\r\nBATCH_SIZE = 4\r\nITERATION_COUNT = 2\r\ndataset = Dataset.from_tensor_slices(tf.constant([[0, 0],\r\n                                                  [0, 1],\r\n                                                  [1, 0],\r\n                                                  [1, 1]], dtype=tf.float32))\r\nbatched_dataset = dataset.batch(BATCH_SIZE)\r\niterator_handle_placeholder = tf.placeholder(tf.string, shape=[])\r\ntf.add_to_collection('placeholders', iterator_handle_placeholder)\r\niterator = tf.contrib.data.Iterator.from_string_handle(iterator_handle_placeholder, batched_dataset.output_types, batched_dataset.output_shapes)\r\n\r\n# create some graph\r\ninputs = iterator.get_next()\r\nsum_placeholder = tf.placeholder_with_default(tf.reduce_sum(inputs), shape=[])\r\ntf.add_to_collection('placeholders', sum_placeholder)\r\nsum_variable = tf.get_variable('sum', initializer=tf.zeros(shape=[]))\r\nassign_sum = tf.assign_add(sum_variable, sum_placeholder)\r\ntf.add_to_collection('assigns', assign_sum)\r\nsaver = tf.train.Saver()\r\n\r\n# run graph and save it at the end\r\nwith tf.Session() as session:\r\n    session.run(tf.global_variables_initializer())\r\n    batched_dataset_iterator = batched_dataset.make_one_shot_iterator()\r\n    batched_dataset_iterator_handle = session.run(batched_dataset_iterator.string_handle())\r\n    tf.add_to_collection('handles', batched_dataset_iterator_handle)\r\n    inputs_feed_dict = {iterator_handle_placeholder: batched_dataset_iterator_handle}\r\n    for i in range(ITERATION_COUNT):\r\n        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)\r\n        inputs_feed_dict[sum_placeholder] = inputs_sum\r\n        print(inputs_sum)\r\n    saver.save(session, 'checkpoints/haha')\r\n\r\n# restore saved graph and run it\r\nwith tf.Session() as session:\r\n    saver = tf.train.import_meta_graph('checkpoints/haha.meta')\r\n    saver.restore(session, 'checkpoints/haha')\r\n    assign_sum = tf.get_collection('assigns')[0]\r\n    iterator_handle_placeholder = tf.get_collection('placeholders')[0]\r\n    batched_dataset_iterator_handle = tf.get_collection('handles')[0]\r\n    sum_placeholder = tf.get_collection('placeholders')[1]\r\n    inputs_feed_dict = {iterator_handle_placeholder: batched_dataset_iterator_handle}\r\n    for i in range(ITERATION_COUNT):\r\n        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)\r\n        print(inputs_sum)\r\n        inputs_feed_dict[sum_placeholder] = inputs_sum\r\n```\r\ngives me\r\n```\r\nC:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\python.exe \"C:/Software Projects/ai/tensorflow/LayerTests.py\"\r\n2017-07-21 20:18:54.013753: W C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-21 20:18:54.257354: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX 670\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.98\r\npciBusID 0000:02:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.64GiB\r\n2017-07-21 20:18:54.257634: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0 \r\n2017-07-21 20:18:54.257773: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y \r\n2017-07-21 20:18:54.257935: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)\r\n4.0\r\n8.0\r\n2017-07-21 20:18:54.705095: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)\r\n2017-07-21 20:18:54.774833: W C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Container localhost does not exist.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Container localhost does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: PlaceholderWithDefault/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_7_PlaceholderWithDefault\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Software Projects/ai/tensorflow/LayerTests.py\", line 47, in <module>\r\n    inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Container localhost does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: PlaceholderWithDefault/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_7_PlaceholderWithDefault\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op 'IteratorFromStringHandle', defined at:\r\n  File \"C:/Software Projects/ai/tensorflow/LayerTests.py\", line 13, in <module>\r\n    iterator = tf.contrib.data.Iterator.from_string_handle(iterator_handle_placeholder, batched_dataset.output_types, batched_dataset.output_shapes)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\dataset_ops.py\", line 238, in from_string_handle\r\n    string_handle)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 362, in iterator_from_string_handle\r\n    string_handle=string_handle, name=name)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2628, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Container localhost does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: PlaceholderWithDefault/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_7_PlaceholderWithDefault\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\nHere's the version that uses just a `make_one_shot_iterator()`:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.data import Dataset\r\n\r\nBATCH_SIZE = 4\r\nITERATION_COUNT = 2\r\ndataset = Dataset.from_tensor_slices(tf.constant([[0, 0],\r\n                                                  [0, 1],\r\n                                                  [1, 0],\r\n                                                  [1, 1]], dtype=tf.float32))\r\nbatched_dataset = dataset.batch(BATCH_SIZE)\r\n\r\n# create some graph\r\nbatched_dataset_iterator = batched_dataset.make_one_shot_iterator()\r\ninputs = batched_dataset_iterator.get_next()\r\nsum_placeholder = tf.placeholder_with_default(tf.reduce_sum(inputs), shape=[])\r\ntf.add_to_collection('placeholders', sum_placeholder)\r\nsum_variable = tf.get_variable('sum', initializer=tf.zeros(shape=[]))\r\nassign_sum = tf.assign_add(sum_variable, sum_placeholder)\r\ntf.add_to_collection('assigns', assign_sum)\r\nsaver = tf.train.Saver()\r\n\r\n# run graph and save it at the end\r\nwith tf.Session() as session:\r\n    session.run(tf.global_variables_initializer())\r\n    inputs_feed_dict = {}\r\n    for i in range(ITERATION_COUNT):\r\n        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)\r\n        inputs_feed_dict[sum_placeholder] = inputs_sum\r\n        print(inputs_sum)\r\n    saver.save(session, 'checkpoints/haha')\r\n\r\n# restore saved graph and run it\r\nwith tf.Session() as session:\r\n    saver = tf.train.import_meta_graph('checkpoints/haha.meta')\r\n    saver.restore(session, 'checkpoints/haha')\r\n    assign_sum = tf.get_collection('assigns')[0]\r\n    sum_placeholder = tf.get_collection('placeholders')[1]\r\n    inputs_feed_dict = {}\r\n    for i in range(ITERATION_COUNT):\r\n        inputs_sum = session.run(assign_sum, feed_dict=inputs_feed_dict)\r\n        print(inputs_sum)\r\n        inputs_feed_dict[sum_placeholder] = inputs_sum\r\n```\r\nResult:\r\n```\r\nC:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\python.exe \"C:/Software Projects/ai/tensorflow/LayerTests.py\"\r\n2017-07-21 20:12:26.273776: W C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-21 20:12:26.516570: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX 670\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.98\r\npciBusID 0000:02:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.64GiB\r\n2017-07-21 20:12:26.516852: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0 \r\n2017-07-21 20:12:26.516992: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y \r\n2017-07-21 20:12:26.517145: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)\r\n4.0\r\n8.0\r\n2017-07-21 20:12:26.926502: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)\r\n12.0\r\n24.0\r\n\r\nProcess finished with exit code 0\r\n```", "comments": ["I simplified the example some more and tried storing the iterator to feed using `tf.add_to_collections()` and using that to get the string handle instead of storing the string handle directly. I get a different error this time regarding initialization, but it exits fine.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.data import Dataset, Iterator\r\n\r\nBATCH_SIZE = 2\r\nITERATION_COUNT = 1\r\ndataset = Dataset.from_tensor_slices(tf.constant([[0, 0],\r\n                                                  [0, 1],\r\n                                                  [1, 0],\r\n                                                  [1, 1]], dtype=tf.float32))\r\niterator_handle_placeholder = tf.placeholder(tf.string, shape=[])\r\niterator = Iterator.from_string_handle(iterator_handle_placeholder, dataset.output_types, dataset.output_shapes)\r\ndataset_iterator = dataset.make_one_shot_iterator()\r\n\r\nelement = iterator.get_next()\r\nv = tf.get_variable('v', initializer=tf.zeros([]))\r\n\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session() as session:\r\n    session.run(tf.global_variables_initializer())\r\n    dataset_iterator_handle = session.run(dataset_iterator.string_handle())\r\n    tf.add_to_collection('iterator_handle_placeholder', iterator_handle_placeholder)\r\n    tf.add_to_collection('iterator', dataset_iterator)\r\n    tf.add_to_collection('element', element)\r\n    output = session.run(element, {iterator_handle_placeholder: dataset_iterator_handle})\r\n    print(output)\r\n    saver.save(session, 'checkpoints/fufu')\r\n\r\nwith tf.Session() as session:\r\n    saver = tf.train.import_meta_graph('checkpoints/fufu.meta')\r\n    saver.restore(session, 'checkpoints/fufu')\r\n    iterator_handle_placeholder = tf.get_collection('iterator_handle_placeholder')[0]\r\n    dataset_iterator_handle = session.run(tf.get_collection('iterator')[0].string_handle())\r\n    element = tf.get_collection('element')[0]\r\n    output = session.run(element, {iterator_handle_placeholder: dataset_iterator_handle})\r\n    print(output)\r\n```\r\nError message is \r\n```\r\nC:\\Users\\Jonathan\\Miniconda3\\envs\\ai\\python.exe \"C:/Software Projects/ai/tensorflow/datasettest.py\"\r\n2017-07-21 22:15:13.371342: W C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-21 22:15:13.612687: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX 670\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.98\r\npciBusID 0000:02:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.64GiB\r\n2017-07-21 22:15:13.612996: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0 \r\n2017-07-21 22:15:13.613141: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y \r\n2017-07-21 22:15:13.613302: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)\r\n[ 0.  0.]\r\nWARNING:tensorflow:Error encountered when serializing iterator.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef.\r\n'Iterator' object has no attribute 'name'\r\n2017-07-21 22:15:13.999859: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:02:00.0)\r\n[ 0.  0.]\r\n\r\nProcess finished with exit code 0\r\n```", "Hello eddiemundo,\r\n\r\n I'm facing exactly the same issue, did you manage to solve it, any advices about how to process ?\r\nThank you a lot", "I successfully saved and restored the model only with the iterator handle placeholder. But you should initialize a new iterator. \r\nOne strange thing is that the placeholder for the handle should have a name parameter. Without naming, the type and content is not correctly loaded when I call get_collection.", "I was also struggling with this. I tried various things along the lines of what eddiemundo did, writing iterators to collections. I also tried  [`tf.contrib.data.make_saveable_from_iterator()`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/make_saveable_from_iterator) (I managed to successfully save an iterator here but not get it back!). No success with this approach.\r\n\r\nI eventually realised what youngwoo-yoon meant by \"new iterator\" -  you can just make an entirely new iterator in your restored model and use that instead. You still need to grab the handle from the saved model though and call `iterator.string_handle()` on the new iterator. Works with both one shot and initialisable iterators.\r\n\r\nHere's a simple example with `make_one_shot_iterator()`:\r\n\r\n```python\r\n# Tensorflow 1.8.0\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef save(dataset):\r\n    \"\"\"\r\n    Create graph with an Dataset and Iterator and save the model.\r\n\r\n    There is some op that is applied to the data from the iterator.\r\n    \"\"\"\r\n    iterator_handle = tf.placeholder(tf.string, shape=[])\r\n    tf.add_to_collection('iterator_handle', iterator_handle)\r\n\r\n    iterator = tf.data.Iterator.from_string_handle(\r\n        iterator_handle,\r\n        dataset.output_types,\r\n        dataset.output_shapes)\r\n    dataset_iterator = dataset.make_one_shot_iterator()\r\n    element = iterator.get_next()\r\n\r\n    some_op = tf.multiply(element, 0.5)\r\n    tf.add_to_collection('some_op', some_op)\r\n    v = tf.get_variable('v', initializer=tf.zeros([]))  # Needs a variable to save the model\r\n\r\n    saver = tf.train.Saver()\r\n\r\n    with tf.Session() as session:\r\n        session.run(tf.global_variables_initializer())\r\n        handle_val = session.run(dataset_iterator.string_handle())\r\n        for _ in range(4):\r\n            print(session.run(some_op,\r\n                feed_dict={iterator_handle: handle_val}))\r\n        saver.save(session, 'checkpoints/fufu')\r\n\r\ndef restore(dataset):\r\n    \"\"\"Restore the model from file and pass some new data through it\"\"\"\r\n    with tf.Session() as session:\r\n        saver = tf.train.import_meta_graph('checkpoints/fufu.meta')\r\n        saver.restore(session, 'checkpoints/fufu')\r\n        iterator_handle = tf.get_collection('iterator_handle')[0]\r\n        # Make new iterator\r\n        iterator = dataset.make_one_shot_iterator()\r\n        new_handle = session.run(iterator.string_handle())\r\n        # Don't need to call iterator.get_next() again as `some_op` will use\r\n        # restored `element`\r\n        some_op = tf.get_collection('some_op')[0]\r\n        for _ in range(4):\r\n            print(session.run(some_op, {iterator_handle: new_handle}))\r\n\r\nif __name__ == '__main__':\r\n\r\n    raw_data = np.array([[0, 0],\r\n                     [0, 1],\r\n                     [1, 0],\r\n                     [1, 1]])\r\n    dataset1 = tf.data.Dataset.from_tensor_slices(tf.constant(raw_data, dtype=tf.float32))\r\n    dataset2 = tf.data.Dataset.from_tensor_slices(tf.constant(raw_data * 2, dtype=tf.float32))\r\n\r\n    save(dataset1)\r\n\r\n    # Restore works with any data of the same shape in a tf.data.Dataset\r\n    # To use different shaped data use initialisable iterator\r\n    restore(dataset1)\r\n    restore(dataset2)\r\n```", "Here's another example with initialisable iterators.\r\n\r\n```python\r\n# Tensorflow 1.8.0\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef train(train_dataset, test_dataset):\r\n    \"\"\"\r\n    Create graph with an Dataset and Iterator and save the model.\r\n\r\n    There is some op that is applied to the data from the iterator.\r\n    \"\"\"\r\n    iterator_handle = tf.placeholder(tf.string, shape=[])\r\n    tf.add_to_collection('iterator_handle', iterator_handle)\r\n\r\n    iterator = tf.data.Iterator.from_string_handle(\r\n        iterator_handle,\r\n        train_dataset.output_types,\r\n        train_dataset.output_shapes)\r\n    train_iter = train_dataset.make_initializable_iterator()\r\n    test_iter = test_dataset.make_initializable_iterator()\r\n    element = iterator.get_next()\r\n\r\n    some_op = tf.multiply(element, 0.5)\r\n    tf.add_to_collection('some_op', some_op)\r\n    v = tf.get_variable('v', initializer=tf.zeros([]))\r\n\r\n    saver = tf.train.Saver()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        train_handle = sess.run(train_iter.string_handle())\r\n        test_handle = sess.run(test_iter.string_handle())\r\n\r\n        # Run data iterator initialisation\r\n        sess.run(train_iter.initializer)\r\n        sess.run(test_iter.initializer)\r\n\r\n        # \"Training\"\r\n        print(\"Training\")\r\n        while True:\r\n            try:\r\n                print(sess.run(some_op,\r\n                    feed_dict={iterator_handle: train_handle}))\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n        # \"Test evaluation\"\r\n        print(\"Testing\")\r\n        while True:\r\n            try:\r\n                print(sess.run(some_op,\r\n                    feed_dict={iterator_handle: test_handle}))\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n        saver.save(sess, 'checkpoints/fufu')\r\n\r\ndef eval(dataset):\r\n    \"\"\"Restore the model from file and pass some new data through it\"\"\"\r\n    with tf.Session() as sess:\r\n        saver = tf.train.import_meta_graph('checkpoints/fufu.meta')\r\n        saver.restore(sess, 'checkpoints/fufu')\r\n        iterator_handle = tf.get_collection('iterator_handle')[0]\r\n        # Make new iterator\r\n        iterator = dataset.make_one_shot_iterator()\r\n        new_handle = sess.run(iterator.string_handle())\r\n        # Don't need to call iterator.get_next() again as `some_op` will use\r\n        # restored `element` \r\n        some_op = tf.get_collection('some_op')[0]\r\n\r\n        # \"Further evaluation\"\r\n        print(\"More testing\")\r\n        while True:\r\n            try:\r\n                print(sess.run(some_op,\r\n                    feed_dict={iterator_handle: new_handle}))\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\nif __name__ == '__main__':\r\n\r\n    train_dataset = tf.data.Dataset.from_tensor_slices(\r\n        tf.constant(np.random.randint(0, 100, (5, 2)), dtype=tf.float32))\r\n    test_dataset = tf.data.Dataset.from_tensor_slices(\r\n        tf.constant(np.random.randint(0, 100, (2, 2)), dtype=tf.float32))\r\n\r\n    train(train_dataset, test_dataset)\r\n\r\n    # Now want to evaluate the results of another test dataset\r\n    another_test_dataset = tf.data.Dataset.from_tensor_slices(\r\n        tf.constant(np.random.randint(0, 100, (4, 2)), dtype=tf.float32))\r\n    eval(another_test_dataset)\r\n```", "Your way of saving an iterator handle and then feeding it back in is the only way that I can get to work. Thanks!\r\n\r\nIs this the most efficient way to do this? It seems like having to do an op lookup from a handle for every train step is less efficient than just grafting in your new iterator one time during graph setup. \r\n\r\nWhat made the most sense to me would have been to save a reinitializable iterator into the metagraph, and then to restore that iterator and call `sess.run(reinitializable_iterator.make_initializer(my_new_dataset))`. That only needs to happen once per iterator initialization instead of every train step.  But, I was unable to save an reinitializable iterator to the metagraph and then properly restore it. After a few hours of trying, I gave up and switched to feedable.\r\n\r\nI think that attaching a new Dataset iterator to a restored model is the most common use case for metagraphs, so I think that these examples you're giving here should be in the docs. If it is indeed the *best* way to do this.\r\n", "Glad someone found the examples helpful. I was going a bit nuts not being able to find an example of what I thought would be a fairly common use case. \r\n\r\nI have no idea RE efficiency of this. I agree that using a `reinitializable_iterator` seems much more elegant, but like you I haven't managed to get it working with a restored model. The op lookup from a handle is _probably_ OK (feels like using `get_variable()` elsewhere in the graph to get some weights), but this is mere conjecture. \r\n\r\nThe great irony of all this is I've not noticed any difference between using `tf.data.Dataset` + iterators and just passing in data via a `feed_dict` like most of the tutorials do. I'm assuming there's an amount of data where using iterators becomes better but I've not hit it yet. ", "Since we both encountered the same difficulty and solved it in the same way, I think it's a good enough hint that the docs should be updated. Filed a new bug.", "Thank you, @annarailton for sharing that complete example.\r\n In addition to training and then loading the saved model in the development phase, I want to load and continue training the saved model. \r\nEverything works fine, but there is a small problem here. Whenever I restore the previous model and resume training, operations related to creating new datasets and iterators are added to the existing graph. This makes the graph bigger and bigger every time that I load it for further training. To be able to see this problem you need to visualize the computation graph in **Tensorboard**.\r\nThe following is a modified version of your code to make the problem clear.  Any thoughts or suggestions?\r\n```python\r\n# Tensorflow 1.8.0\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport shutil\r\n\r\n\r\ndef train(train_dataset, test_dataset):\r\n    \"\"\"\r\n    Create graph with an Dataset and Iterator and save the model.\r\n\r\n    There is some op that is applied to the data from the iterator.\r\n    \"\"\"\r\n    iterator_handle = tf.placeholder(tf.string, shape=[])\r\n    tf.add_to_collection('iterator_handle', iterator_handle)\r\n\r\n    iterator = tf.data.Iterator.from_string_handle(\r\n        iterator_handle,\r\n        train_dataset.output_types,\r\n        train_dataset.output_shapes)\r\n    train_iter = train_dataset.make_initializable_iterator()\r\n    test_iter = test_dataset.make_initializable_iterator()\r\n    element = iterator.get_next()\r\n\r\n    v = tf.get_variable(name='v', initializer=tf.zeros(shape=(1, 2)))\r\n    \r\n    # to use when saving summaries\r\n    global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int32)\r\n    increament_global_step = tf.assign(global_step, global_step + 1)\r\n    global_step = global_step + 1\r\n    tf.add_to_collection('increament_global_step', increament_global_step)\r\n    \r\n    some_op = tf.assign(v,  v + tf.abs(element))\r\n    tf.add_to_collection('some_op', tf.reduce_sum(some_op))\r\n\r\n    tf.summary.scalar('v_sum', tf.reduce_sum(v))\r\n    tf.summary.scalar('some_op', tf.reduce_mean(some_op))\r\n    merged_summary = tf.summary.merge_all()\r\n    tf.add_to_collection('merged_summary', merged_summary)\r\n    \r\n    writer = tf.summary.FileWriter('checkpoints', graph=tf.get_default_graph())\r\n    saver = tf.train.Saver()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        train_handle = sess.run(train_iter.string_handle())\r\n        test_handle = sess.run(test_iter.string_handle())\r\n\r\n        # Run data iterator initialisation\r\n        sess.run(train_iter.initializer)\r\n        sess.run(test_iter.initializer)\r\n\r\n        # \"Training\"\r\n        print(\"Training\")\r\n        while True:\r\n            try:\r\n                [op, summary_values, g_step] = sess.run([some_op, merged_summary, increament_global_step],\r\n                                                feed_dict={iterator_handle: train_handle})\r\n                writer.add_summary(summary_values, global_step=g_step)\r\n                print(op)\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n        # \"Test evaluation\"\r\n        print(\"Testing\")\r\n        while True:\r\n            try:\r\n                print(sess.run(some_op,\r\n                    feed_dict={iterator_handle: test_handle}))\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n        saver.save(sess, 'checkpoints/fufu')\r\n\r\n\r\ndef eval(dataset):\r\n    \"\"\"Restore the model from file and pass some new data through it\"\"\"\r\n    with tf.Session() as sess:\r\n        saver = tf.train.import_meta_graph('checkpoints/fufu.meta')\r\n        saver.restore(sess, 'checkpoints/fufu')\r\n        iterator_handle = tf.get_collection('iterator_handle')[0]\r\n        # Make new iterator\r\n        iterator = dataset.make_one_shot_iterator()\r\n        new_handle = sess.run(iterator.string_handle())\r\n        # Don't need to call iterator.get_next() again as `some_op` will use\r\n        # restored `element`\r\n        some_op = tf.get_collection('some_op')[0]\r\n\r\n        # \"Further evaluation\"\r\n        print(\"More testing\")\r\n        while True:\r\n            try:\r\n                print(sess.run(some_op,\r\n                    feed_dict={iterator_handle: new_handle}))\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n\r\ndef resume_training(train_dataset, test_dataset):\r\n    \"\"\"Restore the model from file and pass some new data through it\r\n     for further training \"\"\"\r\n    with tf.Session() as sess:\r\n        saver = tf.train.import_meta_graph('checkpoints/fufu.meta')\r\n        saver.restore(sess, 'checkpoints/fufu')\r\n        iterator_handle = tf.get_collection('iterator_handle')[0]\r\n        some_op = tf.get_collection('some_op')[0]\r\n        increament_global_step = tf.get_collection('increament_global_step')[0]\r\n        merged_summary = tf.get_collection('merged_summary')[0]\r\n\r\n        writer = tf.summary.FileWriter('checkpoints',\r\n                                       graph=tf.get_default_graph())\r\n\r\n        # Make new iterators and handles\r\n        train_iter = train_dataset.make_initializable_iterator()\r\n        test_iter = test_dataset.make_initializable_iterator()\r\n\r\n        train_handle = sess.run(train_iter.string_handle())\r\n        test_handle = sess.run(test_iter.string_handle())\r\n\r\n        # Further training the model using new datasets (which may be different from original ones)\r\n        print(\"Resume training ...\")\r\n\r\n        train_handle = sess.run(train_iter.string_handle())\r\n        test_handle = sess.run(test_iter.string_handle())\r\n\r\n        # Run data iterator initialisation\r\n        sess.run(train_iter.initializer)\r\n        sess.run(test_iter.initializer)\r\n\r\n        # \"Training\"\r\n        print(\"Training\")\r\n        while True:\r\n            try:\r\n                [op, summary_values, g_step] = sess.run([some_op, merged_summary, increament_global_step],\r\n                                                feed_dict={\r\n                                                    iterator_handle: train_handle})\r\n                writer.add_summary(summary_values, global_step=g_step)\r\n                print(op)\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n        # \"Test evaluation\"\r\n        print(\"Testing\")\r\n        while True:\r\n            try:\r\n                print(sess.run(some_op,\r\n                               feed_dict={iterator_handle: test_handle}))\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n        saver.save(sess, 'checkpoints/fufu')\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    flag = 3  # 1 for train, 2 for test, 3 to resume training\r\n    if flag == 1:\r\n        # delete existing saved models and summary files\r\n        if os.path.exists('checkpoints'):\r\n            shutil.rmtree('checkpoints')\r\n        train_dataset = tf.data.Dataset.from_tensor_slices(\r\n            tf.constant(np.random.randint(0, 100, (5, 2)), dtype=tf.float32))\r\n        test_dataset = tf.data.Dataset.from_tensor_slices(\r\n            tf.constant(np.random.randint(0, 100, (2, 2)), dtype=tf.float32))\r\n\r\n        train(train_dataset, test_dataset)\r\n    elif flag == 2:\r\n        # Now want to evaluate the results of another test dataset\r\n        another_test_dataset = tf.data.Dataset.from_tensor_slices(\r\n            tf.constant(np.random.randint(0, 100, (4, 2)), dtype=tf.float32))\r\n        eval(another_test_dataset)\r\n\r\n    elif flag == 3:\r\n        # Load and fine-tune the saved model using new data\r\n        another_train_dataset = tf.data.Dataset.from_tensor_slices(\r\n            tf.constant(np.random.randint(0, 100, (10, 2)), dtype=tf.float32))\r\n        another_test_dataset = tf.data.Dataset.from_tensor_slices(\r\n            tf.constant(np.random.randint(0, 100, (8, 2)), dtype=tf.float32))\r\n\r\n        resume_training(another_train_dataset, another_test_dataset)\r\n"]}, {"number": 11678, "title": "BUILD Failed: missing input file '@mkl//:LICENSE'.", "body": "\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: latest\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:0.4.5\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: bazel build --config=mkl  -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n\r\nI was trying to build Tensorflow. and hitting this error missing input file '@mkl//:LICENSE'.\r\ni did it a week back, no problem faced. everything worked fine with same command line\r\n\r\n", "comments": ["Meet the same issue, too.", "getting following error message during build in Ubuntu,\r\n\"missing input file '@androidsdk//:build-tools/25.0.2/aapt'.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\"\r\n", "Sorry for the inconvenience, this is probably due to my change to MKL build.\r\n\r\nI have an internal change which should fix MKL build once pushed externally.\r\nMonday or Tuesday we should have a push, which should fix the problem.\r\n\r\n", "@Ishi-Apps your issue is quite different, I recommend filing a new issue by filling out the full template.", "@gunan we are seeing the MKL issue.", "https://github.com/tensorflow/tensorflow/commit/5442825dfd774a1d1f55d4d0e9d2821abb551caf is now pushed out.\r\n@agramesh1 could you sync past that commit and try again to see if the issue is resolved?", "@gunan The issues is resolved in the commit. Thanks."]}, {"number": 11677, "title": "Does orthogonal initialization have GPU implementation (fails when explicitly assigned on a gpu)?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.2.1\r\n- **Python version**:  3.4.3\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**:  8.0/6.0\r\n- **GPU model and memory**: Tesla P100 (16 gb)\r\n\r\n### Describe the problem\r\nI get an error when trying to use orthogonal initialization which is explicitly assigned to be run on GPU. Looking at the log it looks like some parts of it (e.g. QR decomposition) are available on CPU only. Is this the case or is there some bug that is blocking it from being run on GPU only?\r\n\r\n### Source code / logs\r\nThe following code could be used to reproduce the issue:\r\n```python\r\nimport tensorflow as tf\r\nwith tf.device('/gpu:0'):\r\n    a = tf.get_variable('a', (10, 3, 3, 20), tf.float32, tf.orthogonal_initializer())\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n```\r\nThe error log:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1284     try:\r\n-> 1285       return fn(*args)\r\n   1286     except errors.OpError as e:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1254       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1255       self._extend_graph()\r\n   1256       with errors.raise_exception_on_not_ok_status() as status:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1315           tf_session.TF_ExtendGraph(\r\n-> 1316               self._session, graph_def.SerializeToString(), status)\r\n   1317         self._opened = True\r\n\r\n/usr/lib/python3.4/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInvalidArgumentError: Cannot assign a device for operation 'a': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: GPU CPU \r\nReshape: GPU CPU \r\nRealDiv: GPU CPU \r\nAbs: GPU CPU \r\nDiagPart: CPU \r\nStridedSlice: GPU CPU \r\nQr: CPU \r\nAdd: GPU CPU \r\nMul: GPU CPU \r\nIdentity: CPU \r\nVariableV2: GPU CPU \r\nRandomStandardNormal: GPU CPU \r\nMinimum: GPU CPU \r\nConst: GPU CPU \r\nPack: GPU CPU \r\n\t [[Node: a = VariableV2[_class=[\"loc:@a\"], container=\"\", dtype=DT_FLOAT, shape=[10,3,3,20], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-f5bccc7031ec> in <module>()\r\n      5 \r\n      6 sess = tf.Session()\r\n----> 7 sess.run(tf.global_variables_initializer())\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    894     try:\r\n    895       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 896                          run_metadata_ptr)\r\n    897       if run_metadata:\r\n    898         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1123       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1124                              feed_dict_tensor, options, run_metadata)\r\n   1125     else:\r\n   1126       results = []\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1277     if handle is None:\r\n   1278       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1279                            options, run_metadata)\r\n   1280     else:\r\n   1281       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1296         except KeyError:\r\n   1297           pass\r\n-> 1298       raise type(e)(node_def, op, message)\r\n   1299 \r\n   1300   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Cannot assign a device for operation 'a': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: GPU CPU \r\nReshape: GPU CPU \r\nRealDiv: GPU CPU \r\nAbs: GPU CPU \r\nDiagPart: CPU \r\nStridedSlice: GPU CPU \r\nQr: CPU \r\nAdd: GPU CPU \r\nMul: GPU CPU \r\nIdentity: CPU \r\nVariableV2: GPU CPU \r\nRandomStandardNormal: GPU CPU \r\nMinimum: GPU CPU \r\nConst: GPU CPU \r\nPack: GPU CPU \r\n\t [[Node: a = VariableV2[_class=[\"loc:@a\"], container=\"\", dtype=DT_FLOAT, shape=[10,3,3,20], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n\r\nCaused by op 'a', defined at:\r\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-f5bccc7031ec>\", line 4, in <module>\r\n    a = tf.get_variable('a', (10, 3, 3, 20), tf.float32, tf.orthogonal_initializer())\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\r\n    validate_shape=validate_shape)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 283, in _init_from_args\r\n    name=name)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/state_ops.py\", line 131, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 684, in _variable_v2\r\n    name=name)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2576, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'a': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: GPU CPU \r\nReshape: GPU CPU \r\nRealDiv: GPU CPU \r\nAbs: GPU CPU \r\nDiagPart: CPU \r\nStridedSlice: GPU CPU \r\nQr: CPU \r\nAdd: GPU CPU \r\nMul: GPU CPU \r\nIdentity: CPU \r\nVariableV2: GPU CPU \r\nRandomStandardNormal: GPU CPU \r\nMinimum: GPU CPU \r\nConst: GPU CPU \r\nPack: GPU CPU \r\n\t [[Node: a = VariableV2[_class=[\"loc:@a\"], container=\"\", dtype=DT_FLOAT, shape=[10,3,3,20], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n```", "comments": ["Please try adding `allow_soft_placement=True` as follows:\r\n```\r\nconfig = tf.ConfigProto(allow_soft_placement = True)\r\nsess = tf.Session(config = config)\r\n```\r\n\r\nI encourage you to follow a guide here [here](https://www.tensorflow.org/tutorials/using_gpu) ", "Closing as `allow_soft_placement=True` is the solution here."]}, {"number": 11676, "title": "Error in tf.contrib.layers.batch_norm when explicitly assigned on gpu", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.2.1\r\n- **Python version**:  3.4.3\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**:  8.0/6.0\r\n- **GPU model and memory**: Tesla P100 (16 gb)\r\n\r\n### Describe the problem\r\nBatch norm layer fails with an error when explicitly assigned to be run on gpu and zero_debias_moving_mean is False. Interesting that I'm getting this error only when is_training is a placeholder (passing just True doesn't reproduce the error). If commented line is used instead (zero_debias_moving_mean=True) the code also works without any error.\r\n\r\n### Source code / logs\r\nThe following code could be used to reproduce the issue:\r\n```python\r\nimport tensorflow as tf\r\nis_training = tf.placeholder(tf.bool, name='is_training')\r\na = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))\r\nwith tf.device('/gpu:0'):\r\n    b = tf.contrib.layers.batch_norm(a, is_training=is_training)\r\n#     b = tf.contrib.layers.batch_norm(a, is_training=is_training, zero_debias_moving_mean=True)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n```\r\nThe error log:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1284     try:\r\n-> 1285       return fn(*args)\r\n   1286     except errors.OpError as e:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1254       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1255       self._extend_graph()\r\n   1256       with errors.raise_exception_on_not_ok_status() as status:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1315           tf_session.TF_ExtendGraph(\r\n-> 1316               self._session, graph_def.SerializeToString(), status)\r\n   1317         self._opened = True\r\n\r\n/usr/lib/python3.4/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInvalidArgumentError: Cannot assign a device for operation 'BatchNorm/Reshape_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: BatchNorm/Reshape_2 = Reshape[T=DT_BOOL, Tshape=DT_INT32, _device=\"/device:GPU:0\"](is_training, BatchNorm/Reshape_2/shape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-285b7d56d919> in <module>()\r\n      8 #     b = tf.contrib.layers.batch_norm(a, is_training=is_training, zero_debias_moving_mean=True)\r\n      9 sess = tf.Session()\r\n---> 10 sess.run(tf.global_variables_initializer())\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    894     try:\r\n    895       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 896                          run_metadata_ptr)\r\n    897       if run_metadata:\r\n    898         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1123       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1124                              feed_dict_tensor, options, run_metadata)\r\n   1125     else:\r\n   1126       results = []\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1277     if handle is None:\r\n   1278       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1279                            options, run_metadata)\r\n   1280     else:\r\n   1281       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1296         except KeyError:\r\n   1297           pass\r\n-> 1298       raise type(e)(node_def, op, message)\r\n   1299 \r\n   1300   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Cannot assign a device for operation 'BatchNorm/Reshape_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: BatchNorm/Reshape_2 = Reshape[T=DT_BOOL, Tshape=DT_INT32, _device=\"/device:GPU:0\"](is_training, BatchNorm/Reshape_2/shape)]]\r\n\r\nCaused by op 'BatchNorm/Reshape_2', defined at:\r\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-285b7d56d919>\", line 7, in <module>\r\n    b = tf.contrib.layers.batch_norm(a, is_training=is_training)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 634, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/base.py\", line 492, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/base.py\", line 441, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py\", line 395, in call\r\n    decay = _smart_select(training, lambda: self.momentum, lambda: 1.)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py\", line 568, in _smart_select\r\n    pred = array_ops.reshape(pred, [1])\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2502, in reshape\r\n    name=name)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2576, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/igitman/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'BatchNorm/Reshape_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: BatchNorm/Reshape_2 = Reshape[T=DT_BOOL, Tshape=DT_INT32, _device=\"/device:GPU:0\"](is_training, BatchNorm/Reshape_2/shape)]]\r\n```\r\n", "comments": ["@zhangyaobit, do you have any ideas? Perhaps using fused_batch norm?", "This is caused by \"pred = array_ops.reshape(pred, [1])\" in smart_select in tensorflow/python/layers/normalization.py.\r\n\r\npred is a scalar Tensor of boolean type. However, the GPU kernel of reshape doesn't support boolean type, because of the type constraint [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reshape_op.cc#L31).\r\n@vrv, do you have some ideas why boolean is not supported?\r\n\r\n@fchollet, @sguada, as pred is a scalar, could this reshape be replaced by something else? ", "No idea why it isn't supported, perhaps it could be registered.  People only add registrations as they are needed.", "Adding `REGISTER_GPU_KERNEL(bool);` can fix the problem.", "Thanks taehoonlee@ for the fix!"]}, {"number": 11675, "title": "Add Raspberry PI build", "body": "Does what it says on the box; follow instructions in third_party/toolchains/cpus/arm/build_raspberry_pi.sh", "comments": ["I've got a gist showing how to build this on a clean docker image, and I've verified the resulting wheel works on my Pi!\r\nhttps://gist.github.com/petewarden/6aa9650fe72d88889d24efd76598f8cb\r\n\r\nAdding in @samjabrahams to the thread too, since he's the expert on Pi Python building.", "Very cool; I'm really happy to see this getting added in. I won't be able to test it personally until August (due to all my RPis being in storage), but I'm excited to give it a shot as soon as I can.\r\n\r\nCouple questions/comments:\r\n\r\n* [We're targeting the `armv6` instruction set](https://github.com/tensorflow/tensorflow/pull/11675/files#diff-baf419669ca66edf686a287c3dc3f23bR20)- is the plan to have Raspberry Pi 1/Zero support?\r\n* We're not using NEON in the current build (I'm assuming because of the ARMv6 target), but I think it's provides pretty significant optimizations (@petewarden knows more about the details of this than myself). Hopefully, the difference in speed between this and a build with NEON is negligible, but if not, would it be possible to consider adding an option to the build to use it? Granted, this might cause some headaches in the various scripts and would require using GCC 4.8 (I think we could swap in [this gnueabi](https://github.com/raspberrypi/tools/tree/master/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian/lib/gcc/arm-linux-gnueabihf/4.8.3), but it's older and also not my area of expertise).\r\n\r\nFinally, it's satisfying that #3469 is getting addressed almost exactly a year to the day after it was opened :D", "Anything else left to fix for this?  @petewarden should i move any of your docker comments into my PR; or do you want to have a separate .sh file after this one is pushed?  I'll be OOO for 2 weeks starting Wednesday, so any changes should happen by tomorrow.", "@samjabrahams happy to help.  i am happy to add an argument or some comments in the .sh file for making an optimized build for arm7; or alternatively building for arm7 + neon and showing how to make an arm6 build (probably leaning towards this latter approach).", "@ebrevdo I've got an outstanding PR to add ARM7/NEON support and update the comments here, can you take a look? https://github.com/ebrevdo/tensorflow/pull/3", "Looks good after a rebase.\r\nTest failures do not look related to me, we can rerun them after the rebase.", "Added @petewarden's changes to make default build armv7/NEON.  This required upgrading the version of Eigen we use.  @benoitsteiner is this OK?", "@benoitsteiner I believe this addresses issue https://github.com/tensorflow/tensorflow/issues/9697", "@tensorflow-jenkins test this please", "The remaining MacOS failures look like timeouts or flakes.", "Awesome! Once I find some time, I'll note in my repo that official support is available and link to the relevant instructions.", "I think we need to wait for pete to update the version of eigen and fix any\nbreakages before we can make it official.\n\nOn Jul 26, 2017 7:16 AM, \"Sam Abrahams\" <notifications@github.com> wrote:\n\n> Awesome! Once I find some time, I'll note in my repo that official support\n> is available and link to the relevant instructions.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11675#issuecomment-318121502>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2EniNKKRsYrVCUsRo19FMlk9X0sks5sR3RggaJpZM4Of8SU>\n> .\n>\n", "That's correct. We're blocked on #9697 for the Pi 3 build, and once that's done I also need to create a Jenkins CI build to run nightly, so we'll have automatically generated binaries for the wheel. I hope to get to that soon.", "Once done, will these wheels be available on PyPI for pip installation?\r\n\r\nhttps://pypi.python.org/pypi/tensorflow\r\n", "if we can figure out how to fake the architecture to bdist_wheel after the\ncross compile, maybe.\n\nOn Jul 29, 2017 8:08 AM, \"Preston Holmes\" <notifications@github.com> wrote:\n\n> Once done, will these wheels be available on PyPI for pip installation?\n>\n> https://pypi.python.org/pypi/tensorflow\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11675#issuecomment-318848174>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzu6XxauAHkk4qMOICAB_en1xJSdks5sS3UEgaJpZM4Of8SU>\n> .\n>\n", "I believe the grpc team sorted this out somehow - I can forward you an internal thread.\r\n\r\nhttps://pypi.python.org/pypi/grpcio/1.4.0", "I'm not sure what happened, (git-gone-funky?), but the update to the Eigen version did not make it into master:\r\nhttps://github.com/tensorflow/tensorflow/pull/11675/commits/c8485830c2fb5add5e6dcf5fad7d3920d59622c0#diff-455a4c7f8e22d7c514e8c2caa27506c5R162\r\n\r\nAs of the current [master commit](https://github.com/tensorflow/tensorflow/blob/91d0f2eaa8e8bf340c48a36f32140ef0cdbda0a8/tensorflow/workspace.bzl) the eigen archive is still set to rev f3a22f35b044\r\n\r\n", "@ptone Sorry, I should have been more clear in the comments. We're blocked on updating Eigen (#9697) because the newer Eigen version breaks on MacOS. We'll need to see if other Eigen versions work on both, but I'm not working on that at the moment.\r\n\r\nIf anyone wants, they can update the Eigen hash to the Pi 3-capable one locally and build the wheel, but we won't be able to support it officially until the MacOS issue is resolved.", "Hello @petewarden I'm trying to cross compile tensorflow for Raspberry PI 3 from Ubuntu 16.04LTS using the \"build_raspberry_pi.sh\" present inside \"/tensorflow/tools/ci_build/pi/\" with the following steps:-\r\n1:- Installing Bazel\r\n2:- Cloning the Tensorflow and runnining the \"build_raspberry_pi.sh\" \r\nbut not getting success!!\r\nCan you please guide me what are the needed steps to follow to cross compile!!! Thanks in Advance!!"]}, {"number": 11674, "title": "ServingInputReceiver passes Estimator model_fn only dictionary of features, but model_fn is allowed to take single tensor feature", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, I have written custom code. I have more discussion about the bug in this Jupyter notebook: https://gist.github.com/nkashy1/fc1ec4ee218963216dea3ab5242bf611\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nGoobuntu\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nPyPI\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n\r\n- **Python version**: \r\n2.7.6\r\n\r\n- **Bazel version (if compiling from source)**:\r\nNot relevant\r\n\r\n- **CUDA/cuDNN version**:\r\nNot relevant\r\n\r\n- **GPU model and memory**:\r\nNot relevant\r\n\r\n- **Exact command to reproduce**:\r\nCheck this notebook: https://gist.github.com/nkashy1/fc1ec4ee218963216dea3ab5242bf611\r\n\r\n### Describe the problem\r\nThe [tf.estimator.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) interface allows users to provide a `model_fn` which accepts features either within a single tensor or within a dictionary mapping strings to tensors.\r\n\r\nThe Estimator `export_savedmodel` method requires a `serving_input_receiver_fn` argument, which is a function of no arguments that produces a [ServingInputReceiver](https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver). The features tensors from this `ServingInputReceiver` are passed to the `model_fn` for serving.\r\n\r\nUpon instantiation, the `ServingInputReceiver` wraps single tensor features into a dictionary. This raises an error for estimators whose `model_fn` expects a single tensor as its `features` argument.\r\n\r\n### Source code / logs\r\nGist: https://gist.github.com/nkashy1/fc1ec4ee218963216dea3ab5242bf611\r\n\r\nYou can run that notebook to see log messages, etc.\r\n\r\n#### Misc\r\nPossibly related to this stackoverflow thread: https://stackoverflow.com/questions/42835809/how-to-export-estimator-model-with-export-savedmodel-function", "comments": ["Incidentally, there is no type check to ensure that the output of `serving_input_receiver_fn` is a `ServingInputReceiver`.\r\n\r\n(Not that there *should* be one. It would be pretty annoying.)", "@fchollet, @martinwicke, do you have any thoughts on how this should be handled?", "@nkashy1 \r\n\r\nFrom my understanding of export_savedmodel on custom models you will need to work with feature dicts for the export to work.\r\n\r\nI came across this same issue as I just pass tensors to the model. So what I did was check to see what type of features are being passed in (straight tensors or dict of features). If it is just tensors, do the normal code. If it is the features dict, then I convert it to tensors using input_from_feature_columns.\r\n```\r\nlogits = features\r\nif type(features) is dict:\r\n            input_layer = tensorflow.contrib.layers.input_from_feature_columns(features, self.feature_columns)\r\n            logits = input_layer\r\n```", "@michaelpetruzzellocivicom : That's right, you need feature dictionaries in your custom estimator as `ServingInputReceiver` is implemented, but the documentation for `input_fn` + `model_fn` claims that it can be bare tensors or feature dictionaries.\r\n\r\nAlthough one can convert to single tensor the way you did above, and also directly, the point is that the API is inconsistent. Either the API needs to be changed (so that `input_fn` and `model_fn` are no longer expected to take bare tensors, this is mostly a documentation change at this point, but an important design decision) or the implementation of `ServingInputReceiver` needs to be changed.", "@davidsoergel Can you comment on the ServingInputReceiver issue?", "I confirm this is annoying.\r\nThe previous contract was clear. Whatever the serving_input_fn function returns as \"features\" is the same thing that model_fn will accept as \"features\". Now there is this additional constraint for the features to be a dictionary. This is unexpected, undocumented and will make a lot of people stumble.\r\n\r\nIt is also unnecessary. Yes, you need to name your things properly in communication protocols but the serving_input_fn function is there precisely so that you can unpackage your data from the comms protocol and transform it it the \"features\" that your model understands.\r\n\r\nThe two lines where this now break seem to have been written recently and neither needs the features to be a dictionary. The breakage seems to be unintentional:\r\n(saved_model_export_utils.py L163) tests for \u201cif features\u201d which fails on a Tensor.\r\n(estimator.py L1269) accesses features.keys which fails.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@karmel This is another issue you could look at.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@karmel did you fix this? Should I close it?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We have a fix in review right now; will update this thread when that is ready.", "I was actually looking into this feature right now. Where can I check out the fix?", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We have added a [TensorServingInputReceiver](https://github.com/tensorflow/tensorflow/blob/72239e9b0432a26feadc2412abf89a8fb92828f0/tensorflow/python/estimator/export/export.py#L123) that can accept and pass along raw tensors. You can use this class instead of the normal ServingInputReceiver if you would like to avoid having your model_fn input wrapped in a dictionary. The [tests](https://github.com/tensorflow/tensorflow/blob/72239e9b0432a26feadc2412abf89a8fb92828f0/tensorflow/python/estimator/estimator_test.py#L1940) include an example of using this class with estimators and export_savedmodel.\r\n\r\nThis is scheduled to be released with v1.7. LMK if you have any questions, and thanks @nkashy1 for the report and samples. ", "This is not very well documented, I only saw ServingInputReceiver in the doc and got the problem that my features were not a dict. "]}, {"number": 11673, "title": "Error in tf.contrib.layers.batch_norm when center=False, data_format='NCHW' and zero_debias_moving_mean=True", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.2.1\r\n- **Python version**:  3.4.3\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**:  8.0/6.0\r\n- **GPU model and memory**: Tesla P100 (16 gb)\r\n\r\n### Describe the problem\r\nBatch norm layer fails with an error when both center=False, data_format='NCHW' and zero_debias_moving_mean=True arguments are used. It looks like the solution would be just adding additional if to check if beta is None in the same way it is done for gamma, but maybe there are some more dependencies.\r\n\r\n### Source code / logs\r\nThe following code could be used to reproduce the issue:\r\n```python\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))\r\nb = tf.contrib.layers.batch_norm(a, center=False, data_format='NCHW',\r\n                                 zero_debias_moving_mean=True)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n```\r\nThe error log:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)\r\n    489                 as_ref=input_arg.is_ref,\r\n--> 490                 preferred_dtype=default_dtype)\r\n    491           except TypeError as err:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    675         if ret is None:\r\n--> 676           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    677 \r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    120   _ = as_ref\r\n--> 121   return constant(v, dtype=dtype, name=name)\r\n    122 \r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    101   tensor_value.tensor.CopyFrom(\r\n--> 102       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    103   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    363     if values is None:\r\n--> 364       raise ValueError(\"None values not supported.\")\r\n    365     # if dtype is provided, forces numpy array to be the type\r\n\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)\r\n    503               observed = ops.internal_convert_to_tensor(\r\n--> 504                   values, as_ref=input_arg.is_ref).dtype.name\r\n    505             except ValueError as err:\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    675         if ret is None:\r\n--> 676           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    677 \r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    120   _ = as_ref\r\n--> 121   return constant(v, dtype=dtype, name=name)\r\n    122 \r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    101   tensor_value.tensor.CopyFrom(\r\n--> 102       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    103   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    363     if values is None:\r\n--> 364       raise ValueError(\"None values not supported.\")\r\n    365     # if dtype is provided, forces numpy array to be the type\r\n\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-c9cf0f67668a> in <module>()\r\n      3 \r\n      4 a = tf.placeholder(tf.float32, shape=(10, 10, 10, 10))\r\n----> 5 b = tf.contrib.layers.batch_norm(a, center=False, data_format='NCHW', zero_debias_moving_mean=True)\r\n      6 sess = tf.Session()\r\n      7 sess.run(tf.global_variables_initializer())\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    179       current_args = current_scope[key_func].copy()\r\n    180       current_args.update(kwargs)\r\n--> 181     return func(*args, **current_args)\r\n    182   _add_op(func)\r\n    183   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/contrib/layers/python/layers/layers.py in batch_norm(inputs, decay, center, scale, epsilon, activation_fn, param_initializers, param_regularizers, updates_collections, is_training, reuse, variables_collections, outputs_collections, trainable, batch_weights, fused, data_format, zero_debias_moving_mean, scope, renorm, renorm_clipping, renorm_decay)\r\n    806       mean = array_ops.reshape(mean, params_shape_broadcast)\r\n    807       variance = array_ops.reshape(variance, params_shape_broadcast)\r\n--> 808       beta = array_ops.reshape(beta, params_shape_broadcast)\r\n    809       if gamma is not None:\r\n    810         gamma = array_ops.reshape(gamma, params_shape_broadcast)\r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   2500   \"\"\"\r\n   2501   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\r\n-> 2502                                 name=name)\r\n   2503   return result\r\n   2504 \r\n\r\n~/Documents/weight-normalization-exps/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)\r\n    506               raise ValueError(\r\n    507                   \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\r\n--> 508                   (input_name, err))\r\n    509             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\r\n    510                       (input_name, op_type_name, observed))\r\n\r\nValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.\r\n```", "comments": ["Marking as contributions welcome, since it seems #11687 will fix this issue."]}, {"number": 11672, "title": "Update monitors.py", "body": "Renaming PrintTensor to Logging Tensor", "comments": ["Can one of the admins verify this patch?", "Up to Jianwei, but I'm assuming we're not taking renames here and will do any necessary renames during the move to core.", "Ah, I just saw a TODO on renaming", "Yeah, I think the blocker is doing the mass rename internally.", "@jhseu Only renaming the ones in contrib, right?", "@terrytangyuan I meant also updating internal Google code that might refer to it.\r\n\r\nGiven that there's a TF core equivalent called LoggingTensorHook, I'll close this out."]}, {"number": 11671, "title": "Update fft2d.cmake", "body": "The cmake fft2d library was installing to '(fft2d_INSTALL)' instead of the appropriate variable set by CMake. I suspect this was unintentional. \r\n\r\nChanging the parentheses to curly brackets fixes this.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 11670, "title": "Feature Request: Quantized DepthwiseConv2dNative ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: (master as of post) d3edb8c60ed4fd831d62833ed22f5c23486c561c\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nModels, such as MobileNet use _DepthwiseConv2dNative_. I couldn't find a quantized kernel and the quantization pass in _GraphTransform_ doesn't currently support DepthwiseConv2dNative: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/quantize_nodes.cc#L57\r\n\r\nAre there plans to provide a quantized version of DepthwiseConv2dNative?\r\n\r\n", "comments": ["I don't believe anyone is actively working on this at this time. \r\n@petewarden has more state on MobileNet and plans in this area, so he could confirm.\r\n\r\n(FYI @suharshs )", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "We have focused our 8 bit fixed point efforts on TensorFlow Lite (https://www.tensorflow.org/mobile/tflite/). This means the quantized kernels are implemented as TFLite kernels. The current flow is to train \"fake\" quantized models (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize) and then convert them to TFLite models with TOCO, the TFLite converter (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md).\r\n\r\nWe don't have quantized TensorFlow kernels on the roadmap, but TFLite's will grow. That being said contributions are always welcome!\r\n\r\nIf you are looking for quantized mobilenet_v1 models, we have pretrained checkpoints, corresponding TFLite buffers, frozen graphdefs, and training scripts available here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\n\r\nHope that helps!"]}, {"number": 11669, "title": "Branch 162749134", "body": "", "comments": []}, {"number": 11668, "title": "Update sample_distorted_bounding_box to use v2 kernel", "body": "This fix is a follow up of #10840 so that sample_distorted_bounding_box uses _v2 kernel to follow the\r\nAPI compatibility workflow (3 weeks).\r\n\r\nThis fix updates related tests so that redudant tests could be removed.\r\n\r\nThis fix fixes #10715.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11667, "title": "Update losses implementation", "body": "-Various replacements of deprecated functions with equivalent tf.losses\r\n-Minor HTTPS calls", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@vrv could you test it now?", "re-testing.  @tensorflow-jenkins test this please", "Can you remove the unrelated changes that aren't about losses? ", "@vrv i have reverted unrelated changes. I will just submit a separate PR of them.", "Thanks -- some of those changes seemed unnecessary so I didn't want to delay this PR for that."]}]