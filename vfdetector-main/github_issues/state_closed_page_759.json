[{"number": 30789, "title": "TRT: remove serialized native function from TRT Engine Op", "body": "Removing the serialized fallback from the TRTEngineOp, and removed the option to not have a native fallback. This gets around a protobuf size limit for large models (BERT-Large).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30789) for more info**.\n\n<!-- need_sender_cla -->", "@phillip-kravtsov Please sign CLA in order to proceed with next steps. Thanks!", "@phillip-kravtsov please sign the cla.", "Signed the NVIDIA Corporate CLA!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30789) for more info**.\n\n<!-- ok -->", "@aaroey I've reverted the extraneous formatting changes and made the changes you requested, except for the unit tests. The functionality of the `ToGraphDefWithIOPrefix` is the same as the functionality as in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/function.cc#L2192 , but with a different name remapping. \r\nWould it make more sense to change the one in function.cc to accept any renaming function, or keep the duplicate?", "@phillip-kravtsov Could you please resolve the conflicts? Thanks!", "@aaroey Thank you for the thorough review! I've made the major changes requested, excepting a couple of the concerns, on which I've commented above. I've also rebased the commits into one, and rebased onto master."]}, {"number": 30788, "title": "I improving the recognition rate about TensorFlow 2 programmer,Google", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04 x64\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:Tensorflow 2.0.0-beta0\r\n- Python version:Python 3.6\r\n- Installed using virtualenv? pip? conda?:Activate virtualenv,then pip install TensorFlow\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 1 vCore,1024RAM,Bandwidth 1000G \r\n\r\n\r\n\r\n**Describe the problem**\r\nI improving the recognition rate, I mean My train the model have hight sussessful than the guide\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nURL:https://github.com/nongxi/Machine-Learning-tensorfolw-/blob/master/TensorFlow%202.py\r\ncode:\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='sigmoid),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nlog:\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n2019-07-17 02:51:34.258226: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-07-17 02:51:34.287935: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n60000/60000 [==============================] - 4s 66us/sample - loss: 0.0537 - accuracy: 0.9822\r\nEpoch 2/5\r\n60000/60000 [==============================] - 4s 73us/sample - loss: 0.0475 - accuracy: 0.9845\r\nEpoch 3/5\r\n60000/60000 [==============================] - 5s 77us/sample - loss: 0.0451 - accuracy: 0.9852\r\nEpoch 4/5\r\n60000/60000 [==============================] - 4s 72us/sample - loss: 0.0421 - accuracy: 0.9858\r\nEpoch 5/5\r\n60000/60000 [==============================] - 4s 71us/sample - loss: 0.0412 - accuracy: 0.9860\r\n<tensorflow.python.keras.callbacks.History object at 0x7fec4cdd90b8>\r\n>>> model.evaluate(x_test, y_test)\r\n10000/10000 [==============================] - 1s 51us/sample - loss: 0.0678 - accuracy: 0.9813\r\n[0.06783692461897153, 0.9813]", "comments": ["Hi,\r\n\r\nNo offence, but your post makes no sense. Your reported results suggest that either you had an extremely lucky choice of initialization weights (which could be the case, and is nothing of real interest, especially as it cannot be easily reproduced since you did not set a seed), or you basically trained the model multiple times in a row (i.e. increased the number of epochs) - this is typically the loss and accuracy reached around 10th to 15th epoch on this dataset with this architecture.\r\n\r\nIn any case, this is not an \"issue\". Apart from the fact that I highly doubt those are results on the first few epochs, there is a natural variability to the metrics reached (given that there is some randomness in initial weights' generation and dropout processes), so I do not see what it is you are trying to express with this post.\r\n\r\nBy the way, there is a typo in your code. (missing `'` line 8)", "@nongxi \r\nGlad to see the improved results.I am closing this issue as this is not a bug or feature request.Thanks!"]}, {"number": 30787, "title": "can't call tf.keras.Model created with functional api with no input tensors", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI know how to create tf.keras.Model with functional api for sure. I created a tf.keras.Model with no input tensors with functional api. When I call it, tensorflow complains that\r\n>AttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe tf.keras.Model should return expected value rather than raising a error.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```Python\r\n#!/usr/bin/python3\r\n\r\nimport tensorflow as tf;\r\n\r\ndef model():\r\n  a = tf.keras.layers.Lambda(lambda x: tf.ones(x))((2,3,4));\r\n  return tf.keras.Model(inputs = [], outputs = a);\r\n\r\nb = model()();\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n>Traceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    b = model()();\r\n  File \"test.py\", line 7, in model\r\n    return tf.keras.Model(inputs = [], outputs = a);\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 133, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 161, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 267, in _init_graph_network\r\n    base_layer_utils.create_keras_history(self._nested_outputs)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 199, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 225, in _create_keras_history_helper\r\n    op = tensor.op  # The Op that created this Tensor.\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 987, in op\r\n    \"Tensor.op is meaningless when eager execution is enabled.\")\r\nAttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n\r\n\r\n", "comments": ["I think you are trying to treat `a` as a symbolic tensor, when in reality it is an eager tensor. When you create the Lambda layer, invoking the layer's `call` function with non-Input tensors in TF 2.0 builds the layer and returns a \"real-valued\" eager tensor, which is _not_ a symbolic tensor i.e. it cannot be used to represent the target output in instantiating a Model. Instead, do the following:\r\n\r\n`\r\n    class MyModel(tf.keras.Model):\r\n\r\n        def __init__(self, **kwargs):\r\n            super().__init__(**kwargs)\r\n            self.layer = tf.keras.layers.Lambda(lambda x: tf.ones(x))\r\n\r\n        def call(self, inputs):\r\n            return self.layer(inputs)\r\n            # this line is equivalent\r\n            # return tf.ones(inputs)\r\n`\r\n\r\nAfter calling the model once on real data i.e. anything that can be converted to eager tensors like numpy ndarrays etc., the model and all its component layers will be built. This way, you do not have to wrap every non-standard op in a Lambda layer, but still have access to all of the Model niceties (`trainable_variables`, `fit`, etc.)\r\n\r\nIf you want to keep as close to your example as possible, you can even hardcode the values you want to return in the `call` function and ignore the mandatory `inputs` argument", "thx got it. "]}, {"number": 30786, "title": "Feature Request: Adding a \"LayerListWrapper\"-like class for adding list of layers (Non-sequential) to keras model.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 2.0.0b0-gpu\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTypical scenario: Assume that one needs to add a list that contains variable number of `tf.keras.layers.Layer` to a keras Model. By specifying something like `self.list_of_layers = [...]` in the `__init__` method`tf.keras.Model` subclass objects, the model won't track the trainable variables within layers in the list. For now this can be hacked by appending it to self._layers (e.g. `self._layers += self.list_of_layers`) to make the model object track the variables created in the layers in the list. \r\n\r\nIt would be helpful if there were a class like pytorch's `ModuleLists` that wraps list of layers up and enables the model to \"register\" theses layers and the corresponding `tf.Variable` objects automatically. \r\n\r\n**Will this change the current api? How?**\r\nYes. New wrapper that accepts list of tf.keras.layers.Layer objects will be added. \r\nNot sure there will be any side effects\r\n**Who will benefit with this feature?**\r\nPeople who need to add non-sequential layers to `tf.keras.Model` subclass objects. \r\nPeople who are accustomed to the its equivalents in other deep learning framworks (e.g. `ModuleLists` in Pytorch). \r\n**Any Other info.**\r\nThis is also proposed in [this stackoverflow thread](https://stackoverflow.com/questions/56117745/how-to-use-a-list-of-layers-in-tensorflow-2-0).\r\n\r\nA simple user case\r\n```Python\r\nclass TextCNN(tf.keras.Model):\r\n    def __init__(self, embedding, n_kernels, channel_per_kernel, n_cls):\r\n        super(TextCNN, self).__init__()\r\n        self.embedding = embedding\r\n        self.convs = tuple(tf.keras.layers.Conv1D(channel_per_kernel, n_gram, strides=1, padding=\"same\") for n_gram in range(2, 2 + n_kernels))\r\n        # Temporary hack due to not finding a class like `ModuleLists` in pytorch. \r\n        self._layers += self.convs\r\n        self.temporal_pooling = tf.keras.layers.GlobalMaxPool1D(data_format=\"channels_last\")\r\n        self.output_fc = tf.keras.layers.Dense(n_cls)\r\n    \r\n    def call(self, x, training=False):\r\n        output = self.embedding(x)\r\n        output = [conv(output) for conv in self.convs]\r\n        output = tf.concat(output, axis=-1)\r\n        output = self.temporal_pooling(output)\r\n        output = self.output_fc(output)\r\n        return output\r\n```", "comments": ["Hi,\r\nI believe that if you change `self.convs` from a tuple to a list, you get the desired results.\r\n\r\n```python\r\nclass TextCNN(tf.keras.Model):\r\n    def __init__(self, embedding, n_kernels, channel_per_kernel, n_cls):\r\n        super(TextCNN, self).__init__()\r\n        self.embedding = embedding\r\n        self.convs = [\r\n            tf.keras.layers.Conv1D(channel_per_kernel, n_gram, strides=1, padding=\"same\")\r\n            for n_gram in range(2, 2 + n_kernels)\r\n        ]\r\n        self.temporal_pooling = tf.keras.layers.GlobalMaxPool1D(data_format=\"channels_last\")\r\n        self.output_fc = tf.keras.layers.Dense(n_cls)\r\n    \r\n    def call(self, x, training=False):\r\n        output = self.embedding(x)\r\n        output = [conv(output) for conv in self.convs]\r\n        output = tf.concat(output, axis=-1)\r\n        output = self.temporal_pooling(output)\r\n        output = self.output_fc(output)\r\n        return output\r\n```\r\n\r\nGenerate a model using this class constructor, build it and you should see the proper number of parameters in `model.summary()`.", "That worked and thank @pandrey-fr for the answer. \r\nI used tuple to prevent content of `self.convs` from being changed. Didn't know why tuple is not allowed in this case.   ", "You are welcome!\r\nI totally understand why you would intuitively go for a tuple instead of a list, and to be honest it could be nice if TF would recognize those (I think this has to do with wrapper classes implementation, but I do not know much about it). Anyway, what matters is that your code works :)"]}, {"number": 30785, "title": "[tflite] make coco_object_detection:run_eval build", "body": "fix a namespace problem so that we can build\r\n```\r\n//tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval\r\n```\r\n(because Google's internal environment is different from public TensorFlow repo?)\r\n@srjoglekar246 ", "comments": ["Turns out, this change needs to be made in our internal tooling to convert the namespace before syncing with Github. I will make the change, and it should be reflected in the repo within a day or so. Closing the PR, thanks for bringing this to our attention :-)", "@gbaned This should be closed since @srjoglekar246 fixed it already."]}, {"number": 30784, "title": "Where can I find binary tensorflow-gpu 1.12 for PPC64LE", "body": "The community-supported tensorflow-gpu is 1.14 which require CUDA 10 but I only have CUDA 9, does anyone know where can I find built tensorflow-gpu 1.12?", "comments": ["@derekhsu Please find the instructions mentioned in the [Tenosrflow](https://www.tensorflow.org/install/pip) website to install tensorflow.Thanks! ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@derekhsu Perhaps you can try,\r\n```pip install tensorflow-gpu==1.12```", "Also on ppc64le (RHEL 7.6). None of the versioned nor unversioned install commands work, e.g.\r\n```\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n  ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\nobviously due to the lack of matching wheels for this architecture.\r\nI guess the same would happen on the arm64 server I'd get next week...", "Confirmed on arm64. :'("]}, {"number": 30783, "title": "Phase 4 of XLA int8 convolution on CUDA", "body": "This is a breakdown of previous PR (https://github.com/tensorflow/tensorflow/pull/29158) per @timshen91 's suggestion.\r\nPLEASE NOTE: This PR DOES depend on (https://github.com/tensorflow/tensorflow/pull/30761, https://github.com/tensorflow/tensorflow/pull/30762,  and https://github.com/tensorflow/tensorflow/pull/30771).\r\n\r\n1. Change cudnn_pad_for_tensor_cores to cudnn_pad_for_convolutions and add padding for integer convolution to it.\r\n2. Remove checks for same types in convolution shape inference to allow mixing int8 activation with float bias.\r\n3. Rename InitializeFloatBuffer to InitializeBuffer and add S8 support to it.\r\n4. Add and modify tests for cudnn_pad_for_convolutions_test, cudnn_conv_rewriter_test.cc, and cudnn_fused_conv_rewriter_test.cc.", "comments": ["@yongfeng-nv Could you please resolve the conflicts? Thanks!", "@yongfeng-nv Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "@yongfeng-nv Could you please resolve the conflicts? Thanks!", "@yongfeng-nv  Still, conflicts appearing. Could you please resolve those? Thanks!", "Close this PR, as its change has been moved to https://github.com/tensorflow/tensorflow/pull/30771 and https://github.com/tensorflow/tensorflow/pull/31988."]}, {"number": 30782, "title": "Allow_soft_placement=True does not solve \"no supported kernel for GPU devices is available\" problem", "body": "I have got this error: \r\n` InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'model/inference/encoder_LSTM/bidirectional_rnn/fw/fw/Assert/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n`\r\n\r\nother people said that setting tf.ConfigProto to \"allow_soft_placement=True\" will probably solve this problem.\r\n\r\nBut it does not. Any other idea? Please help.\r\n\r\n", "comments": ["I got the same issue, did you solve it? @tdplaza", "I create estimator and preditor based on tensor2tensor following code\r\n\r\n```\r\nhp = create_hparams()\r\ndecode_hp = create_decode_hparams()\r\nrun_conf = t2t_trainer.create_run_config(hp)\r\nestimator = trainer_lib.create_estimator(\r\n    FLAGS.model,\r\n    hp,\r\n    run_conf,\r\n    decode_hparams=decode_hp,\r\n    use_tpu=FLAGS.use_tpu)\r\npredictor=tf.contrib.predictor.from_estimator(estimator, input_fn)\r\n```\r\n\r\nthen got\r\n\r\n> InvalidArgumentError: Cannot assign a device for operation transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]\r\n> ImageSummary: CPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention (ImageSummary) /device:GPU:0\r\n> \r\n> Op: ImageSummary\r\n> Node attrs: max_images=1, T=DT_FLOAT, bad_color=Tensor<type: uint8 shape: [4] values: 255 0 0...>\r\n> Registered kernels:\r\n>   device='CPU'\r\n> \t [[{{node transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention}}]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n\r\nwhen I print session_config\r\n`print(run_conf.session_config)`\r\n\r\nI got \r\n\r\n\r\n> gpu_options {\r\n>   per_process_gpu_memory_fraction: 0.95\r\n> }\r\n> **allow_soft_placement: true**\r\n> graph_options {\r\n>   optimizer_options {\r\n>     global_jit_level: OFF\r\n>   }\r\n> }\r\n> isolate_session_state: true\r\n\r\n\r\nstill can't solve this problem", "yes, I did. \r\n\r\nMy problem was, I need putting allow_soft_placement=True on both training, and inferencing config Session. \r\n\r\nMy problem occurs when I only put it on training config, and inferencing got above  error. \r\n\r\nWhen inferencing, make sure you put it on configProto, then create a Session with that ConfigProto. Then run the tf.global_variables_initilizer(). Then create a saver object and restore the session using that saver and checkpoint path. Like following. \r\n![image](https://user-images.githubusercontent.com/31118781/62921486-d62cd900-bde3-11e9-9d2b-c0a2a68f1340.png)\r\n\r\nIf this does not help, you need find a new way to fix. good luck bro\r\n", "> yes, I did.\r\n> \r\n> My problem was, I need putting allow_soft_placement=True on both training, and inferencing config Session.\r\n> \r\n> My problem occurs when I only put it on training config, and inferencing got above error.\r\n> \r\n> When inferencing, make sure you put it on configProto, then create a Session with that ConfigProto. Then run the tf.global_variables_initilizer(). Then create a saver object and restore the session using that saver and checkpoint path. Like following.\r\n> ![image](https://user-images.githubusercontent.com/31118781/62921486-d62cd900-bde3-11e9-9d2b-c0a2a68f1340.png)\r\n> \r\n> If this does not help, you need find a new way to fix. good luck bro\r\n\r\nThank you for your remind! I solved the problem just as you did. When I put config in predictor's config, the error does not show any more.\r\n\r\n```\r\nsession_config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\r\npredictor = tf.contrib.predictor.from_estimator(estimator, input_fn, config=session_config)\r\n```"]}, {"number": 30781, "title": "Fix incorrect default values of tf.sparse.to_dense", "body": "This fix tries to address the issue in #30750 where tf.sparse.to_dense without specifying default value explicitly leads to TypeError:\r\n```\r\nimport tensorflow as tf\r\nsample_string = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=['a', 'b'], dense_shape=[3, 4])\r\ntf.sparse.to_dense( sample_string )\r\n...\r\nTypeError: Expected string passed to parameter 'default_value' of op 'SparseToDense', got 0 of type 'int' instead. Error: Expected string, got 0 of type 'int' instead.\r\n```\r\n\r\nThe issue was that tf.sparse.to_dense use 0 as the default value\r\nwhich does not work well with string.\r\n\r\nThis fix changes from `default_value=0` -> `default_value=None`\r\nand use zeros instead.\r\n\r\nIt consists of an API change though the change is backward compatible.\r\n\r\nThis fix fixes #30750\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["@alextp @gbaned Previously python 3 testing failed due to the string vs. byte presentation in the tests. I have updated the PR and changed `[['a', '', ''], ['', '', 'b']]` => `[[b'a', b'', b''], [b'', 'b', b'b']]` in the test case setup. Now all tests should pass. Sorry for the additional inconvenience and let me know if there are any issues."]}, {"number": 30780, "title": "tf.keras.losses.categorical_crossentropy ignores batch size and dont have name keywordarg", "body": "I think this function is obsolete in Tensorflow since the class CategoricalCrossEntropy does the job in the standard way (i.e, compute loss by batch and associate with a name to be easier to debug). \r\n\r\nI suggest to drop this from next 1.14 version if iam not taking something wrong. \r\n\r\nThank you.", "comments": ["The class version compute the per-sample loss values and then applies sample weight, reduces loss as per the reduction specified. The categorical_crossentropy function just computes the per-sample loss values. We would like to keep all the class as well as the function versions of losses. "]}, {"number": 30779, "title": "TF-TRT slower than plain TF on FasterRCNN type networks. ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda-10.1\r\n- GPU model and memory: T4\r\n\r\n\r\n**Describe the current behavior**\r\nI have noticed that   throughput of FasterRCNN type object detector is slower with TF-TRT than just TF. \r\n \r\n**Describe the expected behavior**\r\nWas expecting to see  higher throughput  with TF-TRT.\r\n\r\n**Code to reproduce the issue**\r\nTry any off the shelf FasterRCNN network \r\n\r\n**Other info / logs**\r\nNone\r\n", "comments": ["@sgambient \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Hi, tried to recreate the problem with a Tensorflow Object Detection model. Looks like TF_TRT is not even able to convert.  The error I get is as follows: \r\n\r\n`ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node Preprocessor/map/while/ResizeToRange/strided_slice/stack}}`\r\n\r\n\r\n**My hacked up code follow**s: \r\n\r\n(https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb )\r\n\r\n```\r\n# cat object_detection_tutorial.py\r\nimport numpy as np                              \r\nimport os        \r\nimport six.moves.urllib as urllib                      \r\nimport sys                              \r\nimport tarfile\r\nimport tensorflow as tf                                                                              \r\nimport tensorflow.contrib.tensorrt as trt     \r\nimport zipfile                                                                                            \r\nimport time                      \r\n                                        \r\nfrom distutils.version import StrictVersion\r\nfrom collections import defaultdict\r\nfrom io import StringIO                                             \r\n#from matplotlib import pyplot as plt\r\nfrom PIL import Image               \r\n                                   \r\n# This is needed since the notebook is stored in the object_detection folder.\r\nsys.path.append(\"..\")              \r\nfrom object_detection.utils import ops as utils_ops                               \r\n                                                                              \r\nif StrictVersion(tf.__version__) < StrictVersion('1.12.0'):\r\n  raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')\r\n                                               \r\n# This is needed to display the images.            \r\n#%matplotlib inline                                                          \r\n                                                     \r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import visualization_utils as vis_util\r\n                                                \r\n# What model to download.\r\nMODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\r\nMODEL_NAME = 'faster_rcnn_resnet101_coco_2018_01_28'\r\nMODEL_FILE = MODEL_NAME + '.tar.gz'                                    \r\nDOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\r\n\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\nPATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\r\n                                                                                        \r\n# List of the strings that is used to add correct label for each box.    \r\nPATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')                                                  \r\n                                                                                \r\ndef download_ckpt():                                                                 \r\n    opener = urllib.request.URLopener()                                                     \r\n    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)                   \r\n    tar_file = tarfile.open(MODEL_FILE)                                      \r\n    for file in tar_file.getmembers():     \r\n      file_name = os.path.basename(file.name)                   \r\n      if 'frozen_inference_graph.pb' in file_name:                \r\n        tar_file.extract(file, os.getcwd())                                             \r\n                                        \r\ndef load_image_into_numpy_array(image):                                                                                                                                                               [107/1176]\r\n  (im_width, im_height) = image.size\r\n  return np.array(image.getdata()).reshape(\r\n      (im_height, im_width, 3)).astype(np.uint8)\r\n\r\n# For the sake of simplicity we will use only 2 images:\r\n# image1.jpg\r\n# image2.jpg\r\n# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\r\nPATH_TO_TEST_IMAGES_DIR = 'test_images'\r\nTEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\r\n\r\n# Size, in inches, of the output images.\r\nIMAGE_SIZE = (12, 8)\r\n\r\ndef build_graph (detection_graph):\r\n\r\n  with detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\r\n      serialized_graph = fid.read()\r\n      od_graph_def.ParseFromString(serialized_graph)\r\n      tf.import_graph_def(od_graph_def, name='')\r\n\r\n  with detection_graph.as_default():\r\n      # Get handles to input and output tensors\r\n      ops = tf.get_default_graph().get_operations()\r\n      all_tensor_names = {output.name for op in ops for output in op.outputs}\r\n      tensor_dict = {}\r\n      for key in [\r\n          'num_detections', 'detection_boxes', 'detection_scores',\r\n          'detection_classes', 'detection_masks'\r\n      ]:\r\n        tensor_name = key + ':0'\r\n        if tensor_name in all_tensor_names:\r\n          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\r\n              tensor_name)\r\n\r\n      if 'detection_masks' in tensor_dict:\r\n        # The following processing is only for single image\r\n        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\r\n        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\r\n        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\r\n        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\r\n        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\r\n        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\r\n        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\r\n            detection_masks, detection_boxes, image.shape[1], image.shape[2])\r\n        detection_masks_reframed = tf.cast(\r\n            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\r\n        # Follow the convention by adding back the batch dimension\r\n        tensor_dict['detection_masks'] = tf.expand_dims(\r\n            detection_masks_reframed, 0)\r\n\r\n      return tensor_dict\r\n\r\nclass TfTrt():                                                                                                                                                                                         [51/1176]\r\n\r\n    @staticmethod\r\n    def build(sess, graph):\r\n        tensor_dict = build_graph(graph)\r\n\r\n        outputl = []\r\n        for i,nname in enumerate(tensor_dict):\r\n            print( i, nname, tensor_dict[nname].name )\r\n            outputl.append(nname)\r\n        print ( \"outputl \", outputl)\r\n\r\n        # Freeze the graph:\r\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n        sess,\r\n        sess.graph_def,\r\n        output_node_names= outputl)\r\n\r\n        # remove training nodes\r\n        frozen_graph = tf.compat.v1.graph_util.remove_training_nodes(frozen_graph)\r\n        # Now you can create a TensorRT inference graph from your frozen graph\r\n\r\n        trt_graph = trt.create_inference_graph(\r\n            input_graph_def=frozen_graph,\r\n            outputs = outputl,\r\n            max_batch_size=1,\r\n            max_workspace_size_bytes= 1024*1024*1024,\r\n            precision_mode=\"FP16\")\r\n\r\n        output_nodes = tf.import_graph_def(\r\n            trt_graph,\r\n            return_elements = outputl\r\n        )\r\n\r\n        return output_nodes\r\n\r\n    @staticmethod\r\n    def run(sess, tensor_dict, image_tensor, image_np_expanded):\r\n        output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image_np_expanded})\r\n        return output_dict\r\n\r\nclass Tf():\r\n\r\n    @staticmethod\r\n    def build(sess, graph):\r\n        return build_graph(graph)\r\n\r\n    @staticmethod\r\n    def run(sess, tensor_dict, image_tensor, image_np_expanded):\r\n        output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image_np_expanded})\r\n        return output_dict\r\n\r\ndef run (RunClass):\r\n\r\n    category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\r\n    graph = tf.Graph()\r\n    #tensor_dict = RunClass.build(graph)\r\n\r\n    with tf.Session(graph=graph) as sess:\r\n        tensor_dict = RunClass.build(sess, graph)\r\n        file_writer = tf.summary.FileWriter('tensorboard', sess.graph)\r\n        image_path = TEST_IMAGE_PATHS[0]\r\n        print ( \"image_path {}\".format(image_path))\r\n        image = Image.open(image_path)\r\n        image = image.resize((640,480))\r\n        image_np = load_image_into_numpy_array(image)\r\n        image_np_expanded = np.expand_dims(image_np, axis=0)\r\n        image_tensor = graph.get_tensor_by_name('image_tensor:0')\r\n        time0 = time.time()\r\n        for i in range(1,10001):\r\n            # Actual detection.\r\n            #output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image_np_expanded})\r\n            output_dict = RunClass.run(sess, tensor_dict, image_tensor, image_np_expanded)\r\n            #output_dict = run_inference_for_single_image(sess, graph, image_np_expanded, tensor_dict)\r\n            time_taken = (time.time() - time0 )/(i * 1.0)\r\n            print ( \"i \", i, time_taken)\r\n        time_taken = (time.time() - time0 )/(i * 1.0)\r\n        print (\"time_taken \", time_taken)\r\n\r\n        # all outputs are float32 numpy arrays, so convert types as appropriate\r\n        output_dict['num_detections'] = int(output_dict['num_detections'][0])\r\n        output_dict['detection_classes'] = output_dict[\r\n            'detection_classes'][0].astype(np.int64)\r\n        output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\r\n        output_dict['detection_scores'] = output_dict['detection_scores'][0]\r\n        if 'detection_masks' in output_dict:\r\n          output_dict['detection_masks'] = output_dict['detection_masks'][0]\r\n        # Visualization of the results of a detection.\r\n        vis_util.visualize_boxes_and_labels_on_image_array(\r\n            image_np,\r\n            output_dict['detection_boxes'],\r\n            output_dict['detection_classes'],\r\n            output_dict['detection_scores'],\r\n            category_index,\r\n            instance_masks=output_dict.get('detection_masks'),\r\n            use_normalized_coordinates=True,\r\n            line_thickness=8)\r\n        im = Image.fromarray(image_np)\r\n        save_file = \"out-\"+os.path.basename(image_path)\r\n        im.save(save_file)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    run (TfTrt)\r\n```\r\n\r\n\r\n\r\n", "@sgambient Can you try both `tf-nighly` and TF1.14 and let us know whether the issue persists with the latest TF versions. Thanks!", "Hi, tried it on 1.14 and get the following error. \r\n\r\n```\r\nE tensorflow/core/grappler/grappler_item_builder.cc:631] Fetch node num_detections doesn't exist in graph\r\nTraceback (most recent call last):\r\n  File \"./object_detection_tutorial.py\", line 212, in <module>\r\n    run (TfTrt)\r\n  File \"./object_detection_tutorial.py\", line 167, in run\r\n    tensor_dict = RunClass.build(sess, graph)\r\n  File \"./object_detection_tutorial.py\", line 135, in build\r\n    precision_mode=\"FP16\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 51, in create_inference_graph\r\n    session_config=session_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1146, in create_inference_graph\r\n    converted_graph_def = trt_converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 298, in convert\r\n    self._convert_graph_def()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 226, in _convert_graph_def\r\n    self._run_conversion()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 204, in _run_conversion\r\n    graph_id=b\"tf_graph\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/grappler/tf_optimizer.py\", line 41, in OptimizeGraph\r\n    verbose, graph_id)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to import metagraph, check error log for more info.\r\n```", "Same crap out on tensorflow/tensorflow:devel-gpu \r\n\r\n```\r\n2019-08-03 01:33:54.323115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\n2019-08-03 01:33:55.446324: E tensorflow/core/grappler/grappler_item_builder.cc:631] Fetch node num_detections doesn't exist in graph\r\nTraceback (most recent call last):\r\n  File \"./object_detection_tutorial.py\", line 212, in <module>\r\n    run (TfTrt)\r\n  File \"./object_detection_tutorial.py\", line 167, in run\r\n    tensor_dict = RunClass.build(sess, graph)\r\n  File \"./object_detection_tutorial.py\", line 135, in build\r\n    precision_mode=\"FP16\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 51, in create_inference_graph\r\n    session_config=session_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1146, in create_inference_graph\r\n    converted_graph_def = trt_converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 298, in convert\r\n    self._convert_graph_def()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 226, in _convert_graph_def\r\n    self._run_conversion()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 204, in _run_conversion\r\n    graph_id=b\"tf_graph\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/grappler/tf_optimizer.py\", line 41, in OptimizeGraph\r\n    verbose, graph_id)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to import metagraph, check error log for more info.\r\n```", "It's possible the part  where you freeze the graph has some problem. It's cleaner and safer if you switch to  saved_model. \r\n\r\nThis might help: https://devtalk.nvidia.com/default/topic/1056440/create_inference_graph-error/\r\n", "Not possible for the environment I am working in. ", "Thanks @sgambient for the script. I can reproduce the error with 1.15rc1.\r\nThe problem is the `remove_training_nodes` will remove identity nodes like the ones in the `outoutl` list in your script. Adding `protected_nodes=outputl` to the call solved the problem for me. \r\n\r\nI'm closing, feel free to reopen if the problem persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30779\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30779\">No</a>\n"]}, {"number": 30778, "title": "Expose __version__ correctly at the top level", "body": "This fix tries to address the issue raised in #30769 where `tf.__version__` raises an AttributeError exception, not conforming to PEP 396.\r\n\r\nThe issue comes from the fact that the module wrapping removes any symboles with `_` by updating `__all__` so\r\n```\r\nfrom tensorflow_core import *\r\n```\r\ndoes not import `__version__` any more.\r\n\r\nThis fix explicitly import `__version__`:\r\n```\r\nfrom tensorflow_core import __version__\r\n```\r\n\r\nThis fix fixes #30769\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["For reference, the line of __version__ being hided is located in:\r\nhttps://github.com/tensorflow/tensorflow/blob/02433bb9c31e7c172145b8dd86af3e66005ab8f9/tensorflow/python/util/module_wrapper.py#L108-L111", "Thanks @yongtang for the PR! This has been fixed with https://github.com/tensorflow/tensorflow/commit/584a64fa377d3070a05753344a410a5c96685ffd and starting from 0717's nightlies. I'll close this PR."]}, {"number": 30777, "title": "Improved the docs for tensorflow.keras.callbacks.ModelCheckpoint's fi\u2026", "body": "\u2026lepath argument", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30777) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30777) for more info**.\n\n<!-- ok -->", "Can one of the admins verify this patch?"]}, {"number": 30776, "title": "freeze model for inference with output_node_name", "body": "I want to compile the TensorFlow Graph to Movidius Graph. I have used Model Zoo's `ssd_mobilenet_v1_coco` model to train it on my own dataset.\r\n\r\nThen I ran \r\n```\r\npython object_detection/export_inference_graph.py \\\r\n\t\t\t\t--input_type=image_tensor \\\r\n\t\t\t\t--pipeline_config_path=/home/redtwo/nsir/ssd_mobilenet_v1_coco.config \\\r\n                                --trained_checkpoint_prefix=/home/redtwo/nsir/train/model.ckpt-3362 \\\r\n\t\t\t\t--output_directory=/home/redtwo/nsir/output\r\n```\r\nwhich generates me `frozen_interference_graph.pb` & `saved_model/saved_model.pb`\r\n\r\n\r\n![11](https://user-images.githubusercontent.com/8083613/61330462-3fe5b180-a83d-11e9-99a5-7b63aa1b7d2a.png)\r\n\r\n![12](https://user-images.githubusercontent.com/8083613/61330465-4116de80-a83d-11e9-89cb-2d69e213391b.png)\r\n\r\n\r\nNow to convert this saved model into *Movidius graph*. There are commands given\r\n\r\n\r\nExport GraphDef file\r\n```\r\npython3 ../tensorflow/tensorflow/python/tools/freeze_graph.py \\\r\n\t\t--input_graph=inception_v3.pb \\\r\n\t\t--input_binary=true \\\r\n\t\t--input_checkpoint=inception_v3.ckpt \\\r\n\t\t--output_graph=inception_v3_frozen.pb \\\r\n\t\t--output_node_name=InceptionV3/Predictions/Reshape_1\r\n```\r\nFreeze model for inference\r\n```\r\npython3 ../tensorflow/tensorflow/python/tools/freeze_graph.py \\\r\n\t\t--input_graph=inception_v3.pb \\\r\n\t\t--input_binary=true \\\r\n\t\t--input_checkpoint=inception_v3.ckpt \\\r\n\t\t--output_graph=inception_v3_frozen.pb \\\r\n\t\t--output_node_name=InceptionV3/Predictions/Reshape_1\r\n```\r\n\r\nwhich can finally be feed to **NCS Intel Movidius SDK**\r\n\r\n```\r\nmvNCCompile -s 12 inception_v3_frozen.pb -in=input -on=InceptionV3/Predictions/Reshape_1\r\n```\r\n\r\nAll of this is given at Intel Movidius Website here: https://movidius.github.io/ncsdk/tf_modelzoo.html\r\n\r\n\r\n#### Question: My model was already trained i.e. `output/frozen_inference_graph`. Why do I again freeze it using `/slim/export_inference_graph.py` or it's the `output/saved_model/saved_model.py` that will go as input to `slim/export_inference_graph.py`??\r\n\r\nAll I want is **output_node_name=Inceptionv3/Predictions/Reshape_1**. How to get this output_name_name directory structure & anything inside it? I don't know what all it contains \r\n\r\n------------------------\r\n\r\n### System information\r\n- **What is the top-level directory of the model you are using**: \r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: TensorFlow installed with pip\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Bazel version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: V10.1.168/7.*\r\n- **GPU model and memory**: 2080Ti 11Gb\r\n- **Exact command to reproduce**: ", "comments": ["what output node should I use for model zoo's `ssd_mobilenet_v1_coco` model(trained on my own custom dataset)\r\n\r\n```\r\npython freeze_graph.py \\\r\n                 --input_graph=/path/to/graph.pbtxt \\\r\n                 --input_checkpoint=/path/to/model.ckpt-22480 \\\r\n                 --input_binary=false \\\r\n                 --output_graph=/path/to/frozen_graph.pb \\\r\n                 --output_node_names=\"the nodes that you want to output e.g. InceptionV3/Predictions/Reshape_1 for Inception V3 \"\r\n```\r\n\r\nThings I understand & don't understand: \r\ninput_checkpoint: \u2713 [check points that were created during training]\r\noutput_graph: \u2713 [path to output frozen graph]\r\nout_node_names: X\r\n\r\n\r\nI don't understand `out_node_names` parameter & what should inside this considering it's ssd_mobilnet not inception_v3 \r\n", "```\r\nA typical freeze_graph.py command looks like this :\r\n\r\npython freeze_graph.py --input_meta_graph model.ckpt.meta --output_node_names \"Model/Activation_8/softmax_output\" --output_graph model_frozen.pb --input_checkpoint model.ckpt --input_binary=true\r\n```\r\n\r\nThis answer is using a softmax at end. But I'm not sure what is my last layer/node \r\n\r\nhttps://software.intel.com/en-us/forums/computer-vision/topic/806596", "slim  has been discontinued by TF? https://github.com/tensorflow/models/issues/539#issuecomment-363954823 and NCS supports SLIM implementation.  ", "I want to make a \u201cfreeze_graph.pb\u201d but I cannot understand graph and cannot find \u201coutput_node_names\u201d can anyone please help me @AbhimanyuAryan @ymodak", "@pumpkinband it's yet not solved. Also I have switched to OpenVino. Since NCSDK v1 & v2 are both out of support. You can print out graph node names using tensorboard name: https://www.youtube.com/watch?v=QW6532LtiTc\r\n\r\nFollow this video", "Apologies for the delay in response.\r\nYou may also use [```netron```](https://github.com/lutzroeder/netron) for figuring out node names.\r\nTensorboard also provides the ability to visualize the network as you mentioned with [Op-level graph](https://www.tensorflow.org/tensorboard/graphs#op-level_graph) \r\nClosing since original issue is resolved. Thanks!"]}, {"number": 30775, "title": "Enable Use of Cudnn APIs for Backward Input Grouped Convolutions", "body": "As per @akuegel 's comment https://github.com/tensorflow/tensorflow/pull/30370#discussion_r301941616\r\nin https://github.com/tensorflow/tensorflow/pull/30370 , creating a new PR to enable use of cudnn for backward input grouped conv", "comments": ["@akuegel Hi, Could you please review this PR ?", "Hi @akuegel ... I need to make changes to this PR to make it complete. I am working on the depthwise backward filter conv pass like we discuss in https://github.com/tensorflow/tensorflow/pull/30370 and it is almost done. All tests pass and seems to improve mobilnet performance. I will put up a public PR soon. If you want that I enable cudnn for backward input after that PR is merged, I can do that and close this for now. Or I can make changes to this PR now. Please let me know what you think will be appropriate.", "@akuegel Sorry for the delay in pushing this commit. This was sitting on my machine for a while now. In my experiments, even I found that thunking grouped backward input conv to cudnn backwar APIs doesn't do much (unlike grouped backward filter). So till we can make cudnn better for this, I reverted `MatchBackwardInput` back to state where it thunks to cudnn fwd for group_count > 1. However, the existing code that was supposed to map grouped backward input to cudnn, IMO, was incorrect. I change that piece of code to do the right thing. Incase we decide to use cudnn backward for grouped backward inputs, this change in conjunction with the algebraic simplification pipeline I introduced in #31597, will clean up the final graph and eventually do the right thing.", "Can one of the admins verify this patch?"]}, {"number": 30774, "title": "What replaces tf.data.get_output_shapes in TF 2?", "body": "TF 2 Datasets don't appear to have the output_shape property and there is no tf.data.get_output_shapes. \r\n\r\nWhat do we use instead?", "comments": ["`tf.data.experimental.get_structure` should have shape information.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental/get_structure?hl=en", "@ppham27 Thank you! This could be made more obvious in the docs.", "@jbgh2 ,\r\nLooks like issue is resolved.Can i proceed for closure.Thanks!", "Closing since the issue is resolved.Thanks!"]}, {"number": 30773, "title": "[TF 2.0] Can not compile model more than once without running out of memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190702\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: V10.0.130, 7.3.1\r\n- GPU model and memory: Surface Book 1 Nvidia GPU\r\n\r\n**Describe the current behavior**\r\n\r\nBuild a Keras model, compile it, run it.\r\nTry and rebuild model with new parameters.\r\nResult: OOM on GPU. Memory has not been freed or re-used.\r\n\r\n**Describe the expected behavior**\r\n\r\nCompile a model more than once without the GPU running out of memory.\r\nMore specifically, be able to hyper-parameter tuning without restarting the Jupyter kernel.\r\n\r\nI've tried:\r\n-  set_memory_growth on the GPU\r\n- del model + gc.collect\r\n- clear_session\r\nnone of them help.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n#Import basics and check everything works\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\n\r\nprint(\"Versions:\", tf.version.VERSION, tf.version.GIT_VERSION)\r\nprint(\"GPU availablilty:\", tf.test.is_gpu_available())\r\nprint(\"Eager execution:\", tf.executing_eagerly())\r\n\r\n#Quick test\r\nx = [[2.]]\r\nm = tf.matmul(x, x)\r\nprint(\"hello, {}\".format(m))\r\n\r\ndef make_model(input_shape, n_hidden1=2049, n_hidden2=500, n_hidden3=180, batch_n_mom=0.99, dropout_rate=0.1):\r\n\r\n    from tensorflow.keras.initializers import he_normal\r\n    \r\n    stacked_ae = keras.models.Sequential([\r\n        keras.layers.Flatten(input_shape=input_shape),\r\n        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),\r\n        \r\n        keras.layers.Dense(n_hidden1, activation=\"selu\", name=\"he1\", kernel_initializer=he_normal(seed=27)),\r\n        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),\r\n        keras.layers.Dropout(dropout_rate),\r\n        \r\n        keras.layers.Dense(n_hidden2, activation=\"selu\", name=\"he2\", kernel_initializer=he_normal(seed=42)),\r\n        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),\r\n        \r\n        keras.layers.Dense(n_hidden3, activation=\"selu\", name=\"he3\", kernel_initializer=he_normal(seed=65)),\r\n        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),\r\n        \r\n        keras.layers.Dense(n_hidden2, activation=\"selu\", name=\"hd2\", kernel_initializer=he_normal(seed=42)),\r\n        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),\r\n        \r\n        keras.layers.Dense(n_hidden1, activation=\"selu\", name=\"hd1\", kernel_initializer=he_normal(seed=27)),\r\n        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),\r\n        keras.layers.Dropout(dropout_rate),\r\n        \r\n        keras.layers.Dense(input_shape[0] * input_shape[1], name=\"output\", kernel_initializer=he_normal(seed=62)),\r\n        keras.layers.Reshape(input_shape)\r\n    ])\r\n    \r\n    return stacked_ae\r\n\r\nimport numpy as np\r\n\r\n#Data doesn't matter\r\nx_train = np.ones((32,60,80))\r\ny_train = np.ones((32,60,80))\r\n\r\n#Once runs ok\r\ninput_shape = [60,80]\r\nae_model = make_model(input_shape)\r\nae_model.compile(loss=\"mse\",\r\n                 optimizer=keras.optimizers.Adam(learning_rate=0.001, decay=1e-6),\r\n                metrics=['accuracy'])\r\nprint(ae_model.summary())\r\n\r\n#Do something with the model\r\nhistory = ae_model.fit(x=x_train, y=y_train,  epochs=1, steps_per_epoch=1)\r\n\r\n#Second run, new model\r\nae_model = make_model(input_shape, n_hidden1=2150)\r\nae_model.compile(loss=\"mse\",\r\n                 optimizer=keras.optimizers.Adam(learning_rate=0.001, decay=1e-6),\r\n                metrics=['accuracy'])\r\nprint(ae_model.summary())\r\n\r\n#Run again. GPU OOM.\r\nhistory = ae_model.fit(x=x_train, y=y_train,  epochs=1, steps_per_epoch=1)\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n2019-07-16 16:24:06.019147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-07-16 16:24:12.775543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-16 16:24:12.789530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-07-16 16:24:12.799822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-07-16 16:24:12.813163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 517 MB memory) -> physical GPU (device: 0, name: GeForce GPU, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-07-16 16:24:12.847183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GPU major: 5 minor: 0 memoryClockRate(GHz): 0.993\r\npciBusID: 0000:01:00.0\r\n2019-07-16 16:24:12.868383: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-16 16:24:12.887076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-07-16 16:24:12.902106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GPU major: 5 minor: 0 memoryClockRate(GHz): 0.993\r\npciBusID: 0000:01:00.0\r\n2019-07-16 16:24:12.925257: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-16 16:24:12.946163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-07-16 16:24:12.958309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-16 16:24:12.977399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-07-16 16:24:12.988725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-07-16 16:24:13.001442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 517 MB memory) -> physical GPU (device: 0, name: GeForce GPU, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n\r\n", "comments": ["@jbgh2 ,\r\nWhile trying to run your code, I got the error, \r\n`NameError: name 'input_shape' is not defined`. \r\n\r\nThen, I have set `input_shape = (32,60,80)` and tried running again and I got the below error:\r\n\r\n`ValueError: Error when checking input: expected flatten_input to have 4 dimensions, but got array with shape (32, 60, 80)`.\r\n\r\nIn order to expedite the trouble-shooting process can you please provide the exact reproducible code. Thanks!", "@rmothukuru Fixed the code in the original post. input_shape = [60, 80] was missing. Sorry!", "Did you try restarting your kernel before compiling your model again? Also\r\nCan you try to execute your model from terminal may be a python script?", "Restarting the kernel seems to be the only way to be get the model to recompile. But for the use-case of hyperparameter search it's painful.\r\n\r\nI'll try it in a standalone python script.", "I get the same behavior if all the code is in a standalone file.\r\nI've attached the file, it just needs renaming.\r\n[OOM-bug-standalong.txt](https://github.com/tensorflow/tensorflow/files/3438554/OOM-bug-standalong.txt)\r\n\r\nHere's the crash log if you're interested.\r\n[OOM-crash-log.txt](https://github.com/tensorflow/tensorflow/files/3438557/OOM-crash-log.txt)", "Thanks for the code snippet. Can you try reducing ```batch_size``` and test again? \r\nIf not, then next thing can be to limit gpu memory growth.\r\n```tf.config.experimental.set_memory_growth```\r\nSee, https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/config/experimental/set_memory_growth\r\nhttps://www.tensorflow.org/beta/guide/using_gpu#limiting_gpu_memory_growth", "I tried reducing that batch size in `fit` to 2 and adding in `tf.config.experimental.set_memory_growth(physical_devices[0], True)`. Neither made a difference.\r\n\r\nHere's an updated example.\r\n[OOM-bug-standalone.py.txt](https://github.com/tensorflow/tensorflow/files/3453093/OOM-bug-standalone.py.txt)\r\n\r\n", "This is primarily a keras question. Assigning it to @karmel ", "I've reproduced the bug on Colab but it takes more than one compile to run out of memory.", "Can you please provide the amount of memory your GPU has?  You can discover this by running `nvidia-smi`.\r\n\r\n@reedwm any guesses on what's going on here?", "@sanjoy not sure. This is probably a memory leak. I can reproduce by building and running the model in a loop. On my 12GB Titan V, I got an OOM after 96 iterations.\r\n\r\n@robieta any known GPU memory leaks? If not, I can take a look.", "@jbgh2 \r\nPlease let us know if this is still an issue.", "I've moved on from the project so I don't have time to test it again. If the repro I provided still fails then the bug still exists :)", "I can no longer reproduce by building and running in a loop with the latest TF nightly. \r\n\r\nThe memory does go up the second time the model is build and compiled because Python hasn't freed the memory from the first model yet. If the model is built repeatedly in a loop, the Python garbage collector will periodically free the memory and the GPU memory usage never goes above around 1.6 GB. (Python will call `tf.Variable.__del__` which frees the GPU memory for the variable). You can explicitly free the memory of unreferenced models by calling `import gc; gc.collect()`.\r\n\r\nI think the memory would normally be freed as soon as there are no references to the model, but there is a reference cycle within Keras. /CC @tomerk", "I am not able to get the error reported with TF v2.5,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/48828c137c69ed31c8caf7fc282e81f1/untitled291.ipynb)..Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30773\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30773\">No</a>\n"]}, {"number": 30772, "title": "\"--config mkl\" => mkl_version.h: No such file or directory", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise Linux 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 8.2.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuilding TF master with \"--config mkl\" fails because it is looking for Intel MKL (not MKL-DNN).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build --config opt --config mkl //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\nIn file included from external/mkl_dnn/src/cpu/rnn/ref_rnn.hpp:28,\r\n                 from external/mkl_dnn/src/cpu/rnn/cell_gru_lbr.cpp:24:\r\nexternal/mkl_dnn/src/cpu/rnn/../gemm/os_blas.hpp:33:10: fatal error: mkl_version.h: No such file or directory\r\n #include \"mkl_version.h\"\r\n          ^~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n\r\n\r\nNote that mkl_version.h is an Intel MKL header, not an MKL-DNN header. It looks like the current MKL-DNN binary blob requires MKL by default.", "comments": ["Update:\r\n\r\nIf I add these lines (back) to .bazelrc:\r\n\r\n    build --spawn_strategy=standalone\r\n    build --strategy=Genrule=standalone\r\n\r\n...my build works again.\r\n\r\n(Somehow, the \"mkl_linux\" tree does not make its way into processwrapper-sandbox. I do not know enough about Bazel to understand what the underlying problem is here.)\r\n", "It looks like commit [7a2ce469a70e55855b09030a96d2f6eeb825e253](https://github.com/tensorflow/tensorflow/commit/7a2ce469a70e55855b09030a96d2f6eeb825e253) is the reason for this issue. Reverting this commit fixes the problem.", "@nhasabni Thanks. So you could reproduce the problem? I was not sure whether it was somehow my environment.\r\n\r\nP.S. Could I convince you to take a peek at (the unrelated) issue 30383 ?", "@plopresti Yes we could reproduce the issue. Also, for 30383, we are looking into it.", "@plopresti, can you confirm if you are still experiencing this issue? BTW  #30383 is fixed in 1.15", "@preethivenkatesh This issue is resolved.\r\n\r\nInitial testing on #30383 looks good; we are trying other cases and will update soon. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30772\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30772\">No</a>\n"]}, {"number": 30771, "title": "[XLA GPU] int8 convolution on CUDA", "body": "This is a breakdown of previous PR (https://github.com/tensorflow/tensorflow/pull/29158) per @timshen91 's suggestion.  It doesn't depend on https://github.com/tensorflow/tensorflow/pull/30761 or https://github.com/tensorflow/tensorflow/pull/30762.\r\n1. Allow convolution with integer input/kernel and float output/bias/scaling/side and disallow int8-to-int8 convolution node in XLA.\r\n2. Add a new traversal to cudnn_fused_convolution_rewriter to fuse clamping and data type conversion nodes with convolution custom-call node for int8 convolution.\r\n3. Set convolution layout constraints to NHWC for integer convolution.", "comments": ["@timshen91 I have updated the PR to address your comments.  Please review and approve it again.", "@yongfeng-nv Could you please resolve the conflicts? Thanks!", "@yongfeng-nv  gentle ping to resolve the conflicts. Thanks!", "The previous implementation requires a clamp<-128,127> on output for int8-to-float convolution.  This matches the current behavior of CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM.  However, the clamp is not supposed to be there for float output.  This commit removes the clamp from the corresponding patterns.", "Can one of the admins verify this patch?", "@cheshire, I have made changes to function names and return values.  Tests are in the next PR (https://github.com/tensorflow/tensorflow/pull/30783).  Please let me know if I miss any requested changes.", "I vaguely remember having this discussion, but could we have tests for rewrites? For examples, see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/tests/gemm_rewrite_test.cc", "Aha, just saw the previous comment!\r\nThis PR seems to enable quite a few new rewrites, could we test all of them? The single test in the next commit just tests that a certain rewrite does not succeed.\r\nAlso is it possible to run tests by running the optimization pipeline?", "@cheshire, \r\nRegarding the background about int32, Tim and I had discussion here (https://github.com/tensorflow/tensorflow/pull/30761 on/after July 22nd).  We decided to use XLA int32 convolution and a few other instructions to form patterns to model CuDNN int8 convolution semantics and replace the pattern with a single CuDNN custom call.\r\n\r\nRegarding your comment about the single test, are you referring to TestForwardInt8Convolution in cudnn_conv_rewriter_test.cc?  That's for the changes in cudnn_conv_rewriter.cc.  For the new rewrites in cudnn_fused_conv_rewriter.cc, I have added a few new tests.  For example: TestConvInt8ToInt8 tests fused_for_bias and fused_for_clamp in a case where fused_for_convert_to_float is not applicable; TestConvInt8ToFloat tests all three phases, where the last phase only deals with a convert node;  TestFusedConvInt8ToInt8 tests all three phases, where the last phase deals with both convert and clamping; and I have two more to test the last phase with side input variations.  \r\nAll positive new tests have run through RunAndCompare.  I will add MatchOptimizedHlo checks following gemm_rewrite_test.cc.", "Is it possible to move the relevant tests to this PR? It's better to keep the tests and code together.\r\nAdditionally, for a final commit, would it be possible to have a more self-descriptive name?\r\n\r\n> All positive new tests have run through RunAndCompare\r\n\r\nThat might not be enough, as it doesn't check that the rewrite has actually been completed.", "I can't just move the tests, because the source code changes in phase 4 are also required.  We may end up with combining both phases, if we have to have _rewriter.cc and _rewriter_tests.cc together.", "Changes to BufferComparator can go into it's own PR, right?\r\nOther than that, changes in part 4 look to be mostly tests, which should be fine to combine.", "> Changes to BufferComparator can go into it's own PR, right?\r\n> Other than that, changes in part 4 look to be mostly tests, which should be fine to combine.\r\n\r\nYes, BufferComparator can go by itself.  The combined PR will depend on the BufferComparator PR.   I will make the change.", "Phase 4, including updated tests, has been moved into this PR.\r\n", "@gbaned It isn't clear to me what the status of this PR is, I see the \"awaiting review\" label, a \"Changes requested\" note and a \"ready to pull\" label.  Which one is correct?", "This PR depends on another one, which updates the buffer comparator to deal with int8.\r\nThat other PR landed, but got rolled back due to the fact that sm_ version is bumped.\r\n\r\nThis is currently blocked by other PR (comparator + tests) with the same sm_ version.\r\nCan Github do dependencies between PRs?", "> Can Github do dependencies between PRs?\r\n\r\nNot sure, but that's probably not necessary.  I was just curious about the current status.", "@yongfeng-nv Can you please check build failures? Thanks!", "> @yongfeng-nv Can you please check build failures? Thanks!\r\n\r\n@gbaned I have submitted a fix to the failure under \"Ubuntu Sanity \u2014 Internal CI build failed\".  Please let me know if anything else to fix.", "@yongfeng-nv Merged, but this required a few changes: please try to take care of compiler warnings before submitting. I'll try to upgrade those to errors on the presubmission testing bots.", "> @yongfeng-nv Merged, but this required a few changes: please try to take care of compiler warnings before submitting. I'll try to upgrade those to errors on the presubmission testing bots.\r\n\r\nCan you show me the log with warnings?  I will fix them."]}, {"number": 30770, "title": "AttributeError: The layer has never been called and thus has no defined input shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker container\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary (pip)\r\n- TensorFlow version (use command below): 'v2.0.0-beta0-16-g1d91213fe7', '2.0.0-beta1'\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI'm currently trying to get a PMML version of a Tensorflow 2.0 model, and seem to be getting an unrelated error.\r\n\r\nAttributeError: The layer has never been called and thus has no defined input shape.\r\n\r\nI've looked through the things that theoretically should cause this error, and none seem to be relevant. Here's my code:\r\n\r\n```python\r\ndef create_and_train(x_training,y_training,n_cols_in,modelparams):\r\n    layers = [tf.keras.layers.Dense(n_cols_in,activation=\"relu\"),\r\n    tf.keras.layers.Dropout(.5)]\r\n    for param in modelparams:\r\n        layers.extend([tf.keras.layers.Dense(param,activation=\"sigmoid\"),tf.keras.layers.Dropout(.5)])\r\n    layers.append(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\r\n    model = tf.keras.models.Sequential(layers)\r\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\r\n    model.fit(x_training, y_training, epochs = epochs)\r\n    with open(\"NN\"+\"_\".join([str(m) for m in modelparams])+\".pmml\",\"w\") as pmml_file:\r\n        pmml = KerasToPmml(model)\r\n        pmml.export(pmml_file)\r\n```\r\n\r\nThe model gets trained, and if I tell it to give me an AUC for the training set or even a test set before the PMML export, it does so without a problem. However, PMML export requires input shape, which Tensorflow claims doesn't exist.", "comments": ["In order to expedite the trouble-shooting process, please provide a full code snippet to reproduce the issue reported here. Thanks!", "Sure, though the training data is probably not small enough for a Git message attachment and not publicly available online. For the minimal reproducible version, use the following:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom nyoka import KerasToPmml\r\nn_cols_in = 218\r\nmodelparams = [436,]\r\n# get training data here; I'll toss in a CSV version.\r\nimport pandas as pd\r\nlabelcol = 'MxWillReAdmit'\r\ntrainingdata = pd.read_csv('trainingfile.csv')\r\nx_training = np.array(trainingdata.drop(labelcol,axis='columns'))\r\ny_training = np.array(trainingdata[labelcol])\r\n\r\ndef create_and_train(x_training,y_training,n_cols_in,modelparams,epochs=10):\r\n    layers = [tf.keras.layers.Dense(n_cols_in,activation=\"relu\"),\r\n    tf.keras.layers.Dropout(.5)]\r\n    for param in modelparams:\r\n        layers.extend([tf.keras.layers.Dense(param,activation=\"sigmoid\"),tf.keras.layers.Dropout(.5)])\r\n    layers.append(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\r\n    model = tf.keras.models.Sequential(layers)\r\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\r\n    model.fit(x_training, y_training, epochs = epochs)\r\n    with open(\"NN\"+\"_\".join([str(m) for m in modelparams])+\".pmml\",\"w\") as pmml_file:\r\n        pmml = KerasToPmml(model)\r\n        pmml.export(pmml_file)\r\ncreate_and_train(x_training,y_training,n_cols_in,modelparams)\r\n```\r\n\r\nHow should I go about sharing the training data?", "Can you please share the sufficient dummy data to reproduce the issue.Thanks!", "Apologies for taking so long; I wanted to ensure the sample had the same problem, and for unrelated reasons ended up having to do a pretty thorough reinstallation on my system. The same problem still comes up with the data set I'm attaching.\r\n[trainingfile.zip](https://github.com/tensorflow/tensorflow/files/3412470/trainingfile.zip)\r\n\r\n", "I have tried on Jupyter notebook with TF version 2.0 beta and was able to reproduce the issue.Thanks!\r\n\r\n", "@dlepzelter The error trace looked more related to `nyoka` than `tensorflow. Can you create the issue without using `nyoka`. thanks!", "Certainly. Replace\r\n```python\r\n    with open(\"NN\"+\"_\".join([str(m) for m in modelparams])+\".pmml\",\"w\") as pmml_file:\r\n        pmml = KerasToPmml(model)\r\n        pmml.export(pmml_file)\r\n```\r\nwith\r\n```python\r\n    print(model.input_shape())\r\n```\r\nand you get the same error.", "@dlepzelter I can reproduce the error and the error `AttributeError: The layer has never been called and thus has no defined input shape.` clearly points out input shape was not defined. This `tf.keras.layers.Dense(n_cols_in,activation=\"relu\")` is your first layer in the model where `input_shape` was not defined.\r\n\r\nHere is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/78b70e6b720a0b2b847f649c3b5d4d15/tf_30770_inputshape.ipynb). Thanks!", "That makes sense, to a point. That said, the input shape is implicitly defined by the data itself, or the model wouldn't train or run. Is it intentional that an implicit input shape definition results in this error?", "@dlepzelter It is always suggested to use input_shape which is the best way to tell model what is expected. Model understand the shapes and infer the shape for the following layers.  \r\n\r\n`The model needs to know what input shape it should expect. For this reason, the first layer in a sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape.`\r\n\r\nWithout an input_shape, when you check the model.summary()\r\n\r\n`model.summary()`\r\n\r\n```\r\nModel: \"sequential_9\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_30 (Dense)             multiple                  47742     \r\n_________________________________________________________________\r\ndropout_20 (Dropout)         multiple                  0         \r\n_________________________________________________________________\r\ndense_31 (Dense)             multiple                  95484     \r\n_________________________________________________________________\r\ndropout_21 (Dropout)         multiple                  0         \r\n_________________________________________________________________\r\ndense_32 (Dense)             multiple                  437       \r\n=================================================================\r\nTotal params: 143,663\r\nTrainable params: 143,663\r\nNon-trainable params: 0\r\n_______________________\r\n```\r\nModel doesn't understand the what is expected shape. So, when I provided input_shape, the model summary looks clear as below. Please notice that there is no change in `trainable params` before or after adding `input_shape`\r\n\r\n```\r\nModel: \"sequential_10\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_33 (Dense)             (None, 218)               47742     \r\n_________________________________________________________________\r\ndropout_22 (Dropout)         (None, 218)               0         \r\n_________________________________________________________________\r\ndense_34 (Dense)             (None, 436)               95484     \r\n_________________________________________________________________\r\ndropout_23 (Dropout)         (None, 436)               0         \r\n_________________________________________________________________\r\ndense_35 (Dense)             (None, 1)                 437       \r\n=================================================================\r\nTotal params: 143,663\r\nTrainable params: 143,663\r\nNon-trainable params: 0\r\n```\r\n[Here is the gits](https://colab.sandbox.google.com/gist/jvishnuvardhan/06e920de7e91af651d3a67b78d715421/tf_30770_inputshape.ipynb#scrollTo=2K0qo-CkIPJA) for your reference. \r\n\r\nI am closing this issue. Please feel free to open the issue if it persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30770\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30770\">No</a>\n", "@jvishnuvardhan, If this input_shape is such an important aspect in writing the model architecture then it should be added in the documentation. The current documentation doesn't even include this as an optional parameter. Neither does the tutorial example mentions this.\r\n\r\nAnd I wrote a small piece of code to check it myself and without providing input_shape I am able to get proper output shape in model summary. Here is the code.\r\nmodel = Sequential()\r\nmodel.add(Dense(32, input_shape=(16,)))\r\n# now the model will take as input arrays of shape (*, 16)\r\n# and output arrays of shape (*, 32)\r\n\r\n# after the first layer, you don't need to specify\r\n# the size of the input anymore:\r\nmodel.add(Dense(32))\r\n\r\nAnd output\r\n\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 32)                544       \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 32)                1056      \r\n=================================================================\r\nTotal params: 1,600\r\nTrainable params: 1,600\r\nNon-trainable params: 0\r\n\r\nNow my question is how the output shape is correct even after not defining input_shape? "]}, {"number": 30769, "title": "tf.__version__ raises an AttributeError exception", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n```python\r\ntf.version.VERSION='2.0.0-dev20190716'\r\ntf.version.GIT_VERSION='v1.12.1-6367-gb51a1b258b'\r\n```\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\n`tf.__version__` raises an `AttributeError` exception.\r\n\r\n**Describe the expected behavior**\r\n`tf.__version__` should return the version (=`tf.version.VERSION`) to respect [PEP 396](https://www.python.org/dev/peps/pep-0396/).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n```\r\n\r\n**Other info / logs**\r\nHere's the exception:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-d1382151fdd6> in <module>\r\n      1 import tensorflow as tf\r\n      2\r\n----> 3 print(tf.__version__)\r\n\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n```", "comments": ["To me it seems `__version__` should be initialized here \"/tensorflow/python/framework/versions.py\" \r\nHowever if I run the code as @ageron on the tf-nightly version I get the same issue.\r\nHere the full error log:\r\n```---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-1-f83c6d50081b> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 print(tf.__version__)\r\n\r\n1 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py in __getattr__(self, name)\r\n    169     except AttributeError as e:\r\n    170       if not self._tfmw_public_apis:\r\n--> 171         raise e\r\n    172       if name not in self._tfmw_public_apis:\r\n    173         raise e\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py in __getattr__(self, name)\r\n    166   def __getattr__(self, name):\r\n    167     try:\r\n--> 168       attr = getattr(self._tfmw_wrapped_module, name)\r\n    169     except AttributeError as e:\r\n    170       if not self._tfmw_public_apis:\r\n\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n```\r\n\r\nThe process of the declaration of the version is described nicely at [stackoverflow](https://stackoverflow.com/questions/50427475/checking-tensorflow-version-in-python-tf-version-vs-tf-version?rq=1)\r\nMaybe this helps find the source of this problem quicker.\r\n", "The issue is that module wrapping hides all symbols starts with `_` from `__all__` so `__version__` is hided as well. Created a PR #30778 for the fix.", "@ageron,\r\nIssue is fixed in latest TF 2.0 nightly version. Can we close the issue. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30769\">No</a>\n"]}, {"number": 30768, "title": "Cryptic error message when assigning to a variable in a tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=2.0.0-dev20190716\r\nGIT_VERSION=v1.12.1-6367-gb51a1b258b\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen assigning to a TF variable with the `=` operator in a TF Function, I get a really cryptic error message.\r\nThis issue is about the error message, not the error itself (I should have called the variable's assign() method).\r\n\r\n**Describe the expected behavior**\r\nI was hoping for a more explicit error message.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nv = tf.Variable(0.)\r\n\r\n@tf.function\r\ndef f(x):\r\n    global v\r\n    v = v + x  # no problem if use v.assign(v + x)\r\n    return v\r\n\r\nf(42.0)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2019-07-16 21:02:26.141110: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-16 21:02:26.154309: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe6734a6e50 executing computations on platform Host. Devices:\r\n2019-07-16 21:02:26.154329: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-16 21:02:26.222272: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1558] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-07-16 21:02:26.223137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Failed precondition: Error while reading resource variable _AnonymousVar0 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar0/N10tensorflow3VarE does not exist.\r\n\t [[{{node ReadVariableOp}}]]\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-1-a1cd73b43975> in <module>\r\n      9     return v\r\n     10\r\n---> 11 f(42.0)\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    433               *args, **kwds)\r\n    434       # If we did not create any variables the trace we have is good enough.\r\n--> 435       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    436\r\n    437     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    633          if isinstance(t, (ops.Tensor,\r\n    634                            resource_variable_ops.BaseResourceVariable))),\r\n--> 635         self.captured_inputs)\r\n    636\r\n    637   def _call_flat(self, args, captured_inputs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs)\r\n    743     # Only need to override the gradient in graph mode and when we have outputs.\r\n    744     if context.executing_eagerly() or not self.outputs:\r\n--> 745       outputs = self._inference_function.call(ctx, args)\r\n    746     else:\r\n    747       self._register_gradient()\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args)\r\n    457             attrs=(\"executor_type\", executor_type,\r\n    458                    \"config_proto\", config),\r\n--> 459             ctx=ctx)\r\n    460       # Replace empty list with None\r\n    461       outputs = outputs or None\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError:  Error while reading resource variable _AnonymousVar0 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar0/N10tensorflow3VarE does not exist.\r\n\t [[node ReadVariableOp (defined at /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1657) ]] [Op:__inference_f_15]\r\n\r\nFunction call stack:\r\nf\r\n```", "comments": ["I was able to replicate the issue with TF VERSION=2.0.0-dev20190716. Take a look at the Gist of [Colab link](https://colab.research.google.com/drive/1398dDTnZa2U3EDSYZQ3Zp5vwjR_mwe5i) .Thanks!", "The code snippet works with TF 1.14. However it fails after enabling eager execution. I guess it fails in 2.0 since we have eager execution activated by default.", "@ageron `tf.Variable` does not override arithmetic operators and in effect `v = v + x` will replace the `Variable` `v` with a `Tensor` - is this what you expected?\r\n\r\n@alextp @jaingaurav  I suspect the root cause here is that `tf.function` captures a weak reference to the variable, which is then deallocated. If that's indeed the case, then this example shows that the weakref should in fact be a strong reference and be tied to the graph that relies on it.\r\n\r\nNote: this error is unrelated to autograph; the snippet below throws the same error:\r\n\r\n```\r\nv = tf.Variable(0.)\r\n\r\n@tf.function(autograph=False)\r\ndef f(x):\r\n    global v\r\n    v = v + x  # no problem if use v.assign(v + x)\r\n    return v\r\n\r\nf(42.0)\r\n```", "The issue is that `v = v + x` doesn't assign to the variable, it throws the variable away (deleting it) and then assigns a tensor internal to the function (v+x) to the name `v`.\r\n\r\nBecause you threw the variable away and you're trying to execute a graph which reads the value of the variable (so it can compute `v+x`, which translates internally as `add(read_variable(v), x)`) we fail.\r\n\r\nThe error message could be better, but the behavior is WAI.", "Thanks @alextp, I know it is WAI, this issue is about the error message, as I wrote earlier:\r\n\"This issue is about the error message, not the error itself (I should have called the variable's assign() method).\"\r\nThe main complexity in TF 2 is in @tf.function, so its error messages should be as clear as possible, especially for common problems. I ran into this issue a couple times, so I'm guessing others will too.", "Was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/f198bc025239bf602f192a49c57f260e/30768-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/e459e9dbfcc52cd023047787bf0960d6/30768-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Was able to replicate the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/2f660e1ebe6fa9968f9893a8e93b85ba/30768-tf-nightly.ipynb) ..Thanks!", "Hi @ageron , Values can be  only added to variable using  **assign_add/assign** operations .For other variable operations  please refer to [this](https://www.tensorflow.org/api_docs/python/tf/Variable#methods_2) .  providing [gist](https://colab.research.google.com/gist/mohantym/07746dd82f590aee71dfae32c898f3b2/30768-tf-nightly.ipynb#scrollTo=JvDCJ6Cj_Kmt) for reference. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym , yes of course, that's why I wrote this: \"_This issue is about the error message, not the error itself (I should have called the variable's assign() method)._\". The error message is just as cryptic today with TF 2.6 as it was with TF 2.0.\r\n", "Quick note that we're still tracking this issue and intend to fix it, so it should stay open.\r\n\r\nIt might ultimately be easier do what'd be normally expected and use assign instead of raising an error.", "Hi @ageron! This issue has been addressed in [TF 2.8](https://colab.research.google.com/gist/mohantym/eab0952000bbeb1bbb8c806118d00013/untitled1.ipynb?authuser=1#scrollTo=HIwX5hFcjUqH) now. Can we move this issue to closed status now?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30768\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30768\">No</a>\n"]}, {"number": 30767, "title": "Minimum number of epochs before termination for tf.keras.callbacks.EarlyStopping()", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n1.14\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis feature is not yet implemented but is simple to implement. It requires an additional flag, storing one more `int`, and few more `if`-statements in tf.keras.callbacks.EarlyStopping().\r\n\r\n**Will this change the current api? How?**\r\nYes, in that one more variable will be available to be assigned when calling the EarlyStopping callback.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wishes to use EarlyStopping but wants to let the model train for a certain number of epochs before EarlyStopping is activated.\r\n\r\n**Any Other info.**\r\nN/A", "comments": ["I think that `patience` parameter is [what you need](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/EarlyStopping#arguments):\r\n> patience: Number of epochs with no improvement after which training will be stopped.", "Hmm, not quite. \r\n\r\nI'm saying there needs to be a `minimum` number of epochs before the `patience` parameter kicks in.\r\n\r\n@jvishnuvardhan @ravikyram, I'm happy to share the code and submit a PR.", "@joglekara Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature. \r\n\r\nFor most (or all) of the cases I have come across, patience param will be sufficient. In the initial stages of training, there will be high fluctuation in the metrics so it will earlystopping will not kickin for few iterations. Thanks! ", "Yes, definitely a few use cases for this feature.\r\n\r\nThe most straightforward one is if you're using a training schedule w.r.t. epoch, you may only want EarlyStopping to apply after a certain portion of the training schedule is reached. \r\n\r\nSounds good, I will submit a PR", "@joglekara Why PR was closed by you. Why there is no code in the PR? Please let me know if you are not interested in the PR or want to close this issue. Thanks!", "I closed my PR because I was having issues with the email address/CLA. Here's the [open PR](https://github.com/tensorflow/tensorflow/pull/30939)\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 30766, "title": "implementation of tf.dense_tensor_sparse_matmul() such that a matrix product AB can be computed where A is dense and B is sparse.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n1.14\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe feature is not implemented in a transparent fashion. The operation can be performed by using tf.sparse.sparse_tensor_dense_matmul() and some matrix algebra. It is a python-only implementation and does not require going into the C/CUDA code.\r\n\r\n**Will this change the current api? How?**\r\nIt will add a new function call within tf.sparse.\r\n\r\n**Who will benefit with this feature?**\r\nMost people who use the Sparse library.\r\n\r\n**Any Other info.**\r\nI've already implemented and tested this feature against NumPy's matrix math library. I wanted to bring it up and see if this is a desirable feature.\r\n", "comments": ["@ymodak @gadagashwini , I've already implemented this, I am happy to share, or even submit a PR if you like.", "Perhaps you can elaborate more on how the new proposed feature distinguishes itself from the current implementation of [tf.sparse.sparse_dense_matmul](https://www.tensorflow.org/api_docs/python/tf/sparse/sparse_dense_matmul)?", "The current implementation only allows a `left multiply` with a `SparseTensor`, but what if one needs to be able to perform a `right multiply` with a `SparseTensor`? One cannot simply reverse the order of operations because matrix products do not commute i.e. AB != BA. \r\n\r\nThis feature would enable the `right multiply`", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "This was merged into v2.1", "@joglekara Please tell us the function name"]}, {"number": 30765, "title": "Add the missing comma", "body": "A missing comma in a list of string means the string \"Rank\" and \"ShapeN\" will be concatenated implicitly.", "comments": []}, {"number": 30764, "title": "Cannot cross-compile for Raspberry Pi with official instructions in MacOS", "body": "\r\n**System information**\r\n- OSX 10.14.5\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: docker\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nI am following [this official guide](https://www.tensorflow.org/install/source_rpi) to cross-compile for the Raspberry Pi. I am doing:\r\n\r\n       tensorflow/tools/ci_build/ci_build.sh PI \\\r\n       tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\nThe process hungs (all CPUs being used 100% for a long long time) while compiling. Each time I kill it and the next time hungs at a different place. The scenario is always very similar.. I paste below some examples:\r\n\r\n### Example1\r\n\r\n\r\n\t[1,152 / 1,160] Compiling tensorflow/core/kernels/data/experimental/group_by_reducer_dataset_op.cc; 131s local ... (4 actions running)\r\n\tERROR: /workspace/tensorflow/core/kernels/BUILD:3114:1: C++ compilation of rule '//tensorflow/core/kernels:batch_matmul_op' failed (Exit 4): arm-linux-gnueabihf-gcc failed: error executing command\r\n\t  (cd /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n\t  exec env - \\\r\n\t    LD_LIBRARY_PATH='' \\\r\n\t    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n\t    PWD=/proc/self/cwd \\\r\n\t    PYTHON_BIN_PATH=/usr/bin/python \\\r\n\t    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n\t    TF_DOWNLOAD_CLANG=0 \\\r\n\t    TF_NEED_CUDA=0 \\\r\n\t    TF_NEED_OPENCL_SYCL=0 \\\r\n\t    TF_NEED_ROCM=0 \\\r\n\t  /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python2.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTF_USE_SNAPPY -DCURL_STATICLIB -iquote . -iquote bazel-out/armeabi-opt/genfiles -iquote bazel-out/armeabi-opt/bin -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-opt/genfiles/external/eigen_archive -iquote bazel-out/armeabi-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-opt/genfiles/external/local_config_sycl -iquote bazel-out/armeabi-opt/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/armeabi-opt/genfiles/external/nsync -iquote bazel-out/armeabi-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/armeabi-opt/genfiles/external/gif_archive -iquote bazel-out/armeabi-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/armeabi-opt/genfiles/external/jpeg -iquote bazel-out/armeabi-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/armeabi-opt/genfiles/external/protobuf_archive -iquote bazel-out/armeabi-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/armeabi-opt/genfiles/external/farmhash_archive -iquote bazel-out/armeabi-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-opt/genfiles/external/fft2d -iquote bazel-out/armeabi-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/armeabi-opt/genfiles/external/highwayhash -iquote bazel-out/armeabi-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/armeabi-opt/genfiles/external/zlib_archive -iquote bazel-out/armeabi-opt/bin/external/zlib_archive -iquote external/double_conversion -iquote bazel-out/armeabi-opt/genfiles/external/double_conversion -iquote bazel-out/armeabi-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/armeabi-opt/genfiles/external/snappy -iquote bazel-out/armeabi-opt/bin/external/snappy -iquote external/curl -iquote bazel-out/armeabi-opt/genfiles/external/curl -iquote bazel-out/armeabi-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/armeabi-opt/genfiles/external/boringssl -iquote bazel-out/armeabi-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/armeabi-opt/genfiles/external/jsoncpp_git -iquote bazel-out/armeabi-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/armeabi-opt/genfiles/external/aws -iquote bazel-out/armeabi-opt/bin/external/aws -isystem external/eigen_archive -isystem bazel-out/armeabi-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-opt/genfiles/external/nsync/public -isystem bazel-out/armeabi-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/armeabi-opt/genfiles/external/gif_archive/lib -isystem bazel-out/armeabi-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/armeabi-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/armeabi-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/armeabi-opt/genfiles/external/zlib_archive -isystem bazel-out/armeabi-opt/bin/external/zlib_archive -isystem external/double_conversion -isystem bazel-out/armeabi-opt/genfiles/external/double_conversion -isystem bazel-out/armeabi-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/armeabi-opt/genfiles/external/curl/include -isystem bazel-out/armeabi-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/armeabi-opt/genfiles/external/boringssl/src/include -isystem bazel-out/armeabi-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/armeabi-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/armeabi-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/armeabi-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/armeabi-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/armeabi-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-s3/include '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -DTENSORFLOW_MONOLITHIC_BUILD -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/batch_matmul_op_real.cc -o bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.o)\r\n\tExecution platform: @bazel_tools//platforms:host_platform\r\n\tcc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\n\tarm-linux-gnueabihf-gcc: internal compiler error: Killed (program cc1plus)\r\n\tPlease submit a full bug report,\r\n\twith preprocessed source if appropriate.\r\n\tSee <http://gcc.gnu.org/bugs.html> for instructions.\r\n\tINFO: Elapsed time: 781.885s, Critical Path: 218.64s\r\n\tINFO: 108 processes: 108 local.\r\n\tFAILED: Build did NOT complete successfully\r\n\tFAILED: Build did NOT complete successfully\r\n\t>>> elapsed time 16m45s\r\n\r\n### Example2\r\n\tERROR: /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/external/com_github_nanopb_nanopb/BUILD.bazel:1:1: C++ compilation of rule '@com_github_nanopb_nanopb//:nanopb' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command\r\n\t  (cd /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n\t  exec env - \\\r\n\t    LD_LIBRARY_PATH='' \\\r\n\t    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n\t    PWD=/proc/self/cwd \\\r\n\t    PYTHON_BIN_PATH=/usr/bin/python \\\r\n\t    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n\t    TF_DOWNLOAD_CLANG=0 \\\r\n\t    TF_NEED_CUDA=0 \\\r\n\t    TF_NEED_OPENCL_SYCL=0 \\\r\n\t    TF_NEED_ROCM=0 \\\r\n\t  /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/armeabi-opt/bin/external/com_github_nanopb_nanopb/_objs/nanopb/pb_decode.pic.d -fPIC '-DPB_FIELD_32BIT=1' -iquote external/com_github_nanopb_nanopb -iquote bazel-out/armeabi-opt/genfiles/external/com_github_nanopb_nanopb -iquote bazel-out/armeabi-opt/bin/external/com_github_nanopb_nanopb -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_github_nanopb_nanopb/pb_decode.c -o bazel-out/armeabi-opt/bin/external/com_github_nanopb_nanopb/_objs/nanopb/pb_decode.pic.o)\r\n\tExecution platform: @bazel_tools//platforms:host_platform\r\n\texternal/com_github_nanopb_nanopb/pb_decode.c:18:23: fatal error: pb_common.h: No such file or directory\r\n\t #include \"pb_common.h\"\r\n\t                       ^\r\n\tcompilation terminated.\r\n\tINFO: Elapsed time: 380.977s, Critical Path: 35.72s\r\n\tINFO: 390 processes: 390 local.\r\n\tFAILED: Build did NOT complete successfully\r\n\tFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Any other info / logs**\r\nThe very very same instructions, running on a Linux Mint 18.3 work perfectly. Docker version on OSX is \"Docker version 18.09.2, build 6247962\" and in Mint \"Docker version 18.09.6, build 481bc77\".  The documentation says nothing about which Docker version to use and it does says that MacOS is supported: \r\n\r\n> it's easier to build TensorFlow on a more powerful host machine running Linux, macOS, or Windows.\r\n\r\n\r\nAny ideas?", "comments": ["I finally took the time to [write down everything I found during my attempt to get TensorFlow C library compiled for Raspberry Pi](https://dev.to/martinezpeck/challenge-accepted-build-tensorflow-c-binding-for-raspberry-pi-in-2019-4f89). In that tutorial, I point back to this issue in case any progress is done.\r\nThank you", "@marianopeck Is this duplicate of https://github.com/tensorflow/tensorflow/issues/30716? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30763, "title": "Eager and Graph API should be more similar", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0b1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nEager mode just works. However, the moment you do @tf.function decorators to improve performance, you have to convert a bunch of commands from eager mode into graph mode. \r\n\r\nthere are **two ways to do everything in tensorflow: the eager way and the graph way.** This means we have no \"single source of truth\" (do i use eager which just works but is slow and has memory issues, or do i use graph mode which is lightning fast but requires a ton of extra work to \"translate between two dialects\" tensorflow eager into tensorflow graph)\r\n\r\n**Will this change the current api? How?**\r\nYes. This will dramatically simplify the tensorflow API, as there will be fewer redundant commands between graph mode and eager mode \r\n\r\n**Who will benefit with this feature?**\r\nEveryone who switches between eager and graph mode would save days of work on every project if this change were implemented\r\n\r\n**Any Other info.**\r\ntensorflow 2.0 is a golden opportunity to simplify TF we have less code to debug. I wish the team would merge eager and graph mode as much as possible:\r\n\r\n1. tf.shape() vs tensor.shape\r\n2. basic logic often fails in graph mode (for example, if x = \"somestring\") and there's not many good ways to know why\r\n3. input signature inference would save everyone a lot of time... retracing happens a lot in tf.function without signatures, especially on sequence data with variable shapes, so we have to write a bunch of (None, None ) to fix this ? that's not really informative. this should be automated. Maybe if tf.function sees a shape varying, it can handle this case automatically\r\n4. a nice way to set dtype at the top of the file would be handy\r\n5. a nice way to turn tensorboard tracing on globally would also be handy\r\n\r\n```\r\ndef trace_function(fn, *args):\r\n    tf.summary.trace_on(graph=True, profiler=True)\r\n    y = fn(*args)\r\n    with writer.as_default():\r\n        tf.summary.trace_export(name=\"trace_function\", step=0, profiler_outdir=log_dir)\r\n    return y\r\n```", "comments": ["in particular, simple logic breaks as soon as you add @tf.function\r\n\r\nfor example, if you have a multi-task model and the task names are strings, and your eager training loop does\r\n```\r\nif type != \"rxn\":\r\n```\r\nthis stops working in tf.function. Do we need to use some special tensorflow command? Is the string not converted to a tensor? It's so hard to debug this sort of thing because we don't know if it's the data or the API (because there's 2 versions of everything in tensorflow, you have to literally rewrite half your code just to rule out a data problem)", "`if step == 0` does not work\r\n`if step < 1` does ...\r\n```\r\n            if step < 1:\r\n                tf.print(step == 0)\r\n```\r\n\r\n> False", "```\r\n                tf.print(type)\r\n                tf.print(type == 'xyz')\r\n```\r\n\r\n> [\"xyz\"]\r\n> False\r\n\r\n```\r\nrxn = 'rxn'\r\ntf.print(type != rxn)\r\n```\r\n> True\r\n\r\n```\r\n        tf.print('type', type)\r\n        type_is_rxn1 = type == 'rxn'\r\n        rxn = \"rxn\"\r\n        type_is_rxn2 = type == rxn\r\n        type_is_rxn3 = type == tf.constant('rxn')\r\n        tf.print('type is rxn1?', type_is_rxn1)\r\n        tf.print('type is rxn2?', type_is_rxn2)\r\n        tf.print('type is rxn3?', type_is_rxn3)\r\n```\r\n\r\n> type [\"rxn\"]\r\n> type is rxn1? False\r\n> type is rxn2? False\r\n> type is rxn3? False", "Hi,\r\n\r\nI totally agree with you on the global message here - it would be really nice indeed to unify things a bit, especially with Eager yielding messy overheads but being rather practical when debugging.\r\n\r\nThat being said, there currently is a not-so-pleasant solution: do not use tf.function, implement tensorflow control flows instead of using python ones (aka use tf.cond and tf.while_loop), and test your code with `tf.compat.v1.disable_eager_execution()` once it works in Eager. You lose part of the ease that Eager aims at providing, but hey, adding layers of code-translating mechanics is bound to have overheads in addition to the incompatibilities you point out... This is however not perfect because even doing so, I suspect the keras API to embed some code-translation mechanics related to Eager - but at least you can check that your code works without Eager (and probably runs faster too).", "Sorry, there are a lot of issues raised here, let me tackle them one by one.\r\n\r\n> tf.shape() vs tensor.shape\r\n\r\nThis is tricky. In eager mode we can make shapes always available, but in graph mode it often makes sense to work with partially specified shapes, as this lets you build valid graphs in more circumstances.\r\n\r\nNote that working with tf.shape() should give valid results in both eager and graph modes (as we can constant-fold it away if necessary).\r\n\r\n> basic logic often fails in graph mode (for example, if x = \"somestring\") and there's not many good ways to know why\r\n\r\nIn both eager and graph modes tensor equality (operator ==, not the operator = which is not an operator in python) is not overridden. This is due to legacy reasons, and we're trying to fix it in TF2, but it's not a difference between eager and graph modes.\r\n\r\nThat said you rarely need to manipulate strings using tensors of strings, so I don't know when you're hitting this with string dtypes.\r\n\r\n> input signature inference would save everyone a lot of time... retracing happens a lot in tf.function without signatures, especially on sequence data with variable shapes, so we have to write a bunch of (None, None ) to fix this ? that's not really informative. this should be automated. Maybe if tf.function sees a shape varying, it can handle this case automatically\r\n\r\nWe're working on improving this. You can try passing experimental_relax_shapes=True to tf.function, and then we try to infer the signature for you. If you encounter any problems when using that please file issues as we'd like to improve it to the point where we can remove the experimental bit and turn it on by default.\r\n\r\n> a nice way to set dtype at the top of the file would be handy\r\n\r\nWe're working on this as part of the mixed-precision APIs. @reedwm for FYI.\r\n\r\n> a nice way to turn tensorboard tracing on globally would also be handy\r\n\r\nPlease file a separate issue for this.\r\n\r\n", "Thank you @alextp @pandrey-fr ", "@bionicles \r\nCan we close the issue since the query is been answered. Please,let us know. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 30762, "title": "Phase 2 of XLA int8 convolution on CUDA.", "body": "This is a breakdown of previous PR (https://github.com/tensorflow/tensorflow/pull/29158) per @timshen91 's suggestion.\r\nThis PR doesn't depend on (https://github.com/tensorflow/tensorflow/pull/30761).\r\n1. Refactor RunCudnnConvImpl in cudnn_conv_runner.cc to dispatch CuDNN function calls based on input/output types in addition to convolution kind.\r\n2. Modify dnn, cuda_dnn, and rocm_dnn to separate convolution output type from input/element type.\r\n3. Add int8 ThenConvolveWithAlgorithm overloads to stream.h.", "comments": ["@timshen91 I have updated the PR to address your comments.  Please review and approve it again.", "@yongfeng-nv Could you please resolve the conflicts? Thanks!", "@yongfeng-nv  gentle ping to resolve the conflicts. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Since Phase 1 patch is not needed anymore, I'm re-opening this for committing into the repo."]}, {"number": 30761, "title": "Phase 1 of XLA int8 convolution on CUDA.", "body": "This is a breakdown of previous PR (https://github.com/tensorflow/tensorflow/pull/29158) per @timshen91 's suggestion.\r\n1. Add AccumulatorType as a new template argument to HandleConvolution in hlo_evaluator_typed_visitor.h\r\n2. Use it to make HandleConvolution matching CuDNN's semantics for integer inputs.\r\n3. Modify CheckShape in hlo_verifier for integer convolution.", "comments": ["@timshen91 I have updated the PR to address your comments.  Please review and approve it again.", "Sorry, our internal bot messed up and submitted this patch. I rolled it back, as the discussion is not done yet.", "Hi @timshen91 , I saw your message about rolling back merging.  But I still see the PR marked merged.  Just want to make sure you saw my last comment.  I understand we are still discussing design choices.  Once we agree on the design, I will make further commit or PR if necessary. "]}, {"number": 30760, "title": "Remove use_resource from v2 Variable doc", "body": "Think v2 Variables are resource Variables so `use_resource=True`\r\nis not relevant any more (and there is no `use_resource` arg in __init__).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}]