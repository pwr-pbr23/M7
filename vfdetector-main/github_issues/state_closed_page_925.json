[{"number": 25704, "title": "TypeError: Expected config_proto to be a ConfigProto", "body": "    python /root/tensorflow/tensorflow/examples/image_retraining/retrain.py \\\r\n        --image_dir /home/training/data/images \\\r\n        --how_many_training_steps=4000 \\\r\n        --eval_step_interval=100 \\       \r\n        --output_graph /home/training/data/graph.pb \\\r\n        --summaries_dir /home/training/data/summaries \\\r\n        --output_labels /home/training/data/output_labels.txt \\\r\n        --bottleneck_dir /home/training/data/bottleneck/ \\\r\n        --intermediate_store_frequency 1000 \\\r\n        --intermediate_output_graphs_dir /home/training/data/intermediate \\\r\n        --saved_model_dir /home/training/data/saved_model       \r\n        \r\n        \r\n     tensorflowjs_converter \\\r\n        --input_format=tf_saved_model \\\r\n        --output_node_names='final_result' \\\r\n        --saved_model_tags=serve \\\r\n        /home/training/data/saved_model/ \\\r\n        /home/training/data/saved_model_web/\r\n\r\nUsing TensorFlow backend.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0212 21:48:38.775988 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py:161: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nW0212 21:48:42.307756 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1277: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW0212 21:48:44.036837 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nW0212 21:48:44.037100 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py:246: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\n  File \"/bin/tensorflowjs_converter\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/lib/python2.7/site-packages/tensorflowjs/converters/converter.py\", line 322, in main\r\n    strip_debug_ops=FLAGS.strip_debug_ops)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py\", line 294, in convert_tf_saved_model\r\n    skip_op_check=skip_op_check, strip_debug_ops=strip_debug_ops)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py\", line 129, in optimize_graph\r\n    rewriter_config, meta_graph, cluster=get_cluster())\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/grappler/tf_optimizer.py\", line 36, in OptimizeGraph\r\n    type(config_proto))\r\nTypeError: Expected config_proto to be a ConfigProto, saw type <class 'tensorflow.core.protobuf.rewriter_config_pb2.RewriterConfig'>\r\n", "comments": ["@dthomas888 have you managed to do it?", "This issue is more appropriate on TensorFlowJS repository. The folks in TFJS repo may guide you better. Please post it on [tensorflow/tfjs](https://github.com/tensorflow/tfjs/issues) repo. Thanks!"]}, {"number": 25703, "title": "Cache intermediate results in custom op for backwards pass", "body": "Hello,\r\n\r\nI wonder if there is a feature in Tensorflow which allows caching of intermediate results in a custom operation for the backwards computation, similar to the the ctx->save_for_backward interface in Pytorch. Does the C++ context object that we get to work with provide such a functionality?\r\n\r\nBest,\r\n\r\nZiheng Wang", "comments": ["@marsupial-tail Could you describe more details about context of your problem. Thanks! ", "Say I want to implement a custom tensorflow operation that fuses convolution and batch norm, which I believe from reading source code is not supported rn. I would like to cache the input to the batchnorm so I can compute gradients for this op on the back pass. ", "For example, I quote the cudnn documentation for the api cudnnBatchNormalizationForwardTraining: \" resultSaveMean, resultSaveInvVariance\r\nOutputs. Optional cache to save intermediate results computed during the forward pass. These buffers can be used to speed up the backward pass when supplied to the cudnnBatchNormalizationBackward() function. The intermediate results stored in resultSaveMean and resultSaveInvVariance buffers should not be used directly by the user. Depending on the batch normalization mode, the results stored in resultSaveInvVariance may vary. For the cache to work correctly, the input layer data must remain unchanged until the backward function is called. Note that both parameters can be NULL but only at the same time. In such a case intermediate statistics will not be saved, and cudnnBatchNormalizationBackward() will have to re-compute them. It is recommended to use this cache as the memory overhead is relatively small because these tensors have a much lower product of dimensions than the data tensors. \"\r\n\r\nIf I were to indeed cache those in my op, how would I actually save these data pointers in Tensorflow?\r\n", "Any updates?", "@marsupial-tail,\r\nSorry for the delayed response. Can you please refer the detailed documentation regarding [Custom Op](https://www.tensorflow.org/guide/create_op) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@rmothukuru \r\nHi, I have the same problem and I checked the doc, but it didn't help.\r\nIn calculation of backward pass, how I can use the tensor allocated for forward pass by `allocate_persistent` function?"]}, {"number": 25702, "title": "Remove the limitation of int max for dim_size in SparseDenseCwise ops", "body": "This fix tries to address the issue raised in #25701 where the dim_size in SparseDenseCwise ops was set to `std::numeric_limits<int>::max`. The restriction is likely unnecessary, due to historical reasons. This fix remove this limitation.\r\n\r\nThis fix fixes #25701.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@rmlarsen do all of our SparseDense ops use int64 indexing on GPU?"]}, {"number": 25701, "title": "The number of non-zero element exceed 32-bits integer", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source):  1.12.0\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Number of non-zero elements exceeds int32 range [[node graphconvolution_1/mul (defined at /home/fuqiang/anaconda3/envs/python3.6/lib/python3.6/site-packages/gcn-1.0 py3.6.egg/gcn/layers.py:27)  = SparseDenseCwiseMul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](graphconvolution_1/GatherV2, graphconvolution_1/GatherV2_1, graphconvolution_1/Identity, graphconvolution_1/truediv)]]\r\n```\r\nI was running a model using a sparse tensor whose number of non-zero elements exceed int32 range. And I got this.\r\n\r\nDoes that mean I can't do sparsedenseMul on any sparse tensor whose number of non-zero elements exceeds int32 range?", "comments": ["Added a PR #25702 to remove the limitation."]}, {"number": 25700, "title": "Dataset iterator is stalled indefinitely with corrupt TFRecord despite using `ignore_errors`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 9/7\r\n- GPU model and memory: n/a\r\n\r\nCreating this issue per @mrry's [suggestion](https://github.com/tensorflow/tensorflow/issues/12701#issuecomment-462799443) with minimal code to reproduce the issue.\r\n\r\ncc @guillaumekln You're [right](https://github.com/tensorflow/tensorflow/issues/13463#issuecomment-462654013), what I'm seeing may not have to do with `OutofRangeError`, but the execution stalls indefinitely when it encounters corrupt data within a TFRecord, despite using `tf.data.experimental.ignore_errors()`.\r\n\r\n#### Example Data (for snippets below)\r\n- [clean.tfrecord](https://drive.google.com/uc?export=view&id=1Y2yV67jAvYhqz8OJLPV7v0BN2w1Feps9)\r\n- [corrupt.tfrecord](https://drive.google.com/uc?export=view&id=1Fh7Vah0i_XaGqdb724NIhUIZpR_t5YiG)\r\n\r\n#### With `clean.tfrecord`:\r\n```python3\r\nimport tensorflow as tf\r\n\r\nfilenames = ['/data/clean.tfrecord']\r\n#filenames = ['/data/corrupt.tfrecord']\r\n\r\ndataset = tf.data.TFRecordDataset(filenames)\r\ndataset = dataset.apply(tf.data.experimental.ignore_errors())\r\ndataset = dataset.batch(64)\r\ndataset = dataset.repeat(1)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    i = 0\r\n    while True:\r\n        try:\r\n            print(i, sess.run(next_element).shape)\r\n            i = i + 1\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Dataset complete\")\r\n            break\r\n```\r\n\r\n#### Output:\r\n```\r\n0 (64,)\r\n1 (64,)\r\n2 (64,)\r\n3 (64,)\r\n4 (64,)\r\n5 (64,)\r\n6 (64,)\r\n7 (64,)\r\n8 (64,)\r\n9 (64,)\r\n10 (64,)\r\n11 (64,)\r\n12 (64,)\r\n13 (64,)\r\n14 (64,)\r\n15 (64,)\r\n16 (64,)\r\n17 (64,)\r\n18 (64,)\r\n19 (35,)\r\nDataset complete\r\n```\r\n\r\n#### With `corrupt.tfrecord`:\r\n```python3\r\nimport tensorflow as tf\r\n\r\n#filenames = ['/data/clean.tfrecord']\r\nfilenames = ['/data/corrupt.tfrecord']\r\n\r\ndataset = tf.data.TFRecordDataset(filenames)\r\ndataset = dataset.apply(tf.data.experimental.ignore_errors())\r\ndataset = dataset.batch(64)\r\ndataset = dataset.repeat(1)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    i = 0\r\n    while True:\r\n        try:\r\n            print(i, sess.run(next_element).shape)\r\n            i = i + 1\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Dataset complete\")\r\n            break\r\n```\r\n\r\n#### Output:\r\n```\r\n0 (64,)\r\n1 (64,)\r\n2 (64,)\r\n3 (64,)\r\n4 (64,)\r\n5 (64,)\r\n6 (64,)\r\n7 (64,)\r\n8 (64,)\r\n9 (64,)\r\n10 (64,)\r\n11 (64,)\r\n12 (64,)\r\n(execution stalled indefinitely here)\r\n```\r\n\r\nNot sure why the corrupt files within the TFRecord aren't being ignored with `tf.data.experimental.ignore_errors()`. @mrry any idea?", "comments": ["I think the issue was that, when ignore_errors is used, the same file will repeat as the file_index is not moved forward to completion.\r\n\r\nCreated a PR #25705 for a shortcut fix. \r\n\r\nNot very familiar with this part of the code, maybe there are some better ways of fixing it? /cc @mrry\r\n", "Thanks @yongtang!", "Thanks @yongtang. @sjain-stanford If your issue was resolved by @yongtang , then please close it or let me know. Thanks", "@jvishnuvardhan Seems like this will close automatically when #25705 is merged. I can let you know once I test the fix though (need to build from source).\r\n\r\n[UPDATE]: @yongtang's fix worked for me."]}, {"number": 25699, "title": "[INTEL MKL]: Enable quantization using MKL-DNN", "body": "This PR removes the #ifdef INTEL_MKL_QUANTIZED to enable quantization in the default build. ", "comments": ["@penpornk thank you for the quick review. This PR is ready to be merged. Other INT8 PRs are coming but this PR is not dependent on any of them.", "Hi @pragyaak, is there anything blocking this from being merged? ", "@mahmoud-abuzaina yes, I am looking into it. Will update here.", "@mahmoud-abuzaina this is now done. Thanks!", "@pragyaak Thank you too!"]}, {"number": 25698, "title": "Remove deprecation warning on tf.data.v1 functions", "body": "This is for 1.13 only and should not be merged back into master.\r\nThis is a replacement for PR #25578 which was reverted in #25497 due to incompatibility on 1.13 branch.", "comments": []}, {"number": 25697, "title": "FFT parallelized over feature maps, minibatches and within each 2-D transform", "body": "# Performance Issue\r\n\r\nNote: This is not a bug, but it is a performance issue that I have faced, and it might or might not lead to a new feature request. So, I am bringing this up here for discussion.\r\n\r\n**System information**\r\n- I am using Kaggle Kernels for running my tensorflow code with the GPU option enabled.\r\n\r\nEnvironment Information:\r\n> You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n> You can also obtain the TensorFlow version with\r\n> `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n> \r\n> \r\n\r\nOutput:\r\n> b'v1.12.0-0-ga6d8ffae09' 1.12.0\r\n\r\n# The Issue\r\n**Context**\r\n\r\nI reproduced this [paper](https://www.groundai.com/project/fast-training-of-convolutional-networks-through-ffts/) that basically learns CNN maps in the frequency domain. However, when I run it and compare the epoch time on training of regular CNNs, that operates in the spatial domain, against using FFTs, that operates in the frequency domain, the FFT version is much slower. For experiments, I implemented the LeNet-5 and used the MNIST dataset.\r\n\r\n| Experiment | Description | Avrg Epoch Time  | Wall time | Acc |\r\n|:------------:|:-----------:|:-----------------:|-----------|-----|\r\n|  MNIST - Baseline LeNet CNN | Baseline | 1.67633 s  | 25.2 s   | 98.79416 |\r\n| MNIST - Mathieu LeNet CNN | Frequency Domain Convolutions | 5.69431 s | 56.4 s   | 97.26562 |\r\n\r\nThe paper, that claims a faster CNN, comes with the following note:\r\n\r\n> Current GPU implementations of the FFT such as cuFFT are designed to parallelize over individual transforms. This can be useful for computing a limited number of transforms on large inputs, but is not suitable for our task since we are performing many FFTs over relatively small inputs. Therefore, we developed a custom CUDA implementation of the Cooley-Tukey FFT algorithm which enabled us to parallelize over feature maps, minibatches and within each 2-D transform. Note that 2-D FFTs lend themselves naturally to parallelization since they can be decomposed into two sets of 1-D FFTs (one over rows and the other over columns), and each set can be done in parallel.\r\n\r\nI had no clue on how this implementation optimization would occur, so I put [this question](https://dsp.stackexchange.com/questions/54903/how-to-custom-optimize-cufft-for-a-mini-batch-of-multi-channel-images) up to an online community. I assume the paper is accurate on its claim, since one of the authors is the father of CNN, LeCan. So, the only feasible thing I could do, given the answer I got and my lack of understanding on actual `CUDA` development, was to check for the tensorflow implementation and see if`cufftPlanMany` is somehow used. And it is indeed somehow used.\r\n\r\n**Problem**\r\n\r\nUp to this point, I don't know how to reproduce the technique described in that paper. I wish I knew. If there is a better way to run FFT operations, I believe that lots of applications could benefit from it. In additional, the paper mention some memory considerations of their implementation. But I think that it is a separe issue. \r\n> Note that by keeping the Fourier representations in memory for all layers after the forward pass, we could avoid recomputing several of the FFTs during the backward pass. However, this might become pro- hibitively expensive in terms of memory for large networks. Therefore we reuse the same memory for all the different convolutions in the network, so that the necessary amount of memory is de- termined only by the largest convolution layer.\r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25697)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25697)\r\n"]}, {"number": 25696, "title": "Revert timeout for while loop ", "body": "This reverts #23811 aka 224199366\r\nThis was causing regressions in while loops \r\nhttps://github.com/tensorflow/tensorflow/issues/25578", "comments": ["cc @mrry "]}, {"number": 25695, "title": "TF-Gradient issue", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: Python 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0, V9.0.176\r\n- GPU model and memory: GeForce GT 750M\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nI am trying to use @tf.custom gradient for a one of the layers in a RNN, and running to the following problem, when I compute the gradient of loss in the output layer with respect to weights in hidden layer, I get gradient values, however when I try to compute the gradient of the output of the hidden layer with respect to the input to the layer or states in the layer (y=f(x) and calculating dy/dx ) I get [None]. I am using tf.gradients to compute gradients. I am doing the second part to see if I have implemented the custom gradient correctly. \r\nthis is the activation function from hidden state and output \r\n```\r\n@tf.custom_gradient\r\ndef _calcualte_crossings(x):\r\n    \"\"\"input :x : a 2D tensor with batch x n\r\n    outputs a tensor with the same size as x\r\n    and values of 0 or 1 depending on comparison between\r\n    x and threshold\"\"\"\r\n    dtype=x.dtype\r\n    res=tf.greater_equal(x,0.0)\r\n    def grad(dy):\r\n        # calculate 1-|x|\r\n        temp=1-tf.abs(x)\r\n        dyres=tf.scalar_mul(0.3,tf.maximum(temp,0.0))\r\n        return dyres\r\n    return tf.cast(res,dtype=dtype), grad\r\n```\r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25695)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25695)\r\n"]}, {"number": 25694, "title": "TF_gradient", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: Python 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0, V9.0.176\r\n- GPU model and memory: GeForce GT 750M\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nI am trying to use @tf.custom gradient for a one of the layers in a RNN, and running to the following problem, when I compute the gradient of loss in the output layer with respect to weights in hidden layer, I get gradient values, however when I try to compute the gradient of the output of the hidden layer with respect to the input to the layer or states in the layer (y=f(x) and calculating dy/dx ) I get [None]. I am using tf.gradients to compute gradients. I am doing the second part to see if I have implemented the custom gradient correctly. \r\nthis is the activation function from hidden state and output \r\n```\r\n@tf.custom_gradient\r\ndef _calcualte_crossings(x):\r\n    \"\"\"input :x : a 2D tensor with batch x n\r\n    outputs a tensor with the same size as x\r\n    and values of 0 or 1 depending on comparison between\r\n    x and threshold\"\"\"\r\n    dtype=x.dtype\r\n    res=tf.greater_equal(x,0.0)\r\n    def grad(dy):\r\n        # calculate 1-|x|\r\n        temp=1-tf.abs(x)\r\n        dyres=tf.scalar_mul(0.3,tf.maximum(temp,0.0))\r\n        return dyres\r\n    return tf.cast(res,dtype=dtype), grad\r\n```\r\n\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "It would be helpful if you had a minimal reproducing example.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25693, "title": "Enable Keras Applications with pretrained Weights to be used inside the Estimator API", "body": "Examples like #25670 show you that is impossible to use Keras Applications inside the Estimator API. The issue has to do with some global tensors that Keras creates by itself that live or were created in a separate session, thus, when the Estimator API wants to construct the `model_fn` it raises an error saying that it found a Tensor from a different graph.\r\n\r\nI think Keras should be modified to avoid this behavior, TFHub is nice but it doesn't let you use arbitrary input shapes. ", "comments": ["@cgarciae Have you tried to use [model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator)? I believe that might work in this case.", "@karmel Tested, it doesn't work.", "@cgarciae -- thanks; can you provide the code snippet and error output? Thanks--", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25692, "title": "pip install tf-nightly-2.0-preview on Windows 10: missing file TensorSyclConvertToDeviceExpression.h", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: tf-nightly-2.0-preview==2.0.0.dev20190208\r\n- Python version:3.6.6 \r\n- Installed using virtualenv? pip? conda?: pip (18.0)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nCannot install  TF2.0 preview package for Windows 10 (private Windows 10 machine, admin, no proxy) because TensorSyclConvertToDeviceExpression.h is missing\r\n\r\nOne file cannot be found:\r\nxxx/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'\r\n\r\nI used pip install tf-nightly-2.0-preview==2.0.0.dev20190208 --no-clean and I could see that such file doesn't exist\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npip install tf-nightly-2.0-preview==2.0.0.dev20190208\r\n\r\nCollecting tfds-nightly (from -r C:\\Users\\steph\\condaenv.19qkhena.requirements.txt (line 1))\r\n  Downloading https://files.pythonhosted.org/packages/a6/66/8da21433386f85e2c5665c830ee86e79518f73e019846083513b84665d29/tfds_nightly-1.0.0.dev201902120105-py3-none-any.whl (393kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 399kB 6.6MB/s\r\nCollecting tf-nightly-2.0-preview==2.0.0.dev20190208 (from -r C:\\Users\\steph\\condaenv.19qkhena.requirements.txt (line 2))\r\n  Downloading https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl (42.1MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42.1MB 728kB/s\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\steph\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hwy1u5dr\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-2.0.0.dev20190208.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'\r\n\r\nCondaValueError: pip returned an error \r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nsame command a above in verbose mode (selection):\r\n\r\nUsing version 2.0.0.dev20190208 (newest of versions: 2.0.0.dev20190208)\r\n  Created temporary directory: C:\\Users\\steph\\AppData\\Local\\Temp\\pip-unpack-00jv942o\r\n  Looking up \"https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl\" in the cache\r\n  Current age based on date: 290\r\n  Ignoring unknown cache-control directive: immutable\r\n  Freshness lifetime from max-age: 365000000\r\n  The response is \"fresh\", returning cached response\r\n  365000000 > 290\r\n  Using cached https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl\r\n  Downloading from URL https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl#sha256=5b29449c373e7fb2fcd188cb6b0006facfb8da3414c0d555223cdce6307b1442 (from https://pypi.org/simple/tf-nightly-2-0-preview/)\r\nCould not install packages due to an EnvironmentError.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 299, in run\r\n    resolver.resolve(requirement_set)\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\resolve.py\", line 102, in resolve\r\n    self._resolve_one(requirement_set, req)\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\resolve.py\", line 256, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\resolve.py\", line 209, in _get_abstract_dist_for\r\n    self.require_hashes\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 283, in prepare_linked_requirement\r\n    progress_bar=self.progress_bar\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\download.py\", line 836, in unpack_url\r\n    progress_bar=progress_bar\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\download.py\", line 677, in unpack_http_url\r\n    unpack_file(from_path, location, content_type, link)\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 578, in unpack_file\r\n    flatten=not filename.endswith('.whl')\r\n  File \"C:\\Users\\steph\\Anaconda3\\envs\\env_gcp_dl_2_0\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 478, in unzip_file\r\n    fp = open(fn, 'wb')\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\steph\\\\AppData\\\\Local\\\\Temp\\\\pip-install-uvd9so6x\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-2.0.0.dev20190208.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'\r\nCleaning up...\r\n\r\n", "comments": ["@tarrade There is a common issue with Windows10 where path length is restricted by default. Please check this [resource](https://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/) to remove that restriction and then install TF. There are several users [reported](https://github.com/tensorflow/tensorflow/issues/24835) similar issue. Please let us know how it progresses. Thanks! ", "@jvishnuvardhan thanks a lot. I installed more than 300 packages with pip and conda and never saw that. You are right the path was 273 characters long and allowing to have more than 260 characters as mention in your link fixed the issue. Thanks a lot", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25692)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25692)\r\n"]}, {"number": 25691, "title": "Missing mathematical definitions of tf.norm", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/norm\r\n\r\n\r\n**Describe the documentation issue**\r\nThere are no mathematical definitions of the norms. This may result in ambiguity and confusion (e.g. are they Schatten p-norms?).\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** Not anytime soon, and in any case I would face said ambiguity and confusion.", "comments": ["@ziofil,\r\nDifferent types of Normalization, like **`Euclidean`**, **`Fro`**, etc.. is performed based on the argument, `ord` , as shown in [the documentation](https://www.tensorflow.org/api_docs/python/tf/norm#used-in-the-notebooks).\r\n\r\nPlease let me know if this is what you are looking for, or something else. Thanks!"]}, {"number": 25690, "title": "Tensorflowlite: bazel windows build is broken", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n\r\n- TensorFlow installed from (source or binary):\r\nBuild from source\r\n\r\n- TensorFlow version:\r\nmaster, 07d4fe20b305178c9e1c86d0aee25ded59078dbf\r\n\r\n- Python version:\r\n2.7.15\r\n\r\n- Installed using virtualenv? pip? conda?:\r\nn/a\r\n\r\n- Bazel version (if compiling from source):\r\n0.21.0\r\n\r\n- GCC/Compiler version (if compiling from source):\r\nMSVC 2017\r\n\r\n- CUDA/cuDNN version:\r\nn/a\r\n\r\n- GPU model and memory:\r\nn/a\r\n\r\n**Describe the problem**\r\nWindows build of tensorflow lite fails. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nD:\\AI\\tensorflow>bazel.exe --output_user_root ./builddir build --jobs 1 //tensorflow/lite:libtensorflowlite.so\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nd:\\ai\\tensorflow/.bazelrc\r\nINFO: Invocation ID: 4d63708d-d989-452b-8ad1-8cdde66edcdb\r\nDEBUG: D:/ai/tensorflow/builddir/jkmrio3k/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:\r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag No\r\nne). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nINFO: Analysed target //tensorflow/lite:libtensorflowlite.so (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: D:/ai/tensorflow/tensorflow/lite/kernels/BUILD:259:1: C++ compilation of rule '//tensorflow/lite/kernels:lstm_eval' failed (Exit 2)\r\ncl : Command line warning D9002 : ignoring unknown option '/Gw'\r\n.\\tensorflow/lite/c/c_api_internal.h(32) : fatal error C1083: Cannot open include file: 'stdbool.h': No such file or directory\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.579s, Critical Path: 0.10s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["any progress here?", "If you move the include as follows:\r\n\r\n```\r\n#ifndef __cplusplus\r\n#include <stdbool.h>\r\n#endif  // __cplusplus\r\n```\r\n\r\ndoes that solve the problem?", " Yes, it did help with that issue, but new one appeared: \r\n\r\nINFO: Analysed target //tensorflow/lite:libtensorflowlite.so (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nINFO: From Compiling tensorflow/lite/kernels/lstm_eval.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '/Gw'\r\nERROR: D:/ai/tensorflow/tensorflow/lite/kernels/internal/BUILD:473:1: C++ compilation of rule '//tensorflow/lite/kernels/internal:tensor_utils' failed (Exit 2)\r\ncl : Command line warning D9002 : ignoring unknown option '/Gw'\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nd:\\ai\\tensorflow\\builddir\\jkmrio3k\\execroot\\org_tensorflow\\external\\gemmlowp\\internal\\../profiling/pthread_everywhere.h(40) : fatal error C1083: Cannot open include file: 'condition_variable': No such file or\r\ndirectory\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 6.798s, Critical Path: 0.72s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully", "I guess this is because Microsoft's compiler doesn't switch to c++ 11 mode (btw, weird, it should be c++ 11 without any options)... In any case I've tried to pass \"--std=c++11\" or \"/std:c++14\" or \"/std:c++17\" but in all the cases options were not recognized as valid: \r\n\r\ncl : Command line warning D9002 : ignoring unknown option '/Gw'\r\ncl : Command line warning D9002 : ignoring unknown option '--std=c++11'", "Not sure what I did besides retrying (btw, why does bazel show different random build errors on build failures) but now the error is the following: \r\n\r\ncl : Command line warning D9002 : ignoring unknown option '/Gw'\r\ntensorflow/lite/kernels/internal/mfcc_dct.cc(44) : error C2668: 'atan' : ambiguous call to overloaded function\r\n        C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\INCLUDE\\math.h(553): could be 'long double atan(long double)'\r\n        C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\INCLUDE\\math.h(505): or       'float atan(float)'\r\n        C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\INCLUDE\\math.h(108): or       'double atan(double)'\r\n        while trying to match the argument list '(int)'\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build", "any update here?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25690\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25690\">No</a>\n"]}, {"number": 25689, "title": "cannot convert model to tflite", "body": "\r\nwhen I try to converto to tflite, occur below errors \r\n\r\n\r\n$ tflite_convert --output_file=./model1/test.tflite --keras_model_file=./model1/186-0.0481.hdf5\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:96: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1253: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:96: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2019-02-12 18:00:17.730635: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py:636: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\n2019-02-12 18:00:18.207014: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-02-12 18:00:18.207141: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-02-12 18:00:18.210189: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node dense/kernel/Assign doesn't exist in graph\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 191, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py\", line 500, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-02-12 18:00:19.028566: F tensorflow/core/framework/function.cc:1640] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate\r\nAborted (core dumped)\r\n\r\n\r\n\r\nHow can I fixed it?\r\nsomebody has any idea?", "comments": ["Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 3.19.0-25-generic x86_64)\r\nTensorflow build from source (latest version)\r\n\r\nPython version : Python 2.7.6\r\nBazel version : Build label: 0.21.0\r\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\r\n(Don't use GPU)\r\n\r\nThis is my environment information.", "I used test example code --> (part of keras) https://www.tensorflow.org/lite/convert/python_api\r\n\r\nand occur error like this\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1253: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:123: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2019-02-13 11:33:51.428806: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:96: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py:636: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\n2019-02-13 11:33:51.991970: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-02-13 11:33:51.992069: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-02-13 11:33:51.995173: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node dense/kernel/Assign doesn't exist in graph\r\nTraceback (most recent call last):\r\n  File \"./tensorflow/lite/examples/simple_model_test/make_model.py\", line 26, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py\", line 500, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-02-13 11:33:52.824627: F tensorflow/core/framework/function.cc:1640] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate\r\nAborted (core dumped)", "@adppark Please check tested [build configurations](https://www.tensorflow.org/install/source#linux). I think this error may be due to bazel version 0.21.0. Could you check the version of TF by print(tf.__version__). Thanks!", "Python 2.7.6 (default, Nov 13 2018, 12:45:42) \r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.version)\r\n<module 'tensorflow._api.v1.version' from '/usr/local/lib/python2.7/dist-packages/tensorflow/_api/v1/version/__init__.pyc'>\r\n\r\nIs this right?", "@jvishnuvardhan when I try to with bazel version 0.15.0\r\nprinted like this\r\n\r\nrelease@VDBS1109:~/tensorflow$ ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: ignoring http_proxy in environment.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease upgrade your bazel installation to version 0.19.0 or higher to build TensorFlow!\r\n\r\n\r\nWhich version is correct?", "@adppark Were you able to install tensorflow? Could you execute \r\nimport tensorflow as tf\r\nprint tf.__version__\r\n\r\nIf you have TF1.12, then Bazel 0.15.0 is supported and if you are installing recent versions >TF1.12, then Bazel 0.19.0 or Bazel 0.21.0 works. Thanks!", "@jvishnuvardhan \r\nI already installed tensorflow. \r\n\r\nI tried build with Bazel version 0.19.0, 0.21.0, 0.15.0..\r\nBur always occur error message same as..\r\n\r\n\r\n**$ bazel version**\r\nINFO: Invocation ID: 65ccb62c-d398-46bc-8535-f4d2f1c072cb\r\n**Build label: 0.22.0**\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Jan 28 12:58:08 2019 (1548680288)\r\nBuild timestamp: 1548680288\r\nBuild timestamp as int: 1548680288\r\n$ python\r\nPython 2.7.6 (default, Nov 13 2018, 12:45:42) \r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n**import tensorflow as tf**\r\n**print tf.__version__**\r\n**1.12.0**\r\n\r\n\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1253: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:123: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2019-02-15 13:25:49.564196: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:96: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py:636: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\n2019-02-15 13:25:50.157624: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-02-15 13:25:50.157737: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-02-15 13:25:50.160530: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node dense/kernel/Assign doesn't exist in graph\r\nTraceback (most recent call last):\r\n  File \"./make_model.py\", line 26, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py\", line 500, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-02-15 13:25:50.973720: F tensorflow/core/framework/function.cc:1640] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate\r\nAborted (core dumped)", "@gargn @jvishnuvardhan  Can the issue be because of using Python2.7? Python2.7 will retire in 2020 and seeing the warnings quoted below, I think it's because of that only.\r\n\r\n> WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1253: calling **init** (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Call initializer instance with the dtype argument instead of passing it to the constructor\r\n> WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:123: calling **init** (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Call initializer instance with the dtype argument instead of passing it to the constructor\r\n> 2019-02-15 13:25:49.564196: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\n> WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:96: calling **init** (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Call initializer instance with the dtype argument instead of passing it to the constructor\r\n> WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py:636: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\n\r\n", "Can you try using the latest TF version? If that doesn't work, can you provide your model or a minimal model that produces the same error?", "having this issue with this docker container tensorflow/tensorflow:latest-py3\r\n```\r\ndocker run -it --rm -v $PWD:/tmp -w /tmp tensorflow/tensorflow:latest-py3 tflite_convert --outut_file=/tmp/foo.tflite --keras_model_file=/tmp/da19.hdf5\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 122, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 109, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/lite.py\", line 370, in from_keras_model_file\r\n    keras_model = _keras.models.load_model(model_file)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py\", line 232, in load_model\r\n    raise ValueError('No model found in config file.')\r\n```\r\n", "@danilaplee Please file a new GitHub issue which describes your problem in detail and provides instructions on how to reproduce it.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25689\">No</a>\n", "lol tensorflow converter doesnt work and will never work !!! why ? simple because it has never worked to begin with. evidence of this is shown here = zero solutions are shown , zero 100% solution to the issue and above all just waste of time\r\n"]}, {"number": 25688, "title": "Compile issue for TF Lite for Android NDK wit TF ops enabled", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.13.0-rc0\r\n- Python version: 3\r\n- Bazel version: 0.21.0\r\n- GCC/Compiler version (if compiling from source): NDK r14b, r18b, clang\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n- Android SDK/NDK API level: 21\r\n\r\n\r\n**When trying to build TFlite as shared library for Android with the experimental TF ops enabled, I get an compile error**\r\n\r\n**Steps**\r\n\r\nAdded the Android target to the _tensorflow/lite/BUILD_:\r\n```\r\ncc_binary( \r\n    name = \"libtensorflowLite.so\",\r\n    linkopts=[\r\n        \"-shared\", \r\n        \"-Wl,-soname=libtensorflowLite.so\",\r\n    ],\r\n    linkshared = 1,\r\n    copts = tflite_copts(),\r\n    deps = [\r\n        \":framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\"\r\n    ],\r\n)\r\n```\r\nI added the _flex_delegate_ as documented  [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md)\r\n\r\nBuild with command:\r\n```\r\nbazel build //tensorflow/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\n```\r\n\r\nI tried with NDK version r18b and r14b \r\n\r\n**Any other info / logs**\r\n\r\nThis is the important part of the log:\r\n```\r\n[0m [91m[562 / 928] Compiling tensorflow/core/kernels/cwise_op_add_1.cc; 46s local ... (2 actions, 1 running)\r\n[0m [91m[573 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 22s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 152s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 301s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 474s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 672s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 900s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 1223s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 1552s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 1971s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 2415s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 2923s local ... (2 actions running)\r\n[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 3507s local ... (2 actions running)\r\n[0m [91mERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:5770:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 254)\r\nclang: error: unable to execute command: Killed\r\nclang: error: clang frontend command failed due to signal (use -v to see invocation)\r\nAndroid (4751641 based on r328903) clang version 7.0.2 (https://android.googlesource.com/toolchain/clang 003100370607242ddd5815e4a043907ea9004281) (https://android.googlesource.com/toolchain/llvm 1d739ffb0366421d383e04ff80ec2ee591315116) (based on LLVM 7.0.2svn)\r\nTarget: armv7-none-linux-android\r\nThread model: posix\r\nInstalledDir: external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin\r\nclang: note: diagnostic msg: PLEASE submit a bug report to https://bugs.llvm.org/ and include the crash backtrace, preprocessed source, and associated run script.\r\nclang: note: diagnostic msg: \r\n********************\r\nPLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:\r\nPreprocessed source(s) and associated run script(s) are located at:\r\nclang: note: diagnostic msg: /tmp/cwise_op_mul_1-574804.cpp\r\nclang: note: diagnostic msg: /tmp/cwise_op_mul_1-574804.sh\r\nclang: note: diagnostic msg:\r\n********************\r\n[0m [91mTarget //tensorflow/lite:libtensorflowLite.so failed to build\r\n[0m [91mUse --verbose_failures to see the command lines of failed build steps.\r\n[0m [91mINFO: Elapsed time: 4707.414s, Critical Path: 3871.01s\r\nINFO: 564 processes: 564 local.\r\n[0m [91mFAILED: Build did NOT complete successfully\r\n[0m [91mFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI am looking forward to any advice if this is even possible in the current experimental stage\r\nThanks,\r\nThomas", "comments": ["Can you paste the contents of the `.tf_configure.bazelrc` file in the root source directory (where you ran ./configure)? Thanks.", "Here you go:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/local/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\nbuild --python_path=\"/usr/local/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n```", "@tbirn You may want to check if the compiler was killed because of memory pressure. ", "@freedomtan That is certainly the case. I tried again on a more powerful machine in the cloud and it compiled w/o error.\r\n\r\nThanks for the help.", "I have problem in using C++ APIs on android. We have to build a C++ library that access TF Lite. Our library is used by Android application. Can you please share working sample and instructions? ", "@suneshp [TFLite Android examples](https://www.tensorflow.org/lite/examples/) use Java to access underlying C++ libraries. You may want to start from them.", "The [label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image) and [minimal](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal) example demonstrate C++ API usage. You can use those APIs together with the native [libtensorflowlite.so](https://github.com/tensorflow/tensorflow/blob/182fce37819b22bc94b444a851ababf94c464259/tensorflow/lite/BUILD#L414) library.", "@suneshp Yes, as @jdduke said you can take a look at label_image and minimal too. When TFLite was released in late 2017, I got so excited. Then it came to me that there are Android Apps, which uses Java API which uses JNI to call C++ APIs, but there were no working C++ API command line example, so I read TFLite source code and wrote the label_image for TFLite as an exercise and a tool for myself. I mean, so far it seems the most reliable doc and example are in source code :-)", "Thank you both. I will try and update the thread.\n\nSent from Yahoo Mail on Android \n \n  On Wed, Mar 13, 2019 at 8:02 PM, freedomtan<notifications@github.com> wrote:   \n@suneshp Yes, as @jdduke said you can take a look at label_image and minimal too. When TFLite was released in late 2017, I got so excited. Then it came to me that there are Android Apps, which uses Java API which uses JNI to call C++ APIs, but there were no working C++ API command line example, so I read TFLite source code and wrote the label_image for TFLite as an exercise and a tool for myself. I mean, so far it seems the most reliable doc and example are in source code :-)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n   \n", "> @suneshp [TFLite Android examples](https://www.tensorflow.org/lite/examples/) use Java to access underlying C++ libraries. You may want to start from them.\r\n\r\nAll the samples are for models with built-in operations, what about custom operations? I've followed the documentation (https://www.tensorflow.org/lite/guide/ops_custom#defining_the_kernel_in_the_tensorflow_lite_runtime) but don't know how to register my custom operation from Java. \r\n\r\nSo when in the image_classification sample I try to use my model there is an exception informing me that my custom operation cannot be found, which is logical because I haven't registered it. ", "@hamlatzis are you able to use bazel to build the native libraries? That is, if you can run:\r\n\r\n```\r\nbazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n //tensorflow/lite/java:tensorflow-lite\r\n```\r\n\r\nYou can either:\r\n\r\n 1) [Add your sources here and explicitly add your custom op to the BuiltinOpResolver](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/native/builtin_ops_jni.cc#L25), then rebuild the library.\r\n  2) Create your own `std::unique_ptr<OpResolver> tflite::CreateOpResolver()` method implementation, and then link that into the native library in [his build rule](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/native/BUILD#L56) (instead of builtin_ops_jni.cc) and rebuild the native library.\r\n\r\nWe considered adding direct support for passing along custom ops through Java, but the reality is you'd have to write your own JNI bindings to pass through your native custom op, and at that point you may already be doing things in native code anyway. We're happy to reconsider if there is demand, and we could do something like what we've done with the [Delegate.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Delegate.java) class.", "https://github.com/tensorflow/tensorflow/issues/25688#issuecomment-494466083\r\n\r\nYes that's how I built my .aar file and following the documentation I have added my custom ops. Tomorrow when I'm back at the office I'll check the rest of your guidelines\r\n\r\nthnx", "https://github.com/tensorflow/tensorflow/issues/25688#issuecomment-494466083\r\n\r\n@jdduke just modified my code and it worked thnx\r\n\r\n![2019-05-22_09h44_36](https://user-images.githubusercontent.com/761700/58153395-938db180-7c77-11e9-9e7b-46d88e3a3f30.png)\r\n\r\n\r\n(sorry for the Greek labels in the image)\r\n", "Nice! Glad it worked.\r\n\r\nWe'll work on updating the documentation on using custom ops to explicitly call out how to do this with the Java bindings. Thanks for flagging the issue!"]}, {"number": 25687, "title": "Functional safety compliance", "body": "We want to use Tensorflow for building Deep Learning based models in production for Automotive. \r\nThe Automotive industry requires strict compliance with ISO 26262 for functional safety.\r\n\r\nOne of the clause in ISO 26262 is \u201cthe classification of all the software tools used in development shall be performed according to ISO 26262-8, clause 11 \u2018Confidence in the use of software tools\u2019\u201d.\r\nSuch classification report basically means the tool (in this case Tensorflow) is functionally safe to use.\r\n\r\nIs it possible to get such a classification report for Tensorflow? Whom to contact for this topic\r\nThanks.\r\n", "comments": ["Hi @abhi278,  \r\n\r\nPlease see paragraph 7 of the license: https://github.com/tensorflow/tensorflow/blob/master/LICENSE#L145", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25687)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25687)\r\n"]}, {"number": 25686, "title": "Running the transformer model with Tensor2Tensor using Mesh-Tensorflow(GPU implementation)", "body": "Hello I posted the below concern in mesh-tensorflow and tensor2tensor GitHub issues but did not get any response. So I am posting here if any one can help me about this issue.\r\n\r\nI am trying to run the transformer model with Tensor2tensor using mesh-tensorflow (GPU-implementation) but I am facing few errors.\r\n\r\nI am attaching my error log also.\r\n\r\n**steps to reproduce:**\r\n\r\nPROBLEM=translate_enfr_wmt32k\r\nMODEL=mtf_transformer\r\nHPARAMS=mtf_transformer_paper_tr_0_mesh_8\r\nDATA_DIR=$HOME/t2t_data\r\nTMP_DIR=/tmp/t2t_datagen\r\nTRAIN_DIR=$HOME/t2t_train/$PROBLEM/$MODEL-$HPARAMS\r\nmkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR\r\n\r\ndatagen:\r\nt2t-datagen \\\r\n--data_dir=$DATA_DIR \\\r\n--tmp_dir=$TMP_DIR \\\r\n--problem=$PROBLEM\r\n\r\ntrain:\r\nt2t-trainer \\\r\n--data_dir=$DATA_DIR \\\r\n--problem=$PROBLEM \\\r\n--model=$MODEL \\\r\n--hparams_set=$HPARAMS \\\r\n--output_dir=$TRAIN_DIR \\\r\n--train_steps=10\r\n[mesh-error.txt](https://github.com/tensorflow/tensorflow/files/2854483/mesh-error.txt)\r\n", "comments": ["Hello any updates yet?", "@Raviteja1996 Please \r\nThis is not related to TensorFlow Core.  This repo is mainly for addressing bugs in installation and performance related to TF Core.  \r\nPlease post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Thanks!"]}, {"number": 25685, "title": "remove unused variable", "body": "bazel gave me a warning this was unused and I like less warnings", "comments": []}, {"number": 25684, "title": "Lite: Unpack Operator Negative Axis Support", "body": "1:> TFLite: Unpack operator Negative axis support added.\r\n2:> Test cases added for the same feature.\r\n3:> Base warning fixed.\r\n4:> Test case bug(Wrong API used for parameter creation) fixed.", "comments": ["@pragyaak, I don't know very much about TF Lite. Can you find someone more qualified to review?"]}, {"number": 25683, "title": "TF Android Example compile warning fixes", "body": "1. The minSdk and targetSdk version should not be declared in the android manifest file. You can move the version from the manifest to the defaultConfig in the build.gradle file.\r\nReference : https://developer.android.com/guide/topics/manifest/manifest-intro#uses-sdk\r\n2. Configuration 'compile' is obsolete and has been replaced with 'implementation' and 'api'.\r\nReference : http://d.android.com/r/tools/update-dependency-configurations.html\r\n3. Caused by: java.io.FileNotFoundException: imagenet_comp_graph_label_strings.txt this error reported after application compile and run first time using Studio, this fix solves the exist problem.", "comments": []}, {"number": 25682, "title": "in the website \"https://tensorflow.google.cn\",the experience of auto-scroll is so bad for  focusing on the content. I wish it could be improved.Tks", "body": "in the website \"https://tensorflow.google.cn\",the experience of auto-scroll is so bad for  focusing on the content. I wish it could be improved. Thanks.", "comments": ["Could we get more detail on this?  \r\n\r\n* Browser and version?\r\n* How are you invoking autoscroll?\r\n* Which pages are causing issues?\r\n", "No response; closing, but please re-file if you want to tell us about accessibility issues."]}, {"number": 25681, "title": "Error running ./configure on Cuda 10 (Building from Source)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):Linux Ubuntu 18.04)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source ): yes\r\n- TensorFlow version: master branch\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10 \r\n- GPU model and memory: 1080 \r\n\r\nHaving the following error when trying to run ./configure\r\nSince I'm trying to use CUDA 10 with Tensorflow I need to build from the source \r\n\r\n**\r\n```\r\nInvalid SYCL 1.2 library path. /usr/local/computecpp/lib/libComputeCpp.so cannot be found\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1701, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1595, in main\r\n    set_computecpp_toolkit_path(environ_cp)\r\n  File \"./configure.py\", line 1387, in set_computecpp_toolkit_path\r\n    suppress_default_error=True)\r\n  File \"./configure.py\", line 657, in prompt_loop_or_load_from_env\r\n    'Assuming to be a scripting mistake.' % (var_name, n_ask_attempts))\r\n__main__.UserInputError: Invalid COMPUTECPP_TOOLKIT_PATH setting was provided 10 times in a row. Assuming to be a scripting mistake.\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI tried to complete installation starting from Tensorflow part of this blogpost https://medium.com/@vitali.usau/install-cuda-10-0-cudnn-7-3-and-build-tensorflow-gpu-from-source-on-ubuntu-18-04-3daf720b83fe but the first bazel version was not compatible with tensorflow so I needed to downgrade to 21 then this error poped out. \r\n\r\n", "comments": ["Closing this I continued my installation without SYCL."]}, {"number": 25680, "title": "TF2 Upgrader: add support of ipynb files", "body": "As discussed @martinwicke, since locally it was tested in isolated environment, I would be more than happy to run the required tests\r\n\r\nPotentially I'd follow with updating readme and other docs related things", "comments": ["I think once, https://github.com/tensorflow/tensorflow/pull/25653 is in, I can rebase this one", "@martinwicke Thanks for review, I've updated accordingly. \r\nAnd with tree part, I would add it as a second PR potentially.\r\n\r\n", "@martinwicke I fixed linter and other build issues. Please let me know if anything additional is missing."]}, {"number": 25679, "title": "New environment capture script features", "body": "On issue #25461, added:\r\n* **CUDA/cuDNN** version\r\n* **GPU model** and memory\r\n* **Bazel** version\r\n\r\nrefactored to functions:\r\n* **OS Platform and Distribution**\r\n* **GCC/Compiler** version\r\n", "comments": ["Hi there @rthadur @aselle @jianlijianli - would you like me to better document the changes I made to facilitate the review process ?\r\nWould a stacked PR be better perhaps ?", "I'm not a good reviewer for this.", "(I'm also unfamiliar with this script, I think @aselle would know the best person to redirect this to)", "Sorry but I'm not a good reviewer for this change.", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "#25833 has been merged to address the same issue so closing this PR , thanks for your contribution."]}, {"number": 25677, "title": "Build from source with MPI support", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):4.8.5\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nBazel won't build tensorflow with MPI support. Error message says \"ImportError: libmpi.so.40: cannot open shared object file: No such file or directory\". However I had my $LD_LIBRARY_PATH set correctly.\r\n\r\nI've using openmpi-3.0.3. Previously I tried openmpi-1.8.1 and it didn't work either.\r\n\r\nPS: the tested tensorflow build configuration says to use bazel 0.15.0 but I got an error says \"Please upgrade your bazel installation to version 0.19.0 or higher to build TensorFlow!\" when I tried to configure. Then i switched to bazel 0.21.0.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\njon@jon-OptiPlex-3050:~/local_build/tensorflow$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: a65fa066-3efc-406a-aba3-d8f7e8c10bfa\r\nYou have bazel 0.21.0 installed.\r\nPlease specify the location of python. [Default is /home/jon/anaconda3/bin/python]:\r\n\r\nFound possible Python library paths:\r\n  /home/jon/anaconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/jon/anaconda3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]:\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]:\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]:\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [Y/n]: Y\r\nMPI support will be enabled for TensorFlow.\r\n\r\nPlease specify the MPI toolkit folder. [Default is /home/jon/openmpi-3.0.3]:\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare\r\n]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n----------------------------------\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Invocation ID: 26351414-5c95-4a85-91f0-54b998cf6311\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/python/BUILD:3140:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/p\r\nython:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to Tens\r\norFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and wi\r\nll be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/python/BUILD:100:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/pyth\r\non:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlo\r\nw Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be\r\n removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '\r\n//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions\r\n has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive\r\n new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorf\r\nlow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immed\r\niately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorf\r\nlow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately\r\n.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: ta\r\nrget '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Dis\r\ntributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distribution\r\ns are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/con\r\ntrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorF\r\nlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, a\r\nnd will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (387 packages loaded, 23360 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/jon/local_build/tensorflow/tensorflow/BUILD:606:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/jon/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/jon/anaconda3/lib/python3.6/imp.py\", line 347, in load_dynamic\r\n    return _load(spec)\r\n__ImportError: libmpi.so.40: cannot open shared object file: No such file or directory__\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py\r\nthon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/jon/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/jon/anaconda3/lib/python3.6/imp.py\", line 347, in load_dynamic\r\n    return _load(spec)\r\n__ImportError: libmpi.so.40: cannot open shared object file: No such file or directory__\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nModuleSpec(name='_pywrap_tensorflow_internal', loader=<_frozen_importlib_external.ExtensionFileLoader object at 0x7f059de53c18>, origin='/home/jon/.c\r\nache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python\r\n_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so')\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 28.562s, Critical Path: 4.92s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["@jw447 Please check [tested build configurations](https://www.tensorflow.org/install/source#linux). Bazel 0.15.0 is supported with TF1.12. If possible could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) to install TF1.12. I had recently installed TF1.12-gpu successfully. One question. Were you able to follow instructions at [TF website](https://www.tensorflow.org/install/source). Thanks!", "@jvishnuvardhan Thank you for the suggestions. I got the problem fixed. Turns out I need to \"export CC=mpicc\" if MPI support is enabled."]}, {"number": 25676, "title": "[ROCm] Adding ROCm support for code in the tensorflow/core/common_runtime/gpu directory", "body": "This PR contains commits to add ROCm support for code within the directory\r\n     `tensorflow/core/common_runtime/gpu` \r\n\r\nPrior to this PR the gpu implementation specific code within that dir is for CUDA only, and this PR adds the ROCm equivalent. \r\n\r\nSome files/variables have been renamed from *cuda* to *gpu* to make code more generic\r\n\r\nThe PR is broken down into several commits for ease-of-review.  I can squash them into one prior to the merge if need be.\r\n\r\n@tatianashp. Adding you to the cc-list just as an FYI \r\n@whchung ", "comments": ["I'm not the right person to review this TensorFlow change.\r\n\r\n@tatianashp @tatatodd I'm not sure whose domain this falls under.", "@chsigg \r\nThank you for the quick response. \r\n\r\nDo you need me to push out a commit with the `std::string` to `string` change, or is that something you will take care of when you do the manual merge? \r\n\r\nIs there anything else that needs to be done before this PR can be merged?\r\n ", "If you could push a commit, that would be appreciated. Thanks!\n\nOn Wed, Feb 13, 2019 at 3:04 PM Deven Desai <notifications@github.com>\nwrote:\n\n> @chsigg <https://github.com/chsigg>\n> Thank you for the quick response.\n>\n> Do you need me to push out a commit with the std::string to string\n> change, or is that something you will take care of when you do the manual\n> merge?\n>\n> Is there anything else that needs to be done before this PR can be merged?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25676#issuecomment-463209303>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHLOjqg04chsGqdD0WpuwqsUacDW6SRWks5vNBuBgaJpZM4a1GIj>\n> .\n>\n", "@chsigg \r\n\r\ndone...pushed out the commit with the requested change.", "Thanks, I'm working on getting this PR merged. Can't tell yet how long it\nwill take because it also requires some internal changes. I will give an\nupdate if not done by Monday.\n\nOn Wed, Mar 6, 2019, 17:38 Deven Desai <notifications@github.com> wrote:\n\n> *@deven-amd* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/BUILD\n> <https://github.com/tensorflow/tensorflow/pull/25676#discussion_r263026305>\n> :\n>\n> > @@ -156,6 +156,8 @@ load(\n>  )\n>  load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n>  load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda_is_configured\")\n> +load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm\")\n>\n> done.\n>\n> also rebased the commits to squash all the code review related update\n> commits into one\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25676#discussion_r263026305>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHLOjlEKhmD1iyxrEe5M7opSKFYo6h7uks5vT-8PgaJpZM4a1GIj>\n> .\n>\n", "This PR has landed (commit d280d3d). I don't know why our tools haven't updated the PR (maybe just not yet?).", "@chsigg \r\n\r\nThank you for reviewing and getting this PR merged. We appreciate very much all the effort you are putting into merging the ROCm PRs.\r\n\r\ndeven", "@deven-amd could you please rebase your branch", "@rthadur, no need to rebase, this PR has already landed. I will close it manually to avoid confusion."]}, {"number": 25675, "title": "Tests for ZipDataset and ConcatenateDataset", "body": "This PR adds tests for `ZipDataset` and `ConcatenateDataset`. It also fixes some typo issues in `map_dataset_op_test.cc` and `range_dataset_op_test.cc`.\r\n\r\ncc @jsimsa \r\n ", "comments": ["@rachellim could you please help review this PR?", "Thanks for your review @shahzadlone ! The parameter types have been changed at this [commit](https://github.com/tensorflow/tensorflow/pull/25675/commits/683cd69ec98aa23663bb5dcd6997aefb8c117e32).", "@rachellim Thanks for your detailed comments! The issues are addressed in [this commit](https://github.com/tensorflow/tensorflow/pull/25675/commits/ada6d804ff33a7f574644a309a1a6aaddbf66893). Could you help review the changes?", "@rachellim The code has been refactored to reduce the repeated code by [this commit](https://github.com/tensorflow/tensorflow/pull/25675/commits/f1711f4e0366345311846d1985383d5af61389df). Could you have a look at the change? After we finish this, I will refactor the tests for `ZipDataset` as well.", "Thanks for the changes so far! Some final comments to clean it up a little. ", "@rachellim Thanks for your suggestion, which makes the code much cleaner now! Could you please have a look at the change at [this commit](https://github.com/tensorflow/tensorflow/pull/25675/commits/6bcdd92fa68c05f98e66d7d1bcf4c41de6698345) when you have time?"]}, {"number": 25674, "title": "Segfault when using VLOG with with MKL on 1.13rc1", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): from source with MKL\r\n```\r\nbazel build --config=mkl --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mavx --copt=-mfma --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n- TensorFlow version (use command below): 1.13 rc1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): 5.3\r\n- CUDA/cuDNN version: 10.0, 7.4.1\r\n- GPU model and memory: Nvidia V100, 16GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nSegfault when running an estimator test\r\n\r\n**Describe the expected behavior**\r\nNo crash\r\n\r\n**Code to reproduce the issue**\r\n```git clone https://github.com/tensorflow/models && cd models/samples/core/get_started && TF_CPP_MIN_VLOG_LEVEL=1 python premade_estimator.py```\r\nhttps://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py\r\n\r\n**Other info / logs**\r\n```\r\n#0  0x00007fff474a1e57 in tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&) ()\r\n   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#1  0x00007fff4bb9b6dc in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::string, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long long*) ()\r\n   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff4bb9c19c in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff4bb9e478 in tensorflow::DirectSession::GetOrCreateExecutors(absl::Span<std::string const>, absl::Span<std::string const>, absl::Span<std::string const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\r\n   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff4bb9fad4 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\r\n   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff49b36ad5 in tensorflow::SessionRef::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\r\n   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fff49d4c1ff in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) [clone .constprop.654] () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fff49d4ca79 in TF_SessionRun () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fff49b31ec1 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()\r\n   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fff49b31fa2 in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fff49ae633c in _wrap_TF_SessionRun_wrapper () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```", "comments": ["This issue happens only when we turn on VLOG with `TF_CPP_MIN_VLOG_LEVEL=1`", "@rahul003 The is not an MKL issue per say. `options.graph` in `optimization_registry.cc` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/optimization_registry.cc#L44) could be invalid depending on the phase of the graph pass. So dumping it could cause a problem.\r\n\r\nI have a fix for this issue. Let me submit a PR.", "Thanks @nhasabni , look forward to the fix!", "The PR for fix is out for review."]}]