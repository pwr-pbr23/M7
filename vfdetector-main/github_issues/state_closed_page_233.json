[{"number": 47558, "title": "Unable to create custom Wide and Deep model with shared inputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Bundled along with Colab\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n\r\n**Current behavior**\r\nI wanted to create a Bayesian W&D model with MC Dropout (hence, cannot use the standard Tensorflow model). Separate inputs from a CSV file are provided to the Wide & Deep parts of the network. However, there are a few overlaps in the input (e.g. \"discount\" column in the CSV dataset is used for a crossed column in the Wide part & as one-hot encoded categorical feature column in the Deep part). BUT, I'm getting a Graph Disconnected error although the inputs are perfect during model building. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nGiven the following `wide_inputs` and `deep_inputs`:\r\n\r\n**wide_inputs:**\r\n{'campaign_category': <KerasTensor: shape=(None,) dtype=string (created by layer 'campaign_category')>,\r\n 'discount': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'discount')>,\r\n 'free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'free_shipping')>,\r\n 'frequency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'frequency_score')>,\r\n 'has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'has_urgency')>,\r\n 'is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_discount_mentioned')>,\r\n 'is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_exclusive')>,\r\n 'is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_one_for_free')>,\r\n 'recency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'recency_score')>,\r\n 'retention_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'retention_score')>,\r\n 'riid': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid')>,\r\n 'sends_since_last_open': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sends_since_last_open')>,\r\n 'sent_dayofweek': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_dayofweek')>,\r\n 'sent_hr': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_hr')>,\r\n 'sent_week': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_week')>,\r\n 'sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sl_contains_price')>}\r\n\r\n\r\n**deep_inputs:**\r\n{'discount': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'discount')>,\r\n 'free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'free_shipping')>,\r\n 'frequency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'frequency_score')>,\r\n 'has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'has_urgency')>,\r\n 'is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_discount_mentioned')>,\r\n 'is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_exclusive')>,\r\n 'is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_one_for_free')>,\r\n 'promo': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'promo')>,\r\n 'recency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'recency_score')>,\r\n 'retention_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'retention_score')>,\r\n 'riid': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid')>,\r\n 'sale': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sale')>,\r\n 'sends_since_last_open': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sends_since_last_open')>,\r\n 'sent_dayofweek': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_dayofweek')>,\r\n 'sent_hr': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_hr')>,\r\n 'sent_week': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_week')>,\r\n 'sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sl_contains_price')>}\r\n\r\n```python\r\ndef build_bayesian_wide_and_deep_model(wide_inputs, wide_feature_columns, \r\n                        deep_inputs, dnn_feature_columns, dnn_hidden_units, \r\n                        multihead_count = 64, p_value=0.5):\r\n\r\n    #Build the Deep Network\r\n    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(deep_inputs)    \r\n\r\n    for layerno, numnodes in enumerate(dnn_hidden_units):\r\n        deep = tf.keras.layers.Dense(numnodes, activation='relu', name='dnn_{}'.format(layerno+1))(deep)\r\n    \r\n    deep = tf.keras.Model(inputs=deep_inputs, outputs=deep)\r\n    \r\n    #Build the Wide Network\r\n    wide = tf.keras.layers.DenseFeatures(wide_feature_columns, name='wide_inputs')(wide_inputs)\r\n    wide = tf.keras.layers.Dense(1, activation=\"linear\", name=\"wide_output\")(wide)\r\n    wide = tf.keras.Model(inputs=wide_inputs, outputs=wide)\r\n\r\n    #Concatenate the Wide & Deep\r\n    both = tf.keras.layers.concatenate([deep.output, wide.output], name='both')\r\n\r\n    #Create the multi-head layer\r\n    multihead_pre_dropout = tf.keras.layers.Dropout(p_value)(both, training=True)\r\n    multihead = tf.keras.layers.Dense(multihead_count, activation='relu', name='multihead')(multihead_pre_dropout)\r\n    multihead_dropout = tf.keras.layers.Dropout(p_value)(multihead, training=True)\r\n\r\n    #Create the output layer\r\n    output = tf.keras.layers.Dense(2, activation='softmax', name='optimal_action')(multihead_dropout)\r\n    \r\n    #Convert the 2 inputs dictionary into a single list\r\n    full_inputs = (wide.input).copy()\r\n    full_inputs.update(deep.input)\r\n    model = tf.keras.Model(inputs=full_inputs, outputs=output)\r\n    model.compile(optimizer='adam',\r\n                  loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\nbwdmodel = build_bayesian_wide_and_deep_model(wide_inputs = crossed_columns_input, wide_feature_columns = crossed_columns,  deep_inputs = deep_columns_input, dnn_feature_columns = deep_columns, dnn_hidden_units = [512, 256, 128], multihead_count = 64, p_value=0.5)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe traceback is as follows\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-33-5f875643195f> in <module>()\r\n     39 bwdmodel = build_bayesian_wide_and_deep_model(wide_inputs = crossed_columns_input, wide_feature_columns = crossed_columns, \r\n     40                                deep_inputs = deep_columns_input, dnn_feature_columns = deep_columns,\r\n---> 41                                dnn_hidden_units = [512, 256, 128], multihead_count = 64, p_value=0.5)\r\n     42 tf.keras.utils.plot_model(bwdmodel, '/content/drive/MyDrive/Bandit_Project/models/bayesian_w&d.png', show_shapes=False, rankdir='LR')\r\n\r\n5 frames\r\n<ipython-input-33-5f875643195f> in build_bayesian_wide_and_deep_model(wide_inputs, wide_feature_columns, deep_inputs, dnn_feature_columns, dnn_hidden_units, multihead_count, p_value)\r\n     31     full_inputs.update(deep.input)\r\n     32     #pdb.set_trace()\r\n---> 33     model = tf.keras.Model(inputs=full_inputs, outputs=output)\r\n     34     model.compile(optimizer='adam',\r\n     35                   loss='categorical_crossentropy',\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)\r\n    118     generic_utils.validate_kwargs(kwargs, {})\r\n119     super(Functional, self).__init__(name=name, trainable=trainable)\r\n--> 120     self._init_graph_network(inputs, outputs)\r\n    121 \r\n    122   @trackable.no_automatic_dependency_tracking\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)\r\n    202     # Keep track of the network's nodes and layers.\r\n    203     nodes, nodes_by_depth, layers, _ = _map_graph_network(\r\n--> 204         self.inputs, self.outputs)\r\n    205     self._network_nodes = nodes\r\n    206     self._nodes_by_depth = nodes_by_depth\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _map_graph_network(inputs, outputs)\r\n    988                              'The following previous layers '\r\n    989                              'were accessed without issue: ' +\r\n--> 990                              str(layers_with_complete_input))\r\n    991         for x in nest.flatten(node.outputs):\r\n    992           computable_tensors.add(id(x))\r\n\r\nValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name='discount'), name='discount', description=\"created by layer 'discount'\") at layer \"wide_inputs\". The following previous layers were accessed without issue: ['deep_inputs', 'dnn_1']\r\n```\r\n", "comments": ["@karthajee,\r\nOn running the code, I am facing an error stating `NameError: name 'crossed_columns_input' is not defined`. \r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Alternatively, you can also share the Colab notebook you're running, with us. Thanks!", "@amahendrakar Ah yes, I cannot share the notebook because the full notebook is confident. However, I can share the necessary inputs to run the code. \r\n\r\n### For wide_inputs = crossed_columns_input and wide_feature_columns = crossed_columns\r\n```python\r\n#Build a list of crossed columns\r\ncrossed_columns_orig = [\r\n  tf.feature_column.crossed_column([\"riid\", 'campaign_category'], hash_bucket_size=20000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'discount'], hash_bucket_size=10000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'is_one_for_free'], hash_bucket_size=4000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'free_shipping'], hash_bucket_size=4000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'is_exclusive'], hash_bucket_size=4000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'has_urgency'], hash_bucket_size=4000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'sl_contains_price'], hash_bucket_size=4000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'is_discount_mentioned'], hash_bucket_size=4000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'sent_week'], hash_bucket_size=10000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'sent_dayofweek'], hash_bucket_size=60000000),\r\n  tf.feature_column.crossed_column([\"riid\", 'sent_hr'], hash_bucket_size=50000000),\r\n]\r\n\r\n#Wrap an indicator column around each of them to make them compatible with Keras\r\ncrossed_columns = [tf.feature_column.indicator_column(col) for col in crossed_columns_orig]\r\n\r\n#Listing out dataframe column names to create Input layer tensors for wide part of network\r\ncol_names_for_crossed_columns = [\"riid\", 'campaign_category', \"discount\", \"is_one_for_free\",\r\n                                 \"free_shipping\", \"is_exclusive\", \"has_urgency\", 'sl_contains_price',\r\n                                 'is_discount_mentioned', 'sent_week', 'sent_dayofweek', 'sent_hr']\r\n\r\n#Create feature dictionary to be passed for wide part of network\r\ncrossed_columns_input = {}\r\nfor col_name in col_names_for_crossed_columns:\r\n  if col_name == 'campaign_category':\r\n    crossed_columns_input[col_name] = tf.keras.Input(shape=(), name=col_name, dtype=tf.string)\r\n  else:\r\n    crossed_columns_input[col_name] = tf.keras.Input(shape=(), name=col_name, dtype=tf.int64)\r\n```\r\n\r\n#### For deep_inputs = deep_columns_input and dnn_feature_columns = deep_columns\r\n```python\r\n\"\"\"Numeric Features\"\"\"\r\n#Initialize list and dictionary to store feature columns and corresponding input tensors\r\nnumeric_feature_layer = []\r\nnumeric_feature_layer_input = {}\r\n\r\n#Listing out df column names to create both Input layer tensors and numeric feature columns for deep part of network\r\nnumeric_feature_col_names = ['retention_score','recency_score','frequency_score',\r\n                             'sent_week','sent_dayofweek','sent_hr','discount',\r\n                             'sends_since_last_open']\r\n\r\n#Build numeric feature columns and corresponding inputs, to be fed to deep part\r\nfor feature in numeric_feature_col_names:\r\n\r\n  if feature in ['retention_score','recency_score','frequency_score']:\r\n    numeric_feature_col = tf.feature_column.numeric_column(feature, dtype = tf.float32)\r\n    numeric_feature_layer_input[feature] = tf.keras.Input(shape=(), name=feature, dtype=tf.float32)\r\n  else:\r\n    numeric_feature_col = tf.feature_column.numeric_column(feature, dtype = tf.int64)\r\n    numeric_feature_layer_input[feature] = tf.keras.Input(shape=(), name=feature, dtype=tf.int64)\r\n\r\n  numeric_feature_layer.append(numeric_feature_col)\r\n\r\n\"\"\"Categorical Features (OHE)\"\"\"\r\n\r\n#Initialize list and dictionary to store ohe categorical feature columns & corresponding input tensors\r\ncategorical_feature_layer = []\r\ncategorical_feature_layer_input = {}\r\n\r\n#Listing out df column names to create both Input layer tensors and ohe categorical feature columns for deep part of network\r\nCATEGORIES = {\r\n    'promo' : [0, 1],\r\n    'sale' : [0, 1],\r\n    'campaign_category': ['CC1', 'CC2', 'CC3', 'CC4', 'CC5', 'CC6', 'CC7', 'CC8', 'CC9', 'CC10'],\r\n    'is_one_for_free': [0, 1],\r\n    'free_shipping': [0, 1],\r\n    'is_exclusive': [0, 1],\r\n    'has_urgency': [0, 1],\r\n    'sl_contains_price': [0, 1],\r\n    'is_discount_mentioned': [0, 1],\r\n}\r\n\r\n#Build ohe categorical feature columns and corresponding inputs, to be fed to deep part\r\nfor (feature, vocab) in CATEGORIES.items():\r\n  if feature == \"campaign_category\":  \r\n    categorical_feature_col = tf.feature_column.categorical_column_with_vocabulary_list(feature, vocab, dtype=tf.string)\r\n    categorical_feature_layer_input[feature] = tf.keras.Input(shape=(), name=feature, dtype=tf.string)\r\n  else:\r\n    categorical_feature_col = tf.feature_column.categorical_column_with_vocabulary_list(feature, vocab, dtype=tf.int64)\r\n    categorical_feature_layer_input[feature] = tf.keras.Input(shape=(), name=feature, dtype=tf.int64)\r\n  \r\n  categorical_feature_col_ind = tf.feature_column.indicator_column(categorical_feature_col)\r\n  categorical_feature_layer.append(categorical_feature_col_ind)\r\n\r\n\"\"\"\"Categorical Features (Embedding)\"\"\"\r\n#Initialize list and dictionary to store embedding categorical feature column & corresponding input tensors\r\nembedding_feature_layer = []\r\nembedding_feature_layer_input = {}\r\n\r\n#Build embedding categorical feature column and corresponding input for riid column in df, to be fed to deep part\r\nriid = tf.feature_column.categorical_column_with_hash_bucket(\"riid\", hash_bucket_size=2000000, dtype=tf.int64)\r\nriid_embedding = tf.feature_column.embedding_column(riid, dimension=32)\r\nembedding_feature_layer.append(riid_embedding)\r\nembedding_feature_layer_input[\"riid\"] = tf.keras.Input(shape=(), name=\"riid\", dtype=tf.int64)\r\n\r\n\"\"\"Creating deep_columns and deep_columns_input\"\"\"\r\ndeep_columns = numeric_feature_layer + categorical_feature_layer + embedding_feature_layer\r\ndeep_columns_input = {**numeric_feature_layer_input, **categorical_feature_layer_input, **embedding_feature_layer_input}\r\n\r\n```\r\n\r\n", "@amahendrakar Hope you have all the information you need to debug? Is there anything else that I can provide?", "@karthajee,\r\nThank you for the update. \r\n\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/45437#issuecomment-742766096) from a member of the TensorFlow team, from similar issue #45437 and check if it helps.", "Hi @amahendrakar, the comment was not directly helpful as I was using the approprite `tf.keras.Input(...)` with the Functional API. However, it helped clarify certain misconceptions that I had.\r\n\r\nI'm adding the working code here and commenting the changes that I had made so that it will be helpful for people who face similar issues.\r\n\r\n```python\r\n#C1: Share the super set of inputs with both wide and part parts, instead of 2 separate wide and deep inputs\r\ndef build_bayesian_wide_and_deep_model(inputs, wide_feature_columns, dnn_feature_columns, dnn_hidden_units, multihead_count = 64, p_value=0.5):\r\n    \r\n    #C2: Do not build a tf.keras.Model(...) for the wide and deep parts separately for now\r\n    #Building a tf.keras.Model(...) requires one to explicitly specify an additional tf.keras.Input(...) not just for the\r\n    #W&D parts (which is anyways \"inputs\") but also for the layers that follow the W&D parts\r\n    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(inputs)    \r\n\r\n    for layerno, numnodes in enumerate(dnn_hidden_units):\r\n        deep = tf.keras.layers.Dense(numnodes, activation='relu', name='dnn_{}'.format(layerno+1))(deep)\r\n    \r\n    wide = tf.keras.layers.DenseFeatures(wide_feature_columns, name='wide_inputs')(inputs)\r\n\r\n    both = tf.keras.layers.concatenate([wide, deep], name='both')\r\n\r\n    pre_multihead_dropout = tf.keras.layers.Dropout(p_value)(both, training=True)\r\n    multihead = tf.keras.layers.Dense(multihead_count, activation='relu', name='multihead')(pre_multihead_dropout)\r\n    post_multihead_dropout = tf.keras.layers.Dropout(p_value)(multihead, training=True)\r\n\r\n    output = tf.keras.layers.Dense(2, activation='softmax', name='optimal_action')(post_multihead_dropout)\r\n    \r\n    model = tf.keras.Model(inputs=inputs, outputs=output)\r\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n    return model```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47558\">No</a>\n"]}, {"number": 47557, "title": "What's the correct matcher of 'tf.nn.conv2d_transpose' in quantization aware training ?", "body": "To some reason, I have to use TF1.X (actually TF1.14.0) to quantize a custom model.\r\nAs general,\r\n```\r\ng = tf.get_default_graph()\r\ntf.contrib.quantize.create_training_graph(input_graph=g,quant_delay=0)\r\n```\r\nwas used to create fakequant min/max variable and I could find the min/max value of 'tf.nn.conv2d_transpose' in checkpoint. But when\r\n```\r\ntf.contrib.quantize.create_eval_graph()\r\n```\r\ncreated the Fakequant Evaluation Graph, 'tf.nn.conv2d_transpose' was not added fake quantization nodes. I tried transpose only, transpose --> relu6, transpose --> bias --> relu6, transpose --> bacthnorm --> relu6, but all of them did not work.\r\n\r\n### **Thus the direct question: What's the correct matcher of 'tf.nn.conv2d_transpose' in quantization aware training ? Is there any resources documented the correct matchers of every op ?**", "comments": ["@ethkim could you triage this issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47557\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47557\">No</a>\n"]}, {"number": 47556, "title": "Lambda(tf.identity) fails to deserialize, while Lambda(lambda t: tf.identity(t)) works fine", "body": "Note: I'm not sure if this belongs here or the Keras bug tracker. I'm filing this report here because I use `tf.keras`, not `keras`.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): bundled in Colab\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n\r\n**Describe the current behavior**\r\n\r\nThe following snippet works fine:\r\n\r\n```python\r\nimport tensorflow as tf\r\nkeras = tf.keras\r\n\r\nmodel_input = keras.layers.Input(shape=[256], dtype=tf.float32)\r\n\r\nmodel_output = keras.layers.Lambda(\r\n    lambda t: tf.identity(t),\r\n    name='identity_in_lambda',\r\n)(model_input)\r\n\r\nmodel = keras.Model(inputs=model_input, outputs=model_output, name='test_model')\r\n\r\nmodel.save('test_model.h5')\r\n\r\nloaded_model = keras.models.load_model('test_model.h5')\r\n```\r\n\r\nIf, however, I replace `lambda t: tf.identity(t)` with just `tf.identity`, deserialization crashes with a rather long traceback:\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py in wrapper(*args, **kwargs)\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n\r\nTypeError: 'str' object is not callable\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n14 frames\r\n<ipython-input-4-ba5e25a08d15> in <module>()\r\n     10 model.save('test_model_broken.h5')\r\n     11 \r\n---> 12 loaded_model = keras.models.load_model('test_model_broken.h5')\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)\r\n    205           (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\r\n    206         return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\r\n--> 207                                                 compile)\r\n    208 \r\n    209       filepath = path_to_string(filepath)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\r\n    182     model_config = json_utils.decode(model_config.decode('utf-8'))\r\n    183     model = model_config_lib.model_from_config(model_config,\r\n--> 184                                                custom_objects=custom_objects)\r\n    185 \r\n    186     # set weights\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)\r\n     62                     '`Sequential.from_config(config)`?')\r\n     63   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n---> 64   return deserialize(config, custom_objects=custom_objects)\r\n     65 \r\n     66 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n    175       module_objects=LOCAL.ALL_OBJECTS,\r\n    176       custom_objects=custom_objects,\r\n--> 177       printable_module_name='layer')\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    356             custom_objects=dict(\r\n    357                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 358                 list(custom_objects.items())))\r\n    359       with CustomObjectScope(custom_objects):\r\n    360         return cls.from_config(cls_config)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in from_config(cls, config, custom_objects)\r\n    667     \"\"\"\r\n    668     input_tensors, output_tensors, created_layers = reconstruct_from_config(\r\n--> 669         config, custom_objects)\r\n    670     model = cls(inputs=input_tensors, outputs=output_tensors,\r\n    671                 name=config.get('name'))\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)\r\n   1283       if layer in unprocessed_nodes:\r\n   1284         for node_data in unprocessed_nodes.pop(layer):\r\n-> 1285           process_node(layer, node_data)\r\n   1286 \r\n   1287   input_tensors = []\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in process_node(layer, node_data)\r\n   1231         input_tensors = (\r\n   1232             base_layer_utils.unnest_if_single_tensor(input_tensors))\r\n-> 1233       output_tensors = layer(input_tensors, **kwargs)\r\n   1234 \r\n   1235       # Update node index map.\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    950     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    951       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 952                                                 input_list)\r\n    953 \r\n    954     # Maintains info about the `Layer.call` stack.\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1089         # Check input assumptions set after layer building, e.g. input shape.\r\n   1090         outputs = self._keras_tensor_symbolic_call(\r\n-> 1091             inputs, input_masks, args, kwargs)\r\n   1092 \r\n   1093         if outputs is None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    820       return nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    821     else:\r\n--> 822       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    823 \r\n    824   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    861           # TODO(kaftan): do we maybe_build here, or have we already done it?\r\n    862           self._maybe_build(inputs)\r\n--> 863           outputs = call_fn(inputs, *args, **kwargs)\r\n    864 \r\n    865         self._handle_activity_regularization(inputs, outputs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)\r\n    915     with backprop.GradientTape(watch_accessed_variables=True) as tape,\\\r\n    916         variable_scope.variable_creator_scope(_variable_creator):\r\n--> 917       result = self.function(inputs, **kwargs)\r\n    918     self._check_variables(created_variables, tape.watched_variables())\r\n    919     return result\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py in wrapper(*args, **kwargs)\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n    204       # TypeError, when given unexpected types.  So we need to catch both.\r\n--> 205       result = dispatch(wrapper, args, kwargs)\r\n    206       if result is not OpDispatcher.NOT_SUPPORTED:\r\n    207         return result\r\n\r\nTypeError: 'module' object is not callable\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nReplacing `lambda t: tf.identity(t)` with `tf.identity` should not change behavior.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1K24uCFzIBeYW2bPBuu01cRHNRQ9vuld1?usp=sharing", "comments": ["I am able to replicate the issue reported on tf 2.3,tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e7dfabaed4c7a2b5037e67c92b2e5b77/untitled557.ipynb).", "A Python `lambda` is serialized as bytecode (so it's not cleanly serialized and won't be portable). Meanwhile a function like `tf.identity` is serialized via its name \"identity\" (so it's portable even across TF versions and across different OSes).\r\n\r\nIf you want to be able to deserialize the name \"identity\", it's simple, just register it so that Keras can recognize it:\r\n\r\n```python\r\nloaded_model = keras.models.load_model('test_model.h5', custom_objects={'identity': tf.identity})\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47556\">No</a>\n", "@fchollet I believe that users of Keras in general expect that a call to `model.save()` followed by `keras.models.load_model()` should produce an object that behaves in the exact same way as the original. With the current version, this is not the case: a call to `load_model()` ends up with an obscure exception that is hard to decipher and hard to debug (although, now that this thread exists, perhaps people who encounter the same problem will find it). This is making Keras UX unnecessarily worse than it could be.\r\n\r\nIt seems that `.save()` is aware that `tf.identity` is some sort of a special object, since it serializes it as a string instead of pickling it. Therefore, perhaps Keras should register `\"identity\"` as a \"custom object\" by default, in order to enable `load_model()` to do the same? Or maybe `.save()` could throw an exception saying that it cannot serialize `tf.identity` in such way that it will be possible to deserialize it? Or maybe there is another idiomatic way to make `load_model()` the inverse of `.save()` (and vice versa)?\r\n\r\nI'd also like to note two things:\r\n\r\n1. `Lambda(tf.identity)` is useful for renaming tensors due to its `name=` kwarg.\r\n2. `pylint` suggests replacing `lambda t: tf.identity(t)` with `tf.identity`, which is how I encountered this problem in the first place."]}, {"number": 47555, "title": "Optimize TFLite argmin/argmax", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Pixel 3a Android 11\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): nightly\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```python\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.Input(shape=(512,), batch_size=1, dtype=tf.int32),\r\n  tf.keras.layers.Embedding(1000, 128),\r\n  tf.keras.layers.LSTM(64, return_sequences=True),\r\n  tf.keras.layers.Dense(5000),\r\n  tf.keras.layers.Lambda(lambda x: tf.math.argmax(x, axis=-1)),\r\n])\r\n```\r\n\r\nGiven a simple sequence model with 5000 output labels, ArgMax takes as much time as computation-intense ops like FullyConnected and LSTM. Here is my benchmark on pixel 3a\r\n\r\n```\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n        UNIDIRECTIONAL_SEQUENCE_LSTM            1           27.552          33.447%         33.447%          0.000              1\r\n                 FULLY_CONNECTED                1           27.474          33.353%         66.800%          0.000              1\r\n                         ARG_MAX                1           24.868          30.189%         96.989%          0.000              1\r\n                         RESHAPE                1            2.343           2.844%         99.834%          0.000              1\r\n                          GATHER                1            0.137           0.166%        100.000%          0.000              1\r\n```\r\n\r\nLooks like compiler is not smart enough to vectorize naive-for-loop argmin-max reduction. In order to improve the performance, we should at least specialize the implementation when `axis=dims-1`, that is, `inner_size=1` in the following code snippet. If the request is valid, I can try to handcraft some Neon intrinsics to see if the performance is better.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ea49c350b3b812bd852ae8456d2c8cc29aff6c95/tensorflow/lite/kernels/internal/reference/arg_min_max.h#L49-L63", "comments": ["Thanks for filing the feature request!", "@abattery Here is my benchmark on argmax of 1-D float32 tensor because I only focus on `axis=-1`. For 2-D or higher tensor with `axis=-1`, it will roughly take `outer_size * timeof(axis_size)` time.\r\n\r\nOn Pixel 3a android 11.\r\n\r\n| Size | Naive (min/max/avg microsecs) | Hand-crafted Neon intrinsics (min/max/avg microsecs) |\r\n| ---- | -------| ------------ |\r\n| 10 | 0/63/0.260737| 0/49/0.216432  |\r\n| 100 | 1/101/1.17919   | 0/77/0.271518 |\r\n| 1000 | 9/98/9.80665 | 0/65/1.07653  |\r\n| 10000 | 95/190/96.2557  | 8/110/9.63559 |\r\n| 100000 | 952/1102/967.445  | 88/215/94.8766 |\r\n\r\nLet me know if it's acceptable!", "Looks like it can achieve 10x improvement. Very impressive! Look forward to seeing the PR soon."]}, {"number": 47553, "title": "A sentence in Basic Image Classification Tutorial is not clear", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/keras/classification#make_predictions\r\n\r\n## Description of issue (what needs changing): \r\nThe statement, **The model's linear outputs, [logits](https://developers.google.com/machine-learning/glossary#logits).** can't be understood.\r\n\r\nMore explanation can be added on why the Outputs are Linear.", "comments": ["In that blog, the outputs of the model are of course linear. They define the model as \r\n\r\n```python\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dense(10) # < ---------------- Look here\r\n])\r\n```\r\n\r\nHere you can see they didn't use any activation and by default, it's `None`. And that makes it a linear layer. So the output would be `logit`. And further, they add another layer which is `softmax` to get `probabilities` from the `logit`. \r\n\r\n```\r\nprobability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\r\n```\r\n\r\nSo, `model` here is giving linear outputs (`logit`) but when we add the `softmax` layer then it will give `probabilities` from the `logit`. ", "@innat,\r\nThank you for your response. I agree with your explanation but my point is that we can add the same explanation in the Tutorial so that it will be clear to Newbies. Also, the sentence formation of the statement, **`The model's linear outputs, logits`**, is not proper and is incomplete.", "It is a image classification problem that is why we are using Softmax()  activation function in\r\nprobability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])", "@worldpeaceaspirer logits are normally raw outputs of a classification model. A Normalization function like softmax is added for normalized probabilities in the prediction. And the logits hyperlink clearly explains what logits are and how we can use normalization functions based on the type of classification problem. Beginners normally have an idea of logits and the use of activation functions so I guess there is no need to add anything new to the tutorial.", "Reviewed. Thanks everyone!"]}, {"number": 47552, "title": "Hyperlinks can be added for improved understand-ability of Basic Image Classification Tutorial ", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/keras/classification#import_the_fashion_mnist_dataset\r\nhttps://www.tensorflow.org/tutorials/keras/classification#set_up_the_layers\r\nhttps://www.tensorflow.org/tutorials/keras/classification#compile_the_model\r\nhttps://www.tensorflow.org/tutorials/keras/classification#feed_the_model\r\n\r\n## Description of issue (what needs changing):\r\n1. The phrase, **`load the Fashion MNIST data`** In the statement, **`Import and load the Fashion MNIST data directly from TensorFlow:`**, can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data\r\n\r\n2. The word, **`layer`**, In the statement, **`The basic building block of a neural network is the layer`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/layers\r\n\r\n3. For better readability, The arguments in the Dense Layers can be named i.e., `tf.keras.layers.Dense(128)` can be replaced with `tf.keras.layers.Dense(units = 128)`\r\n\r\n4. The word **`compile`** in the statement, **`These are added during the model's compile step:`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\r\n\r\n5. The word **`Loss Function`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/losses\r\n\r\n6. The word **`Optimizer`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\r\n\r\n7. The word **`Metrics`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/metrics\r\n\r\n8. **`model.fit`** in the statement, **`To start training, call the model.fit method\u2014`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n\r\n### Clear description\r\n\r\nAdding the Hyperlinks for each API helps the beginners understand the functionality.", "comments": ["Closing since the associated PR has been merged. Thanks!"]}, {"number": 47551, "title": "Wrong output values in TFLite_Detection_PostProcess operation when max_classes_per_detection > 1", "body": "**System information**\r\n\r\nOS Platform and Distribution: Ubuntu 18.04\r\nTensorFlow installed from: pypi\r\nTensorFlow version: 1.15.2\r\nPython version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nWhen you add the NMS operation at exporting the TFlite graph with the option `max_classes_per_detection > 1`, you'll get more than one class per detection as expected, but the class IDs and their corresponding scores are not consecutive in the output array. Instead, the output arrays have a position without value, that only contains garbage memory.\r\n\r\nFor example, with `max_classes_per_detection = 2`, you get:\r\n\r\noutput[0] = detection1_top1_class_id\r\noutput[1] = garbage\r\noutput[2] = garbage\r\noutput[3] = detection1_top2_class_id\r\noutput[4] = garbage\r\noutput[5] = garbage\r\n...\r\n\r\n**Describe the expected behavior**\r\n\r\noutput[0] = detection1_top1_class_id\r\noutput[1] = detection1_top2_class_id\r\noutput[2] = detection2_top1_class_id\r\n...\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nExport the model with:\r\n\r\n```bash\r\npython3 export_tflite_ssd_graph.py \\\r\n    --pipeline_config_path=configs/ssdlite_mobiledet_cpu_320x320_coco_sync_4x4.config \\\r\n    --trained_checkpoint_prefix=ssdlite/model.ckpt-100000 \\\r\n    --output_directory=ssdlite/tflite \\\r\n    --max_detections=10 \\\r\n    --max_classes_per_detection=2 \\\r\n    --add_postprocessing_op=true\r\n```\r\n\r\n```\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path='ssdlite/tflite/model.tflite'))\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model\r\ninput_shape = input_details[0]['shape']\r\ninput_data = load_image_into_numpy_array(image_paths[0])\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data\r\ncount = int(interpreter.get_tensor(output_details[3]['index'])[0])\r\nboxes = interpreter.get_tensor(output_details[0]['index'])[:, :count]\r\nclasses = interpreter.get_tensor(output_details[1]['index'])[:, :count]\r\nscores = interpreter.get_tensor(output_details[2]['index'])[:, :count]\r\n\r\n# classes[1] and scores[1] will contain random numbers from unassigned memory slots\r\n```\r\n\r\nI think that the error stems from this line of code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/127adf71b4a995265cdcc960de38c164c93d53a9/tensorflow/lite/kernels/detection_postprocess.cc#L710\r\n\r\n`box_offset` should have the value of `output_box_index` (which is already incremented in the inner loop where the top classes of the detection are assigned) or move the line `output_box_index++;` out of the inner loop to really make it account for the number of detections", "comments": ["@thaink could you review this suggestion?", "Agreed that that loop looks wrong. Basically, we can do either stack all valid values first like:\r\noutput[0] = detection1_top1_class_id\r\noutput[1] = detection2_top1_class_id\r\noutput[2] = detection2_top2_class_id\r\noutput[3] = garbage (because detection 1 has only one class)\r\nOr use memory like for a matrix:\r\noutput[0] = detection1_top1_class_id\r\noutput[1] = garbage (because detection 1 has only one class)\r\noutput[2] = detection2_top1_class_id\r\noutput[3] = detection2_top2_class_id\r\nWe are following the second pattern (so people can random access to a detection). But the implementation looks wrong.\r\n\r\nIn my idea, the box_offset should be: ` int box_offset = max_categories_per_anchor * output_box_index + col; `\r\nand the `output_box_index++;` should be moved to outer loop.", "Thanks @thaink and @abattery,\r\n\r\nI agree with your idea to solve the problem. If the bug is fixed, would it be solved both in TF1 and TF2?", "It is fixed at commit 0b2560e217cf5bc7c9f931a3185c9c8e3f2e9d5e in master branch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47551\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47551\">No</a>\n", "Awesome, thanks!"]}, {"number": 47550, "title": "Validation fails while model fit", "body": "**System information**\r\n- OS Platform and Distribution: Linux Mint 19.3\r\n- TensorFlow installed from: pypi\r\n- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6081336/tf_env.txt)\r\n\r\n**Describe the current behavior**\r\nTrain finished complete, but validation fails\r\n\r\n**Describe the expected behavior**\r\nTrain and validation succeeded\r\n\r\n**Standalone code to reproduce the issue**\r\nI have some TF.Example files and load it as:\r\n```python\r\ndef get_dataset(files_pattern: str, strategy: tf.distribute.Strategy, batch_size: int):\r\n    global_batch_size = _get_global_batch_size(strategy, batch_size)\r\n    dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(files_pattern))\r\n    dataset = dataset.shuffle(10 * batch_size)\r\n    # dataset = dataset.repeat()\r\n    dataset = dataset.batch(global_batch_size)\r\n    dataset = dataset.map(_parse_example)  # vectorised tf.train.Example parsing\r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n    options = dataset_ops.Options()\r\n    options.experimental_distribute.auto_shard_policy = dataset_ops.distribute_options.AutoShardPolicy.DATA\r\n    dataset = dataset.with_options(options)\r\n\r\n    return dataset\r\n```\r\n\r\nModel training:\r\n```python\r\ndef run_local(train_pattern, eval_pattern, batch_size, model_dir, log_path):\r\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\r\n\r\n    train_ds = data.get_dataset(train_pattern, strategy, batch_size)\r\n    eval_ds = data.get_dataset(eval_pattern, strategy, batch_size)\r\n\r\n    with strategy.scope():\r\n        model = _build_keras_model(\r\n            hidden_units=DNN_HIDDEN_UNITS,\r\n            learning_rate=LEARNING_RATE)\r\n\r\n    profile_options = tf.profiler.experimental.ProfilerOptions(\r\n        host_tracer_level=1, python_tracer_level=1, device_tracer_level=1,\r\n    )\r\n    tf.profiler.experimental.start(log_path, options=profile_options)\r\n\r\n    # Write logs to path\r\n    callbacks = get_callbacks(log_path, 0)\r\n\r\n    model.fit(\r\n        train_ds,\r\n        validation_data=eval_ds,\r\n        callbacks=callbacks)\r\n\r\n    tf.profiler.experimental.stop()\r\n\r\n    _save_model(model_dir, model, strategy)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTrain on None steps\r\n   1000/Unknown - 492s 491ms/step - batch: 499.5000 - size: 1.0000 - loss: 0.0650 - tp: 32328.0000 - fp: 28630.0000 - tn: 9838817.0000 - fn: 100225.0000 - accuracy: 0.9871 - precision: 0.5303 - recall: 0.2439 - auc: 0.8881 - mse: 0.0125 - binary_crossentropy: 0.0646/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\r\nTraceback (most recent call last):\r\n  File \"/snap/pycharm-community/226/plugins/python-ce/helpers/pydev/pydevd.py\", line 1477, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/snap/pycharm-community/226/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/local/projects/***/model/local_train.py\", line 12, in <module>\r\n    run_local(train_pattern, eval_pattern, batch_size, model_path, log_path)\r\n  File \"/home/local/projects/***/model/keras.py\", line 310, in run_local\r\n    model.fit(\r\n  File \"/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 789, in fit\r\n    return func.fit(\r\n  File \"/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_distributed_v1.py\", line 675, in fit\r\n    return training_arrays_v1.fit_loop(\r\n  File \"/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\", line 417, in model_iteration\r\n    val_results = model_iteration(\r\n  File \"/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\", line 347, in model_iteration\r\n    index_array = np.arange(num_samples_or_steps)\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\n```", "comments": ["@ferryvg,\r\nIn the given code snippet, you have defined the functions but are not calling them anywhere. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/156837ac5e88bef8d10badc553ec840f/47550.ipynb).\r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Thanks!", "@amahendrakar , it's works in notebook, very strange", "Error raises when i use `tf.compat.v1.disable_eager_execution()`"]}, {"number": 47549, "title": "How can i secure my .tflite model on edge device?", "body": "I have built the TensorFlow model and converted it into a .tflite model, I want to deploy my .tflite model into an edge device(android or IoT).\r\nFinal goal: I want to protect my model from being a copy or reverse engineered and also the user is only able to use this model in my android app or IoT device.\r\nhow can I do that?", "comments": ["There are some options for encrypting your model, for example, applying [AES method](https://stackoverflow.com/questions/6788018/android-encryption-decryption-with-aes).\r\n\r\nEncryption requires a key and a key should be stored in somewhere safely. For example, you can look at the [security key attestation](https://developer.android.com/training/articles/security-key-attestation)", "how can i secure model in raspberry pi ?\r\n\r\nThanks in advance", "@anilnishad19799,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47549\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47549\">No</a>\n"]}, {"number": 47545, "title": "tf.data.zip and tf.data.cache throws cachefile AlreadyExistsError and/or deletes valid cachefiles", "body": "**System information**\r\nRunning on Ubuntu 20.04 using the docker container 2.4.1-gpu-jupyter.\r\nAlso occurs on Google Colab environment running TF 2.4.1\r\n\r\n**Describe the current behavior**\r\n`tf.data.Dataset.cache` and `tf.data.Dataset.zip` do not operate well together in certain situations, where there is a hierarchy between datasets but nevertheless they are both desired and cached. Imagine the scenario of an audio data pipeline where client extracts spectrograms and uses those spectrograms to extract MFCCs. Furthermore, the user wants to retain the spectrogram dataset at the end of the pipeline and includes in the final `zip` operation. Now, if the spectrogram dataset is cached to disk to allow for faster feature extraction later (say with different hyperparameters), user will be faced the exception: \r\n```\r\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists \r\n```\r\nCurrently the user can sort of get behind this behavior by simply moving the `.cache` operation on the intermediary dataset to after the `.map` operation that creates the conflicting dataset. This will introduce a performance hit, however, unless all the datasets  upstream of the intermediary dataset are cached. Which brings us to the second issue.\r\nEven if the upstream datasets are cached, there is a bug/behavior that is causing the cache files to be deleted after an iteration over the entire zipped dataset is completed. This second bug essentially renders the workaround useless when the user wants to cache both the downstream and upstream datasets.\r\n\r\n**Describe the expected behavior**\r\nThe expected or ideal behavior would be the ability of caching&retaining intermediary dataset without a problem. This would make intuitive sense for users building multi-step pipelines with the `tf.data` API.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe notebook below contains the problematic case as well as the workaround described (and its demise). \r\n[Link to Google Colab Notebook](https://colab.research.google.com/drive/1MOOfB5eJKAEyNTSxHY1vci6Unc5J2mi0?usp=sharing)\r\n\r\n**Other info / logs** \r\n```\r\n---------------------------------------------------------------------------\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2112       ctx.executor = executor_new\r\n-> 2113       yield\r\n   2114     finally:\r\n\r\n9 frames\r\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('spectrogram_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1614817563 [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     67   def wait(self):\r\n     68     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 69     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     70 \r\n     71   def clear_error(self):\r\n```\r\n", "comments": ["@kdonbekci \r\nI ran the code shared but do not see the error reported on tf 2.4, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/0392fdc2e633c9207b98abf329f64cf1/untitled557.ipynb).", "I am sorry if I can't see but running the gist you've sent, I still see that the `AlreadyExistsError` is being caught by the code block demonstrating the issue. I've pasted the mentioned block here. (Changed unnecessary AUTOTUNE to 1 in `num_parallel_calls`) \r\n```\r\ntry:\r\n  ds1 = ds.map(hard_problem, num_parallel_calls=1, deterministic=True).cache('spectrogram')\r\n  ds2 = ds1.map(hard_problem, num_parallel_calls=1, deterministic=True).cache('mfcc')\r\n  zipped = tf.data.Dataset.zip((ds1,ds2))\r\n  for x in zipped:\r\n      pass\r\nexcept tf.errors.AlreadyExistsError as e:\r\n    print(e)\r\n```\r\n", "And here is the entire self-sufficient code block + output demonstrating the issue again,\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint('TF Version: ',tf.__version__)\r\nhard_problem = lambda x: x+1\r\nds = tf.data.Dataset.from_tensor_slices(np.random.rand(100, 50, 50))\r\nds1 = ds.map(hard_problem, num_parallel_calls=1).cache('spectrogram')\r\nds2 = ds1.map(hard_problem, num_parallel_calls=1).cache('mfcc')\r\nzipped = tf.data.Dataset.zip((ds1,ds2))\r\ntry:\r\n  for x in zipped:\r\n      break\r\nexcept tf.errors.AlreadyExistsError as e:\r\n  print('ERROR')\r\n  print(e)\r\n```\r\nOUTPUT\r\n```\r\n>>> TF Version:  2.4.1\r\n>>> ERROR\r\n>>> There appears to be a concurrent caching iterator running - cache lockfile already exists ('spectrogram_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1614907806\r\n```\r\n", "@ymodak \r\nI am able to replicate the issue reported on tf 2.3, 2.4 and nightly, please fin the [gist here](https://colab.research.google.com/gist/Saduf2019/311a685d3e3cffe63736427ceb4720d5/untitled558.ipynb).", "Yes, can confirm the gist contains the error. \r\n\r\nWhat's missing in the gist is the second failure of `.cache` and `.zip` where if multiple cached datasets are zipped together, Tensorflow will fail to cache all of the datasets. In fact, it will only cache a single dataset in the zip and emit warnings for the rest as such:\r\n```\r\n2021-03-04 23:36:18.516294: W tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n2021-03-04 23:36:19.080730: W tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n2021-03-04 23:36:19.317442: W tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n```", "The snippet below illustrates the second issue of incomplete iterators. \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom pathlib import Path\r\nprint('TF Version: ',tf.__version__)\r\nPath('cache').mkdir(exist_ok=True)\r\nhard_problem = lambda x: x+1\r\nds = tf.data.Dataset.from_tensor_slices(np.random.rand(100, 50, 50))\r\nds1 = ds.map(hard_problem, num_parallel_calls=1).cache('cache/spectrogram')\r\nds2 = ds.map(hard_problem, num_parallel_calls=1).cache('cache/mfcc')\r\nzipped = tf.data.Dataset.zip((ds1,ds2))\r\nfor x in zipped:\r\n    pass\r\nprint(list(Path('cache').iterdir()))\r\n```", "Right now, the only way to avoid this not-fully-consumed iterator issue is to a dummy iteration over all of the to-be-zipped dataset before the `.zip` in a redundant/wasteful way.", "Hi @kdonbekci,\r\n\r\nRegarding the AlreadyExistsError, can you try replacing `.cache(filename)` with `.apply(tf.data.experimental.snapshot(filename))`? `snapshot` is similar to cache, but supports concurrent access.\r\n\r\nThe second issue is an unfortunate interaction between `cache` and `zip`. The contract for `cache` (and `snapshot` for that matter) is that they will only finalize the cached data (making it available for future iterations) if the first pass through the data runs to completion. But `zip` will stop reading from all of its inputs as soon as any of the inputs run out of data. As a result, only one of the inputs to zip will run to completion and finalize its cache.", "I will try the `snapshot` experimental feature today and let you know how it goes @aaudiber. \r\n\r\nOn the second issue, I understand the contracts for these ops require complete iteration. Although I am not familiar with the design limitations of either `zip` or `cache|snapshot` ops, it would be great if this unfortunate interaction was removed going forward.\r\n\r\nEither caching without full iteration, like this [torchdata](https://github.com/szymonmaszke/torchdata) package or improving `zip` to accommodate this use case seem like possible alternatives.\r\nEspecially in the case (which is my case) the zipped datasets have valid `.cardinality()` results which are all equal. ", "@aaudiber \r\nI've tried the new `snapshot` feature. By the way, reading the documentation, it's a great addition to the `tf.data` API! \r\n\r\nUnfortunately `snapshot` is not a fix to my issue. It removes the AlreadyExistsError, however, it does not work as expected and the incomplete iteration causes buildup on disk storage. Here is the [colab](https://colab.research.google.com/drive/1s5zNq1xYkvOtxeR7dY9s1iWSm7jqRz79?usp=sharing) that uses the `snapshot` feature and demonstrates the growing cache directory. It seems that this new issue is related to the original second issue of `zip` and `cache|snapshot` interaction.  \r\n", "I have the same issue.\r\nIt seems like you circumvent the issue by force caching by iterating over it (in this example through tfds benchmark).\r\nAfter that is done you can zip without causing an error.\r\n\r\nThis is not really a practical solution, though.\r\n\r\nExample:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow_datasets as tfds\r\nprint('TF Version: ', tf.__version__)\r\nhard_problem = lambda x: x + 1\r\nds = tf.data.Dataset.from_tensor_slices(np.random.rand(100, 50, 50))\r\nds1 = ds.map(hard_problem, num_parallel_calls=1).cache('spectrogram')\r\nds2 = ds1.map(hard_problem, num_parallel_calls=1).cache('mfcc')\r\n\r\n# force full cache write\r\ntfds.core.benchmark(ds1)\r\ntfds.core.benchmark(ds2)\r\n\r\nzipped = tf.data.Dataset.zip((ds1, ds2))\r\ntry:\r\n    for x in zipped:\r\n        break\r\n    print(\"SUCCESS\")\r\nexcept tf.errors.AlreadyExistsError as e:\r\n    print('ERROR')\r\n    print(e)\r\n    \r\n```\r\n\r\n```\r\nTF Version:  2.4.0-rc0\r\n99it [00:00, 6064.76it/s]\r\n\r\n************ Summary ************\r\n\r\nExamples/sec (First included) 1539.73 ex/sec (total: 100 ex, 0.06 sec)\r\nExamples/sec (First only) 25.58 ex/sec (total: 1 ex, 0.04 sec)\r\nExamples/sec (First excluded) 3829.63 ex/sec (total: 99 ex, 0.03 sec)\r\n99it [00:00, 6875.45it/s]\r\n\r\n************ Summary ************\r\n\r\nExamples/sec (First included) 3563.53 ex/sec (total: 100 ex, 0.03 sec)\r\nExamples/sec (First only) 74.66 ex/sec (total: 1 ex, 0.01 sec)\r\nExamples/sec (First excluded) 6749.06 ex/sec (total: 99 ex, 0.01 sec)\r\nSUCCESS\r\n\r\nProcess finished with exit code 0\r\n```", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/7d6217b7373a876032e870a86e5e8625/untitled147.ipynb)..Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47545\">No</a>\n"]}, {"number": 47544, "title": "Obtain the output of intermediate layer (Functional API) and use it in SubClassed API", "body": "# Documents \r\n\r\nIn the [keras doc](https://keras.io/getting_started/faq/), it says that if we want to pick the **output of the model** (sequential and functional), all we need to do as follows: \r\n\r\n```python\r\nmodel = ...  # create the original model\r\n\r\nlayer_name = 'my_layer'\r\nintermediate_layer_model = keras.Model(inputs=model.input,\r\n                                       outputs=model.get_layer(layer_name).output)\r\nintermediate_output = intermediate_layer_model(data)\r\n```\r\n\r\nSo, here we get two models, the `intermediate_layer_model` is the sub-model of its parent model. And they're independent as well. Likewise, if we get the **output feature maps** of the parent model (or base model), and **do some operation** with it and get some output feature maps from this operation, then we can also impute **this output feature maps** to the parent model.\r\n\r\n```python\r\n\r\ninput = tf.keras.Input(shape=(size,size,3))\r\nmodel = tf.keras.applications.DenseNet121(input_tensor = input)\r\n\r\nlayer_name = \"conv1_block1\" # for example \r\noutput_feat_maps = SomeOperationLayer()(model.get_layer(layer_name).output)  \r\n\r\n# assume, they're able to add up\r\nbase = Add()([model.output, output_feat_maps])\r\n\r\n# bind all \r\nimputed_model = tf.keras.Model(inputs=[model.input], outputs=base)\r\n```\r\nSo, in this way we have one modified model. It's quite easy with functional API. All the `keras` imagenet models are written with functional API (mostly). In model subclassing API, we can use these models. My concern here is, what to do if we need the intermediate output feature maps of these functional API models' inside `call` function. \r\n\r\n```python\r\nclass Subclass(tf.keras.Model): \r\n    def __init__(self, dim):\r\n         super(Subclass, self).__init__()\r\n         self.dim = dim\r\n         self.base = DenseNet121(input_shape=self.dim)\r\n\r\n         # building new model with the desired output layer of base model \r\n         self.mid_layer_model = tf.keras.Model(self.base.inputs, \r\n\t\t\t\t\t\t\t         self.base.get_layer(layer_name).output)\r\n\r\n    def call(self, inputs):\r\n         # forward with base model \r\n         x = self.base(inputs)\r\n\r\n         # forward with mid_layer_model \r\n         mid_feat = self.mid_layer_model(inputs)\r\n\r\n         # do some op with it \r\n         mid_x = SomeOperationLayer()(mid_feat)\r\n         \r\n         # assume, they're able to add up\r\n         out = tf.keras.layers.add([x, mid_x])\r\n\r\n         return out \r\n```\r\n\r\nThe issue is, here we've **two models** and it's not desirable. Here we simply want the intermediate output feature maps of the base model. What can we do to get one modified subclasses model? Creating a new model with desired intermediate out works but by the same time, we repeat the same operation i.e the base model and its subset model. Maybe I'm missing something obvious, please add up something. ", "comments": ["Any update? ", "Guys, any update? There's probably a gap in my understanding. If any of you need some working code, then say it. Ready to prepare. ", "@jvishnuvardhan \nHere we described in details of this issue with reproducible  code. Please check and respond. We like to close this iissue. \n\nhttps://stackoverflow.com/questions/66513819/obtain-the-output-of-intermediate-layer-functional-api-and-use-it-in-subclasse", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47544\">No</a>\n", "> Are you satisfied with the resolution of your issue?\r\n\r\n# NO", "@jvishnuvardhan It's solved. Thanks. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47544\">No</a>\n"]}, {"number": 47543, "title": "[Intel MKL] Upgrading oneDNN to v2.1", "body": "This PR upgrades oneDNN version to 2.1 and makes few required API changes. It also makes native format the default execution mode.", "comments": ["@penpornk looks like the build is failing. We are not able to reproduce in our environment so far. We will continue trying on that. If you have any input, please let us know.", "I have resolved the conflicts. Thank you!", "I also made the change to make both MKL and vanilla TF use the same oneDNN build.", "> I also made the change to make both MKL and vanilla TF use the same oneDNN build.\r\n\r\nThis one is risky of being rolled back. Let's make it a separate PR. \r\nYou don't need to revert the new commit. I just won't reapprove the PR. :)", "Thank you @penpornk . I saw you merged the PR. Should I fix the conflicts now so you can test the latest patch?", "@mahmoud-abuzaina Please open a new PR and mention me. I'll close this PR now since it is merged. :)", "This PR is rolled back in a253da7ebe03a34840020a2e526fca4819005115 because it broke the docker build. To reproduce:\r\n\r\n- Run the TensorFlow CPU docker image. [Instructions here](https://www.tensorflow.org/install/source#cpu-only):\r\n```\r\ndocker pull tensorflow/tensorflow:devel\r\ndocker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\r\n    tensorflow/tensorflow:devel bash\r\n```\r\n- Inside the container, go to the tensorflow source folder that have this v2.1 changes. Build with\r\n```\r\nyes \"\" | python3 configure.py\r\nbazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=opt --config=v2 @mkl_dnn_v1//:dnnl_single_threaded\r\n```\r\nThe error message:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/dd986257047f8a12e681831016f9a417/external/mkl_dnn_v1/BUILD.bazel:124:11: C++ compilation of rule '@mkl_dnn_v1//:dnnl_single_threaded' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 78 argument(s) skipped)\r\nexternal/mkl_dnn_v1/src/cpu/x64/gemm/f32/jit_avx2_f32_copy_an_kern_autogen.cpp: In member function 'virtual void dnnl::impl::cpu::x64::jit_avx2_f32_copy_an_kern::generate()':\r\nexternal/mkl_dnn_v1/src/cpu/x64/gemm/f32/jit_avx2_f32_copy_an_kern_autogen.cpp:29:6: internal compiler error: in equal_mem_array_ref_p, at tree-ssa-scopedtables.c:429\r\n void jit_avx2_f32_copy_an_kern::generate() {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget @mkl_dnn_v1//:dnnl_single_threaded failed to build\r\n```\r\n\r\nOnce the problem is fixed, please create a new PR to bring this back. Thank you very much! :)"]}, {"number": 47542, "title": "Lower tf.IsFinite.", "body": "Fixes #47361. It allows to use `tf.is_finite` in TFLite without flex. `tf.is_finite` appears in `tf.keras.layers.Softmax` with multi-axes reduction.\r\n\r\n/cc @abattery for visibility.", "comments": ["FYI, @thaink", "This is almost the same as my PR. But It has not been able to get approved yet due to concern to affect other paths.", "Got it. Sorry that I forget you're working on this. Just pull one thing from my TODO list without a second look. So let me close it as there has been a pending one. Thanks!"]}, {"number": 47541, "title": "Update docs for bucket_by_sequence_length api.", "body": "Added more examples for bucket_by_sequence_length API. Explained the pitfall of using the option pad_to_buckt_boundary option.\r\n\r\nThis pull request is for the issue: #47513", "comments": ["Could someone guide me, why the check related to copybara is failing? I tried to understand, but when I am trying to open it, it says the page is not reachable. \n\nAny help will be appreciated. Thank you in advance. "]}, {"number": 47540, "title": "merge", "body": "", "comments": ["Closing this PRs, since there is no files changed. Thank you!"]}, {"number": 47539, "title": "47513 docs bucket", "body": "Added more examples for bucket_by_sequence_length API. Explained the pitfall of using the option pad_to_buckt_boundary option.\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47539) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 47538, "title": "Add example for bucket_by_sequence_length.", "body": "Added more examples for bucket_by_sequence_length API. Explained the pitfall of using the option `pad_to_buckt_boundary` option. \r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47538) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47538) for more info**.\n\n<!-- need_author_cla -->", "Kindly close this pull request, as somehow my user was not recognized correctly. I will commit with the correct user and create another pull request. Very sorry for the inconvenience caused."]}, {"number": 47537, "title": "tf.keras.backend.in_train_phase / tf.keras.backend.in_test_phase not documented", "body": "Tensorflow 2.4.1\r\n\r\ntf.keras.backend.in_train_phase / tf.keras.backend.in_test_phase not documented. \r\n\r\nReference tf 2.4.1 docs here on the tf.keras.backend lib.  You will see neither function appears.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/backend", "comments": ["Docs are not generated intentionally for some of the `tf.keras.backend` functions.\r\nSee commit [a37bf6c](https://github.com/tensorflow/tensorflow/commit/91e5ad0fad9bbf8462a797ddd7183df1c15f6832#diff-3bac640d4fd7baf52009c2ed77c693723025a266052799f44c16cf53c3bdfc53) to know more. Thanks!\r\n[tf.keras.backend.in_train_phase](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4693-L4694)\r\n[tf.keras.backend.in_test_phase](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4739-L4740)", "@ymodak There is not documentation for differing behavior between test/train time despite the flag learning_phase() being provided and documented.", "Tensorflow has [documented](https://www.tensorflow.org/api_docs/python/tf/keras/backend) only those backend functions listed at https://keras.io/api/utils/backend_utils/, rest all are undocumented.\r\nHowever, you see see all the hidden functionalities with the code in the github [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py).\r\nClosing this issue, feel free to reopen if you need further clarification. Thanks!"]}, {"number": 47536, "title": "Native C++ Tensorflow Lite project build failing on Raspberry Pi", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Following [this guide](https://www.tensorflow.org/lite/guide/build_cmake) to build from source\r\n- TensorFlow version: TensorFlow Lite\r\n- Python version: N/A, trying to create a C++ project\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): N/A, using CMake 3.20.0-rc2\r\n- GCC/Compiler version (if compiling from source): arm-linux-gnueabihf 8.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nRight now, I'm trying to use the TensorFlow C++ API. I am able to build the library using cmake without any fatal errors using this [this guide](https://www.tensorflow.org/lite/guide/build_cmake). However, when I try to build the minimal script (code provided) and run the make command (CMakeLists provided), I get a fatal error (traceback provided). \r\n\r\n**Any other info / logs**\r\nCommands ran to build TensorFlow using CMake:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\nmkdir tflite_build\r\ncd tflite_build\r\ncmake ../tensorflow_src/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug -DTFLITE_ENABLE_XNNPACK=OFF\r\ncmake --build . -j3\r\n```\r\n\r\nCommands ran to build project using CMake:\r\n```\r\nmkdir build && cd build\r\ncmake -DCMAKE_BUILD_TYPE=Debug ..\r\ncmake --build . -j\r\n```\r\n\r\n`CMakeLists.txt`:\r\n```\r\ncmake_minimum_required(VERSION 3.13)\r\nproject(minimal C CXX)\r\n\r\nset(TENSORFLOW_SOURCE_DIR \"/home/pi/tensorflow_src\")\r\n\r\nadd_subdirectory(\r\n  \"${TENSORFLOW_SOURCE_DIR}/tensorflow/lite\"\r\n  \"${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite\"\r\n  EXCLUDE_FROM_ALL\r\n)\r\n\r\nset(CMAKE_CXX_STANDARD 11)\r\nadd_executable(minimal\r\n  minimal.c\r\n)\r\ntarget_link_libraries(minimal\r\n  tensorflow-lite\r\n  ${CMAKE_DL_LIBS}\r\n)\r\n```\r\n\r\n`minimal.c`:\r\n```\r\n#include <cstdio>\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/optional_debug_tools.h\"\r\n\r\n// This is an example that is minimal to read a model\r\n// from disk and perform inference. There is no data being loaded\r\n// that is up to you to add as a user.\r\n//\r\n// NOTE: Do not add any dependencies to this that cannot be built with\r\n// the minimal makefile. This example must remain trivial to build with\r\n// the minimal build tool.\r\n//\r\n// Usage: minimal <tflite model>\r\n\r\n#define TFLITE_MINIMAL_CHECK(x)                              \\\r\n  if (!(x)) {                                                \\\r\n    fprintf(stderr, \"Error at %s:%d\\n\", __FILE__, __LINE__); \\\r\n    exit(1);                                                 \\\r\n  }\r\n\r\nint main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n    fprintf(stderr, \"minimal <tflite model>\\n\");\r\n    return 1;\r\n  }\r\n  const char* filename = argv[1];\r\n\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter with the InterpreterBuilder.\r\n  // Note: all Interpreters should be built with the InterpreterBuilder,\r\n  // which allocates memory for the Intrepter and does various set up\r\n  // tasks so that the Interpreter can read the provided model.\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  tflite::InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<tflite::Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Fill input buffers\r\n  // TODO(user): Insert code to fill input tensors.\r\n  // Note: The buffer of the input tensor with index `i` of type T can\r\n  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`\r\n\r\n  // Run inference\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n  printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Read output buffers\r\n  // TODO(user): Insert getting data out code.\r\n  // Note: The buffer of the output tensor with index `i` of type T can\r\n  // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\nTraceback:\r\n```\r\nConsolidate compiler generated dependencies of target absl_spinlock_wait\r\nConsolidate compiler generated dependencies of target absl_log_severity\r\nConsolidate compiler generated dependencies of target pthreadpool\r\nConsolidate compiler generated dependencies of target absl_dynamic_annotations\r\nConsolidate compiler generated dependencies of target absl_city\r\nConsolidate compiler generated dependencies of target absl_int128\r\nConsolidate compiler generated dependencies of target absl_time_zone\r\nConsolidate compiler generated dependencies of target farmhash\r\nConsolidate compiler generated dependencies of target absl_civil_time\r\nConsolidate compiler generated dependencies of target flatbuffers\r\nConsolidate compiler generated dependencies of target fft2d_fftsg\r\nConsolidate compiler generated dependencies of target ruy\r\n[  0%] Built target absl_spinlock_wait\r\n[  0%] Built target absl_log_severity\r\n[  0%] Built target absl_dynamic_annotations\r\n[  1%] Built target absl_city\r\n[  1%] Built target pthreadpool\r\n[  1%] Built target farmhash\r\n[  1%] Built target absl_int128\r\n[  1%] Built target absl_civil_time\r\n[  3%] Built target flatbuffers\r\n[  3%] Built target fft2d_fftsg\r\n[  3%] Built target absl_time_zone\r\nConsolidate compiler generated dependencies of target clog\r\nConsolidate compiler generated dependencies of target absl_raw_logging_internal\r\nConsolidate compiler generated dependencies of target fft2d_fftsg2d\r\n[  3%] Built target clog\r\n[  3%] Built target absl_raw_logging_internal\r\n[  3%] Built target fft2d_fftsg2d\r\nConsolidate compiler generated dependencies of target cpuinfo\r\n[  7%] Built target ruy\r\nConsolidate compiler generated dependencies of target absl_throw_delegate\r\nConsolidate compiler generated dependencies of target absl_debugging_internal\r\nConsolidate compiler generated dependencies of target absl_strings_internal\r\nConsolidate compiler generated dependencies of target absl_base\r\nConsolidate compiler generated dependencies of target absl_bad_optional_access\r\nConsolidate compiler generated dependencies of target absl_bad_variant_access\r\n[  7%] Built target absl_throw_delegate\r\n[  9%] Built target cpuinfo\r\n[  9%] Built target absl_strings_internal\r\n[  9%] Built target absl_debugging_internal\r\n[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/cycleclock.cc.o\r\n[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/sysinfo.cc.o\r\n[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/spinlock.cc.o\r\n[  9%] Built target absl_bad_optional_access\r\n[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/thread_identity.cc.o\r\n[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/unscaledcycleclock.cc.o\r\n[  9%] Built target absl_bad_variant_access\r\nConsolidate compiler generated dependencies of target absl_stacktrace\r\n[ 10%] Built target absl_stacktrace\r\nScanning dependencies of target XNNPACK\r\nConsolidate compiler generated dependencies of target XNNPACK\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up8x9-minmax-neon-mul16.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up16x9-minmax-neon-mul16.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up24x9-minmax-neon-mul16.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up32x9-minmax-neon-mul16.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8-minmax-neon-mlal-lane.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/4x8-minmax-neon-mlal-lane.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/2x16-minmax-neon-mlal-lane.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x16-minmax-neon-mlal-lane.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/1x8-minmax-neon-mlal-lane.c.o\r\n[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/4x8-minmax-neon-mlal-lane.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/1x16-minmax-neon-mlal-lane.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-requantization/precise-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/2x16-minmax-neon-mlal-lane.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-requantization/fp32-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-requantization/q31-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-avgpool/9p8x-minmax-neon-c8.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-avgpool/9x-minmax-neon-c8.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-dwconv/up8x9-minmax-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gavgpool/7x-minmax-neon-c8.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gavgpool/7p7x-minmax-neon-c8.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gemm/4x8-minmax-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gemm/8x8-minmax-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-requantization/precise-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-igemm/8x8-minmax-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-igemm/4x8-minmax-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-requantization/fp32-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-requantization/q31-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/u8-maxpool/9p8x-minmax-neon-c16.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-vadd/minmax-neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/u8-rmax/neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-fill/neon.c.o\r\n[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/u8-clamp/neon-x64.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-packx/x4-neon-st4.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-pad/neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/x3-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-unpool/neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/x2-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/x4-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/x2-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/xm-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/x3-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/x4-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/xm-neon.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundne-neon-addsub.c.o\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c: In function \u2018xnn_qs8_gavgpool_minmax_ukernel_7p7x__neon_c24_acc2\u2019:\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:437:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n     ^~~~~~~~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:437:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundd-neon-addsub.c.o\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:437:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundu-neon-addsub.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundd-neon-cvt.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundz-neon-addsub.c.o\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundu-neon-cvt.c.o\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c: In function \u2018xnn_qs8_gavgpool_minmax_ukernel_7x__neon_c32_acc2\u2019:\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:261:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n     ^~~~~~~~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:261:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundz-neon-cvt.c.o\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8322: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/sigmoid-neon-frac-p9-p10-nr1recps.c.o\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c: In function \u2018xnn_qs8_gavgpool_minmax_ukernel_7p7x__neon_c16_acc2\u2019:\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:261:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:262:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:262:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:294:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n     ^~~~~~~~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:294:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:294:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c: In function \u2018xnn_qs8_gavgpool_minmax_ukernel_7x__neon_c24_acc2\u2019:\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c: In function \u2018xnn_qs8_gavgpool_minmax_ukernel_7x__neon_c16_acc2\u2019:\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:174:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n     ^~~~~~~~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:174:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8280: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c.o] Error 1\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:218:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n     ^~~~~~~~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:174:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:218:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:218:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8308: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c.o] Error 1\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8252: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c.o] Error 1\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8266: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c.o] Error 1\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c: In function \u2018xnn_qs8_gavgpool_minmax_ukernel_7p7x__neon_c32_acc2\u2019:\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:518:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n     ^~~~~~~~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:518:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:518:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:519:51: error: incompatible type for argument 1 of \u2018vcombine_s16\u2019\r\n     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));\r\n                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n               ~~~~~~~~~~^~~\r\n/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:519:77: error: incompatible type for argument 2 of \u2018vcombine_s16\u2019\r\n     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));\r\n                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:\r\n/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected \u2018int16x4_t\u2019 but argument is of type \u2018int8x8_t\u2019\r\n vcombine_s16 (int16x4_t __a, int16x4_t __b)\r\n                              ~~~~~~~~~~^~~\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8336: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:14888: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[ 14%] Linking CXX static library libabsl_base.a\r\n[ 14%] Built target absl_base\r\nmake: *** [Makefile:156: all] Error 2\r\n```", "comments": ["@terryheo could you take a look at this Cmake failure for RPi boards?", "I think you need \"-DTFLITE_ENABLE_XNNPACK=OFF\" option for your project. The error happens with XNNPACK build.", "This worked perfectly, thank you! On another note, is XNNPACK not supported by RPi or should I file an issue on the XNNPACK repo?", "I wonder if it requires to specify NEON extension.\r\n\r\nCould you try to use the ARMCC_FLAGS?\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm#run_cmake_2", "@AritroSaha10 Is this still an issue for you? Can you try @terryheo suggestion and let us know how it progresses. thanks!", "Hi, sorry about the late response. I'm trying out the suggestion right now, but I'm not sure what alternative I should be using opposed to the x86 cross-compiling tools for gcc. If you want, here's the error msg:\r\n\r\n```\r\npi@raspberrypi:~/tflite_build $ cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \\\r\n>   -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \\\r\n>   -DCMAKE_C_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n->   -DCMAKE_CXX_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n>   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\r\n>   -DCMAKE_SYSTEM_NAME=Linux \\\r\n>   -DCMAKE_SYSTEM_PROCESSOR=armv7 \\\r\n>   ../tensorflow_src/tensorflow/lite\r\n-- Setting build type to Release, for debug builds use'-DCMAKE_BUILD_TYPE=Debug'.\r\n-- The C compiler identification is unknown\r\n-- The CXX compiler identification is unknown\r\n-- Check for working C compiler: /home/pi/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc\r\n-- Check for working C compiler: /home/pi/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -- broken\r\nCMake Error at /usr/share/cmake-3.16/Modules/CMakeTestCCompiler.cmake:60 (message):\r\nThe C compiler\r\n\"/home/pi/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc\" is not able to compile a simple test program.  \r\nIt fails with the following output:\r\nChange Dir: /home/pi/tflite_build/CMakeFiles/CMakeTmp\r\nRun Build Command(s):/usr/bin/make cmTC_96367/fast && /usr/bin/make -f CMakeFiles/cmTC_96367.dir/build.make CMakeFiles/cmTC_96367.dir/build\r\nmake[1]: Entering directory '/home/pi/tflite_build/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_96367.dir/testCCompiler.c.o\r\n/home/pi/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc   -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations    -o CMakeFiles/cmTC_96367.dir/testCCompiler.c.o   -c /home/pi/tflite_build/CMakeFiles/CMakeTmp/testCCompiler.c\r\n/home/pi/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc: 1: /home/pi/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc: Syntax error: \"(\" unexpected\r\nmake[1]: *** [CMakeFiles/cmTC_96367.dir/build.make:66: CMakeFiles/cmTC_96367.dir/testCCompiler.c.o] Error 2\r\nmake[1]: Leaving directory '/home/pi/tflite_build/CMakeFiles/CMakeTmp'\r\nmake: *** [Makefile:121: cmTC_96367/fast] Error 2\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47536\">No</a>\n"]}, {"number": 47534, "title": "Dataset not ending when created inside predict_step function.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.9 \r\n\r\n**Describe the current behavior**\r\nWhen overriding the `predict_step` of a `tf.keras.Model` and creating a `Dataset` inside, it will not end.\r\n\r\n**Describe the expected behavior**\r\n`Dataset` should end.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1lKF1KxUdUDjJq_DTfle5HZITWIcICgdG?usp=sharing\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3103c98f5f9947333fe79ce72631b59e/47534.ipynb). Thanks!", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/feceeca87ab1701a9850f517d877b149/untitled144.ipynb)..Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47534\">No</a>\n"]}, {"number": 47532, "title": "working with sparse input", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) colab\r\n- TensorFlow installed from (source or binary): colab\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nSparse input works with dense and dropout layers as of v2.4.1. However, when you stack dense with dropout, it throws an error. If you try to reshape sparse input, you also get an error.\r\n\r\n**Describe the expected behavior**\r\nSparse input works just like dense input.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nsee [gist](https://colab.research.google.com/gist/hoondy/74b5c6cca796d0229941868b43c66592/git210303.ipynb)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nspawn from #25980", "comments": ["I am able to reproduce this on tf 2.3,tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/29a944c75db2fbc56f02f682b300736c/untitled557.ipynb).", "I can confirm that I have the same issue when using an Con1D after the input layer.", "Was able to reproduce the issue in TF 2.6.0-dev20210530,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/86dad8df4e4a046afb443670bce03e48/untitled142.ipynb#scrollTo=HkzVWKaTuJiI)..Thanks !", "simple reproduction\r\nhttps://colab.research.google.com/drive/1bZT__4iOpHiQPeQ9eCMKO4OPn-Mj_nQ1?usp=sharing\r\n\r\nimport tensorflow as tf\r\nfrom platform import python_version\r\nprint(python_version(), tf.__version__)\r\n3.7.12 2.6.0\r\n\r\nst = tf.SparseTensor(indices=[[4, 2], [4, 3]],\r\n                      values=[10, 20],\r\n                      dense_shape=[10, 5])\r\ntf.constant(tf.sparse.reorder(st))\r\n\r\n", "Here issue is with converting `Sparse_tensor` to `Dense_tensor`. Since `tf.constant` is Dense tensor, first we need to convert `Sparse_tensor` to `Dense_tensor` as given in the below code\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom platform import python_version\r\nprint(python_version(), tf.version)\r\n\r\nst = tf.SparseTensor(indices=[[4, 2], [4, 3]],\r\nvalues=[10, 20],\r\ndense_shape=[10, 5])\r\n#tf.constant(tf.sparse.reorder(st))\r\ndt = tf.sparse.reorder(st)\r\ndt_numpy = tf.sparse.to_dense(dt).numpy()\r\ntf.constant(dt_numpy)\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47532\">No</a>\n"]}, {"number": 47531, "title": "Unable to access resources using tf.data.experimental.service", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19.160-2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ntf resources used in mapped functions are not available to workers when using `tf.data.experimental.service.distribute` to process a dataset. When the dataset is iterated over locally, this does not produce an error.\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior would be that locally run calls to `Dataset.map` and ones which use `tf.data.experimental.service.distribute` work equally.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nkeys = 'abcdefghijklmnopqrstuvwxyz'\r\ntable = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(list(keys), range(len(keys))), -1)\r\n\r\nds = tf.data.Dataset.from_tensor_slices([['a', 'b'], ['c', 'd'], ['e','f']])\r\nds = ds.map(lambda x: table.lookup(x))\r\n\r\n# When True, this produces an error, but works when set to False\r\nUSE_DATA_SERVICE = True\r\nif USE_DATA_SERVICE:\r\n  # Set up data service\r\n  dispatcher = tf.data.experimental.service.DispatchServer()\r\n  dispatcher_address = dispatcher.target.split(\"://\")[1]\r\n  workers = [\r\n    tf.data.experimental.service.WorkerServer(\r\n      tf.data.experimental.service.WorkerConfig(\r\n          dispatcher_address=dispatcher_address)) for _ in range(2)\r\n  ]\r\n\r\n  processing_mode = \"parallel_epochs\"\r\n  ds = ds.apply(tf.data.experimental.service.distribute(\r\n    processing_mode=processing_mode, service=dispatcher.target))\r\n\r\nds.__iter__().get_next()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2112       ctx.executor = executor_new\r\n-> 2113       yield\r\n   2114     finally:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    732           output_types=self._flat_output_types,\r\n--> 733           output_shapes=self._flat_output_shapes)\r\n    734 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)\r\n   2578     except _core._NotOkStatusException as e:\r\n-> 2579       _ops.raise_from_not_ok_status(e, name)\r\n   2580     except _core._FallbackException:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6861   # pylint: disable=protected-access\r\n-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6863   # pylint: enable=protected-access\r\n\r\n/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: Failed to get element: Container localhost does not exist. (Could not find resource: localhost/28)\r\n\t [[{{node None_Lookup/LookupTableFindV2}}]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-2-e1a0821191f4> in <module>\r\n     22     processing_mode=processing_mode, service=dispatcher.target))\r\n     23 \r\n---> 24 ds.__iter__().get_next()\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in get_next(self)\r\n    798 \r\n    799   def get_next(self):\r\n--> 800     return self._next_internal()\r\n    801 \r\n    802   def get_next_as_optional(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    737         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    738       except AttributeError:\r\n--> 739         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    740 \r\n    741   @property\r\n\r\n/opt/conda/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    128                 value = type()\r\n    129             try:\r\n--> 130                 self.gen.throw(type, value, traceback)\r\n    131             except StopIteration as exc:\r\n    132                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2114     finally:\r\n   2115       ctx.executor = executor_old\r\n-> 2116       executor_new.wait()\r\n   2117 \r\n   2118 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     67   def wait(self):\r\n     68     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 69     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     70 \r\n     71   def clear_error(self):\r\n\r\nNotFoundError: Failed to get element: Container localhost does not exist. (Could not find resource: localhost/28)\r\n\t [[{{node None_Lookup/LookupTableFindV2}}]]\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/128826b210f2d4b5598040e902fce4ed/47531.ipynb) and TF-nightly.\r\n\r\nWhereas on running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/658b9346d3e74da9a03c269c8f09169b/47531-2-3.ipynb#scrollTo=5dc9GGSJcknG), I am facing an error stating `TypeError: __init__() missing 1 required positional argument: 'port'`. Please check the linked gist for reference. Thanks!", "Thanks for reporting this @ialdencoots. The error happens because the serialized Dataset sent to the tf.data servers contains only a handle to the lookup table. We hope to fix the issue in TF 2.5 by serializing the information needed to re-instantiate the lookup table on the tf.data servers (e.g. filename for lookup table initialization, or possibly a serialization of the full lookup table content).\r\n\r\nIn the short term, the workaround is to move the lookup table transformation to happen after the call to `tf.data.experimental.service.distribute` if possible.", "Thanks for the information! Unfortunately for me, the lookup occurs within the expensive operation that I'm hoping to distribute, so moving the call to happen later isn't possible.\r\n\r\nThis may be outside of the scope of this repo, but I was wondering if there is any intention of adding more built in support for things like cluster resolution in deploying the distribute dispatch and worker servers. When running as a GCP AI Platform job, I would like to be able to spin up additional worker machines to do data preprocessing as part of the job I'm submitting, much like when one trains with `tf.distribute.Strategy`. I'm not too familiar with the internals, but I'm assuming proper grpc channels would have to be opened between the machines and that it might be something the AI Platform would have to do. Do you have any knowledge on the topic?", "I tried to run the code on colab with [TF v2.5 ](https://colab.research.google.com/gist/sushreebarsa/1381b4518359bed9c195ff0a908894b3/untitled173.ipynb)& [TF 2.6.0-dev20210531](https://colab.research.google.com/gist/sushreebarsa/1e147c87dbf1f27308e9630cbcc1dc27/untitled172.ipynb#scrollTo=rJFumDKB2xNv),please find the gists for your  reference ....Thanks !", "This issue should be fixed in TF v2.5. @sushreebarsa When I run those colabs I don't see any errors, do you see an error?", "@aaudiber I also didn't see any error in TF 2.5 and nightly ...it seems to be fixed .", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47531\">No</a>\n"]}, {"number": 47530, "title": "Failed to get convolution algorithm [CUDA 11.0 - cuDNN 8.0.4 - Python 3.8]", "body": "**System information**\r\n- Windows 10\r\n- PC\r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: Cuda 11.0 | cuDNN 8.0.4\r\n- GPU model and memory:\r\nRTX 2060 \r\n6GB\r\n\r\n\r\n**Describe the problem**\r\nI Coded a CNN in Tensorflow on CPU it works without any problems, then i wanted more performance so i try\u00b4d the GPU version , after 2 days of figuring out wich cuda bla bla\r\n\r\nnow i have no idea to fix this , this is my first Neuronal Network and i only wanted to Check is a Cat or a Dog on the Picture.\r\n\r\ni hope someone can help me\r\n\r\ni got everytime the following error :\r\n> 2021-03-03 15:23:58.929966: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n> 2021-03-03 15:23:58.930097: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n> 2021-03-03 15:23:58.930188: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n> 2021-03-03 15:23:59.186498: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n> 2021-03-03 15:23:59.186672: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n> 2021-03-03 15:23:59.186834: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n> 2021-03-03 15:23:59.189687: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n> 2021-03-03 15:23:59.190360: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n> 2021-03-03 15:23:59.190471: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\n\r\n**Any other info / logs**\r\nthe traindata paths and labes are in a SQL database and the sql manger build the full path\r\ni testet it on CPU -> works without Problems\r\n\r\nmy Programcode itself (Pastebin):\r\nhttps://pastebin.com/cNeGByBm\r\nThe Full Log :\r\nhttps://pastebin.com/fe3D9Ubk\r\n", "comments": ["You may try gpu memory resources management by limiting gpu memory growth.  \r\nTo know more see https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\nMake sure your exit all python processes and create a new python session and try by limiting gpu memory growth on the top of your script.\r\n```python\r\nimport tensorflow as tf\r\ngpu = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpu[0], True)\r\n#Rest of your code\r\n```", "> \r\n> \r\n> You may try gpu memory resources management by limiting gpu memory growth.\r\n> To know more see https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\n> Make sure your exit all python processes and create a new python session and try by limiting gpu memory growth on the top of your script.\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> gpu = tf.config.experimental.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(gpu[0], True)\r\n> #Rest of your code\r\n> ```\r\n\r\nit works thanks :3 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47530\">No</a>\n"]}, {"number": 47529, "title": "Estimator#export_saved_model section links to irrelevant guide or guide missing relevant section", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model\r\nhttps://www.tensorflow.org/guide/saved_model#savedmodels_from_estimators\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### The api_docs link has a link to the guide/saved_model section. The section referenced does not exist in the guide, nor does any mention of the estimator method export_saved_model.\r\n\r\n", "comments": ["@C-Compton I agree with you. The link is misleading but the link is a more detailed guide on [Using the SavedModel format](https://www.tensorflow.org/guide/saved_model#savedmodels_from_estimators). May be we need to change from \"For a detailed guide, see [SavedModel from Estimators](https://www.tensorflow.org/guide/saved_model#savedmodels_from_estimators).\"  to \"For a detailed guide on SavedModel, see [Using the SavedModel format](https://www.tensorflow.org/guide/saved_model#savedmodels_from_estimators)\"\r\n\r\nPlease let us know if you want to contribute and update any docs through a PR. Thanks!", "I have created PR, as per your suggestion @jvishnuvardhan ?", "@PratsBhatt Sorry, that link was not correct. I have updated the doc with correct [link](https://www.tensorflow.org/guide/estimator#savedmodels_from_estimators). It takes some time to show up correct link in the  TF website. Will close this issue once it merges. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue can be closed the PR has been merged. @jvishnuvardhan . Thank you.", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47528, "title": "Error while Compile to the model into TFLite model", "body": "I use it in google Colab\r\n\r\n[Here is notebook link 1](https://colab.research.google.com/drive/1voMaXutl7pOcybTZKwtjSjU_5aqncBdp?usp=sharing)\r\n[Here is notebook link 2](https://nbviewer.jupyter.org/github/samyon7/Convert_TFLite_Problem/blob/master/Untitled243.ipynb)\r\n\r\nWas that because I used the branching model?", "comments": ["There is an uncovered op by TFLite builtin op set, tf.Selu, in your model. In such case, we recommend using the Select TF option. Please refer to https://www.tensorflow.org/lite/guide/ops_select.\r\n\r\nYou can convert your model with the following snippet:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\n```", "> There is an uncovered op by TFLite builtin op set, tf.Selu, in your model. In such case, we recommend using the Select TF option. Please refer to https://www.tensorflow.org/lite/guide/ops_select.\r\n> \r\n> You can convert your model with the following snippet:\r\n> \r\n> ```\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n> converter.target_spec.supported_ops = [\r\n>   tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n>   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n> ]\r\n> tflite_model = converter.convert()\r\n> ```\r\n\r\nSo can't we use selu?\r\nEven though this activation is actually pretty good compared to others, except relu.\r\nBut thanks for the solution, cheers!", "If you really want to support selu op in TFLite builtin op set, please file a new issue for the feature request. Thanks", "Closing for now. Please re-open this if there is any other concern or file a new issue for the other request.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47528\">No</a>\n"]}, {"number": 47527, "title": "expose c `env` headers", "body": "There is one missing line that is needed to expose the `env` header.\r\n\r\nI am really sorry for the back and forth.\r\n\r\nrelated to #46302 https://github.com/tensorflow/io/issues/1183\r\n\r\n@yongtang @mihaimaruseac ", "comments": ["@mihaimaruseac Could you tell me why the internal tests are falling ? Thank you!"]}, {"number": 47526, "title": "Need for Normalization of Pixel Data can be mentioned in Basic Image Classification Tutorial", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/keras/classification#preprocess_the_data\r\n\r\n## Description of issue (what needs changing): \r\nThe Tutorial states \r\n\r\n> Scale these values to a range of 0 to 1 before feeding them to the neural network model.\r\n\r\nIt would be clear to the beginners if the reason for **`Normalization`** has been specified because there has been questions out of confusion. Example [Question 1](https://datascience.stackexchange.com/questions/39929/why-normalize-when-all-features-are-on-the-same-scale), [Question 2](https://stackoverflow.com/questions/20486700/why-do-we-always-divide-rgb-values-by-255) and [Question 3](https://datascience.stackexchange.com/questions/29958/when-inputting-image-rgb-values-to-mlp-should-i-divide-by-255)", "comments": ["Created a CL to fix this issue. Thanks!", "The necessary information is already present in the document as below.\r\n```\r\nThe data must be preprocessed before training the network. If you inspect the first image\r\n in the training set, you will see that the pixel values fall in the range of 0 to 255:\r\nScale these values to a range of 0 to 1 before feeding them to the neural network model.\r\n To do so, divide the values by 255. It's important that the training set and the testing set\r\n be preprocessed in the same way:\r\n```\r\nFor more basic information on these concepts for beginners you can refer to the crash course on ML/DL offered by google [here](https://developers.google.com/machine-learning/crash-course). Thanks. \r\n\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47525, "title": "[TFLite] Segmentation fault on inference due to int overflow in im2col", "body": "Hello,\r\n\r\nThe convolution kernel uses im2col to optimize some convolutions and [uses](https://github.com/tensorflow/tensorflow/blob/7677422a56f18c10e3827e29dc8da8a7b23fbede/tensorflow/lite/kernels/conv.cc#L453) an intermediate buffer of size `batches*output_height*output_width*(input_depth*filter_height*filter_width)` for that.\r\n\r\nIf the size of this buffer overflows an `int`, there is a risk of integer overflow in the `Im2col` [method](https://github.com/tensorflow/tensorflow/blob/7677422a56f18c10e3827e29dc8da8a7b23fbede/tensorflow/lite/kernels/internal/optimized/im2col_utils.h#L247) which uses `int` as type for indexing.\r\n\r\nSuch large buffers can quickly happen in super resolution models with large output resolution. The script bellow is based on the [How to generate super resolution images using TensorFlow Lite on Android](https://blog.tensorflow.org/2020/12/how-to-generate-super-resolution-images-using-tensorflow-lite-on-android.html) tutorial and illustrates the problem. It uses a larger 700x700 input image to generate a 2800x2800 output image.\r\n\r\nThe last convolution needs an im2col buffer of size `1*2800*2800*(32*3*3) = 2 257 920 000` which is larger than the 32-bit `int` used for indexing in `Im2col` and result in a `Segmentation fault` with the following stack trace due to some of the indexes going negative:\r\n```\r\n#0  __memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:366\r\n#1  0x00007fffae09dc86 in void tflite::optimized_ops::Im2col<signed char>(tflite::ConvParams const&, int, int, int const*, int, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*) () from ~/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n#2  0x00007fffae0c7cdd in tflite::optimized_ops::HybridConvPerChannel(tflite::ConvParams const&, float*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::RuntimeShape const&, signed char*, float const*, int*, tflite::RuntimeShape const&, int*, int*, bool*, tflite::CpuBackendContext*) () from ~/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n#3  0x00007fffae0c861f in TfLiteStatus tflite::ops::builtin::conv::EvalHybridPerChannel<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*) ()\r\n```\r\n\r\nThe test was done using the latest `tf-nightly-2.5.0.dev20210303`.\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\ninput_shape = (1, 700, 700, 3)\r\n\r\nmodel = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\r\nconcrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape(input_shape)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(\r\n    model_content=tflite_model, num_threads=os.cpu_count()\r\n)\r\ninterpreter.allocate_tensors()\r\ninterpreter.invoke()\r\n```\r\n\r\nForcing the usage of the reference kernels which don't use the im2col optimization circumvent the problem.\r\n\r\nOne way to solve it would be to use larger integers for indexing or disabling the im2col optimization when the intermediate buffer is too large.\r\n\r\nThibaut", "comments": ["@multiverse-tf could you take a look at this issue? Here is an external report on the out-of-memory issue regarding the im2col optimization technique.", "Hi Tessil, \r\n\r\nThanks for uncovering such a subtle issue! Coincidentally, we have been aware of this issue internally recently. We have a WIP fix that will avoid this overflow, and it will be pushed out very soon. \r\n\r\nBasically, in such scenarios, we think this im2col temporary tensor takes too much memory, and it's likely to cause OOM when running the inference. So the fix is simply to skip such im2col operation but fallbacks to a not-well-optimized CONV implementation when this tensor is over 1GB.\r\n\r\n> Hello,\r\n> \r\n> The convolution kernel uses im2col to optimize some convolutions and [uses](https://github.com/tensorflow/tensorflow/blob/7677422a56f18c10e3827e29dc8da8a7b23fbede/tensorflow/lite/kernels/conv.cc#L453) an intermediate buffer of size `batches*output_height*output_width*(input_depth*filter_height*filter_width)` for that.\r\n> \r\n> If the size of this buffer overflows an `int`, there is a risk of integer overflow in the `Im2col` [method](https://github.com/tensorflow/tensorflow/blob/7677422a56f18c10e3827e29dc8da8a7b23fbede/tensorflow/lite/kernels/internal/optimized/im2col_utils.h#L247) which uses `int` as type for indexing.\r\n> \r\n> Such large buffers can quickly happen in super resolution models with large output resolution. The script bellow is based on the [How to generate super resolution images using TensorFlow Lite on Android](https://blog.tensorflow.org/2020/12/how-to-generate-super-resolution-images-using-tensorflow-lite-on-android.html) tutorial and illustrates the problem. It uses a larger 700x700 input image to generate a 2800x2800 output image.\r\n> \r\n> The last convolution needs an im2col buffer of size `1*2800*2800*(32*3*3) = 2 257 920 000` which is larger than the 32-bit `int` used for indexing in `Im2col` and result in a `Segmentation fault` with the following stack trace due to some of the indexes going negative:\r\n> \r\n> ```\r\n> #0  __memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:366\r\n> #1  0x00007fffae09dc86 in void tflite::optimized_ops::Im2col<signed char>(tflite::ConvParams const&, int, int, int const*, int, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*) () from ~/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n> #2  0x00007fffae0c7cdd in tflite::optimized_ops::HybridConvPerChannel(tflite::ConvParams const&, float*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::RuntimeShape const&, signed char*, float const*, int*, tflite::RuntimeShape const&, int*, int*, bool*, tflite::CpuBackendContext*) () from ~/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n> #3  0x00007fffae0c861f in TfLiteStatus tflite::ops::builtin::conv::EvalHybridPerChannel<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*) ()\r\n> ```\r\n> \r\n> The test was done using the latest `tf-nightly-2.5.0.dev20210303`.\r\n> \r\n> ```python\r\n> import os\r\n> import tensorflow as tf\r\n> import tensorflow_hub as hub\r\n> \r\n> input_shape = (1, 700, 700, 3)\r\n> \r\n> model = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\r\n> concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n> concrete_func.inputs[0].set_shape(input_shape)\r\n> \r\n> converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> tflite_model = converter.convert()\r\n> \r\n> interpreter = tf.lite.Interpreter(\r\n>     model_content=tflite_model, num_threads=os.cpu_count()\r\n> )\r\n> interpreter.allocate_tensors()\r\n> interpreter.invoke()\r\n> ```\r\n> \r\n> Forcing the usage of the reference kernels which don't use the im2col optimization circumvent the problem.\r\n> \r\n> One way to solve it would be to use larger integers for indexing or disabling the im2col optimization when the intermediate buffer is too large.\r\n\r\nThx for the suggestion! We did take the 2nd one as described earlier :-)\r\n\r\n> \r\n> Thibaut\r\n\r\n", "Hi Thibaut,\r\n\r\nWe have checked in the fix https://github.com/tensorflow/tensorflow/commit/174fbcfed43c816bdc98c760d2cd1705af27a819 described above. Could you help to double check it on your side? Many thanks!\r\n\r\n-Chao", "Hello @multiverse-tf\r\n\r\nThank you very much for the fix. I tested it with the example above and another model we had which was affected by this problem and everything is working fine now.\r\n\r\nI'll thus close the issue, thanks.\r\n\r\nThibaut", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47525\">No</a>\n"]}, {"number": 47524, "title": "Increase GPU memory usage in tensorflow 2.4.1 versus tensorflow 2.3.2", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.5\r\n- TensorFlow installed from: tensorflow dockers _tensorflow:2.3.2-gpu_ and _tensorflow:2.4.1-gpu_\r\n- TensorFlow version (use command below): 2.3.2 and 2.4.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: for **2.3.2**: CUDA 10.1.243 CUDNN 7.6.4.38-1 for **2.4.1**: CUDA 11.0.3 CUDNN 8.0.4.30-1\r\n- GPU model and memory: GeForce RTX 2070 SUPER, 7979MiB\r\n\r\n**Describe the current behavior**\r\nAfter upgrading to tensorflow 2.4.1 GPU memory consumption is increased. Training with the same batch size as for tensorflow 2.3.2 results in GPU running out of memory. \r\n\r\nThe script below works fine when run with\r\n`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.3.2-gpu python3 /data/script.py`\r\n\r\nbut crashes when run with\r\n`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.4.1-gpu python3 /data/script.py`\r\n\r\nWhen using tensorflow 2.4.1 the batch size must be lower to 3 for the script to run without crashing.\r\n\r\n**Describe the expected behavior**\r\nThe script below should be possible to run with tensorflow 2.4.1 without crashing due to GPU running out of memory.\r\n\r\n**Standalone code to reproduce the issue**\r\nscript.py: \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import Input\r\nfrom tensorflow.python.keras.layers import Conv3D, Activation\r\nfrom tensorflow.python.keras.models import Model\r\n\r\n\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n\r\ndef get_model():\r\n    inputs = Input((100, 100, 100, 1), name='image3d')\r\n    x = Conv3D(80, kernel_size=(3, 3, 3), padding='valid', activation='relu')(inputs)\r\n\r\n    class_scores = Conv3D(1, kernel_size=(1, 1, 1))(x)\r\n    out = Activation('softmax', name='out')(class_scores)\r\n\r\n    return Model(inputs=inputs, outputs=out, name='net')\r\n\r\n\r\nbatch_size = 8\r\n\r\nmodel = get_model()\r\nx = np.empty((batch_size, 100, 100, 100, 1), dtype=np.float32)\r\ny = np.empty((batch_size, 98, 98, 98, 1), dtype=np.float32)\r\n\r\nmodel.compile('sgd', 'mse')\r\nmodel.fit(x, y, batch_size=batch_size)\r\n```\r\n\r\nworks fine when run with\r\n`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.3.2-gpu python3 /data/script.py`\r\n\r\nbut crashes when run with\r\n`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.4.1-gpu python3 /data/script.py`\r\n\r\nWhen using tensorflow 2.4.1 the batch size must be lower to 3 for the script to run without crashing.\r\n\r\n**Other info / logs** \r\noutput when executing \r\n`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.4.1-gpu python3 /data/script.py`\r\n[trace-2.4.1.log](https://github.com/tensorflow/tensorflow/files/6075341/trace-2.4.1.log)\r\n", "comments": ["@maunzzz,\r\nI was able to run the code without any issues on both [TF v2.3](https://colab.research.google.com/gist/amahendrakar/12660c3c92cbf4f10728b05c450f116d/47524-2-3.ipynb) and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/0b4fc66059bc127e630904cd1a764bc3/47524.ipynb). Please check the linked gist for reference.\r\n\r\nTo prevent the out of memory error please try setting a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth). Thanks!", "Alright, thank you. What hardware did you run it on?\r\nSetting a hard limit on the GPU helped for 2.4.1\r\n", "@maunzzz,\r\n> Alright, thank you. What hardware did you run it on?\r\n\r\nThe Colab notebook had a Intel(R) Xeon(R) CPU and a Tesla T4 GPU.\r\n\r\n\r\n> Setting a hard limit on the GPU helped for 2.4.1\r\n\r\nPlease feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47524\">No</a>\n"]}, {"number": 47523, "title": "getting error while adding metadata to object detection tflite model", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or GitHub SHA, if built from source): tf-nightly\r\n\r\n### 2. Code\r\n\r\nReference Link- https://www.tensorflow.org/lite/convert/metadata\r\n\r\nI am trying to add metadata into my tflite model as metadata is required for the android app. I am trying the following method but getting error.\r\n\r\n```bash\r\npython ./metadata_writer_for_image_classifier.py \\\r\n    --model_file=./model_without_metadata/mobilenet_v1_0.75_160_quantized.tflite \\\r\n    --label_file=./model_without_metadata/labels.txt \\\r\n    --export_directory=model_with_metadata\r\n```\r\n\r\nGoogle-Colab Link for refernce-\r\nhttps://colab.research.google.com/drive/1MkPxvYWVWdygFYxDk8U-jXDBhEjsSnMa#scrollTo=q3AtK7ErmrVl\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n```bash\r\nFile \"metadata_writer_for_image_classifier.py\", line 176, in main\r\n    \"The model info for, {0}, is not defined yet.\".format(model_basename))\r\nValueError: The model info for, , is not defined yet.\r\n```\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@lu-wang-g could you take a look at this issue?", "Seems like model_file was not passed in successfully, i.e. it became an empty string. Please try using the full path for model_file.\r\n\r\nYou can also try the new MetadataWriter Library. Here is an example to write metadata for an Object Detection model:\r\n\r\n1. Install the TFLite Support nightly Pypi package:\r\n```\r\npip install tflite_support_nightly\r\n```\r\n2. Write metadata to the model using the following script:\r\n\r\n**_EDITTED for object detection instead of image classification on March 4, 2021_** \r\n\r\n```\r\nfrom tflite_support.metadata_writers import object_detector\r\nfrom tflite_support.metadata_writers import writer_utils\r\nfrom tflite_support import metadata\r\n\r\nObjectDetectorWriter = object_detector.MetadataWriter\r\n_MODEL_PATH = <Path to the model>\r\n_LABEL_FILE = <Path to the label file>\r\n_SAVE_TO_PATH = \"model_with_metadata.tflite\"\r\n\r\nwriter = ObjectDetectorWriter.create_for_inference(\r\n    writer_utils.load_file(_MODEL_PATH), [127.5], [127.5], [_LABEL_FILE])\r\nwriter_utils.save_file(writer.populate(), _SAVE_TO_PATH)\r\n````\r\n3. Verify the populated metadata and associated files.\r\n```\r\ndisplayer = metadata.MetadataDisplayer.with_model_file(_SAVE_TO_PATH)\r\nprint(\"Metadata populated:\")\r\nprint(displayer.get_metadata_json())\r\nprint(\"Associated file(s) populated:\")\r\nprint(displayer.get_packed_associated_file_list())\r\n```\r\n\r\n", "I Think It will not work in my case.  I am creating tflite model for the object detection problem and I am getting the error while writing the metadata with the above script.\r\n\r\n```bash\r\nValueError: The number of output tensors (4) should match the number of output tensor metadata (1)\r\n```\r\n\r\nColab:- https://colab.research.google.com/drive/1MkPxvYWVWdygFYxDk8U-jXDBhEjsSnMa#scrollTo=wrKT_tf2dYKX", "I thought you were interested at the ImageClassification API. I've corrected my previous answer for ObjectDetection. Please try it again and see if it works. ", "@lu-wang-g yup it's working, I think it's missing in Tensorflow documentation.\nThanks for sharing the script.", "We'll add the documentation once the full MetadataWriter Library is done. Please stay tuned. ", "@codePerfectPlus \r\nAs the request for object detection problem is resolved please move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47523\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47523\">No</a>\n"]}]