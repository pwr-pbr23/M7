[{"number": 42304, "title": "Keras Accuracy with sequence auto-encoder is different than manual calculation", "body": "Hi,\r\n\r\nI'm using GRU-based seq2seq model with byte pair encoding for spelling correction problem. \r\n_This is my model architecture/code:_\r\n\r\n```\r\nencoder_inputs = Input(shape=(padding_length,), name=\"EncoderInput_1\")\r\nembedded_encoder_inputs = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\r\nencoder = GRU(latent_dim, return_state=True)\r\n_, state_h = encoder(embedded_encoder_inputs)\r\n\r\nencoder_states = state_h\r\n\r\ndecoder_inputs = Input(shape=(padding_length,), name=\"DecoderInput_1\")\r\nembedded_decoder_inputs = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)(decoder_inputs)\r\ndecoder_lstm = GRU(latent_dim, return_sequences=True, return_state=True)\r\nx, _ = decoder_lstm(embedded_decoder_inputs, initial_state=encoder_states)\r\ndecoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'))\r\ndecoder_outputs = decoder_dense(x)\r\n\r\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\n\r\nmodel.compile(optimizer='adam', loss=sparse_categorical_crossentropy,metrics=[\"accuracy\"])\r\nmodel.fit([ip_encoded,op_encoded],\r\n              op_offset_encoded,\r\n              epochs=epochs,\r\n              verbose=1,\r\n              batch_size=batch_size,\r\n              validation_split=0.2,\r\n```\r\nI trained it for 10 epochs with ~700K training examples and the final validation accuracy was ~97%.\r\n\r\nI looked into the sample predictions and accuracies (I am using model.evaluate to get single example accuracy), but it is deviating from manual calculations. \r\n\r\nI'd like to mention my understanding of the accuracy calculation which I'm using during manual calculation:\r\n- For all Input tokens to decoder, it predicts the next token. If predicted token == expected token, the \"count\" of correctly predicted is increased by 1. I apply the same for all decoder Input tokens and at the last calculate accuracy by dividing \"count\" with the count of total Input tokens.\r\n\r\nExamples: ( ip: input to model, op: model output, ex: expected)\r\n_(Note: I'm assuming padding tokens are not considered for metric calculation)_\r\n_(Note: \\<start\\>: 1, \\<pad\\>: 0, \\<end\\>: 2)_\r\n\r\nCode used for finding accuracy for single example:\r\n`model.evaluate([ip_encoded[idx:idx+1],op_encoded[idx:idx+1]],op_offset_encoded[idx:idx+1],batch_size=64)`\r\n\r\n1. (all tokens are predicted correctly, manual and Keras accuracies are same)\r\n```\r\nip:  Do you get a pattern similar to t hat shown in Fig.\r\nop:  Do you get a pattern similar to that shown in Fig.\r\nex:  Do you get a pattern similar to that shown in Fig.\r\n\r\nip:  [1, 1313, 416, 742, 261, 1953, 1239, 299, 259, 308, 270, 911, 283, 814, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nop:  [1313, 416, 742, 261, 1953, 1239, 299, 336, 911, 283, 814, 16, 2]\r\nex:  [1313, 416, 742, 261, 1953, 1239, 299, 336, 911, 283, 814, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n\r\n1/1 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 1.0000\r\n[0.003069164464250207, 1.0]\r\n```\r\n2. (manual accuracy: 17/23=0.73, keras: 0.95)\r\n```\r\nip:  The equal part that we get in is, therefore, larger than the equal part we get in.\r\nop:  The equal part that we get in  is, therefore, larger than the equal part in which we get .\r\nex:  The equal part that we get in  is, therefore, larger than the equal part we get in .\r\n\r\nip:  [1, 316, 752, 496, 336, 370, 742, 283, 294, 14, 1578, 14, 2115, 621, 264, 752, 496, 370, 742, 283, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nop:  [316, 752, 496, 336, 370, 742, 283, 223, 294, 14, 1578, 14, 2115, 621, 264, 752, 496, 283, 406, 370, 742, 1964, 2]\r\nex:  [316, 752, 496, 336, 370, 742, 283, 223, 294, 14, 1578, 14, 2115, 621, 264, 752, 496, 370, 742, 283, 1964, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n\r\n1/1 [==============================] - 0s 1ms/step - loss: 0.0289 - accuracy: 0.9565\r\n[0.028948727995157242, 0.95652174949646]\r\n```\r\n3. (manual: 10/13=0.77, Keras: 0.92)\r\n```\r\nip:  Can you tell what advanxtage the fungus derives from this association?\r\nop:  Can you tell what advantage affect the sewage from this association?\r\nex:  Can you tell what advantage the fungus derives from this association?\r\n\r\nip:  [1, 1321, 416, 2797, 985, 2923, 282, 1424, 596, 264, 9590, 19016, 389, 428, 6330, 33, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nop:  [1321, 416, 2797, 985, 6089, 2211, 264, 4739, 389, 428, 6330, 33, 2]\r\nex:  [1321, 416, 2797, 985, 6089, 264, 9590, 19016, 389, 428, 6330, 33, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n\r\n1/1 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 0.9286\r\n[0.012137999758124352, 0.9285714030265808]\r\n```\r\n\r\n- It'd be really helpful if community can help me understand Keras accuracy metrics internals and verify my manual metric calculation process.", "comments": ["@brijesh-6899 \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there."]}, {"number": 42303, "title": "Don't include json headers as include/json/json.h", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Bazel version (if compiling from source): 3.1\r\n\r\n**Describe the problem**\r\n\r\nCurrently the JsonCPP headers are all included by `#include \"include/json/json.h\"`. This is very unusual as in pretty much all cases the \"include\" folder is added to the compiler include paths and does not appear in `#include` statements\r\n\r\nDue to this workarounds are required such as symlinking the \"system headers\" into a folder structure such that the above include works which may lead to e.g. #42267\r\n\r\nWhat is the reasoning to use this include scheme?\r\n\r\nI'd like to propose using \"standard\" include schemes like `#include \"json/json.h\"` and adjust the include paths appropriately. By doing so the symlinking is avoided which would resolve #42267 ", "comments": ["Can you send a PR please?", "Added https://github.com/tensorflow/tensorflow/pull/42516, please verify carefully as I still don't know why it was done like it was in the first place.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42303\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42303\">No</a>\n"]}, {"number": 42302, "title": " java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.nnapi.NnApiDelegate.createDelegate()", "body": "  java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.nnapi.NnApiDelegate.createDelegate() (tried Java_org_tensorflow_lite_nnapi_NnApiDelegate_createDelegate and Java_org_tensorflow_lite_nnapi_NnApiDelegate_createDelegate__)\r\n        at org.tensorflow.lite.nnapi.NnApiDelegate.createDelegate(Native Method)\r\n\r\n\r\nHow can i resolve this problem? Thanks very much!", "comments": ["@chuanpham,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Also, please take a look at similar issues [#31114](https://github.com/tensorflow/tensorflow/issues/31114), [#35931](https://github.com/tensorflow/tensorflow/issues/35931) and let us know if it works. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42301, "title": "Loading model with tf.linalg.band_part in regularisation", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Model trained on linux, loading fails on windows\r\n- Model trained on tf 2.2.0, but loading on 2.3.0 also doesn't work - Py 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.models.load_model(model_path)` results in \r\n\r\n```\r\nValueError: Inconsistent values for attr 'Tindex' DT_INT32 vs. DT_INT64 while building NodeDef 'tf_op_layer_MatrixBandPart_3/MatrixBandPart' using Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>\r\n```\r\n\r\nWhich probably comes from this custom regularisation that has been added to the model\r\n\r\n```\r\ndef orthog_reg(weight_matrix):\r\n    \"\"\" The orthogonality inducing regularization. Compute: |upper_triang(dot_prod(W,W))| \"\"\"\r\n    # weight_matrix shape is batch_size x latent_dim so we have row vectors so dot prod is WW^T\r\n    dot_prod = tf.linalg.matmul(weight_matrix, tf.transpose(weight_matrix))\r\n    # we only care about the upper triangular matrix, because the matrix is symmetric and the diagonal represents\r\n    # orthogonality of vectors with itself which we do not care about\r\n    # https://www.tensorflow.org/api_docs/python/tf/linalg/band_part\r\n    upper_triang_dot_prod = tf.linalg.band_part(dot_prod, -1, 0) - tf.linalg.band_part(dot_prod, 0, 0)\r\n    return tf.reduce_sum(upper_triang_dot_prod)\r\n```\r\n\r\nIt is added like this\r\n\r\n```\r\n   # get regularisation\r\n    weights = embedding_one_layer.get_weights()[0]\r\n    weights_relu = tf.nn.relu(weights)\r\n    orthog_reg_result = orthog_reg(weights_relu)\r\n\r\n    # add orthogonal regularisation loss to model loss\r\n    orthog_reg_result = 0.01 * orthog_reg_result\r\n    model.add_loss(orthog_reg_result)\r\n``` \r\n\r\n**Describe the expected behavior**\r\n\r\nExpect it to load ;)\r\n\r\n**Standalone code to reproduce the issue**\r\n(Will add this soon) \r\n", "comments": ["@RoelantStegmann \r\nPlease provide with simple stand alone code to replicate the issue faced or if possible share a colab gist with the issue faced for us to analyse.\r\n\r\nWith respect to the error, follow the below:\r\n#32375 ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42301\">No</a>\n"]}, {"number": 42300, "title": "TensorMap: Add Stack Keys Op to TensorMap", "body": "Add another op for the TensorMap feature: TensorMapStackKeys. Given a TensorMap handle and key dtype, TensorMapStackKeys returns a stacked Tensor containing all keys in the map. Code changes include adding and registering op kernel, wrapping op in python, and testing. @mdanatg @saxenasaurabh @dynamicwebpaige\r\n\r\n", "comments": []}, {"number": 42299, "title": "TensorMap: Modify Erase Op to Not Return Erased Element", "body": "Modified erase op to not return erased element and modified corresponding map ops tests. Also, made a minor change to switch order of Lookup and Insert kernels to be consistently Lookup then Insert. @mdanatg @saxenasaurabh @dynamicwebpaige", "comments": []}, {"number": 42298, "title": "CUBLAS_STATUS_NOT_INITIALIZED", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tf-nightly\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: GeFroce 2080 Ti (X2)\r\n\r\n**Describe the problem**\r\nI'm struggling to get off the ground here with multi-gpu tensorflow.  I've eventually gotten stuck with\"cublas not initialized.\"  Thank you for having a look. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  # Create 2 virtual GPUs with 1GB memory each\r\n  try:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\r\n         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Virtual devices must be set before GPUs have been initialized\r\n    print(e)\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\n\r\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\r\nx_train = x_train[:,:,:,np.newaxis] / 255.0\r\nx_test = x_test[:,:,:,np.newaxis] / 255.0\r\ny_train = to_categorical(y_train)\r\ny_test = to_categorical(y_test)\r\n\r\nwith mirrored_strategy.scope():\r\n    model4 = Sequential()\r\n    model4.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28, 1))) \r\n    model4.add(Conv2D(filters=64, kernel_size=2))\r\n    model4.add(MaxPooling2D(pool_size=2))\r\n    model4.add(Flatten())\r\n    model4.add(Dense(10, activation='softmax'))\r\n\r\nmodel4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nmodel4.summary()\r\n\r\nmodel4.fit(x_train, y_train, epochs=2, validation_split=0.1)\r\n\r\n_, test_acc = model4.evaluate(x_test, y_test)\r\nprint(test_acc)\r\n\r\nmodel4.save_weights('cnn.h5')\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n(base_gpu) C:\\Users\\Mo\\BTC>python test.py\r\n2020-08-12 23:09:37.482332: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-12 23:09:40.326028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-12 23:09:40.483740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:67:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-08-12 23:09:40.497852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:68:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-08-12 23:09:40.515569: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-12 23:09:40.530217: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-12 23:09:40.545759: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll \r\n2020-08-12 23:09:40.552588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-12 23:09:40.566705: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-12 23:09:40.574656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-12 23:09:40.597844: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-12 23:09:40.602568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\nNum GPUs Available:  2\r\n2020-08-12 23:09:40.606521: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-12 23:09:40.672282: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x227a512f230 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-12 23:09:40.700825: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-12 23:09:41.131362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:67:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-08-12 23:09:41.146897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:68:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-08-12 23:09:41.183979: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-12 23:09:41.228784: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-12 23:09:41.251706: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-12 23:09:41.259688: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-12 23:09:41.303717: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-12 23:09:41.328316: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-12 23:09:41.353029: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-12 23:09:41.379921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-08-12 23:09:42.687780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-12 23:09:42.695670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-08-12 23:09:42.700946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N\r\n2020-08-12 23:09:42.723277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N \r\n2020-08-12 23:09:42.747771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5)\r\n2020-08-12 23:09:42.782663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 1024 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5)\r\n2020-08-12 23:09:42.814118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 8582 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5)\r\n2020-08-12 23:09:42.864267: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22831d21960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-12 23:09:42.889142: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-08-12 23:09:42.914052: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2 Physical GPU, 3 Logical GPUs\r\nWARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 28, 28, 64)        320\r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 27, 27, 64)        16448     \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0\r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 10816)             0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 10)                108170\r\n=================================================================\r\nTotal params: 124,938\r\nTrainable params: 124,938\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2020-08-12 23:09:44.300510: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\nEpoch 1/2\r\n2020-08-12 23:09:47.355213: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-12 23:09:47.364005: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.387331: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.431316: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.473173: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.491439: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.499352: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.539511: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.569205: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.593318: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.605302: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-12 23:09:47.643972: E tensorflow/stream_executor/cuda/cuda_dnn.cc:325] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.643972: E tensorflow/stream_executor/cuda/cuda_dnn.cc:325] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.673588: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-08-12 23:09:47.649853: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-08-12 23:09:47.700174: E tensorflow/stream_executor/cuda/cuda_dnn.cc:325] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.723925: E tensorflow/stream_executor/cuda/cuda_dnn.cc:325] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.746358: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-08-12 23:09:47.769343: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-08-12 23:09:47.819798: E tensorflow/stream_executor/cuda/cuda_dnn.cc:325] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.844975: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-08-12 23:09:47.870709: E tensorflow/stream_executor/cuda/cuda_dnn.cc:325] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-08-12 23:09:47.880941: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 55, in <module>\r\n    model4.fit(x_train, y_train, epochs=2, validation_split=0.1)\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 103, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1102, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 787, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 847, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2922, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1853, in _filtered_call\r\n    return self._call_flat(\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1933, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 552, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnknownError: 4 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node replica_2/sequential/conv2d/Conv2D (defined at C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\threading.py:932) ]]\r\n         [[div_no_nan_1/ReadVariableOp_4/_88]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node replica_2/sequential/conv2d/Conv2D (defined at C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\threading.py:932) ]]\r\n  (2) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node replica_2/sequential/conv2d/Conv2D (defined at C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\threading.py:932) ]]\r\n         [[gradient_tape/replica_1/sequential/conv2d/BiasAdd/BiasAddGrad/_144]]\r\n  (3) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node replica_2/sequential/conv2d/Conv2D (defined at C:\\Users\\Mo\\anaconda3\\envs\\base_gpu\\lib\\threading.py:932) ]]\r\n         [[AddN_2/_132]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_3014]\r\n", "comments": ["@chocolate-skyr \r\n\r\nCan you see the issues with similar error #35029, #9489 and see if it helps you.Thanks!", "Thank you.  What ended up working fixing my issue was:\r\nreinstalling python with conda which downgraded the cuda toolkit / cudnn to appropriate versions\r\n", "@chocolate-skyr \r\n\r\nGlad to know your issue was resolved.Please, close this thread as your issue was resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42297, "title": "import tensorflow  issue", "body": "Hi guys,\r\n\r\nI have reinstalled my python 3.8.5 64 bit and use pip installed tensorflow 2.3.0. on my window 10 intel Core i7-6700HQ CPU @2.6Ghz.  And I can see it installed successfully using pip list\r\n\r\nhowever whenever I try to import : import tensorflow   in jupyter notebook.\r\n\r\nIt shows following error:\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-3-bc8d927cf602> in <module>\r\n      1 import import_ipynb\r\n----> 2 import tensorflow\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nAny one can help me ? how can I get over it?\r\n\r\nthanks\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@yapifu,\r\nLooks like the Jupyter notebook is using a different Python interpreter from the one where TensorFlow is installed. \r\n\r\nPlease make sure you are installing TensorFlow and running Jupyter from the same Python installation and check if it works. Thanks! ", "@amahendrakar \r\nThanks for your feedback. Can you tell me how to check it and how to make the Jupyter notebook use same python interpreter as the one where TensorFlow is installed?\r\n\r\nI am pretty new to the config of the Jupyter notebook.\r\nthanks", "@yapifu,\r\nRun the below code snippet from the terminal where you installed TensorFlow and the same code within a Jupyter Notebook.\r\n\r\n```\r\nimport os\r\nimport sys\r\nos.path.dirname(sys.executable)\r\n```\r\nThe output will print the path of the Python interpreter being used. Make sure both the environments are using the same interpreter.\r\n\r\nPlease take a look at [this](https://stackoverflow.com/a/39070588) StackOverflow comment for reference. Thanks!", "@amahendrakar \r\nI run your code in Jupyter notebook, it gave me this : \"'d:\\\\Anaconda3'\"\r\nhowever, I don't know whether it's the right version of python it uses.\r\nI also check the link you gave. \r\n\r\nI didn't see any Conda tab. Here is what my jupyter notebook look like.\r\n![image](https://user-images.githubusercontent.com/67816810/90591117-d787a300-e1a7-11ea-8043-cb1fb75fbb81.png)\r\n\r\nHow can I change so that I can use TensorFlow\r\n\r\nthanks", "It  looks like I have 2 pythons\r\nOne is under anacodna folder, one is in another folder\r\n![image](https://user-images.githubusercontent.com/67816810/90591986-10287c00-e1aa-11ea-980b-a5f8b71da06c.png)\r\n\r\n![image](https://user-images.githubusercontent.com/67816810/90592006-25050f80-e1aa-11ea-918d-0544f7fe7ffe.png)\r\n\r\n\r\nany idea? the 2nd one, is the one I installed before anaconda, the first one is the one came with the installation of anaconda .", "@yapifu,\r\nIn this case, you'll have to install TensorFlow in the Anaconda folder. You can do this through the Anaconda prompt.\r\n\r\nBut to avoid any further conflicts, I'd suggest you to delete both the environments, restart your machine and then install a single fresh copy of Anaconda or Python. Thanks!", "thanks for your feedback. The issue was solved. I installed TensorFlow under windows prompt rather than Anaconda prompt.\r\n\r\nthanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42297\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42297\">No</a>\n"]}, {"number": 42296, "title": "How to set different learning rate at different layers when using tf.keras.model.fit", "body": "hello:\r\nI am wondering if there is a way that I can use different learning rate for different layers. I am trying to modify a pre-trained model and use it for other tasks. What I want is to speed up the training for new added layers and keep the trained layers at low learning rate in order to prevent them from being distorted. for example, I have a 5-conv-layer pre-trained model. Now I add a new conv layer and fine tune it. The first 5 layers would have learning rate of 0.00001 and the last one would have 0.001. Any idea how to achieve this? another important part is I want to use model.fit().\r\nthanks\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["You have to custom implement this piece as we don't have out of the box functionality.\r\nThis [blog](https://ksaluja15.github.io/Learning-Rate-Multipliers-in-Keras/) can be good starting point in your case.", "I read a part of source code of op and think this method can be used.\r\n```\r\nfrom tensorflow.python.training import training_ops\r\nimport tensorflow as tf\r\n\r\nclass Adam_with_layer(tf.keras.optimizers.Adam):\r\n    def __init__(self, lr=0.001):\r\n        super(Adam_with_layer, self).__init__(learning_rate=lr)\r\n        self.dict_lr_schedul = {\r\n            0.01: [\"/bert/embeddings\"] + [\"/bert/encoder/layer_._\" + str(i) for i in range(0, 6)],\r\n            0.02: [\"/bert/encoder/layer_._\" + str(i) for i in range(6, 9)],\r\n            0.1: [\"/bert/encoder/layer_._\" + str(i) for i in range(9, 12)] + [\"/bert/pooler\"],\r\n            1: [\"bidirectional\", \"layer_normalization\", \"label_attention\"]\r\n        }\r\n    def _resource_apply_dense(self, grad, var, apply_state=None):\r\n        var_device, var_dtype = var.device, var.dtype.base_dtype\r\n        coefficients = ((apply_state or {}).get((var_device, var_dtype))\r\n                        or self._fallback_apply_state(var_device, var_dtype))\r\n\r\n        m = self.get_slot(var, 'm')\r\n        v = self.get_slot(var, 'v')\r\n\r\n        if not self.amsgrad:\r\n            return training_ops.resource_apply_adam(\r\n                var.handle,\r\n                m.handle,\r\n                v.handle,\r\n                coefficients['beta_1_power'],\r\n                coefficients['beta_2_power'],\r\n                # coefficients['lr_t'],  # replaced by next\r\n                coefficients['lr_t']*self.lr_with_layer(var),\r\n                coefficients['beta_1_t'],\r\n                coefficients['beta_2_t'],\r\n                coefficients['epsilon'],\r\n                grad,\r\n                use_locking=self._use_locking)\r\n        else:\r\n            vhat = self.get_slot(var, 'vhat')\r\n            return training_ops.resource_apply_adam_with_amsgrad(\r\n                var.handle,\r\n                m.handle,\r\n                v.handle,\r\n                vhat.handle,\r\n                coefficients['beta_1_power'],\r\n                coefficients['beta_2_power'],\r\n                # coefficients['lr_t'],# replaced by next\r\n                coefficients['lr_t'] * self.lr_wide_layer(var),\r\n                coefficients['beta_1_t'],\r\n                coefficients['beta_2_t'],\r\n                coefficients['epsilon'],\r\n                grad,\r\n                use_locking=self._use_locking)\r\n\r\n    def lr_with_layer(self,var):\r\n        \"\"\"\r\n        \r\n        :return:\r\n        \"\"\"\r\n        name = var.name\r\n        for k, v in self.dict_lr_schedul.items():\r\n            for title in v:\r\n                if name.find(title) != -1:\r\n                    return k\r\n        return 1\r\n```", "@jianrui1995 Your solution looks good to me, I think the options are either to do that or to override `Model.train_step` to process the gradients in a custom way. In `tf-nightly`, there's also the option to supply the `gradient_transformers` argument in `Optimizer.__init__`: [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L313). With that argument, you could write a function that multiplies each gradient by some fraction based on its position in the list of Variables.", "`tfa.optimizers.MultiOptimizer`"]}, {"number": 42295, "title": "Fix uint8 output conversion", "body": "uint8 `output` should be converted according to `output_zero_point` and `output_scale`.", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/42295\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "@leondgarse  Can you please resolve conflicts? Thanks!", "Hi leondgarse,\r\n\r\nThanks for the PR on post-training quantization. Just wondering what the benefits is for the extra conversion? The \"argmax()\" works on both 8bit output as well as float output, so the converting back to float seems unnecessary. Right?", "Hi @jianlijianli ,\r\n\r\nYa, you are right. It currently runs fine without converting back to float. My thought is that as a tutorial notebook, we should contain that converting part. This really confused me as I learning how to do this using another model, that it outputs `embedding` value other than `softmax` value.\r\n\r\nPlease feel free to reject this if you dont like it. :)", "Understand and I think it worth some clarification.\r\n\r\nInstead of converting that back to float, is it possible to just add a comment for \"predictions[i] = output.argmax()\"? Something alone the line:\r\n\"It's a classification so we can call argmax() directly on the quantized output, instead of converting back to float.\"\r\n\r\nDoes this make sense? Thanks.\r\n", "I finally found out why I make this checkin...\r\nAs in the earlier version, it is actually converted to float, like this:\r\n```py\r\nif output_details['dtype'] == np.uint8:\r\n    output_scale, output_zero_point = output_details[\"quantization\"]\r\n    test_image = test_image.astype(np.float32)\r\n    test_image = test_image / input_scale + input_zero_point\r\n```\r\nWhich is not right, it should be\r\n```py\r\nif output_details['dtype'] == np.uint8:\r\n    output_scale, output_zero_point = output_details[\"quantization\"]\r\n    output = output.astype(np.float32)\r\n    output = (output - output_zero_point) * output_scale\r\n```\r\nWhich is my original checkin...\r\nThen it is just removed in [Fix copy/paste mistake error specifying incorrect dequantization code.](https://github.com/tensorflow/tensorflow/commit/17eb743235f667272865dab7ca944f2a8e706d99#diff-ff5219d777bd073cc5525bcfcc71c24d8667e004ff476a3c25980a2d0df76537)...\r\n\r\nIn my understanding, even in classification task, converting back to float is essential to get the actual probability value.\r\n```py\r\n# Given the output value of `test_image_indices == 18`, where it's actually `3`, but predicted as `8`\r\noutput = np.array([ 99,  47, 142, 163,  79, 161, 116,  61, 176, 110], dtype=uint8)\r\n# Calculate the probability value using this uint8 value, and the max value is `9.9999738e-01`\r\nprint(tf.nn.softmax(tf.cast(output, 'float32')).numpy())\r\n# [3.6251316e-34 0.0000000e+00 1.7139039e-15 2.2603235e-06 0.0000000e+00\r\n#  3.0590152e-07 8.7564878e-27 0.0000000e+00 9.9999738e-01 2.1705163e-29]\r\n\r\n# The true probability value should be this, where the max value is `8.1651992e-01`\r\nprint(tf.nn.softmax((tf.cast(output, 'float32') - output_zero_point) * output_scale).numpy())\r\n# [4.0755808e-06 1.0709489e-09 3.7232505e-03 1.0395887e-01 1.7104284e-07\r\n#  7.5709976e-02 6.0354825e-05 9.8566248e-09 8.1651992e-01 2.3312372e-05]\r\n```\r\nBut in this notebook, it just works using `uint8`... So shall I close this?", "Now I see where you are coming from and your understanding is absolutely right on the numerics. Since now there is no confusion, if you are OK, please feel free to close this.  Thanks."]}, {"number": 42293, "title": "tf.keras.backend.temporal_padding crashes(bad_alloc) when padding is large", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.backend.temporal_padding` crashes(abort, bad_alloc) when `padding` is large\r\n\r\n**Describe the expected behavior**\r\nExpect no crashes\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\n\r\nimport tensorflow as tf\r\ntf.keras.backend.temporal_padding(x=tf.ones((2,1,1)), padding=(6400000000000000000, 4314310000000000000))\r\n~~~\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nIn TF2.1\r\n~~~python\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n~~~\r\nIn TF nightly\r\n~~~\r\n2020-08-13 01:04:13.669045: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-13 01:04:13.683160: F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-7732434073709551615 vs. 0)\r\nAborted (core dumped)\r\n\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.1, TF v2.3 and TF-nightly. \r\n\r\nSession crashes on running the `tf.keras.backend.temporal_padding` line. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f7850e90c6c3500a02b4a555770e4bcd/42293.ipynb). Thanks!", "This is expected, since TF is trying to allocate the memory for a huge tensor with shape (2, 6400000000000000000 + 1 + 4314310000000000000, 1), and it will certainly OOM. \r\n\r\nI am closing this issue since I don't think it is working as intended. Feel free to reopen if you feel otherwise.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42293\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42293\">No</a>\n"]}, {"number": 42292, "title": "fix path of hipcc to match rocm packaging", "body": "Resolves #42291 \r\n\r\nThis PR helps move along the build when building tensorflow from source.", "comments": []}, {"number": 42291, "title": "[ROCm] sh: /opt/rocm/bin/hipcc: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): 10.1.0\r\n- CUDA/cuDNN version: N/A\r\n- ROCm version: 3.5.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen building I am getting the following error:\r\n```\r\nERROR: /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/tensorflow/core/kernels/BUILD:3132:18: error while parsing .d file: /home/acxz/.cache/bazel/_bazel_acxz/032a3a4c537da51b5f6f596867eba382/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/non_max_suppression_op_gpu/non_max_suppression_op.cu.pic.d (No such file or directory)\r\nsh: /opt/rocm/bin/hipcc: No such file or directory\r\nINFO: Elapsed time: 784.106s, Critical Path: 142.83s\r\nINFO: 773 processes: 773 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. git clone\r\n2. `export TF_NEED_ROCM=1`\r\n3. `./configure`\r\n4. \r\n```\r\n  bazel \\\r\n        build --config=mkl --config=avx2_linux -c opt \\\r\n          //tensorflow:libtensorflow.so \\\r\n          //tensorflow:libtensorflow_cc.so \\\r\n          //tensorflow:install_headers \\\r\n          //tensorflow/tools/pip_package:build_pip_package\r\n      bazel-bin/tensorflow/tools/pip_package/build_pip_package --gpu \"${srcdir}\"/tmpoptrocm\r\n```\r\n\r\nTo be exactly precise I am using the following build script (PKGBUILD):\r\nhttps://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=tensorflow-rocm\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDownstream issue: https://github.com/rocm-arch/tensorflow-rocm/issues/9\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42291\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42291\">No</a>\n"]}, {"number": 42290, "title": "Deploy micro_speech  to ESP32 using ESP IDF", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Target platform: esp32\r\n\r\n**Describe the problem**\r\nUnable to build the micro_speech in Tensorflow using esp-idf. \r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nGenerate examples\r\n\r\n```\r\ncd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf\r\n```\r\nBuilding the example\r\n```\r\nidf.py build\r\n```\r\n\r\nExecuting action: all (aliases: build)\r\nRunning ninja in directory /home/george/Documents/MLANDAI/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"ninja all\"...\r\n[1/367] Performing build step for 'bootloader'\r\nninja: no work to do.\r\n[6/365] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/round.cc.obj\r\nFAILED: esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/round.cc.obj \r\n/home/george/.espressif/tools/xtensa-esp32-elf/esp-2020r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++   -Iconfig -I../components/tfmicro -I../components/tfmicro/third_party/gemmlowp -I../components/tfmicro/third_party/flatbuffers/include -I../components/tfmicro/third_party/ruy -I../components/tfmicro/third_party/kissfft -I/home/george/esp/esp-idf/components/newlib/platform_include -I/home/george/esp/esp-idf/components/freertos/include -I/home/george/esp/esp-idf/components/freertos/xtensa/include -I/home/george/esp/esp-idf/components/heap/include -I/home/george/esp/esp-idf/components/log/include -I/home/george/esp/esp-idf/components/lwip/include/apps -I/home/george/esp/esp-idf/components/lwip/include/apps/sntp -I/home/george/esp/esp-idf/components/lwip/lwip/src/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include/arch -I/home/george/esp/esp-idf/components/soc/src/esp32/. -I/home/george/esp/esp-idf/components/soc/src/esp32/include -I/home/george/esp/esp-idf/components/soc/include -I/home/george/esp/esp-idf/components/esp_rom/include -I/home/george/esp/esp-idf/components/esp_common/include -I/home/george/esp/esp-idf/components/esp_system/include -I/home/george/esp/esp-idf/components/xtensa/include -I/home/george/esp/esp-idf/components/xtensa/esp32/include -I/home/george/esp/esp-idf/components/esp32/include -I/home/george/esp/esp-idf/components/driver/include -I/home/george/esp/esp-idf/components/driver/esp32/include -I/home/george/esp/esp-idf/components/esp_ringbuf/include -I/home/george/esp/esp-idf/components/efuse/include -I/home/george/esp/esp-idf/components/efuse/esp32/include -I/home/george/esp/esp-idf/components/espcoredump/include -I/home/george/esp/esp-idf/components/esp_timer/include -I/home/george/esp/esp-idf/components/esp_ipc/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/. -I/home/george/esp/esp-idf/components/soc/soc/esp32/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/../include -I/home/george/esp/esp-idf/components/vfs/include -I/home/george/esp/esp-idf/components/esp_wifi/include -I/home/george/esp/esp-idf/components/esp_wifi/esp32/include -I/home/george/esp/esp-idf/components/esp_event/include -I/home/george/esp/esp-idf/components/esp_netif/include -I/home/george/esp/esp-idf/components/esp_eth/include -I/home/george/esp/esp-idf/components/tcpip_adapter/include -I/home/george/esp/esp-idf/components/app_trace/include -mlongcalls -Wno-frame-address   -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -Wall -Werror=all -Wno-error=unused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Wno-unused-parameter -Wno-sign-compare -ggdb -O2 -std=gnu++11 -fno-exceptions -fno-rtti -D_GNU_SOURCE -DIDF_VER=\\\"v4.3-dev-771-gc77c4ccf6\\\" -DESP_PLATFORM -Wno-maybe-uninitialized -Wno-missing-field-initializers -Wno-type-limits -std=c11 -DTF_LITE_STATIC_MEMORY -DNDEBUG -O3 -Wno-nonnull -std=c++11 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -MD -MT esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/round.cc.obj -MF esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/round.cc.obj.d -o esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/round.cc.obj -c ../components/tfmicro/tensorflow/lite/micro/kernels/round.cc\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\n[8/365] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/neg.cc.obj\r\nFAILED: esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/neg.cc.obj \r\n/home/george/.espressif/tools/xtensa-esp32-elf/esp-2020r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++   -Iconfig -I../components/tfmicro -I../components/tfmicro/third_party/gemmlowp -I../components/tfmicro/third_party/flatbuffers/include -I../components/tfmicro/third_party/ruy -I../components/tfmicro/third_party/kissfft -I/home/george/esp/esp-idf/components/newlib/platform_include -I/home/george/esp/esp-idf/components/freertos/include -I/home/george/esp/esp-idf/components/freertos/xtensa/include -I/home/george/esp/esp-idf/components/heap/include -I/home/george/esp/esp-idf/components/log/include -I/home/george/esp/esp-idf/components/lwip/include/apps -I/home/george/esp/esp-idf/components/lwip/include/apps/sntp -I/home/george/esp/esp-idf/components/lwip/lwip/src/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include/arch -I/home/george/esp/esp-idf/components/soc/src/esp32/. -I/home/george/esp/esp-idf/components/soc/src/esp32/include -I/home/george/esp/esp-idf/components/soc/include -I/home/george/esp/esp-idf/components/esp_rom/include -I/home/george/esp/esp-idf/components/esp_common/include -I/home/george/esp/esp-idf/components/esp_system/include -I/home/george/esp/esp-idf/components/xtensa/include -I/home/george/esp/esp-idf/components/xtensa/esp32/include -I/home/george/esp/esp-idf/components/esp32/include -I/home/george/esp/esp-idf/components/driver/include -I/home/george/esp/esp-idf/components/driver/esp32/include -I/home/george/esp/esp-idf/components/esp_ringbuf/include -I/home/george/esp/esp-idf/components/efuse/include -I/home/george/esp/esp-idf/components/efuse/esp32/include -I/home/george/esp/esp-idf/components/espcoredump/include -I/home/george/esp/esp-idf/components/esp_timer/include -I/home/george/esp/esp-idf/components/esp_ipc/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/. -I/home/george/esp/esp-idf/components/soc/soc/esp32/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/../include -I/home/george/esp/esp-idf/components/vfs/include -I/home/george/esp/esp-idf/components/esp_wifi/include -I/home/george/esp/esp-idf/components/esp_wifi/esp32/include -I/home/george/esp/esp-idf/components/esp_event/include -I/home/george/esp/esp-idf/components/esp_netif/include -I/home/george/esp/esp-idf/components/esp_eth/include -I/home/george/esp/esp-idf/components/tcpip_adapter/include -I/home/george/esp/esp-idf/components/app_trace/include -mlongcalls -Wno-frame-address   -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -Wall -Werror=all -Wno-error=unused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Wno-unused-parameter -Wno-sign-compare -ggdb -O2 -std=gnu++11 -fno-exceptions -fno-rtti -D_GNU_SOURCE -DIDF_VER=\\\"v4.3-dev-771-gc77c4ccf6\\\" -DESP_PLATFORM -Wno-maybe-uninitialized -Wno-missing-field-initializers -Wno-type-limits -std=c11 -DTF_LITE_STATIC_MEMORY -DNDEBUG -O3 -Wno-nonnull -std=c++11 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -MD -MT esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/neg.cc.obj -MF esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/neg.cc.obj.d -o esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/neg.cc.obj -c ../components/tfmicro/tensorflow/lite/micro/kernels/neg.cc\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\n[9/365] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/unpack.cc.obj\r\nFAILED: esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/unpack.cc.obj \r\n/home/george/.espressif/tools/xtensa-esp32-elf/esp-2020r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++   -Iconfig -I../components/tfmicro -I../components/tfmicro/third_party/gemmlowp -I../components/tfmicro/third_party/flatbuffers/include -I../components/tfmicro/third_party/ruy -I../components/tfmicro/third_party/kissfft -I/home/george/esp/esp-idf/components/newlib/platform_include -I/home/george/esp/esp-idf/components/freertos/include -I/home/george/esp/esp-idf/components/freertos/xtensa/include -I/home/george/esp/esp-idf/components/heap/include -I/home/george/esp/esp-idf/components/log/include -I/home/george/esp/esp-idf/components/lwip/include/apps -I/home/george/esp/esp-idf/components/lwip/include/apps/sntp -I/home/george/esp/esp-idf/components/lwip/lwip/src/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include/arch -I/home/george/esp/esp-idf/components/soc/src/esp32/. -I/home/george/esp/esp-idf/components/soc/src/esp32/include -I/home/george/esp/esp-idf/components/soc/include -I/home/george/esp/esp-idf/components/esp_rom/include -I/home/george/esp/esp-idf/components/esp_common/include -I/home/george/esp/esp-idf/components/esp_system/include -I/home/george/esp/esp-idf/components/xtensa/include -I/home/george/esp/esp-idf/components/xtensa/esp32/include -I/home/george/esp/esp-idf/components/esp32/include -I/home/george/esp/esp-idf/components/driver/include -I/home/george/esp/esp-idf/components/driver/esp32/include -I/home/george/esp/esp-idf/components/esp_ringbuf/include -I/home/george/esp/esp-idf/components/efuse/include -I/home/george/esp/esp-idf/components/efuse/esp32/include -I/home/george/esp/esp-idf/components/espcoredump/include -I/home/george/esp/esp-idf/components/esp_timer/include -I/home/george/esp/esp-idf/components/esp_ipc/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/. -I/home/george/esp/esp-idf/components/soc/soc/esp32/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/../include -I/home/george/esp/esp-idf/components/vfs/include -I/home/george/esp/esp-idf/components/esp_wifi/include -I/home/george/esp/esp-idf/components/esp_wifi/esp32/include -I/home/george/esp/esp-idf/components/esp_event/include -I/home/george/esp/esp-idf/components/esp_netif/include -I/home/george/esp/esp-idf/components/esp_eth/include -I/home/george/esp/esp-idf/components/tcpip_adapter/include -I/home/george/esp/esp-idf/components/app_trace/include -mlongcalls -Wno-frame-address   -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -Wall -Werror=all -Wno-error=unused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Wno-unused-parameter -Wno-sign-compare -ggdb -O2 -std=gnu++11 -fno-exceptions -fno-rtti -D_GNU_SOURCE -DIDF_VER=\\\"v4.3-dev-771-gc77c4ccf6\\\" -DESP_PLATFORM -Wno-maybe-uninitialized -Wno-missing-field-initializers -Wno-type-limits -std=c11 -DTF_LITE_STATIC_MEMORY -DNDEBUG -O3 -Wno-nonnull -std=c++11 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -MD -MT esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/unpack.cc.obj -MF esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/unpack.cc.obj.d -o esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/unpack.cc.obj -c ../components/tfmicro/tensorflow/lite/micro/kernels/unpack.cc\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\n[15/365] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/add.cc.obj\r\nFAILED: esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/add.cc.obj \r\n/home/george/.espressif/tools/xtensa-esp32-elf/esp-2020r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++   -Iconfig -I../components/tfmicro -I../components/tfmicro/third_party/gemmlowp -I../components/tfmicro/third_party/flatbuffers/include -I../components/tfmicro/third_party/ruy -I../components/tfmicro/third_party/kissfft -I/home/george/esp/esp-idf/components/newlib/platform_include -I/home/george/esp/esp-idf/components/freertos/include -I/home/george/esp/esp-idf/components/freertos/xtensa/include -I/home/george/esp/esp-idf/components/heap/include -I/home/george/esp/esp-idf/components/log/include -I/home/george/esp/esp-idf/components/lwip/include/apps -I/home/george/esp/esp-idf/components/lwip/include/apps/sntp -I/home/george/esp/esp-idf/components/lwip/lwip/src/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include/arch -I/home/george/esp/esp-idf/components/soc/src/esp32/. -I/home/george/esp/esp-idf/components/soc/src/esp32/include -I/home/george/esp/esp-idf/components/soc/include -I/home/george/esp/esp-idf/components/esp_rom/include -I/home/george/esp/esp-idf/components/esp_common/include -I/home/george/esp/esp-idf/components/esp_system/include -I/home/george/esp/esp-idf/components/xtensa/include -I/home/george/esp/esp-idf/components/xtensa/esp32/include -I/home/george/esp/esp-idf/components/esp32/include -I/home/george/esp/esp-idf/components/driver/include -I/home/george/esp/esp-idf/components/driver/esp32/include -I/home/george/esp/esp-idf/components/esp_ringbuf/include -I/home/george/esp/esp-idf/components/efuse/include -I/home/george/esp/esp-idf/components/efuse/esp32/include -I/home/george/esp/esp-idf/components/espcoredump/include -I/home/george/esp/esp-idf/components/esp_timer/include -I/home/george/esp/esp-idf/components/esp_ipc/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/. -I/home/george/esp/esp-idf/components/soc/soc/esp32/include -I/home/george/esp/esp-idf/components/soc/soc/esp32/../include -I/home/george/esp/esp-idf/components/vfs/include -I/home/george/esp/esp-idf/components/esp_wifi/include -I/home/george/esp/esp-idf/components/esp_wifi/esp32/include -I/home/george/esp/esp-idf/components/esp_event/include -I/home/george/esp/esp-idf/components/esp_netif/include -I/home/george/esp/esp-idf/components/esp_eth/include -I/home/george/esp/esp-idf/components/tcpip_adapter/include -I/home/george/esp/esp-idf/components/app_trace/include -mlongcalls -Wno-frame-address   -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -Wall -Werror=all -Wno-error=unused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Wno-unused-parameter -Wno-sign-compare -ggdb -O2 -std=gnu++11 -fno-exceptions -fno-rtti -D_GNU_SOURCE -DIDF_VER=\\\"v4.3-dev-771-gc77c4ccf6\\\" -DESP_PLATFORM -Wno-maybe-uninitialized -Wno-missing-field-initializers -Wno-type-limits -std=c11 -DTF_LITE_STATIC_MEMORY -DNDEBUG -O3 -Wno-nonnull -std=c++11 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -MD -MT esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/add.cc.obj -MF esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/add.cc.obj.d -o esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/add.cc.obj -c ../components/tfmicro/tensorflow/lite/micro/kernels/add.cc\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\nninja: build stopped: subcommand failed.\r\nninja failed with exit code 1\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@gigwegbe \r\n\r\nPlease, provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "@ravikyram sequence of commands has been updated. Thanks", "The issue has been fixed on master branch with following commit:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/dfafcc5758a33335a57d062481af3783ee16f0db \r\n\r\nCan you please cherry-pick the commit and check?"]}, {"number": 42289, "title": "Adding methods and tests for ndarray: ndarray.sum(), ndarray.argmax(), ndarray.argmin(), ndarray.any(), ndarray.all(), ndarray.clip(), ndarray.diagonal().", "body": "", "comments": ["@alfredoav  Can you please resolve conflicts? Thanks!", "@alfredoav Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 42288, "title": "[ROCm] hip-clang (3.5) bugfixes", "body": "This fixes a few problems that surfaced in ROCm after the switch to hip-clang with version 3.5.\r\n* Several kernels crash or execute incorrectly without these fixes\r\n* impl_rsqrt requires changes because hip-clang does FP rounding differently, and code that used to work correctly now results in NaN's for inputs close to zero", "comments": ["Changes have been merged into master by commit eb0f1ed. So closing the PR."]}, {"number": 42287, "title": "graph_util.remove_training_nodes removes library from graphdef", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: only cpu\r\n- GPU model and memory: only on cpu\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ngraph_util.remove_training_nodes will remove library from graphdef, which is needed for operator like StatefulPartitionedCall. \r\n\r\n**Describe the expected behavior**\r\nThe library should not be removed when freezing graph.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@yongwww,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, TensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42287\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42287\">No</a>\n"]}, {"number": 42286, "title": "In TF2.3 Reshape is not converted correctly", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nAttached to issue \r\n[saved_model.zip](https://github.com/tensorflow/tensorflow/files/5065959/saved_model.zip)\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n\r\nWhen converting a reshape, the resulting model has extra ops. Here's the expected output, as produced with `converter.experimental_new_converter = False`:\r\n\r\n![image](https://user-images.githubusercontent.com/379235/90071187-f8349200-dca9-11ea-882f-1ed354fbe36a.png)\r\n\r\nAnd here's the output with the new converter, note the extra ops before the reshape:\r\n\r\n![image](https://user-images.githubusercontent.com/379235/90071275-1f8b5f00-dcaa-11ea-90e6-0ee502e148c6.png)\r\n", "comments": ["@dansitu \r\nI am unable to open the file shared, can you please share a colab gist with the error faced.", "I've checked the attached file and it works. It's extremely simple to reproduce; just extract the .zip file and do this:\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\n```\r\n\r\nThen save and inspect the `tflite_model`.", "@dansitu \r\nI ran your code as mentioned above on tf-nightly and did not face any error, please find [gist here](https://colab.research.google.com/gist/Saduf2019/2a93aacc1403d6561356aca9fe072f1f/untitled377.ipynb)", "@Saduf2019 The code executes, but it produces an incorrect output.\r\n\r\nI've created a gist that demonstrates the issue:\r\n\r\nhttps://colab.research.google.com/gist/dansitu/fdf90b82ed194d592716f777cf92e7dc/untitled377.ipynb\r\n\r\nNote how when `converter.experimental_new_converter=True`, the output contains spurious ops and is missing another op.\r\n\r\nI also noticed that `tf-nightly` crashes when `converter.experimental_new_converter=True` is set, but it does not crash with TF2.3.", "I am also facing this issue", "@dansitu @idenc \r\nThis is not an issue with the converter, actually it looks working as expected.\r\nLet me explain why the extra nodes/discrepancy,\r\n\r\n* Why using old converter looks as working ?\r\nThe old converter didn't support dynamic shapes (tensors with unknown dimension like (None, 637). Incorrectly it used to set 1 for the dynamic dimension if it is batch dimension (first dimension).\r\nThis is incorrect, because \r\n   Assuming static shape allows the converter to do extra optimizations, that will be incorrect if you tried doing resize_input_tensor on the interpreter. (You can find many issues already created with old converter about failures during resizing).\r\n\r\n* Why the model using current converter has extra ops ?\r\nThe new converter now supports dynamic shapes, and if the graph has dynamic shape it will honor this, and passes this information to the runtime, so users can resize the input tensors correctly.\r\n\r\n* My use case doesn't need dynamic shape, what can i do ?\r\nThen in your model set the shape to a static shape and things should work as expected.\r\n\r\nI modified the conversion lines to set a static shape using something like below (I suggest you modify your original model instead)\r\nmodel = tf.saved_model.load('/tmp/saved_model')\r\nconcrete_func = model.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([1, 637])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nmodel_new_converter = converter.convert()\r\nopen('/tmp/model_new_converter_static_input_shape.tflite', 'wb').write(model_new_converter)\r\n\r\nThanks", "Closing the issue. Please reopen or file new one.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42286\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42286\">No</a>\n", "This makes sense, thanks for the clarification @karimnosseir!  Hope you are doing well :)", "FYI it's worth updating the converter docs with this info, since by default Keras models have a batch dimension of None.", "@dansitu Thanks, hope you are doing well too :) \r\nWill update the docs.\r\n\r\nThanks"]}, {"number": 42285, "title": "TFLite: Improved memory access pattern in integer_ops depthwise_conv.h", "body": "This PR improves the memory access pattern of the int8 version of `DepthwiseConvPerChannel` in `tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h`. The implementation iterates input/filter data in a sequential manner (for each (x, y), access the data in each channel from 0, 1, ..., C - 1) which becomes consistent with the flattened memory layout (NHWC) in TFLite tensors. This design involves a small fixed size accumulator to store intermediate values in order to sequentially access the memory and utilizes memory hierarchy to reduce the latency caused by cache miss.\r\n\r\n[Updated] August 19: Added uint8 version of the same implementation.\r\n\r\nThe change results in a 5.03% (uint8) / 5.05% (int8) of speedup during inference (or a total of 18.25% speedup if combined with the changes in #41947 ) when running person_detection_experimental (MobileNetV1) on Arty A7 using the soft CPU provided by Antmicro, see antmicro/litex-vexriscv-tensorflow-lite-demo for more info.", "comments": ["Following-up on the discussion initiated by [this comment](https://github.com/tensorflow/tensorflow/pull/42477#issuecomment-676610773) and the follow-on chat that we had, I am closing this PR since the optimizations proposed here will instead be implemented in a TFLM-specific kernel."]}, {"number": 42284, "title": "(please help) model.predict  Error   ", "body": "Hello, I am new to coding but I keep running into an error with what should be a very simple code (I have pasted below)\r\n\r\n**import tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential \r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Reshape\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.utils import to_categorical\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom keras import backend as K\r\nplt.style.use('fivethirtyeight')\r\n\r\nfrom tensorflow.keras.datasets import cifar10\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n\r\n\r\nprint(type(x_train))\r\nprint(type(y_train))\r\nprint(type(x_test))\r\nprint(type(y_test))\r\n\r\nprint('x_train shape:', x_train.shape)\r\nprint('y_train shape:', y_train.shape)\r\nprint('x_test shape:', x_test.shape)\r\nprint('y_test shape:', y_test.shape)\r\n\r\nindex = 1\r\nx_train[index]\r\n\r\nimg = plt.imshow(x_train[index])\r\n\r\nclassification = ['airplane', 'automobile','bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\r\n                  \r\nprint('the image class is:', classification[y_train[index][0]])\r\n\r\ny_train_one_hot = to_categorical(y_train)\r\ny_test_one_hot = to_categorical(y_train)\r\n\r\nprint('the new one hot label is:', y_train_one_hot)\r\n\r\nx_train = x_train / 255\r\nx_test = x_train / 255\r\n\r\nx_train[index]\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add( Conv2D(32, (5,5), activation='relu', input_shape=(32,32,3)) )\r\n\r\nmodel.add(MaxPooling2D(pool_size = (2,2)))\r\n\r\nmodel.add( Conv2D(32, (5,5), activation='relu') )\r\n\r\nmodel.add(MaxPooling2D(pool_size = (2,2)))\r\n\r\nmodel.add(Flatten())\r\n\r\nmodel.add(Dense(1000, activation='relu'))\r\n\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(500, activation='relu'))\r\n\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(250, activation='relu'))\r\n\r\nmodel.add(Dense(10, activation='softmax'))\r\n\r\n\r\nmodel.compile(loss = 'categorical_crossentropy',\r\n              optimizer = 'adam',\r\n              metrics = ['accuracy'])\r\n#hist = model.fit(x_train, y_train_one_hot,\r\n               #batch_size = 256,\r\n               #epochs = 10,\r\n               #validation_split = 0.2)\r\n#model.evaluate(x_test, y_test_one_hot)[1]\r\n\r\n#plt.plot(hist.history['accuracy'])\r\n#plt.plot(hist.history['val_accuracy'])\r\n#plt.title('Model Accuracy')\r\n#plt.ylabel('Accuracy')\r\n#plt.xlabel('Epoch')\r\n#plt.legend(['Train','Val'],loc='upper right')\r\n#plt.show()\r\n\r\nnew_image = plt.imread('/Users/home/Sunflower_from_Silesia2.jpg')\r\nimg=plt.imshow(new_image)\r\nfrom skimage.transform import resize\r\nresized_image = resize(new_image, (32,32,3))\r\nimg = plt.imshow(resized_image)\r\n\r\npredictions = model.predict(np.array([resized_image]))\r\n\r\npredictions**\r\n\r\n\r\nThe error I am recieving is:\r\n\r\n**INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x155eb1680>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x155eb1680>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x155eb1680>: DoNotConvert rule for tensorflow\r\nAllowlisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x155eb1680>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x155eb1830>\r\n    args: (<tf.Tensor 'args_0:0' shape=(1,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x155eb1830>\r\n    args: (<tf.Tensor 'args_0:0' shape=(1,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x155eb1830>: DoNotConvert rule for tensorflow\r\nAllowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x155eb1830>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x155eb1d40>\r\n    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, <tf.Tensor 'args_1:0' shape=(1, 32, 32, 3) dtype=float32>)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x155eb1d40>\r\n    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, <tf.Tensor 'args_1:0' shape=(1, 32, 32, 3) dtype=float32>)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x155eb1d40>: DoNotConvert rule for tensorflow\r\nAllowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x155eb1d40>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0>\r\n    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x155e9a050>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0>\r\n    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x155e9a050>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:<function Model.make_predict_function.<locals>.predict_function at 0x156df00e0> is not cached for subkey ConversionOptions[{}]\r\n<function Model.make_predict_function.<locals>.predict_function at 0x156df00e0> is not cached for subkey ConversionOptions[{}]\r\nINFO:tensorflow:Source code of <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\ndef predict_function(iterator):\r\n  \"\"\"Runs an evaluation execution with one step.\"\"\"\r\n  return step_function(self, iterator)\r\n\r\n\r\nSource code of <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\ndef predict_function(iterator):\r\n  \"\"\"Runs an evaluation execution with one step.\"\"\"\r\n  return step_function(self, iterator)\r\n\r\n\r\nINFO:tensorflow:Error transforming entity <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0>\r\nTraceback (most recent call last):\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 444, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 284, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 286, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 470, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 363, in transform_function\r\n    result = self.transform_ast(node, context)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 252, in transform_ast\r\n    node = self.initial_analysis(node, ctx)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 240, in initial_analysis\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 708, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 578, in visit_FunctionDef\r\n    node = self._visit_arg_annotations(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 554, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 559, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\nError transforming entity <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0>\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nINFO:tensorflow:Converted call: <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x156df0950>\r\n    args: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 32, 32, 3) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x156df0950>\r\n    args: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 32, 32, 3) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Allowlisted: <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x156df0950>: DoNotConvert rule for tensorflow\r\nAllowlisted: <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x156df0950>: DoNotConvert rule for tensorflow\r\nWARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x156df00e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\nTraceback (most recent call last):\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 444, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 284, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 286, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 470, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 363, in transform_function\r\n    result = self.transform_ast(node, context)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 252, in transform_ast\r\n    node = self.initial_analysis(node, ctx)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 240, in initial_analysis\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 708, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 578, in visit_FunctionDef\r\n    node = self._visit_arg_annotations(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 554, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"/Users/home/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 559, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'**\r\n\r\n\r\nEverything works except for the predictions line. Are there any idea how to fix this? I am running this on Spyder on Anaconda-Navigator. Thank you!", "comments": ["@nashWX \r\n\r\nI have tried in colab with TF version 2.3 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b7bdff2dd8cba17bbe3e367d1cc207eb/untitled246.ipynb). Thanks!", "Thanks for the speedy reply @ravikyram . I ran the exact code you sent in Spyder and received this error when it attempts to predict the image:\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x1552da560> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x1552da560> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n\r\n\r\nMy TF version is 2.3 and my keras version is also 2.3.", "@nashWX \r\n\r\nYou can ignore those warning.If you want to disable the warning messages add the below code.\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\n```\r\n\r\nThanks!", "Thank you! I'm now just ignoring the error message. ", "@nashWX \r\n\r\nPlease, close this thread if your issue was resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42284\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42284\">No</a>\n"]}, {"number": 42283, "title": "[breaking? change] Concatenate dict of keras layers breaks on tf 2.3 (but worked in 2.2)", "body": "**System information**\r\n- Have I written custom code : **No**\r\n- OS Platform and Distribution : **macos-latest, ubuntu-latest (github actions)** , macOS Mojave 10.14.1 (locally)\r\n- TensorFlow installed from (source or binary): **pip install**\r\n- TensorFlow version (use command below): *2.2 -> 2.3*\r\n- Python version: **3.6 and 3.7**\r\n\r\n**Describe the current behavior**\r\n(this is on `tensorflow 2.2.0`)\r\n```python\r\nfeatures_in = ['a', 'b']\r\ninput_layers = {\r\n    colname: tf.keras.layers.Input(\r\n        name=colname, shape=(1,), dtype=tf.float32)\r\n    for colname in features_in\r\n}\r\n\r\n# the following line just works\r\nx = tf.keras.layers.Concatenate(axis=-1)(input_layers.values())\r\nx\r\n```\r\n`<tf.Tensor 'concatenate_1/Identity:0' shape=(None, 2) dtype=float32>`\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n(The following is in tensorflow 2.3.0)\r\n`<the same snippet from above>`\r\n\r\nThe following error occurs:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-332d856dc8d7> in <module>\r\n      6 }\r\n      7 \r\n----> 8 x = tf.keras.layers.Concatenate(axis=-1)(input_layers.values())\r\n      9 x\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    924     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    925       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 926                                                 input_list)\r\n    927 \r\n    928     # Maintains info about the `Layer.call` stack.\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1115           try:\r\n   1116             with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n-> 1117               outputs = call_fn(cast_inputs, *args, **kwargs)\r\n   1118 \r\n   1119           except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/merge.py in call(self, inputs)\r\n    120   def call(self, inputs):\r\n    121     if not isinstance(inputs, (list, tuple)):\r\n--> 122       raise ValueError('A merge layer should be called on a list of inputs.')\r\n    123     if self._reshape_required:\r\n    124       reshaped_inputs = []\r\n\r\nValueError: A merge layer should be called on a list of inputs.\r\n```\r\n", "comments": ["I am not sure why this happens for you, but I believe the `Concatenate` layer has expected inputs to be a list or tuple even in [TF 2.2](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/layers/merge.py#L120-L122). In my local environments, the same `ValueError` is thrown in TensorFlow 2.2 and even earlier versions.\r\n\r\nOne easy fix for you would be to use `list(input_layers.values())`, because `dict.values` returns a view rather than a list. Maybe you were using \"Python 2\" previously --- in Python 2 `dict.values` returns a list, which is not true anymore in PY3.", "It worked with `list(input_layers.values())`.\r\n\r\nI see no reason why it should not work with just `input_layers.values()`, but I'll leave the design of the api to the tensorflow team.\r\n\r\nMany thanks :) ", "Was able to reproduce the issue. Code runs fine on [TF v2.2](https://colab.research.google.com/gist/amahendrakar/839eaa92ee21ae1050e6c4cd1c08ed85/42283-2-2.ipynb), but throws a `ValueError` with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/72a0da103d1057fd15cada9d5defc737/42283.ipynb). Please find the attached gist. Thanks!", "@ntakouris This issue has been fixed in the latest version of nightly. Please find my gist [here](https://colab.research.google.com/gist/gowthamkpr/9a1e119b2f06cc59ee62190ce80482aa/untitled308.ipynb)", "Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42283\">No</a>\n"]}, {"number": 42282, "title": "How can i solve data transferred from slave to master using SPI?", "body": "I tried to make a communication between two AVR (ATmega128) using SPI. Data transferred correctly from master to slave but data transferred wrong from slave to master, the first sampled bit is always wrong. Slave sends (0X7E) to master but the received data is (0X3F). Where is the mistake?\r\n\r\nCode of MASTER\r\n\r\n#define F_CPU 16000000UL\r\n\r\n#include <avr/io.h>\r\n#include <util/delay.h>\r\n\r\n#define DDR_SPI DDRB\r\n#define DD_MOSI PB2\r\n#define DD_SCK PB1\r\n#define DD_SS PB0\r\n\r\n#define ACK 0x7E\r\n\r\nvoid SPI_MasterInit(void)\r\n{\r\n/* Set MOSI and SCK output /\r\nDDR_SPI |= (1<<DD_MOSI) | (1<<DD_SCK) | (1<<DD_SS);\r\nPORTB |= 1;\r\n/ Enable SPI, Master, set clock rate fck/128 */\r\nSPCR |= (1<<SPE)|(1<<MSTR)|(1<<SPR0)|(1<<SPR1);\r\n}\r\n\r\nunsigned char SPI_MasterTransmit(unsigned char cData)\r\n{\r\nPORTB &= ~(1<<0);\r\nwhile( (PORTB & (1<<0)) );\r\n/* Start transmission /\r\nSPDR = cData;\r\n/ Wait for transmission complete */\r\nwhile(!(SPSR & (1<<SPIF)));\r\nPORTB |= 1;\r\nreturn SPDR;\r\n}\r\n\r\nvoid ADC_Init()\r\n{\r\n//ADC enable\r\nADCSRA |= (1<<ADEN);\r\n//division factor --> 128\r\nADCSRA |= (1<<ADPS0) | (1<<ADPS1) | (1<<ADPS2);\r\n//left adjusted (8 bit mode) (ADCH)\r\nADMUX |= (1<<ADLAR);\r\n//input channel --> ADC2 (only one channel can be used in the conversion at a time)\r\nADMUX |= (1<<MUX1);\r\n}\r\n\r\nunsigned char ADC_StartConversion()\r\n{\r\nADCSRA |= (1 << ADSC);\r\nwhile(ADCSRA & (1<<ADSC));\r\nreturn ADCH;\r\n}\r\n\r\nint main(void)\r\n{\r\nDDRC = 1;\r\nADC_Init();\r\nSPI_MasterInit();\r\nunsigned char data;\r\nunsigned char ret;\r\n\r\nwhile (1) \r\n{\r\n\tdata = ADC_StartConversion();\r\n\tret = SPI_MasterTransmit(data);\r\n\t\r\n\tif(ret == ACK)\r\n\t{\r\n\t\tPORTC = 1;\r\n\t}\r\n\t\r\n\telse\r\n\t{\r\n\t\tPORTC = 0;\r\n\t}\r\n\t\r\n}\r\n}\r\n\r\nCode of SLAVE\r\n\r\n#define F_CPU 16000000UL\r\n\r\n#include <avr/io.h>\r\n#include <util/delay.h>\r\n\r\n#define DDR_SPI DDRB\r\n#define DD_MISO 3\r\n#define LED PC0\r\n\r\n#define ACK 0x7E\r\n\r\nvoid SPI_SlaveInit(void)\r\n{\r\n/* Set MISO output, all others input /\r\nDDR_SPI = (1<<DD_MISO);\r\n/ Enable SPI */\r\nSPCR = (1<<SPE);\r\n}\r\n\r\nunsigned char SPI_SlaveReceive(char data)\r\n{\r\nSPDR = data;\r\n/* Wait for reception complete /\r\nwhile(!(SPSR & (1<<SPIF)));\r\n/ Return data register */\r\nreturn SPDR;\r\n}\r\n\r\nint main(void)\r\n{\r\nDDRC |= (1<<LED);\r\nSPI_SlaveInit();\r\nunsigned char data;\r\n\r\nwhile (1) \r\n{\t\r\n\tdata = SPI_SlaveReceive(ACK);\r\n\tif(data >= 128)\r\n\t{\r\n\t\tPORTC |= (1<<LED);\r\n\t}\r\n\telse\r\n\t{\r\n\t\tPORTC &= ~(1<<LED);\r\n\t}\r\n\t//_delay_ms(100);\t\r\n}\r\n}", "comments": ["@mhmedeng \r\nI cannot see you using tensorflow in the code shared, or any issue cause due to tensorflow, please provide acolab gist with error faced due to tensor flow for us to help you.\r\nPlease update the issue template with the tensorflow version used.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42282\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42282\">No</a>\n"]}, {"number": 42281, "title": "tf.nn.ctc_beam_search_decoder crashes(bad_alloc) when top_paths is large", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.nn.ctc_beam_search_decoder` crashes(bad_alloc) when `top_paths` is extremely large\r\n\r\n**Describe the expected behavior**\r\nExpect no crashes\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\nimport tensorflow as tf\r\ntf.nn.ctc_beam_search_decoder(inputs=tf.ones((1,1,1)), sequence_length=[1], top_paths=1000000000000)\r\n~~~\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n~~~python\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n~~~\r\n", "comments": ["I have tried in colab with TF version 2.1,2.3,nightly versions(`2.4.0-dev20200812`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2969447f67d86720918d264620763d03/untitled247.ipynb).Thanks!", "Depending on the version of TF i use, I get different errors.\r\n\r\nInternally; I get the error\r\n```ValueError: Attr top_paths has value 1000000000000 out of range for an int32```\r\n\r\nusing python3.6; which I think is the real expected error.\r\n\r\nIn OSS ipython3 I get the error:\r\n\r\n```MemoryError: std::bad_alloc```\r\n\r\nat the line\r\n\r\n```\r\n--> 118   _result = _execute.execute(b\"CTCBeamSearchDecoder\", top_paths + top_paths +\r\n    119                              top_paths + 1, inputs=_inputs_flat, attrs=_attrs,\r\n    120                              ctx=ctx, name=name)\r\n```\r\n\r\n(eager execution)", "@alextp who should take this over?  looks like an error in attr size validation?  or perhaps we're allowing an int64 through when it should be an int32; and then differences in tcmalloc vs. some other malloc externally?", "The internal error is correctly identified here, I think:\r\n\r\n```\r\n.../tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   2003         op_def = self._graph._get_op_def(node_def.op)\r\n   2004       self._c_op = _create_c_op(self._graph, node_def, inputs,\r\n-> 2005                                 control_input_ops, op_def)\r\n   2006       name = compat.as_str(node_def.name)\r\n   2007     # pylint: enable=protected-access\r\n\r\n.../tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)\r\n   1843   except errors.InvalidArgumentError as e:\r\n   1844     # Convert to ValueError for backwards compatibility.\r\n-> 1845     raise ValueError(str(e))\r\n   1846 \r\n   1847   return c_op\r\n\r\nValueError: Attr top_paths has value 1000000000000 out of range for an int32\r\n```", "@rohan100jain can you help triage this? The error seems to be in the TF C API layer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42281\">No</a>\n"]}, {"number": 42279, "title": "tf.keras.backend.ctc_decode crashes(bad_alloc) when top_paths is large", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.backend.ctc_decode` crashes(abort, bad_alloc) when `top_paths` is large and greedy is `False`\r\n\r\n**Describe the expected behavior**\r\nExpect no crashes\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\n\r\nimport tensorflow as tf\r\ntf.keras.backend.ctc_decode(y_pred=tf.ones((1,1,1)), input_length=1, top_paths=1000000000, greedy=False)\r\n\r\n~~~\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n~~~\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n~~~", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7fb6ad311e6cbd3899fb94cc658f1b35/untitled367.ipynb).", "Hi @DNXie, I've just tested this in nightly and I now see `ValueError: Number of outputs is too big: 3000000001`", "@nikitamaia Yes. It seems to be fixed in nightly version. I will close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42279\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42279\">No</a>\n"]}, {"number": 42278, "title": "tf.nn.fractional_max_pool aborts when pooling_ratio is negative", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.nn.fractional_max_pool` aborts when `pooling_ratio` is negative in tf2.1. It throws segfault in nightly version.\r\n\r\n**Describe the expected behavior**\r\nExpect no crashes or aborts. \r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\nimport tensorflow as tf\r\ntf.nn.fractional_max_pool(value=tf.ones((1,1,1,1)), seed=1, pooling_ratio=-1)\r\n~~~\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nError message in TF2.1:\r\n~~~python\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n~~~\r\nError message in TF nightly:\r\n~~~\r\nSegmentation fault (core dumped)\r\n~~~", "comments": ["@DNXie \r\n\r\nThis is intended behavior.`Pooling ratio for each dimension of value, currently only supports row and col dimension and should be >= 1.0.`.\r\nPlease, refer the document [here](https://www.tensorflow.org/api_docs/python/tf/nn/fractional_max_pool#args).I i take value pooling ration =1 i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a37924989af91984416d6043aa64f0d2/untitled249.ipynb).Thanks!", "Hi, I would like to work on this can anyone help me with how it should behave with `pooling ratio<1` as I am new in this.", "@ravikyram @aavishkarmishra  It should throw an exception instead of crash. ", "So, mainly test pooling ratio to be>=1 and throw error if it is not.\r\n", "@aavishkarmishra I am not an expert, but some exception message thrown in python level would be good. ", "I ran the code on nightly [2.4.0-dev20200916] and the issue still persist. please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ecc94688daba6d6f4f7a73ce8923ae0d/untitled415.ipynb).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ravikyram This issue is resolved after the merge of pull request please close this issue.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42278\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42278\">No</a>\n"]}, {"number": 42276, "title": "tensorflow lite native app, undefined reference to `NnApiImplementation()'", "body": "I am getting the following error on these line:\r\n\r\n```\r\nuint32_t device_count = 0;\r\nauto nnapi_result = NnApiImplementation()->ANeuralNetworks_getDeviceCount(&device_count);\r\n```\r\n.../benchmark_jni.cpp:413: undefined reference to `NnApiImplementation()'\r\n\r\nNnApiImplementation() is referenced in nnapi_implementation.h:\r\n```\r\n/**\r\n * Load the NNAPI implementation from the shared libraries.\r\n * The NnApi structure is filled with all the pointers. If one function doesn't\r\n * exist, a null pointer is stored.\r\n */\r\nconst NnApi* NnApiImplementation();\r\n```\r\n\r\nI had previously built the arm64-v8a shared library as illustrated in this guide: \r\nhttps://www.tensorflow.org/lite/guide/android#build_in_android_studio \r\n\r\nusing the docker image method and finally building with this command:\r\n\r\n`bazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so`\r\n\r\nHowever when I try to find the NnApiImplementation in the .so file using:\r\n`nm -gDC libtensorflowlite.so | grep \"NnApiImplementation\"\r\n`\r\nNothing is returned. \r\n\r\n", "comments": ["@Saduf2019 can you put the lite, and build/install tags on this, its been 2 days", "@amitDaMan \r\nCan you please fill/update the issue template with tf version, steps before error was faced, simple stand alone code if applicable and system information for us to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Saduf2019 \r\nI do not think that this problem is related to my particular usecase and is really about whether the proper nnapi functions are packaged in the shared library.\r\n\r\nHowever, to answer your question. \r\nThere is a machine learning test suite located in AOSP\r\nhttps://android.googlesource.com/platform/test/mlts/\r\n\r\nAnd it is built in soong and make.\r\n\r\nAnd I am trying to build it in gradle and cmake to work with android studio. \r\n\r\nHowever MLTS uses the C++ api of tensorflow lite, which means I need to build the C++ shared libraries:\r\n\r\nusing: \r\nbazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so\r\n\r\nas described:\r\nhttps://www.tensorflow.org/lite/guide/android#build_in_android_studio", "*ping*", "NnApiImplementation() is a TFLite internal function for NNAPI wrapping. It's not for external use.\r\nIf you want to use NNAPI, you can use it directly instead of calling it via TFLite internal functions.\r\nhttps://developer.android.com/ndk/reference/group/neural-networks#aneuralnetworks_getdevicecount", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42276\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42276\">No</a>\n"]}, {"number": 42275, "title": "Fix broken compilation", "body": "On ESP32, the -std=c11 flags are also passed to .cc files for some reason, causing the compiler to throw an error.\r\nI am not quite sure if the `CFLAGS` there are required though.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42275) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42275) for more info**.\n\n<!-- ok -->"]}, {"number": 42274, "title": "Add log to s3 filesystem", "body": "@mihaimaruseac \r\nThis PR adds some loggings to s3 filesystem. I will convert `Aws::Utils::Logging::LogSystemInterface` to `TF_Log` in the next PR.", "comments": []}, {"number": 42273, "title": "Add Init test hadoop", "body": "@mihaimaruseac \r\nThis PR adds initialization for testing hadoop filesystem", "comments": ["@mihaimaruseac \r\nHow can I test the integration of modular filesystem with Tensorflow to ensure that there isn't any ODR violation on Windows", "I would build the pip package and the plugins and then from Python I would use `tf.load_library` to load the plugin DLLs and then issue a few filesystem calls.\r\n\r\nOr I would use the test suite and pass the plugins as arguments but I think the test suite is still not working on Windows.", "> I would build the pip package and the plugins and then from Python I would use tf.load_library to load the plugin DLLs and then issue a few filesystem calls.\r\n\r\nI tried this method but the filesystem call does not work. It keeps throw `Unimplemented`\r\n\r\n> Or I would use the test suite and pass the plugins as arguments but I think the test suite is still not working on Windows.\r\n\r\nI think the main cause of ODR is happened when we link against pip package ( `pywrap_tensorflow_internal` )  so this approach isn't what we need.", "Hmm, I would add prints to the filesystem registration routines and debug what happen during registration and during lookup. Something seems wrong.", "AFAIK, we have to issue a few filesystem calls using `modular_filesystem`, but `modular_filesystem` isn't included anywhere outside `//tensorflow/c/experimental`. Where should I add `modular_filesystem` as a dependency inside the pip package so I can get it work as a normal filesystem ?", "That shouldn't be the case. [These lines](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/modular_filesystem_registration.cc;l=261-270;drc=e777af90e878cb3618e583bc1b8ec796903ccbdb) register the filesystem.\r\n\r\nMaybe we need adding it as a dependency to https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/BUILD;l=222-231;drc=dcbf62e6b2588c83c3b0fdfcb1fe101eb2afae47 ?", "When I use `tf.load_library(\"libgcs_filesystem.so\")` in TF 2.3.0 on Linux, it throws \r\n`\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/vovannghia56789/.cache/bazel/_bazel_vovannghia56789/203e9f31192603e9d3065c5e6101525c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/experimental/filesystem/plugins/hadoop/../../../../../../_solib_k8/libtensorflow_Sc_Slibtf_Ustatus.so: undefined symbol: _ZN10tensorflow7IOErrorERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEi\r\n`\r\n\r\nIt cannot find the symbol of this function here.\r\nhttps://github.com/tensorflow/tensorflow/blob/7e703f509409d00f99e9c87f69909c2808cc1142/tensorflow/c/tf_status.cc#L41\r\n\r\nThis error is thrown when loading `posix`\r\n`tensorflow.python.framework.errors_impl.NotFoundError: /home/vovannghia56789/.cache/bazel/_bazel_vovannghia56789/203e9f31192603e9d3065c5e6101525c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/experimental/filesystem/plugins/posix/../../../../../../_solib_k8/libtensorflow_Sc_Slibtf_Ustatus.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl14lts_2020_02_2511string_viewEOSt6vectorINS_10StackFrameESaIS7_EE`\r\n", "Both look like C++ symbols somehow got used in the interface. I'll have to debug this but it's unlikely I'll be able to this week.\r\n\r\nDid you compile both plugin and TF code with the same toolchain?", "I compiled both plugin and TF code on the same machine so I am quite sure it is the same toolchain", "@mihaimaruseac, @vnvo2409 Any update on this PR? Please. Thanks!", "@mihaimaruseac \r\nCould you debug the plugins and merge this PR in the next week ? Thank you"]}, {"number": 42272, "title": "checkpoint.restore  can't work", "body": "version: tensorflow2.2  2.3\r\n\r\n```python\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(MyModel, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.dense = tf.keras.layers.Dense(2)\r\n\r\n    def call(self, inputs):\r\n        out = self.dense(inputs)\r\n        return out\r\n\r\nif __name__ == '__main__':\r\n    data = tf.constant(tf.ones((10, 20)))\r\n\r\n    tag = tf.constant(tf.ones((10)))\r\n    testmodel = MyModel()\r\n    optimizer = tf.keras.optimizers.Adam()\r\n    checkpoint = tf.train.Checkpoint(tmodel=testmodel)\r\n    with tf.GradientTape() as tape:\r\n        out = testmodel(data)\r\n        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(tag, out))\r\n    grad = tape.gradient(loss, testmodel.trainable_variables)\r\n    optimizer.apply_gradients(zip(grad, testmodel.trainable_variables))\r\n\r\n    checkpoint.save(\"test/model.ckpt\")\r\n```\r\n\r\nThen I load the model. It reports an error\r\n\r\n```python\r\nif __name__ == '__main__':\r\n    testmodel = MyModel()\r\n    checkpoint = tf.train.Checkpoint(tmodel=testmodel)\r\n    checkpoint.restore(tf.train.latest_checkpoint('test'))\r\n```\r\n It reports an error.\r\n\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).tmodel.dense\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).tmodel.dense.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).tmodel.dense.bias\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n\r\nHow to fix it?\r\n\r\nand in here ,also have this error  https://www.tensorflow.org/tutorials/keras/save_and_load?hl=zh-cn\r\n![image](https://user-images.githubusercontent.com/19524544/90044790-be42aa80-dd00-11ea-9c2c-c937d5a29b27.png)\r\n\r\n\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42272\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42272\">No</a>\n", "oh, `checkpoint.restore` need  the model load data, then it will work. \r\n\r\nBut , the picture still have some Compile bug\uff0cthe doc needs to renew.\r\n"]}]