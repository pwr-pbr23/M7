[{"number": 818, "title": "fix docs of 'Saver.max_to_keep'", "body": "", "comments": ["Can one of the admins verify this patch?\n", "LGTM\n"]}, {"number": 817, "title": "crosstool_wrapper_driver_is_not_gcc failed: error executing command", "body": "I am using the master branch of tensorflow and also build bazel master \n/home/xxxxx/bazel/output/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\n\nERROR: /home/xxxxx/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/external/gemmlowp/BUILD:77:1: C++ compilation of rule '@gemmlowp//:eight_bit_int_gemm' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/lli/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/tensorflow && \\\n  exec env - \\\n    PATH='~/.local/bin:~/.local/bin:~/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/lli/bin:/home/lli/bin:/home/lli/bin' \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/gemmlowp -iquote bazel-out/local_linux-opt/genfiles/external/gemmlowp -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o' -MD -MF bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.d -c external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.cc -o bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o): crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/lli/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/tensorflow && \\\n  exec env - \\\n    PATH='~/.local/bin:~/.local/bin:~/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/lli/bin:/home/lli/bin:/home/lli/bin' \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/gemmlowp -iquote bazel-out/local_linux-opt/genfiles/external/gemmlowp -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o' -MD -MF bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.d -c external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.cc -o bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o).\nsrc/main/tools/namespace-sandbox.c:645: execvp(argv[0], argv): No such file or directory\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n", "comments": ["Can you try \n\n```\nbazel build -c opt config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/cc:tutorials_example_trainer\n```\n\nand if that doesn't work, post the entire logs?\n", "/home/lli/bazel/output/bazel build -c opt --config=cuda --spawn_strategy=standalone //tensorflow/cc:tutorials_example_trainer\nINFO: Found 1 target...\nERROR: /home/lli/tensorflow/google/protobuf/BUILD:270:1: Linking of rule '//google/protobuf:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/google/protobuf/protoc ... (remaining 15 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: bazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o: relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making a shared object; recompile with -fPIC\nbazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o: error adding symbols: Bad value\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 23.745s, Critical Path: 3.69s\n", "@lberki @davidxchen @damienmg\n", "`/home/lli/bazel/output/bazel build -c opt --config=cuda --spawn_strategy=standalone --verbose_failure //tensorflow/cc:tutorials_example_trainer`\n\nThen you will get the failing command completely.\n\nMaybe try a `bazel clean --expunge` first\n", "btw, @davidzchen is the one you wanted to ping (with a z not a x)\n", "I tried but no luck. BTW, I can't use verbose_failure with my built bazel. Unrecognized option: --verbose_failure\nlli@mobvoi-rhea-01:~/jdk1.8.0_65$ /home/lli/bazel/output/bazel clean --expunge\nThe 'clean' command is only supported from within a workspace.\nlli@mobvoi-rhea-01:~/jdk1.8.0_65$ cd \nlli@mobvoi-rhea-01:~$ cd tensorflow/\nlli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel clean --expunge\n................\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nlli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel build -c opt --config=cuda --spawn_strategy=standalone --verbose_failure //tensorflow/cc:tutorials_example_trainer\n.............\nUnrecognized option: --verbose_failure\nlli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel build -c opt --config=cuda --spawn_strategy=standalone //tensorflow/cc:tutorials_example_trainer\nINFO: Found 1 target...\nERROR: /home/lli/tensorflow/google/protobuf/BUILD:270:1: Linking of rule '//google/protobuf:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/google/protobuf/protoc ... (remaining 15 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: bazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o: relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making a shared object; recompile with -fPIC\nbazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o: error adding symbols: Bad value\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 166.539s, Critical Path: 18.48s\n", "My bad the correct option is --verbose_failures\n\nOn Fri, Jan 22, 2016, 8:53 AM LiLi notifications@github.com wrote:\n\n> I tried but no luck. BTW, I can't use verbose_failure with my built bazel.\n> Unrecognized option: --verbose_failure\n> lli@mobvoi-rhea-01:~/jdk1.8.0_65$ /home/lli/bazel/output/bazel clean\n> --expunge\n> The 'clean' command is only supported from within a workspace.\n> lli@mobvoi-rhea-01:~/jdk1.8.0_65$ cd\n> lli@mobvoi-rhea-01:~$ cd tensorflow/\n> lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel clean\n> --expunge\n> ................\n> INFO: Starting clean (this may take a while). Consider using\n> --expunge_async if the clean takes more than several minutes.\n> lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel build -c\n> opt --config=cuda --spawn_strategy=standalone --verbose_failure\n> //tensorflow/cc:tutorials_example_trainer\n> .............\n> Unrecognized option: --verbose_failure\n> lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel build -c\n> opt --config=cuda --spawn_strategy=standalone\n> //tensorflow/cc:tutorials_example_trainer\n> INFO: Found 1 target...\n> ERROR: /home/lli/tensorflow/google/protobuf/BUILD:270:1: Linking of rule\n> '//google/protobuf:protoc' failed: crosstool_wrapper_driver_is_not_gcc\n> failed: error executing command\n> third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o\n> bazel-out/host/bin/google/protobuf/protoc ... (remaining 15 argument(s)\n> skipped): com.google.devtools.build.lib.shell.BadExitStatusException:\n> Process exited with status 1.\n> /usr/bin/ld:\n> bazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o:\n> relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making\n> a shared object; recompile with -fPIC\n> bazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o:\n> error adding symbols: Bad value\n> collect2: error: ld returned 1 exit status\n> Target //tensorflow/cc:tutorials_example_trainer failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> INFO: Elapsed time: 166.539s, Critical Path: 18.48s\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/817#issuecomment-173837062\n> .\n", "lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel build -c opt --config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/cc:tutorials_example_trainer\n.............\nINFO: Found 1 target...\nERROR: /home/lli/tensorflow/google/protobuf/BUILD:270:1: Linking of rule '//google/protobuf:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/lli/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/google/protobuf/protoc bazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/google/protobuf/libprotoc_lib.a bazel-out/host/bin/google/protobuf/libprotobuf.a bazel-out/host/bin/google/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/lli/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/google/protobuf/protoc bazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/google/protobuf/libprotoc_lib.a bazel-out/host/bin/google/protobuf/libprotobuf.a bazel-out/host/bin/google/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: bazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o: relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making a shared object; recompile with -fPIC\nbazel-out/host/bin/google/protobuf/_objs/protoc/google/protobuf/src/google/protobuf/compiler/main.o: error adding symbols: Bad value\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 16.794s, Critical Path: 5.31s\n", "relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making a shared object; recompile with -fPIC\n\nAdding -fPIC to the options in the cuda crosstool might do the trick. Sorry I cannot do much here, it's a C++ related issue, not really related to bazel\n", "@damienmg \nso which file should I modify and how?\n", "Try adding to https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL#L89\n", "thanks, it works.\nI have successfully built cifar10 by /home/lli/bazel/output/bazel build -c opt --config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/models/image/cifar10:cifar10\nbut how can I run it?\n$python cifar10_train.py --data_dir=/home/lli/cifar10_data\nTraceback (most recent call last):\n  File \"cifar10_train.py\", line 43, in <module>\n    import tensorflow.python.platform\nImportError: No module named tensorflow.python.platform\nHow can I build all thing like it's installed by pip?\n", "Is it fixed because we have the similar problem? @fancyerii \n", "The same problem\n"]}, {"number": 816, "title": "Can tensorflow run on freescale ubuntu system?", "body": "I have one board called udoo neo (from www.udoo.org) ,which run ubuntu system .\nI want to port tensorflow on it. How to do it step by step?\n", "comments": ["For step by step instructions, stackoverflow is probably the better forum.\nBut if the board runs Ubuntu, you should be able to follow the regular\ninstallation instructions.\nOn Tue, Jan 19, 2016 at 20:41 david wang wei notifications@github.com\nwrote:\n\n> I have one board called udoo neo (from www.udoo.org) ,which run ubuntu\n> system .\n> I want to port tensorflow on it. How to do it step by step?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/816.\n", "Thanks for your answer!\n", "Hi. Did you get Tensorflow to work on the udoo neo?"]}, {"number": 815, "title": "ci_build updates", "body": "- cleanup some warnings and errors when runnign the container in interactive mode\n- add python3 support\n", "comments": ["merged.\n"]}, {"number": 814, "title": "Improved the performance of the contrast adjustment code", "body": "Improved the performance of the contrast adjustment code by enabling nvcc to optimize the broadcast of scalars\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please\n", "I think @martinwicke was playing around with jenkins, giving it another shot:\n\n@tensorflow-jenkins, test this please\n", "@tensorflow-jenkins: test this please\n", "LG, merging.\n"]}, {"number": 813, "title": "Cumulative sum (cumsum) op needed", "body": "C.f. [`numpy.cumsum`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.cumsum.html)\n", "comments": ["You can achieve what you're looking for by padding your input with zeros and then convolving the padded tensor with ones. For example, if you have a vector of N elements, you can start by padding it with N-1 elements on the left, and the launch a 1D convolution with a kernel of size N.\n\nOf course that won't be very efficient, but that can get you going for the time being.\n", "I'd like to learn how to write ops by implementing this.\nI'll work based on the [adding_an_op](https://www.tensorflow.org/versions/0.6.0/how_tos/adding_an_op/index.html) section in the docs.\n\nCan someone give me a few pointers on where this should end up in the tensorflow source tree?\nThe op itself would probably be registered in [`core/ops/math_ops.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc).\nShould I put the kernel into  a new `core/kernels/cumsum_op.cc` file?\n", "If we do this, it should probably have a name pattern similar to `reduce_sum`, `reduce_prod`, etc.  Maybe `accumulate_sum`, `accumulate_prod` ([somewhat following numpy](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ufunc.accumulate.html))?  @dave-andersen: Do you have opinions here?\n\nAlso, I'm not sure if it should be inclusive or exclusive.\n\nOnce we figure that out: yes, the op definitions would go in `core/ops/math_ops.cc`, and the kernel files would have a similar pattern to the reduction ops in `core/kernels`.\n", "I did the following in a personal project. It is not very efficient but it's probably a little better than the convolution.\n\nhttps://gist.github.com/leconteur/82f968f483ddd2234a03\n", "Hello ibab.  Have you been able to make any progress on this?  Thanks!\n", "Hi, I haven't made a lot of progress yet, but I've spent some time understanding the implementation of the `reduce` kernels.\nWriting `accumulate` kernels should be a bit more work, as there's no corresponding function in Eigen that we can delegate the operation to.\n", "@benoitsteiner: Does eigen have accumulate (scan) capability? \n", "I've needed `cumsum` and `cumprod` ops today and wrote the following based on @leconteur's code above:\n\n``` python\nimport tensorflow as tf\n\ndef cumsum(xs):\n    values = tf.unpack(xs)\n    out = []\n    prev = tf.zeros_like(values[0])\n    for val in values:\n        s = prev + val\n        out.append(s)\n        prev = s\n    result = tf.pack(out)\n    return result\n```\n\nThis seems to give me relatively decent speed:\n\n``` python\ns = tf.Session()\nX = np.random.uniform(0, 1, size=(10000, 1000))\nv = tf.Variable(X)\ns.run(tf.initialize_all_variables())\ny = cumsum(v)\n\n [2]: %timeit s.run(y)\n10 loops, best of 3: 110 ms per loop\n [3]: %timeit np.cumsum(X, axis=0)\n10 loops, best of 3: 80.3 ms per loop\n```\n\nIt would still be nice to have dedicated (and parallel?) ops for this, though.\nMaybe @benoitsteiner can comment on whether adding an `accumulate` implementation to Eigen would make sense. I might have time to contribute that.\n", "I don't know the tensorflow internals, but it seems to me that this would be very slow.  Your for loop adds 10,000 nodes to the graph, and the cumsum evaluation requires evaluation of 10,000 separate nodes, rather than one node that calls into an optimized loop.  Each node adds two length 1000 vectors, and this is fast, but I don't imagine iterating through 10,000 nodes is fast.  I must be misunderstanding, how?\n", "I've changed the shape to `(10000,)` to isolate the overhead from calling that many nodes a bit better.\nResults:\n\n```\n[2]: %timeit s.run(y)\n10 loops, best of 3: 36.6 ms per loop\n [3]: %timeit np.cumsum(X, axis=0)\nThe slowest run took 8.94 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000 loops, best of 3: 28.2 \u00b5s per loop\n```\n\nSo it seems that although there's considerable overhead, it's smaller than the time required to add the length `1000` vectors above.\n", "@ibab @girving Eigen currently doesn't provide the functionality needed to efficiently implement an  accumulate op. You're more than welcome to contribute the code to Eigen, that would be a very useful extension.\n", "Thanks, @benoitsteiner!\nI assume that I could structure it in a similar way to [`TensorReduction.h`](https://bitbucket.org/eigen/eigen/src/59859fde3bc7173175087ed5d002389b4e76876f/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h?at=default&fileviewer=file-view-default), except that `accumulate` would preserve the original shape of the tensor.\nThis might turn out to be too difficult for me, but I'll give it a try over the next few weeks.\n", "Does this article help?\nhttp://http.developer.nvidia.com/GPUGems3/gpugems3_ch39.html\n", "@ibab The TensorReduction.h code is very optimized and therefore pretty complex. A simpler operation that you could use as a template is TensorReverse.h (which reverse the order of the coefficients along one or more dimensions) or TensorShuffling.h (which reorders the dimension - this is a transposition in the 2d case)\n", "I've submitted a [PR to the Eigen repo](https://bitbucket.org/eigen/eigen/pull-requests/189/add-scan-op-to-tensor-module) containing a simple single-threaded implementation of a `TensorScanOp`, which has just been merged.\nIdeally I'd like to extend this with thread-based and a CUDA implementations using an approach like in the article that @chemelnucfin quoted above.\nLet's see if I'll have enough time to do it.\nFailing that, I think that adding TensorFlow ops that use the naive implementation would still be useful.\n", "@benoitsteiner: How does the merging / syncing process work with external Eigen changes?  Do we have to wait for any period of time before using them in TensorFlow?\n", "@girving We can update the bazel build and workspace files to pull the latest version of Eigen any time. I typically wait for about one day before doing so though, since that gives me a change to check that the Eigen nightly regressions tests are clean before updating TensorFlow.\n", "The product version of this would be useful to fix https://github.com/tensorflow/tensorflow/issues/2641.\n", "This has been fixed by #2711, so this issue can be closed.\n", "Yup, thanks!!\n", "As a potential alternative, we have implemented a python-only framework for adding custom operators to tensorflow, the Operator Vectorization Library: \nhttps://github.com/opveclib/opveclib. Multi-threaded C++ and CUDA code is automatically generated from the python operators, which can be used in a Tensorflow application with only a binary install of tensorflow. We have implemented cumsum and cumprod operators as an example and benchmarks against the new Tensorflow/Eigen scan operators on the original example in this issue are favorable - test code at: https://github.com/opveclib/opveclib/blob/master/opveclib/examples/test_accumulate.py\n\ninput: X = np.random.uniform(0, 1, size=(10000,1000))\nMachine: 48 CPU cores\nGPU - GeForce GTX TITAN\nDEBUG:opveclib:Best numpy time (ms): 148.728299141\nDEBUG:opveclib:Best ovl cpp time (ms): 41.827047\nDEBUG:opveclib:Best ovl cuda time  (ms): 4.738103\nDEBUG:opveclib:Best tf + ovl time  (ms) on /cpu:0 :62.8859996796\nDEBUG:opveclib:Best tf cumsum time  (ms) on /cpu:0 :141.862869263\nDEBUG:opveclib:Best tf + ovl time  (ms) on /gpu:0 :4.96506690979\nDEBUG:opveclib:Best tf cumsum time  (ms) on /gpu:0 :9.82308387756\n", "opveclib is very cool!\nNote that both the CPU and GPU implementation that I've added to Eigen are extremely naive.\nI've been planning to add optimized implementations, but haven't managed to get around to it yet.\nIt would be interesting to test the performance again with the optimized implementations.\n"]}, {"number": 812, "title": "Feature suggestion: expose weights for RNNCell", "body": "I realize that the API for RNNs hasn't been finalized yet but eventually it would be nice if the weights of the RNNCell are exposed so that they can be tracked using the summary ops like `histogram_summary`. Maybe just a property that returns all the internal weights.\n", "comments": ["You can already retrieve the weights easily with variable scope, e.g.:\n\n```\ncell = RNNCell(...)\nwith tf.variable_scope(\"RNN\") as vs:\n  # use cell here.\n  for var in tf.trainable_variables():\n    if var.name.startswith(vs.name):\n      print var.name, var.get_shape()\n      tf.histogram_summary(var.name, var)\n```\n", "This strikes me as a very hacky solution.\n", "If you just need histogram summaries for all the variables, then you don't need to use it in any specific variable scopes:\n\n```\nfor var in tf.trainable_variables():\n  tf.histogram_summary(var.name, var)\n```\n", "That's a nice way to track everything, thanks. My initial point wasn't that it's impossible, but that it would be useful to provide a convenient mechanism to get at the variables for a specific cell.\n", "Yes, I understand. For common use cases this doesn't seem to be needed .. and you can always use variable scope to get specific variables, like: `rnn_matrix = tf.get_variable(vs.name + \"/Matrix\")` but you need to be in the proper scope and know the names.\n\nWith the current design, the rnn cells are not associated with any specific variables (and are stateless) unless in a specific variable scope so having properties to keep references to them sounds hacky as well.\n", "Ok. Thanks for clarifying.\n", "Two years later now. Is there a better solution or should we still use the code in @rafaljozefowicz 's example?", "I'm looking for a better solution to this as well", "In tensorboard, if you use the LSTMCell class of tensorflow you can visualize the kernel variable in tensorboard. \r\nYou can instead define your weights in the call method. By saying weights I mean the W_xh and W_hh.\r\n\r\n```\r\nW_xh = tf.get_variable('W_xh',\r\n\t\t\t\t[x_size, 4 * self.num_units], initializer=w_init)\r\n\r\nW_hh = tf.get_variable('W_hh_i',\r\n\t\t\t\t[self.num_units, 4*self.num_units], initializer=h_init)\r\n\r\n```\r\n\r\nand then during training, you add \r\n\r\n```\r\nfor var in tf.trainable_variables():\r\n  tf.histogram_summary(var.name, var)\r\n```"]}, {"number": 811, "title": "relu_layer doesn't appear in tensorflow.org/versions/master/api_docs", "body": "relu_layer has a docstring in the code, but it doesn\u2019t appear in the tensorflow online documentation for some reason.\n\nIn case it is not public yet, it should be removed from the cifar example.\n", "comments": ["I'm not sure whether it is supposed to be public or not.\n", "It is not supposed to be public, and it will likely be deprecated (and\nremoved) in favor of layers.fully_connected_relu once that's matured.\n\nI agree using unstable functions in the tutorials is suboptimal, and since\nrelu_layer is so simple, a simplified version of it should probably be\ninlined in cifar.\n\nOn Tue, Jan 19, 2016 at 11:44 AM josh11b notifications@github.com wrote:\n\n> I'm not sure whether it is supposed to be public or not.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/811#issuecomment-172964027\n> .\n", "What about xw_plus_b?\n\nI can make a PR replacing relu_layer and xw_plus_b with public ops in the tutorials. Is someone already working on it?\n\nIs there any other ops that should be replaced in the tutorials?\n", "Nobody is working on it, if you wrote a pull request, we'd be grateful for it.\n", "Looks like it was fixed in #828 \n"]}, {"number": 810, "title": "Error importing tensorflow", "body": "I had a problem with the cifar10 example. When running cifar10_eval.py I got: \n\"AttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore'\"\n\nI then saw #802 and tried to follow the instructions for \"Installing from sources\" on the website. Now I get a new error when trying to import tensorflow:\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n\nHow do I solve this? I'm using ubuntu 15.04.\n", "comments": ["1) You might go back to #802 and checking out the code at the 0.6.0 branch if you have installed the pip package at 0.6.0.\n\n2) As the instructions say, you can't import tensorflow when running in the root of the source tree.  Go to another directory and you should be able to import it.\n\nLet us know if you have any problems and we'll reopen this ticket.\n", "I must have missed to uninstall something before. After uninstalling and redoing all the steps the source works. However, I now get a new error when running cifar10_eval.py. The training runs fine but during evaluation I get the output below when running the file. I get the same output if I use the pip installation and use the cifar10_eval.py from the 0.6.0 branch.\n\n2016-01-20 12:05:07.620594: precision @ 1 = 0.661\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e8c0015d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2ec00515c0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2eb00018d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2eb4001840 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2ea8001610 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e880016e0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e98001660 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2ec806eae0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2eb80018e0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e780015d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e90001c10 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e9c001b70 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2eac001970 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e800015d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2e940015d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2ebc04c1e0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1091] 0x7f2ea0001610 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:283] Skipping cancelled enqueue attempt\n"]}, {"number": 809, "title": "Remove mutable default value.", "body": "Up to now, it's not a problem since `unigrams` is always given.\nHowever, it might cause a problem in the future because\na mutable default parameter value is created once and shared.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Geoffrey, assigning this to you since you last touched this line.  It looks good to me, though.\n", "@tensorflow-jenkins: test this please.\n\n( @josh11b, you don't have access to kick off jenkins.  @martinwicke).\n", "Please see the section \"4.7.1. Default Argument Values\". The following is the last example of that section.\n\n```\nIf you don\u2019t want the default to be shared between subsequent calls, you can write the function like this instead:\n\ndef f(a, L=None):\n    if L is None:\n        L = []\n    L.append(a)\n    return L\n```\n", "By the way, @girving , could you give me your reference about `the standard way`? I just want to update my knowledge about that.\n", "I don't have a reference in mind.  By the standard way, I mean that if we want to have default value `x`, you do something like\n\n```\ndef foo(arg=x):\n  ...\n```\n\nIf you want to have the default be immutable, and can choose `x` to be immutable (`()` in this case), the above works fine.  It seems clearly superior to your code, which adds extra lines and doesn't document the behavior in the function signature.  Mine even documents that `x` is not written to, since it's immutable.\n\nIn contrast, the example you're pulling from is only for when you really do want a `list` instead of a `tuple`, perhaps because you want mutability.  Here, as far as I know, a `tuple` would be fine.\n", "Thank you for your kind explanation, @girving . Now, I got your point. I'll update the PR.\nIf the unittests pass, I've no objection. Shorter is better.\n", "I've rebased and updated the code according to the comments. Thank you again.\n", "Jenkins, test this please.\n(Probably still won't work, but I can hope that I've been given permission by now.)\n", "@tensorflow-jenkins, test this please\n", "There is one failure, `coordicator_test`, in `Linux CPU Tests`. I've took a look at `coordinator_test.py`, but it seems irrelevant to this PR.\n\n```\n//tensorflow/python:coordinator_test                                     FAILED\nExecuted 289 out of 289 tests: 288 tests pass and 1 fails locally.\n```\n", "Thanks.\n", "Yes, the coordinator_test.py failure is not this PR.\n\nThank you @dongjoon-hyun!\n", "Thank you for confirming that, @jendap . :)\n"]}, {"number": 808, "title": "CUDA_ERROR_NO_DEVICE inside docker with GTX Titan X", "body": "Running `b.gcr.io/tensorflow/tensorflow:latest-gpu` having CUDA 7.0 installed on the host when I try to create the session it returns `CUDA_ERROR_NO_DEVICE` and `was unable to find libcuda.so DSO loaded into this program` but when the strange thing is that when the module is imported all the libraries are loaded correctly.\n#### Log:\n\n```\nroot@5b1e79697b49:~# python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\n>>> \n>>> with tf.Session() as sess:\n...   with tf.device(\"/gpu:0\"):\n...     matrix1 = tf.constant([[3., 3.]])\n...     matrix2 = tf.constant([[2.],[2.]])\n...     product = tf.matmul(matrix1, matrix2)\n...     sess.run(product)\n... \nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: 5b1e79697b49\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 5b1e79697b49\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  358.16  Mon Nov 16 19:25:55 PST 2015\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 358.16\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: \nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8\nTraceback (most recent call last):\n  File \"<stdin>\", line 6, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'Const_1': Could not satisfy explicit device specification '/gpu:0'\n         [[Node: Const_1 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [2,1] values: 2 2>, _device=\"/gpu:0\"]()]]\nCaused by op u'Const_1', defined at:\n  File \"<stdin>\", line 4, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 165, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\n>>> \n```\n#### Versions:\n- OS: `CentOS Linux elease.2.1511(Core)                                                                                                        \n  `\n- Kernel: `Linux localhost.localdomain 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux` \n- GPU: `NVIDIA Corporation GM200 [GeForce GTX TITAN X] (rev a1)`\n- Docker: `1.9.1`\n#### Env:\n\n```\nCUDA_HOME=/usr/local/cuda-7.0\nLD_LIBRARY_PATH=/usr/local/cuda-7.0/lib64:\n```\n#### Command:\n\n`docker run -it -v /usr/lib/x86_64-linux-gnu/libcudadevrt.a:/usr/lib/x86_64-linux-gnu/libcudadevrt.a -v /usr/lib/x86_64-linux-gnu/libcudart.so:/usr/lib/x86_64-linux-gnu/libcudart.so -v /usr/lib/x86_64-linux-gnu/libcudart.so.7.0:/usr/lib/x86_64-linux-gnu/libcudart.so.7.0 -v /usr/lib/x86_64-linux-gnu/libcudart.so.7.0.28:/usr/lib/x86_64-linux-gnu/libcudart.so.7.0.28 -v /usr/lib/x86_64-linux-gnu/libcudart_static.a:/usr/lib/x86_64-linux-gnu/libcudart_static.a -v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-modeset:/dev/nvidia-modeset b.gcr.io/tensorflow/tensorflow:latest-gpu`\n", "comments": ["@mcuadros, TF docker has trouble finding your Cuda driver, not the Cuda libraries. Could you check? \n- /usr/lib/x86_64-linux-gnu/libcuda.so does exist on your machine, and in the docerk container? \n- Please make sure that path is on your LD_LIBRARY_PATH as well\n- If still having trouble with the docerk container, could you also try without the docker container just to see if that works? \n\nThanks. \n", "@mcuadros, the images taged with `-gpu` are intended to be used with the [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) project, as you may well note the parent image they are built from, [`FROM nvidia/cuda:7.0-cudnn2-runtime`](https://github.com/tensorflow/tensorflow/blob/5abead8c434d5c99c0eb43385f833844eff55721/tensorflow/tools/docker/Dockerfile.gpu#L1).  Go check out [nvidia-docker wiki](https://github.com/NVIDIA/nvidia-docker/wiki) on how to get started and launch GPU enabeled containers. Cuda is already setup in the image, all your host needs is the nvidia driver, docker, and the nvidia-docker plugin. \n\nLooks like this wasn't yet made clear in the [README.md](https://github.com/tensorflow/tensorflow/blob/5abead8c434d5c99c0eb43385f833844eff55721/tensorflow/tools/docker/README.md), could a dev kindly touch that up?\n", "@ruffsl I will try to run it with nvidia-docker, to see if works. \n\nIn the [documentation](https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html#docker_install) always speaks about the normal docker-engine and also the old bash to run tf with docker was running the normal command.\n", "I tried to start this using `nvidia-docker` and it still does not work:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:93] Couldn't open CUDA library libcuda.so. LD_LIBRARY_PATH: /usr/local/cuda/lib64\n...\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:109] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n```\n", "Here are the debugging steps that I recently went through to get TensorFlow working (kind of) with GPU and Docker. (I say \"kind of\" because there are still some GPU-related bugs in Tensorflow, which caused some test failures and will likely cause some user-code errors in that regard as well). \n\nSee: \nhttps://github.com/tensorflow/tensorflow/issues/952\nhttps://github.com/tensorflow/tensorflow/issues/953\n\nThat said, here are the things you want to check on: \n\n1) On the host, outside Docker, you have NVIDIA driver installed. If you have it installed, the binaries \"nvidia-smi\" and \"nvidia-debugdump\" ought to be available on the host. Make sure that the following two commands list your GPU: \n`nvidia-smi`\n`nvidia-debugdump -l`\n\n2) On the host, the output of nvidia-smi tells you the version of the NVIDIA driver installed. It needs to be recent enough for your GPU. For example, on my machine, version 340 doesn't work, but version 352 does. \n\n3) On the host, get the CUDA sample code and compile the deviceQuery binary:\nhttp://docs.nvidia.com/cuda/cuda-samples/#axzz3z3C3lhk1\n\nFor this you'll need to install the CUDA toolkit, which includes the nvcc compiler and supporting libraries. \nhttps://developer.nvidia.com/cuda-downloads\n\nOnce the deviceQuery binary is compiled, try to run it\n`./deviceQuery`\n\nIf it fails, don't panic, just try\n`sudo ./deviceQuery`\n\nThere are some file permissions issues related to NVIDIA devices in /dev that requires you to do sudo like the above for each boot cycle, for detailed dicussion, see: \nhttps://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/\n\n4) After step 3, CUDA GPU is all set on the host. Let's now look inside Docker. Make sure that you install the same CUDA Toolkit as listed in step 3. TensorFlow additionally requires CUDA DNN (cudnn) libraries which you can also get from the CUDA website, after a somewhat time consuming user approval process. \n\nAfter this make sure that the following files are present in your docker, as they will be used by (the current version of) TensorFlow:\n\n```\n/usr/local/cuda/include/cudnn.h \n/usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so.6.5/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so/libcudnn.so.6.5\n```\n\nIt is okay for NVIDIA driver to be unavailable inside the Docker, even though the CUDA Toolkit is required inside it.\n\n5) Before you can try to start your Docker container, make sure you use docker flags to map a few devices so the the NVIDIA devices are available under /dev, inside the container:\n\n```\n\"--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl\"\n```\n\nAlso, map a bunch of lib files, to make sure that cuda library files are visible inside the container\n\n```\n\"-v /usr/lib/x86_64-linux-gnu/libcudadevrt.a:/usr/lib/x86_64-linux-gnu/libcudadevrt.a\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so:/usr/lib/x86_64-linux-gnu/libcudart.so\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22\" \"-v /usr/lib/x86_64-linux-gnu/libcudart_static.a:/usr/lib/x86_64-linux-gnu/libcudart_static.a\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.352.63:/usr/lib/x86_64-linux-gnu/libcuda.so.352.63\"\n```\n\nNow the NVIDIA docker container should be ready to run TensorFlow with GPU. \n", "@cancan101 , that's odd. Just did a fresh build of both cuda and tensorflow with [`Dockerfile.devel-gpu` ](https://github.com/tensorflow/tensorflow/blob/41671d980d1c0e517e588d217d0d7d63b430d03b/tensorflow/tools/docker/Dockerfile.devel-gpu) and then got the same error you got:\n\n```\n...\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:93] Couldn't open CUDA library libcuda.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 6816d202e2bd\n...\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1061] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\n```\n\nI checked to make sure the library was installed and ldconfig finds it just fine:\n\n``` command\nenv\n...\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nPATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nCUDA_PATH=/usr/local/cuda\n...\n\nldconfig -p | grep libcuda\n        libcudart.so.7.0 (libc6,x86-64) => /usr/local/cuda/lib64/libcudart.so.7.0\n        libcudart.so (libc6,x86-64) => /usr/local/cuda/lib64/libcudart.so\n        libcuda.so.1 (libc6,x86-64) => /usr/local/nvidia/lib64/libcuda.so.1\n        libcuda.so.1 (libc6) => /usr/local/nvidia/lib/libcuda.so.1\n```\n\nIs there a path missing somewhere?\n\nI even tried the old images up on [docker hub last repo](https://hub.docker.com/r/tensorflow/tensorflow/tags/) modified 2 months ago and the b.gcr.io updated recently by adding in the new nvidia meta label:\n\n``` Dockerfile\nFROM tensorflow/tensorflow:0.6.0-gpu # or b.gcr.io/tensorflow/tensorflow-devel-gpu\nLABEL com.nvidia.volumes.needed=\"nvidia_driver\"\n```\n\nfor nvidia-docker, but still got the same error.\n", "@ruffsl I suppose you used `nvidia-docker` given your paths.\n\nI quickly looked at the Tensorflow code, the [DsoLoader](https://github.com/tensorflow/tensorflow/blob/97f585d506cccc57dc98f234f4d5fcd824dd3c03/tensorflow/stream_executor/dso_loader.cc#L71-L75) tries to find `libcuda` in `third_party/gpus/cuda/driver/lib64`. Not sure how it is supposed to end up there, but `libcuda` is not part of the CUDA toolkit so you can't rely on the standard CUDA path to find it.\nIn the case of `nvidia-docker` it is mounted at runtime in `/usr/local/nvidia/`, you will need to add this location into the third_party search script.\n", "Pardon the noise, but pinging @vrv @craigcitro or @jendap , do you know what the intended setup for tensorflow cuda paths are? Or perhaps what [this feature](https://github.com/tensorflow/tensorflow/issues/20#issuecomment-178915032) is meant to address? I'm not sure how this `third_party` path is coming into play.\n", "@zheng-xq, any ideas here?\n", "also pinging @ebrevdo who may have hit this more recently.\n", "You have cuda_path set, maybe it should be CUDA_HOME?\n", "Also add this to ld library path:\n\n/usr/lib/x86_64-linux-gnu\n", "For the open-source bulid, I don't think the third_party/gpus path matters\nat all. libcuda.so needs to be found at the system path.\n\nLooking at\n\"tensorflow/tools/docker/docker_run_gpu.sh\",\"/usr/lib/x86_64-linux-gnu/libcuda\"\nneeds to be changed to the libcuda.so on the local system.\n\nOn Thu, Feb 4, 2016 at 9:38 PM, Craig Citro notifications@github.com\nwrote:\n\n> also pinging @ebrevdo https://github.com/ebrevdo who may have hit this\n> more recently.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/808#issuecomment-180207303\n> .\n", "The problem here is that Tensorflow dlopen `libcuda.so` instead of the soname `libcuda.so.1`.\n\n@ruffsl After reading the [code](https://github.com/tensorflow/tensorflow/blob/97f585d506cccc57dc98f234f4d5fcd824dd3c03/tensorflow/stream_executor/dso_loader.cc#L125-L176)  more thoroughly here is a workaround (don't ask me why):\n`ln -s /usr/local/nvidia/lib64/libcuda.so.1 /usr/bin/driver/driver_sh.runfiles/third_party/gpus/cuda/lib64/libcuda.so`\n\nThis works too: \n`ln -s /usr/local/nvidia/lib64/libcuda.so.1 /usr/lib/x86_64-linux-gnu/libcuda.so`\n", "In this problem solved?\n\nI am using the latest version `0.7.1`, which supports `cuda 7.5`.\nI tried out all the above proposed suggestions, but none of them worked...\n\nI would like to get tensorflow work on `centos 6.7`. Directly installing tensorflow on the system has the glibc problem #527 \n\nSo I turned to docker and it leads to the current problem.\n\nAny other suggestions on how to get it work?\n", "@zcyang: the `0.7.1-devel-gpu` image seems to work:\n\n```\n$ nvidia-docker run -ti b.gcr.io/tensorflow/tensorflow:0.7.1-devel-gpu sh -c \"python -m tensorflow.models.image.mnist.convolutional\"\n```\n\nBut the `0.7.1-gpu` image doesn't work:\n\n```\n$ nvidia-docker run -ti b.gcr.io/tensorflow/tensorflow:0.7.1-gpu sh -c \"python -m tensorflow.models.image.mnist.convolutional\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:1407] Unable to load cuDNN DSO\n```\n\nThe problem here is that it tries to dlopen `libcudnn.so` but it doesn't exist if you don't have the package `libcudnn4-dev` (which is the case for `nvidia/cuda:7.5-cudnn4-runtime`). Instead, `libcudnn.so.4` is present, which corresponds to the `SONAME`. It works for other CUDA libraries though, it seems we have a small inconsistency here.\n\nThis code seems to be handling the cuDNN version correctly:\nhttps://github.com/tensorflow/tensorflow/blob/ea083742206761139f734c04b354b84c0cb027de/tensorflow/stream_executor/dso_loader.cc#L49-L57\nSo it looks like it could be a problem when calling [configure](https://github.com/tensorflow/tensorflow/blob/master/configure), the cuDNN version was maybe not set explicitly.\n", "I tried `docker` instead of `nvidia-docker`, it still reports the problem.\nIs `nvidia-docker` necessary? I have some trouble to install `nvidia-docker`...\n", "You can use `nvidia-docker` or the wrapper script provided by Tensorflow (I haven't tried). But you need a solution to make Docker work with GPUs.\nIf you have a problem when installing `nvidia-docker`, can you open an [issue](https://github.com/NVIDIA/nvidia-docker/issues)? Thanks!\n", "Many thanks! I used the wrapper script. I open the `nvidia-docker` issue here\nhttps://github.com/NVIDIA/nvidia-docker/issues/54\n", "I got a similar error with @ruffsl on \n- CentOS 7, Linux 3.10.0-327.10.1.el7.x86_64\n- CUDA 7.5, GPU NVIDIA TESLA C2070, Driver Version: 352.79\n- docker 1.10.2\n- b.gcr.io/tensorflow/tensorflow   latest-devel-gpu or 0.7.1-devel-gpu\n\n```\nroot@e09b51ed3473:~# python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13)\n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: e09b51ed3473\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1051] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: wrong ELF class: ELFCLASS32\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n```\n\nIf I use nvidia-docker, the `import tensorflow as tf` can pass. But I got a segment-fault with tensorflow examples.\n\n```\n>> nvidia-docker run -ti b.gcr.io/tensorflow/tensorflow:0.7.1-devel-gpu sh -c \"python -m tensorflow.models.image.mnist.convolutional\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nSegmentation fault (core dumped)\n```\n\nI am not sure whether this issue has been addressed. \nWould anyone give me some advices?\nThanks.\n", "@zheng-xq, @ebrevdo: What's the status of this?  \n", "If you have a problem try the [nvidia-docker](https://github.com/NVIDIA/nvidia-docker). It is in much better shape than i used to be half a year ago.\n\nI believe we can close this. I'm going to update the documentation to use nvidia-docker.\n", "Per comments from @jendap, closing this issue. \n", "\ud83d\udc4d\n", "I am using nvidia-docker and the latest everything on RHEL 7.2 and I still have this issue.\n\n`manju@o185i089:/lvol/manju$ docker run -it gcr.io/tensorflow/tensorflow:latest-devel-gpu`\n`Unable to find image 'gcr.io/tensorflow/tensorflow:latest-devel-gpu' locally`\n`latest-devel-gpu: Pulling from tensorflow/tensorflow`\n`759d6771041e: Pull complete`\n`8836b825667b: Pull complete`\n`c2f5e51744e6: Pull complete`\n`a3ed95caeb02: Pull complete`\n`d5b215eff347: Pull complete`\n`34d39ef2e894: Pull complete`\n`a7a36155eb9e: Pull complete`\n`b1bfa35e6e17: Pull complete`\n`8210f508bb7c: Pull complete`\n`fd02f6ca3839: Pull complete`\n`c7e296ba93f3: Pull complete`\n`7b9159cc5928: Pull complete`\n`25abb1637a5a: Pull complete`\n`71595b8a5fcb: Pull complete`\n`00fa70dea9b1: Pull complete`\n`4c3f9ea22d7e: Pull complete`\n`077e0cee8ca6: Pull complete`\n`65b6e5754aad: Pull complete`\n`0811f4c80537: Pull complete`\n`e0a07be9804b: Pull complete`\n`c8e3ab48d3ab: Pull complete`\n`cc5ff4523b24: Pull complete`\n`Digest: sha256:a38c40a95869d8e36380f88bb0682c77988d12cefd28d4e161093049a4bbb4b5`\n`Status: Downloaded newer image for gcr.io/tensorflow/tensorflow:latest-devel-gpu`\n`root@40afc612354b:~#`\n`root@40afc612354b:~#`\n`root@40afc612354b:~# python`\n`Python 2.7.6 (default, Jun 22 2015, 17:58:13)`\n`[GCC 4.8.2] on linux2`\n`Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.`\n`>>> import tensorflow as tf`\n`I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally`\n`I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally`\n`I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally`\n`I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:`\n`I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 40afc612354b`\n`I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program`\n`I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.93  Tue Apr  5 18:18:24 PDT 2016`\n`GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)`\n`\"\"\"`\n`I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.93`\n`I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1051] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:`\n`I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory`\n`I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally`\n`>>>`\n\nIs there a fix for this issue ?\n", "Did you guys find a solution to this issue? I am on dual Titan X. The error that I get is similar to the one reported above:\n\n`$ python input_data.py \nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: maximus\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: maximus\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  340.96  Sun Nov  8 22:33:28 PST 2015\nGCC version:  gcc version 4.8.5 (Ubuntu 4.8.5-2ubuntu1~14.04.1) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 340.96.0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n0.9092`\n", "RiemannianAnalyzer,\n\nIn my case the libcuda.so was not available in the container at the specified location. I was also using docker instead of nvidia-docker. I added in a libcuda\\* from the host machine into the cuda path (/usr/local/cuda/lib/*) and then froze the image. Now I no longer get the error, with the new image.\n\nIn your case, did your driver install successfully (Have you looked at the driver install log?) ? It seems that the cuda library is missing. Also I am using a more recent version of the driver than you are. I don't know if that makes a difference. In one of my machines I had to install cuda and other support libraries by myself, too.\n\nHTH.\n", "import os\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""]}, {"number": 807, "title": "Add missing arguments.", "body": "- For `docs.py`, add `name` for exception message generation.\n- For `rnn_cell.py` and `rnn_test.py`, add `scope=None` for inheritance.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "@tensorflow-jenkins: test this please\n", "@ebrevdo: I assume adding 'scope' is the right change to the API, can you confirm?\n", "Can you justify the need for passing scope to the **call** interface?\n", "I know that it's not required or used in those functions. I just tried to make the derived classes of `RNNCell` have the same format literally. If you think it's redundant, I will remove the changes related _call_.\n", "Ah sorry, I see now.  Changes LGTM.\n", "Thank you for reviewing, @ebrevdo !\n", "@vrv In the majority of the cases (like this one) seems that your are following [this merging process](http://stackoverflow.com/questions/23015168/how-to-close-a-github-pull-request-with-a-commit-comment). This alter the github pulse and other stats. Is there a way in your process to let this PRs result as merged?\n", "Sometimes it appears as merged, sometimes closed.  I'm not sure what I have to do to tell github it's been merged.  Help would be appreciated\n"]}, {"number": 806, "title": "google-tensorflow", "body": "", "comments": []}, {"number": 805, "title": "Off-by-one-error in shapes table doc", "body": "A rank n tensor's shapes should be [D0, D1, ..., Dn-1]\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(We are waiting for the CLA before proceeding.)\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "There are 2 diffs in that file, but I can't see what has changed in the second.  Is it a whitespace-only change?\n", "> Is it a whitespace-only change?\n\nProbably. Did not change anything in (or around) the second diff.  Clicking on \"rich diff\" removes the second change. (Possible bug in the Github UI/Github Editor?)\n\nhttps://github.com/tensorflow/tensorflow/pull/805/files?diff=split&short_path=6f0a487#diff-6f0a4870f1edfecc7868242cc8c4c426\n", "LGTM\n", "merged.\n"]}, {"number": 804, "title": "Applying batch normalization to a non-convolutional layer fails due to restriction of input to rank 4", "body": "Starting from [this top answer of stackoverflow](http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow?answertab=votes#tab-top), I tried to make batch normalization work for a fully connected layer. This is the relevant part of the code:\n\n```\n            if self.convolutional:\n              mean, variance = tf.nn.moments(x, [0, 1, 2])\n            else:\n              mean, variance = tf.nn.moments(x, [0])\n            assign_mean = self.mean.assign(mean)\n            assign_variance = self.variance.assign(variance)\n            with tf.control_dependencies([assign_mean, assign_variance]):\n                return tf.nn.batch_norm_with_global_normalization(\n                    x, mean, variance, self.beta, self.gamma, self.epsilon,\n                    scale_after_normalization=self.scale_after_normalization)\n```\n\nThe call to tf.nn.batch_norm_with_global_normalization fails however:\n\n```\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 104, in batch_norm_with_global_normalization\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 659, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1896, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1524, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 313, in _BatchNormShape\n    input_shape = op.inputs[0].get_shape().with_rank(4)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 614, in with_rank\n    return self.merge_with(unknown_shape(ndims=rank))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 542, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 585, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes (64, 512) and (?, ?, ?, ?) must have the same rank\n```\n\nsince, as far as I can tell, the input is constraint to have rank 4 in in [nn_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L313). Is this really a necessary constraint?\n", "comments": ["As currently written, the op expects inputs of 4 dimensions.  https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/batch_norm_op.h\n\nBut it doesn't seem to need to -- it just flattens everything but the last dimension anyway.  We would need to update the kernels and then the shape function, but it seems possible.\n\n@sherrym, @shlens in case they want to comment.\n", "My workaround for now is to just reshape my tensor to `[-1, 1, 1, depth]` (with depth as the number of outgoing connections of my fully connected layer). This seems to work, but it would be nice to have this built in.\n", "Many ops still have historical and/or arbitrary rank restrictions. Wherever knowledge of the rank is not needed, they should be generalized. \n", "Note that batch normalization is a different operation depending on whether the layer is convolutional or not: when using it on a convolutional layer, you can aggregate sufficient statistics across patches. Not so when the layer is fully connected. Reshaping to [-1, 1, 1, depth] is the correct thing to do though, and we should probably just do that.\n", "We'd be happy to accept PRs if anyone wants to generalize batch normalization to arbitrary rank.\n", "This is obsolete. `nn.batch_norm_with_global_normalization` has been replaced by `nn.batch_normalization` which accepts arbitrary geometries.\n"]}, {"number": 803, "title": "Error in cifar10_eval.py", "body": "Hi,\ni'm trying to run the cifar10 network and getting problems when evaluating the trained variables. I've runned the cifar_10 train with a maximum steps of 100 and then it should theoretically be able to be evaluated, but i get this error:\n.../tensorflow/models/image/cifar10/cifar10_eval.py\", line 138, in evaluate\n    variables_to_restore = variable_averages.variables_to_restore()\nAttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore'\nCan someone help me?\nThanks\n", "comments": ["Dupe of https://github.com/tensorflow/tensorflow/issues/802\n"]}, {"number": 802, "title": "AttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore'", "body": "I am running the code provided with the tutorial Convolutional Neural Networks https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html \n\nThe training works fine, but when I run the evaluation I get the following error. I have not modified in any way the code, I am running it as is.\n\nCode snippet:\n...\n# Restore the moving average version of the learned variables for eval.\n\n```\nvariable_averages = tf.train.ExponentialMovingAverage(\n    cifar10.MOVING_AVERAGE_DECAY)\nvariables_to_restore = variable_averages.variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore)\n```\n\n...\n\nError:\n\nTraceback (most recent call last):\n  File \"cifar10_eval.py\", line 161, in <module>\n    tf.app.run()\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"cifar10_eval.py\", line 157, in main\n    evaluate()\n  File \"cifar10_eval.py\", line 135, in evaluate\n    variables_to_restore = variable_averages.variables_to_restore()\nAttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore'\n\nPlease help,\n", "comments": ["I get the same error, I think that they have changed something on the code moving_averages.py code\n", "What is your tensorflow version? You might need master branch version of tensorflow to make that script running. (In 0.6.0, ExponentialMovingAverage has no attributes variables_to_restore, but master version has it.)\n", "Yes it was version 0.6 as it says in the download section of tensorflow,\nbut i don't know how to install the master version as a package, should I\ndownload the github repository on the library folder (\n   /Library/Python/2.7/site-packages/tensorflow/...) ?\nThanks a lot,\nManu\n\nOn Mon, Jan 18, 2016 at 3:30 PM, Aymeric Damien notifications@github.com\nwrote:\n\n> What is your tensorflow version? You might need master branch version of\n> tensorflow to make that script running. (In 0.6.0, ExponentialMovingAverage\n> has no attributes variables_to_restore, but master version has it.)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/802#issuecomment-172540505\n> .\n", "After @aymericdamien recommendation I have copied the tensorflow folder with all the training scripts from master version (downloaded from github) into 0.6 (installed version, which is different) and now cifar10_eval goes a bit further, it doesn't say that 'ExponentialMovingAverage' object has no attribute 'variables_to_restore' but once in the variables to restore called function it says\n\n...\nFile \"/Library/Python/2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 307, in variables_to_restore\n    moving_avg_variables = list(set(variables.moving_average_variables() +\nAttributeError: 'module' object has no attribute 'moving_average_variables'\n...\n\nAny ideas?\n", "Sherry, this looks related to 110808120 or 110781462.\n", "If you're using the 0.6.0 pip installation, you should check out the code at the 0.6.0 branch.  There's no guarantee that the scripts at HEAD will work with the old binaries, since we add features to HEAD and then update our examples.\n", "I have downloaded the master version from github just like @manucarbonell did. I am using a virtualenv called tf. If I don't activate the tf I keep on getting the original error. If I activate tf I get the following. Please advice.\n\n(tf)Carloss-MacBook-Pro:~ Carlos$ python ~/../../Volumes/Transcend/Dropbox/workspace/TensorFlow/CIFAR10/tensorflow-master-tensorflow-models-image-cifar10/cifar10_train.py \nTraceback (most recent call last):\n  File \"/Users/Carlos/../../Volumes/Transcend/Dropbox/workspace/TensorFlow/CIFAR10/tensorflow-master-tensorflow-models-image-cifar10/cifar10_train.py\", line 43, in <module>\n    import tensorflow.python.platform\n  File \"/Users/Carlos/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/Carlos/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 43, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/Users/Carlos/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 37, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\nImportError: No module named core.framework.graph_pb2\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n", "I managed to run the example successfully again after using the version 0.6.0, which doesn't have the line\n\"variables_to_restore = variable_averages.variables_to_restore()\"\nin cifar10_eval\n\nThanks to the moderators!\n", "Manu or Carlos, could you elaborate a bit on how you solved this problem?\nOr rather how this tutorial can work for tensorflow 0.6.0.\n", "The commit 64f8a3b0dc54b6125ad0ea94d0959b175b3eaad1  introduced a change in the API by adding the variables_to_restore() function.\n\nYou need to get the source code 0.6.0 of the example to run this example:\n\n``` bash\ngit clone https://github.com/tensorflow/tensorflow.git\ngit checkout 0.6.0\ncd tensorflow/tensorflow/models/image/cifar10/\npython cifar10_train.py --max_steps=20                   # Just a few steps to check the script\npython cifar10_eval.py --num_examples 1000 --run_once 1  # Just once to check the script\n```\n", "Thanks for the answer. I found the 0.6.0 tutorial and it works. (while still producing the big number of warnings and errors about the queues. Same as in  #389 . It is something related to the threads but it takes place after the computation is complete).\n", "This should have long been fixed now that both the code and pip package containing this change has been available for several months.\n"]}, {"number": 801, "title": "Fix typos in core/framework.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Merged\n", "Thank you, @vrv .\n"]}, {"number": 800, "title": "Sudden Loss Explosion, followed by total missevaluation", "body": "Hello altogether =)\n\nI'm testing Tensorflow 0.60 (GPU Version) for Python 3.4 on Ubuntu 15.10. The used GPU is a GT 960m. Testing the CIFAR10 Demo for over two days in a row gave me the ~86.x% everbody gets after evaluation. After transforming my own data into batches (cifar10 demo like) and adjusting the cifar-python files accordingly, i achieved the following results for my binary classification task (always stopped training, then evaluating, then restarting).\n\nafter 240 steps (of 128 size batches)  -- >> 92 %\nafter 420 steps (of 128 size batches)  -- >> 92 %\nafter 1500 steps (of 128 size batches)  -- >> 94 %\nafter 2000 steps (of 128 size batches)  -- >> 95 %\n\nOver the course of these tests, the loss value - printed on the console and retrieved via tensorboard, went from about 12 to about 3. Over the (last) weekend, i raised the maximum steps to 10 million, and let things just run. Today in the morning i did an evaluation on the network, which arrived at a loss of 0.6 but evaluated with 50.4 % which looks like totally random (binary task). Scrolling through the logs i realized, there was a sudden loss explosion at around step 70000, jumping from like 2 to 2000 (!!), and - after slowly recovering from that, again up to about 2 000 000 (!!) million, and also slowly recovering from that downto said 0.6, but with the said total missevaluation. Also noteworthy, the speed from almost constantly 100 examples per second broke downto 30 when the loss spikes occured. The eventfile (one of them) went up to 9 Gigabytes in size, and re-checking with tensorboard gave strange results (showing sometimes 4 million, and sometimes 900 million learn steps, appeared to be totally broken somehow).\n\nAppart from just a different batch file, and adjusted image size parameters, the network is exactly the same as in CIFAR 10.\n\nI'm sorry, i can't provide the logs, but right now i am relearning the whole thing and monitoring if this happens again.\n\nDid anybody of experience something like this before?\n", "comments": ["Providing additional data after relearning to step 11000 : \n![loss plot](http://i.imgur.com/fPFEImG.png)\n\nsteps _____eval_________loss\n03000_____96.064%_____1.57\n04000_____97.389%_____0.77\n06850_____97.854%_____0.17\n08000_____98.487%_____0.13\n09000_____98.467%_____0.07\n10000_____98.556%_____0.08\n11000_____98.556%_____0.09\n\nso far, it looks quite like expected, i will just let it run over night again\n", "(this type of question belongs on StackOverflow, github is for bugs / feature requests, etc).\n", "it happend again, this time at (around) step 47030. Sudden loss jump from average 0.05 to 1.8 (thank god it wasnt 2 million again), then slowly back to 0.71 @ step 55000 ( i stopped there). The model seems totally corrupted after that, evaluation gives 48%, which is worse than throwing a dice for a binary task! I understand that i would have to identify the bug for reporting this as a bug, but since i use the default out of the box model for cifar10, this horrible behaviour doesn't look much like just \"bad learning\". I will reroll to step 11000 and monitor again. \n\n![loss plot2](http://i.imgur.com/V8xpR5U.png)\n\n@vrv please re-evaluate the state of this issue or direct it to someone who could have an idea what's going on\n", "What was your learning rate? What kind of data are you feeding? Right now i'm doing similar changes to cifar10 model to my own 200x200 vegetation images classification. The datasets have some noise (mixed classes).\n\nMy model decreases from 11.6 loss to 3.6 loss after 2k, and then jumps to 300 at 3k and stabilizes at ~80.\n\nMy ILR is 0.07 and decay is also 0.07\n", "I used the normal learning rates (and parameters) from the standard CIFAR10 models. My way of successfully combating this problem was actually kind of working around it : Since i work with very suspicious and paradox data (difficult and human labeled), sometimes, due to contradicting information, there is a sequence of learning examples generating very high errors. So i simply reduced the batch size to not stack these errors too high, and in case of failure (see above) i simply switched back to an \"OK\" model before that point.\n\nI'd suggest you check (evaluate) your models once before and then after the loss jump. Chances are high, that the model after the loss jump is unusable.\n", "@HellMood  that is probably my problem. I'm also using difficult and human labeled data which in some cases might be mislabeled. If I decrease my learning rate from 0.07 to 0.02 the training goes on without loss peaks. I'm also using a small batch size (16 to 32). Using a 32 size will most probably induce my loss to sudden rise, agreeing with what you said about reducing the batch size to not stack errors too high.\n\nYou're telling me that if however a model loss peaks at K steps, you will use the model at K-n steps (step where the loss is still stably converging)?\n", "Yes, and guided by the evaluation values. If, for a binary classification task, the evaluation drops to \"random\" after a loss explosion, (eval(k) = 0.50) and i can ensure that before this happened, everything was okay (eval(k-1) = 0.99), then i reroll to model (k-1)\n\nEdit : Practically, it's more like (k-x), because i only save the model so often ;)\n", "Hi, I'm also having a similar issue. \r\nI changed my cifar-10 into binary classification with some adjustments, and it doesn't seem to be doing the right feature extraction as training step increases.\r\nFor my case, within 100 steps, the predicted class is all set to one label and shows no change after.\r\nAs I see my fully connected layer, I can see that the nodes are mostly dead to one value and the ones with distributed values are looking same.\r\nI tried to lower my learning rate but have the same problem.\r\nAny suggestions?", "Or is there any reason that I have to lower my learning rate the only thing I changed was labeling to binary with same data?"]}, {"number": 799, "title": "Got error when initialize bidirectional rnn with LSTM cell ", "body": "I want to build a bi-rnn model with tensorflow with LSTM cell, when i try to initialize bidirectional_rnn,\nit gives: `ValueError: Over-sharing: Variable BiRNN_FW/RNN/BasicLSTMCell/Linear/Matrix already exists, disallowed. Did you mean to set reuse=True in VarScope?`\n\n```\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn, rnn_cell\nfrom tensorflow.python.ops.constant_op import constant\nimport numpy as np\n\nclass Model(object):\n    def __init__(self, batch_size, len_word, num_chars, dim_embed, dim_hidden):\n        self.batch_size = batch_size\n        self.dim_embed = dim_embed\n        self.dim_hidden = dim_hidden\n        self.num_chars = num_chars\n        self.len_word = len_word\n\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([num_chars, dim_embed], -0.1, 0.1), name='embedding')\n\n        self.W_emb = tf.Variable(tf.random_uniform([dim_hidden*2, dim_embed], -0.1, 0.1), name='W_emb')\n        self.b_emb = tf.Variable(tf.zeros([dim_embed]), name='b_emb')\n        self.lstm_fw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n        self.lstm_bw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n\n    def build_model(self):\n        inputs = tf.placeholder(tf.int32, [self.batch_size, self.len_word])\n        input_length = tf.placeholder(tf.int64, [self.batch_size])\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.batch_size, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.batch_size, tf.float32)\n\n        with tf.device(\"/cpu:0\"):\n            embedded_input = tf.nn.embedding_lookup(self.embedding, tf.transpose(inputs))\n\n        brnn_output = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(embedded_input),\n            sequence_length=input_length,\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n        )\n\n        pooled_output = tf.reduce_sum( tf.pack(brnn_output), 0 )\n        pooled_output = pooled_output / tf.expand_dims( tf.to_float(input_length) + 1e-6, 1)\n        final_emb = tf.nn.xw_plus_b(pooled_output, self.W_emb, self.b_emb)\n        final_emb = tf.nn.l2_normalize(final_emb, dim=1, epsilon=1e-7)\n\n        return final_emb\n```\n", "comments": ["You need to specify different variable scopes for the LSTM cells. \n\n```\nwith tf.variable_scope('forward'):\n    self.lstm_fw_cell = rnn_cell.BasicLSTMCell(dim_hidden)   \nwith tf.variable_scope('backward'):\n    self.lstm_bw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n```\n\nOtherwise there will be a name collision (both cells try to use the \"BiRNN_FW/RNN/BasicLSTMCell/Linear/Matrix\" name) and tf will interpret this as if your intention is to share the parameters in the two cells, which is not what you want. TF will throw an exception because you haven't explicitly told it to reuse variables in the second scope: `with variable_scope(name, reuse=True)`.  \n\nSetting the variable scopes, as above, will create unique names for the variables:\nBiRNN_FW/RNN/BasicLSTMCell/**forward**/Linear/Matrix\nBiRNN_FW/RNN/BasicLSTMCell/**backward**/Linear/Matrix\n\nRead the [Sharing Variables](https://www.tensorflow.org/versions/0.6.0/how_tos/variable_scope/index.html#sharing-variables) guide for more information.\n", "Note that this will not happen anymore because the variables are created inside `...Cell.__call__` and not in  `...Cell.__init__`, thus it does not need a scope for the construction of the cells, and it will handle the variable scope itself inside `bidirectional_rnn` thus you don't need to scope it yourself.", "Im still having the same issue. Any sugestions? I have tried the approch proposed by salomons, same results. Doenst return any tuple as a result.\r\n((encoder_fw_outputs,\r\n  encoder_bw_outputs),\r\n (encoder_fw_final_state,\r\n  encoder_bw_final_state)) = (\r\n    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\r\n                                    cell_bw=encoder_cell,\r\n                                    inputs=encoder_inputs_embedded,\r\n                                    sequence_length=encoder_inputs_length,\r\n                                    dtype=tf.float64, time_major=True)\r\n    )\r\n\r\nValueError                   Traceback (most recent call last)\r\n<ipython-input-141-f7ed36f14c22> in <module>()\r\n     20                                     inputs=encoder_inputs_embedded,\r\n     21                                     sequence_length=encoder_inputs_length,\r\n---> 22                                     dtype=tf.float32, time_major=True)\r\n     23     )\r\n     24 \r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    348           initial_state=initial_state_fw, dtype=dtype,\r\n    349           parallel_iterations=parallel_iterations, swap_memory=swap_memory,\r\n--> 350           time_major=time_major, scope=fw_scope)\r\n    351 \r\n    352     # Backward direction\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    544         swap_memory=swap_memory,\r\n    545         sequence_length=sequence_length,\r\n--> 546         dtype=dtype)\r\n    547 \r\n    548     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    711       loop_vars=(time, output_ta, state),\r\n    712       parallel_iterations=parallel_iterations,\r\n--> 713       swap_memory=swap_memory)\r\n    714 \r\n    715   # Unpack final output if not using output tuples.\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2603     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\r\n   2604     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\r\n-> 2605     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2606     return result\r\n   2607 \r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2436       self.Enter()\r\n   2437       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2438           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2439     finally:\r\n   2440       self.Exit()\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2386         structure=original_loop_vars,\r\n   2387         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2388     body_result = body(*packed_vars_for_body)\r\n   2389     if not nest.is_sequence(body_result):\r\n   2390       body_result = [body_result]\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)\r\n    694           call_cell=call_cell,\r\n    695           state_size=state_size,\r\n--> 696           skip_conditionals=True)\r\n    697     else:\r\n    698       (output, new_state) = call_cell()\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _rnn_step(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\r\n    175     # steps.  This is faster when max_seq_len is equal to the number of unrolls\r\n    176     # (which is typical for dynamic_rnn).\r\n--> 177     new_output, new_state = call_cell()\r\n    178     nest.assert_same_structure(state, new_state)\r\n    179     new_state = nest.flatten(new_state)\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in <lambda>()\r\n    682 \r\n    683     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\r\n--> 684     call_cell = lambda: cell(input_t, state)\r\n    685 \r\n    686     if sequence_length is not None:\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\r\n    336       # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n    337       lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True,\r\n--> 338                             scope=scope)\r\n    339       i, j, f, o = array_ops.split(\r\n    340           value=lstm_matrix, num_or_size_splits=4, axis=1)\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in _linear(args, output_size, bias, bias_start, scope)\r\n    745   with vs.variable_scope(scope) as outer_scope:\r\n    746     weights = vs.get_variable(\r\n--> 747         \"weights\", [total_arg_size, output_size], dtype=dtype)\r\n    748     if len(args) == 1:\r\n    749       res = math_ops.matmul(args[0], weights)\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    986       collections=collections, caching_device=caching_device,\r\n    987       partitioner=partitioner, validate_shape=validate_shape,\r\n--> 988       custom_getter=custom_getter)\r\n    989 get_variable_or_local_docstring = (\r\n    990     \"\"\"%s\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    888           collections=collections, caching_device=caching_device,\r\n    889           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 890           custom_getter=custom_getter)\r\n    891 \r\n    892   def _get_partitioned_variable(self,\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    346           reuse=reuse, trainable=trainable, collections=collections,\r\n    347           caching_device=caching_device, partitioner=partitioner,\r\n--> 348           validate_shape=validate_shape)\r\n    349 \r\n    350   def _get_partitioned_variable(\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\r\n    331           initializer=initializer, regularizer=regularizer, reuse=reuse,\r\n    332           trainable=trainable, collections=collections,\r\n--> 333           caching_device=caching_device, validate_shape=validate_shape)\r\n    334 \r\n    335     if custom_getter is not None:\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\r\n    637                          \" Did you mean to set reuse=True in VarScope? \"\r\n    638                          \"Originally defined at:\\n\\n%s\" % (\r\n--> 639                              name, \"\".join(traceback.format_list(tb))))\r\n    640       found_var = self._vars[name]\r\n    641       if not shape.is_compatible_with(found_var.get_shape()):\r\n\r\nValueError: Variable bidirectional_rnn/fw/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\r\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\r\n  File \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 338, in __call__\r\n    scope=scope)\r\n  File \"<ipython-input-23-f4f28501e56f>\", line 24, in <module>\r\n    time_major=True\r\n"]}, {"number": 798, "title": "Converting Tensor to ndarray", "body": "```\n>>> a = tf.random_uniform((3,3))\n>>> b = tf.tensor_util.MakeNdarray(a)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 414, in MakeNdarray\n    shape = [d.size for d in tensor.tensor_shape.dim]\nAttributeError: 'Tensor' object has no attribute 'tensor_shape'\n```\n\nIt looks like `MakeNdarray` expects a `TensorProto` instead of a `Tensor`, so this works fine:\n\n```\na = tf.tensor_util.make_tensor_proto([1,2,3])\n```\n\nIs there any way to convert a `Tensor` to a `TensorProto`? If not, is there any other way to convert a `Tensor` to an `ndarray`?\n", "comments": ["- I believe many of the functions in tensor_util are actually supposed to be private, so I probably would try to avoid using any function that's not currently publicly documented.\n- In your code snippet, 'a' is really a symbolic tensor indicating that it will produce a random 3x3 matrix when evaluated in the context of a session, so to actually get its content, you need to 'eval' it, which will return a numpy array:\n\n```\nwith tf.Session():\n  a = tf.random_uniform((3, 3))\n  b = a.eval()  # Runs to get the output of 'a' and converts it to a numpy array\n```\n", "Also, this type of question is better suited for StackOverflow -- you'll get better feedback / advice there.  Try to use github issues for bug reports / installation issues / feature requests.\n"]}, {"number": 797, "title": "Update license year range to 2016", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 796, "title": "Problem running Convolutional.py", "body": "I am trying to run the convolutional.py through the tensorflow virtualenv i am getting an error as follows \nTraceback (most recent call last):\n  File \"convolutional.py\", line 290, in <module>\n    tf.app.run()\n  File \"/home/akshay/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 121, in main\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n  File \"convolutional.py\", line 56, in maybe_download\n    os.mkdir(WORK_DIRECTORY)\nOSError: [Errno 13] Permission denied: 'data'\n\nI am getting this error even when i am running as super user.\n", "comments": ["You don't have permission to create a directory in your current working directory. We won't attempt to change that programmatically for you since it might have unintended consequences.\n", "I tried changing the permissions,working now.\nThanks!\n", "Hi,\nRecently I tried to check tensorflow with : python -m tensorflow.models.image.mnist.convolutional.\nAnd I get same error:\n\n(tensorflowGPU)administrator@zsp515-6a:/home$ python -m tensorflow.models.image.mnist.convolutional\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/administrator/tensorflowGPU/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 316, in <module>\n    tf.app.run()\n  File \"/home/administrator/tensorflowGPU/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/administrator/tensorflowGPU/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 122, in main\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n  File \"/home/administrator/tensorflowGPU/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 57, in maybe_download\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\n  File \"/home/administrator/tensorflowGPU/local/lib/python2.7/site-packages/tensorflow/python/platform/gfile.py\", line 294, in MakeDirs\n    os.makedirs(path, mode)\n  File \"/home/administrator/tensorflowGPU/lib/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\n**OSError: [Errno 13] Permission denied: 'data'**\n\nWhat name of the folder ,current working directory, in this/my case?\ufeff\nThanks.\n", "For now, I recommend cloning the git repository into another directory and running that version of the file.  I think we'd have to rewrite our tutorials and examples to not assume relative path otherwise.\n", "Probably you are facing problem when a download request is made by the maybe_download function call in base.py file.\r\n\r\nThere is a conflict in the permissions of the temporary files and I myself couldn't work out a way to change the permissions, but was able to work around the problem.\r\n\r\nDo the following...\r\n\r\n - Download the four .gz files of the MNIST data set from the link ( http://yann.lecun.com/exdb/mnist/ )\r\n - Then make a folder names MNIST_data (or your choice in your working directory/ site packages folder in the tensorflow\\examples folder).\r\n - Directly copy paste the files into the folder.\r\n - Copy the address of the folder (it probably will be \r\n( C:\\Python\\Python35\\Lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\MNIST_data  ))\r\n - Change the \"\\\" to \"/\" as \"\\\" is used for escape characters, to access the folder locations.\r\n - Lastly, if you are following the tutorials, your call function would be **( mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) )** ;\r\nchange the **\"MNIST_data/\"** parameter to your **folder location**. As in my case would be *( mnist = input_data.read_data_sets(\"C:/Python/Python35/Lib/site-packages/tensorflow/examples/tutorials/mnist/MNIST_data\", one_hot=True) )*\r\n\r\nThen it's all done.\r\nHope it works for you."]}, {"number": 795, "title": "Tensor Flow IN Android", "body": "Please someone help me how i set the tensor flow library in android i try to follow the steps explain in the android example but stuck in first and i do not know how to do it which is Get the recommended Bazel version\n", "comments": ["On MacOSX, I first upgraded java to Java8, and then installed Bazel.\n\nI've also prepared Android Example building environment with AndroidStudio and NDK, without Bazel.\nhttps://github.com/miyosuda/TensorFlowAndroidDemo\nIf you would like to quickly try Android Example, it might help.\n", "http://bazel.io/docs/install.html for installing bazel\n"]}, {"number": 794, "title": "Docker versus direct install?", "body": "I picked up a TitanX card and I'm about to install Linux 14.04.3 Desktop.  Are there any performance considerations or Nvidia GPU driver considerations between choosing a Docker install or direct install?\n", "comments": ["Not that I know of, but we haven't done any heavy testing. (Unless @ebrevdo has done some?) \n\nIn principle, the GPU device is directly exposed to the docker container, so if there is a difference, it should be negligible. If you try it and find otherwise, @roschler, we'd definitely like to know!\n", "Thanks Craig.  I'll try the Docker container then.  I'm guessing it's easier and since you haven't indicated any loss of performance or programming features due to its usage, I see no reason not to.\n"]}, {"number": 793, "title": "Fix typos in python/training module.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins, test this please.\n", "(apparently our CI is down -- we'll look at this again when it's back up, sorry).\n", "@tensorflow-jenkins, test this please\n", "Thanks, this is great!\n", "Thank you, @vincentvanhoucke , @vrv , and @martinwicke .\n"]}, {"number": 792, "title": "Fix syntax error when executing setup.py using python 3.4", "body": "When I execute `python3 ./tensorflow/tools/pip_package/setup.py install` on raspberry pi 2, I encountered syntax error message at these two code snippets. This commit is for fixing such error.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I also encountered this problem when executing `python3 ./tensorflow/tools/pip_package/setup.py install` on Lubuntu 15.10.\n", "I have signed the  Contributor License Agreement !\n", "(looks like it hasn't registered -- did you verify that the commit email matches?)\n", "@vrv Looks like I have some trouble about different gmail and github account. I don't know how to resolve it. My usage scenario is : \n\n1) I use brchiu@ms35.hinet.net as my github account for my development purpose, commit this modification to my fork then create this pull request. \n2) my google account is biruei.chiu@gmail.com. I signed CLA using it and designate my github account as brchiu@ms35.hinet.net in contact information.\n\nPlease advice how to resolve this ?\nThanks.\n", "Options\n- You can sign the CLA using the email address that you use in your git commits\n- You can git commit --amend your change to use the email address you already registered the CLA with and then repush / resend the pull request.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins: test this please\n", "```\n==================== Test output for //tensorflow/models/rnn/ptb:reader_test:\n.E.\n======================================================================\nERROR: testPtbRawData (__main__.PtbReaderTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader_test.py\", line 49, in testPtbRawData\n    output = reader.ptb_raw_data(tmpdir)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py\", line 83, in ptb_raw_data\n    word_to_id = _build_vocab(train_path)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py\", line 47, in _build_vocab\n    key=lambda word, count: (-count, word))\nTypeError: <lambda>() takes exactly 2 arguments (1 given)\n```\n\nLooks like this doesn't work for python 2 ...\n", "Wrap word, count in parens.  It should work.\nOn Jan 18, 2016 8:23 PM, \"Vijay Vasudevan\" notifications@github.com wrote:\n\n> ==================== Test output for //tensorflow/models/rnn/ptb:reader_test:\n> \n> # .E.\n> \n> ## ERROR: testPtbRawData (**main**.PtbReaderTest)\n> \n> Traceback (most recent call last):\n>   File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader_test.py\", line 49, in testPtbRawData\n>     output = reader.ptb_raw_data(tmpdir)\n>   File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py\", line 83, in ptb_raw_data\n>     word_to_id = _build_vocab(train_path)\n>   File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py\", line 47, in _build_vocab\n>     key=lambda word, count: (-count, word))\n> TypeError: <lambda>() takes exactly 2 arguments (1 given)\n> \n> Looks like this doesn't work for python 2 ...\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/792#issuecomment-172733176\n> .\n", "That's what we had before, so not sure why it's not python 3 compatible, would have to look into it.\n", "After searching on internet, the problem might comes from [PEP 3113 -- Removal of Tuple Parameter Unpacking](https://www.python.org/dev/peps/pep-3113/). According to a post on StackOverflow, the author suggest [a fix on this kind of problem](http://stackoverflow.com/questions/11328312/python-lambda-does-not-accept-tuple-argument), \nbut I don't know how to execute the unit test (i.e. reader_test.py) on my machine.\n\nCan anyone advice ? For example, do I need to install bazel environment first ?\n", "Jenkins, test this please.\n", "I found that 'MacOS CPU Test' failed from 2016/1/10. \nIs there anyone looking into it ?\n", "Yeah it's known and unrelated, let me disable that for now.\n", "Thanks, can you squash your commits?  I'll merge once that's done.\n", "I have squashed my commits. Thanks.\n", "@brchiu: only admins can kick off tests right now.  I'll test and merge tomorrow morning.\n", "@vrv Got it ! I just try to see whether 'Open Sesame!' work or not. :)\n"]}, {"number": 791, "title": "Building TF on archlinux with python3", "body": "Hi\n\nSounds like the python3 building script have some issues:\n\n```\nPC% bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n...........\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /home/zenol/tensorflow/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 18.140s, Critical Path: 2.02s\n```\n\nWith verbose_faillures : \n\n```\nERROR: /home/zenol/tensorflow/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command \n  (cd /home/zenol/.cache/bazel/_bazel_zenol/67d52188e906c7f900d4635d7d1ff821/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: 2to3 failed: error executing command \n  (cd /home/zenol/.cache/bazel/_bazel_zenol/67d52188e906c7f900d4635d7d1ff821/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 30.007s, Critical Path: 29.28s\n```\n\nNb : The python2 build seems to work.\n", "comments": ["I get a similar error message on OSX 10.11.2 with python 3.5.0:\n\n```\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/util/time_util.cc [for host]:\ngoogle/protobuf/src/google/protobuf/util/time_util.cc:52:18: warning: unused variable 'kMicrosPerMillisecond' [-Wunused-const-variable]\nstatic const int kMicrosPerMillisecond = 1000;\n                 ^\ngoogle/protobuf/src/google/protobuf/util/time_util.cc:56:19: warning: unused variable 'kTimestampFormat' [-Wunused-const-variable]\nstatic const char kTimestampFormat[] = \"%E4Y-%m-%dT%H:%M:%S\";\n                  ^\ngoogle/protobuf/src/google/protobuf/util/time_util.cc:371:6: warning: unused function 'ToUint128' [-Wunused-function]\nvoid ToUint128(const Timestamp& value, uint128* result, bool* negative) {\n     ^\ngoogle/protobuf/src/google/protobuf/util/time_util.cc:396:6: warning: unused function 'ToTimestamp' [-Wunused-function]\nvoid ToTimestamp(const uint128& value, bool negative, Timestamp* timestamp) {\n     ^\n4 warnings generated.\nINFO: From Compiling google/protobuf/src/google/protobuf/util/internal/utility.cc [for host]:\ngoogle/protobuf/src/google/protobuf/util/internal/utility.cc:50:19: warning: unused function 'SkipWhiteSpace' [-Wunused-function]\nconst StringPiece SkipWhiteSpace(StringPiece str) {\n                  ^\n1 warning generated.\nERROR: /Users/zeis/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command \n  (cd /private/var/tmp/_bazel_zeis/137871672d8bee8042ab5135b1863c28/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_darwin-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: 2to3 failed: error executing command \n  (cd /private/var/tmp/_bazel_zeis/137871672d8bee8042ab5135b1863c28/tensorflow && exec env - bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_darwin-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 7.380s, Critical Path: 7.22s\n```\n", "Attempted to build TF on Archlinux as well, with the same problem \u2026\nsome additional info: The 2to3 linked/created by bazel seems to be only an empty stub:\n\n```\n> cat bazel-out/host/bin/external/bazel_tools/tools/python/2to3\n#!/bin/bash\n\nexit 1\n```\n\nI'm totally new to bazel, so I didn't easily spot what went wrong, but someone familiar with it might know what's going on \u2026\nArchlinux differs from many distributions, in that python points to python3, not python2; I however tried to pass /usr/bin/python3 to `./configure`, without any change in the error \u2026\n\nAs a workaround, I changed `exit 1` to `exec 2to3 $@` [and changed the mtime of the file (soft-link target!) to the future, as bazel apparently checks if the build tools have been tampered with), and could successfully build TF.\n\nRegards, Christian\n", "Some python targets are missing the srcs_version=\"PY2AND3\" annotation in the BUILD file. We're working on it. In the meantime, any python target in python/BUILD without this annotation will fail when compiled with python 3.\n", "@csachs Thanks for sharing, this worked well for me too.\n", "Any estimate of when this problem will be fixed?", "@csachs I have tried this but bazel still complains about the build being corrupted. How exactly did you change mtime?\r\n\r\n> As a workaround, I changed exit 1 to exec 2to3 $@ [and changed the mtime of the file (soft-link target!) to the future, as bazel apparently checks if the build tools have been tampered with), and could successfully build TF.\r\n\r\n", "I think I used `touch` (cf. https://superuser.com/questions/604443/manually-update-the-mtime-of-a-file-in-unix ). A pity that his bug is still an issue."]}, {"number": 790, "title": "Print the correct host information instead of `localhost` alias.", "body": "If a user specify a host ip by using `--host` option,\nany connections to `http://localhost:6006` are refused.\n", "comments": ["Can one of the admins verify this patch?\n", "Looks good to me. Thanks!\n", "@tensorflow-jenkins: test this please.\n", "merged.\n", "Thank you, @danmane , @vrv , and @martinwicke .\n"]}, {"number": 789, "title": "Small document fixes", "body": "1. typo fixes\n2. Adds instructions to chmod a+r on cudnn.so as NVIDIA's package is not by default readable.  Necessary as tested when tried on Mac.\n", "comments": ["Can one of the admins verify this patch?\n"]}]