[{"number": 31705, "title": " The arguments and returns of tf.keras.layers.GRUCell.call() make no sense at all", "body": "## URL(s) with the issue: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1615 https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1718\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell\r\n\r\n## Description of issue (what needs changing):\r\nThe `states` argument to tf.keras.layers.GRUCell.call() is indexed with `h_tm1 = states[0]  # previous memory`, and the function returns `h` and `[h]`, which is the same value?\r\n\r\n### Clear description\r\nWhy does this occur? Its inconsistent with the pytorch torch.nn.GRUCell implementation (https://pytorch.org/docs/stable/nn.html#grucell). \r\n\r\nI noticed the `states` issue when I was converting a project from pytorch to tf.keras and the same code, with just the GRUCell swapped from pytorch to tf.keras, did not work. The error message was `tensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] is not a matrix. Instead it has shape [200] [Op:MatMul] name: transition/gru_cell/MatMul/`, and the solution was to replace my `hidden` with `[hidden]` for the states parameter. \r\n\r\nFurthermore, when I got my return types, they were a tuple rather than the output. Upon further inspection, the tuple CONTAINED THE SAME VALUE TWICE.\r\n\r\nIs there any reason it does this? In the doc this is not explained AT ALL.\r\n\r\n### Correct links\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1615\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1718\r\n\r\n### Parameters defined\r\n`states` parameter is the confusing one in question.\r\n\r\n### Returns defined\r\n`return h, [h]` makes no sense\r\n\r\n### Raises listed and defined\r\nIrrelevant\r\n\r\n### Usage example\r\nself.rnn = tf.keras.layers.GRUCell(num_units)\r\nself.rnn(input, hidden)\r\n\r\nwhere input, hidden = shape(N, num_units) doesn't work. Needs to be changed to:\r\nself.rnn(input, [hidden]) to execute.\r\n\r\nFurthermore, on the LHS of self.rnn, rather than just an x = self.rnn(...), I need to do x, _ = self.rnn(...)\r\nWhy?\r\n\r\n### Submit a pull request?\r\nI would gladly change this if someone would confirm this is an issue.", "comments": ["Hi @jpatts, \r\n\r\nThanks for reporting this, but I believe this code is correct.\r\n\r\nThe keras rnn api is: You write a layer that returns an output, and a list of internal states. \r\nThe list of internal states is what the layer passes to the next timestep as `states`. \r\n\r\nCompare to [LSTM](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L2196)\r\n\r\nGRUCell returns `h,[h]`: the first `h` is the output. `[h]` is the list of internal states to pass to the next timestep. This makes sense because a GRU has one recurrent connection. That connection is its output from the last timestep.\r\n\r\nSee the [Keras RNN API](https://www.tensorflow.org/beta/guide/keras/rnn) for more detail. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31705\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31705\">No</a>\n", "Currently, this issue is the best documentation on GRUCell usage. GRUCell could really do with some improved documentation.", "@kevindoran Yes you're right, also it took me a while to find this page and understand the outputs of GRUCell"]}, {"number": 31704, "title": "r2.0-CherryPick:Fix LSTMs in TPUStrategy.", "body": "", "comments": []}, {"number": 31703, "title": "[ROCm] updating README.md with information on ROCm Community Supported Builds", "body": "ROCm CSBs have arrived...hallelujah!\r\n\r\nThis PR simply updates README.md file, with details on the ROCm CSB builds. Please review and merge.\r\n\r\n-------------------------------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg @gunan @parallelo \r\n", "comments": []}, {"number": 31702, "title": "[ROCm] updating Dockerfile.rocm to pick a specific version of the rocm libraries ", "body": "The ROCm libraries get updated on a monthly basis, and picking the \"latest\" one is not always the right thing to do.  \r\n\r\nThis is a trivial and ROCm specific change, please review and approve\r\n\r\n------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg @gunan @parallelo @sunway513 ", "comments": []}, {"number": 31701, "title": "ImportError: Could not find 'cudart64_100.dll'", "body": "Hi everyone, I installed `tensorflow-gpu==2.0.0-beta1` according official documentation, but catch this error:\r\n`ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive`\r\n\r\nI made all things from:\r\n- official documentation: https://www.tensorflow.org/install/gpu\r\n- from here: https://medium.com/@teavanist/install-tensorflow-gpu-on-windows-10-5a23c46bdbc7\r\n- Checked `PATH` variable: `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin`\r\n- also have `CUDA_PATH` : `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0` in variables\r\n- file `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudart64_100.dll` exists\r\n- Did system restart\r\n\r\nHow can I fix that?\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tensorflow-gpu==2.0.0-beta1\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN v7.6.2 (July 22, 2019), for CUDA 10.0\r\n- GPU model and memory: MSI 1070 TI\r\n\r\n**### UPD**\r\nWindows reinstall solved the issue", "comments": ["You have to add the cuda/bin and cuda/lib to your path.", "> You have to add the cuda/bin and cuda/lib to your path.\r\n\r\nAlready added, but error still occurs.\r\nAlso tried without x64 in the end of lib record\r\n\r\n![Variables](https://user-images.githubusercontent.com/10476019/63215561-17e5b880-c131-11e9-89fa-793515cb9d6e.png)\r\n", "I meant you additionally need to link your cudnn. Move your inside the cudnn zip is a folder called cuda. Move that to your c:\\  drive then go ahead and add c:\\cuda\\bin and c:\\cuda\\lib to your path. Reboot and you are good to go :)", "> I meant you additionally need to link your cudnn. Move your inside the cudnn zip is a folder called cuda. Move that to your c:\\ drive then go ahead and add c:\\cuda\\bin and c:\\cuda\\lib to your path. Reboot and you are good to go :)\r\n\r\nThat didn't help :(\r\nThe same result when adding x64 to the end\r\n\r\n![Variables](https://user-images.githubusercontent.com/10476019/63216005-46ff2880-c137-11e9-85ec-7075ba5f3da7.png)\r\n\r\n\r\n", "Did you reboot ? Please verify you have the right cudnn for CUDA v10. ", "> Did you reboot ? Please verify you have the right cudnn for CUDA v10.\r\n\r\nI have installed CUDA v10 and cudnn for that version and did reboot after each PATH change. But still have this error.\r\n\r\n![Cuda](https://user-images.githubusercontent.com/10476019/63216080-7c584600-c138-11e9-952f-d2c9828839ff.png)\r\n", "Windows reinstall solved this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31701\">No</a>\n", "Actually, reboot windows solved this issue"]}, {"number": 31700, "title": "Implement Hessian for sparse softmax cross entropy", "body": "This reapplies #22231 with implementation of `_IsZero` in eager mode.", "comments": ["Can one of the admins verify this patch?", "So this was actually Alex's suggestion originally to use @tf.custom_gradient so we don't run into the need to check for zeros. It's a bit complicated to do at the moment; I have a CL out for review which will hopefully make defining second-order gradients with custom_gradient easier. But something like this passes the hessian the test you've written:\r\n\r\n```\r\ndef _sparse_softmax_cross_entropy_with_logits_and_gradients(\r\n    logits, labels, name):\r\n  @custom_gradient.custom_gradient\r\n  def _zeroth_order(unused_logits):\r\n    loss, softmax_grad = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\r\n        logits, labels, name=name)\r\n    def _first_order_wrapper(dloss):\r\n      \"\"\"Swap dloss for logits so we can override a second-order gradient.\"\"\"\r\n      @custom_gradient.custom_gradient\r\n      def _first_order(unused_logits):\r\n        grad = _BroadcastMul(dloss, softmax_grad)\r\n        def _second_order(ddloss):\r\n          softmax_logits = softmax(logits)\r\n          return ((ddloss - array_ops.squeeze(\r\n              math_ops.matmul(\r\n                  array_ops.expand_dims(ddloss, 1),\r\n                  array_ops.expand_dims(softmax_logits, 2)),\r\n              axis=1)) * softmax_logits)\r\n        return grad, _second_order\r\n      return _first_order(logits)\r\n    return loss, _first_order_wrapper\r\n  return _zeroth_order(logits)\r\n```\r\n\r\nThat'd be a helper called instead of gen_nn_ops.sparse_softmax_cross_entropy_with_logits. I think we can do something similar for non-sparse softmax_cross_entropy_with_logits if we split up its labels and logits into separate custom_gradients (since there we need gradients for labels too).\r\n\r\nThe trick for second-order gradients with nested `@tf.custom_gradient` is that the inner custom_gradient needs to have the original/primal input as its argument so its gradient can return a second-order gradient for that primal rather than for `dloss`.\r\n\r\nAny objections to this approach rather than the Tensor-tagging approach? Happy to clean the example up or explain further if it's helpful.", "@MichaelKonobeev gentle ping to check latest comments , thank you", "I will work on it in soon.", "@allenlavoie have the CL which makes defining second order gradients with custom_gradient easier been submitted? If no, I could implement it as in the example you provided. Also are there any ideas why the previous PR broke the convergence of NCF keras model with run_eagerly=True? Maybe I could add a test for it? The Hessian computation itself seems correct after I checked it multiple times.", "Yes, see the documentation for \"primals\" in custom_gradient now:\nhttps://github.com/tensorflow/tensorflow/blob/27d8d592ce2ca9be52690de734c799eb8731cc5d/tensorflow/python/ops/custom_gradient.py#L86\n\nOn Mon, Sep 30, 2019 at 9:26 PM MichaelKonobeev <notifications@github.com>\nwrote:\n\n> @allenlavoie <https://github.com/allenlavoie> have the CL which makes\n> defining second order gradients with custom_gradient easier been submitted?\n> If no, I could implement it as in the example you provided. Also are there\n> any ideas why the previous PR broke the convergence of NCF keras model with\n> run_eagerly=True? Maybe I could add a test for it? The Hessian computation\n> itself seems correct after I checked it multiple times.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31700?email_source=notifications&email_token=AAABHRPPDTJP3CKPVUV6LH3QMLGQ3A5CNFSM4IMLTVRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD774KRY#issuecomment-536855879>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMSPHRSRFGWQYJPRUDQMLGQ3ANCNFSM4IMLTVRA>\n> .\n>\n\n\n-- \n - Alex\n", "@allenlavoie in the example you provided why do we call `_BroadcastMul(dloss, softmax_grad)` inside `_first_order` and not inside `first_order_wrapper`? Wouldn't this lead to a part of computation being skipped when computing second order derivatives like in the following example:\r\n\r\n```{python}\r\n@tf.custom_gradient\r\ndef f(x):\r\n  y, grad_x = x ** 3, 3 * x ** 2\r\n  \r\n  def first_order_grad(dy):\r\n    @tf.custom_gradient\r\n    def first_order_custom(unused_x):\r\n      def second_order_grad(ddy):\r\n        return 6 * x * ddy\r\n      return grad_x * dy, second_order_grad\r\n    return first_order_custom(x)\r\n  \r\n  return y, first_order_grad\r\n\r\nx = tf.Variable([1.])\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n  y = tf.square(f(x))\r\n  dx = tape.gradient(y, x)\r\n\r\nprint(dx) # 6, expected 6\r\nddx = tape.gradient(dx, x)\r\nprint(ddx) # 6, but expected 30\r\n```\r\n\r\nIf multiplication by `dy` is moved one line below, then it works as expected.", "You're right, it needs to be outside the inner custom_gradient. And I need to fix that for `primals=` which will be somewhat tricky.", "A second component that will stop us from being able to solve this problem using tf.custom_gradient is the fact that custom_gradient is not serialized with the graph, so serializing a savedmodel containing a cross entropy loss using tf.cusotm_gradient, when you deserialize you can get either errors or incorrect behavior if taking gradients of the resulting tensor.  So there are two major issues here.", "I don't think the SavedModel issue needs to block this. We will need a workaround, possibly some indication that this custom_gradient is safe to ignore (meaning we'll still only be able to take first-order gradients from SavedModels).", "custom_gradient with primals set should implement vector-Jacobian product, right? Assuming this works in the general case, the downside is that it will prevent some optimizations like the one done in cross entropy gradient function which computes only the product between dependent parts of gradient and Jacobian. I could implement it using the variant with a wrapper function to keep this optimization.\r\n\r\n@allenlavoie could you point me to a way of ignoring custom_gradient for serialization or maybe there are other workarounds?", "Yeah the primals= argument has been removed, I don't see a way to support that API (and it was never in a stable release). I've updated the example in the custom_gradient docstring with your suggested fix.\r\n\r\nThe easiest way to ignore these custom_gradients in SavedModels is probably to set an attribute. I'd make an internal-only version with the extra argument, then [add an attribute](https://github.com/tensorflow/tensorflow/blob/d3c1452077ffaaf2473c12226675788607c600c2/tensorflow/python/ops/while_v2.py#L293) to [this node](https://github.com/tensorflow/tensorflow/blob/d3c1452077ffaaf2473c12226675788607c600c2/tensorflow/python/ops/custom_gradient.py#L387). Then you can delete the _gradient_op_type attribute [here](https://github.com/tensorflow/tensorflow/blob/d3c1452077ffaaf2473c12226675788607c600c2/tensorflow/python/saved_model/function_deserialization.py#L375).", "@MichaelKonobeev  Could you please check reviewer comments and keep us posted. Thanks!", "I implemented Hessian computation through tf.custom_gradient but still need to implement the workaround for SavedModels. Hope to work on this next week. ", "I noticed that when `SparseSoftmaxCrossEntropyWithLogits` from `nn_ops_grad.py` is removed, using `tf.custom_gradient` and then wrapping and decorating the function with `tf.function` causes an exception if working with persistent tape. Consider the following code:\r\n\r\n```\r\n@tf.custom_gradient\r\ndef xent(logits, labels):\r\n  loss, grad = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\r\n      logits, labels)\r\n  def grad_fn(dy):\r\n    return tf.expand_dims(dy, -1) * grad, None\r\n  return loss, grad_fn\r\n\r\n@tf.function\r\ndef function(logits, lables):\r\n  return xent(logits, labels)\r\n\r\nlogits = tf.Variable([[1., 2.]])\r\nlabels = tf.Variable([1])\r\nmodule = Module()\r\nwith tf.GradientTape(persistent=True) as tape:\r\n  loss = function(logits, labels)\r\n\r\ngrad = tape.gradient(loss, logits)\r\n```\r\n\r\nThis will lead to LookupError from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_util.py#L623). If the tape is not persistent or `xent` is called directly, then it works fine. Is this expected behavior? Seems like it is necessary to support this case before moving to savable models as they work with `tf.function`s.\r\n", "Yes; this is the problem I ran into :(  Serializing custom gradients (as\ntf.functions?) would be required to do this; we don't currently have a\ndesign for this.  If it's something you'd like to work on then I believe TF\nteam would welcome a short design.\n\nOn Tue, Dec 10, 2019 at 2:32 PM MichaelKonobeev <notifications@github.com>\nwrote:\n\n> I noticed that when SparseSoftmaxCrossEntropyWithLogits from\n> nn_ops_grad.py is removed, using tf.custom_gradient and then wrapping and\n> decorating the function with tf.function causes an exception if working\n> with persistent tape. Consider the following code:\n>\n> @tf.custom_gradient\n> def xent(logits, labels):\n>   loss, grad = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\n>       logits, labels)\n>   def grad_fn(dy):\n>     return tf.expand_dims(dy, -1) * grad, None\n>   return loss, grad_fn\n>\n> @tf.function\n> def function(logits, lables):\n>   return xent(logits, labels)\n>\n> logits = tf.Variable([[1., 2.]])\n> labels = tf.Variable([1])\n> module = Module()\n> with tf.GradientTape(persistent=True) as tape:\n>   loss = function(logits, labels)\n>\n> grad = tape.gradient(loss, logits)\n>\n> This will lead to LookupError from here\n> <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_util.py#L623>.\n> If the tape is not persistent or xent is called directly, then it works\n> fine. Is this expected behavior? Seems like it is necessary to support this\n> case before moving to savable models as they work with tf.functions.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31700?email_source=notifications&email_token=AANWFGZRHSZ4DXY64RNIORDQYAKGLA5CNFSM4IMLTVRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGRGC2I#issuecomment-564289897>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG2XHEW4RCJLSZHTBC3QYAKGLANCNFSM4IMLTVRA>\n> .\n>\n", "I think in this case `persistent=True` is changing the way function gradients are generated, preparing to possibly take higher-order gradients. Whereas with `persistent=False` it's using the first-order-only path. [This case for first-order](https://github.com/tensorflow/tensorflow/blob/069fd4e66fd5c767a345edd0e9ffba3bf2fc9c3c/tensorflow/python/eager/function.py#L1896) vs. [this case for higher-order](https://github.com/tensorflow/tensorflow/blob/069fd4e66fd5c767a345edd0e9ffba3bf2fc9c3c/tensorflow/python/eager/function.py#L1927).\r\n\r\nThe difference is just that it [goes over the function with tf.gradients multiple times until it doesn't need more side-outputs](https://github.com/tensorflow/tensorflow/blob/069fd4e66fd5c767a345edd0e9ffba3bf2fc9c3c/tensorflow/python/eager/function.py#L1426) for persistent tapes. It adds side outputs in the (successful) first iteration, then tries again to take gradients this time with respect to those side outputs too and that fails for some reason. I'm guessing that the side output at issue is `grad` itself, which of course doesn't have a gradient.\r\n\r\nIf that's a correct diagnosis, then even without the SavedModel issue we won't be able to use custom_gradients which capture undeclared outputs since that's incompatible with our higher-order function gradient code. There are hand-wavey plans to generate function gradients with tapes which might fix this, but I wouldn't hold your breath.\r\n\r\nSo basically that's a good find. Maybe we should reconsider whether custom_gradient is useful for library code as things are; it seems like we can't use it.", "@allenlavoie seems like you're right. The code adds side output tensor with id `'SparseSoftmaxCrossEntropyWithLogits:1'` and passing only this tensor to `_build_functions_for_outputs` leads to the exception. \r\n\r\nWould it be possible to implement Hessian computation by marking zero tensors with an attribute as in this PR? At the same time, to me it's still unclear why the NCF model might fail after the previous PR.", "But what about third-order derivatives? Can you add a\ncompute_gradient_error on the result of calling tf.gradients on tf.hessians?\n\nOn Wed, Jan 8, 2020 at 2:05 PM MichaelKonobeev <notifications@github.com>\nwrote:\n\n> *@MichaelKonobeev* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/nn_grad.py\n> <https://github.com/tensorflow/tensorflow/pull/31700#discussion_r364466301>\n> :\n>\n> > +  # Second derivative is just softmax derivative w.r.t. logits.\n> +  softmax_grad = op.outputs[1]\n> +  grad = _BroadcastMul(grad_loss, softmax_grad)\n> +\n> +  logits = op.inputs[0]\n> +  if (grad_grad is not None\n> +      and not getattr(grad_grad, \"_is_zeros_tensor\", False)):\n> +    softmax = nn_ops.softmax(logits)\n> +\n> +    grad += ((grad_grad - array_ops.squeeze(\n> +        math_ops.matmul(\n> +            array_ops.expand_dims(grad_grad, 1),\n> +            array_ops.expand_dims(softmax, 2)),\n> +        axis=1)) * softmax)\n> +\n> +  return grad, None\n>\n> This None refers to the gradient wrt labels passed as the second input\n> into the operation, isn't it?\n>\n> Because we break out of fused implementation when Hessian is computed, I\n> thought this should work properly when higher order derivatives are\n> requested. I tested it locally now by adding compute_gradient_error for\n> the result of tf.hessians similar to the testSecondGradient test case to\n> be added with this PR and got error around 2.12e-8.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31700?email_source=notifications&email_token=AAABHRLBQAJ7B65ELCJYLZTQ4ZEZRA5CNFSM4IMLTVRKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCRDKTUA#discussion_r364466301>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNJCW55SWOVHCDMOJDQ4ZEZRANCNFSM4IMLTVRA>\n> .\n>\n\n\n-- \n - Alex\n", "`compute_gradient_error` takes as input the input-output pair (`x`, `y`) of a function `f`, where `y=f(x)` and returns the error between computing `dy/dx` and estimating it numerically, that is if we pass it the result of `tf.hessians` in place of `y` we should get the error for computing third order derivatives which is what I did. ", "Sorry, my mistake! Approving now.", "@MichaelKonobeev can you please resolve conflicts ?", "@MichaelKonobeev Can you please resolve conflicts? Thanks!"]}, {"number": 31699, "title": "Ensure native libs are loaded when using NnApiDelegate", "body": "Also resolve an issue where the legacy native Interpreter::useNNAPI API is used for enabling NNAPI from Java.", "comments": []}, {"number": 31698, "title": "Docs could mention that Dataset sharding is deterministic", "body": "Hi everyone,\r\nThanks for your work on maintaining and developing Tensorflow.\r\n\r\nI wish to raise a suggestion for the documentation of the `tf.data.Dataset` Python API.\r\nIn particular, I am referring to the documentation of the `shard` operation.\r\n\r\n## URL(s) with the issue:\r\n\r\nr1.14 docs: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard\r\n\r\nr2.0 docs: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shard\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFrom my understanding of the source code, the `shard` operation is **deterministic**. \r\ni.e. if we apply `shard` on a Dataset A with some fixed values of `num_shards` and `index`, the operation will always return the same subset of Dataset A.\r\n\r\nPerhaps the documentation should mention that this operation is deterministic?\r\nThis will help readers understand that the sharding does not involve any randomness.\r\nCurrently, the docs do not mention this aspect of `shard`'s behaviour.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? Yes\r\n\r\n### Usage example\r\n\r\nIs there a usage example? Yes\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? No, but this issue does not require visuals\r\n\r\n### Submit a pull request?\r\n\r\nI can submit a PR to update the docs - if this is indeed considered a useful fix.\r\n\r\nThanks for your time.", "comments": ["@anubh-v Can you raise a PR to update the docs? Thanks!", "@jvishnuvardhan Sure, I will update the docs and raise a PR in 2-3 days. Thanks.", "Automatically closing this out since I understand it to be resolved by the https://github.com/tensorflow/tensorflow/pull/31897 (merged already), but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31698\">No</a>\n"]}, {"number": 31697, "title": "Tensorflow 2 beta - error save model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190731\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to save a model, I obtain the following errror:\r\n```\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_2:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would like to save the model.\r\n\r\n**Code to reproduce the issue**\r\n\r\nGoogle Colab link: [https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp](https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp)\r\n\r\nInteresting Code:\r\n\r\n```\r\nn = 200\r\ndf = pd.DataFrame(data={'a': [x for x in range(n)], 'b': [x for x in range(n+10,n+n+10)], 'labels': [int(x%2==0) for x in range(n)]})\r\ndf = df.astype({'b': str})\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  \r\n  labels = dataframe.pop('labels')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\ntrain, test = train_test_split(df, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\ntrain_ds = df_to_dataset(train)\r\nval_ds = df_to_dataset(val, shuffle=False)\r\ntest_ds = df_to_dataset(test, shuffle=False)\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\nfor c in df.columns:\r\n  if c == 'labels':\r\n    continue\r\n  elif c == 'b':\r\n    el = feature_column.categorical_column_with_vocabulary_list(c, df[c].unique(), default_value=-10)\r\n    el_one_hot = feature_column.indicator_column(el)\r\n    feature_columns.append(el_one_hot)\r\n    feature_layer_inputs[c] = tf.keras.Input(shape=(1,), name=c, dtype=tf.string)\r\n  elif c == 'a':\r\n    feature_columns.append(feature_column.numeric_column(c, default_value=-10))\r\n    feature_layer_inputs[c] = Input(shape=(1,), name=c)\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\nf_layer = feature_layer(feature_layer_inputs)\r\ninput = [v for v in feature_layer_inputs.values()]\r\nx = Dense(2048, activation='relu')(f_layer)\r\nx = Dropout(0.5)(x)\r\nout = Dense(1, activation='sigmoid')(x)\r\nmodel = Model(inputs=[input], outputs=out)\r\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])\r\nmodel.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)\r\nmodel.save('aaa.model')\r\n```\r\n\r\n", "comments": ["Is it possible for you to try latest TF versions (!pip install tf-nightly-2.0-preview==2.0.0.dev20190818) and let us know whether the issue persists? . I am able to save the model in recent nightly versions.Thanks!", "With the latest TF version, there are no problems, thx!"]}, {"number": 31696, "title": "tf.gather not supported when both params dtype is int and params is ResourceVariable", "body": "`tf.gather` has both GPU and CPU kernels for params dtype int64 / int32, however, when the params is a tensor wrapped in a variable it does not work when dtype is int32 or int64.\r\n\r\nWrapping the params input in e.g. a `tf.identity` solves the issue, but the behaviour seems like a bug.\r\n\r\nMinimal case to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\ntensor_float = tf.range(10, dtype=tf.float32)\r\ntensor_int = tf.range(10, dtype=tf.int64)\r\n\r\nvar_float = tf.Variable(tensor_float)\r\nvar_int = tf.Variable(tensor_int)\r\n\r\nfor i in [tensor_float, tensor_int, var_float, var_int]:\r\n    try:\r\n        tf.gather(i, tf.constant([[1]], dtype=tf.int64))\r\n        print('worked for {}'.format(i))\r\n    except tf.errors.NotFoundError as e:\r\n        print('didn\\'t work for {}'.format(i))\r\n        print(e)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n2.0.0-beta1\r\nworked for [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\r\nworked for [0 1 2 3 4 5 6 7 8 9]\r\nworked for <tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)>\r\ndidn't work for <tf.Variable 'Variable:0' shape=(10,) dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>\r\nNo registered 'ResourceGather' OpKernel for GPU devices compatible with node {{node ResourceGather}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: Tindices=DT_INT64, batch_dims=0, dtype=DT_INT64, validate_indices=true\r\n\t.  Registered:  device='XLA_CPU'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_GPU'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_CPU_JIT'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_GPU_JIT'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_QINT32]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_QINT32]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_QUINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_QUINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_QINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_QINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT32]\r\n [Op:ResourceGather] name: Gather/\r\n```", "comments": ["Created a PR #31734 to add int64 (and other types) for ResourceGather.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31696\">No</a>\n"]}, {"number": 31695, "title": "Is it possible to dynamically switch between CPU and GPU?", "body": "I have a code that does training/test together.\r\n\r\nHowever, the eager execution mode seems taking more GPU memory than I have when inference is taken place.\r\n\r\nI need to train the data in GPU and do inference in CPU. How can I do this in TensorFlow? Thank you!", "comments": ["@zhulingchen ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!\r\n", "I think I solved my problem:\r\n\r\nI was intending to use `model(x_test)` to do some inference with TensorFlow Probability `tfp` (when the outputs are `tfp.distributions` and with eager execution enabled as a requirement) since `tfp` does not allow me to use `model.predict(x_test)` for `tfp.distributions` outputs yet. But `model(x_test)` reports GPU Resource OOM issue but `model.predict(x_test)` for the regular non-`tfp` model on the same test data does not have such issue.\r\n\r\nSo at the very beginning, I wanted to do such inference using `model(x_test)` with CPU instead of GPU to solve the problem. But later, I realized that `model.predict(x_test)` does the inference in a batch-by-batch manner (default batch size is 32) but `model(x_test)` has no such mechanism at all. So finally I designed my manual batch-by-batch inference mechanism for `model(x_test)` and everything now can smoothly work in GPU now. No more GPU Resource OOM issue.\r\n\r\nThe version of my TensorFlow is 1.13.1 and the version of my TensorFlow Probability is 0.6.0.", "Closing since the issue is resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31695\">No</a>\n", "@oanush This issue should be re-opened because \r\n\r\n1. It's not really been fixed (AFAIK)\r\n2. the method `predict` cannot be used with TensorFlow Probability yet"]}, {"number": 31694, "title": "Check failed: array->has_shape()", "body": "Hi,\r\n\r\nFirst, Thanks for all your work! i'm struggling to export my tf model to tflite, seeing the error, i think i got 2 problems : \r\n- input format\r\n- unsupported ops\r\nI can't find a way to solve this, maybe the issue is to do a more compatible tflite export when freezing the graph (close to something you did for export_tflite_ssd_graph.py but not sure i can make something equivalent for my case) \r\n\r\nif you can help me for documentations and or good workaround...\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): googlecolab Window 10 x64 in conda venv\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.14.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-08-16 13:17:37.784025: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794406: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794453: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794480: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794526: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794563: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794592: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794619: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794644: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794669: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794689: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794714: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794740: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794765: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.794790: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.819192: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: RFFT\r\n2019-08-16 13:17:37.819259: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: ComplexAbs\r\n2019-08-16 13:17:37.819678: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\n2019-08-16 13:17:37.819727: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-08-16 13:17:37.819759: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\n2019-08-16 13:17:37.819787: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-08-16 13:17:37.819814: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.819844: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.819871: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.819896: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.819921: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.819942: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-08-16 13:17:37.819963: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.819983: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-08-16 13:17:37.820005: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3\r\n2019-08-16 13:17:37.820048: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-08-16 13:17:37.820076: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond\r\n2019-08-16 13:17:37.820127: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit\r\n2019-08-16 13:17:37.820214: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3\r\n2019-08-16 13:17:37.820241: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3\r\n2019-08-16 13:17:37.820319: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\r\n2019-08-16 13:17:37.820434: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2019-08-16 13:17:37.829208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 420 operators, 721 arrays (0 quantized)\r\n2019-08-16 13:17:37.841887: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 401 operators, 686 arrays (0 quantized)\r\n2019-08-16 13:17:37.854995: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 401 operators, 686 arrays (0 quantized)\r\n2019-08-16 13:17:37.897923: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 171 operators, 321 arrays (0 quantized)\r\n2019-08-16 13:17:37.903043: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 153 operators, 287 arrays (0 quantized)\r\n2019-08-16 13:17:37.905901: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 150 operators, 281 arrays (0 quantized)\r\n2019-08-16 13:17:37.908277: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 150 operators, 281 arrays (0 quantized)\r\n2019-08-16 13:17:37.909972: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 150 operators, 281 arrays (0 quantized)\r\n2019-08-16 13:17:37.912697: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 704 bytes, theoretical optimal value: 512 bytes.\r\n2019-08-16 13:17:37.913694: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXPAND_DIMS, FILL, FLOOR_DIV, FULLY_CONNECTED, GATHER, LESS, LOG, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, REVERSE_V2, SHAPE, SPARSE_TO_DENSE, SPLIT_V, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, ComplexAbs, Enter, Exit, LoopCond, Merge, RFFT, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXPAND_DIMS, FILL, FLOOR_DIV, FULLY_CONNECTED, GATHER, LESS, LOG, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, REVERSE_V2, SHAPE, SPARSE_TO_DENSE, SPLIT_V, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, ComplexAbs, Enter, Exit, LoopCond, Merge, RFFT, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n\r\n\r\nhere is the link to the shared notebook : https://colab.research.google.com/drive/1FhMBJHynRVfd_o-q2JH6Wn867YmnomjY\r\n\r\nthe frozen model .pb is available there : https://we.tl/t-1mwjdnJfxe\r\n\r\nThank you for helping, you're doing a great job\r\n\r\nRaphael\r\n", "comments": ["It looks like you need to freeze the model first.", "Thanks for helping, did you looked at the shared notebook, the model is already frozen to a *.pb file, did i missed something?", "Sachin, do you mind taking a look on this (since this is related with exporting ssd graph)? Thanks!", "@kezakool Looks like you have a bunch of ops that are not supported :-). The vanilla response would be for you to implement custom ops for all these. However, we are adding better support for Control Flow (Merge, Switch, etc) & TensorList ops by next year, with an improved converter & builtin TFLite ops. However, these structures are *usually* required when you have ambiguous shapes in the graph. If you are training/exporting this model to a `.pb`, you could try 'freezing' some of these shapes?\r\nAlso, if binary size isn't a factor for you, you could also try using the [Select TF Ops in TFLite](https://www.tensorflow.org/lite/guide/ops_select) option.\r\n\r\nI know this isn't the ideal answer, but its usually better to work with a simpler model than try to convert a complex model, albeit at a loss of accuracy :-)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31694\">No</a>\n", "@srjoglekar246, thanks for helping me! :)\r\n\r\n What do you mean with \"you could try 'freezing' some of these shapes?\" can you describe a bit please?, i'm not sure to get it\r\n\r\nAlso, with your suggestion, I tried to convert using  target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nand i still get this error : \r\nHere is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, Merge, Switch.\r\n\r\nand if i set allow_custom_ops = True, i get this error :  \r\nException: TensorFlow Lite currently doesn't support control flow ops: Merge, Switch.\r\n\r\nthis come from the LSTM layer of my graph, do you know an implementation that doesn't have switch and merge ops?\r\n\r\nMany thanks\r\n\r\n", "So ops like Merge & Switch come from Tensorflow's old (and complex) style of doing IF/WHILE loops - called Control Flow v1. Last year, they implemented a (newer) version called Control Flow v2 (watch [this](https://www.youtube.com/watch?v=IzKXEbpT9Lg) video if you want to learn more), which greatly simplifies these ops - you essentially just have ops called IF/WHILE instead of Merge, Switch, NextIteration, etc.\r\n\r\nIf you want to use Control Flow v2, you will have to do the following:\r\n1. Train your model again by setting [this flag](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_util.py#L32) to True.\r\n2. Convert with the (yet experimental) [MLIR converter](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/lite), that supports Control Flow v2.\r\nOR\r\nYou could change your graph a bit by removing statements like these:\r\n`tf.unstack(x ,time_steps,1,name=\"input_tensor\")` , which insert control flow ops into the model. @haozha111 , what can they do to support LSTM?\r\n\r\nTBH the best way forward for you would be to get rid of control flow from your graph, and instead do the iterations 'in code' - so build a model to return a state & output, with the former being fed back into the model at the next stage - not the ideal way to do things, but the easiest to go forward.", "Thanks for investing this Sachin, is ControlFlow v2 compatible with Tensorflow 1.14?\r\n\r\nBefore seeing your answer, I dug into into the differents LSTM components availables, and using BasicLSTMCell with Static_rnn seems to be a solution (still have to tune it) and doesn't use Merge/Switch.\r\nWith that, the only piece missing is the CTC_BEAM_SEARCH_DECODER op, i saw this issue :\r\nhttps://github.com/tensorflow/tensorflow/issues/28045\r\nSo it seems its a fixed bug, but i still got the issue...\r\n\r\nhere is the updated graph in case you need it : https://we.tl/t-dKBgCsXhHt\r\n\r\nThank you for your time\r\n\r\nRaphael"]}, {"number": 31693, "title": "Error in example of tensorflow lite", "body": "Hello, I got an error in code while tried to run object detection. Let me know how to fix this\r\n![Screenshot (15)](https://user-images.githubusercontent.com/39325207/63173591-d6d0a400-c05d-11e9-807b-42d1c4cc626b.png)\r\n code", "comments": ["@KanwarpartapSinghBimrah, Please provide the information asked in the [template](https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md). Thanks!", "Yes please fill in details on how to reproduce the issue, the version you're using etc. thx", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31693\">No</a>\n"]}, {"number": 31692, "title": "Rectified Adam in tensorflow", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n1.13\r\nNo \r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nPytorch has already Rectified Adam implemented\r\nCan you add it ?\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nAll people in the world.\r\n\r\n**Any Other info.**\r\n", "comments": ["[PR](https://github.com/tensorflow/tensorflow/pull/31740#issue-308393916) has been already raised for the changes.", "Thanks.\nPyTorch has already has it for months..\nIt will be a huge improvement\n\n\n> On Aug 19, 2019, at 22:09, oanush <notifications@github.com> wrote:\n> \n> PR has been raised for the changes.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@arita37 and @nhuthai Please raise a PR in addons repository [here](https://github.com/tensorflow/addons/pulls) and post a link here. Thanks!", "@arita37 Please close this issue if you have raised PR in addons repo. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "When will you inlcude Rectified Adam\nin TF ?\n\nThanks\n\n> On Sep 10, 2019, at 23:00, Vishnuvardhan Janapati <notifications@github.com> wrote:\n> \n> Closed #31692.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n"]}, {"number": 31691, "title": "TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. ", "body": "import tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\ntf.reset_default_graph()\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport random\r\nimport scipy\r\nfrom datetime import datetime\r\nimport progressbar\r\nfrom image_data_handler_joint_multimodal import ImageDataHandler\r\nfrom resnet18 import ResNet\r\nfrom layer_blocks import *\r\nfrom utils import flat_shape, count_params, log_file\r\nfrom imgaug import augmenters as iaa\r\nfrom keras.objectives import categorical_crossentropy\r\nfrom keras.layers import GRU, Dense\r\nfrom keras.metrics import categorical_accuracy\r\nfrom keras import backend as K\r\nimport numpy as np\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\nos.environ['CUDA_VISIBLE_DEVICES']='0,2,3'\r\n\r\n\"\"\" Configuration setting \"\"\"\r\ntf.set_random_seed(7)\r\nconfig=tf.ConfigProto()\r\n\r\n# Data-related params\r\nif len(sys.argv) != 3:\r\n    print(\"The script requires 2 arguments: (1) the dataset root directory and (2) the parameters root directory.\")\r\n#dataset_root_dir = sys.argv[1] #'/mnt/datasets/ocid_dataset'\r\n#params_root_dir = sys.argv[2] #'/mnt/params/models'\r\ndataset_root_dir='/home/users/xdlan/myexperiment/rcfusion_master/ocid_dataset_owncloud'\r\nparams_root_dir='/home/users/xdlan/myexperiment/rcfusion_master/resnet18_ocid_params'\r\n\r\n\r\ndataset_train_dir_rgb = dataset_root_dir + '/ARID20_crops/squared_rgb/'\r\ndataset_val_dir_rgb = dataset_root_dir + '/ARID10_crops/squared_rgb/'\r\nparams_dir_rgb = params_root_dir + '/resnet18_ocid_rgb++_params.npy'\r\n\r\n\r\ndataset_train_dir_depth = dataset_root_dir + '/ARID20_crops/surfnorm++/'\r\ndataset_val_dir_depth = dataset_root_dir + '/ARID10_crops/surfnorm++/'\r\nparams_dir_depth = params_root_dir + '/resnet18_ocid_surfnorm++_params.npy'\r\n\r\n\r\ntrain_file = dataset_root_dir + '/split_files_and_labels/arid20_clean_sync_instances.txt'\r\nval_file = dataset_root_dir + '/split_files_and_labels/arid10_clean_sync_instances.txt'\r\n\r\n# Log params\r\nlog_dir = '../log/'\r\nif not os.path.exists(log_dir):\r\n    os.makedirs(log_dir)\r\nlog = [\"epoch\", \"train_loss\", \"val_loss\", \"val_acc\"]\r\ntensorboard_log = '/tmp/tensorflow/'\r\n\r\n# Solver params\r\nlearning_rate = [[0.0001]]\r\nnum_epochs = 50\r\nbatch_size = [[16]]\r\nnum_neurons = [[100]]\r\nl2_factor = [[0.0]]\r\nmaximum_norm = [[4]]\r\ndropout_rate = [[0.4]]\r\n\r\ndepth_transf = [[256]]\r\ntransf_block = transformation_block_v1\r\n\r\n# Checkpoint dir\r\ncheckpoint_dir = \"/tmp/my_caffenet/\"\r\nif not os.path.isdir(checkpoint_dir): os.mkdir(checkpoint_dir)\r\n\r\n# Input/Output\r\nnum_classes = 49\r\nimg_size = [224, 224]\r\nnum_channels = 3\r\n\r\n\r\n\"\"\" Online data augmentation \"\"\"\r\n\r\ndef random_choice(start, end , _multiply ):\r\n    start = start * _multiply\r\n    end = end * _multiply\r\n    num = random.randrange(start,end + 1 ,1)\r\n    #print (\"il num e'\",num/_multiply)\r\n    return float( num / _multiply)\r\ndef x_y_random_image(image):\r\n    width = image.shape[1] \r\n    hight = image.shape[0]\r\n\r\n    border_x = int( ( 256 * 5 ) / 100.0)\r\n    border_y = int( ( 256 * 9 ) / 100.0)\r\n\r\n    pos_x = random.randrange(0 + border_x , width - border_x , 1)\r\n    pos_y = random.randrange(0 + border_y, hight - border_y , 1)\r\n    #print (\"la pos e' \", pos_x , pos_y)\r\n    return pos_x , pos_y\r\n\r\ndef data_aug(batch , batch_depth):\r\n    num_img = batch.shape[0]\r\n    list = []\r\n    list_depth = []\r\n    for i in range(num_img):\r\n        val_fliplr = random.randrange(0,2,1) #in questo modo il due non e compreso e restituisce i valori 0 o 1 \r\n        list.extend([iaa.Fliplr( val_fliplr )])\r\n        list_depth.extend([iaa.Fliplr( val_fliplr )])\r\n        \r\n        val_fliplr = random.randrange(0,2,1) #in questo modo il due non e compreso e restituisce i valori 0 o 1 \r\n        list.extend([iaa.Flipud( val_fliplr )])\r\n        list_depth.extend([iaa.Flipud( val_fliplr )])\r\n        \r\n        val_scala = random.randrange(5 , 11 , 1)\r\n        val = float (val_scala / 10.0) \r\n        list.extend([iaa.Affine( val , mode = 'edge')])\r\n        list.extend([iaa.Affine( 10.0 / val_scala , mode = 'edge')])\r\n        list_depth.extend([iaa.Affine( val , mode = 'edge')])\r\n        list_depth.extend([iaa.Affine( 10.0 / val_scala , mode = 'edge')])\r\n        \r\n        val_rotation = random.randrange( -180, 181 , 90)\r\n        list.extend( [ iaa.Affine( rotate = val_rotation ,mode = 'edge') ] )\r\n        list_depth.extend( [ iaa.Affine( rotate = val_rotation ,mode = 'edge') ] )\r\n        \r\n        augseq = iaa.Sequential(list)\r\n        batch[i] = augseq.augment_image( batch[i] )\r\n        augseq_depth = iaa.Sequential(list)\r\n        batch_depth[i] = augseq_depth.augment_image( batch_depth[i])\r\n        \r\n        list=[]\r\n        list_depth = []\r\n\r\n\r\n\"\"\" Loop for gridsearch of hyper-parameters \"\"\"\r\n\r\nset_params = [lr+nn+bs+aa+mn+do+dt for lr in learning_rate for nn in num_neurons for bs in batch_size for aa in l2_factor for mn in maximum_norm for do in dropout_rate for dt in depth_transf]\r\n\r\nfor hp in set_params:\r\n\r\n    lr = hp[0]\r\n    nn = hp[1]\r\n    bs = hp[2]\r\n    aa = hp[3]\r\n    mn = hp[4]\r\n    do = hp[5]\r\n    dt = hp[6]\r\n\r\n    \"\"\" Data management \"\"\"\r\n\r\n    # Place data loading and preprocessing on the cpu\r\n    #with tf.device('/cpu:0'):\r\n    dataset_train_dir = [dataset_train_dir_rgb, dataset_train_dir_depth]\r\n    dataset_val_dir = [dataset_val_dir_rgb, dataset_val_dir_depth]\r\n\r\n    tr_data = ImageDataHandler(\r\n        train_file,\r\n        data_dir=dataset_train_dir,\r\n        params_dir=params_root_dir,\r\n        img_size=img_size,\r\n        batch_size=bs,\r\n        num_classes=num_classes,\r\n        shuffle=True,\r\n        random_crops=False)\r\n\r\n    val_data = ImageDataHandler(\r\n        val_file,\r\n        data_dir=dataset_val_dir,\r\n        params_dir=params_root_dir,\r\n        img_size=img_size,\r\n        batch_size=bs,\r\n        num_classes=num_classes,\r\n        shuffle=False,\r\n        random_crops=False)\r\n\r\n    # Create a re-initializable iterator given the dataset structure\r\n    # no need for two different to deal with training and val data,\r\n    # just two initializers\r\n    iterator = tf.data.Iterator.from_structure(tr_data.data.output_types,\r\n                                       tr_data.data.output_shapes)\r\n    next_batch = iterator.get_next()\r\n\r\n    # Ops for initializing the two different iterators\r\n    training_init_op = iterator.make_initializer(tr_data.data)\r\n    validation_init_op = iterator.make_initializer(val_data.data)\r\n\r\n    # Get the number of training/validation steps per epoch\r\n    tr_batches_per_epoch = int(np.floor(tr_data.data_size/bs))\r\n    val_batches_per_epoch = int(np.floor(val_data.data_size/bs))\r\n\r\n    init_op_rgb = {'training': training_init_op, 'validation': validation_init_op}\r\n    batches_per_epoch = {'training': tr_batches_per_epoch, 'validation': val_batches_per_epoch}\r\n\r\n    config.gpu_options.allow_growth=True\r\n    config.gpu_options.per_process_gpu_memory_fraction=0.8\r\n    config.allow_soft_placement=True\r\n    \r\n    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\r\n  \r\n    # Log vars\r\n    log_epoch = []\r\n    log_train_loss = []\r\n    log_val_loss = []\r\n    log_val_acc = []\r\n\r\n\r\n    # Start Tensorflow session\r\n    #with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, device_count={'GPU':1})) as sess:\r\n    with tf.Session(config=config) as sess: \r\n        \"\"\" Network definition \"\"\"        \r\n\r\n        ## RGB branch\r\n\r\n        # TF placeholder for graph input and output\r\n        x_rgb = tf.placeholder(tf.float32, [None, img_size[0], img_size[1], 3])\r\n        x_depth = tf.placeholder(tf.float32, [None, img_size[0], img_size[1], 3])\r\n        y = tf.placeholder(tf.float32, [None, num_classes])\r\n\r\n        x_rgb = tf.reshape(x_rgb, [-1, img_size[0], img_size[1], 3])\r\n        x_depth = tf.reshape(x_depth, [-1, img_size[0], img_size[1], 3])\r\n        y = tf.reshape(y, [-1, num_classes])\r\n\r\n        keep_prob = tf.placeholder(tf.float32)\r\n        training_phase = tf.placeholder(tf.bool)\r\n\r\n        # Max norm\r\n        ckn = np.inf\r\n        rkc = mn#np.inf\r\n        rrc = mn#np.inf\r\n        dkc = mn#np.inf\r\n        conv_kernel_constraint = tf.keras.constraints.MaxNorm(ckn, axis=[0,1,2])\r\n        rnn_kernel_constraint = tf.keras.constraints.MaxNorm(rkc, axis=[0,1])\r\n        rnn_recurrent_constraint = tf.keras.constraints.MaxNorm(rrc, axis=1)\r\n        dense_kernel_constraint = tf.keras.constraints.MaxNorm(dkc, axis=0)\r\n\r\n        # Initialize models\r\n        with tf.variable_scope('rgb', reuse=None):\r\n            model_rgb = ResNet(x_rgb, num_classes, mode='rgb')\r\n        with tf.variable_scope('depth', reuse=None):\r\n            model_depth = ResNet(x_depth, num_classes, mode='depth')\r\n\r\n        # Extract features\r\n        res1_rgb = model_rgb.relu1\r\n        inter_res2_rgb = model_rgb.inter_res2\r\n        res2_rgb = model_rgb.res2\r\n        inter_res3_rgb = model_rgb.inter_res3\r\n        res3_rgb = model_rgb.res3\r\n        inter_res4_rgb = model_rgb.inter_res4\r\n        res4_rgb = model_rgb.res4\r\n        inter_res5_rgb = model_rgb.inter_res5\r\n        res5_rgb = model_rgb.res5\r\n\r\n        pool2_flat_rgb = model_rgb.pool2_flat\r\n\r\n        res1_depth = model_depth.relu1\r\n        inter_res2_depth = model_depth.inter_res2\r\n        res2_depth = model_depth.res2\r\n        inter_res3_depth = model_depth.inter_res3\r\n        res3_depth = model_depth.res3\r\n        inter_res4_depth = model_depth.inter_res4\r\n        res4_depth = model_depth.res4\r\n        inter_res5_depth = model_depth.inter_res5\r\n        res5_depth = model_depth.res5\r\n\r\n        pool2_flat_depth = model_depth.pool2_flat\r\n\r\n        # Conv1x1\r\n        with tf.variable_scope('conv1x1'):\r\n            \r\n            #depth_transf = 64\r\n            \r\n            relu_conv1x1_res1_rgb = transformation_block(res1_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res1')\r\n            relu_conv1x1_inter_res2_rgb = transf_block(inter_res2_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res2')\r\n            relu_conv1x1_res2_rgb = transf_block(res2_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res2') \r\n            relu_conv1x1_inter_res3_rgb = transf_block(inter_res3_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res3')\r\n            relu_conv1x1_res3_rgb = transf_block(res3_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res3')\r\n            relu_conv1x1_inter_res4_rgb = transf_block(inter_res4_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res4')\r\n            relu_conv1x1_res4_rgb = transf_block(res4_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res4')\r\n            relu_conv1x1_inter_res5_rgb = transf_block(inter_res5_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res5')\r\n            relu_conv1x1_res5_rgb = transf_block(res5_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res5')\r\n            \r\n            relu_conv1x1_res1_depth = transformation_block(res1_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res1')\r\n            relu_conv1x1_inter_res2_depth = transf_block(inter_res2_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res2')\r\n            relu_conv1x1_res2_depth = transf_block(res2_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res2')\r\n            relu_conv1x1_inter_res3_depth = transf_block(inter_res3_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res3')\r\n            relu_conv1x1_res3_depth = transf_block(res3_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res3')\r\n            relu_conv1x1_inter_res4_depth = transf_block(inter_res4_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res4')\r\n            relu_conv1x1_res4_depth = transf_block(res4_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res4')\r\n            relu_conv1x1_inter_res5_depth = transf_block(inter_res5_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res5')\r\n            relu_conv1x1_res5_depth = transf_block(res5_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res5')\r\n \r\n        relu_conv1x1_res1_rgb = tf.reshape(relu_conv1x1_res1_rgb, [-1, flat_shape(relu_conv1x1_res1_rgb)])\r\n        relu_conv1x1_inter_res2_rgb = tf.reshape(relu_conv1x1_inter_res2_rgb, [-1, flat_shape(relu_conv1x1_inter_res2_rgb)])\r\n        relu_conv1x1_res2_rgb = tf.reshape(relu_conv1x1_res2_rgb, [-1, flat_shape(relu_conv1x1_res2_rgb)])\r\n        relu_conv1x1_inter_res3_rgb = tf.reshape(relu_conv1x1_inter_res3_rgb, [-1, flat_shape(relu_conv1x1_inter_res3_rgb)])\r\n        relu_conv1x1_res3_rgb = tf.reshape(relu_conv1x1_res3_rgb, [-1, flat_shape(relu_conv1x1_res3_rgb)])\r\n        relu_conv1x1_inter_res4_rgb = tf.reshape(relu_conv1x1_inter_res4_rgb, [-1, flat_shape(relu_conv1x1_inter_res4_rgb)])\r\n        relu_conv1x1_res4_rgb = tf.reshape(relu_conv1x1_res4_rgb, [-1, flat_shape(relu_conv1x1_res4_rgb)])\r\n        relu_conv1x1_inter_res5_rgb = tf.reshape(relu_conv1x1_inter_res5_rgb, [-1, flat_shape(relu_conv1x1_inter_res5_rgb)])\r\n        relu_conv1x1_res5_rgb = tf.reshape(relu_conv1x1_res5_rgb, [-1, flat_shape(relu_conv1x1_res5_rgb)])\r\n\r\n        relu_conv1x1_res1_depth = tf.reshape(relu_conv1x1_res1_depth, [-1, flat_shape(relu_conv1x1_res1_depth)])\r\n        relu_conv1x1_inter_res2_depth = tf.reshape(relu_conv1x1_inter_res2_depth, [-1, flat_shape(relu_conv1x1_inter_res2_depth)])\r\n        relu_conv1x1_res2_depth = tf.reshape(relu_conv1x1_res2_depth, [-1, flat_shape(relu_conv1x1_res2_depth)])\r\n        relu_conv1x1_inter_res3_depth = tf.reshape(relu_conv1x1_inter_res3_depth, [-1, flat_shape(relu_conv1x1_inter_res3_depth)])\r\n        relu_conv1x1_res3_depth = tf.reshape(relu_conv1x1_res3_depth, [-1, flat_shape(relu_conv1x1_res3_depth)])\r\n        relu_conv1x1_inter_res4_depth = tf.reshape(relu_conv1x1_inter_res4_depth, [-1, flat_shape(relu_conv1x1_inter_res4_depth)])\r\n        relu_conv1x1_res4_depth = tf.reshape(relu_conv1x1_res4_depth, [-1, flat_shape(relu_conv1x1_res4_depth)])\r\n        relu_conv1x1_inter_res5_depth = tf.reshape(relu_conv1x1_inter_res5_depth, [-1, flat_shape(relu_conv1x1_inter_res5_depth)])\r\n        relu_conv1x1_res5_depth = tf.reshape(relu_conv1x1_res5_depth, [-1, flat_shape(relu_conv1x1_res5_depth)])\r\n\r\n        # RGB and depth pipelines' merge point\r\n        relu_conv1x1_res1 = tf.concat([relu_conv1x1_res1_rgb, relu_conv1x1_res1_depth], axis=1)\r\n        relu_conv1x1_inter_res2 = tf.concat([relu_conv1x1_inter_res2_rgb, relu_conv1x1_inter_res2_depth], axis=1)\r\n        relu_conv1x1_res2 = tf.concat([relu_conv1x1_res2_rgb, relu_conv1x1_res2_depth], axis=1)\r\n        relu_conv1x1_inter_res3 = tf.concat([relu_conv1x1_inter_res3_rgb, relu_conv1x1_inter_res3_depth], axis=1)\r\n        relu_conv1x1_res3 = tf.concat([relu_conv1x1_res3_rgb, relu_conv1x1_res3_depth], axis=1)\r\n        relu_conv1x1_inter_res4 = tf.concat([relu_conv1x1_inter_res4_rgb, relu_conv1x1_inter_res4_depth], axis=1)\r\n        relu_conv1x1_res4 = tf.concat([relu_conv1x1_res4_rgb, relu_conv1x1_res4_depth], axis=1)\r\n        relu_conv1x1_inter_res5 = tf.concat([relu_conv1x1_inter_res5_rgb, relu_conv1x1_inter_res5_depth], axis=1)\r\n        relu_conv1x1_res5 = tf.concat([relu_conv1x1_res5_rgb, relu_conv1x1_res5_depth], axis=1)\r\n\r\n        rnn_input = tf.stack([relu_conv1x1_res1, relu_conv1x1_inter_res2, relu_conv1x1_res2, relu_conv1x1_inter_res3, relu_conv1x1_res3, relu_conv1x1_inter_res4, relu_conv1x1_res4, relu_conv1x1_inter_res5, relu_conv1x1_res5], axis=1)\r\n\r\n\r\n        # Recurrent net\r\n        with tf.variable_scope(\"rnn\"):\r\n            rnn_h = GRU(nn, activation='tanh', dropout=do, recurrent_dropout=do, name=\"rnn_h\", \r\n                        kernel_constraint=rnn_kernel_constraint, recurrent_constraint=rnn_recurrent_constraint)(rnn_input)\r\n            preds = Dense(num_classes, activation='softmax', kernel_constraint=dense_kernel_constraint)(rnn_h)\r\n\r\n        # Include keras-related metadata in the session\r\n        K.set_session(sess)\r\n\r\n        # Define trainable variables\r\n        trainable_variables_rnn = [v for v in tf.trainable_variables(scope=\"rnn\")]\r\n        trainable_variables_conv1x1 = [v for v in tf.trainable_variables(scope=\"conv1x1\")]\r\n        #trainable_variables_rgb = [v for v in tf.trainable_variables(scope=\"rgb\")]\r\n        #trainable_variables_depth = [v for v in tf.trainable_variables(scope=\"depth\")]\r\n\r\n        # Define the training and validation opsi\r\n        global_step = tf.Variable(0, trainable=False)\r\n        increment_global_step = tf.assign(global_step, global_step+1)\r\n        lr_boundaries = [int(num_epochs*tr_batches_per_epoch*0.5)]#, int(num_epochs*tr_batches_per_epoch*0.6)]\r\n        lr_values = [lr, lr/10]\r\n        decayed_lr = tf.train.piecewise_constant(global_step, lr_boundaries, lr_values)\r\n\r\n        # L2-regularization\r\n        alpha_rnn = aa\r\n        alpha_conv1x1 = aa\r\n        l2_rnn = tf.add_n([tf.nn.l2_loss(tv_rnn) for tv_rnn in trainable_variables_rnn\r\n                                                    if 'bias' not in tv_rnn.name])*alpha_rnn\r\n        l2_conv1x1 = tf.add_n([tf.nn.l2_loss(tv_conv1x1) for tv_conv1x1 in trainable_variables_conv1x1\r\n                                                    if 'bias' not in tv_conv1x1.name])*alpha_conv1x1\r\n        \r\n\r\n        # F2-norm\r\n        #rgb_nodes = [relu_conv1x1_inter_res2_rgb, relu_conv1x1_res2_rgb, relu_conv1x1_inter_res3_rgb, relu_conv1x1_res3_rgb, relu_conv1x1_inter_res4_rgb, relu_conv1x1_res4_rgb, relu_conv1x1_inter_res5_rgb, relu_conv1x1_res5_rgb]\r\n        rgb_nodes = [relu_conv1x1_res1_rgb, relu_conv1x1_inter_res2_rgb, relu_conv1x1_res2_rgb, relu_conv1x1_inter_res3_rgb, relu_conv1x1_res3_rgb, relu_conv1x1_inter_res4_rgb, relu_conv1x1_res4_rgb, relu_conv1x1_inter_res5_rgb, relu_conv1x1_res5_rgb]\r\n        #depth_nodes = [relu_conv1x1_inter_res2_depth, relu_conv1x1_res2_depth, relu_conv1x1_inter_res3_depth, relu_conv1x1_res3_depth, relu_conv1x1_inter_res4_depth, relu_conv1x1_res4_depth, relu_conv1x1_inter_res5_depth, relu_conv1x1_res5_depth]\r\n        depth_nodes = [relu_conv1x1_res1_depth, relu_conv1x1_inter_res2_depth, relu_conv1x1_res2_depth, relu_conv1x1_inter_res3_depth, relu_conv1x1_res3_depth, relu_conv1x1_inter_res4_depth, relu_conv1x1_res4_depth, relu_conv1x1_inter_res5_depth, relu_conv1x1_res5_depth]\r\n        reg = tf.Variable(0.0, trainable=False)\r\n        \r\n        #sigm_reg = 0.0001*tf.sigmoid(((12*tf.cast(global_step,tf.float32))/(num_epochs*tr_batches_per_epoch))-6, name='reg_sigmoid')\r\n        sigm_reg = 1.0\r\n        reg_values = [sigm_reg, 0.0]\r\n        decayed_reg = tf.train.piecewise_constant(global_step, lr_boundaries, reg_values)\r\n        #recompute_reg = tf.assign(reg, sigm_reg)\r\n        reg_lambda = [0.0*decayed_reg, 0.0*decayed_reg, 1e-6*decayed_reg, 1e-5*decayed_reg, 1e-5*decayed_reg, 1e-4*decayed_reg, 1e-4*decayed_reg, 0.0*decayed_reg, 0.0*decayed_reg]\r\n        lr_mult_conv1x1 = decayed_reg\r\n       \r\n        # Loss\r\n        loss_l2 = l2_rnn + l2_conv1x1\r\n        loss_cls = tf.reduce_mean(categorical_crossentropy(y, preds))\r\n        loss = loss_cls #+ loss_l2\r\n        **train_step_rnn = tf.keras.optimizers.RMSprop(lr=decayed_lr).get_updates(loss=loss, params=trainable_variables_rnn)\r\n        train_step_conv1x1 = tf.keras.optimizers.RMSprop(lr=decayed_lr*lr_mult_conv1x1).get_updates(loss=loss, params=trainable_variables_conv1x1)**\r\n\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)                                                                           \r\n        with tf.control_dependencies(update_ops):\r\n            train_step = tf.group(train_step_rnn, train_step_conv1x1, increment_global_step)\r\n            #train_step = tf.group(train_step_rnn, train_step_conv1x1, train_step_rgb, train_step_depth, increment_global_step, recompute_reg)\r\n        accuracy = tf.reduce_mean(categorical_accuracy(y, preds))   \r\n\r\n        # Create summaries for Tensorboard\r\n        tf.summary.scalar(\"loss_cls\", loss_cls)\r\n        tf.summary.scalar(\"loss_l2\", loss_l2)\r\n        tf.summary.scalar(\"loss\", loss)\r\n        tf.summary.scalar(\"accuracy\", accuracy)\r\n        tf.summary.scalar(\"learning_rate\", decayed_lr)\r\n        tf.summary.scalar(\"lambda\", decayed_reg)\r\n        summary_op = tf.summary.merge_all()\r\n\r\n        name = str(lr) + '_' + str(bs) + '_' +  str(nn)\r\n        train_writer = tf.summary.FileWriter(tensorboard_log + name + '/train/', graph = sess.graph)\r\n        val_writer = tf.summary.FileWriter(tensorboard_log + name + '/val/')\r\n\r\n        # Initialize all variables\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        # Load the pretrained weights into the non-trainable layer\r\n        model_rgb.load_params(sess, params_dir_rgb, trainable=False)\r\n        model_depth.load_params(sess, params_dir_depth, trainable=False)\r\n\r\n        print(\"\\nHyper-parameters: lr={}, #neurons={}, bs={}, l2={}, max_norm={}, dropout_rate={}\".format(lr,nn,bs,aa,mn,do))     \r\n        print(\"Number of trainable parameters = {}\".format(count_params(trainable_variables_rnn)+count_params(trainable_variables_conv1x1)))    \r\n        print(\"\\n{} Generate features from training set\".format(datetime.now()))\r\n         \r\n        tb_train_count=0        \r\n        tb_val_count = 0\r\n\r\n        # Loop over number of epochs\r\n        num_samples = 0\r\n        # Training set     \r\n        sess.run(training_init_op)\r\n\r\n        # Progress bar setting\r\n        bar = progressbar.ProgressBar(maxval=tr_batches_per_epoch, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\r\n        bar.start()\r\n        train_loss = 0\r\n        for i in range(tr_batches_per_epoch):\r\n            bar.update(i+1)\r\n            tb_train_count+=1\r\n            rgb_batch, depth_batch, label_batch = sess.run(next_batch)\r\n\r\n            num_samples += np.shape(rgb_batch)[0]\r\n            feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: (1-do), training_phase: True, K.learning_phase(): 1}\r\n            batch_loss, summary = sess.run([loss, summary_op], feed_dict=feed_dict)\r\n            train_loss += batch_loss\r\n            train_writer.add_summary(summary, tb_train_count)\r\n\r\n        bar.finish()\r\n\r\n        train_loss /= tr_batches_per_epoch #num_samples\r\n        print(\"training loss = {}\\n\".format(train_loss))\r\n\r\n        val_acc = 0\r\n        val_loss = 0\r\n        num_samples = 0  \r\n\r\n        sess.run(validation_init_op)\r\n        for i in range(val_batches_per_epoch):\r\n\r\n            tb_val_count+=1\r\n            rgb_batch, depth_batch, label_batch = sess.run(next_batch)\r\n            num_samples += np.shape(rgb_batch)[0]\r\n\r\n            feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: 1.0, training_phase: False, K.learning_phase(): 0}\r\n            batch_loss, batch_acc, summary = sess.run([loss, accuracy, summary_op], feed_dict=feed_dict)\r\n\r\n            val_loss+=batch_loss\r\n            val_acc+=batch_acc\r\n            val_writer.add_summary(summary, tb_val_count*(tr_batches_per_epoch/val_batches_per_epoch))\r\n\r\n        val_loss /= val_batches_per_epoch #num_samples\r\n        val_acc /= val_batches_per_epoch #num_samples\r\n        print(\"\\n{} Validation loss : {}, Validation Accuracy = {:.4f}\".format(datetime.now(), val_loss, val_acc))\r\n\r\n\r\n        # Loop over number of epochs\r\n        for epoch in range(num_epochs):\r\n            num_samples = 0\r\n                 \r\n            # Training set\r\n            print(\"\\nEpoch: {}/{}\".format(epoch + 1, num_epochs))\r\n            sess.run(training_init_op)\r\n\r\n            # Progress bar setting\r\n            bar = progressbar.ProgressBar(maxval=tr_batches_per_epoch, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\r\n            bar.start()\r\n            train_loss = 0\r\n            for i in range(tr_batches_per_epoch):\r\n                bar.update(i+1)\r\n                tb_train_count+=1\r\n                rgb_batch, depth_batch, label_batch = sess.run(next_batch)\r\n\r\n                num_samples += np.shape(rgb_batch)[0] \r\n                # apply data augmentation\r\n                data_aug(rgb_batch,depth_batch)\r\n                feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: (1-do), training_phase: True, K.learning_phase(): 1}\r\n                batch_loss, _, summary = sess.run([loss, train_step, summary_op], feed_dict=feed_dict)\r\n                train_loss += batch_loss                \r\n                train_writer.add_summary(summary, tb_train_count)\r\n                \r\n            bar.finish()\r\n            train_loss /= tr_batches_per_epoch #num_samples\r\n            print(\"training loss = {}\\n\".format(train_loss))\r\n    \r\n            if (epoch+1)%1 == 0:\r\n                sess.run(validation_init_op)\r\n                num_samples = 0\r\n                val_loss = 0\r\n                val_acc = 0\r\n                tsne_feat_rgb, tsne_feat_depth, tsne_feat_rgbd = None, None, None \r\n                pred_feat, gt_feat = None, None\r\n\r\n                for i in range(val_batches_per_epoch):\r\n\r\n                    tb_val_count+=1\r\n                    rgb_batch, depth_batch, label_batch = sess.run(next_batch)\r\n                    num_samples += np.shape(rgb_batch)[0]\r\n                    feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: 1.0, training_phase: False, K.learning_phase(): 0}\r\n                    batch_loss, batch_acc, summary, tsne_batch_rgb, tsne_batch_depth, tsne_batch_rgbd, batch_preds = sess.run([loss, \r\n                        accuracy, summary_op, pool2_flat_rgb, pool2_flat_depth, rnn_h, preds], \r\n                        feed_dict=feed_dict)\r\n\r\n                    if tsne_feat_rgb is None:\r\n                        tsne_feat_rgb = tsne_batch_rgb\r\n                        tsne_feat_depth = tsne_batch_depth\r\n                        tsne_feat_rgbd = tsne_batch_rgbd\r\n\r\n                        pred_feat = batch_preds\r\n                        gt_feat = label_batch\r\n                    else:\r\n                        tsne_feat_rgb = np.append(tsne_feat_rgb, tsne_batch_rgb, axis=0)\r\n                        tsne_feat_depth = np.append(tsne_feat_depth, tsne_batch_depth, axis=0)\r\n                        tsne_feat_rgbd = np.append(tsne_feat_rgbd, tsne_batch_rgbd, axis=0)\r\n\r\n                        pred_feat = np.append(pred_feat, batch_preds, axis=0)\r\n                        gt_feat = np.append(gt_feat, label_batch, axis=0)\r\n\r\n                    val_loss+=batch_loss\r\n                    val_acc+=batch_acc\r\n                    val_writer.add_summary(summary, tb_val_count*(tr_batches_per_epoch/val_batches_per_epoch))                \r\n\r\n                val_loss /= val_batches_per_epoch #num_samples\r\n                val_acc /= val_batches_per_epoch #num_samples\r\n\r\n                print(\"\\n{} Validation loss : {}, Validation Accuracy = {:.4f}\".format(datetime.now(), val_loss, val_acc))\r\n\r\n                if (epoch + 1) == num_epochs:\r\n                    with open(\"../features/tsne_feat_rgb.npy\", \"w+\") as f:\r\n                        np.save(f, tsne_feat_rgb)\r\n                    with open(\"../features/tsne_feat_depth.npy\", \"w+\") as f:\r\n                        np.save(f, tsne_feat_depth)\r\n                    with open(\"../features/tsne_feat_rgbd.npy\", \"w+\") as f:\r\n                        np.save(f, tsne_feat_rgbd)\r\n                    with open(\"../features/feat_preds.npy\", \"w+\") as f:\r\n                        final_preds = np.zeros([pred_feat.shape[0],2])\r\n                        final_preds[:,0] = np.argmax(pred_feat, axis=1)\r\n                        final_preds[:,1] = np.argmax(gt_feat, axis=1)\r\n                        np.save(f, final_preds)\r\n\r\n                del tsne_feat_rgb, tsne_feat_depth, tsne_feat_rgbd\r\n\r\n                log_epoch.append(epoch)\r\n                log_train_loss.append(train_loss)\r\n                log_val_loss.append(val_loss)\r\n                log_val_acc.append(val_acc)\r\n\r\n            # Early stopping for ill-posed params combination\r\n            if ((epoch == 0) and (val_acc < 0.2)) or ((epoch == 9) and (val_acc < 0.5)) or np.isnan(train_loss):\r\n                print(\"Training stopped due to poor results or divergence: validation loss = {}\".format(val_acc))\r\n                break\r\n                \r\n        history_callback = {log[0]:log_epoch, log[1]:log_train_loss, log[2]:log_val_loss, log[3]:log_val_acc}\r\n        log_file(history_callback, hp)\r\n\r\n    #tf.reset_default_graph()\r\n    shutil.rmtree(tensorboard_log)\r\n\r\n\r\n\r\n\r\n\r\nIPython 7.5.0 -- An enhanced Interactive Python.\r\n\r\nrunfile('/home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py', wdir='/home/users/xdlan/myexperiment/rcfusion_master/code')\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0816 21:23:15.258742 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\nUsing TensorFlow backend.\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 17689183678501733557\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 14027451888762908702\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 11081742533860949395\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:XLA_GPU:1\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 12973763672644918679\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:XLA_GPU:2\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 8653398481352517287\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:XLA_GPU:3\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 4684263568990964685\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 11100815360\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n    link {\r\n      device_id: 1\r\n      type: \"StreamExecutor\"\r\n      strength: 1\r\n    }\r\n  }\r\n}\r\nincarnation: 2874952132781754303\r\nphysical_device_desc: \"device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1\"\r\n, name: \"/device:GPU:1\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 11970700903\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n    link {\r\n      type: \"StreamExecutor\"\r\n      strength: 1\r\n    }\r\n  }\r\n}\r\nincarnation: 16227167641656641306\r\nphysical_device_desc: \"device: 1, name: TITAN Xp, pci bus id: 0000:03:00.0, compute capability: 6.1\"\r\n, name: \"/device:GPU:2\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 2625634304\r\nlocality {\r\n  bus_id: 2\r\n  numa_node: 1\r\n  links {\r\n    link {\r\n      device_id: 3\r\n      type: \"StreamExecutor\"\r\n      strength: 1\r\n    }\r\n  }\r\n}\r\nincarnation: 1497904576540092906\r\nphysical_device_desc: \"device: 2, name: TITAN Xp, pci bus id: 0000:82:00.0, compute capability: 6.1\"\r\n, name: \"/device:GPU:3\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 2545942528\r\nlocality {\r\n  bus_id: 2\r\n  numa_node: 1\r\n  links {\r\n    link {\r\n      device_id: 2\r\n      type: \"StreamExecutor\"\r\n      strength: 1\r\n    }\r\n  }\r\n}\r\nincarnation: 18157037722769851594\r\nphysical_device_desc: \"device: 3, name: TITAN Xp, pci bus id: 0000:83:00.0, compute capability: 6.1\"\r\n]\r\nThe script requires 2 arguments: (1) the dataset root directory and (2) the parameters root directory.\r\nW0816 21:23:16.917065 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0816 21:23:17.117713 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py:177: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_types(dataset)`.\r\nW0816 21:23:17.118752 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py:178: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(dataset)`.\r\nW0816 21:23:17.124696 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_types(iterator)`.\r\nW0816 21:23:17.125599 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(iterator)`.\r\nW0816 21:23:17.126456 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_classes(iterator)`.\r\nW0816 21:23:17.165664 140077442987776 deprecation.py:506] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0816 21:23:19.061071 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:276: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.AveragePooling2D instead.\r\nW0816 21:23:19.161852 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:286: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv2D` instead.\r\nW0816 21:23:19.317812 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:287: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\r\nW0816 21:23:19.389662 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:291: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.MaxPooling2D instead.\r\nW0816 21:23:21.193200 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nW0816 21:23:21.194731 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\r\n\r\nW0816 21:23:21.272165 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\r\n\r\nW0816 21:23:21.281805 140077442987776 deprecation.py:506] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nW0816 21:23:21.627247 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-aa215b7e30b4>\", line 1, in <module>\r\n    runfile('/home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py', wdir='/home/users/xdlan/myexperiment/rcfusion_master/code')\r\n\r\n  File \"/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 710, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py\", line 374, in <module>\r\n    train_step_rnn = tf.keras.optimizers.RMSprop(lr=decayed_lr).get_updates(loss=loss, params=trainable_variables_rnn)\r\n\r\n  File \"/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/rmsprop.py\", line 106, in __init__\r\n    super(RMSprop, self).__init__(name, **kwargs)\r\n\r\n  File \"/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 257, in __init__\r\n    if kwargs[k] < 0:\r\n\r\n  File \"/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 690, in __bool__\r\n    raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the minimal code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Current conda install:\r\n\r\n               platform : linux-64\r\n          conda version : 4.3.30\r\n       conda is private : False\r\n      conda-env version : 4.3.30\r\n    conda-build version : 3.4.1\r\n         python version : 3.6.4.final.0\r\n       requests version : 2.18.4\r\n       root environment : /home/server/anaconda3  (read only)\r\n    default environment : /home/users/xdlan/.conda/envs/xdlan\r\n       envs directories : /home/users/xdlan/.conda/envs\r\n                          /home/server/anaconda3/envs\r\n          package cache : /home/server/anaconda3/pkgs\r\n                          /home/users/xdlan/.conda/pkgs\r\n           channel URLs : https://repo.continuum.io/pkgs/main/linux-64\r\n                          https://repo.continuum.io/pkgs/main/noarch\r\n                          https://repo.continuum.io/pkgs/free/linux-64\r\n                          https://repo.continuum.io/pkgs/free/noarch\r\n                          https://repo.continuum.io/pkgs/r/linux-64\r\n                          https://repo.continuum.io/pkgs/r/noarch\r\n                          https://repo.continuum.io/pkgs/pro/linux-64\r\n                          https://repo.continuum.io/pkgs/pro/noarch\r\n            config file : /home/users/xdlan/.condarc\r\n             netrc file : None\r\n           offline mode : False\r\n             user-agent : conda/4.3.30 requests/2.18.4 CPython/3.6.4 Linux/4.15.0-50-generic debian/stretch/sid glibc/2.23    \r\n                UID:GID : 1002:1002\r\n\r\n>>> tf.__version__\r\n'1.14.0'\r\n>>> \r\n\r\nI compile from source by using conda install --use-local ***/conda install ***", "In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31691\">No</a>\n"]}, {"number": 31690, "title": "Import of TF Failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): with pip\r\n- TensorFlow version: 1.14 and 2.0.0-beta\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: installed with pip\r\n- Bazel version (if compiling from source): not installed\r\n- GCC/Compiler version (if compiling from source): Not used\r\n- CUDA/cuDNN version: just using CPU\r\n- GPU model and memory:  just using CPU\r\n\r\n\r\n\r\n**Describe the problem** I cant import tensorflow. I guess there have been installation issues. I have this problem with both the old and the new version.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow==2.0.0-beta1\r\npython\r\nimport tensorflow as tf\r\n\r\n\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow-GPU** (TF) prebuilt binaries.\n\n **TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.** \n\n* If you have above configuration and using _**Windows**_ platform -\n     * Try adding the CUDA, CUPTI, and cuDNNinstallation directories to the %PATH% environment variable. \n     * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n     * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n     * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n\nPlease let us know if this helps.", "How exactly do I add tis path?\r\nThe Doc says execute \"SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;%PATH%'.\r\nWhat exactly do I have to write in %PATH%?", "TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nDoes your CPU model support AVX instruction sets?. ", "I Run an Intel(R) Core(TM) i7 CPU 860.\r\nI think it does.", "Did you download and install the Microsoft Visual C++ 2015 Redistributable Update 3?\r\nhttps://visualstudio.microsoft.com/vs/older-downloads/", "Yes Microsoft Visual C++ 2015 Redistributable Update 3 is installed.\r\nI have noticed that \"python3 --version\" does not work.\r\nOnly \"python --version\" works fine. Is that a problem?", "That's fine. The problem is that your cpu model doesn't support AVX instruction sets.\r\nSee https://ark.intel.com/content/www/us/en/ark/products/41316/intel-core-i7-860-processor-8m-cache-2-80-ghz.html\r\n You can still use TensorFlow with the alternatives given below:\r\n* Try Google Colab to use TensorFlow.\r\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\r\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n  * All you need is a good internet connection and you are all set.\r\n* Try to build TF from sources by changing CPU optimization flags.\r\n", "I use Google Colab now.\r\nThank you very much for your help you're awesome.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31690\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31690\">No</a>\n"]}, {"number": 31689, "title": "I need to use tensorflow2.0 beta1 GPUs one computer to write a GAN ,but get error as follows. Because of BatchNorm layers, how to solve this problem?", "body": "```python\r\ndef G_D_fn(epoch, batch_data):\r\ntrain_data, feature = batch_data\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    generates = G( train_data)\r\n    logits_fake = D(generates) #If I use vgg, it works normal. If I use D, it has error.\r\n    logits_real = D(feature)\r\n    d_loss = tl.cost.sigmoid_cross_entropy(logits, tf.ones_like(logits_fake))\r\n    g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake,tf.ones_like(logits_fake)\r\ngrad = tape.gradient(g_loss, G.trainable_weights)\r\ng_optimizer.apply_gradients(zip(grad, G.trainable_weights))\r\n    grad = tape.gradient(d_loss, D.trainable_weights)\r\n    d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\r\n    return d_loss, g_gan_loss\r\n\r\nprint(\"Begin training...\")\r\ndevices = ['/device:GPU:{}'.format(i) for i in range(num_gpu)] \r\nstrategy = tf.distribute.MirroredStrategy(devices)\r\nwith strategy.scope():\r\n    train_log_dir = 'logs/gradient_tape/train'\r\n    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\r\n    # obtain models\r\n    G = get_G((batch_size, None, None, 3))\r\n    D = get_D((batch_size, None, None, 3)) \r\n    lr_v = tf.Variable(lr_init)\r\n    g_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\r\nd_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\r\n\r\n    G.train()\r\nD.train()\r\n\r\n    train_ds = get_train_data()\r\ndist_train_ds = strategy.experimental_distribute_dataset(train_ds)\r\n\r\n    n_batch_epoch = round(n_epoch // batch_size)  \r\n    for epoch in range(n_epoch):    \r\n        total_mse_loss = 0.0\r\n        batch = 0\r\n        for batch_data in dist_train_ds:\r\n            batch += 1                 \r\n            per_d_loss, per_g_gan_loss = strategy.experimental_run_v2(G_D_fn, args=(epoch, batch_data, ))\r\n            total_d_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_d_loss, axis = None)\r\n            total_g_gan_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_g_gan_loss, axis = None)\r\n\r\n        d_loss = total_d_loss/batch\r\n        g_gan_loss = total_g_gan_loss/batch\r\n\r\n        print(\"g_gan:{:.6f}, d_loss: {:.9f}\".format(g_gan_loss, d_loss))\r\n\r\n```\r\nI have find the problem.There are BatchNorm layers in my discriminator. When I delete them, the program can work. Or when I didn't use tf.distribute.MirroredStrategy, the program can work. Do tf.distribute.MirroredStrategy permit batch norm ?", "comments": ["Can you please help us with the stack trace? Thanks!", "#! /usr/bin/python\r\n# -*- coding: utf8 -*-\r\nimport os, time, random, time, csv\r\nfrom PIL import Image\r\nfrom datetime import datetime\r\nimport numpy as np\r\nimport matplotlib as mpl\r\nimport matplotlib.pyplot as plt\r\nfrom time import localtime, strftime \r\nimport scipy, multiprocessing\r\nimport tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom model import get_G, get_D\r\nfrom config import config\r\nimport utils\r\n\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" #use GPU\r\n\r\n\r\n###====================== HYPER-PARAMETERS ===========================###\r\n##gpu\r\nnum_gpu = 2\r\n\r\n\r\n## Adam\r\nbatch_size = config.TRAIN.batch_size\r\nlr_init = config.TRAIN.lr_init\r\nbeta1 = config.TRAIN.beta1\r\n\r\n\r\n## initialize G\r\nn_epoch_init = config.TRAIN.n_epoch_init\r\n\r\n\r\n## adversarial learning (SRGAN)\r\nn_epoch = config.TRAIN.n_epoch\r\nlr_decay = config.TRAIN.lr_decay\r\ndecay_every = config.TRAIN.decay_every\r\nshuffle_buffer_size = 128   \r\n\r\n\r\n##True: test Y channel only; False: test RGB channels\r\ntest_Y = False  \r\ncrop_border = 4\r\nvalid_image_save = 80\r\nper_num = 8\r\n\r\n\r\n\r\n\r\n# create folders to save result images and trained models\r\nsave_dir = \"samples\"\r\ntl.files.exists_or_mkdir(save_dir)\r\ncheckpoint_dir = \"models\"\r\ntl.files.exists_or_mkdir(checkpoint_dir)\r\nmetric_dir = \"metric\"\r\ntl.files.exists_or_mkdir(metric_dir)\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef get_train_data():\r\n    \r\n    # load dataset\r\n    train_lr_img_list = tl.files.load_file_list(path=config.TRAIN.lr_img_path, regx='.*.png', printable=False)\r\n    train_hr_img_list = tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False)  \r\n\r\n\r\n    step_time1 = time.time()\r\n    #If your machine have enough memory, please pre-load the whole train set.\r\n    train_lr_imgs = tl.vis.read_images(train_lr_img_list, path=config.TRAIN.lr_img_path, n_threads=32)\r\n    train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=config.TRAIN.hr_img_path, n_threads=32)\r\n    #train_imgs = zip(train_lr_imgs, train_hr_imgs)(\u4e0d\u80fd\u5728\u8fd9\u91cc\u538b\u7f29\uff0c\u5426\u5219train_ds\u7684\u6570\u636e\u53ea\u6709\u4e00\u4e2aepoch\uff0c\u5fc5\u987b\u5728generator_train()\u4e2d\u538b\u7f29\u3002)\r\n    print('read images time: {}s'.format(time.time()-step_time1))\r\n\r\n\r\n    def generator_train():\r\n        for imglr,imghr in zip(train_lr_imgs, train_hr_imgs):\r\n            yield imglr,imghr\r\n        \r\n\r\n\r\n    def _map_fn_train(imglr,imghr):\r\n\r\n\r\n        wiggle = 8\r\n        off_x, off_y = 25-wiggle, 60-wiggle   \r\n        crop_size = 128\r\n        crop_size_plus = crop_size + 2*wiggle\r\n\r\n\r\n        lr_patch = tf.image.crop_to_bounding_box(imglr, off_y, off_x, crop_size_plus, crop_size_plus)\r\n        hr_patch = tf.image.crop_to_bounding_box(imghr, off_y, off_x, crop_size_plus, crop_size_plus)\r\n\r\n\r\n\r\n\r\n        lr_patch = tf.image.resize_with_crop_or_pad(lr_patch, target_height=crop_size, target_width=crop_size)\r\n        hr_patch = tf.image.resize_with_crop_or_pad(hr_patch, target_height=crop_size, target_width=crop_size)\r\n\r\n\r\n        lr_patch = lr_patch / (255. / 2.)\r\n        lr_patch = lr_patch - 1.\r\n\r\n\r\n        hr_patch = hr_patch / (255. / 2.) \r\n        hr_patch = hr_patch - 1.             \r\n        \r\n        \r\n        lr_patch = tf.reshape(lr_patch,(128,128,3))  \r\n        hr_patch = tf.reshape(hr_patch,(128,128,3))\r\n\r\n\r\n        return lr_patch, hr_patch\r\n  \r\n    train_ds = tf.data.Dataset.from_generator(generator_train, output_types=(tf.float32,tf.float32))  #\u9996\u5148\u6784\u5efa\u4e00\u4e2agenerator:gen\uff0c\u7136\u540e\u4f7f\u7528tf.data.Dataset\u7684from_generator\u51fd\u6570\uff0c\u901a\u8fc7\u6307\u5b9a\u6570\u636e\u7c7b\u578b\uff0c\u6570\u636e\u7684shape\u7b49\u53c2\u6570\uff0c\u6784\u5efa\u4e00\u4e2aDataset\r\n\r\n\r\n    train_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count()) #tf.data.Dataset.map(f, num_parallel_calls):Dataset.map \u8f6c\u6362\u901a\u8fc7\u5c06\u51fd\u6570 f \u5e94\u7528\u4e8e\u8f93\u5165\u6570\u636e\u96c6\u7684\u6bcf\u5bf9\u5143\u7d20\uff08data\uff0c label\uff09\u6765\u751f\u6210\u65b0\u6570\u636e\u96c6\u3002num_parallel_calls \u53c2\u6570\u9009\u62e9\u6700\u4f73\u503c\u53d6\u51b3\u4e8e\u60a8\u7684\u786c\u4ef6\uff0c\u8bad\u7ec3\u6570\u636e\u7684\u7279\u5f81\uff08\u4f8b\u5982\u5176\u5927\u5c0f\u548c\u5f62\u72b6\u3002Map \u529f\u80fd\u7684\u6210\u672c\u4ee5\u53ca\u5728 CPU \u4e0a\u540c\u65f6\u8fdb\u884c\u7684\u5176\u4ed6\u5904\u7406\uff1b\u4e00\u4e2a\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u662f\u4f7f\u7528\u53ef\u7528\u7684 CPU \u5185\u6838\u6570\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5c06 num_parallel_calls \u8bbe\u7f6e\u4e3a\u8fdc\u5927\u4e8e\u53ef\u7528 CPU \u6570\u91cf\u7684\u503c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8c03\u5ea6\u6548\u7387\u4f4e\u4e0b\uff0c\u4ece\u800c\u5bfc\u81f4\u901f\u5ea6\u51cf\u6162\u3002\r\n\r\n\r\n    train_ds = train_ds.shuffle(shuffle_buffer_size)  \r\n    train_ds = train_ds.prefetch(buffer_size=2) #tf.data.Dataset.prefetch(buffer_size):\u8be5\u4ee3\u7801\u53ef\u4ee5\u4fdd\u8bc1\u4e0b\u4e00\u4e2a batch \u7684\u6570\u636e\u5bf9\u4e8e GPU \u53ef\u4ee5\u7acb\u5373\u53ef\u7528\uff0c\u51cf\u5c11 GPU \u7684\u6570\u636e\u7b49\u5f85\u65f6\u95f4. \u5176\u4e2d\uff0cbuffer_size \u662f\u9884\u5148\u88ab\u62c9\u53d6\u6570\u636e\u7684 batches \u6570. \u4e00\u822c\u60c5\u51b5\u4e0b\uff0cbuffer_size=1. \u5982\u679c\u5904\u7406\u6bcf\u4e2a batch \u7684\u8017\u65f6\u4e0d\u540c\u65f6\uff0c\u53ef\u4ee5\u589e\u52a0\u5176\u503c.###############################\r\n    train_ds = train_ds.batch(batch_size) #\u91c7\u7528 tf.data.Dataset.batch() \u521b\u5efa batches \u6570\u636e. \r\n    return train_ds\r\n\r\n\r\n\r\n\r\ndef train():\r\n\r\n\r\n    def init_G_fn(epoch, batch_data):\r\n\r\n\r\n        lr_patchs_init, hr_patchs_init = batch_data\r\n\r\n\r\n        if (lr_patchs_init.shape[0] != batch_size//2) or (hr_patchs_init.shape[0] != batch_size//2): # if the remaining data in this epoch < batch_size\r\n            print('error')\r\n        #tf.GradientTape() \u662f\u4e00\u4e2a\u81ea\u52a8\u6c42\u5bfc\u7684\u8bb0\u5f55\u5668,\u5728\u5176\u4e2d\u7684\u53d8\u91cf\u548c\u8ba1\u7b97\u6b65\u9aa4\u90fd\u4f1a\u88ab\u81ea\u52a8\u8bb0\u5f55\u3002\r\n        with tf.GradientTape() as tape:  \r\n            gene_patchs_init = G(lr_patchs_init, training=True)\r\n            mse_loss_init = tl.cost.mean_squared_error(gene_patchs_init, hr_patchs_init, is_mean=True)\r\n        # TensorFlow\u81ea\u52a8\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u5173\u4e8e\u81ea\u53d8\u91cf\uff08\u6a21\u578b\u53c2\u6570\uff09\u7684\u68af\u5ea6\r\n        grad = tape.gradient(mse_loss_init, G.trainable_weights)\r\n        # TensorFlow\u81ea\u52a8\u6839\u636e\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\r\n        g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\r\n\r\n\r\n\r\n\r\n        if (epoch != 0) and (epoch % 10 == 0):                   \r\n            image_init = tf.concat([lr_patchs_init, gene_patchs_init, hr_patchs_init], 2)       \r\n            tl.vis.save_images(image_init.numpy(), [batch_size, 1],  os.path.join(save_dir, 'train_g_init_{}.png'.format(epoch)))#tensor.numpy() tensor\u8f6c\u6362\u4e3anumpy \r\n\r\n\r\n        return mse_loss_init\r\n\r\n\r\n\r\n\r\n    def G_D_fn(epoch, batch_data):\r\n\r\n\r\n        lr_patchs, hr_patchs = batch_data\r\n\r\n\r\n        if (lr_patchs.shape[0] != batch_size//2) or (hr_patchs.shape[0] != batch_size//2): # if the remaining data in this epoch < batch_size\r\n            print('error')\r\n        \r\n        with tf.GradientTape(persistent=True) as tape:\r\n\r\n\r\n            gene_patchs = G(lr_patchs) \r\n            print(\"%%%%%%%%%%%%%%%%%%%%%%\")\r\n            logits_fake = D(gene_patchs) \r\n            print(\"~~~~~~~~~~~~~~~~~~~~~~\")\r\n            logits_real = D(hr_patchs)\r\n\r\n\r\n            feature_fake = VGG((gene_patchs+1)/2.)  #the pre-trained VGG uses the input range of [0, 1]\r\n            feature_real = VGG((hr_patchs+1)/2.)  \r\n\r\n\r\n\r\n\r\n            g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))\r\n\r\n\r\n\r\n\r\n            d_loss_real = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real))\r\n            d_loss_fake = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake))\r\n            d_loss = d_loss_real + d_loss_fake\r\n\r\n\r\n\r\n\r\n            mse_loss = tl.cost.mean_squared_error(gene_patchs, hr_patchs, is_mean=True)\r\n            vgg_loss = 2e-6 * tl.cost.mean_squared_error(feature_fake, feature_real, is_mean=True)\r\n            g_loss = mse_loss + vgg_loss + g_gan_loss \r\n\r\n\r\n        grad = tape.gradient(g_loss, G.trainable_weights)\r\n        g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\r\n     \r\n        grad = tape.gradient(d_loss, D.trainable_weights)\r\n        d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\r\n\r\n\r\n        if (epoch != 0) and (epoch % 10 == 0):           \r\n            image_train = tf.concat([lr_patchs, gene_patchs, hr_patchs], 2)       \r\n            tl.vis.save_images(image_train.numpy(), [batch_size, 1], os.path.join(save_dir, 'train_g_{}.png'.format(epoch))) #tensor.numpy() tensor\u8f6c\u6362\u4e3anumpy\r\n\r\n\r\n        return d_loss, g_loss, g_gan_loss, mse_loss, vgg_loss, style_loss\r\n\r\n\r\n\r\n\r\n    print(\"Begin training...\")\r\n    \r\n    devices = ['/device:GPU:{}'.format(i) for i in range(num_gpu)] \r\n    strategy = tf.distribute.MirroredStrategy(devices)\r\n    \r\n    with strategy.scope():\r\n        train_log_dir_init = 'logs/gradient_tape/train_init'\r\n        train_log_dir = 'logs/gradient_tape/train'\r\n    \r\n        train_summary_writer_init = tf.summary.create_file_writer(train_log_dir_init)\r\n        train_summary_writer = tf.summary.create_file_writer(train_log_dir)\r\n\r\n\r\n        # obtain models\r\n        G = get_G((batch_size, None, None, 3)) #(batch_size, 128, 128, 3)\r\n        D = get_D((batch_size, None, None, 3)) \r\n        VGG = tl.models.vgg19(pretrained=True, end_with='pool4', mode='static') \r\n    \r\n        print(G)\r\n        print(D)\r\n        print(VGG)\r\n    \r\n        lr_v = tf.Variable(lr_init)\r\n        g_optimizer_init = tf.optimizers.Adam(lr_v, beta_1=beta1)\r\n        g_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\r\n        d_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\r\n\r\n\r\n        geneaic_G = RelativisticAverageLoss(G, type_ = \"G\")\r\n        geneaic_D = RelativisticAverageLoss(D, type_ = \"D\")\r\n    \r\n        G.train()\r\n        D.train()\r\n        VGG.train()\r\n       \r\n        train_ds = get_train_data()\r\n        dist_train_ds = strategy.experimental_distribute_dataset(train_ds)\r\n\r\n\r\n        # initialize learning (G)\r\n        num_batch_init = round(n_epoch_init // batch_size) #round() \u65b9\u6cd5\u8fd4\u56de\u6d6e\u70b9\u6570x\u7684\u56db\u820d\u4e94\u5165\u503c\u3002\u5982\u679c\u8ddd\u79bb\u4e24\u8fb9\u4e00\u6837\u8fdc\uff0c\u4f1a\u4fdd\u7559\u5230\u5076\u6570\u7684\u4e00\u8fb9\u3002\u6bd4\u5982round(0.5)\u548cround(-0.5)\u90fd\u4f1a\u4fdd\u7559\u52300\uff0c\u800cround(1.5)\u4f1a\u4fdd\u7559\u52302\u3002\r\n        for epoch in range(n_epoch_init):\r\n            step_time2 = time.time()\r\n            total_mse_loss = 0.0\r\n            batch = 0\r\n            for batch_data_init in dist_train_ds:\r\n                batch += 1\r\n                per_mse_loss = strategy.experimental_run_v2(init_G_fn,args=(epoch, batch_data_init, ))\r\n                total_mse_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_mse_loss, axis = None)\r\n                print(\"batch_init: [{}/{}]\".format(batch,num_batch_init))\r\n    \r\n            mse_loss_init = total_mse_loss/batch\r\n          \r\n            print(\"Epoch_init: [{}/{}] time: {}s, mse: {} \".format(epoch, n_epoch_init, time.time() - step_time2, mse_loss_init))\r\n    \r\n            with train_summary_writer_init.as_default():\r\n                tf.summary.scalar('mse_loss_g_init', mse_loss_init, step = epoch)  \r\n   \r\n            if epoch != n_epoch_init-1:\r\n                mse_loss_init = 0.0\r\n                #mse_loss_init.reset_states()\r\n                \r\n    \r\n        # adversarial learning (G, D)\r\n        num_batch = round(n_epoch // batch_size)  \r\n        iteration_times = 0\r\n        for epoch in range(n_epoch): \r\n            step_time3 = time.time()\r\n            # update the learning rate\r\n            if (epoch != 0) and (epoch % decay_every == 0):  \r\n                new_lr_decay = lr_decay**(epoch // decay_every)\r\n                lr_v.assign(lr_init * new_lr_decay) #tf\u4e2dassign()\u51fd\u6570\u53ef\u7528\u4e8e\u5bf9\u53d8\u91cf\u8fdb\u884c\u66f4\u65b0\u5305\u62ec\u53d8\u91cf\u7684value\u548cshape\u3002\r\n                log = \" ** new learning rate: %f (for GAN)\" % (lr_init * new_lr_decay)\r\n                print(log)\r\n    \r\n            total_d_loss = 0.0\r\n            total_g_loss = 0.0\r\n            total_g_gan_loss = 0.0\r\n            total_mse_loss = 0.0\r\n            total_vgg_loss = 0.0\r\n                                                                        \r\n            batch = 0\r\n            for batch_data in dist_train_ds:\r\n                batch += 1                 \r\n\r\n\r\n                per_d_loss, per_g_loss, per_g_gan_loss, per_mse_loss, per_vgg_loss, per_style_loss = strategy.experimental_run_v2(G_D_fn, args=(epoch, batch_data, ))\r\n\r\n\r\n                total_d_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_d_loss, axis = None)\r\n                total_g_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_g_loss, axis = None)\r\n                total_g_gan_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_g_gan_loss, axis = None)\r\n                total_mse_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_mse_loss, axis = None)\r\n                total_vgg_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_vgg_loss, axis = None)\r\n                print(\"batch: [{}/{}]\".format(batch,num_batch))\r\n    \r\n            d_loss = total_d_loss/batch\r\n            g_loss = total_g_loss/batch\r\n            g_gan_loss = total_g_gan_loss/batch\r\n            mse_loss = total_mse_loss/batch\r\n            vgg_loss = total_vgg_loss/batch\r\n          \r\n            print(\"Epoch: [{}/{}]  g_loss(mse:{:.6f}, vgg:{:.6f}, g_gan:{:.6f}) d_loss: {:.6f}\".format(epoch, n_epoch, mse_loss, vgg_loss, g_gan_loss, d_loss))\r\n    \r\n            G.save_weights(os.path.join(checkpoint_dir, 'g_current.h5'))           \r\n    \r\n            with train_summary_writer.as_default():\r\n                tf.summary.scalar('d_loss', d_loss, step = epoch) \r\n                tf.summary.scalar('g_loss', g_loss, step = epoch)  \r\n                tf.summary.scalar('g_gan_loss', g_gan_loss, step = epoch) \r\n                tf.summary.scalar('mse_loss_g2', mse_loss, step = epoch) \r\n                tf.summary.scalar('vgg_loss', vgg_loss, step = epoch) \r\n\r\n\r\n            if epoch != n_epoch-1:\r\n                d_loss = 0.0 \r\n                g_loss = 0.0 \r\n                g_gan_loss = 0.0 \r\n                mse_loss = 0.0 \r\n                vgg_loss = 0.0\r\n\r\n\r\n            if (epoch != 0) and (epoch % 10 == 0):                \r\n                G.save_weights(os.path.join(checkpoint_dir, 'g_{}.h5'.format(epoch)))\r\n                D.save_weights(os.path.join(checkpoint_dir, 'd_{}.h5'.format(epoch)))\r\n    \r\n            \r\n\r\n\r\nif __name__ == '__main__':\r\n    \r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n\r\n\r\n    parser.add_argument('--mode', type=str, default='train', help='train, test')\r\n\r\n\r\n    args = parser.parse_args()\r\n\r\n\r\n    tl.global_flag['mode'] = args.mode\r\n\r\n\r\n    if tl.global_flag['mode'] == 'train':\r\n        train()\r\n    elif tl.global_flag['mode'] == 'test':\r\n        test()\r\n    else:\r\n        raise Exception(\"Unknow --mode\")\r\n\r\n\r\n\r\n\r\n\r\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\r\n\u53d1\u4ef6\u4eba: \"\u8463\u8c6a\"<notifications@github.com>; \r\n\u53d1\u9001\u65f6\u95f4: 2019\u5e748\u670824\u65e5(\u661f\u671f\u516d) \u4e0a\u53487:08\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>; \r\n\u6284\u9001: \"\u59d0\u59b9\u7684\u6d77\u6d0b\"<2259949930@qq.com>; \"Author\"<author@noreply.github.com>; \r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] I need to use tensorflow2.0 beta1 GPUsone computer to write a GAN ,but get error as follows. Because of BatchNormlayers, how to solve this problem? (#31689)\r\n\r\n\r\n\r\n\r\nCan you please help us with the stack trace? Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub, or mute the thread.", "I am very sorry, our computer is wrong. I can't run my code. This is the error ever.\r\n\r\n\r\nTraceback (most recent call last):   File \"train.py\", line 680, in <module>     train()   File \"train.py\", line 319, in train     per_d_loss, per_g_loss, per_d_loss1, per_d_loss2, per_g_gan_loss, per_mse_loss, per_vgg_loss, per_style_loss = strategy.experimental_run_v2(G_D_fn, args=(epoch, batch_data, ))   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 708, in experimental_run_v2     return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1710, in call_for_each_replica     return self._call_for_each_replica(fn, args, kwargs)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 708, in _call_for_each_replica     fn, args, kwargs)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 195, in _call_for_each_replica     coord.join(threads)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join     six.reraise(*self._exc_info_to_raise)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/six.py\", line 693, in reraise     raise value   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception     yield   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 189, in _call_for_each_replica     **merge_kwargs)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py\", line 105, in merge_fn     return update(strategy, v, value)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py\", line 96, in update     return strategy.extended.update(v, update_fn, args=(value,))   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1458, in update     return self._update(var, fn, args, kwargs, group)   File \"/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 758, in _update     assert isinstance(var, values.DistributedVariable) AssertionError\r\n\r\n\r\n\r\n\r\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\r\n\u53d1\u4ef6\u4eba: \"\u8463\u8c6a\"<notifications@github.com>;\r\n\u53d1\u9001\u65f6\u95f4: 2019\u5e748\u670824\u65e5(\u661f\u671f\u516d) \u4e0a\u53487:08\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\r\n\u6284\u9001: \"\u59d0\u59b9\u7684\u6d77\u6d0b\"<2259949930@qq.com>;\"Author\"<author@noreply.github.com>;\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] I need to use tensorflow2.0 beta1 GPUsone computer to write a GAN ,but get error as follows. Because of BatchNormlayers, how to solve this problem? (#31689)\r\n\r\n\r\n\r\n\r\nCan you please help us with the stack trace? Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub, or mute the thread.", "Sorry for the delay in reverting. It's very challenging to debug lengthy scripts such as these. At this point this looks more a support question. Please try posting this issue on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31689\">No</a>\n"]}, {"number": 31688, "title": "Different result when evaluating tflite model from python and from android", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nokia 7 plus\r\n- TensorFlow installed from (source or binary): don't know (I am using Google Colab)\r\n- TensorFlow version (use command below): 1.14.0 (git version: v1.14.0-0-g87989f6959 - compiler version: 7.4.0)\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): c++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA-SMI 418.67 - 15079MiB\r\n\r\n**Describe the current behavior**\r\n\r\nI've trained a model using keras (in google colab), then I've converted the keras h5 file to tflite using `TFLiteConverter` (also in google colab) by using this code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntflite_converter = tf.lite.TFLiteConverter.from_keras_model_file(<KERAS_H5_MODEL_PATH>)\r\n\r\ntflite_model = tflite_converter.convert()\r\n\r\nwith open(<KERAS_TFLITE_DEST_PATH>, 'wb') as tflite_model_file:\r\n    tflite_model_file.write(tflite_model)\r\n```\r\n\r\nAfter that, I've run the tflite model (in google colab also):\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=tflite_model_path)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_data = np.ones((1, 256, 256, 3), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\noutput = [interpreter.get_tensor(output_details[0]['index'])[0], interpreter.get_tensor(output_details[1]['index'])[0]]\r\n```\r\n\r\nThe tflite model can be found [here](https://drive.google.com/file/d/1cxXHtBGJk5h__6Elf4ZLvtJW8mdRs4_f/view?usp=sharing)\r\n\r\nHowever, when I run the tflite model in an android app (using the same input data) y get different outputs:\r\n\r\n(This code is based on [this example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) from tensorflow)\r\n\r\n```java\r\nAssetFileDescriptor fileDescriptor = assets.openFd(<MODEL_TFLITE_FILENAME>);\r\nFileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\nFileChannel fileChannel = inputStream.getChannel();\r\nlong startOffset = fileDescriptor.getStartOffset();\r\nlong declaredLength = fileDescriptor.getDeclaredLength();\r\nMappedByteBuffer model = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n\r\nInterpreter.Options interpreterOptions = new Interpreter.Options().setNumThreads(1);\r\ntfLite = new Interpreter(model, interpreterOptions);\r\n\r\nfloat[][][][] floatValues = new float[1][256][256][3];\r\n\r\nfor (int i = 0; i < 256; i++) {\r\n    for (int j = 0; j < 256; j++) {\r\n        for (int k = 0; k < 3; k++) {\r\n            floatValues[0][i][j][k] = 1.0f;\r\n        }\r\n    }\r\n}\r\n\r\nObject[] inputArray = {floatValues};\r\n\r\noutput1 = new float[1][8][8][18];\r\noutput2 = new float[1][16][16][18];\r\n\r\nMap<Integer, Object> outputMap = new HashMap<>();\r\noutputMap.put(0, output1);\r\noutputMap.put(1, output2);\r\n\r\ntfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n```\r\n\r\nThe output values I get in the android code are not the same as the outputs obtained with python code.\r\n\r\n```\r\n// This outputs correspond to output1[0][0][0]\r\n\r\n Python results    |  Android results\r\n 0 >   0.06118933  |  0 >   0.061190486\r\n 1 > - 0.50384498  |  1 > - 0.50384396\r\n 2 > - 0.30500048  |  2 > - 0.30500013\r\n 3 > - 0.18725708  |  3 > - 0.18725668\r\n 4 > -12.56872463  |  4 > -12.568727\r\n 5 > - 3.53239870  |  5 > - 3.5323968\r\n 6 >   0.26756036  |  6 >   0.26756087\r\n 7 > - 0.63708508  |  7 > - 0.6370841\r\n 8 > - 0.11959708  |  8 > - 0.119596675\r\n 9 > - 0.42219949  |  9 > - 0.42219883\r\n10 > -12.26699734  | 10 > -12.266998\r\n11 > - 3.42504168  | 11 > - 3.4250417\r\n12 >   0.29714739  | 12 >   0.2971481\r\n13 > - 0.76750284  | 13 > - 0.7675022\r\n14 > - 0.13859260  | 14 > - 0.13859189\r\n15 > - 0.02096577  | 15 > - 0.020965554\r\n16 > -12.78137684  | 16 > -12.781382\r\n17 > - 3.34643984  | 17 > - 3.3464384\r\n```\r\n\r\nHere the results are similar, there is a small difference. However I've also tried with a real input (comming from an image), and in this case, the results in android are totally different.\r\n\r\nThis is a model to detect objects in images (yolo v3 tiny) and I've constated that the values obtained with python code are the correct ones. When I run the python code with a real input (an image) it detects the objects.\r\n\r\n- Why do I get different outputs in android?\r\n- Do I have to do something special to run model in android?\r\n- Do I have to do something different in model conversion from H5 to tflite?\r\n- Does it has something to do with the way as java works with floats?\r\n- Should I use a quantized version of the model?\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is to get the same result running the model in android that when I run the model in python\r\n\r\n**Code to reproduce the issue**\r\n\r\nAlready provided in the description of the current behavior\r\n\r\n**Other info / logs**\r\n\r\nThe tflite model can be found [here](https://drive.google.com/file/d/1cxXHtBGJk5h__6Elf4ZLvtJW8mdRs4_f/view?usp=sharing)", "comments": ["Seems like those differences are normal. After correcting some bug in the code that processes the output of the model I have noticed that these differences do not alter the final result.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31688\">No</a>\n", "Hey stuck with same issue for almost 3 days now. Can you tell me what the bug was in your android code?\r\n(I think I responded to your SO post as well)", "I've already seen your SO comment :-)\r\n\r\nThe first bug was iterating the output vectors of the TFLITE model. YOLOv3 Tiny has two outputs o sizes 8x8 and 16x16 when the input has 256x256 size. However I had the same for (with the same limit) to iterate both outputs. It was a copy+paste bug.And also, at the beginning I was also comparing badly the python results and the Android ones (the comparison in this question is ok). This was because in the python implementation there was a function that modified an input and I was using that modified input to compare with the Android one (unmodified)."]}, {"number": 31687, "title": "TPUStrategy incompatibility with tf.io.read_file", "body": "**System information**\r\n\r\nI am using Colaboratory and Google cloud.\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.9.168-1+deb9u5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): unknown\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.5.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThe script ends with a segmentation fault or abort.\r\n\r\n**Describe the expected behavior**\r\n\r\nJust a clean run.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n# Importing all necessary libraries\r\nimport tensorflow as tf\r\nimport cv2\r\nimport random as rnd\r\nimport os\r\n\r\nif int(tf.__version__.split('.')[0]) == 1:\r\n    print(tf.__version__)\r\n    tf.enable_eager_execution()\r\n\r\n@tf.function\r\ndef _read_test(filename):\r\n    img_raw = tf.io.read_file(tf.squeeze(filename))\r\n    return img_raw\r\n\r\nimport numpy as np\r\nimage = np.array([[rnd.randint(0, 255) for _ in range(936)] for _ in range(1024)])\r\ncv2.imwrite('0.png', image);\r\n\r\nraw = _read_test(tf.constant('0.png'))\r\n\r\ntf.keras.backend.clear_session()\r\n\r\nif 'TPU_NAME' in os.environ:\r\n    TPU_WORKER = 'grpc://' + os.environ['TPU_NAME']\r\n    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\r\n    tf.config.experimental_connect_to_host(resolver.master())\r\n    tf.contrib.distribute.initialize_tpu_system(resolver)\r\n    strategy = tf.contrib.distribute.TPUStrategy(resolver)\r\nelif 'COLAB_TPU_ADDR' in os.environ:\r\n    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\r\n    tf.config.experimental_connect_to_host(resolver.master())\r\n    tf.contrib.distribute.initialize_tpu_system(resolver)\r\n    strategy = tf.contrib.distribute.TPUStrategy(resolver)\r\nelse:\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n#print(strategy)\r\n\r\nwith strategy.scope():\r\n    pass\r\n\r\nprint('success')\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe previous code is part of an image processing Neural Network. The image files are read while running the code to minimize the disk usage. The code runs smoothly in a CPU or GPU environment, however it crashes in a TPU one.", "comments": ["```\r\nAll input files and the model directory must use a cloud storage\r\nbucket path (gs://bucket-name/...), and this bucket must be accessible \r\nfrom the TPU server. Note that all data processing and model checkpointing \r\nis performed on the TPU server, not the local machine\r\n```", "The example has been run in Colaboratory and the TPU server through a ssh connection. I am not running it on my local machine.", "@jmgc your images/ data should be stored on a GCS bucket, and then change it to something like this\r\n```\r\n# raw = _read_test(tf.constant('0.png'))\r\nraw = _read_test(tf.constant('gs://your-bucket-name/0.png'))\r\n```\r\n", "@srihari-humbarwadi I have tested it in a TPU server and it seems to work, however, in Colaboratory it does not find the bucket. Is it possible to access a drive in a similar way as using a bucket? If not, how can I read files in Colaboratory?", "You will have to create a service account in GCP with access to GCS bucket you intend to read data from. Then use the credentials(json) of this service account in the snippet given below\r\n```\r\nwith tf.Session() as sess:\r\n    with open('<YOUR CREDENTIALS JSON>', 'r') as f: \r\n        auth_info = json.load(f)\r\n        tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\r\n```", "Sorry, I am not able to follow all the acronyms. Can I access a drive disk from a TPU in Colaboratory?", "@jmgc I never used TPU before (plan to give it a try soon), but my understanding is that TPU does not give you access to a local file system so you have to use a remote file system such as GCS (with `gcs://...` scheme that TF support)\r\n\r\nI would assume other types of remote file system such as s3 (AWS) or azfs (Azure) should work as well, as long as the TF version you use support the scheme (e.g, `s3://bucket/object`, `azfs://...`).", "@jmgc Did you try @yongtang's solution. ", "@gadagashwini, I did not because I have tried the bucket solution, and I still have a segmentation fault in my full script. I am working on another minimal test to show the error.", "I have been working and modified my code to use a bucket. I have found the same error reported in #32043.", "Hi @jmgc, do you have any sample code / colab that I can look at? Also in general we are currently not supporting TPUs with 2.0 yet.", "Closing issue for now, feel free to reopen.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31687\">No</a>\n"]}, {"number": 31686, "title": "[TF 2.0] Saving model containing categorical_column_with_vocabulary_list feature_column fails", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-8794-ge36271a61d 2.0.0-dev20190814\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nSaving a trained keras model containing a `categorical_column_with_vocabulary_list` feature_column results in the error:\r\n\r\n `AssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_1:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.`\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be possible to save a model containing all types of feature_columns.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ncategory = tf.constant([\"A\", \"B\", \"A\", \"C\", \"C\", \"A\"])\r\nlabel = tf.constant([1, 0, 1, 0, 0, 0])\r\n\r\nds = tf.data.Dataset.from_tensor_slices(({\"category\": category}, label))\r\nds = ds.batch(2)\r\n\r\nfc_category = tf.feature_column.indicator_column(\r\n    tf.feature_column.categorical_column_with_vocabulary_list(\r\n        \"category\", vocabulary_list=[\"A\", \"B\", \"C\"]\r\n    )\r\n)\r\nfeature_layer = tf.keras.layers.DenseFeatures([fc_category])\r\n\r\nmodel = tf.keras.Sequential(\r\n    [\r\n        feature_layer,\r\n        tf.keras.layers.Dense(10, activation=\"relu\"),\r\n        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\r\n    ]\r\n)\r\n\r\nmodel.compile(\r\n    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\r\n)\r\n\r\nmodel.fit(ds, epochs=2)\r\n\r\ntf.keras.models.save_model(model, \"model\")\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@emla2805 ,\r\nI tried executing the given code in latest [TF -nightly version 2.0.0.dev20190819](https://pypi.org/project/tf-nightly-2.0-preview/), I didnot face any issue. Can you please try in the latest version.Thanks", "@oanush you are right! It works with the latest TF-nightly, closing this issue. Thanks", "This error seems to be still present in stable tensorflow 2.1.0. \r\n@emla2805 I think it makes sense to re-open the issue ?", "I'm using `tf-2.2.0.dev20200315`  along with `tf_agents-0.4.0.dev20200315`, and I'm using PolicySavers for reinforcement learning.  I am calling the following: \r\n\r\n```\r\n        if self.use_N_agents:\r\n            # Get train and evaluation policies\r\n            self.train_savers = [PolicySaver(self.collect_policies[i],\r\n                                             batch_size=None) for i in\r\n                                 range(self.num_agents)]\r\n            self.eval_savers = [PolicySaver(self.eval_policies[i],\r\n                                            batch_size=None) for i in\r\n                                range(self.num_agents)]\r\n\r\n        else:\r\n            # Get train and evaluation policy savers\r\n            self.train_saver = PolicySaver(self.collect_policy, batch_size=None)\r\n            self.eval_saver = PolicySaver(self.eval_policy, batch_size=None)\r\n\r\ndef save_policies(self, epochs_done=0):\r\n# Multiple PPO agents\r\n        if self.use_N_agents:\r\n\r\n            # Iterate through training policies and save each of them\r\n            for i, train_saver in enumerate(self.train_savers):\r\n                train_save_dir = os.path.join(self.policy_save_dir, \"train\",\r\n                                              \"epochs_{}\".format(epochs_done),\r\n                                              \"agent_{}\".format(i))\r\n                if not os.path.exists(train_save_dir):\r\n                    os.makedirs(train_save_dir, exist_ok=True)\r\n                train_saver.save(train_save_dir)\r\n\r\n            print(\"Training policies saved...\")\r\n\r\n            # Iterate through eval policies\r\n            for i, eval_saver in enumerate(self.eval_savers):\r\n                eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\r\n                                             \"epochs_{}\".format(epochs_done),\r\n                                             \"agent_{}\".format(i))\r\n                if not os.path.exists(eval_save_dir):\r\n                    os.makedirs(eval_save_dir, exist_ok=True)\r\n                eval_saver.save(eval_save_dir)\r\n\r\n            print(\"Eval policies saved...\")\r\n\r\n        # Master PPO agent\r\n        else:\r\n            # Save training policy\r\n            train_save_dir = os.path.join(self.policy_save_dir, \"train\",\r\n                                          \"epochs_{}\".format(epochs_done))\r\n            if not os.path.exists(train_save_dir):\r\n                os.makedirs(train_save_dir, exist_ok=True)\r\n            self.train_saver.save(train_save_dir)\r\n\r\n            print(\"Training policy saved...\")\r\n\r\n            # Save eval policy\r\n            eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\r\n                                         \"epochs_{}\".format(epochs_done))\r\n            if not os.path.exists(eval_save_dir):\r\n                os.makedirs(eval_save_dir, exist_ok=True)\r\n            self.eval_saver.save(eval_save_dir)\r\n\r\n            print(\"Eval policy saved...\")\r\n\r\n# Here's where I call the function:\r\n    if i % self.save_interval == 0 and i == 0:\r\n           self.save_policies(epochs_done=i)\r\n           print(\"Epochs: {}\".format(i))\r\n```\r\nThis works fine if I only run it with a single PPO agent, but when I create multiple PPO agents (which I create deepcopies of to avoid overwriting/sharing parameters), I receive the following error, specifically when I try to save the training/collection policy:\r\n\r\n```\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"34253:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\nAppreciate any help or input you can offer, thank you!\r\n\r\n", "@q-55555 I cannot reproduce this issue with the TF 2.1.0 release. What platform and python version are you using?\r\n\r\n@rmsander I don't think your problem is related to this issue, you are not using feature_columns are you?", "@emla2805 I am using python 3.7.4 on Windows 10.\r\nI have tested on pre release tensorflow 2.2.0rc3 and the problem is not present."]}, {"number": 31685, "title": "How to append placeholder tf.gradient and things like that to a multiprocess.Manager.list", "body": "I want to share some objects between different process, but my class has some tensorflow object in it ,and triggered an error message\r\n\r\nProcess Process-7:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zkx74\\Anaconda3\\lib\\multiprocessing\\process.py\", line 297, in _bootstrap\r\n    self.run()\r\n  File \"C:\\Users\\zkx74\\Anaconda3\\lib\\multiprocessing\\process.py\", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\zkx74\\PycharmProjects\\DDPG_QuantTrade\\MADDPG.py\", line 162, in init_nn\r\n    agent_list.append(ACModel(actor, None, env, current_stock_state, current_agent_state))\r\n  File \"<string>\", line 2, in append\r\n  File \"C:\\Users\\zkx74\\Anaconda3\\lib\\multiprocessing\\managers.py\", line 795, in _callmethod\r\n    conn.send((self._id, methodname, args, kwds))\r\n  File \"C:\\Users\\zkx74\\Anaconda3\\lib\\multiprocessing\\connection.py\", line 206, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File \"C:\\Users\\zkx74\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\nTypeError: can't pickle _thread.RLock objects\r\n\r\n\r\nthe code is here: https://paste.ubuntu.com/p/Z54XbRkk8h/", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31685\">No</a>\n"]}, {"number": 31684, "title": "TimeDistributed does not propagate mask and prevents inner Sequential from propagating mask", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.14.0, 2.0.0b\r\n- Python version: 3.6.8\r\n\r\n**Code to reproduce the issue**\r\nThis fails due to failed assertion:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef check_mask(inputs, mask=None):\r\n    assert mask is not None\r\n    return inputs\r\n\r\nmodel = tf.keras.layers.TimeDistributed(tf.keras.models.Sequential([\r\n    tf.keras.layers.Masking(0),\r\n    tf.keras.layers.Lambda(check_mask)\r\n]))\r\n\r\nmodel(tf.convert_to_tensor([[[0]]]))\r\n```\r\n\r\nI would expect the mask to be propagated, as is normal in `Sequential`. The `TimeDistributed` seems to somehow prevent the inner `Sequential` from doing so.\r\n\r\nThis also fails:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef check_mask(inputs, mask=None):\r\n    assert mask is not None\r\n    return inputs\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Masking(0)),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Lambda(check_mask)),\r\n])\r\n\r\nmodel(tf.convert_to_tensor([[[0]]]))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis might be related to another issue I also submitted: https://github.com/tensorflow/tensorflow/issues/31638\r\n\r\n", "comments": ["I am able to replicate the issue with Tensorflow 2.0.0.beta1 on colab. Please take a look at gist [here](https://colab.research.google.com/drive/1z8DpMed2ft6Wxdq-79ldjo0ruZtAxia4). Thanks!", "In sequential and functional models, mask is propagated by the `_keras_mask` property (check [here](https://github.com/tensorflow/tensorflow/blob/1933f1b166cc141d9f85de1ff3772f96c2c3760c/tensorflow/python/keras/engine/sequential.py#L297) for the example in sequential model). However, the explicitly specified arg `mask` has higher priority than the private attribute `_keras_mask`. So you will always get `None` in the `check_mask` method as it's specified as `None` explicitly. \r\n\r\nYou may check [this example](https://github.com/tensorflow/tensorflow/blob/0659606b82c81516d52c2f449b7929b5d5e76c79/tensorflow/python/keras/layers/wrappers_test.py#L257) to see how `TimeDistributed` layer works with mask.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31684\">No</a>\n"]}, {"number": 31683, "title": "TFRT Built From Source does not Improve Inference Speed", "body": "[convertTFRT_frozenGraph.zip](https://github.com/tensorflow/tensorflow/files/3509287/convertTFRT_frozenGraph.zip)\r\n\r\n<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - \r\n- TensorFlow installed from (source or binary): built from source \r\n- TensorFlow version: **1.14**\r\n- Python version: **2.7**\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: **10.0.0 / 7.4.2**\r\n- GPU model and memory: 1080Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have built TensorFlow from Sources, and checked the \"yes\" option when asked about compiling with TensorRT. Graph builds, trt_only nodes exist, inference speed on SSD + FPN model is not optimised (actually slightly worse than the original, pre-optimization frozen_graph.pb\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBuild from source tensorFlow 1.14\r\nCreate optimised graph from original frozen_graph.pb\r\nInfer over optimised graph ====> **no speedup whatsoever**\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nUploaded conversion script.\r\nNotice I try to get outputs from all FPN levels (not just final feature map), in order to use those in another LSTM-type network classifier.\r\n\r\n----------------------------------------------------------------------\r\nPrints from optimization script : \r\n2019-08-16 12:25:39.310289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph\r\n2019-08-16 12:25:39.310333: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2836 nodes (-1660), 4183 edges (-1854), time = 728.073ms.\r\n2019-08-16 12:25:39.310337: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 2880 nodes (44), 4255 edges (72), time = 143.931ms.\r\n2019-08-16 12:25:39.310341: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2880 nodes (0), 4255 edges (0), time = 167.55ms.\r\n2019-08-16 12:25:39.310345: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 1907 nodes (-973), 3218 edges (-1037), time = 7242.89307ms.\r\ngraph_size(MB)(native_tf): 123.3\r\ngraph_size(MB)(trt): 336.9\r\nnum_nodes(native_tf): 4496\r\nnum_nodes(tftrt_total): 1907\r\n**num_nodes(trt_only): 94**\r\ntime(s) (trt_conversion): 10.6979\r\n**number of TRT ops in the converted graph :  94**\r\n\r\n\r\n-----------------------------------------------------------------------------------------------------------------\r\nJust tried using tf-nightly-gpu 1.15.0.dev20190816 , with Cuda 10.1 and cudnn 7.6, and the inference time is even worse! (with mode trt_only nodes as shows the print : )\r\n\r\ngraph_size(MB)(native_tf): 123.3\r\ngraph_size(MB)(trt): 159.7\r\nnum_nodes(native_tf): 4496\r\nnum_nodes(tftrt_total): 1894\r\nnum_nodes(trt_only): 102\r\ntime(s) (trt_conversion): 8.1412\r\nnumber of TRT ops in the converted graph :  102\r\n\r\n-----------------------------------------------------------------------------------------------------------------\r\n\r\nAverage [1:100] random images Inference Speeds : \r\noriginal .pb inference speed : **0.028s**\r\ntf1.14+trt5 compiled from source inference speed : **0.029**\r\ntf1.15+trt installed with pip : **0.032**\r\n\r\n**None** of these had errors when building or optimizing the graph :)\r\n\r\n\r\n\r\n", "comments": ["@PetreanuAndi you mentioned that you built from source using 1.14 branch, is that right? If so, could you share your `configure` output (the content of the `.tf_configure.bazelrc` file)? Also, I think the 1.14 branch is in a bad state now, could you try using the 1.15 branch instead?\r\n\r\nAlternatively, both [1.15rc1](https://pypi.org/project/tensorflow-gpu/1.15.0rc1/) and [2.0rc1](https://pypi.org/project/tensorflow-gpu/2.0.0rc1/) were out and TF-TRT was enabled in both, would you please try either of them and let me know?\r\n\r\nThanks.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 31682, "title": "Extended GRU.call() documentation", "body": "As requested here: https://github.com/tensorflow/tensorflow/pull/31646#pullrequestreview-275621107", "comments": []}, {"number": 31681, "title": "Debugging tf.Keras Models with TFDBG get TypeError: Fetch argument None has invalid type <class 'NoneType'>", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution ( Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): tensorflow-gpu  1.14.0     \r\n- Python version: python 3.7.1\r\n- CUDA/cuDNN version: CUDA 10.1  cuDNN 7.5\r\n- GPU model and memory: GTX 2060 \r\n\r\n**Describe the current behavior**\r\n\r\nI want to use debugging tf.keras model, but got an error message like:\r\n```sh\r\nTraceback (most recent call last):\r\n  File \"./debug_keras.py\", line 82, in <module>\r\n    vae.add_loss(loss_fn(x_in, x_out, \u03bc, \u03c3))\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 917, in add_loss\r\n    new_layers = base_layer_utils.create_keras_history(symbolic_loss)\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 200, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 244, in _create_keras_history_helper\r\n    constants[i] = backend.function([], op_input)([])\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 628, in wrapped_runner\r\n    callable_runner_args=feed_values)\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 569, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1158, in _run\r\n    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 474, in __init__\r\n    self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n  File \"/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 261, in for_fetch\r\n    type(fetch)))\r\nTypeError: Fetch argument None has invalid type <class 'NoneType'>\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be work...\r\n\r\n**Code to reproduce the issue**\r\n\r\nrun this python script, then type in:` run -t 10` will get an error message...\r\n```python\r\nimport tensorflow.python as tf\r\nfrom tensorflow.python import keras as k\r\nfrom tensorflow.python.keras import layers as kl\r\nfrom tensorflow.python.keras import activations as ka\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom scipy.stats import norm\r\nfrom scipy.special import expit\r\nfrom tensorflow.python import debug as tfdebug\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\nk.backend.set_session(tfdebug.LocalCLIDebugWrapperSession(tf.Session(config=config)))\r\n# k.backend.set_session(tf.Session(config=config))\r\n\r\n(x_train, y_train), (x_test, y_test) = k.datasets.fashion_mnist.load_data()\r\nx_train = np.expand_dims(x_train, -1) / 255.\r\nx_test = np.expand_dims(x_test, -1) / 255.\r\n\r\nimage_size = 28\r\ninput_shape = (image_size, image_size, 1)\r\nbatch_size = 100\r\nkernel_size = 3\r\nfilters = 16\r\nlatent_dim = 2  # \u9690\u53d8\u91cf\u53d62\u7ef4\u53ea\u662f\u4e3a\u4e86\u65b9\u4fbf\u540e\u9762\u753b\u56fe\r\nepochs = 30\r\ntf.set_random_seed(9102)\r\n\r\n\r\ndef encoder_fn(inputs, filters):\r\n    x = inputs\r\n    for i in range(2):\r\n        filters *= 2\r\n        x = kl.Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x)\r\n    x = kl.Flatten()(x)\r\n    x = kl.Dense(32, activation='relu')(x)\r\n    \u03bc = kl.Dense(latent_dim)(x)\r\n    \u03c3 = kl.Dense(latent_dim)(x)\r\n    return \u03bc, \u03c3\r\n\r\n\r\ndef sampling(args):\r\n    \"\"\" \u91cd\u53c2\u6570\u6280\u5de7 \"\"\"\r\n    \u03bc, \u03c3 = args\r\n    \u03b5 = tf.random_normal(shape=tf.shape(\u03bc))\r\n    return \u03bc + tf.exp(\u03c3 / 2) * \u03b5\r\n\r\n\r\ndef decoder_fn(z, filters):\r\n    x = kl.Dense(7 * 7 * 32, activation='relu')(z)\r\n    x = kl.Reshape((7, 7, 32))(x)\r\n    for i in range(2):\r\n        x = kl.Conv2DTranspose(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x)\r\n        filters //= 2\r\n    x = kl.Conv2DTranspose(1, kernel_size, activation=None, padding='same')(x)\r\n    return x\r\n\r\n\r\ndef loss_fn(inputs, outputs, \u03bc, \u03c3):\r\n    # \u8fd9\u91cc\u6c42\u548c\u7684\u65f6\u5019\u5148\u6309\u6bcf\u4e2a\u6837\u672c\u6c42\u548c\uff0c\u518d\u6309\u6837\u672c\u6c42\u5e73\u5747\r\n    xent_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=x_in, logits=x_out), axis=[1, 2, 3])\r\n    # xent_loss = tf.reduce_sum(k.backend.binary_crossentropy(x_in, x_out), axis=[1, 2, 3])\r\n    kl_loss = - 0.5 * tf.reduce_sum(1 + \u03c3 - tf.square(\u03bc) - tf.exp(\u03c3), axis=-1)\r\n    vae_loss = tf.reduce_mean(xent_loss + kl_loss)\r\n    return vae_loss\r\n\r\n\r\nx_in = k.Input(shape=(image_size, image_size, 1))\r\n\u03bc, \u03c3 = encoder_fn(x_in, filters)\r\nz = kl.Lambda(sampling, output_shape=(latent_dim,))([\u03bc, \u03c3])\r\n\r\nlatent_inputs = k.Input(shape=(latent_dim,), dtype=tf.float32)\r\noutputs = decoder_fn(latent_inputs, filters)\r\ndecoder = k.Model(latent_inputs, outputs)\r\nx_out = decoder(z)\r\n\r\n\r\nencoder = k.Model(x_in, \u03bc)\r\n\r\nvae = k.Model(x_in, x_out)\r\nvae.add_loss(loss_fn(x_in, x_out, \u03bc, \u03c3))\r\nvae.compile(k.optimizers.Nadam(0.001))\r\nvae.fit(x=x_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_test, None))\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with Tensorflow 1.14.0.", "@zhen8838, were you ever able to find a solution to this? I am running into the same issue!", "> @zhen8838, were you ever able to find a solution to this? I am running into the same issue!\r\n\r\nThis is a bug. You should try tensorflow 1.15 or tensorflow 2.0.  I'm not sure which version of the bug was fixed yet.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31681\">No</a>\n"]}, {"number": 31680, "title": "quantization: errror during quantizing bert model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux version 3.10.107-1-tlinux2_kvm_guest-0048, Red Hat 4.8.2-16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ni want to use quantize_graph.py to quantize bert model under tensorflow/tool/quantization on git branch r1.11, but load generated pb file generated error\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ni use the script to convert the model\r\n```\r\npython quantize_graph.py --input=/root/batch-bert-op/2layer_model.pb --output_node_names=\"classifier/predict_probability\" --output=/root/batch-bert-op/eight_bit.pb --mode=eightbit\r\n```\r\nin which 2layer_model.pb is the original pb and eight_bit.pb is the generated pb. 2layer_model.pb can be loaded and get inference result successfully but eight_bit.pb produce error. the code to load the pb file is:\r\n```\r\nwith tf.Session() as sess:\r\n    g = tf.Graph().as_default()\r\n    pb_file = sys.argv[1]\r\n    with open(pb_file, \"rb\") as f:\r\n        g_def = tf.GraphDef()\r\n        g_def.ParseFromString(f.read())\r\n        _ = tf.import_graph_def(g_def, name=\"\")\r\n```\r\nafter i remove this line in quantize_graph.py I can load quantized pb successfully but inference take 4s while the original pb takes only 200ms\r\n```\r\nself.set_input_graph(graph_util.remove_training_nodes(self.input_graph, protected_nodes=output_node_names))\r\n```\r\nprobability the error is introduced by remove_training_nodes, but without it, inference time is tremendous.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nThe full traceback is below:\r\n```\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n-I/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1\r\n2019-08-16 19:09:44.358947: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-08-16 19:09:44.384488: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494140000 Hz\r\n2019-08-16 19:09:44.384818: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7f2a68c2d470 executing computations on platform Host. Devices:\r\n2019-08-16 19:09:44.384885: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nOMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #213: KMP_AFFINITY: cpuid leaf 11 not supported - decoding legacy APIC ids.\r\nOMP: Info #149: KMP_AFFINITY: Affinity capable, using global cpuid info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-7\r\nOMP: Info #156: KMP_AFFINITY: 8 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #159: KMP_AFFINITY: 1 packages x 8 cores/pkg x 1 threads/core (8 total cores)\r\nOMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 5 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 6 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 7 \r\nOMP: Info #250: KMP_AFFINITY: pid 9830 tid 9830 thread 0 bound to OS proc set 0\r\n2019-08-16 19:09:44.419205: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 426, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bert/embeddings/cond/Slice/begin}}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"pb_inference_test.py\", line 122, in <module>\r\n    _ = tf.import_graph_def(g_def, name=\"\")\r\n  File \"/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 430, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bert/embeddings/cond/Slice/begin}}\r\n\r\n\r\n\r\n```\r\n", "comments": ["Are you quantize bert in CPU or in mobile phone.", "> Are you quantize bert in CPU or in mobile phone.\r\n\r\ni want to use it in CPU instead of mobile phone.", "@huanghaifeng1 ,\r\nIn order to expedite the trouble-shooting process, please provide compelete code snippet to reproduce the issue reported here. Thanks!", "I  also quantize bert model in CPU\uff0ci quantized it success\uff0cbut the output result is wrong\u3002precision is lost.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31680\">No</a>\n"]}, {"number": 31679, "title": "tf.subtract got different result in python , java", "body": "code: out = tf.subtract(input, output)\r\ngot different result in python vs java\r\n\r\nthe version is same ,   even the 1.9.0 ,1.13.1, 1.14.0\r\n", "comments": ["I sovle the prob,thk"]}, {"number": 31678, "title": "Error \"swig.exe failed: error executing command\" when build Tensorflow on MSVC", "body": "Hi All,\r\n\r\nI encountered the following error when building Tensorflow on MSVC, which looks like a configuration issue, but I don't know what was missing. Can you help me? I attached the build log. Thank you very much.\r\n\r\nEnvironment: VS2017 + Windows Server 2016 +Python3\r\n\r\nBuild log: [Tensorflow_build.log](https://github.com/tensorflow/tensorflow/files/3508725/Tensorflow_build.log)\r\n\r\n**Repro steps:**\r\n1. git clone  https://github.com/tensorflow/tensorflow F:\\tensorflow\\src\r\n2. install msys2 under F:\\tensorflow\\tools\r\n3. open VS 2017 tools command\r\n4. cd F:\\tensorflow\\src\r\n5. set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\\r\n6. set PATH=F:\\tensorflow\\tools;%path%\r\n7. set PATH=F:\\tensorflow\\tools\\msys64\\usr\\bin;%path%\r\n8. yes \"\" 2>nul | python ./configure.py\r\n9. bazel build --config=opt --subcommands //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Failures:**\r\nSUBCOMMAND: # //tensorflow/core/kernels:eigen_contraction_kernel_with_mkl [action 'Compiling tensorflow/core/kernels/eigen_contraction_kernel.cc']\r\ncd C:/users/cpptestlocal/_bazel_cpptestlocal/cy65jqax/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\\\MSBuild\\15.0\\bin;C:\\windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\Tools\\;;C:\\windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python/Python37/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\CPPTESTLOCAL\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TMP=C:\\Users\\CPPTESTLOCAL\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/eigen_contraction_kernel_with_mkl/eigen_contraction_kernel.obj /c tensorflow/core/kernels/eigen_contraction_kernel.cc\r\n**ERROR: F:/git_projects/tensorflow/tensorflow/tensorflow/lite/python/interpreter_wrapper/BUILD:58:1: SWIGing tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.i failed (Exit -1073741515): swig.exe failed: error executing command \r\n  cd C:/users/cpptestlocal/_bazel_cpptestlocal/cy65jqax/execroot/org_tensorflow**\r\nbazel-out/x64_windows-opt/bin/external/swig/swig.exe -c++ -python -module tensorflow_wrap_interpreter_wrapper -o bazel-out/x64_windows-opt/bin/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.cc -outdir bazel-out/x64_windows-opt/bin/tensorflow/lite/python/interpreter_wrapper -Iexternal/eigen_archive -Iexternal/swig -Ibazel-out/x64_windows-opt/bin -Ibazel-out/x64_windows-opt/bin/external/local_config_python -Iexternal/gemmlowp -Iexternal/com_google_absl -Iexternal/flatbuffers -Iexternal/arm_neon_2_x86_sse -Iexternal/farmhash_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.i\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 599.811s, Critical Path: 126.23s\r\nINFO: 1791 processes: 1791 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully", "comments": ["Hello, can someone help me? This issue still blocks me, but I don't know what I should do.", "Looks like some environment setup causing swig to fail.", "I actually cannot see the full failure message emitted by swig.\r\nCould you share the full terminal output using pastebin?", "I have attached the build log, that's all the output, does it not work? BTW, I try and I can reproduce this issue with the following step. I know it's not easy to repro since it looks like a configuration issue, if you need any info please let me know.\r\nbazel build --config=opt --subcommands //tensorflow/lite/python/interpreter_wrapper:tensorflow_wrap_interpreter_wrapper_py_wrap", "I can see that swig.exe failed. But I see no output from swig command itself.\r\n\r\n@meteorcloudy @laszlocsomor any ideas?", "@QuellaZhang : Could you try this please:\r\n1. open `cmd.exe`\r\n2. Run:\r\n    ```\r\n    md c:\\tmp\\gh31678\r\n\r\n    cd C:\\users\\cpptestlocal\\_bazel_cpptestlocal\\cy65jqax\\execroot\\org_tensorflow\r\n\r\n    bazel-out\\x64_windows-opt\\bin\\external\\swig\\swig.exe -c++ -python -module tensorflow_wrap_interpreter_wrapper -o c:/tmp/gh31678/tensorflow_wrap_interpreter_wrapper.cc -outdir c:/tmp/gh31678 -Iexternal/eigen_archive -Iexternal/swig -Ibazel-out/x64_windows-opt/bin -Ibazel-out/x64_windows-opt/bin/external/local_config_python -Iexternal/gemmlowp -Iexternal/com_google_absl -Iexternal/flatbuffers -Iexternal/arm_neon_2_x86_sse -Iexternal/farmhash_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.i\r\n    ```\r\n\r\nDoes this generate `c:\\tmp\\gh31678\\tensorflow_wrap_interpreter_wrapper.cc`? If not, what error does it print?\r\n", "@laszlocsomor  Sorry for the late. I try the step, it run successfully and generate \"C:\\tmp\\gh31678\\tensorflow_wrap_interpreter_wrapper.cc\"", "Hello, we have find the reason. swig.exe is crash due to missing \u201cVCRuntime140.dll\u201d, we have install vc_redist.x64.exe and Tensorflow build successfully.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31678\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31678\">No</a>\n"]}, {"number": 31677, "title": "[Intel MKL] Bugfix for mkl slice", "body": "We identified that mkldnn slice not working properly with block data format (nchw16c): it will crash when offset is 8.\r\n\r\nThis PR is to fix the problem.", "comments": []}, {"number": 31676, "title": "Can't import  tokenizer_from_json", "body": "When I  use 'keras.preprocessing.text.tokenizer_from_json', can't find.\r\nI check keras/preprocessing/text.py, find there is no tokenizer_from_json;\r\nThen add \"tokenizer_from_json = text.tokenizer_from_json\", is ok;\r\n![image](https://user-images.githubusercontent.com/54168679/63150263-ec7fa280-c038-11e9-9ac5-63018957ad79.png)\r\nand add \"from tensorflow.python.keras.preprocessing.text import tokenizer_from_json\"  in \r\n\"site-packages/tensorflow/_api/v1/keras/preprocessing/text/__init__.py\"\r\n![image](https://user-images.githubusercontent.com/54168679/63257680-ebef4200-c2ac-11e9-8c3b-e0e69146e343.png)\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31676\">No</a>\n", "#31946", "cool!"]}]