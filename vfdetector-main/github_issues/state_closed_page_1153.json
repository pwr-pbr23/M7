[{"number": 18616, "title": "QueueOptions var data types to size_t", "body": "`QueueOptions`' `max_batch_size` and `max_enqueued_batches` are positive quantities, and when compared, in the code, with unsigned member functions, a warning is raised. By changing the data type from `int` to `size_t`, not only the meaning of the member variables are more aligned with their intent, but also the comparisons are done between unsigned integers, thus fixing the warnings.", "comments": ["It seems all failures have a common theme; `n` out of `m` tests were executed and the specified size is too big. Looks like to be unrelated to the changes.\r\n\r\n```\r\nExecuted 508 out of 536 tests: 508 tests pass and 28 fail to build.\r\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n```", "\"28 fail to build\" is the suspicious part. Can you look at the compiler\nerrors?\n\nOn Thu, Apr 19, 2018 at 6:29 PM Dalmo Cirne <notifications@github.com>\nwrote:\n\n> It seems all failures have a common theme; n out of m tests were executed\n> and the specified size is too big. Looks like to be unrelated to the\n> changes.\n>\n> Executed 508 out of 536 tests: 508 tests pass and 28 fail to build.\n> There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18616#issuecomment-382835371>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXTkKLPc-Q1GnVMQgGoz2tJFu_1uks5tqNeggaJpZM4TZB8G>\n> .\n>\n\n\n-- \n - Alex\n", "It seems that this is the failing test: `//tensorflow/core:common_runtime_ring_reducer_test`\r\n\r\nThe error message is produced here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/ring_reducer_test.cc#L183\r\n\r\n```cpp\r\n    if (!dev_mgr_ || device_type == DEVICE_CPU) {\r\n      LOG(ERROR) << \"resetting dev_mgr for \" << local_devices.size()\r\n                 << \" devices: \";\r\n```\r\n\r\nBut the error seems to be failing to obtain a mutex: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/ring_reducer.cc#L210\r\n\r\n```cpp\r\n    mutex_lock l(status_mu_);\r\n    if (status_.ok()) {\r\n      LOG(ERROR) << \"Aborting RingReduce with \" << s;\r\n```\r\n\r\nAnd here are the log messages:\r\n```\r\n2018-04-19 17:49:27.698219: E tensorflow/core/common_runtime/ring_reducer_test.cc:184] resetting dev_mgr for 16 devices:\r\n2018-04-19 17:49:27.705532: E tensorflow/core/common_runtime/ring_reducer.cc:212] Aborting RingReduce with Internal: Deliberate failure\r\n2018-04-19 17:49:27.705581: W tensorflow/core/common_runtime/base_collective_executor.cc:189] BaseCollectiveExecutor::StartAbort Internal: Deliberate failure\r\n2018-04-19 17:49:27.705598: E tensorflow/core/common_runtime/ring_reducer.cc:212] Aborting RingReduce with Internal: Deliberate failure\r\n...\r\n```", "Hard to say why this one test is refusing to pass. As far as I can tell it seems to be unrelated to the change on this PR.\r\n\r\nIt fails because it times out.\r\n\r\n```\r\n//tensorflow/core:common_runtime_ring_reducer_test                      TIMEOUT in 480.5s\r\n  /home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/core/common_runtime_ring_reducer_test/test.log\r\n//tensorflow/core/grappler/optimizers:constant_folding_test              PASSED in 22.7s\r\n  Stats over 5 runs: max = 22.7s, min = 16.6s, avg = 20.7s, dev = 2.1s\r\nExecuted 457 out of 457 tests: 456 tests pass and 1 fails locally.\r\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n```", "This test is known to fail right now. A fix is ready and being tested for merge.", "We are having trouble with CI right now."]}, {"number": 18615, "title": "Add shape check to TFRecordDataset", "body": "The inputs of TFRecordDataset have the requirements for shapes.\r\n\r\nHowever, the check was not done in the shape function. This fix adds shape checks whenever possible.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18614, "title": "Remove duplicate import in compat.py", "body": "Noticed there are a couple of places in compat.py that have duplicate import:\r\n```\r\nfrom tensorflow.python.util.tf_export import tf_export\r\nfrom tensorflow.python.util.tf_export import tf_export\r\n```\r\n\r\nThis fix remove duplicate imports.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18613, "title": "Branch 193234819", "body": "", "comments": ["cc @jsimsa", "All the changes should be in for cherry-picks. cc @yk5 @ispirmustafa ", "Thanks!"]}, {"number": 18612, "title": "TFlite conversation failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.13.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.6\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.9\r\n- **CUDA/cuDNN version**: 9/7\r\n- **GPU model and memory**: 1080Ti & 12G\r\n- **Exact command to reproduce**:\r\n\r\nI wrote MobileNet code to train a model from scratch. Also tried using MobileNet keras function for training but none of the model converts to tflite.\r\n\r\nI am running following command\r\n\r\n```\r\n./bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=/MobileNet-trined.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=train-1.tflite --inference_type=FLOAT --input_arrays=input_1 --output_arrays=dense_1/Sigmoid --allow_custom_ops\r\n```\r\n\r\nIt gives me following result\r\n\r\n```\r\n2018-04-17 15:25:11.070338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1236] Converting unsupported operation: RandomUniform\r\n2018-04-17 15:25:11.098993: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 865 operators, 1364 arrays (0 quantized)\r\n2018-04-17 15:25:11.139921: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 809 operators, 1280 arrays (0 quantized)\r\n2018-04-17 15:25:11.183528: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 809 operators, 1280 arrays (0 quantized)\r\n1984959600\r\n2006980784\r\n2018-04-17 15:25:11.184242: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:90] Check failed: mean_shape.dims() == multiplier_shape.dims() \r\nAbort trap: 6\r\n```", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/17684\r\nAs GarryLau points out at issue 17684 - do not use training graph but export evaluation graph that you then freeze. ", "@svarjo Can you help me with how to generate eval.pbtxt? I tried using mobilenet_v1_eval but it generates nothing ", "Study export_inference_graph.py example in slim folder for exporting inference graph. After that you have to combine model with the trained variables using tensorflow freeze_graph tool (ie call for example 'python3 -m tensorflow.python.tools.freeze_graph' with suitable input_graph and checkopoint).", "Nagging Assignee @shivaniag: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 62 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 77 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please note that you need to create an eval graph, freeze it and send it to toco for conversion. Here it looks like you are using a training graph. If you are using Keras, you can export to saved model and pass that to tflite_convert (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/python/tflite_convert.py)\r\n\r\n", "Please reopen if you are still encountering this issue."]}, {"number": 18611, "title": "Improve shape function check for `tf.roll`", "body": "The `tf.roll` op has requirements for the shape of inputs. However, the shape of the inputs are only done at the runtime inside the kernel.\r\n\r\nThis fix improve the shape function so that the check could be done early if shape is already known in the shape function.\r\n\r\nThe following validations have been added in the shape function with test cases:\r\n- The `input` must be 1-D or higher\r\n- The `shift` must be scalar or 1-D.\r\n- The `axis` must be scalar or 1-D.\r\n- The  `shift` and `axis` should be the same size.\r\n\r\nThey matches validations in the kernel.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18610, "title": "Using xrange from six", "body": "In python 2 vs 3 xrange is different. This fix is an enhancement to use xrange from six, instead of additional logic of handling xrange in python 2 vs python 3.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["nvm just fixed it.", "Thanks @martinwicke for the help.\r\n\r\nI looked into the build failure. One failure:\r\n```\r\n//tensorflow/core:common_runtime_ring_reducer_test\r\n```\r\nseems to be unrelated as well.", "Yeah, known failure. "]}, {"number": 18609, "title": "Branch 193218331", "body": "Manually merged:\r\ntensorflow/contrib/data/python/kernel_tests/sequence_dataset_op_test.py\r\ntensorflow/contrib/data/python/ops/batching.py\r\ntensorflow/contrib/metrics/python/ops/metric_ops.py\r\ntensorflow/contrib/tensorrt/convert/convert_nodes.cc\r\ntensorflow/core/common_runtime/process_util.cc\r\ntensorflow/core/kernels/cwise_op_clip.cc\r\ntensorflow/core/kernels/cwise_op_clip.h\r\ntensorflow/core/kernels/cwise_op_clip_gpu.cu.cc\r\ntensorflow/core/kernels/maxpooling_op.cc\r\ntensorflow/python/framework/dtypes.py\r\ntensorflow/python/framework/tensor_shape_test.py\r\ntensorflow/python/kernel_tests/clip_ops_test.py\r\ntensorflow/python/ops/clip_ops.py\r\ntensorflow/python/util/tf_inspect.py\r\ntensorflow/tools/api/generator/create_python_api.py\r\ntensorflow/tools/docs/parser.py\r\ntensorflow/tools/docs/parser_test.py", "comments": ["I've seen this Windows failure before, and it seemed like a flake. Not sure though.", "cc @meteorcloudy looks like the error is still here?  Might need to revert cl/193168327.", "@yifeif Sorry for that, this is strange, let me disable remote cache on github presubmit"]}, {"number": 18608, "title": "Make signal warning more clear", "body": "Before the change:\r\n```tensorflow.python.framework.errors_impl.InvalidArgumentError: Bad audio format for WAV: Expected 1 (PCM), but got3 [Op:DecodeWav]```\r\n\r\nAfter the change:\r\n```tensorflow.python.framework.errors_impl.InvalidArgumentError: Bad audio format for WAV: Expected 1 (PCM), but got audio format number 3 [Op:DecodeWav]```", "comments": ["I removed the duplicate mention of \"audio format number\", and changed it to chunk size. \r\n\r\nThanks for the fix!", "@martinwicke Is there a lookup dictionary of audio format integer to string? It's not clear what audio format 3 is.", "http://www-mmsp.ece.mcgill.ca/Documents/AudioFormats/WAVE/WAVE.html\r\n\r\n1 is PCM, 3 is floats. We could replicate this table, but I don't think it's worth it.\r\n\r\n", "@martinwicke Replicated the table! The number \"3\" is not easy to understand! ", "Please look at the build log and fix the errors. Sample:\r\n\r\n```\r\ntensorflow/core/lib/wav/wav_io.cc:233:26: error: no match for 'operator<<' (operand types are 'std::string {aka std::basic_string<char>}' and 'const char [11]')\r\n         audio_format_str << \"IEEE FLOAT\";\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "I am facing with same error.\r\n\r\n''' InvalidArgumentError (see above for traceback): Bad audio format for WAV: Expect\r\ned 1 (PCM), but got3\r\n         [[node data/DecodeWav (defined at H:\\Trainig_Tensorflow\\1 training_back\r\nup_dnn - Cancel\\input_data.py:360)  = DecodeWav[desired_channels=1, desired_samp\r\nles=16000, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](data/ReadFile\r\n)]] '''\r\n\r\nI am trying to train own audio sample dataset which is 16-bit signed (Mono ) PCM samples.\r\n\r\nFor audio recognition  using tensorflow , which properties are required?"]}, {"number": 18607, "title": "[tf.data] Fix a device placement issue in `prefetch_to_device()`.", "body": "Previously, the `iterator_get_device()` op was being infeasibly colocated with\r\nboth the iterator and placed on the prefetch target device. Move the\r\nconstruction of that op outside the `with device():` block to fix this.\r\n\r\nAlso enable the relevant test to run as a CUDA test.", "comments": []}, {"number": 18606, "title": "Make wignal warning more clear", "body": "Before the change:\r\n```tensorflow.python.framework.errors_impl.InvalidArgumentError: Bad audio format for WAV: Expected 1 (PCM), but got3 [Op:DecodeWav]```\r\n\r\nAfter the change:\r\n```tensorflow.python.framework.errors_impl.InvalidArgumentError: Bad audio format for WAV: Expected 1 (PCM), but got audio format number 3 [Op:DecodeWav]```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 18605, "title": "Improve deprecation assignment with deprecated_argument_lookup", "body": "This PR is to improve deprecation assignment with deprecated_argument_lookup, which is used in all other arguments deprecations.\r\nThe `tf.losses.cosine_distance` deprecated `dim` and switched to `axis`. ", "comments": ["This is an odd failure, with no detail link. I'm assuming this is a Kokoro failure?\r\n\r\n"]}, {"number": 18604, "title": "No module named tensorflow.tools", "body": "### System information\r\n- **OS Platform and Distribution**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Used binary\r\n- **GCC/Compiler version (if compiling from source)**: Used binary\r\n- **CUDA/cuDNN version**: No GPU \r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: from tensorflow import tools\r\n\r\n\r\n### Describe the problem\r\nI am trying to run the tensorflow to onnx converter (https://github.com/onnx/tensorflow-onnx) and for that reason I had to use some outdated versions of tf and onnx. I finally managed to get everything installed correctly, when I try to run any example I get the message that there is no module named tensorflow \r\n\r\nI checked this [previous issue from last year](https://github.com/tensorflow/tensorflow/issues/9778) and from what I understood, tf tools weren't supported on Windows. Is that still the case? Is there any workaround? \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Edited the original post, closed by mistake.", "The best way to see if the situation has changed is by using the latest version of TF.\r\n\r\n@gunan has the situation described in https://github.com/tensorflow/tensorflow/issues/9778 changed?\r\n\r\nWe also don't support most third-party use cases by default (e.g. Onnx), but it looks to me like Onnx would be best run on a Linux platform, not Windows. You could try using Docker or Vagrant for this, but the specifics would be a question for [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow).", "No, on windows situation has not changed. Libraries under tools still have POSIX dependencies, that are still unavailable on windows. I do not expect the situation to change.\r\nFor the onnx converter, please reach out to the owners of the repository/tool, as we do not own it.\r\nAll I can recommend from TF side is as @angersson suggested, to use linux or macos."]}, {"number": 18603, "title": "Fix deprecateion warnings for tf.initialize_variables", "body": "This PR is to fix deprecateion warnings for tf.initialize_variables.\r\nThe `initialize_variables` has been deprecated and replaced with `tf.variables_initializer` according to [tf.initialize_variables](https://www.tensorflow.org/api_docs/python/tf/initialize_variables).\r\n\r\nThis fix makes the change and fixes the following warning in *_test.py:\r\n> initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\n> Instructions for updating: Use `tf.variables_initializer` instead.", "comments": []}, {"number": 18602, "title": "Cuda 7.5 Configuration Error on r1.7 ", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: r1.7\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.11 \r\n- **GCC/Compiler version (if compiling from source)**: 5.3.1\r\n- **CUDA/cuDNN version**: 7.5/5.1.3\r\n- **GPU model and memory**: Tesla K20\r\n\r\n--------------------------\r\n\r\nWe are trying to compile tf from source, tried both the r1.7 and master branch. Both gives out the following error message,\r\n\r\n```\r\nSingularity nvidia.img:~/tensorflow> bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n..............\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1001, in _create_local_cuda_repository\r\n\t\t_find_nvvm_libdevice_dir(repository_ctx, cuda_config)\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 724, in _find_nvvm_libdevice_dir\r\n\t\tauto_configure_fail((\"Cannot find libdevice.10.bc un...))\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-7.5\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1001, in _create_local_cuda_repository\r\n\t\t_find_nvvm_libdevice_dir(repository_ctx, cuda_config)\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 724, in _find_nvvm_libdevice_dir\r\n\t\tauto_configure_fail((\"Cannot find libdevice.10.bc un...))\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow version\nExact command to reproduce", "Marking this as community support as unfortunately we don't have the bandwidth to diagnose problems in configurations other than the [tested ones](https://www.tensorflow.org/install/install_sources#tested_source_configurations).\r\n\r\nNothing obvious strikes me in the error messages you alluded to, hopefully someone else in the community will have some ideas.", "@jerrin92: the relevant error is \r\n`Cuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-7.5`\r\n\r\nYou probably have a `libdevice.*.10.bc` that you can either rename or link to `/usr/local/cuda-7.5/nvvm/libdevice/libdevice.10.bc` which is where it's needed (at least according to your logs).  If you don't have or can't find a `libdevice.*.10.bc` file, I'd suggest you try reinstalling tf cuda requirements.  "]}, {"number": 18601, "title": "Fix tf.compat.as_str returns bytes issue in Python 3", "body": "This fix tries to address the issue raised in #18598 where tf.compat.as_str returns bytes (vs. str) in Python 3.\r\n\r\nThe issue was that `tf_export` decorator:\r\n```\r\n@tf_export('compat.as_bytes', 'compat.as_str')\r\n```\r\ncould not be assigned to `as_bytes` or `as_text` based on python 2 or 3.\r\n\r\nThis fix invokes tf_export explicitly based on `_six.PY2` (for python 2/3) so that `as_str` calls `as_bytes` or `as_text` conditionally.\r\n\r\nThis fix fixes #18598.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 18600, "title": "Switch slim.learning.train to use tf.train.MonitoredTrainingSession, and expose sessionrunhooks", "body": "Hi! \r\n\r\nI've been getting warnings when using slim.learning.train about switching to tf.train.MonitoredTrainingSession. Is there a plan to make the switch?\r\n\r\nThanks!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code No\r\nOS Platform and Distribution Ubuntu 16.04\r\nTensorFlow installed from virtualenv\r\nTensorFlow version 1.7.0\r\nBazel version 0.10\r\nCUDA/cuDNN version 9.0/7.0\r\nGPU model and memory Titan X\r\nExact command to reproduce N/A \r\n", "@krishnashankar Could you try TensorFlow 1.11.0 and see if this issue is still there?", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to inactivity, feel free to reopen if problem persists."]}, {"number": 18599, "title": "Fix unintialized var warning in bfloat16", "body": "This contribution initializes `result` to 0, then inside the `#if` statement only one byte needs to be set, depending on the endian, the other will already be zero from the initialization. This also fixes the compilation warning.", "comments": ["The fail seems to be unrelated.\r\n\r\n```\r\ngzip: stdin: not in gzip format\r\ntar: Child returned status 1\r\ntar: Error is not recoverable: exiting now\r\n================================================================================\r\n//tensorflow/tools/ci_build/builds:gen_android_out                       FAILED in 4.1s\r\n  /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/tools/ci_build/builds/gen_android_out/test.log\r\n```"]}, {"number": 18598, "title": "tensorflow-1.8.0rc0: tf.compat.as_str returns bytes for python3 since 20180409", "body": "The issue appeared first in `tf-nightly==1.8.0.dev20180409` but is now present in `tensorflow==1.8.0rc0`.\r\n\r\nReproduce steps:\r\n\r\n```\r\n$ python3 -c \"import tensorflow as tf; print(tf.VERSION, type(tf.compat.as_str('hello')) == str)\r\n```\r\n\r\nIs expected to always print \"True\".  But gets:\r\n\r\n```\r\n# tensorflow\r\n1.6.0 True\r\n1.7.0 True\r\n1.8.0-rc0 False           <= Broken!\r\n\r\n# tf-nightly\r\n1.8.0-dev20180408 True\r\n1.8.0-dev20180409 False   <= Broken!\r\n```", "comments": ["@annarev When I trace through this code in Python 3, it does indeed look like `tf.compat.as_str()` is mapped to this function:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3128b43eb0bf37ac3c49cb22a6e1789d8ea346e8/tensorflow/python/util/compat.py#L48-L68\r\n\r\nHowever, if I do `from tensorflow.python.util import compat`, `compat.as_str()` maps to `as_text()` via (presumably) this appropriate redirection:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3128b43eb0bf37ac3c49cb22a6e1789d8ea346e8/tensorflow/python/util/compat.py#L93-L97\r\n\r\nIs it possible that the `@tf_export()` decorator is rebinding the symbol incorrectly? From what I can tell, it has the correct behavior in Python 2, but should have different behavior in Python 3.", "@mrry @annarev I think the tf_export could be explicitly called based on python 2 or 3 to address the issue. The following diff might work:\r\n```diff\r\ndiff --git a/tensorflow/python/util/compat.py b/tensorflow/python/util/compat.py\r\nindex 4163fca..738479c 100644\r\n--- a/tensorflow/python/util/compat.py\r\n+++ b/tensorflow/python/util/compat.py\r\n@@ -45,7 +45,6 @@ from tensorflow.python.util.tf_export import tf_export\r\n from tensorflow.python.util.tf_export import tf_export\r\n \r\n \r\n-@tf_export('compat.as_bytes', 'compat.as_str')\r\n def as_bytes(bytes_or_text, encoding='utf-8'):\r\n   \"\"\"Converts either bytes or unicode to `bytes`, using utf-8 encoding for text.\r\n \r\n@@ -68,7 +67,6 @@ def as_bytes(bytes_or_text, encoding='utf-8'):\r\n                     (bytes_or_text,))\r\n \r\n \r\n-@tf_export('compat.as_text')\r\n def as_text(bytes_or_text, encoding='utf-8'):\r\n   \"\"\"Returns the given argument as a unicode string.\r\n \r\n@@ -93,8 +91,12 @@ def as_text(bytes_or_text, encoding='utf-8'):\r\n # Convert an object to a `str` in both Python 2 and 3.\r\n if _six.PY2:\r\n   as_str = as_bytes\r\n+  tf_export('compat.as_bytes', 'compat.as_str')(as_bytes)\r\n+  tf_export('compat.as_text')(as_text)\r\n else:\r\n   as_str = as_text\r\n+  tf_export('compat.as_bytes')(as_bytes)\r\n+  tf_export('compat.as_text', 'compat.as_str')(as_text)\r\n \r\n \r\n @tf_export('compat.as_str_any')\r\n```", "Added a PR #18601 for the fix. Please take a look to see if it fixes the issue.", "Can we get this merged in 1.8? \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/util/compat.py", "@andresusanopinto I added a PR #18662 against r1.8 to carry the fix. As 1.8.0 release is very close, not sure if the fix could be applied in time though.", "Thanks @yongtang!"]}, {"number": 18597, "title": "Add deprecated_args decoration to array_ops/ sparse_ops", "body": "This PR is to add deprecated_args decoration to several ops in array_ops/ sparse_ops as below:\r\n- [tf.reverse_sequence](https://www.tensorflow.org/api_docs/python/tf/reverse_sequence)\r\n- [tf.sparse_concat](https://www.tensorflow.org/api_docs/python/tf/sparse_concat)\r\n- [sparse_split](https://www.tensorflow.org/api_docs/python/tf/sparse_split)\r\n", "comments": ["(OK for API change)", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There are some sanity build issues introduced by this PR:\r\n```\r\n53 FAIL: Found 4 non-whitelited pylint errors:\r\n54 tensorflow/python/ops/sparse_ops.py:87: [C0301(line-too-long), ] Line too long (94/80)\r\n55\r\n56 tensorflow/python/ops/sparse_ops.py:594: [C0301(line-too-long), ] Line too long (92/80)\r\n57\r\n58 tensorflow/python/ops/array_ops.py:2622: [C0301(line-too-long), ] Line too long (92/80)\r\n59\r\n60 tensorflow/python/ops/array_ops.py:2623: [C0301(line-too-long), ] Line too long (98/80)\r\n```\r\n\r\nCreated a PR #19327 for the pylint fix.", "@yifeif was this Kokoro swallowing results without telling us? I only saw the summary view all green and merged, but it only shows GPU and Ubuntu CC -- I believe those shouldn't even run if sanity breaks, no?\r\n\r\nThank you @yongtang for the fix!", "This was due to the Kokoro flag. It should be fixed with the rollback. We are no long blocking other builds with sanity. But we can add all builds to merge requirements(currently only cc build is)."]}, {"number": 18596, "title": "Dynamically change image classifier.", "body": "This is an UI improvement feature.\r\n\r\nCurrently, user has to build two apks with one classifier each.\r\nThis feature makes user to switch between classifiers by swiping\r\nthe screen.\r\n", "comments": ["@aselle any update?", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 74 days with no activity and the `awaiting review` label has been applied.", "@zhongleiwang  wondering if you still need this PR , if yes can you please resolve conflicts"]}, {"number": 18595, "title": "import_scoped_meta_graph() got an unexpected keyword argument 'return_elements'", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.3 \r\n- **CUDA/cuDNN version**: CUDA/9.0\r\n\r\n### Describe the problem\r\n\r\n`trans, = tf.train.import_meta_graph('./Model/DehazeNet_model_1gpu.ckpt.meta'\r\n                                                           , input_map={'batch:0': X} , return_elements=['clip_by_value:0'])`\r\n\r\nthere raise the error: **TypeError: import_scoped_meta_graph() got an unexpected keyword argument 'return_elements'**\r\n\r\nif the keyword argument 'return_elements' in import_scoped_meta_graph()  has been removed in 1.7.0 ?\r\n\r\n", "comments": ["@ysw361564483 Based on some searching, I don't think `return_elements` has ever been supported in `import_meta_graph`. Are you thinking of `tf.import_graph_def`?\r\n", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could you provide a stack trace? How was `DehazeNet_model_1gpu` created?", "It has been 22 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 37 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18594, "title": "floating point of image classifier is broken in tflite demo", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Zhongleis-MBP 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Zhongleis-MBP 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIn current tflite demo, if I enable floating point image classifier, the app doesn't work.\r\n\r\nThe error message is\r\n04-17 19:56:31.538 29418 29418 E TfLiteCameraDemo: Exception java.lang.NullPointerException: Can not allocate memory for the interpreter\r\n\r\nI think it's caused by\r\n\r\n$ git show 91c3199\r\ncommit 91c31997e6854a3d07acc76381cff7436df1c1dd\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Fri Apr 13 08:12:42 2018 -0700\r\n\r\n    Add support to TFLite for dilated convolution.\r\n\r\n    PiperOrigin-RevId: 192770919\r\n\r\n### Source code / logs\r\nThe error message is\r\n04-17 19:56:31.538 29418 29418 E TfLiteCameraDemo: Exception java.lang.NullPointerException: Can not allocate memory for the interpreter\r\n", "comments": ["@zhongleiwang How did you determine that it was due to that commit? That just sounds like you're out of memory.", "I'm bit puzzled by out of memory problem, however, git bisect seems to indicate the breakage happened from this cl. Maybe the author say more.\r\n", "I have the same issue. However I'm on TF 1.7.0 release.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18593, "title": "unsupported operand type(s) for *=: 'float' and 'NoneType'", "body": "any one happened this?\r\n\r\n\r\ntf_X = tf.placeholder(tf.float32, [None, 8, 8, 1])\r\ntf_Y = tf.placeholder(tf.float32, [None, 10])\r\n\r\n\r\n\r\nconv_out2 = tf.nn.conv2d(relu_feature_maps1, conv_filter_w2, strides=[1, 2, 2, 1], padding='SAME',use_cudnn_on_gpu=False) + conv_filter_b2\r\n\r\nprint(\"log+++\")\r\nprint (type(conv_out2.get_shape()[0].value))\r\n\r\n\r\n\r\nbatch_mean, batch_var = tf.nn.moments(conv_out2, axes=[0, 1, 2])", "comments": ["@naniqingkuang Works fine for me, assuming you're defining `relu_feature_maps1`, `conv_filter_w2`, and `conv_filter_b2` like in [this](https://www.kaggle.com/zbasper/test-for-cnn).\r\n", "Please fill out the issues template and give a small self-contained code example reproducing the issue. The above code is not self contained because there are some undefined variables like `relu_feature_maps1`.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18592, "title": "[Deprecation Warning] -  tf.contrib.layers.xavier_initializer", "body": "Hello dear friends,\r\n\r\nIf you run the following code, you will have a deprecation warning in version 1.7.0\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(\"TF Version:\", tf.__version__)\r\na = tf.contrib.layers.xavier_initializer()\r\n```\r\n\r\n```raw\r\nTF Version: 1.7.0\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating: Use the retry module or similar alternatives.\r\n```\r\n\r\nFor information, this warning did not appear in version **1.6.0**, it appears from **1.7.0**.\r\n\r\nHave a good day,\r\n\r\nJonathan", "comments": ["Can replicate the problem, with an even simpler command:\r\n\r\n    from tensorflow.contrib import layers\r\n\r\nSeems like the problem is with the `__init__.py` in that folder.\r\n\r\nWhy it's loading `tensorflow/contrib/learn/python/learn/datasets/base.py` I have no idea.", "Replaced deprecation warning with raising an error, here's the traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 35, in <module>\r\n    from tensorflow.contrib import distributions\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distributions/__init__.py\", line 38, in <module>\r\n    from tensorflow.contrib.distributions.python.ops.estimator import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distributions/python/ops/estimator.py\", line 21, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/__init__.py\", line 95, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/__init__.py\", line 28, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn import datasets\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\", line 31, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets import base\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 198, in <module>\r\n    @retry(initial_delay=1.0, max_delay=16.0, is_retriable=_is_retriable)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 243, in new_func\r\n    raise ValueError(\"dep\")\r\nValueError: dep\r\n```", "Seems to be fixed in master lol. https://github.com/tensorflow/tensorflow/commit/fdec18588d7f8b5f6383601f1030ed71f634d1c0\r\n\r\nRecommend close.", "@cchan just tested with 1.8.0rc0 you are absolutely right.\r\nClosing this issue"]}, {"number": 18591, "title": "Branch 193152683", "body": "", "comments": []}, {"number": 18590, "title": "Distributed Tensorflow:tensorflow prediction ", "body": "I want to use many arm as tensorflow cluster to prediction.but I can't find some helpful demo or instructions about it.\r\nIn tensorflow distributed demo training we can see\r\nCluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\nCluster from the parameter server and worker hosts, I would like to know that the computing cluster created by the code here is only for training? What should I do if I want to create a cluster for prediction?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "thanks for your ask,I runing tensorflow mobile_net on Raspberry Pi3 Raspbian OS according to the tutorial.I download tensorflow-1.5.0-cp35-none-linux_armv7l.whl and install tensorflow by pip.\r\nI have a model that was trained in PC.And now I want to implement real-time prediction.but model  prediction is very slow.\r\nSo I want to use the tensorflow distributed computing approach to speed up my model prediction.\r\nBut the problem is that I can't get relevant information about tensorflow distributed prediction.\r\nIf you can give me relevant information or demo I will be very grateful. Thanks", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18589, "title": "Add decode uint16 PNG images support for tf.image.decode_image.", "body": "", "comments": []}, {"number": 18588, "title": "XLA implementation of FFT on CPU pulls in tf/core:framework", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos 10.11\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: commit 63c6562df68ade3a03481874a71b536a4e02b6f5 (master as of April 15 2018)\r\n- **Python version**:  n/a\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: `bazel build --config=opt //tensorflow/compiler/xla/service/cpu:runtime_fft`. It *works*, but see below.\r\n\r\n### Describe the problem\r\nShort version: The CPU implementation of the XLA FFT operation appears to pull in `tensorflow::Tensor` and `...::TensorShape` as a dependency, via `//tensorflow/core:framework`.\r\n\r\nLong version: The FFT implementation comes in three flavors; real-to-complex, complex-to-real, and complex-to-complex. The first two flavors involve allocating a temporary buffer for an intermediate step in the computation. This is currently achieved by creating a `tensorflow::Tensor` object. This requires linking against `//tensorflow/core:framework`.\r\n\r\nThis feels like a bug, or at least unintentional and undesirable. For instance, every other op listed in `tensorflow/compiler/xla/service/cpu/BUILD`, besides runtime_fft, depends only on `:framework_lite`. My understanding re: allocating temporary space was that (at least for the AOT compiler, not sure about JIT) there were specific temporary buffers set aside, and that allocation should work through that system; not by just letting malloc run wild. Is that understanding correct? (*Aside: Eigen's FFT op internally calls malloc, regardless of FFT flavor, which likewise bypasses the AOT temporary buffers. Is that ok?*)\r\n\r\nIs anyone currently working on this? (Has anyone noticed anything awry?) If I were to try fixing this myself, would anyone have any suggestions on how to allocate a temporary buffer in an XLA-friendly way? I thought one possibility would be writing an Algebraic Simplifier pass to rewrite real-to-complex and complex-to-real flavors in terms of the complex-to-complex flavor (which doesn't need to create a `tf::Tensor`), but that's my only idea.\r\n\r\n### Source code / logs\r\nReferences:\r\n[tensorflow/compiler/xla/service/cpu/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD)\r\n[tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h)", "comments": ["Update: I went ahead and fixed it. If it's ok, I'll open a PR (after I run the unit tests). It still allocates a buffer on the fly, so the memory profiling tools won't catch it, but it'll do it with an `Eigen::Tensor` instead of a `tf::Tensor` so it won't need to link in `core:framework`. I also added a proper option for single-threaded FFT (status quo assigns the multi-threaded version even when single-threaded is requested).", "Nagging Assignee @sanjoy: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed in #18685"]}, {"number": 18587, "title": "Add decode uint16 PNG images support for tf.image.decode_image.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}]