[{"number": 14051, "title": "ci_build CPU tests failing locally", "body": "### System information\r\n\r\nRunning docker on macOS Sierra 10.12.3, summary from `ci_build.sh` output below\r\n```\r\n{ \r\n  container_type: \"cpu\", \r\n  command: \"bazel test //tensorflow/contrib/distributions/...\", \r\n  source_HEAD: \"4bdccb274954260ed300710a7fde4750e918ca7e\", \r\n  source_remote_origin: \"git@github.com:cshenton/tensorflow.git\", \r\n  OS: \"Linux\", \r\n  kernel: \"4.9.49-moby\", \r\n  architecture: \"x86_64\", \r\n  processor: \"Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\", \r\n  processor_count: \"4\", \r\n  memory_total: \"2046752 kB\", \r\n  swap_total: \"1048572 kB\", \r\n  Bazel_version: \"Build label: 0.5.4\", \r\n  Java_version: \"1.8.0_141\", \r\n  Python_version: \"2.7.6\", \r\n  gpp_version: \"g++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\", \r\n  swig_version: \"\", \r\n  NVIDIA_driver_version: \"\", \r\n  CUDA_device_count: \"0\", \r\n  CUDA_device_names: \"\", \r\n  CUDA_toolkit_version: \"\"\r\n}\r\n```\r\n\r\n### Describe your problem\r\n\r\nWhen attempting to run tests locally, on a branch of my fork with the CI scripts:\r\n```bash\r\ntensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/contrib/distributions/...\r\n```\r\n\r\nI am getting C++ compilation failures at varying times during the build process, for example at this point:\r\n```\r\n[2,060 / 2,070] Compiling tensorflow/core/kernels/remote_fused_graph_rewriter_transform.cc\r\n```\r\nAnd previously at this point\r\n```\r\n[1,463 / 1,475] Compiling tensorflow/core/kernels/sparse_tensors_map_ops.cc\r\n```\r\n\r\nIn both cases, the error is the same or similar, for the former:\r\n\r\n```bash\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:2518:1: C++ compilation of rule '//tensorflow/core/kernels:argmax_op' failed (Exit 4): gcc failed: error executing command\r\n  (cd /Users/charlesshenton/github/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_charlesshenton/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/nsync -iquote bazel-out/local-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/jemalloc -iquote bazel-out/local-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/local-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/curl -iquote bazel-out/local-opt/genfiles/external/curl -iquote external/boringssl -iquote bazel-out/local-opt/genfiles/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/local-opt/genfiles/external/jsoncpp_git -iquote external/aws -iquote bazel-out/local-opt/genfiles/external/aws -isystem external/nsync/public -isystem bazel-out/local-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/jemalloc/include -isystem bazel-out/local-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/local-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -isystem external/curl/include -isystem bazel-out/local-opt/genfiles/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/local-opt/genfiles/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/local-opt/genfiles/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/local-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/local-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/kernels/argmax_op.cc -o bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.pic.o).\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\n```", "comments": ["Similar issue running this again, except this time, got up to:\r\n```\r\n[2,219 / 2,224] Still waiting for 4 jobs to complete:\r\n      Running (local):\r\n        Compiling tensorflow/core/kernels/cwise_op_greater_equal.cc, 33 s\r\n        Compiling tensorflow/core/kernels/segment_reduction_ops.cc, 30 s\r\n        Compiling tensorflow/core/kernels/cwise_op_asinh.cc, 11 s\r\n        Compiling tensorflow/python/eager/pywrap_tensor.cc, 10 s\r\n```", "On MacOS, we use clang. Your build seems to be using GCC.\r\nWe have not tested these builds and it is possible these are broken.\r\nHowever, these log lines give me the impression that you are running into a compiler bug:\r\n\r\n```\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\n```\r\n\r\nThese lines are emitted by GCC, so I recommend reaching out to GCC developers.", "This error is from running the CI script, which builds a ubuntu based docker container, so I think gcc is correct in that case? \r\n\r\nI'm following the instructions on the [contributing info page](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md), to run unit tests for PRs I'm making. Really I just want to run cpu unit tests, and I ran into this issue on the way.", "Ah,  I see.\r\nBut still, this looks like a GCC bug looking at your error message.\r\n\r\nWhen I search the error message at google, I see some relevant issues, please make sure to check existing issues to see if these apply to you. It looks like this can be a memory problem. Also in stackoverflow I can see some recommended solutions.\r\nhttps://github.com/tensorflow/serving/issues/13\r\nhttps://stackoverflow.com/questions/36264339/tensorflow-serving-compile-error-using-docker-on-osx", "Thanks for that! I didn't find those when I first googled the problem.\r\n\r\nI've allocated more memory to docker, and am compiling again now. Assuming that goes fine I'll close the issue.\r\n\r\nWould it be appropriate if I PRd in a short note on `CONTRIBUTING.md`? My reasoning is that if there are issues building the CI docker image using default settings on docker for mac, that other people may also run into this issue.", "I think the main problem is, our CI runs on the beefiest machines we can\nfind(average 50+ GB memory, 10+ CPUs), and our ci build scripts are\noptimized for speed on them.\nWe will need to create separate scripts for contributors to perform local\ntesting on real world computers.\n\nOn Sun, Oct 29, 2017 at 11:31 PM, Charles Shenton <notifications@github.com>\nwrote:\n\n> Thanks for that! I didn't find those when I first googled the problem.\n>\n> I've allocated more memory to docker, and am compiling again now. Assuming\n> that goes fine I'll close the issue.\n>\n> Would it be appropriate if I PRd in a short note on CONTRIBUTING.md? My\n> reasoning is that if there are issues building the CI docker image using\n> default settings on docker for mac, that other people may also run into\n> this issue.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14051#issuecomment-340354874>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCORuHcipYptu_gE9AIo-aTLHpFxQbks5sxW1MgaJpZM4QJxES>\n> .\n>\n", "I think so long as the amount of RAM allocated per CPU core isn't too low the scripts should work fine on most dev machines (albeit slowly). \r\n\r\nThe issue in my case is that the container was using just under 4 logical cores, but only had 2GB ram allocated, hence the memory issues. Now that I've allocated 4GB to docker, the container is only using ~2.2GB RAM. \r\n\r\nIt would make it easier to contribute to tensorflow if one could just test against the beefy CI servers with impunity, but it's also not a huge barrier.\r\n\r\nEither way, the build and tests are now running properly, so I'll close the issue. Thanks again for your help, I really appreciate it."]}, {"number": 14050, "title": "Feature request: Exporting TensorBoard graphs to graphviz DOT files", "body": "As recently discussed on the TensorFlow mailing list (https://groups.google.com/a/tensorflow.org/d/msg/discuss/GUW0KOmN7MM/2lRMD4JVAQAJ), it would be nice if TensorBoard would have an option to export a graph as graphviz DOT file. This would allow users and developers to create e.g., Python packages based on pygraphviz that can further modify the graph structure, such as simplifying or summarizing the graph ops into a publication-ready figure, etc.\r\n\r\nFor example, scikit-learn has such a function to export decision trees to DOT files; maybe the source code is useful as an example: https://github.com/scikit-learn/scikit-learn/blob/f3320a6f/sklearn/tree/export.py#L74", "comments": ["Hey @rasbt I've recently been using something simple like this: \r\n\r\nhttps://blog.jakuba.net/2017/05/30/tensorflow-visualization.html\r\n\r\nIt makes mention of the following:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/quantization/graph_to_dot.py\r\n\r\nWhat do you think?", "That's an awesome post, Diego, thanks for sharing! It's really handy, and I didn't know that it was so easy to export graphs via graphviz's \"Digraph\" already! Of course, for the feature request, it would be even more convenient to have an \"export\" function in TensorBoard that does exactly this -- saving the graph as dot file for further manipulation.\n\nBest,\nSebastian\n\n> On Oct 28, 2017, at 10:58 PM, Diego Mesa <notifications@github.com> wrote:\n> \n> Hey @rasbt I've recently been using something simple like this:\n> \n> https://blog.jakuba.net/2017/05/30/tensorflow-visualization.html\n> \n> It makes mention of the following:\n> \n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/quantization/graph_to_dot.py\n> \n> What do you think?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n\n", "This is an excellent suggestion and it's something we're already in the process of thinking about. Please follow https://github.com/tensorflow/tensorboard/issues/570. I'm going to close this one out as a duplicate."]}, {"number": 14049, "title": "Feature request: Exporting TensorBoard graphs to vector graphics formats", "body": "As recently discussed on the TensorFlow mailing list (https://groups.google.com/a/tensorflow.org/d/msg/discuss/GUW0KOmN7MM/2lRMD4JVAQAJ), it would be nice if TensorBoard would include an export function that supports exporting the graph in a vector graphics format (e.g., SVG or EPS, or both) in addition to the current PNG export function.\r\n\r\nFor instance, I recently bumped into a case where I wanted to include the TensorBoard graph as an example output of a tutorial section on TensorBoard in my book and found that the PNG version is \"too low-res\" and not very helpful so that I had to manually redraw it. Also, I like to include TensorBoard graphs in reports some times after applying some stylistic changes and recently stumbled upon a browser utility called \"SVG crowbar\" that can get the graph from TensorBoard in SVG format -- with some workarounds. This indicates that it may already be in SVG format, and it would be nice to allow to export it to disk for styling and generating high res figures.\r\n\r\n", "comments": ["I believe this is the same request as in the Tensorboard repository: https://github.com/tensorflow/tensorboard/issues/32 - so closing this as a duplicate.", "Sure, makes sense! I just realized that I should have searched & submitted this to the tensorboard repo anyways (for some reason, I thought tensorboard was included in the tensorflow main repo)"]}, {"number": 14048, "title": "CUDNN_STATUS_INTERNAL_ERROR", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 \r\n- **TensorFlow installed from (source or binary)**: binary (via pip)\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0') \r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: Cuda 8.0 (via pip) / cuDNN 6.0.21\r\n- **GPU model and memory**: GeForce GTX 1060 6GB\r\nHost compiler version : GCC 4.9.3\r\n\r\nExact steps to reproduce (as per [nvidia Tensorflow demo](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/tensorflow/)):\r\n$ git clone -b update-models-1.0 https://github.com/tensorflow/models\r\n$ cd models/tutorials/image/imagenet\r\n$ python classify_image.py\r\n\r\n### Describe the problem\r\nTensorflow fails to run demo script, despite having installed (and re-installed) as per the [manual](https://www.tensorflow.org/install/install_linux). Any help would be *greatly* appreciated. \r\n\r\n### Source code / logs\r\n2017-10-27 16:09:51.154970: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-27 16:09:51.155000: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-27 16:09:51.349207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-27 16:09:51.349589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7845\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.80GiB\r\n2017-10-27 16:09:51.349610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-10-27 16:09:51.349617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-10-27 16:09:51.349631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)\r\n2017-10-27 16:09:51.672521: W tensorflow/core/framework/op_def_util.cc:333] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\n2017-10-27 16:09:52.006440: E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-10-27 16:09:52.006471: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-10-27 16:09:52.006481: F tensorflow/core/kernels/conv_ops.cc:672] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted (core dumped)\r\n\r\n", "comments": ["The error messages suggest that there was an issue initializing CUDNN.\r\n\r\nCould you share the output of `nvidia-smi` right after you observe this crash? Are there any other processes that might be using the GPU that are running along with your program? ", "Sure! See below. No other processes are running when I've tried this. I've also tried restarting to ensure that the GPU memory was totally cleared, but the same issue occurs.\r\n\r\n `+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 106...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   36C    P8     9W / 120W |     52MiB /  6071MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1250      G   /usr/lib/xorg/Xorg                            50MiB |\r\n+-----------------------------------------------------------------------------+\r\n`", "#### I've since upgraded my GPU, upgraded gcc to v5.4, reinstalled CUDA-8.0 (from with apt-get install), and modified my .bashrc file as per [this post](https://groups.google.com/forum/#!topic/theano-users/qD_HHtO3b-k). The relevant portion of the latest .bashrc file is shown here:\r\n\r\n'export CUDA_ROOT=/usr/local/cuda\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64/\"\r\nexport PATH=\"/usr/local/cuda/bin:$PATH\"\r\nexport PATH=\"/usr/local/cuda-8.0/bin:$PATH\"\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda/lib64\r\n#export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64\"'\r\n\r\n#### But I still get the following error. I've tried the solutions in the [troubleshooting guide](https://www.tensorflow.org/install/install_sources#common_installation_problems) to no avail. \r\n\r\n'>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/adam/miniconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/adam/miniconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/adam/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/adam/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/adam/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/adam/miniconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.'\r\n\r\n#### Output of 'nvidia-smi':\r\n\r\n'+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| 22%   40C    P8    14W / 250W |      2MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GT 610      Off  | 00000000:07:00.0 N/A |                  N/A |\r\n| 40%   35C    P8    N/A /  N/A |     48MiB /   963MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    1                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+'", "@meccaLeccaHi : Sorry that you're running into this. The error messages suggest trouble using CUDA/CUDNN, or in this last case, errors in loading the `libcusolver.so` library.\r\n\r\nSo the errors have changed after your reinstall? What is the location of `libcusolver.so.8.0` on your filesystem?\r\n", "Thanks for your help!\r\nI was able to move past the original error by adding the following line, in particular, to my .bashrc file:\r\n`export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda/lib64`\r\nBut, I am still unable to import the tensorflow module.\r\n\r\nlibcusolver.so.8.0 is located at:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcusolver.so.8.0", "Hmm....so not in your `LD_LIBRARY_PATH`. Just to verify that `libcusolver.so.8.0` is accessible to the linker, can you also list out the output of `ldconfig -p | grep libcusolver`?", "Happy to.\r\n`$ ldconfig -p | grep libcusolver          \r\n\tlibcusolver.so.7.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcusolver.so.7.5\r\n\tlibcusolver.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcusolver.so\r\n`", "OK, if I add '/usr/local/cuda-8.0/targets/x86_64-linux/lib' to my LD_LIBRARY_PATH, importing tensorflow is successful (so, many thanks to you for that). But, I am still experiencing my original error which is shown below:\r\n\r\n`$ python classify_image.py \r\n>> Downloading inception-2015-12-05.tgz 100.0%\r\nSuccessfully downloaded inception-2015-12-05.tgz 88931400 bytes.\r\n2017-10-30 22:14:54.542348: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-30 22:14:54.542378: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-30 22:14:54.705978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-30 22:14:54.706540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.80GiB\r\n2017-10-30 22:14:54.706561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-10-30 22:14:54.706568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-10-30 22:14:54.706578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\r\n2017-10-30 22:14:55.237837: W tensorflow/core/framework/op_def_util.cc:333] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\n2017-10-30 22:14:58.319995: E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-10-30 22:14:58.320026: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-10-30 22:14:58.320035: F tensorflow/core/kernels/conv_ops.cc:672] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted (core dumped)\r\n`", "Based on that, it would appear that `libcusolver.so.8.0` is not available to the linker when TensorFlow is loading (i.e., `/usr/local/cuda-8.0/targets/x86_64-linux/lib` is not in the search path for dynamic libraries).\r\n\r\nNot quite sure how you ended up in this state, but it seems that you'd want to correct that by updating your `ld.conf`, or perhaps including `/usr/local/cuda-8.0/targets/x86_64-linux/lib` and related paths in your `LD_LIBRARY_PATH`", "Just to be clear (as my response preceded your last comment)- that worked in eliminating that error, which is great, but the original error still persists (see my last comment for details). Thanks again, asimshankar!", "We ran into similar error (`CUDNN_STATUS_INTERNAL_ERROR`), but it was not happening when we ran `python` as root. It always failed when run as other, non-root user.\r\nThe root cause in the end was in the `nvidia-persistenced` daemon, which by default runs on Ubuntu under `nvidia-persistenced` user (which is member of same named group).\r\nAdding the non-root user to `nvidia-persistenced` group (`sudo usermod -a -G nvidia-persistenced my-nonroot-user`) resolved the issue.", "Thanks jstastny, but I did exactly as you say and the error still persists.", "@meccaLeccaHi -- did you log out and back again (or issued other way of reloading the groups of your user)? You can validate what are you in by `groups` command.\r\nJust to validate -- it works for you when run under `root` user, right?", "- I have tried logging out and then back in again, but still experience the same error.\r\n- Running the command as sudo does not work, as the tensorflow module is not found (see below). Perhaps because I installed using conda as non-root user (e.g. 'conda install tensorflow-gpu').\r\n`$ sudo python classify_image.py \r\n[sudo] password for adam: \r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 46, in <module>\r\n    import tensorflow as tf\r\nImportError: No module named tensorflow\r\n`\r\n- 'nvidia-persistenced' is among the groups shown when I run the 'groups' command.", "Does it work if you do the conda thing as root?", "Strangely, I get the same missing module error even after running the following commands:\r\n`sudo -s`\r\n`conda install tensorflow-gpu`\r\ntensorflow-gpu seems to install without error, but is not available when I try running python as root.", "I have the exact same problem. Tried all recommended steps here, no luck. Running on Linux Mint 18.2 (Ubuntu 16.04), nvidia-375 (384.90), CUDA 8.0.61-1 + CuDNN 6.0, TensorFlow 1.3.0 and 1.4.0, default Python 2.7, no Anaconda. I once got it working right after reboot, but subsequent executions always failed. Seems like driver gets stuck in some weird state... It started happening only a few days ago, maybe some Ubuntu/Mint update exposed this problem.", "Just to be sure, could you try the newest version?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I'm facing the same issue. \r\n\r\nI have\r\nGeforce GTX 1050Ti\r\nTensorflow 1.8.0\r\nCUDA 9.0\r\ncuDNN 7.0\r\nNo process is occupying the GPU, and rebooting has no effect. The last time this stopped working, it was a user rights problem, and disappeared when I added my user to nvidia-persistenced. User is probably not the issue, since it won't work with sudo either. Please help.", "could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nI am also facing the same issue.", "@chintler I have the exact same problem as yours, have you solved it yet?", "same for me in windows , any help plz", "Please re-open issue.  This still hasn't been solved, even with latest TF 1.10", "@reedwm can you take a look or redirect? Thanks.", "Someone who still has the issue, can you paste the full output of the error and refill the issues template? TensorFlow now requires cuda 9.0 unless built from source, and I cannot reproduce this issue.", "I am getting the same issue with window 10, tensorflow 1.10 and cuda 9.0. The error is \r\n2018-09-25 17:15:39.657620: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018-09-25 17:15:39.657926: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018-09-25 17:15:39.679944: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I get the same error on Linux with TensorFlow 1.10 and CUDA 9.0.\r\n\r\nApparently cuDNN is trying to allocate more memory than it should, as if the `gpu_options.per_process_gpu_memory_fraction` option (which we use) weren't strictly respected. In my case this error occurs when there is little free GPU memory:\r\n\r\n    tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\nThen the process crashes with segmentation fault (`SIGSEGV`).", "@adampl So there is a floor that we need? Are you comfortable enough with the code to send a fix to print out a more meaningful error message?", "@drpngx Unfortunately I'm not familiar with TF code, but it's not the error message that needs to be fixed, but the error itself.", "If I understand correctly, the problem is that the memory fraction cannot be honored if there's too little GPU memory available, resulting in an error. I am assuming that there's a reason for that and it can't be fixed. If it cannot be fixed, then we would want to detect that case and return a more informative error message. (If it can be fixed, then we should fix it). Am I right?", "The error occurs only when there is little free memory, because in such case if cuDNN exceeds the fraction, the GPU is unable to allocate the requested amount of memory. Nonetheless I suspect that the fraction is always (or often) exceeded regardless of the amount of free memory, but it just doesn't result in a fatal error because there is enough free memory.", "This work for me: \r\nhttp://tuxvoid.blogspot.com/2017/08/tensorflow-could-not-create-cudnn.html", "Closing as this issue is resolved, feel free to reopen if it comes up again.", "@wt-huang How do you know it's resolved?", "I got the same error when using convolutions (repeatedly), other Tensor operations were running fine. I have not updated anything and kept the same Tensorflow and CUDA version under which the same code was running before.\r\n\r\nDeleting the .nv dir (nvidia cache) in my local home directory completely solved the issue for me. Might also explain why another commenter had success with running Python as root.", "I am getting the same issue with window 10, tensorflow 1.10 and cuda 9.0. The error is\r\n\r\n2018-12-15 03:57:16.143788: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-12-15 03:57:16.525560: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce GTX 750 Ti major: 5 minor: 0 memoryClockRate(GHz): 1.163\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.00GiB freeMemory: 819.61MiB\r\n2018-12-15 03:57:16.533704: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-12-15 03:57:17.570206: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-15 03:57:17.575885: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\r\n2018-12-15 03:57:17.579652: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\r\n2018-12-15 03:57:17.583488: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 537 MB memory) -> physical GPU (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2018-12-15 03:57:24.779936: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n[I 03:57:42.363 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\nkernel be56b0b3-f661-4997-a9de-8543bfea49f9 restarted\r\n[I 03:57:54.253 NotebookApp] Saving file at /Desktop/tensorflow/tfobject/models/research/object_detection/object_detection_tutorial.ipynb"]}, {"number": 14047, "title": "Feature Request (API Design review) tf.get_shape(), tf.get_size(), tf.Tensor.size, tf.Tensor.get_size(), etc", "body": "Below, everything with `get` in the name refers to dynamic (symbolic), everything without to static (integer).\r\n\r\n1. Introduce `tf.get_shape(x)`. Then `tf.get_shape(x) <=> tf.shape(x)` will be analogous to existing `tf.Tensor.get_shape() <=> tf.Tensor.shape`\r\n2. Introduce `tf.set_shape(x)`, matching `tf.get_shape(x)`\r\n3. Introduce `tf.get_size(x)`. Then  `tf.get_size(x) <=> tf.size(x)`  will be analogous to `tf.get_shape(x) <=> tf.shape(x)`\r\n4. Ditto for `tf.get_shape_n(x)`, see `tf.shape_n(x)`.\r\n5. Introduce `tf.Tensor.size` for compatibility with `np.ndarray.size`. `tf.size(x) <=> x.size` will be analogous to `tf.shape(x) <=> x.shape`\r\n6. Ditto for `tf.Tensor.get_size()`\r\n7. Should there also be `tf.size_n(x)` and `tf.get_size_n(x)`?", "comments": ["Thanks very much for the suggestion.\r\n\r\nCurrently, I believe all the `tf.*` methods (like `tf.shape`, `tf.size` etc.) return symbolic tensors (unless [eager execution](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/README.md) is enabled), while the methods on the `tf.Tensor` object return values (well `TensorShape` objects for `tf.Tensor.shape`).\r\n\r\nIMHO, having some `tf.*` methods return symbolic tensors and others return concrete values may be confusing, especially with the various aliases.", "This is a good argument against introducing those methods into `tf.*`, but arguments for it are \r\n\r\n1.  `tf.*` methods have different signatures than eponymous `tf.Tensor.*` methods (like `dtype` parameters)\r\n2. `tf.*` static methods could accept numpy arrays, python lists, etc\r\n\r\nwhat about items 5 and 6?", "@martinwicke what do you think of this proposal?", "We did not implement `x.size == tf.size(x)` because it would be similar to numpy. `tf.size(x)` returns a `Tensor`, while `ndarray.size` is an `int`. Almost no code that relies on the numpy behavior would work with `Tensor`s, so I'd rather not give the impression it would.\r\n\r\nEven in Eager mode, the things returned from tf.shape will be TensorShape objects, not lists of integers. While these are mostly equivalent, we have seen problematic incompatibilies.\r\n\r\nSo we'd rather not add this."]}, {"number": 14046, "title": "Branch 173716375", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 14045, "title": "Fixing the sources docs in master.", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 14044, "title": "Remove name_scope from convolutional calls.", "body": "Cherry-picking conv layer fix to 1.4 branch.", "comments": ["Is this all we need for the conv layer issue to fix it in 1.4?", "Jenkins, test this please.", "reassigned to @gunan who has permission to merge code into the 1.4 branch.", "Jenkins, test this please", "Jenkins, test this please", "Errors on the test runs are...\r\n\r\n===== ci.tensorflow.org bot =====\r\nERROR: Error fetching remote repo 'origin'\r\nERROR: [GitHub Commit Status Setter] - Cannot retrieve Git metadata for the build, setting build result to FAILURE\r\nUnable to get pull request builder trigger!!\r\n\r\nand..\r\n\r\n===== Unbuntu Makefile Bot =====\r\nERROR: Skipping '//tensorflow/contrib/makefile:build_all_linux': no such package 'tensorflow/contrib/makefile': BUILD file not found on package path\r\n\r\nBoth of these seems unrelated to this PR. Not sure how to get them to pass.", "I think the problem is that the CI build scripts are not in TensorFlow so they didnt branch. The build script is trying to run something added after the branch point. So I think we are going to have to force-submit these."]}, {"number": 14043, "title": "Update cudnn_rnn.py", "body": "For addressing this issue,\r\nhttps://github.com/tensorflow/tensorflow/issues/14001#issuecomment-340042335\r\n\r\nUpdating the path on line 33 to make import cudnn_rnn work. Change worked on Ubuntu.", "comments": ["Can one of the admins verify this patch?", "LGTM.\r\nI think something might still be missing for windows as it fails our release test.", "@protoget I just changed the path lookup? What you think the fix should be?"]}, {"number": 14042, "title": "tf.estimator.EstimatorSpec doesn't have evaluation_hooks parameter", "body": "I planned to visualize the evaluation result in tensorboard. Therefore I need to create the evaluation_hook using `tf.train.SummarySaverHook` in model_fn, and pass it into the [EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec). \r\n\r\nHowever, [EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec) doesn't accept `evaluation_hooks` for now. It only has `training_hooks`. Will `evaluation_hooks` be added in future versions?", "comments": ["Though [Estimator.evaluate](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate) has `hooks` parameter. It is outside of the model_fn context and can't get the `summary_op` conveniently. It will be great if EstimatorSpec could have `evaluation_hooks` parameter.", "@martinwicke I'm marking as contributions welcome, please remove if this isn't something we'd want.", "@ispirmustafa, WDYT", "During training mode of an Estimator the summaries are added automatically (no need to use hooks). Why isn't it the same behavior for evaluation mode?\r\n\r\nRelated StackOverflow question: https://stackoverflow.com/questions/46852018/tensorflow-estimator-api-save-image-summary-in-eval-mode", "It makes sense for consistency with training.\r\nOn the other hand I haven't see a valid eval hook request. @hanfeisun how are you planning to use SummaryHook in evaluation? There are more then one eval-run in evaluation of one global-step.", "I've created a kludgey solution by modifying the SummarySaverHook code and getting the summary op via tf.get_collections(tf.GraphKeys.SUMMARY_OP).  It may be better to create your own special op with just the summaries you want to see. Because a lot of the training summaries are not that useful when using evaluate. \r\n\r\n\r\nIn my use case I want only one summary saved (to show example images and neuron response images. ), But maybe you can modify this to do whatever you want. NOTE: For some reason, my summary images were entirely black on the first execution of 'run'. So I skip that one. There's something going on that i don't understand.\r\n\r\nYou pass this into the estimator.evaluate like this:\r\n\r\n```python\r\n    hooks = [EvalSummarySaverHook(output_dir=eval_model_dir)]\r\n    eval_results = eval_estimator.evaluate(input_fn=eval_input_func, hooks=hooks)\r\n```\r\n\r\nHere is the code: (tested on python 3.5 and tensorflow-gpu==1.3.0)\r\n```python\r\n     class EvalSummarySaverHook(tf.train.SessionRunHook):\r\n        \"\"\"Saves summaries during eval loop.\"\"\"\r\n\r\n        def __init__(self,\r\n                     output_dir=None,\r\n                     stop_after=1):\r\n            \"\"\"Initializes a special `SummarySaverHook` to run during evaluations\r\n\r\n            Args:\r\n              output_dir: `string`, the directory to save the summaries to.\r\n            \"\"\"\r\n            self._summary_op = None\r\n            self._output_dir = output_dir\r\n            self._global_step_tensor = None\r\n            self._stop_after = stop_after\r\n            self._saves = None\r\n\r\n        def begin(self):\r\n            self._global_step_tensor = tf.train.get_or_create_global_step()\r\n            if self._global_step_tensor is None:\r\n                raise RuntimeError(\r\n                    \"Global step should be created to use SummarySaverHook.\")\r\n\r\n        def before_run(self, run_context):  # pylint: disable=unused-argument\r\n            requests = {\"global_step\": self._global_step_tensor}\r\n            if self._saves is None: # skip first time, because summaries only appear after first run\r\n                self._saves = 0\r\n            elif self._saves < self._stop_after and self._get_summary_op() is not None:\r\n                requests[\"summary\"] = self._get_summary_op()\r\n\r\n            return tf.train.SessionRunArgs(requests)\r\n\r\n        def after_run(self, run_context, run_values):\r\n            _ = run_context\r\n            if \"summary\" in run_values.results:\r\n                print('Saving eval summaries')\r\n                global_step = run_values.results[\"global_step\"]\r\n                summary_writer = tf.summary.FileWriterCache.get(self._output_dir)\r\n\r\n                for summary in run_values.results[\"summary\"]:\r\n                    summary_writer.add_summary(summary, global_step)\r\n\r\n                self._saves += 1\r\n\r\n        def end(self, session=None):\r\n            _ = session\r\n            summary_writer = tf.summary.FileWriterCache.get(self._output_dir)\r\n            summary_writer.flush()\r\n\r\n        def _get_summary_op(self):\r\n            \"\"\"Fetches the summary op from collections\r\n            \"\"\"\r\n\r\n            if self._summary_op is None:\r\n                self._summary_op = tf.get_collection(tf.GraphKeys.SUMMARY_OP)\r\n\r\n            return self._summary_op\r\n```\r\n", "The API lists `evaluation_hooks` as a [valid parameter](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec) for `EstimatorSpec` \u2014 does this not work?", "@mtngld I need the summary hook because I want to config `save_steps` and `output_dir` parameter using SummarySaverHook.  My codes looks like this:\r\n\r\n```\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        tf.summary.image(\"original\", features['source'])\r\n        tf.summary.image(\"target\", features['target'])\r\n        tf.summary.image(\"transfer_sigmoid\", generated)\r\n    else:\r\n        # Evaluation\r\n        tf.summary.image(\"eval_original\", features['source'], max_outputs=10)\r\n        tf.summary.image(\"eval_target\", features['target'], max_outputs=10)\r\n        tf.summary.image(\"eval_transfer_sigmoid\", generated, max_outputs=10)\r\n    summary_hook = tf.train.SummarySaverHook(\r\n        save_steps=10,\r\n        output_dir=\"./board\",\r\n        summary_op=tf.summary.merge_all())\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        loss=loss,\r\n        train_op=tf.group(d_train, g_train),\r\n        eval_metric_ops=None,\r\n        training_hooks=[summary_hook]\r\n    )\r\n```\r\n\r\nAs I remember, during the evaluation, the image summary is not shown in tensorboard somehow..", "@huw That looks great! I think the API is newly added as there is no `evaluation_hooks` yet in \"[tensorflow 1.3](evaluation_hooks)\"", "I would love to get this working with the higher level APIs [`tf.estimator.train_and_eval`][1] and [`Estimator.evaluate`][2]\r\n\r\n[1]: https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\r\n[2]: https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate", "@martinwicke Have you assigned this after was already closed? Is there a plan to cover @bodokaiser  request?", "I don't think there's anything else to do:\r\n\r\n- `train_and_evaluate` uses EvalSpec, which has a `hooks` field.\r\n- `Estimator.evaluate` has `hooks` argument.", "I see this goes on in #15332 ."]}, {"number": 14041, "title": "tf.metrics doesn't include cross_entropy.", "body": "In tensorflow estimator, I want to use _cross entropy_ as the evaluation metrics (`eval_metric_ops` parameter of `EstimatorSpec`)\r\n\r\nHowever, `tf.metrics` doesn't have this function. Also, tensorflow estimator doesn't allow me to use `tf.nn.sigmoid_cross_entropy_with_logits` as the `eval_metric_ops`.", "comments": ["Let's make it clear, do you expect this?\r\n![image](https://user-images.githubusercontent.com/1112263/32140251-b75b71be-bc94-11e7-9909-0a4ac3df906a.png)\r\nwhere `p` is label, and `q` is prediction for binary classification.\r\n", "Yes.  There's a [cross_entropy](https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy) in `tf.losses` but it can't be used as `eval_metric_ops` either..", "There is a slightly difference: `cross_entropy` expects `(labels, logits)`, while `tf.metrics` only handles `(labels, predictions)`.", "@facaiy I see. But I was still a bit curious why there is no `cross_entropy` for `tf.metrics`..", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Added the API review to consider whether adding cross entropy to tf.metrics makes sense.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "To clarify, why can't you use tf.metrics.mean here and pass in the result of tf.nn.sigmoid_cross_entropy_with_logits? It would effectively be the same thing.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@jhseu Thanks, but I find tensorflow estimator doesn't allow me to use tf.nn.sigmoid_cross_entropy_with_logits as the eval_metric_ops..", "Yeah, but couldn't it be wrapped with tf.metrics.mean()?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing. I'm fairly certain my above suggestion will work."]}, {"number": 14040, "title": "Add go format check as part of the sanity check", "body": "In go, it is very common to format the code with `gofmt -s -w file.go`. This fix adds the gofmt check as part of the sanity check.\r\n\r\nThere was only one file `tensorflow/go/tensor.go` in the whole `tensorflow/go` directory that is not properly formatted.\r\n\r\nThis fix formatted `tensorflow/go/tensor.go` with `gofmt -s -w` as well.\r\n\r\nIt is also possible to check the format status in:\r\nhttps://goreportcard.com/report/github.com/tensorflow/tensorflow\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks for the PR @yongtang .\r\n\r\nHowever, for now let's not do this in the sanity check. We currently don't enforce formatting for C++, Python, or Java in the sanity check and I'd prefer to not to special case go for now.\r\n\r\nHappy to accept the change to `tensor.go` though.\r\n\r\nThanks!", "Thanks @asimshankar. I have updated the PR and removed the sanity check part. Only the changes in `tensor.go` remains in the PR. Please take a look.", "Jenkins, test this please."]}, {"number": 14039, "title": "Boringssl support for s390x", "body": "Creating this issue with reference : https://github.com/tensorflow/tensorflow/pull/11170\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: master\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **CUDA/cuDNN version**: Not used\r\n- **Exact command to reproduce**: bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n", "comments": ["Hello @gunan Continuing discussion about disabling the oauth support /GCP on big endian from issue https://github.com/tensorflow/tensorflow/pull/11170#issuecomment-320071548\r\n\r\nI could see that the PR changes made for s390x are now reverted in Tensorflow master.\r\nSo as suggested, disabling GCP support on a big-endian systems,  I followed following steps:\r\n~/tensorflow# ./configure\r\n\r\n> You have bazel 0.6.1- (@non-git) installed.\r\n> Please specify the location of python. [Default is /usr/bin/python]:\r\n> Found possible Python library paths:\r\n>   /usr/local/lib/python2.7/dist-packages\r\n>   /usr/lib/python2.7/dist-packages\r\n> Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n> \r\n> Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:\r\n> jemalloc as malloc support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\n> No Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\n~/tensorflow# bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nEven after disabling GCP support, build fails with an error:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/30b0ed2738c25ecefa4b108f71649fc1/external/boringssl/BUILD:115:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\r\nIn file included from external/boringssl/src/include/openssl/bio.h:60:0,\r\n                 from external/boringssl/src/crypto/bio/printf.c:57:\r\nexternal/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n  ^\r\n\r\n```\r\nNeed more changes here?\r\n\r\nOn test cases part, I have added `tags = [\"no_big_endian\"] ` in `tensorflow/core/platform/cloud/BUILD `file. \r\nYet to verify this changes. \r\n\r\n\r\n\r\n", "Looks like S3 support also depends on curl, which depends on boringssl.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/BUILD#L47\nCould you also try turning off s3 support?\n\nOn Fri, Oct 27, 2017 at 11:34 AM, Nayana Thorat <notifications@github.com>\nwrote:\n\n> Hello @gunan <https://github.com/gunan> Continuing discussion about\n> disabling the oauth support /GCP on big endian from issue #11170 (comment)\n> <https://github.com/tensorflow/tensorflow/pull/11170#issuecomment-320071548>\n>\n> I could see that the PR changes made for s390x are now reverted in\n> Tensorflow master.\n> So as suggested, disabling GCP support on a big-endian systems, I followed\n> following steps:\n> ~/tensorflow# ./configure\n>\n> You have bazel 0.6.1- (@non-git) installed.\n> Please specify the location of python. [Default is /usr/bin/python]:\n> Found possible Python library paths:\n> /usr/local/lib/python2.7/dist-packages\n> /usr/lib/python2.7/dist-packages\n> Please input the desired Python library path to use. Default is\n> [/usr/local/lib/python2.7/dist-packages]\n>\n> Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:\n> jemalloc as malloc support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:\n> n\n> No Google Cloud Platform support will be enabled for TensorFlow.\n>\n> ~/tensorflow# bazel build -c opt //tensorflow/tools/pip_\n> package:build_pip_package\n>\n> Even after disabling GCP support, build fails with an error:\n>\n> ERROR: /root/.cache/bazel/_bazel_root/30b0ed2738c25ecefa4b108f71649fc1/external/boringssl/BUILD:115:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\n> In file included from external/boringssl/src/include/openssl/bio.h:60:0,\n>                  from external/boringssl/src/crypto/bio/printf.c:57:\n> external/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\n>  #error \"Unknown target CPU\"\n>   ^\n>\n>\n> Need more changes here?\n>\n> On test cases part, I have added tags = [\"no_big_endian\"] in\n> tensorflow/core/platform/cloud/BUILDfile.\n> Yet to verify this changes.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14039#issuecomment-340049932>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOQ69vwqT3DmG_4bOXfdizxCtg2I4ks5swiIfgaJpZM4QJcJ0>\n> .\n>\n", "@Nayana-ibm have you tried also turning off s3 support?", "hi @gunan I have tried turning off s3 support through configure however still I'm facing an issue with boringssl code.\r\n\r\n```\r\ntensorflow# ./configure\r\nYou have bazel 0.6.1- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.5/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: N\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: N\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: N\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]:\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]:\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]:\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n```\r\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\n\r\nERROR: /root/.cache/bazel/_bazel_root/79d3d3c667100526634e40f5a15c8098/external/boringssl/BUILD:115:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\r\nIn file included from external/boringssl/src/include/openssl/asn1.h:61:0,\r\n                 from external/boringssl/src/crypto/asn1/f_int.c:57:\r\nexternal/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n  ^\r\n```\r\n\r\nI'm looking  into BUILD files if there is any code change requires for s390x. \r\n", "@case540 Do we need to disable s3 and gcp support through config options?", "@case540 Could you please let me if disabling support by config options is sufficient? Do I need to investigate further on this? ", "You can disable s3 and gcp support by not having --config=s3 or --config=gcp in your Bazel build command (answering No to s3 and gcp support question when running configure.py will do this (they default to the Yes choice)).\r\n\r\nLooks like you already did this though.", "Looks like now we need boringssl for grpc.\r\n@martinwicke WDYT?", "Also cc @vjpai \r\nWe would like to build TF for s390x. But boring SSL strictly has no ppc64 support. Looks like GRPC now depends on boringssl. Below is my bazel query output:\r\n```\r\n@grpc//:grpc++_unsecure\r\n@grpc//:grpc++_base_unsecure\r\n@grpc//:grpc_unsecure\r\n@grpc//:grpc_common\r\n@grpc//:census\r\n//external:libssl\r\n@boringssl//:ssl\r\n@boringssl//:crypto\r\n```\r\n How would you recommend for us to proceed?", "This looks like a bug in the grpc_unsecure rule. It seems like it shouldn't depend on any ssl code. @mrry do you know who to best tell about this?", "I think @vjpai would probably have it covered - he recently switched us over to using gRPC's own `BUILD` file (instead of our old fork).", "Thanks for the heads-up; I'll look this over. It is indeed a problem in the rule for grpc_unsecure if it actually depends on boringssl.", "Ok, I see the issue. This dependence is being pulled up through census. Not sure why census wants boringssl but I'll investigate.", "I think I have this under control with grpc/grpc#13525 .\r\n", "Awesome, thanks! We need to update our dependency again once this is merged. ", "@gunan   As grpc dependency version is updated through #14981,  I have tried building tensorflow master for s390x again.  I have disabled `Google Cloud Platform` and  `Amazon S3 File System` support through `./configure` options.  (answering No to s3 and gcp support question when running `configure.py` )\r\nNow, build is successful (No error related to boringssl) however test execution failed with \r\n```\r\nERROR: external/boringssl/BUILD:115:1: Couldn't build file external/boringssl/_objs/crypto/external/boringssl/src/crypto/bn_extra/bn_asn1.pic.o: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\r\nIn file included from external/boringssl/src/include/openssl/bn.h:126:0,\r\n                 from external/boringssl/src/crypto/bn_extra/bn_asn1.c:15:\r\nexternal/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n  ^\r\nIn file included from external/boringssl/src/crypto/bn_extra/bn_asn1.c:15:0:\r\nexternal/boringssl/src/include/openssl/bn.h:163:2: error: #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n  ^\r\n```\r\n\r\nI guess, I need to add `no_big_endian` tag in the BUILD files to disable the tests as you mentioned in https://github.com/tensorflow/tensorflow/pull/11170#issuecomment-329252942\r\nI have identified below files which may require change:\r\n```\r\ntensorflow/core/platform/cloud/BUILD\r\ntensorflow/contrib/tpu/profiler/BUILD\r\ntensorflow/core/platform/default/build_config.bzl\r\ntensorflow/core/platform/s3/BUILD\r\nthird_party/aws.BUILD\r\ntensorflow/BUILD\r\ntensorflow/core/platform/cloud/BUILD\r\ntensorflow/contrib/cloud/kernels/BUILD\r\nthird_party/curl.BUILD\r\n```\r\nPlease correct me if I'm missing anything here. \r\n", "What was the exact command you tried and got this result?", "Configure using: \r\n```\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_CUDA=0\r\nexport TF_NEED_MKL=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_S3=0\r\nyes \"\" | ./configure\r\n```\r\n\r\n\r\nBuild\r\n`bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n==> Build successful.\r\n  \r\nRun bazel test command. \r\n\r\n```\r\nbazel --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\" test \\\r\n    --test_tag_filters=-gpu,-benchmark-test -k \\\r\n    --test_timeout 300,450,1200,3600 --build_tests_only \\\r\n    --test_output=errors -- \\\r\n    //tensorflow/... -//tensorflow/compiler/...\r\n```\r\n\r\n==> Facing an issue related to boringssl : #error \"Unknown target CPU\" ", "The command somehow seems to be trying to build cloud and mobile targets.\r\nusing no_big_endian tags will not help with the command you are running, because you are not filtering with the \"no_big_endian\" tag in your command.\r\nOne immediate solution can be to exclude some more modules in your command. Could you try adding these at the end of your command:\r\n```\r\n-//tensorflow/core/platform/aws/... -//tensorflow/core-platform/cloud/... -//tensorflow/contrib/lite/... -//tensorflow/contrib/cloud/...\r\n```", "@gunan Executed below command to execute tests:\r\n\r\n```\r\nbazel --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\" test \\\r\n    --test_tag_filters=-gpu,-benchmark-test -k \\\r\n    --test_timeout 300,450,1200,3600 --build_tests_only \\\r\n    --test_output=errors -- \\\r\n    //tensorflow/... -//tensorflow/compiler/... -//tensorflow/core/platform/aws/... -//tensorflow/core-platform/cloud/... -//tensorflow/contrib/lite/... -//tensorflow/contrib/cloud/... \r\n```\r\n    \r\nStill facing an error: \r\n```\r\n\r\nexternal/boringssl/src/ssl/../crypto/internal.h:338:19: warning: 'constant_time_select_int' defined but not used [-Wunused-variable]\r\n static inline int constant_time_select_int(crypto_word_t mask, int a, int b) {\r\n                   ^\r\n____[9,702 / 9,711] Compiling external/boringssl/src/ssl/s3_pkt.cc\r\nERROR: /data/TF_tmp/_bazel_root/be3f47674f2731fd84874c35a2feb28b/external/boringssl/BUILD:128:1: Couldn't build file external/boringssl/_objs/ssl/external/boringssl/src/ssl/s3_pkt.pic.o: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1).\r\nIn file included from external/boringssl/src/include/openssl/ssl.h:145:0,\r\n                 from external/boringssl/src/ssl/s3_pkt.cc:109:\r\nexternal/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n  ^\r\nIn file included from external/boringssl/src/include/openssl/asn1.h:68:0,\r\n                 from external/boringssl/src/include/openssl/x509.h:70,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/boringssl/src/ssl/s3_pkt.cc:109:\r\nexternal/boringssl/src/include/openssl/bn.h:163:2: error: #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n  ^\r\nIn file included from external/boringssl/src/ssl/s3_pkt.cc:121:0:\r\nexternal/boringssl/src/ssl/../crypto/internal.h:202:2: error: #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n```\r\n\r\nguess, I need to investigate more on this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Could you run another bazel query at head? grpc bug was fixed and our dependency was bumped. It is possible boringssl crept into another one of our libraries.", "\r\n\r\n@gunan I have executed tests with below command still facing an issue with boringssl.\r\n\r\n```\r\nbazel --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\" test \\\r\n    --test_tag_filters=-gpu,-benchmark-test -k \\\r\n    --test_timeout 300,450,1200,3600 --build_tests_only \\\r\n    --test_output=errors -- \\\r\n    //tensorflow/... -//tensorflow/compiler/... -//tensorflow/core/platform/aws/... -//tensorflow/core-platform/cloud/... -//tensorflow/contrib/lite/... -//tensorflow/contrib/cloud/... \r\n```\r\n    \r\nlog available at : http://ci.tensorflow.org/job/tensorflow-contrib-s390x/1516/console\r\n\r\nIs there any way to find out which component pulls boringssl?\r\n\r\n", "Here is the message that looks suspicious:\r\n```\r\nERROR: Skipping '//tensorflow/core/platform/aws/...': no targets found beneath 'tensorflow/core/platform/aws'.\r\nERROR: Skipping '//tensorflow/core-platform/cloud/...': no targets found beneath 'tensorflow/core-platform/cloud'.\r\nERROR: Skipping '//tensorflow/core/platform/aws/...': no targets found beneath 'tensorflow/core/platform/aws'.\r\nERROR: Skipping '//tensorflow/core-platform/cloud/...': no targets found beneath 'tensorflow/core-platform/cloud\r\n```\r\nSo lets modify the last line of your command, looking at our folders in the git repo, like this:\r\n```\r\n//tensorflow/... -//tensorflow/compiler/... -//tensorflow/core/platform/s3/... -//tensorflow/core/platform/cloud/... -//tensorflow/contrib/lite/... -//tensorflow/contrib/cloud/... \r\n```\r\n\r\n", "@gunan  Could resolve the issue `Unknown target CPU`  by using command provided by you in previous comment. \r\n\r\nBuild\r\n`bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n   \r\nbazel test command. \r\n```\r\nbazel --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\" test \\\r\n    --test_tag_filters=-gpu,-benchmark-test -k \\\r\n    --test_timeout 300,450,1200,3600 --build_tests_only \\\r\n    --test_output=errors -- \\\r\n    //tensorflow/... -//tensorflow/compiler/... -//tensorflow/core/platform/s3/... -//tensorflow/core/platform/cloud/... -//tensorflow/contrib/lite/... -//tensorflow/contrib/cloud/... \r\n```\r\n", "Great!\r\nSo this issue is resolved?", "I could see `28 tests fail to build` in the results. But I guess it's not related to boringssl. \r\nFor now, I am closing this issue.\r\n@gunan Thanks for your help on this. "]}, {"number": 14038, "title": "Fix documentation error in tf.size() - output type", "body": "`out_type` can be any non-quantized numeric type, not just `int32` or `int64`. \r\n\r\nAlso some formatting changes, and sync docstrings of `size()` and `size_internal()`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14037, "title": "Complex matrix_inverse inconsistency error", "body": "If I run the following code: https://github.com/nunodsousa/tensorflow_matrix_inversion/blob/master/tf_matrix_inversion.ipynb I obtain the inverse of a complex matrix. However, If I reduce 1E-4 times the values of the matrix, the results should be the same, apart of the 1E4 factor. However it returns similar errors to the one found in #13558.\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary): binary\r\n- **TensorFlow version (use command below): 1.3\r\n- **Python version: 3.6\r\n- **CUDA/cuDNN version: 5.5\r\n- **GPU model and memory**: Nvidia Gforce GTX 1080ti\r\n", "comments": ["@rmlarsen did you get a chance to look at #13558? Closing this for now as a duplicate of that issue but please reopen if it turns out not to be.", "Yes, I saw the #13558, however I don't know if it is exactly the same problem. I'm making a more detail set of tests and I repost the result. However, for small matrices (like 10x10) the inversion works fine. Also I already confirm that it is not a memory problem."]}, {"number": 14036, "title": "Update Scikit Flow link and description", "body": "", "comments": ["@martinwicke Yeah that makes more sense. Removed the whole thing. "]}, {"number": 14035, "title": "Fix an ouput typo in `ci_sanity.sh`", "body": "In the last PR #13924 (clang sanity check) the output message should be changed:\r\n`due to the absence of Python code changes`\r\n->\r\n`due to the absence of .h or .cc code changes`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14034, "title": "Segmentation fault when using Intel MKL with np.linalg.svd", "body": "### System information\r\n\r\nI am running this on the [Graham supercomputer](https://docs.computecanada.ca/wiki/Graham) of Compute Canada. I tested the bug on computation nodes but it also appears on login nodes without GPUs.\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux - CentOS 7\r\n- **TensorFlow installed from (source or binary)**: Custom build with Intel MKL I guess?\r\n- **TensorFlow version (use command below)**: b'v1.3.0-0-g9e76bf3' 1.3.0\r\n- **Python version**: Python 3.5.2 (default, Jun 25 2016, 21:38:40) [GCC 5.4.0] on linux\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 7.5\r\n- **GPU model and memory**: Tesla P100\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = np.ones((64,256))\r\nu, _, v = np.linalg.svd(a, full_matrices=False)\r\n```\r\n\r\nWithout the `import tensorflow as tf`, the bug doesn't appear.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1422363/tf_env.txt)\r\n\r\n\r\n### Describe the problem\r\n\r\nSo basically, when using Intel MKL with the python code above you get a segmentation fault. Without the `import tensorflow as tf`, the bug doesn't appear. Strangely, when I change the size of the 2nd axis of matrix `a` to below 201, it works (at some point that I tested, it was 188). When setting the shape of the matrix `a` to something bigger like `(64,256)`, it just using all CPUs without returning anything as if it was in a deadlock or something. When setting `MKL_NUM_THREADS` to 1, both bugs disappear. This bug report seems related to all of these issues: https://github.com/tensorflow/tensorflow/issues/9234 https://github.com/tensorflow/tensorflow/issues/13004 https://github.com/tensorflow/tensorflow/issues/11724 https://github.com/tensorflow/tensorflow/issues/10005. They are not identical to this problem but really similar so this bug report is just to let you know another symptom related to the same problem. \r\n\r\n\r\n### Source code / logs\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1422363/tf_env.txt)\r\n[gdb_segfault.txt](https://github.com/tensorflow/tensorflow/files/1422365/gdb_segfault.txt)\r\n\r\n", "comments": ["@rmlarsen can you look into this or redirect? Thanks!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So I've talked to guy at Compute Canada and he told me that tensorflow isn't even compiled with libmkl but numpy is. He also told me that he updated to the last version of tensorflow and it solved the problem. So I guess it is not an issue anymore?"]}, {"number": 14033, "title": "Typo fix in file 'fully_connected_feed.py'", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I have signed CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 14032, "title": "Add `double` support for `tf.decode_csv`", "body": "In the current tensorflow `tf.decode_csv` accepts `float`, `int32`, `int64`, `string` but not `double`.\r\nIt seems adding `double` support makes sense as `StringToNumber` already support `double` type.\r\n\r\nThis fix adds `double` support for `tf.decode_csv`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 14031, "title": "Fix incorrect annotation tag in the docs of tf.Variable", "body": "In tf.Variable the annotation tag of `@compatiblity` (missing `i`) should be `@compatibility` , without the fix the rendering of docs may be incorrect. \r\n\r\nSee `add_check_numerics_ops()` for an example of `@compatibility`:\r\nhttps://github.com/tensorflow/tensorflow/blob/e43e514b7418756a828c6a0f60e43aa6a638e961/tensorflow/python/ops/numerics.py#L68-L72\r\n\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Thanks @benoitsteiner for the review. The Jenkins CI error on `//tensorflow/tools/docs:build_docs_test` is because doc string for compatibility can only be specified at the method level, not at the class level (See Ln 120 in `tensorflow/tools/docs/pretty_docs.py`).\r\n\r\nI have move the `@compatibility(eager)` from `Class` level docstring to `__init__` docstring. The test should pass now.", "Jenkins, test this please\r\n"]}, {"number": 14030, "title": "Add `SANITY_STEPS_DESC` for do_clang_format_check", "body": "This fix is a follow up to PR #13924 to add the corresponding description in `SANITY_STEPS_DESC`.\r\n\r\nSee comment https://github.com/tensorflow/tensorflow/pull/13924#discussion_r147314599 for details.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 14029, "title": "fix broken link", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 14028, "title": "tf.train.MonitoredTrainingSession does not have request_stop() method", "body": "```\r\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"/tmp/train_logs\",\r\n                                       config=config,\r\n                                       hooks=hooks) as mon_sess:\r\n  while not mon_sess.should_stop():\r\n    # Perform synchronous training.\r\n    mon_sess.run(train_op)\r\n```\r\nI can not use mon_ses.request_stop().", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 14027, "title": "Problem with parameters use_bias=True and bias_initializer=None", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: tensorflow (1.3.0)\r\n- **Python version**: Python 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nSomething weird is happening when I'm using both the parameter use_bias and bias_initializer like this:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.reset_default_graph()\r\ninp = np.ones((1, 2, 2, 1))\r\ninputs_ = tf.constant(inp, dtype=tf.float32)\r\nn_filters = 1\r\nconv2d_tp = tf.layers.conv2d_transpose(inputs_, n_filters, [3, 3], \r\n                                       kernel_initializer=tf.ones_initializer(),\r\n                                       use_bias=True,\r\n                                       bias_initializer=None,\r\n                                       strides=(1, 1),\r\n                                       padding='same')\r\n\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    out = sess.run(conv2d_tp)\r\n    print(out)\r\n```\r\nAnd I got output like this(may differ at different runtime):\r\n[[[[ 3.28321671]\r\n   [ 3.28321671]]\r\n\r\n  [[ 3.28321671]\r\n   [ 3.28321671]]]]\r\nAs far as I'm concerned, the output element should all be integers, not floating numbers. I know setting the use_bias to True doesn't agree with setting bias_initializer to None in the first place since it's contradictory. But bad things happen when we ignore the use_bias and use the default value, at the same time setting the bias_initializer to None.\r\n### Source code / logs\r\nThe source code above should be enough to reproduce the output.\r\n", "comments": ["@fchollet this seems like at best a documentation bug. The doc says \"bias_initializer: An initializer for the bias vector. If None, then no bias will be applied.\" However it looks as if setting the initializer to None pipes all the way down to the lowest level get_variable which uses random_uniform as the default initializer for floats.", "I agree with @michaelisard that document should be revised: `If None, the default initializer will be used`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/562045bb93743b09f2652924fdf300fc28fca3db/tensorflow/python/layers/convolutional.py#L144-L153", "The expected behavior is that the absence or presence of a bias is controlled by `use_bias` (boolean). `None` passed to an initializer defaults to the current global default for initializers (glorot uniform). Please feel free to open a PR to improve the docs."]}, {"number": 14026, "title": "Raspberry example DecodeJpeg issue with Inception Retraining model ", "body": "# **Environment info**\r\n\r\nOperating System: raspbian\r\n\r\n# **Steps to reproduce**\r\n\r\n-   1.Follow the contrib/makefile/README to install the tensorflow raspbian core lib\r\n-   2.Run the pi example, both successfully\r\n-   3.Create inception model generally with the Tensorflow For Poets in Ubuntu 16.04 \r\n-   4.Retrain the model and get 'retrained_graph.pb' and 'retrained_labels.txt'\r\n-   5.Run the pi example with those two files by \r\n\r\n`tensorflow/contrib/pi_examples/label_image/gen/bin/label_image --image=xxx.jpg --graph=retrained_graph.pb --labels=retrained_labels.txt `\r\nand get error log \r\n`Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs. registered device:[CPU],registered kernels: <no registered kernels> \r\n[[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]`\r\n\r\n# **related**\r\n#2883 And I have also read your blog \u2018tensorflow for mobile poets\u2019 ... Is raspberry pi a mobile device? cuz when I run the command `tensorflow/python/tools:optimize_for_inference ` in the file made by contrib/makefile/README , it suggested that no such file or dictionery..\r\n\r\n\r\n**Any comment on what I do wrong.\r\nmany thanks**\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14024, "title": "support DepthwiseConv2dNative op when use python/tools/optimize_for_i\u2026", "body": "support DepthwiseConv2dNative op when use python/tools/optimize_for_inference.py", "comments": ["Can one of the admins verify this patch?", "@petewarden can you take a look?", "@dnecho Any updates? Marking as stalled.", "@dnecho feel free to reopen if you have more comments here."]}, {"number": 14023, "title": "TFRecords and Inference issues", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: conda\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: Nvidia quadro M6000\r\n- **Exact command to reproduce**:\r\n\r\nI converted my dataset into TFrecords and trained my model using a custom Network. Everything was quite fine until i tried to use the trained model by running an inference with the checkpoint files and the problems started there.Without using the _ = tf.contib.data.Dataset line it produces the following error\r\n\r\n ```\r\ndef create_graph():\r\n     with gfile.FastGFile(os.path.join(model_dir, 'frozen_net.pb'), 'rb') as f:\r\n         graph_def = tf.GraphDef()\r\n         graph_def.ParseFromString(f.read())\r\n         #_ = tf.contrib.data.Dataset   --------------------------->issue with this line \r\n         _ = tf.import_graph_def(graph_def, name='')\r\n\r\n\r\n def load_graph(frozen_graph_filename):\r\n     # We load the protobuf file from the disk and parse it to retrieve the \r\n     # unserialized graph_def\r\n     with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n         graph_def = tf.GraphDef()\r\n         graph_def.ParseFromString(f.read())\r\n\r\n     with tf.Graph().as_default() as graph:\r\n                     tf.import_graph_def(\r\n                         graph_def, \r\n                         input_map=None, \r\n                         return_elements=None, \r\n                         name=\"\", \r\n                         op_dict=None, \r\n                         producer_op_list=None\r\n                     )\r\n                     return graph\r\n\r\n```\r\n Traceback (most recent call last):\r\n  File \"/home/sysgen/files/TENSOR_FROZEN/UNFREEZING_NET.py\", line 159, in <module>\r\n    create_graph()\r\n  File \"/home/sysgen/files/TENSOR_FROZEN/UNFREEZING_NET.py\", line 28, in create_graph\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n  File \"/home/sysgen/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named Iterator in defined operations.\r\n\r\nThis was also the same issue which i faced even for freezing the whole graph ,without importing the Dataset module I'm not able to move forward. Even a normal restoring operation with saver.restore fails.\r\n\r\nFYI: I know how to solve the issue but my actual question is why it occurs and why was it not happening when i pickled the dataset and fed in the data? ", "comments": []}, {"number": 14022, "title": "tf.zeros doesn't accept a tensor argument", "body": "ValueError: Shape must be rank 1 but is rank 0 for 'zeros_2' (op: 'Fill') with input shapes: [], [].", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 14021, "title": "tf.train.start_queue_runners cannot run under GPU\uff1f", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\npython 3.5\r\ncuda 8.0 / cudnn 5.1\r\ntensorflow 1.2.1\r\ngpu memory Usage > 95%\r\ngpu Load < 30%\r\ncpu load > 85%\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen using tf.train.start_queue_runners in trainning,  the trainning process automatically \r\ngoes into cpu mode, when use tf.device('/gpu:0') an error accur thats could not find the data in queue;\r\nso, how should i use queue_runners+tfrecords in GPU?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nMain Trainning Code:\r\n\r\nwith tf.Session() as sess:\r\n    #with tf.device('/gpu:0')\r\n    sess.run(init)\r\n    saver = tf.train.Saver()\r\n\r\n    coord = tf.train.Coordinator() \r\n    threads = tf.train.start_queue_runners(sess=sess, coord = coord)\r\n    \r\n    global_step = 1\r\n   \r\n    if isTrain: \r\n        tf.train.write_graph(sess.graph_def, './MaxoutModel/', \"model.pb\", as_text=True)\r\n        ckpt = tf.train.get_checkpoint_state('./MaxoutModel/')\r\n        \r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            saver.restore(sess, ckpt.model_checkpoint_path)\r\n            print('Load Model OK!')\r\n        else:\r\n            print('Load Model Error!!')\r\n            pass\r\n    \r\n        for i in range(100):\r\n            \r\n            # learning rate decay\r\n            max_learning_rate = 0.02\r\n            min_learning_rate = 0.0001\r\n            decay_speed = 16000\r\n            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\r\n\r\n            tra_images, tra_labels = sess.run([img_batch, label_batch])\r\n            sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate,  keep_prob: 0.75})\r\n\r\n            if i%20 == 0:\r\n                cost = sess.run(cross_entropy, \r\n                        {X: tra_images, Y_: tra_labels , keep_prob: 1.0})\r\n            if i%20 == 0 and i !=0 :    \r\n                saver.save(sess,save_path='./BNModel/model.ckpt',global_step=global_step+1)\r\n        saver.save(sess,save_path='./BNModel/model.ckpt',global_step=global_step+1)\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]