[{"number": 11303, "title": "Catch keyboard interrupt in session run", "body": "Right now, Ctrl+c does not abort a long session run.", "comments": ["@danijar I want to work on this, pls provide some more info. ", "A fix for this might need significant changes to C++ part of the runtime.\r\nThis is similar to the issue of Dequeue of an empty queue causing an uninterruptible hang.\r\n\r\nPython client thread detects ctrl+C, but how to tell underlying C threads to die?\r\nTo see an example of complexity, this is the commit that added timeout option to session.run:\r\nhttps://github.com/tensorflow/tensorflow/commit/fa07bfda6896748b747334e750763f6feea91129\r\n\r\ncc: @mrry in case he can see a way for client to interrupt the runtime", "Slightly different question (from https://stackoverflow.com/q/45033413):\r\n\r\n> I wish to add functionality to my code such that if I desire to terminate the code at any point it will safely terminate training and save the variables. Although I've tried searching for a better solution, I think catching a KeyboardInterrupt exception would be my best bet.\r\n> \r\n> Would it, however, be safe? More specifically, would the following code work:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n  try:\r\n    for i in range(FLAGS.max_steps):\r\n      sess.run(train_op, feed_dict=some_feed_dictionary)\r\n      # Some other summary writing and evaluative operations\r\n  except KeyboardInterrupt:\r\n    print(\"Manual interrupt occurred.\")\r\n\r\n  print('Done training for {} steps'.format(global_steps))\r\n  save_path = saver.save(sess, 'Standard CNN', global_step=global_steps, write_meta_graph=False)\r\n```\r\n\r\n> Or is it unsafe and can result in corrupted save files considering the Keyboard Interrupt is free to occur in the middle of any tensorflow operation? Is there an adequate way of doing this?\r\n\r\nIs the call to `sess.run` (or the important parts of it) atomic on the Python side?\r\nAt least [this comment](https://github.com/tensorflow/tensorflow/issues/11982#issuecomment-361428885) by @petewarden suggests it is.", "Any update here?", "Any updates?"]}, {"number": 11171, "title": " [Feature] Node mirroring for GPU-memory reduction", "body": "In the paper [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174) Chen et al. introduced a very good idea to greatly reduce GPU memory requirements. \r\n\r\nThe idea bowls down to discarding the output of some nodes during the forward pass and recompute those values when they are needed again in the backward pass. Only the output of some key ops is kept in memory. During back-prop all forward computation from those key nodes are redone. They also describe how this can be implemented in a graph based computation model by mirroring non-key ops. This is depicted in figure 3 (see below).\r\n\r\nPerforming this kind of graph manipulation in [MxNet](https://github.com/dmlc/mxnet) is quite easy and I have played around with this myself. I am able to reduce the memory cost of a SotA segmentation model from `13504 MB` to `3382 MB` for the cost of about ~40% increase in computational time. (Given that we have plenty 1080 TI and view P100 GPUs, I am very happy to pay this cost).\r\n\r\nFor me as deep learning researcher this is a totally awesome killer feature. In most of my experiments I am limited by the amount of available GPU memory. Doing node mirroring allows me to try a whole bunch of new stuff, I was always wanting to do. \r\n\r\n1. Is anything like this planned to be implemented in Tensorflow any time soon?\r\n2. In the current API, is there already a way to build and / or manipulate the computational graph to perform node mirroring (like in figure 3)?\r\n\r\n![](https://i.imgur.com/CnkUzwJ.png)\r\n\r\nRegarding question two, I don't mind if it gets messy. Copying some nodes inside the graph is possible in tensorflow. Gradient flow can also be stopped for the first copy. What is missing is to utilize the second node for gradient computation. I don't know how I can archive this using the python API in tensorflow. Any ideas with this?\r\n\r\n", "comments": ["@prb12 can you comment or redirect? Thanks.", "I'm not really the best person to answer this, but yes we are aware of this technique and similar things crop up in various places in TensorFlow:\r\n\r\n- XLA has an optimization pass that does selective rematerialization of HLO expressions when necessary to fit in a memory budget `tensorflow/tensorflow/compiler/xla/service/hlo_rematerialization.h`\r\n- Grappler has a rewriting pass that does this: `tensorflow/tensorflow/core/grappler/optimizers/memory_optimizer.cc`\r\n- Not strictly speaking recomputation, but the dynamic RNN classes support a `swap_memory` flag that temporarily copies intermediate values required for backprop from the GPU back to host memory to allow very long sequence lengths without running out of GPU memory.\r\n\r\n@benoitsteiner is the best person to answer Grappler questions, @ebrevdo for the RNN code, and @eliben or @meheffernan for the XLA work.\r\n", "@sanjoy ", "Thanks for the answers. I had a short look through the gradient computation code in tensorflow. By now I am quite confident that node-mirroring can be be (fully) implemented in python. The whole think bowls done to manipulating the code which produces the [gradient computation subgraph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py#L488-L516).\r\n\r\nAt the moment the intermediate results of each tensor are saved in GPU memory (if gradients are computed). This is an *incredible* wasteful behavior. I think this is the main reason why tensorflow is so extremly hungry for GPU memory. \r\n\r\nAs an example, I have recently implemented a (gaussian) crf model in tensorflow. When performing back-propagation through the CRF the memory requirement went up from `320MB` to `10.45GB`, compared to plain inference. The issue in this case is, that the different computation along the line of  `tf.exp ( tf.reduce_sum(   -0.5 * tf.square( f_i - f_j ) * M ) )` are done very often. For each minor step in those kind of computations (i.e. `-0.5 * sqr_diff` ) the intermediate output is kept in memory.  The time it take to run the entire core of the CRF is only `347 \u03bcs`. This is negligible, compared to the rest of the model. \r\n\r\nSo the bottom line of this example is: Recomputation during backward pass could save 10GB for a negligible amount of extra computation time. In all models the majority of memory is used for storing tensors for back-prop. Allowing recomputation will reduce the memory requirement of all deep models (i.e. >= 12 layers) by an order of magnitude.", "Also, I had a short look at the other memory optimization projects mentioned. XLA and grapper implement good ideas. However those projects aim at performing memory optimization fully automated. I feel for advanced users it will be much more valuable to give them options for manual memory optimization. I think many users know the bottleneck of their model and manual optimization can archive much better results then automated memory optimization projects. [See the crf example].\r\n\r\nThe swap memory used for RNNs is a pretty cool idea. This can be used as an alternative to the recomputation approach. I think it would be great to have a function which could perform `swap_memory` on arbitrary (i.e. non-rnn) tensors.  There are long chain models even outside the rnn world and this can be very useful for those as well. [*Btw. could someone point me to the line where swap_memory is implemented? I am not familiar with the rnn code. What I have seen so far most functions just pass this parameter on. What would it take to have the output of an arbitrary tensor `t` swapped to CPU memory?*]\r\n\r\nOverall I think it would be cool, if this could be pinged to the right person at the tensorflow team. I feel that the large memory usage is the [biggest down-side of tensorflow](https://github.com/tensorflow/tensorflow/issues/492). We know two solutions for this. Namely `recomputation` of some tensors during back-prop (i.e. via node mirroring) or `memory swapping`. Both can be implemented in pure python. So I don't see much reason for not tackling this.", "You are right that the default automatic gradient code assumes that it's always cheaper to hang on to intermediate values than recompute.  This is usually true.  *But* there's nothing forcing you to use the automatic gradient code, and there are multiple ways to override gradient behavior.\r\n\r\nOne approach I've seen used for things like this is to write parts of your model using TensorFlow functions  (see `function.Defun`) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function.py#L42\r\n\r\nThis allows you to define a reusable subgraph as a python function, and optionally specify a second function for the gradient.  The gradient function is at liberty to recompute if it likes, or reuse the output(s) of the forward function.\r\n\r\nI'm not the expert on this, but @zffchen78 could probably suggest interesting ways to use these mechanisms. \r\n", "Thanks for the very fast reply, @prb12. I am very happy to learn about existing ways to recompute gradients instead of keeping them. This will allow me to use Tensorflow for my next CRF based project. I will take a closer look at defun and how it might be used for gradient recomputation after I return from my conference. I am very happy about additional suggestions in the meantime.\r\n\r\nRegarding the [feature-request]: In any model it is cheaper in terms of computational time to keep all intermediate results. However it is always cheaper in terms of memory requirement to recompute instead. GPU memory usually is a hard constrained. Having an easy way of doing recomputation in tf is therefore a very useful feature.", "One other way to temporarily override gradient code is to use `gradient_override_map`  \r\nhttps://github.com/tensorflow/tensorflow/blob/2cfbd3347b943a389bb688125c2a90095b7735b5/tensorflow/python/framework/ops.py#L3773\r\n\r\n```python\r\n @tf.RegisterGradient(\"CustomSquare\")\r\n\u00a0  def _custom_square_grad(op, grad):\r\n\u00a0  # ...\r\n\u00a0  \u00a0\r\n\u00a0with tf.Graph().as_default() as g:\r\n\u00a0  c = tf.constant(5.0)\r\n\u00a0  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\r\n\u00a0  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\r\n\u00a0    s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the gradient of s_2.\r\n```\r\nSince the custom gradient function is passed the original `Operation`, it can recompute the op from the same inputs.\r\n\r\nThe main reason this doesn't get used for recomputation is that this mechanism is done at an op granularity, whereas most people probably want to do recomputation at a layer granularity.\r\n\r\nThe `Defun` trick is basically the same thing but applied to a larger subgraph.    ", "We may be able to add a decorator/wrapper that just does this Defun trick.\nCall it \"@recompute_on_gradient\" or something.\n\nOn Fri, Jul 7, 2017 at 9:18 AM, Paul Barham <notifications@github.com>\nwrote:\n\n> One other way to temporarily override gradient code is to use\n> gradient_override_map\n> https://github.com/tensorflow/tensorflow/blob/\n> 2cfbd3347b943a389bb688125c2a90095b7735b5/tensorflow/python/\n> framework/ops.py#L3773\n>\n>  @tf.RegisterGradient(\"CustomSquare\")\n>    def _custom_square_grad(op, grad):\n>    # ...\n>\n>  with tf.Graph().as_default() as g:\n>    c = tf.constant(5.0)\n>    s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n>    with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n>      s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the gradient of s_2.\n>\n> Since the custom gradient function is passed the original Operation, it\n> can recompute the op from the same inputs.\n>\n> The main reason this doesn't get used for recomputation is that this\n> mechanism is done at an op granularity, whereas most people probably want\n> to do recomputation at a layer granularity.\n>\n> The Defun trick is basically the same thing but applied to a larger\n> subgraph.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313727272>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1TG9PmZIC7cGgBeK-kmWYy09-YRks5sLlpqgaJpZM4OKdxb>\n> .\n>\n", "> [Btw. could someone point me to the line where swap_memory is implemented? \r\n\r\nThe `swap_memory` flags get passed through to `tf.while_loop` and into the [`WhileContext`](https://github.com/tensorflow/tensorflow/blob/cf7c008ab150ac8e5edb3ed053d38b2919699796/tensorflow/python/ops/control_flow_ops.py#L1907),  which pass flags to the stack related ops [here](https://github.com/tensorflow/tensorflow/blob/cf7c008ab150ac8e5edb3ed053d38b2919699796/tensorflow/python/ops/control_flow_ops.py#L877-L878). \r\n\r\n`StackPush` operations have an [attr](https://github.com/tensorflow/tensorflow/blob/cf7c008ab150ac8e5edb3ed053d38b2919699796/tensorflow/core/ops/data_flow_ops.cc#L1132-L1149) to enable the functionality, and the actual swapping happens [here](https://github.com/tensorflow/tensorflow/blob/cf7c008ab150ac8e5edb3ed053d38b2919699796/tensorflow/core/kernels/stack_ops.cc#L222-L259).\r\n\r\n", "Getting Chen's performance is too complicated for gradient decorator -- for a uniform chain of length n, you want to insert \"checkpoints\" every sqrt(n) nodes, which doesn't necessarily correspond to number of gradient calls. This kind of mirroring is indeed possible by using contrib.graph_editor to clone nodes. This enables a node execution order which uses less memory, and you can use control dependencies to force TensorFlow to use this schedule. In other words, if you have a node like \"conv-forward\", you could duplicate it and then disconnect one of them from the gradient. Since the original node is not connected to gradient computation, its value can be forgotten at the stage of forward prop.\r\n\r\nTensorFlow default strategy will execute nodes as soon as possible, which is the opposite of what you want to do you in your scenario. Here's a [utility](https://github.com/yaroslavvb/stuff/tree/master/linearize) that forces TensorFlow to execute nodes as late as possible, breaking ties by using estimate of memory usage.\r\n\r\nForgetting intermediate results to fit in memory budget is actually a pretty interesting general problem, studied in compiler literature as \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can also be solved exactly for typical neural networks, since they tend to have small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes with 2^k states such that resulting \"merged\" graph is a chain)).", "Not sure I fully understand your checkpoints model, Yaroslav.  How is it\ndifferent from just using the Defun every sqrt(n) nodes - and the rest of\nthe time using the standard calls?\n\nOn Sat, Jul 8, 2017 at 11:01 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> Getting Chen's performance is too complicated for gradient decorator --\n> for a uniform chain of length n, you want to insert \"checkpoints\" every\n> sqrt(n) nodes, which doesn't necessarily correspond to number of gradient\n> calls. This kind of mirroring is indeed possible by using\n> contrib.graph_editor to clone nodes. This enables a node execution order\n> which uses less memory, and you can use control dependencies to force\n> TensorFlow to use this schedule. In other words, if you have a node like\n> \"conv-forward\", you could duplicate it and then disconnect one of them from\n> the gradient. Since the original node is not connected to gradient\n> computation, its value can be forgotten at the stage of forward prop.\n>\n> TensorFlow default strategy will execute nodes as soon as possible, which\n> is the opposite of what you want to do you in your scenario. Here's a\n> utility <https://github.com/yaroslavvb/stuff/tree/master/linearize> that\n> forces TensorFlow to execute nodes as late as possible, breaking ties by\n> using estimate of memory usage.\n>\n> Forgetting intermediate results to fit in memory budget is actually a\n> pretty interesting general problem, studied in compiler literature as\n> \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can\n> also be solved exactly for typical neural networks, since they tend to have\n> small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes\n> with 2^k states such that resulting \"merged\" graph is a chain)).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313871515>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimz_9lLTEe_QOPxaMVE8n7glMfIWBks5sL8PggaJpZM4OKdxb>\n> .\n>\n", "@ebrevdo manually wrapping sqrt(n) nodes into defun would do the trick, I just didn't get how the decorator would do this.\r\n\r\nThere's some ambiguity in docs on whether Defun recomputes the values. There's `noinline=True` option to Defun, although when I tried on 1.1, it worked without having to set that attribute.\r\n\r\nPS, swapping stuff to main memory is not that great for O(n) operations. I benchmarked it on TitanX, and recomputing large `Mul` from inputs on GPU was 7x faster than fetching result from main memory. Recomputing `Concat` was 10x faster", "To do fw recomputation in the gradient with a defun you would do something\nlike:\n\ndef fw(x1, x2):  # x1 and x2 are tensors\n  ...\n  return y1, y2  # y1 and y2 are tensors\n\ndef recalc_fw_grad(fw_op, dy1, dy2):\n  x1, x2 = fw_op.inputs\n  # DON'T USE THESE: # y1, y2 = fw_op.outputs\n  y1, y2 = fw(x1, x2)\n  return tf.gradients(ys=[y1, y2], xs=[x1, x2], grad_ys=[dy1, dy2])\n\n@Defun(python_grad_func=recalc_fw_grad, shape_func=...)\ndef fw_with_recalc_fw_grad(x1, x2):\n  return fw(x1, x2)\n\nOn Sun, Jul 9, 2017 at 5:34 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> manually wrapping sqrt(n) nodes\n> into defun would do the trick, I just didn't get how the decorator would do\n> this.\n>\n> There's some ambiguity in docs on whether Defun recomputes the values.\n> There's noinline=True option to Defun, although when I tried on 1.1, it\n> worked without having to set that attribute.\n>\n> PS, swapping stuff to main memory is not that great for O(n) operations. I\n> benchmarked it on TitanX, and recomputing large Mul from inputs on GPU\n> was 7x faster than fetching it from main memory. Recomputing Concat was\n> 10x faster\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313917326>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimz2q0GkE8o8g-scWStoufZ5lT4V5ks5sMMjpgaJpZM4OKdxb>\n> .\n>\n", "You probably also want to add noinline=True to the Defun constructor.\n\nOn Sun, Jul 9, 2017 at 3:02 PM, Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> To do fw recomputation in the gradient with a defun you would do something\n> like:\n>\n> def fw(x1, x2):  # x1 and x2 are tensors\n>   ...\n>   return y1, y2  # y1 and y2 are tensors\n>\n> def recalc_fw_grad(fw_op, dy1, dy2):\n>   x1, x2 = fw_op.inputs\n>   # DON'T USE THESE: # y1, y2 = fw_op.outputs\n>   y1, y2 = fw(x1, x2)\n>   return tf.gradients(ys=[y1, y2], xs=[x1, x2], grad_ys=[dy1, dy2])\n>\n> @Defun(python_grad_func=recalc_fw_grad, shape_func=...)\n> def fw_with_recalc_fw_grad(x1, x2):\n>   return fw(x1, x2)\n>\n> On Sun, Jul 9, 2017 at 5:34 AM, Yaroslav Bulatov <notifications@github.com\n> > wrote:\n>\n>> @ebrevdo <https://github.com/ebrevdo> manually wrapping sqrt(n) nodes\n>> into defun would do the trick, I just didn't get how the decorator would do\n>> this.\n>>\n>> There's some ambiguity in docs on whether Defun recomputes the values.\n>> There's noinline=True option to Defun, although when I tried on 1.1, it\n>> worked without having to set that attribute.\n>>\n>> PS, swapping stuff to main memory is not that great for O(n) operations.\n>> I benchmarked it on TitanX, and recomputing large Mul from inputs on GPU\n>> was 7x faster than fetching it from main memory. Recomputing Concat was\n>> 10x faster\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313917326>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtimz2q0GkE8o8g-scWStoufZ5lT4V5ks5sMMjpgaJpZM4OKdxb>\n>> .\n>>\n>\n>\n", "Hi, I'd like to reopen this discussion on whether there is a better way to do this. I followed @ebrevdo suggestion and implemented a `recompute_on_gradient` decorator that uses `Defun`. I have included the decorator and sample usage below. It works as desired, but it has some major limitations:\r\n1. I couldn't find a good way to pass in function parameters, e.g. `reduction_dim` in the example below.\r\n2. The tensors being passed to the grad function are of unknown shape, so you can't figure out certain properties of the inputs, e.g. `broadcast_dim_a` and `broadcast_dim_b` in the example below.\r\n\r\nFor 1., I tried using a closure to pass in the parameters, but that significantly slows down the execution. I also tried using `tf.name_scope`, but that is not that useful since the grad function doesn't get the `op`.\r\n\r\nIn general, it would be really useful to have a way of defining a subgraph of ops as a single op so that you can define a gradient for that subgraph as a whole, and thus preventing tensorflow from storing the intermediate tensors (which can be really huge) of that subgraph for the backward pass.\r\n\r\nI have looked into `gradient_override_map` and `tf.py_func`, but they don't seem to be applicable here. The former only seems to work for individual ops that have been defined in C++. The latter does group a subgraph of ops into a single one, however this is for numpy functions. What's needed is something like `tf.tf_func` for a subgraph of tensorflow ops.\r\n\r\n```\r\ndef recompute_on_gradient(func_name, grad_func=None):\r\n    \"\"\"\r\n    Assumes func takes in multiple inputs and returns a single output.\r\n    \"\"\"\r\n    def _recompute_on_gradient(func):\r\n        def _shape_func(op):\r\n            try:\r\n                return [output.shape for output in func(*op.inputs)]\r\n            except TypeError:\r\n                return [func(*op.inputs).shape]\r\n\r\n        if grad_func is None:\r\n            def _grad_func(*args):\r\n                inputs = list(args[:-1])\r\n                grad = args[-1]\r\n                output = func(*inputs)\r\n                return tf.gradients(ys=[output], xs=inputs, grad_ys=[grad])\r\n        else:\r\n            _grad_func = grad_func\r\n\r\n        _grad_func = function.Defun(func_name=func_name + \"_grad\",\r\n                                    noinline=True)(_grad_func)\r\n\r\n        return function.Defun(grad_func=_grad_func,\r\n                              shape_func=_shape_func,\r\n                              func_name=func_name,\r\n                              noinline=True)(func)\r\n    return _recompute_on_gradient\r\n\r\n\r\ndef _multiply_sum_grad(a, b, grad_c):\r\n    reduction_dim = -2\r\n    broadcast_dim_a = -1\r\n    broadcast_dim_b = 0\r\n\r\n    grad_c = tf.expand_dims(grad_c, axis=reduction_dim)\r\n    grad_a = grad_c * b\r\n    grad_b = grad_c * a\r\n    # sum over broadcast dimensions\r\n    grad_a = tf.reduce_sum(grad_a, axis=broadcast_dim_a, keep_dims=True)\r\n    grad_b = tf.reduce_sum(grad_b, axis=broadcast_dim_b, keep_dims=True)\r\n    return grad_a, grad_b\r\n\r\n\r\n@recompute_on_gradient(\"multiply_sum\", _multiply_sum_grad)\r\ndef multiply_sum(a, b):\r\n    reduction_dim = -2\r\n    return tf.reduce_sum(a * b, axis=reduction_dim)\r\n\r\n\r\n@recompute_on_gradient(\"multiply_sum_no_grad\")\r\ndef multiply_sum_no_grad(a, b):\r\n    reduction_dim = -2\r\n    return tf.reduce_sum(a * b, axis=reduction_dim)\r\n\r\n\r\ntime = 3\r\nbatch = 16\r\nin_height = 64\r\nin_width = 64\r\nin_channels = 1\r\nkernel_size = [17, 17]\r\nfilters = 4\r\npatches = tf.get_variable('a', [time, batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels])\r\nkernel = tf.get_variable('b', [batch, in_height, in_width, kernel_size[0], kernel_size[1], in_channels, filters])\r\nkernel_reshaped = tf.reshape(kernel, [batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels, filters])\r\n\r\noutput = tf.reduce_sum(patches[..., None] * kernel_reshaped[None, ...], axis=-2)  # uses more than 3GB to compute the gradient\r\noutput_mulsum = multiply_sum(patches[..., None], kernel_reshaped[None, ...])\r\noutput_mulsum_no_grad = multiply_sum_no_grad(patches[..., None], kernel_reshaped[None, ...])\r\n```", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "BTW, we just released a package to do this in TensorFlow:\r\nhttps://github.com/openai/gradient-checkpointing\r\nhttps://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9", "That looks great, thanks!", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Very cool @yaroslavvb!\r\n\r\n@MarvinTeichmann does Yaroslav's package resolve this issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "/CC @prb12 any updates on this? @yaroslavvb package is very interesting. Should something similar be in TensorFlow itself?", "@reedwm I think Grappler is eventually supposed to take over these things (@allenlavoie )", "There's also the [recompute_grad decorator](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/recompute_grad) now. And Grappler does some swapping to host memory by default. But yes, it'd be nice if Grappler's memory optimizer did checkpointing automatically keeping device memory constraints in mind. (But unfortunately I have no immediate plans to work on it.)", "I'll assign to you @allenlavoie and mark as \"awaiting tensorflow\", which now means you currently aren't working on it.", "I think it's fine as \"contributions welcome\". I'll grab the bug if I start working on it. Happy to discuss if anyone wants to take it first.", "Maybe I'm doing something wrong, but the recompute grad decorator isn't seemingly saving myself any memory. I'm using it naively on every `tf.layers.conv2d` like so:\r\n\r\n```\r\n@recompute_grad\r\ndef _x(x):\r\n    return tf.layers.conv2d(x, ...)\r\n```\r\nHowever, it uses the same amount of memory as standard TF 1.10 (i.e. not using the decorator). \r\n\r\nAlso, I used to have gradient_checkpointing allowing me to have larger batch sizes (back in april) however it now doesn't seem to work either.\r\n\r\n@allenlavoie Any ideas why? ", "@joeyearsley recompute_grad needs to catch an intermediate activation which would otherwise feed into a gradient op in order to save memory. `tf.layers.conv2d` has a conv op and maybe a ReLU, and ReLU automatically recomputes, so if the model is just a sequence of `_x` calls then it's already saving only one activation per `_x`. To save memory, you could wrap a sequence of `_x` calls in `recompute_grad` (but include too many and the recomputation of the gradient will take as much memory as the model would have otherwise). If there's a batch norm layer I'd include that in the decorated function, since it isn't automatically recomputed like ReLU."]}, {"number": 11085, "title": "Tensorflow - Metal Support for Mac OS", "body": "Hello!\r\nI have seen and read some requests for OpenCL support and GPU support on Mac OS, this seems to have been abandoned, am I correct?\r\nBut it also seems like Apple is really trying to make Metal big, is this something you have thought of implementing? I understand its not just made by thinking of it, but I would just like to know if any progress is made with GPU support for Mac OS?", "comments": ["@petewarden can you comment or redirect? Thanks.", "@keveman has been looking at this.", " https://github.com/caffe2/caffe2/pull/215 could serve as a reference. ", "Given the developments in the eGPU space, it would be wonderful to see continued GPU support for macOS. eGPUs on macOS may become a popular alternative for running locally vs. a linux server.", "@andrewrech Yeah the new version of macOS comming this fall supports eGPU, but not supporting the built in graphics would be a bummer for me as I was not planning on buying an NVIDIA card, just to play with Tensorflow. From my understanding just supporting Metal in Tensorflow means that it supports most of the GPU's connected to the computer?", "See https://github.com/tensorflow/tensorflow/issues/7958", "I'm gonna close this as a duplicate of #7958 (thanks @cancan101), please reopen if I'm mistaken.", "I can't reference #7958 for some reason... but I don't think my issue is a duplicate of this, as this is asking for metal support in iOS.", "The title of that issue is: \"Support MPSCNN (MetalPerformanceShaders) on iOS\".", "Oh sorry I don't fully understand how GitHub works, when I tried referencing #7958 it didn't turn blue like other references but I can see it did now. \r\nBut what I tried to explain is that I don't think my is a duplicate of #7958 as that is asking for Metal support in iOS I am asking for Metal support in macOS", "I think the other ticket can just be expanded to encompass Mac os ", "I'm not sure if there's extra work required to implement Mac OS support on top of iOS support, so I'll just reopen this ticket.", "+1", "+1", "@BilboSwagginz Yes, but I would also love Nvidia eGPU support, but I think I am a small use case here.", "+1", "+1", "+1", "+1", "+1", "+1", "Apple just announced at WWDC that they\u2019ll be helping to bring Metal support to Tensorflow. ", "@soleares I am looking for more information about that - does anyone have specifics about Apples Metal / Metal Performance Shader additions to Tensorflow?", "If there will be a plan to port for Metal API, I would Iike to contribute with coding :)", "![screen shot 2018-06-06 at 1 44 55 pm](https://user-images.githubusercontent.com/38001252/41064252-e2de3404-698f-11e8-9a04-6812ef738d8b.png)\r\n", "@JacquisLewis yea, exactly what my comment was referencing - not sure why folks are giving me the confused reaction. My only real question is what is the timing for the TF metal additions?", "@Vade , I would say just Apple's ongoing commitment to offer great low-level support for the multitude of ML training libraries now supported by Core ML/Core ML 2.   (Edit: \"My only real question is what is the timing for the TF metal additions?\" TensorFlow is now supported ML library in Core ML 2.)\r\n![screen shot 2018-06-06 at 1 58 54 pm](https://user-images.githubusercontent.com/38001252/41064982-045d734a-6992-11e8-941b-ea694bebae75.png)", "Not sure I understand, is there support for metal accelerated tensorflow right now? If yes, is there any documentation/resource on how to access it? Is the latest version of tensorflow metal accelerated?", "No support publicly and Apple is being typically opaque about what their partnership with google + metal really refers to, and when it will see the light of day. \n\nIn other words best to figure something else out. \n\n   \n   \n\n http://vade.info\n\n> On Sep 22, 2018, at 11:37 AM, vashat <notifications@github.com> wrote:\n> \n> Not sure I understand, is there support for metal accelerated tensorflow right now? If yes, is there any documentation/resource on how to access it? Is the latest version of tensorflow metal accelerated?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "I would love to see Metal support in Tensorflow, even if I know it's definitely a niche. ", "> Not sure I understand, is there support for metal accelerated tensorflow right now? If yes, is there any documentation/resource on how to access it? Is the latest version of tensorflow metal accelerated?\r\n\r\nIn one of the talk of WWDC 2018, on Metal Performance Shaders for training, the talker, at the very end of the presentation,  mentioned that they were working with Google on using MPS as a backend for Tensorflow. She gave a demo using an external GPU to accelerate Tensorflow. I could not find any news after that. Hope they would not delay its release until WWDC next year.", "If you have a Metal enable Mac machine you can follow this tutorial and run MPS based neural networks in Autodesk Flame 2020.\r\n\r\nhttps://www.youtube.com/watch?v=ChHrlM0njKs\r\n\r\nYou can get a free trial here:\r\nhttps://www.autodesk.com/products/flame/free-trial\r\n\r\nFalls back to using CPU if there is no Metal Support which is what I found on my 2013 iMac.\r\n\r\nI am not from Autodesk but I found it quite mind blowing that this is working.", "any update on this?", "+1", "I am interested in this and have been looking into nGraph and PlaidML", "I have used PlaidML with Keras on MacBook Pro with Metal. Works well. \r\n\r\nEdit: Works well for small projects. It's a tiny GPU. ;) ", "@vikrantt Did you do any production work with it, or just the PlaidML \"hello world\" benchmarks?", "@vikrantt If you have a link to the source for the project you used in PlaidML that would be super useful.", "No production work. I did some toy tasks such as mnist. Most of the production stuff we do is too large for the MBP GPU. I have access to many Nvidia GPUs so didn't spend too much time on it.  I will post the code if I can find it; been a while. ", "@vikrantt Thanks, mate. PlaidML looks like a feasible solution to do some toy experimenting on Macs. Please let us know if you could find any code more than MNIST. Best. ", "Any update on this?", "Any `updates`?", "> I have used PlaidML with Keras on MacBook Pro with Metal. Works well.\r\n> \r\n> Edit: Works well for small projects. It's a tiny GPU. ;)\r\n\r\nI tested Tensorflow 2.2.0 on Nvidia GTX 1080Ti/Ubuntu 16.04 on a machine three and half years old and AMD Radeon Pro 580x in Mac Pro 12 cores on the same natural language processing code. 1080Ti is loosely 2.5 faster than running Radeon Pro 580x on Metal. Given the DDR/GDDR speed is quite different I'd say the gap between cuda and PlaidML/metal support is even bigger. Even worst I start to see my training on PlaidML produce completely different result than my CUDA or CPU result, suggesting there are bugs likely in PlaidML or Metal. In fact Radeon Pro 580x only gave me performance loosely match with my 12-core Xeon. So my conclusion is Metal support in Tensorflow in close to not useful at all.\r\n\r\nWe probably have to wait until Apple start to get more driven to push for better eco-system support for their Metal API, or throw the towel to give room to CUDA playing a fair game.", "Thanks, great summary", "> > I have used PlaidML with Keras on MacBook Pro with Metal. Works well.\r\n> > Edit: Works well for small projects. It's a tiny GPU. ;)\r\n> \r\n> Even worst I start to see my training on PlaidML produce completely different result than my CUDA or CPU result, suggesting there are bugs likely in PlaidML or Metal.\r\n\r\nYea, when I tried it many moons ago I saw some discrepancies as well, as well as some exploding/vanishing\r\ngradients training with Keras+PlaidML that I didn't have just using the CPU.  I just kept using the CPU instead,\r\nand eventually got an Ubuntu PC with a GPU to use over the network.\r\n", "Hi! Noticed a thing they said on the Apple Silicon announcement yesterday. 33:53 minutes in to the video https://www.apple.com/apple-events/november-2020/ she says: \"It also makes Mac mini a great machine for developers, scientists and engineers utilizing deep learning technologies like Tensorflow or Create ML which are now accelerated by M1\". Sounds like they may be using the M1 chip Neural Engine for accelerating Tensorflow in the new Silicon Macs?", "> she says: \"It also makes Mac mini a great machine for developers, scientists and engineers utilizing deep learning technologies like Tensorflow or Create ML which are now accelerated by M1\"\r\n\r\n> Sounds like they may be using the M1 chip Neural Engine for accelerating Tensorflow in the new Silicon Macs?\r\n\r\nWell, is not it the classical marketing? It _might seem to imply_ that yet the reality is likely trivial. You can _just run TF on ARM_. ", "> > she says: \"It also makes Mac mini a great machine for developers, scientists and engineers utilizing deep learning technologies like Tensorflow or Create ML which are now accelerated by M1\"\r\n> \r\n> > Sounds like they may be using the M1 chip Neural Engine for accelerating Tensorflow in the new Silicon Macs?\r\n> \r\n> Well, is not it the classical marketing? It _might seem to imply_ that yet the reality is likely trivial. You can _just run TF on ARM_.\r\n\r\nCall me naive, but I wouldn't treat a machine designed for inference next to the other one designed for training. Not only they are completely on different caliber but also there is an implicit requirement for handling very small weight. (training is in general more sensitive to small value change in back-propagation) The floating point unit basically have to be designed with that considered.", "Good news! https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html . Works both for Intel and  M1. No SciPy om M1 yet though. Here is the Apple repo https://github.com/apple/tensorflow_macos .", "> Good news! https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html . Works both for Intel and M1. \r\n\r\nNone of the metrics include Intel plus external AMD eGPUs so looks like it's purely CPU acceleration ", "TensorFlow 2.4 use [ML Compute](https://developer.apple.com/documentation/mlcompute) as backend, so it can be accelerated by GPU.", "> Good news! https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html . Works both for Intel and M1. No SciPy om M1 yet though. Here is the Apple repo https://github.com/apple/tensorflow_macos .\r\n\r\nLooks like a PR post sponsored by Apple to me.\r\n\r\n![MacBook Pro 1](https://user-images.githubusercontent.com/5262321/99635322-1b59a500-2a42-11eb-8055-42681e7173ed.png)\r\n\r\nWhat does the graph show exactly? It makes it seem that M1 trashes Intel CPUs. Does both use GPUs? Could someone here find/post an independent comparison of with (CPU, GPU) x (Intel i7, M1) properly identified?\r\n\r\n", "@letalvoj \r\n> What does the graph show exactly? It makes it seem that M1 trashes Intel CPUs. Does both use GPUs? Could someone here find/post an independent comparison of with (CPU, GPU) x (Intel i7, M1) properly identified?\r\n\r\nAccording to Apple's documentation,\r\n\r\n> ML Compute\r\n> Accelerate training and validation of neural networks across the CPU and one or more GPUs.\r\n\r\nThe backend *might* use GPU. I would be surprised if their custom NPU is not used. M1 has 2TFLOPS so trashing a Intel CPU with an years-old integrated GPU is not surprising at all. Their post below does show the performance of Mac Pro using AMD GPUs and it is significantly faster than M1.\r\n\r\nNow that Apple finally delivered what they promised on WWDC 2018, I suppose this issue could be closed?\r\n", "> M1 has 2TFLOPS so trashing a Intel CPU with an years-old integrated GPU is not surprising at all.\r\n\r\n- Iris 1.6 TFLOPS\r\n- M2 2.6 TFLOPS\r\n\r\nPR numbers are usually cherry picked. The rough estimate is that the GPU should be like 1.5x faster not 4x faster \ud83e\udd14 Maybe of the more tightly unified GPU-CPU memory on M1? \ud83e\udd37\r\n\r\n> I would be surprised if their custom NPU is not used.\r\n\r\nAccording to [CoreML](https://developer.apple.com/documentation/coreml/mlupdatetask) the dedicated NPU can do incremental updated of running models, yet the API seems to be _inference oriented._ According to the [TF documentation](https://github.com/apple/tensorflow_macos#device-selection-optional) the available backends are GPU, CPU, ANY where ANY chooses the one of the former. ", "> > M1 has 2TFLOPS so trashing a Intel CPU with an years-old integrated GPU is not surprising at all.\r\n> \r\n> * Iris 1.6 TFLOPS\r\n> * M2 2.6 TFLOPS\r\n> \r\n> PR numbers are usually cherry picked. The rough estimate is that the GPU should be like 1.5x faster not 4x faster \ud83e\udd14 Maybe of the more tightly unified GPU-CPU memory on M1? \ud83e\udd37\r\n> \r\n> > I would be surprised if their custom NPU is not used.\r\n> \r\n> According to [CoreML](https://developer.apple.com/documentation/coreml/mlupdatetask) the dedicated NPU can do incremental updated of running models, yet the API seems to be _inference oriented._ According to the [TF documentation](https://github.com/apple/tensorflow_macos#device-selection-optional) the available backends are GPU, CPU, ANY where ANY chooses the one of the former.\r\n\r\nApple's neural engine is reported 11 TOPS according to Wikipedia, although I am not sure among that 11 TOPS how many of them are 8-bit/16-bit/32-bit. 8-bit is good for inference but pretty useless for training in general. (yes I know it is possible but not mainstream, TF are mostly used on half precision as far as I can tell). ", "Announced apart of WWDC21 you can now get metal support in TensorFlow 2.5 via a plugin provided by apple.\r\n\r\n[https://developer.apple.com/metal/tensorflow-plugin/ ](https://developer.apple.com/metal/tensorflow-plugin/ )\r\n\r\nP.S. An additional link from the TF blog post that enabled this [https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html](https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html)", "Anyone had any success with the new metal tensorflow plugin? For me it works even worse than the accelerated tensorflow alpha released last year.", "@vashat the plugin released above only works right now in macOS 12.0+ with that version just being released in beta as of last week I would expect some stability / performance issues until Monterey officially comes out this fall.\r\n\r\nOut of curiosity what are the specs of you system? I'm hoping to have a chance soon to install the beta OS and dive into this more.", "@nicollis using it on Big Sur on Macmini M1 with 16GB RAM. Using the plugin with mnist I get 10 seconds per epoch which is extremly slow. With the standard CPU tensorflow compiled from source I get 1 second per epoch. So Apples plugin is 10 times slower using GPU (I've checked GPU is used 100%) than using CPU only on same machine. Trying more complex projects like DGAN networks work fine on the machine with CPU version of tensorflow, but freeze within seconds with Apples plugin. I have to add that I didn't notice this was for Monterey, hopefully the issues are related that I run this under Big Sur.", "I believe you need macOS 12 Monterey and not macOS 11 Big Sur. \n\n   \n   \n\n http://vade.info\n\n> On Jun 15, 2021, at 10:54 AM, vashat ***@***.***> wrote:\n> \n> \ufeff\n> @nicollis using it on Big Sur on Macmini M1 with 16GB RAM. Using the plugin with mnist I get 10 seconds per epoch which is extremly slow. With the standard CPU tensorflow compiled from source I get 1 second per epoch. So Apples plugin is 10 times slower using GPU (I've checked GPU is used 100%) than using CPU only on same machine. Trying more complex projects like DGAN networks work fine on the machine with CPU version of tensorflow, but freeze within seconds with Apples plugin.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "Can we close this?", "This issue is not solved yet.\n\nOn Thu, Sep 9, 2021 at 2:12 AM bhack ***@***.***> wrote:\n\n> Can we close this?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11085#issuecomment-915657333>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADJAWUDI2W4KE34HGJ2DCNLUA73YFANCNFSM4DQ3PJ4Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n\n\n-- \n-Saeed\n", "> This issue is not solved yet.\r\n\r\nWhat is missing?", "It does not work in macOS Big Sur Version 11.5.2. It has to work otherwise,\nwhy did we have to pay for a GPU that we never used?\n\nOn Thu, Sep 9, 2021 at 9:49 PM bhack ***@***.***> wrote:\n\n> This issue is not solved yet.\n>\n> What is missing?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11085#issuecomment-916388253>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADJAWUCQA7VRAFRSDCWAEG3UBEFSZANCNFSM4DQ3PJ4Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n\n\n-- \n-Saeed\n", "Metal support is available now as the above [comment](https://github.com/tensorflow/tensorflow/issues/11085#issuecomment-857003736) suggests. @saeedm2020 ! Could you try again after updating your MacOS to 12.0 beta as per the plugin requirement . Thanks!", "I am using MacOS 12.0.1.\nI followed the instruction in\nhttps://developer.apple.com/metal/tensorflow-plugin/. I have got error\nmessage when tried to install tensorflow-macos with pip (python -m pip\ninstall tensorflow-macos).\nReceived error message:\nERROR: Could not find a version that satisfies the requirement\ntensorflow-macos (from versions: none)\nERROR: No matching distribution found for tensorflow-macos\n\nOn Sat, Nov 27, 2021 at 1:23 AM mohantym ***@***.***> wrote:\n\n> Metal support is available now as the above comment\n> <https://github.com/tensorflow/tensorflow/issues/11085#issuecomment-857003736>\n> suggests. @saeedm2020 <https://github.com/saeedm2020> ! Could you try\n> again after updating your MacOS to 12.0 beta as per plugin requirement .\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11085#issuecomment-980474680>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADJAWUEFKYA7OBJOO32DYALUOAQILANCNFSM4DQ3PJ4Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n\n\n-- \n-Saeed\n", "@saeedm2020 ! You should run the command  `python -m pip install tensorflow-macos` after specifying tensorflow version in this command `conda install -c apple tensorflow-deps==2.6.0`  from step 1. Thanks!\r\n\r\n\r\n", "@Dakkerad! Is this issue good to close then?", "No, it's not.\n\nOn Fri, Dec 3, 2021 at 2:41 PM mohantym ***@***.***> wrote:\n\n> @Dakkerad <https://github.com/Dakkerad>! Is this issue good to close then?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11085#issuecomment-985530063>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADJAWUEWAOQPJLGUYFWHN3TUPDCIZANCNFSM4DQ3PJ4Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n\n\n-- \n-Saeed\n", "@mohantym <https://github.com/mohantym> I have a Graphics \"Radeon Pro Vega\n16 4 GB\" which is an AMD graphics card. Installing Conda is in the\ninstruction for \"arm64 : Apple Silicon\" which is not applicable to my\ngraphics card. I have followed \"x86 : AMD\" in Step 1 as instructed. Then\nwent to Step 2. Thanks for your response. /saeed\n\nOn Mon, Dec 6, 2021 at 8:00 AM Saeed Mohammadi ***@***.***> wrote:\n\n> No, it's not.\n>\n> On Fri, Dec 3, 2021 at 2:41 PM mohantym ***@***.***> wrote:\n>\n>> @Dakkerad <https://github.com/Dakkerad>! Is this issue good to close\n>> then?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/11085#issuecomment-985530063>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ADJAWUEWAOQPJLGUYFWHN3TUPDCIZANCNFSM4DQ3PJ4Q>\n>> .\n>> Triage notifications on the go with GitHub Mobile for iOS\n>> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n>> or Android\n>> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>>\n>>\n>\n>\n> --\n> -Saeed\n>\n\n\n-- \n-Saeed\n", "I don't think this should be closed until `pip install tensorflow` does the right thing. Not everyone wants to or can use conda.", "Now that Monterey is out, is anybody having any good performance with the new metal tensorflow plugin? For me it is still slow on M1. Mnist with vanilla cpu tensorflow gives me 1 s per epoch, metal tensorflow is now down to 7 s on the same hardware. More complex models don't even work at all, they just get stuck.", "Hi.\r\n\r\nI think that this issue should be answered as the progress of this article \"[Accelerating TensorFlow Performance on Mac](https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html)\" here.\r\nI want to know the progress of the plan merging tensorflow-macos.\r\n\r\nIf your question is about tensorflow-macos and tensorflow-metal plugin rather than tensorflow itself, I guess that you should ask it on Apple Developer Forums as [Apple suggests](https://developer.apple.com/metal/tensorflow-plugin/).", "tensorflow-macos != tensorflow"]}, {"number": 10785, "title": "Feature Request-Randomized Hashing", "body": "This is feature request to see if randomized hashing can be implemented to relevant part of the library to allow the option to be utilized for better computational resource utilization:\r\n\r\nhttps://arxiv.org/abs/1602.08194", "comments": ["We will likely not work on this in the foreseeable future, but marking contributions welcome to see if there is additional interest.", "I am wondering the exact use case you are trying to address, as we already have randomised hashing in `sklearn` ([here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html) and [there](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)).\r\n\r\nOr you would like to implement this in C/C++ and export the symbol to bindings of other languages?", "@byronyi Thank you for pointing this functionality in sklearn out for me.  The use case I am trying accomplish is the randomized hashing to radically reduce training time/resource consumption for deep neural networks in Keras/TensorFlow (as demonstrated in the paper).  I will reread the paper to see if it will be as simple as the initial feature extraction or a more complex architecture.  I suspect it is latter case.", "@Erstwild Has it been implemented yet? Or is it in progress?  "]}, {"number": 10767, "title": "[Feature Request] Exclude ties in function in_top_k", "body": "This is a feature request related to #10489 (tie handling in function in_top_k)\r\n\r\nWould it be possible to add an option/argument specifying whether ties should be included or excluded?\r\nIn other words, make it possible for ties to return False instead of True.\r\n\r\nThanks!", "comments": ["@gfolego, could you give some motivation for why this is needed. It seems you worry that there is a trivial null-space to the loss function where you return constant 1's as your logits which causes zero loss since every class is in top_k. However, with random weight initialization this doesn't seem likely to be found with sgd since the gradient will not reliably target that.", "@aselle Sure! I'll use ImageNet as an example, but my case is quite similar.\r\nLet's say I have the ILSVRC2017 dataset, and I'm working on image classification only, so my main objective is to get a high top 5 accuracy.\r\nNow, I'm analyzing a CNN's performance given a specific number of training images, which will vary from very few to the complete training set. The motivation behind this analysis is to determine the best trade-off between size and accuracy, so we know how many images we need when collecting a new dataset.\r\n\r\nIn particular, ILSVRC2017 training dataset (for classification/localization) has between 732 and 1300 images for each class. If I use as few as 5 from each class for training a (large) CNN from scratch, this will most likely result in a CNN with random predictions or maybe predicting always the same class. So the top 5 accuracy for this model will be artificially (and incorrectly, IMHO) high.\r\n\r\nSo, if I'm early stopping or choosing a model based on this metric, I'll get a CNN that doesn't work in practice. Also, based on this approach, training with 5 samples will most likely achieve better top 5 accuracy than training with the complete set.\r\n\r\nPlease note that I'm not the first one experiencing this, as indicated in #10489 and in this issue on Keras (using TF's in_top_k) https://github.com/fchollet/keras/issues/6167\r\n\r\nTL;DR: If a CNN predicts the same probability for every class, its top 5 accuracy in ILSVRC2017 classification is 100%, which is naturally incorrect.\r\n\r\nThanks!", "@fchollet, @martinwicke, what is your feeling on this feature? Should we add an argument to in_top_k? It seems like the conceptional conflict is that it is hard to make a top_k that ignores ties in a consistent way but is also fixed size.", "It seems it's hard either way -- if you include ties, you may end up with a larger than wanted set, if you exclude them, you may end up with a smaller than wanted set. If you want to enforce a constant size, you have to pick from the set of ties.\r\n\r\nThe logic for this would be comparatively complicated, since I assume you'd definitely want to include ties if the set of tied values fits in k (i.e. for top 5, if places 1 and 2 are tied, you still want them in the set).\r\n\r\nI think an extra argument `handle_ties` with possible values `include`, `exclude`, or `sample` (the default being sample) would make sense.", "@martinwicke Thanks for sharing your thoughts. I agree there's no trivial way to enforce a fixed set size. I'm not familiar with the tensorflow code base, so I'm not exactly sure how functions tf.nn.in_top_k and tf.nn.top_k relate to each other, but it seems that they handle ties differently.\r\n\r\nGiven that, would it be better to change them in order to behave similarly, or would it be better to keep current behaviors? If we were to keep tf.nn.top_k's current behavior, I believe we would need the respective parameter option, such as `index` or `order`. Here, I'm assuming your `sample` means random.\r\n\r\nRegarding the logic, here's one option (assuming zero index):\r\n\r\n```\r\nsort probabilities\r\nif prob[k-1] != prob[k]\r\n    // happy ending\r\nelse\r\n    if handle_ties == include\r\n        // return all positions greater than or equal to prob[k-1]\r\n    else if handle_ties == exclude\r\n        // return all positions greater than prob[k-1]\r\n    else if handle_ties == sample or index\r\n        // get all positions greater than prob[k-1] (which will be returned)\r\n        // get all positions equal to prob[k-1] (ties)\r\n        // fill remaining positions (according to sample or index)\r\n```\r\n\r\nI hope this helps somehow. But it's a great discussion anyway.\r\nThanks and best regards!", "I think adding such an option would be a good addition if fixed length top k is asked for, and it would clarify the current behavior.", "@martinwicke I have sent a PR, please take a look if you have time.", "Current implementation looks really dangerous. A real case from StackOverflow: [Mysterious ReLu](https://stackoverflow.com/q/48993004/712995)", "I would love a PR to fix this. @nolanliou valiantly tried. A backwards compatible version as described above should not be terribly hard to do."]}, {"number": 10679, "title": "recovery_wait_secs feature for tf.train.MonitoredTrainingSession() similar to the one present in tf.train.SessionManager()", "body": "I ran the distributed model for a small data set and for very few epochs for some test run.\r\n\r\nThe chief worker started normally and finished training (training time was less than 30 seconds) but the other workers did not start.\r\n\r\nBelow log message has been displayed in all the workers other than chief:\r\n`Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: <list_of_variables >`\r\n\r\nThen I found the reason behind this in [this](https://stackoverflow.com/questions/42986653/distributed-tensorflow-not-running-some-workers/43007657#43007657) Stackoverflow answer.\r\n\r\nA `recover_wait_secs` feature would make it easier rather handling that timeout period manually.", "comments": ["@ispirmustafa, could you have a look.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I have the same problem. Waiting for 30s is too long for some test case.  I want to set it to 1s or below. ", "I agree it's a good idea to reveal `recover_wait_secs` in `MonitoredTrainingSession`. ", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@abishgj-rzt  How did you handle the timeout now? I am also facing the same issue. "]}, {"number": 10352, "title": "Feature request: Update OpDef proto to ease 1-based indexing", "body": "It would be nice if the `OpDef` proto included information on which inputs and attributes are indices, so that TensorFlow bindings for index-from-1 languages (like Julia) could automatically subtract 1 from the parameters of client calls that refer to those parameters. \r\n\r\nCurrently, the Julia binding has to rely on [rough heuristics](https://github.com/malmaud/TensorFlow.jl/blob/40d963f010bd394258d4a950069db85401430050/src/ops.jl#L232), like checking if the operation's input's type attribute is called \"Tidx\", to provide the conversion. ", "comments": ["@josh11b Our friend here maintains the [Julia language wrapper for TensorFlow](https://github.com/malmaud/TensorFlow.jl). It would make his job easier if the OpDef proto specified which inputs and attributes are indices. Does this sound like a reasonable request to you?", "It sounds like a reasonable feature, but it would be hard to write a test that it is correct.  Combined with 1-based-indexing languages being off the radar for most people authoring ops, it seems like it would likely be unreliable in practice even if we added it to the OpDef. I'm further concerned that if we get this wrong for an op it could very well be a backwards-incompatible change to fix the op's definition. We a planning to switch to keeping only info needed by the runtime for interpreting graphs in the OpDef and moving other information needed by the APIs (such as docs) into separate files (tensorflow/cc/ops/op_gen_overrides.pbtxt was the first iteration of this, but we are planning another iteration).  Are there other languages with TF bindings that have this issue?", "I suppose other 1-based languages could benefit as well...Matlab/Octave, Lua, and Mathematica come to mind. \r\n\r\nRe moving things out of `OpDef`: Indeed, for my purposes it only matters that the information is somehow programmatically available, not necessary that it is part of the `OpDef` proto. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @josh11b: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @josh11b: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Re: my last comment, we would not put this into the `OpDef` proto now that we have `ApiDef` proto that is more appropriate."]}, {"number": 10250, "title": "Error on importing metagraph that uses unbound input multiple times", "body": "If I export a scoped metagraph which has multiple references to a tensor which will be unbound after exporting, I cannot re-import the metagraph. This problem can be reproduced with the following MWE.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    inputs = tf.placeholder(shape=[], dtype=tf.float32, name=\"inputs\")\r\n\r\n    with tf.name_scope(\"scope\"):\r\n        output = inputs + inputs\r\n\r\n    tf.train.export_meta_graph(\"./mwe.meta\",\r\n                               export_scope=\"scope\",\r\n                               as_text=True)\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    inputs = tf.constant(shape=[], value=1.0)\r\n\r\n    tf.train.import_meta_graph(\"./mwe.meta\",\r\n                               import_scope=\"scope\",\r\n                               input_map={\r\n                                   \"$unbound_inputs_inputs\": inputs\r\n                               })\r\n\r\n```\r\nRunning this code results in the following error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"<snip>\\metagraph_bug_mwe.py\", line 23, in <module>\r\n    \"$unbound_inputs_inputs\": inputs\r\n  File \"<snip>\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1595, in import_meta_graph\r\n    **kwargs)\r\n  File \"<snip>\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 479, in import_scoped_meta_graph\r\n    \",\".join([compat.as_str(v) for v in field.value\r\nValueError: Graph contains unbound inputs: $unbound_inputs_inputs,$unbound_inputs_inputs. Must provide these inputs through input_map.\r\n```\r\nUpon inspection of the generated metagraph file (see [mwe.meta](https://github.com/tensorflow/tensorflow/files/1033570/mwe.meta.txt), lines 67 - 75), I noticed that the `inputs` tensor actually has two entries in the `unbound_inputs` collection. Since the `import_scoped_meta_graph` function compares the full collection read from the proto to the `input_map` parameter, they do not match and the error is raised.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see above\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0/5.1.10\r\n- **GPU model and memory**: Nvidia GeForce GTX 860M with 2GB VRAM + 4GB shared\r\n- **Exact command to reproduce**: See MWE above\r\n\r\n", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Is this still an issue?", "Yes, the MWE provided above still breaks with the same exception in 1.4.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Looks like this is not expected behavior. Are you in a position to send a fix?", "I am currently working on a PR to fix this issue", "Any news on this topic?\r\nI am getting a list of unbound inputs:\r\nif not input_map or v not in input_map]))\r\nValueError: Graph contains unbound inputs: $unbound_inputs_tower_0/IteratorGetNext,$unbound_inputs_cnn/conv_1/kernel/read,$unbound_inputs_cnn/conv_1/bias/read,$unbound_inputs_cnn/conv_2/kernel/read,$unbound_inputs_cnn/conv_2/bias/read,$unbound_inputs_linear_layer/linear_cnn/kernel/read,$unbound_inputs_linear_layer/linear_cnn/bias/read,$unbound_inputs_tower_0/seq_real_size_slice,$unbound_inputs_lstm/rnn/multi_rnn_cell/cell_0/lstm_cell_0/kernel/read,$unbound_inputs_lstm/rnn/multi_rnn_cell/cell_0/lstm_cell_0/bias/read,$unbound_inputs_lstm/rnn/multi_rnn_cell/cell_0/lstm_cell_0/projection/kernel/read,$unbound_inputs_lstm/rnn/multi_rnn_cell/cell_1/lstm_cell_1/kernel/read,$unbound_inputs_lstm/rnn/multi_rnn_cell/cell_1/lstm_cell_1/bias/read,$unbound_inputs_lstm/rnn/multi_rnn_cell/cell_1/lstm_cell_1/projection/kernel/read,$unbound_inputs_fully_connected/fc_1/kernel/read,$unbound_inputs_fully_connected/fc_1/bias/read,$unbound_inputs_fully_connected/fc_2/kernel/read,$unbound_inputs_fully_connected/fc_2/bias/read,$unbound_inputs_fully_connected/output_layer/kernel/read,$unbound_inputs_fully_connected/output_layer/bias/read. Must provide these inputs through input_map.\r\n", "@freitmi We see that you are using an older version of TF(1.x) which is out of the support window. We recommend you to migrate from TF v1.x to 2.x and upgrade to the latest versions(2.4 or later), please refer to this [migration](https://www.tensorflow.org/guide/migrate) document .I tried to replicate the issue and faced AttributeError, please find the [gist](https://colab.research.google.com/gist/sushreebarsa/2e0d9f62ff3fc5eb4b11e3e10ce67e39/10250.ipynb).Thanks!"]}, {"number": 10195, "title": "Use freeze_graph only with an input checkpoint", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.0-rc0\r\n\r\n`freeze_graph` method from `tensorflow.python.tools` should be able to work just with an input checkpoint, it needn't a graph definition from a protobuf file. Just restoring the metagraph and using the graph from the session lets you get rid of the graph def file.\r\n\r\nAlso, as you have provided from 1.2.0-rc0 a method to freeze from code without loading the files ([`freeze_graph_with_def_protos`](https://github.com/tensorflow/tensorflow/blob/v1.2.0-rc0/tensorflow/python/tools/freeze_graph.py#L58)), it should be able to work without a checkpoint but just with a session.\r\n\r\nThese will make freezing way simpler.", "comments": ["@petewarden As an author of freeze_graph, what are your thoughts on this feature request? See also https://github.com/tensorflow/tensorflow/commit/5407d8951263f8961a6d124b35bbbeb7762253a1.", "I like this feature. ", "> @petewarden As an author of freeze_graph, what are your thoughts on this feature request? See also 5407d89.\r\n\r\nyeah that was the commit I was referring to", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 9978, "title": "graph_editor.copy_with_input_replacements crashes for some orderings of inputs", "body": "Graph editor copy_with_input_replacements  visits nodes in order provided, and assumes that op referenced by \"op._original_op\" has already already been visited. When this assumption is false, it fails with KeyError inside transform.py\r\n\r\nReproducible case\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow.contrib.graph_editor as ge\r\n\r\nif __name__=='__main__':\r\n  params = tf.Variable(1, dtype=np.float32, name=\"params\")\r\n  temp = tf.reduce_sum(params, name=\"sum_temp\")\r\n  cost1 = tf.square(temp, name=\"cost1\")\r\n  gradients1 = tf.gradients([cost1], [params])\r\n  ops = tf.get_default_graph().get_operations()\r\n  ops = list(sorted(ops, key=lambda op: op.name))\r\n  copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops), {})\r\n```\r\nIt fails with following error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"graph_editor_test.py\", line 13, in <module>\r\n    copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops), {})\r\n  File \"/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 620, in copy_with_input_replacements\r\n    sgv, dst_graph, dst_scope, src_scope, reuse_dst_scope=reuse_dst_scope)\r\n  File \"/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 436, in __call__\r\n    self._copy_ops(info)\r\n  File \"/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 450, in _copy_ops\r\n    op_, op_outputs_ = self.transform_op_handler(info, op)\r\n  File \"/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 173, in copy_op_handler\r\n    original_op = info.transform_original_op_handler(info, op._original_op)\r\n  File \"/Users/yaroslav/anaconda/envs/memory/lib/python3.5/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 125, in transform_op_if_inside_handler\r\n    return info.transformed_ops[op]\r\nKeyError: <tf.Operation 'sum_temp' type=Sum>\r\n```\r\n\r\nA work-around is to clear `_original_op` entries for all ops\r\n\r\n```\r\ndef clear_original_ops(ops):\r\n  for op in ops:\r\n    op._original_op = None\r\n```\r\n\r\n@purpledog ", "comments": ["@sherrym can you comment or redirect to someone who can? Thanks!", "Frank Perbet. I am not sure if he has a github account.", "It's @purpledog . Simple fix would be just to remove anything to do with `_original_op`. That field hasn't been touched for 2 years, not sure if it's actually used by anything. I've been using graph editor recently and fixing things that prevent large subgraph copies, and\u00a0could aggregate all the fixes into PRs if there's interest in checking them in", "Yaroslav, your fixes would be very welcome.", "@yaroslavvb I'd tremendously appreciate a PR for the fixes as I've been having issues with _original_op as well (#9125)", "@poolio ps, as a temporary work-around, the hack I used was just to grep for \"original_op\" inside contrib.graph_editor and comment out lines that set original op", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 9918, "title": "preserving specific checkpoints", "body": "Savers automatically clean up checkpoints and that's lovely.  But there are special points during training that I want to be sure to save (e.g. transitioning from a pre-training phrase to full training), which I can't ensure with the current options (unless I just keep everything, by making ```max_to_keep``` & ```keep_checkpoint_every_n_hours``` huge).\r\n\r\nTwo possible approaches:\r\n\r\n1)  ```saver.save(..., preserve=True)```\r\nNever clean up this checkpoint.\r\n\r\n2)  ```max_to_keep``` is defined **per** ```save_path```\r\ni.e. Whenever I change the ```save_path``` argument to ```save()``` in the middle of the session, don't clean up the checkpoints with the previous ```save_path```.\r\n\r\nThis is related to #8658 (with a little book-keeping on the client, you could do #8658 yourself).", "comments": ["@sherrym ", "I would like to see this implemented. I simply want to retain a permanent checkpoint at the end of each epoch but since each epoch runs for a long time I also need at least one transient checkpoint retained within the current epoch so I can recover from a crash/server eviction efficiently. Saving every checkpoint is inefficient (especially as the \"delete checkpoint\" code is inaccessible so I can't cause the checkpoints I don't need to be deleted easily) and saving every N hours doesn't enable me to see how the model evolved at consistent epoch boundaries.", "Can we close this?"]}, {"number": 9917, "title": "Feature request: tf.nn.depthwise_conv2d_transpose", "body": "### System information\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.1.0-0-g1ec6ed5', '1.1.0')\r\n- **Bazel version (if compiling from source)**: 0.4.5- (@non-git)\r\n- **CUDA/cuDNN version**: 7.5/5.1\r\n- **GPU model and memory**: NVIDIA M40, 12GB\r\n\r\n### Describe the problem\r\nI would like to apply a `tf.nn.conv2d_transpose` operation to each channel of a feature image independently. There is no `tf.nn.depthwise_conv2d_transpose` operation.\r\n\r\nI tried using `tf.nn.depthwise_conv2d_native_backprop_input` however, when I try to optimize a function that involves one of these operations, it results in an error because there is no gradient operation defined:\r\n\r\n```\r\nLookupError: No gradient defined for operation 'DepthwiseConv2dNativeBackpropInput_1' (op type: DepthwiseConv2dNativeBackpropInput)\r\n```\r\n\r\nThis is related to https://github.com/tensorflow/tensorflow/issues/7934\r\n\r\nIt is possible to achieve this functionality using `conv2d_transpose` by constructing a large filter with many coefficients set to zero. However, it is relatively inefficient, especially for a large number of channels.", "comments": ["thanks  for  your  share", "Why would you like to do a transpose operation to each channel of a feature image independently?", "```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import nn_ops\r\nfrom tensorflow.python.ops import array_ops\r\n\r\nNUM_ITERATIONS = 1000\r\n\r\n@ops.RegisterGradient(\"DepthwiseConv2dNativeBackpropInput\")\r\ndef _DepthwiseConv2DNativeBackpropInputGrad(op, grad):\r\n  \"\"\"The derivatives for depth-wise deconvolution.\r\n  Args:\r\n    op: the depth-wise deconvolution op.\r\n    grad: the tensor representing the gradient w.r.t. the output\r\n  Returns:\r\n    the gradients w.r.t. the input and the filter\r\n  \"\"\"\r\n  return [None,\r\n          nn_ops.depthwise_conv2d_native_backprop_filter(\r\n              grad,\r\n              array_ops.shape(op.inputs[1]),\r\n              op.inputs[2],\r\n              op.get_attr(\"strides\"),\r\n              op.get_attr(\"padding\"),\r\n              data_format=op.get_attr(\"data_format\")),\r\n          nn_ops.depthwise_conv2d_native(\r\n              grad,\r\n              op.inputs[1],\r\n              op.get_attr(\"strides\"),\r\n              op.get_attr(\"padding\"),\r\n              data_format=op.get_attr(\"data_format\"))]\r\n\r\nfilter1 = tf.Variable(np.ones((3, 3, 2, 1), dtype=np.float32))\r\ndeconv1 = tf.nn.depthwise_conv2d_native_backprop_input(\r\n  [2, 30, 30, 2],\r\n  filter1,\r\n  tf.ones([2, 28, 28, 2]),\r\n  [1, 1, 1, 1],\r\n  \"VALID\")\r\nfilter2 = tf.Variable(np.ones((3, 3, 2, 1), dtype=np.float32))\r\ndeconv2 = tf.nn.depthwise_conv2d_native_backprop_input(\r\n  [2, 32, 32, 2],\r\n  filter2,\r\n  deconv1,\r\n  [1, 1, 1, 1],\r\n  \"VALID\")\r\nloss = tf.reduce_mean(tf.square(deconv2 - 1))\r\ntrain_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run(filter1))\r\nfor i in range(NUM_ITERATIONS):\r\n  print(sess.run([train_op, loss])[1], end=\"\\r\")\r\nprint(sess.run(filter1))\r\n```\r\n\r\nI have attempted to implement the gradient for `tf.nn.depthwise_conv2d_native_backprop_input` in the code above. If someone could tell me what is missing for a pull request (test cases, wrapper functions etc.), then I would gladly contribute.", "@rdinse I think you could create the PR as an open PR will likely generate more discussion and attention.", "+1 for this feature request! AFAIK this is much needed for an implementation of the recent [GradNorm paper](https://arxiv.org/abs/1711.02257).\r\n\r\n@rdinse [Here are the contributing guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md), does that help? :)\r\n\r\nEDIT: fixed a link", "@rdinse Are you still working on this issue?", "I am also interested in this issue, and I think I have some idea how this can be done. If you read about how the depthwise_conv2d is implemented, you will find find that \"_depthwise_conv2d applies a different filter to each input channel (expanding from 1 channel to channel\\_multiplier channels for each), then concatenates the results together._\" -- therefore, depthwise_conv2d_transpose could theoretically be implemented via N regular conv2d_transpose's (where N is the number of input channels to the depthwise_conv2d) applied to the corresponding selection of the channels from the encoding, and using the corresponding selection from the filter for each, and then concatenating the results. You'd just need to know how the original depthwise_conv2d interleaves the input channels to the output (1,1,2,2,3,3 or 1,2,3,1,2,3), so that you know how to select the correct parts of the encoding and filter for the N regular \"deconvolutions\". I could perhaps attempt to implement this, I will update if I do.", "Here is an implementation for you. I don't have time to submit pull request with tests etc, but this works for me. Although, it's slower than when I use the regular conv2d and conv2d_transpose with the same number of input and output channels (and larger, full filters). If implemented in C++ (or in some other smarter way), it should be in fact faster. Anyway, at least it works. P.S. I've also tried using `tf.while_loop` when the number of channels is only known at run-time, and it also works, but I don't think this extra complication is necessary, since the number of channels is usually known at \"compile time\" (for TensorFlow graph).\r\n\r\n```\r\ndef depthwise_conv2d_transpose(\r\n  value,\r\n  filter,\r\n  strides,\r\n  output_shape,\r\n  data_format='NHWC',\r\n  padding='SAME',\r\n  name='depthwise_conv2d_transpose'):\r\n\r\n  if data_format == 'NHWC':\r\n    channel_axis = 3\r\n    channel_output_shape = [\r\n      output_shape[0],\r\n      output_shape[1],\r\n      output_shape[2],\r\n      1]\r\n  elif data_format == 'NCHW':\r\n    channel_axis = 1\r\n    channel_output_shape = [\r\n      output_shape[0],\r\n      1,\r\n      output_shape[2],\r\n      output_shape[3]]\r\n  else:\r\n    raise ValueError(\"Unsupported data_format: {}\".format(data_format))\r\n\r\n  filter_shape = filter.get_shape().as_list()\r\n  if len(filter_shape) != 4:\r\n    raise ValueError(\"Expected filter of dim 4, got filter shape: {}\".format(filter_shape))\r\n\r\n  conv_in_channels = filter_shape[2]\r\n  if conv_in_channels is None:\r\n    raise ValueError(\"Number of convolution input channels in filter (dim 2) \" +\r\n                     \"must be known at compile time, got: {}\".format(conv_in_channels))\r\n\r\n  channel_multiplier = filter_shape[3]\r\n  if channel_multiplier is None:\r\n    raise ValueError(\"Channel multiplier in filter (dim 3) \" +\r\n                     \"must be known at compile time, got: {}\".format(channel_multiplier))\r\n\r\n  # collect outputs for all input channels\r\n  c_outputs = []\r\n\r\n  # create all operations in scope with name\r\n  with tf.variable_scope(name):\r\n\r\n    # regular conv2d_transpose for each input channel\r\n    for c in range(conv_in_channels):\r\n\r\n      # grab subset of value\r\n      c_value = tf.gather(value,\r\n        indices=list(range(\r\n          channel_multiplier*c,\r\n          channel_multiplier*c+channel_multiplier)),\r\n        axis=channel_axis)\r\n\r\n      # grab subset of filter\r\n      c_filter = tf.gather(filter,\r\n        indices=[c],\r\n        axis=2)\r\n\r\n      # run for this input channel\r\n      c_outputs.append(\r\n        tf.nn.conv2d_transpose(\r\n          value=c_value,\r\n          filter=c_filter,\r\n          strides=strides,\r\n          output_shape=channel_output_shape,\r\n          data_format=data_format,\r\n          padding=padding,\r\n          name='channel{}'.format(c)))\r\n\r\n    # concatenate outputs within variable scope\r\n    output = tf.concat(c_outputs, axis=channel_axis, name='concat')\r\n\r\n  return output\r\n```", "Hey, I'd like to work on this. Looks it still hasn't been done. Is that alright?"]}, {"number": 9901, "title": "tf.gradients runtime scales suboptimally with size of the graph", "body": "`tf.gradients` can be inefficient on large graphs and it runtime increases with size of the graph, even when the amount of work it needs to do is constant. This inefficiency is apparent when trying to differentiate small parts of large graph many times.\r\n\r\nDiscovered when trying to scale to 8 GPUs using data parallelism using 8 identical copies of model -- time spent inside gradients grows for each new replica even though replicas are identical and independent. We are calling `tf.gradients` many times (calling tf.gradients on parts of model in order to do memory saving gradients [trick](https://arxiv.org/abs/1604.06174)), our largest models spend >2 hours inside `tf.gradients`.\r\n\r\nI've profiled the runs and saw that most of the time is spent inside\r\n\r\n`_MarkReachedOps(from_ops, reached_ops)` inside `gradients_impl.py`\r\n\r\nIt's called as follows\r\n\r\n```\r\n  reached_ops = [False] * (graph._last_id + 1)\r\n  for op in to_ops:\r\n    reached_ops[op._id] = True\r\n```\r\nYou can see that it's using Python list initialized with the size of the entire graph so this initialization step would grow with size of the graph.\r\n\r\n![screenshot 2017-05-14 15 35 58](https://cloud.githubusercontent.com/assets/23068/26038340/102545da-38bb-11e7-873b-b57bc628ae7c.png)\r\n\r\n\r\nProfile of the `_MarkReachedOps` when calling when calling tf gradients 560 times, with each gradient call adding 35 nodes on average, and total size of the graph being 200k nodes\r\n\r\n```\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n   101                                           @profile\r\n   102                                           def _MarkReachedOps(from_ops, reached_ops):\r\n   103                                             \"\"\"Mark all ops reached from \"from_ops\".\r\n   104                                           \r\n   105                                             Args:\r\n   106                                               from_ops: list of Operations.\r\n   107                                               reached_ops: list of booleans, indexed by operation id.\r\n   108                                             \"\"\"\r\n   109       568         1967      3.5      0.0    queue = collections.deque()\r\n   110       568         4835      8.5      0.0    queue.extend(from_ops)\r\n   111  39912648     14661885      0.4      8.4    while queue:\r\n   112  39912080     17079278      0.4      9.8      op = queue.popleft()\r\n   113  39912080     41203709      1.0     23.7      if not reached_ops[op._id]:\r\n   114  28997056     21267924      0.7     12.2        reached_ops[op._id] = True\r\n   115  58483932     42196549      0.7     24.2        for output in op.outputs:\r\n   116  29486876     37595214      1.3     21.6          queue.extend(output.consumers())\r\n```\r\n\r\nPossible solutions could be a more efficient implementation of `_PendingCount`, or a different algorithm for `tf.gradients` which is more efficient for large graphs", "comments": ["Thank you, Yaroslav!", "@prb12, any thoughts about this? (:", "@prb12 @tfboyd any ideas?", "Could you provide the code that you used? I would like to reproduce this performance issue and see how it does in my machine.\r\n\r\nThank you!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "This should be deprecated if TF is switching to C++ gradients", "@RickBoss The instruction to reproduce this slowness is here: https://medium.com/@yaroslavvb/testing-memory-saving-on-v100-8aa716bbdf00\r\n\r\nThe time grows super-linearly with graph size due to tf.gradients slowness\r\n<img width=\"663\" alt=\"screenshot 2018-01-15 13 43 46\" src=\"https://user-images.githubusercontent.com/23068/34962860-66c5a50a-f9fa-11e7-85cf-51ac4f13e73e.png\">\r\n", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I'm a member of TensorFlow organization, but I'm also the person who filed the original issue", "Marking as contributions welcome; @yaroslavvb has already diagnosed the problem and listed potential solutions.", "@yaroslavvb Alfred / tensorflowbutler is new on the job and is slowly learning.  Your not really an edge case is kind of funny.  I will let the author know about the author being an org member, which is likely not super uncommon  :-)  ", "@yaroslavvb besides time to transform the graph, did you also record the number of nodes of the transformed graph?  does it also grow super-linearly with respect to the number of nodes in the original graph?", "Is there any update/perspective on this issue (which exactly hurts my current production level use case)?", "This is a long shot, but I am going to add Taylor who is has been working on startup time related to graph creation.  I might be totally sending this the wrong direction but after 2 years, it cannot really hurt.\r\n@robieta ", "This isn't so much what I've been looking into, but I can offer one bit of advice for the specific case that a graph has a high degree of self similarity (either because it's a data parallel model or because it's a stack of structured blocks): `tf.function` can help a lot. If you consider the following bit of code:\r\n\r\n```\r\nimport timeit\r\nimport tensorflow as tf\r\n\r\ndef some_computation(x):\r\n  \"\"\"Doesn't really matter what. Just add some nodes to the graph.\"\"\"\r\n  y = x + 1\r\n  z = x ** 2\r\n  q = x + y + z\r\n  return q\r\n\r\ndef define_many_blocks(x, n_blocks):\r\n  x = tf.identity(x)\r\n  for _ in range(n_blocks):\r\n    x = some_computation(x)\r\n  return x\r\n\r\nsome_computation_fn = tf.function(some_computation, autograph=False)\r\ndefine_many_blocks_fn = tf.function(define_many_blocks, autograph=False)\r\n\r\n@tf.function(autograph=False)\r\ndef define_many_blocks_very_functional(x, n_blocks):\r\n  x = tf.identity(x)\r\n  for _ in range(n_blocks):\r\n    x = some_computation_fn(x)\r\n  return x\r\n\r\n\r\nnum_devices = 8\r\n\r\nfor use_tf_fn in [\"No\", \"top_level\", \"turtles all the way down\"]:\r\n  print(\"Use tf.function: {}\".format(use_tf_fn))\r\n  for num_blocks in [50, 100, 500]:\r\n    with tf.Graph().as_default():\r\n      inp = tf.placeholder(dtype=tf.float32, shape=())\r\n\r\n      if use_tf_fn == \"turtles all the way down\":\r\n        fn = define_many_blocks_very_functional\r\n      elif use_tf_fn == \"top_level\":\r\n        fn = define_many_blocks_fn\r\n      else:\r\n        fn = define_many_blocks\r\n\r\n      outputs = []\r\n      for i in range(num_devices):\r\n        with tf.device(\"CPU:{}\".format(i)):\r\n          outputs.append(fn(inp, num_blocks))\r\n\r\n      output = tf.math.add_n(outputs)\r\n\r\n      start_time = timeit.default_timer()\r\n      grads = tf.gradients(output, inp)\r\n      time_per_block = (timeit.default_timer() - start_time) / num_blocks\r\n\r\n      block_str = \"({} blocks):\".format(num_blocks).ljust(16)\r\n      print(\"Grad compute time {}{:.2f} ms / block\".format(block_str, time_per_block  * 1000))\r\n  print()\r\n```\r\n\r\nThe result is:\r\n```\r\nUse tf.function: No\r\nGrad compute time (50 blocks):    97.11 ms / block\r\nGrad compute time (100 blocks):   101.93 ms / block\r\nGrad compute time (500 blocks):   100.22 ms / block\r\n\r\nUse tf.function: top_level\r\nGrad compute time (50 blocks):    19.75 ms / block\r\nGrad compute time (100 blocks):   18.98 ms / block\r\nGrad compute time (500 blocks):   25.21 ms / block\r\n\r\nUse tf.function: turtles all the way down\r\nGrad compute time (50 blocks):    6.98 ms / block\r\nGrad compute time (100 blocks):   6.51 ms / block\r\nGrad compute time (500 blocks):   6.82 ms / block\r\n```\r\n\r\nThe reason that this helps is that TensorFlow will happily chain rule functions. Obviously it would be better to improve efficiency in the general case, but hopefully this helps somewhat in the interim. "]}, {"number": 9837, "title": "Optimizers in the C++ API", "body": "There is currently no Optimizer in the [C++ API](https://www.tensorflow.org/api_docs/cc/) compared as the ones that we can find in the [Python API](https://www.tensorflow.org/api_docs/python/).\r\n\r\nIt means, when using only the C++ API that we have to manually collect the gradients and apply them.\r\n\r\nDon't you think it should be a convenient add? Retrieving the gradients and applying them is kind of hard and error prone. If yes, I could work on it and create an Optimizer + GradientDescentOptimizer in the C++ API.\r\n\r\nI maybe missed something.", "comments": ["I would recommend working with us to define the interface and design that looks like [gradients](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients). That would be a nice contribution!\r\n\r\nIdeally, we would have  a check that things don't go out of sync with python.", "Yes, the C++ (and other language) APIs lack many of the higher level Python conveniences right now.\r\n(Side note: The gradient registry for all operations in C++ is not as comprehensive as Python, so there is work to be done there - for details see the link that @drpngx pointed to).\r\n\r\nThe TensorFlow team doesn't have the cycles to pursue this ourselves right now, but if you'd like to give it a shot we'd be happy to discuss any interfaces/design that you propose.\r\n\r\nFYI @suharshs @skye @josh11b ", "Ok. I'll see how I can do things and I'll make proposals here. I'll surely contribute to the C++ gradients as you pointed it out.", "My observations until now.\r\n\r\nIn python the Optimizer class provides a minimize method that calls compute_gradient and apply_gradient. compute_gradient adds the gradient ops to the graph, apply_gradient adds the apply ops to the graph and return an operation that applies the gradients.\r\n\r\nThe mains files are:\r\n* optimizer.py\r\n* gradient_descent.py\r\n* gradients_impl.py\r\n\r\nIn C++ some gradient operations are implemented. There is also a SymbolicGradientBuilder that applies the gradient operations to the graph. The missing part is the applying of the gradients.\r\n\r\nThe steps would be:\r\n* Create the Optimizer and GradientDescentOptimizer class\r\n* Use the SymbolicGradientBuilder in the ComputeGradient method\r\n* Add apply operations to the graph and return an operation in the ApplyGradient method\r\n* Combine both methods in the minimize method\r\n* Unit tests\r\n\r\nWhat do you think?", "Hi @theflofly ! Your steps sounds reasonable in general. I would like to work on that but need guidance to start. Please contact if you are still interested in.", "Hi @AhmetCanSolak, I am working on it and I made good progress even if it is not as easy as it sounds. \r\n\r\nI don't see a way to split the task unfortunately. Especially at this point. Once the optimizer part is added, there will be plenty of work. Namely adding all the Optimizers (adam, adagrad, momentum, etc.). If you want I can ping you at this time and we'll share the work.\r\n\r\nIn the meantime, there is always the gradient operations that need to be ported.", "Great. It is better idea to ping me when sharing tasks possible. Meanwhile I better check again the contribution guidelines since I have not contributed to TF yet.", "@theflofly Do you have any ideas about bringing automatic differentiation (gradients) to C++ API? \r\nalso, If any help needed, you can ping me:)", "@Moriadry AD is already here.\r\n\r\n```\r\nstd::vector<Output> grad_outputs;\r\nTF_ASSERT_OK( AddSymbolicGradients(scope, {z}, {x, y}, {dz}, &grad_outputs));\r\n```\r\nWill add the gradient operations to the graph. You can find more example here. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/framework/gradients_test.cc\r\n\r\nThe goal of this issue is mainly about the update part. If you want to add some gradients you can follow these [guidelines](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients).\r\n\r\nDoes this answer?", "@theflofly ok, your answer looks like making sense, I will try to figure that out. thank you!", "Hello, this is a message to keep you updated. I think that I have a working version, I'll soon share the code. I still have a TODO list of minor things to do.", "@theflofly happy to hear that, looking forward to your code.", "I opened a pull request. Feel free to share your thoughts.\r\n\r\nIt basically adds this kind of operation:\r\n```auto train = GradientDescentOptimizer(0.01).Minimize(scope, {loss});```\r\n\r\nYou can see a detailed example here: https://github.com/theflofly/tensorflow/blob/optimizer-cc-issue-9837/tensorflow/cc/training/optimizer_test.cc in the second function.\r\n\r\nI put some TODO(theflofly) listing some improvements. I will add other optimizers and some gradients operations, then I'll work on these TODOs. \r\n\r\nWe can easily share the work, so if someone wants to work either on the TODOs, gradients or optimizers please create an issue and ping me so that we don't work on the same feature.", "Presumably to achieve this you need to have created a function similar to tf.gradients() right? Is there any plan on exposing this too in C++ ?", "@fferroni Indeed. The function is already existing and was created by someone at Google. In this issue I basically wrap existing stuffs into a cleaner API similar to the python.\r\n\r\nI already explained above but basically you can use:\r\n\r\n```\r\nauto x = Const(scope_, {{1.0f, 2.0f}, {3.0f, 4.0f}, {5.0f, 6.0f}});\r\nauto y = Const(scope_, {{1.0f}, {2.0f}, {3.0f}});\r\n\r\nauto w1 = Variable(scope_, {2, 1}, DT_FLOAT);\r\nauto assign_w1 = Assign(scope_, w1, Const(scope_, {{0.1f}, {0.2f}}));\r\n\r\nauto layer_1 = Tanh(scope_, MatMul(scope_, x, w1));\r\n\r\nstd::vector<Output> grad_outputs;\r\nTF_CHECK_OK(AddSymbolicGradients(scope_, {layer_1}, {assign_w1}, &grad_outputs));\r\n\r\nauto apply = ApplyGradientDescent(\r\n      scope_, {w1},\r\n      Cast(scope_, 0,01,  static_cast<DataType>(w1.type() - 100)),\r\n      {grad_outputs[0]});\r\n\r\nClientSession session(scope_);\r\nTF_CHECK_OK(session.Run({assign_w1}, nullptr));\r\nTF_CHECK_OK(session.Run({apply}, nullptr));\r\n\r\n```\r\nThe equivalent to gradients from python is AddSymbolicGradients. Not sure the above code is working as I can't test it, but you get the idea on how to put things together.\r\n\r\nKeep in mind that you first have to call session run on assign nodes, also the AddSymbolicGradients will not work if you pass the var w1 and not the assign node whereas to apply gradient descent you have to pass the var node and not the assign node. I edited AddSymbolicGradients on my PR to solve this issue.\r\n\r\nWith more than one var the above code increases in complexity. That's why I replaced it by: `auto train = GradientDescentOptimizer(0.01).Minimize(scope, {loss});`.", "@asimshankar @skye ", "FYI this will certainly not be merged, you can see the answer of @asimshankar in https://github.com/tensorflow/tensorflow/pull/11377.", "So how can it be possible to add gradients manually now to a graph?\r\n\r\nI can run and train a linear regression graph when I compute the gradients myself.\r\n\r\n```\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/cc/framework/gradients.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\nint main()\r\n{\r\n  using namespace tensorflow;\r\n  using namespace tensorflow::ops;\r\n  Scope root = Scope::NewRootScope();\r\n  \r\n  auto W = Variable(root.WithOpName(\"W\"), {3,1}, DT_DOUBLE);\r\n\r\n  auto x = Placeholder(root.WithOpName(\"x\"), DT_DOUBLE);\r\n  auto y = Placeholder(root.WithOpName(\"y\"), DT_DOUBLE);\r\n\r\n  auto m = MatMul(root, x, W);\r\n  auto dd = Subtract(root, y, m);\r\n  \r\n //compute loss\r\n  auto loss = MatMul(root, MatMul(root, dd, dd, MatMul::TransposeA(true)), {0.5}); \r\n  double learning_rate = 0.1;\r\n \r\n\r\n  std::vector<Output> grad_outputs;\r\n  TF_CHECK_OK(AddSymbolicGradients(root, {loss}, {m}, &grad_outputs));\r\n  //Compute gradients with formula\r\n  //  auto grad_W = Subtract(root, MatMul(root, MatMul(root, x, x, MatMul::TransposeA(true)), W), MatMul(root, x, y,  MatMul::TransposeA(true)));\r\n\r\n  auto apply_grad_W = ApplyGradientDescent(root, W, learning_rate,  grad_outputs[0]);\r\n\r\n  //Initialize variables\r\n  auto init_W = Assign(root, W, {{1.0},{1.0},{1.0}});\r\n\r\n  std::vector<Tensor> outputs;\r\n  ClientSession session(root);\r\n\r\n  //Run variable initializers\r\n  session.Run({init_W}, &outputs);\r\n\r\n  for(unsigned int i=0;i<200;++i)\r\n  {\r\n    TF_CHECK_OK(session.Run( { {x,{{1.0,-1.0,3.0}, {1.0,2.0,1.0}, {1.0,-2.0,-2.0}, {1.0,0.0,2.0}}}, {y,{{14.0}, {15.0}, {-9.0}, {13.0}}} } , {loss, apply_grad_W}, &outputs));\r\n    std::cout << std::string(\"loss: \") << outputs[0].scalar<double>() << std::endl << std::string(\"weights: \")<< outputs[1].matrix<double>() << std::endl;\r\n  }\r\n  return 0;\r\n}\r\n\r\n```\r\n\r\nBut when I try to get the gradients with AddSymbolicGradients and despite MatMulGrad been already defined in math_grad.cc, I got this error message:\r\n\r\n\"Not found: No gradient defined for op: MatMul. Please see https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md for instructions on how to add C++ gradients.\"\r\n\r\n", "As the core is in C++ I guess:\r\n* The PR should be merged\r\n* tensorflow/cc/gradients should be moved in tensorflow/core/\r\n* All the python conveniences should be added to tensorflow/core\r\n* The non C++ API should all become bindings to the C++ core code\r\n\r\nThe problem is that there is a lot of python code to be rewritten (but I would be happy to work on that with the community) but more importantly, the data scientist + researchers are well versed in python, that's why we have so much more features in python.\r\n\r\nThe C++ is not sexy, less easy and the use of bazel means no proper IDE support for now. Debugging is hard, especially on OS X because of a bazel bug that don't add symbols to the binary generated. I am using VS Code personally, there is a project called tulsi that convert a bazel project to Xcode but it does not work for TF because it does not handle \"cc_library\" if I remember correctly.\r\n\r\nI am waiting for the @asimshankar answer to more insights. But clearly one implementation of the gradient descent and maybe other optimizers in the future for each language is clearly a bad design choice... He talked about a C implementation, but the core is in C++ so I don't understand why.\r\n\r\nLet's wait. :)\r\n\r\nFor your problem, are you using the last version of master ? Because me it gives me: `Not found: No gradient defined for op: Sub.`. But the MatMul is not the problem.\r\n\r\nThere is currently no gradient for the binary maths operations, so you can't do basic stuff like add, sub, ... I was working on them because I also need them. But I'll wait because duplicating the gradients operations for each languages is also a bad idea.", "Definetely I'd like to get more insights as well. I'd like to see a working example of training a basic neural net or convnet with C++ API. If basic gradient ops such as Subtract are not supported yet, I guess there is still a long way to go until that. I see SubGrad is already defined in /tensorflow/core/ops/math_grad.cc, so I don't know why it isn't available from the C++ API yet.", "Even if the gradient is defined into the core, it has to be put in the gradient registry in order to be used by the  AddSymbolicGradients method. Basically register grad has to be called, and it is not the case.", "Some news about this issue, I am currently migrating the gradient ops from Python to C++ and I'll work on the Optimizers when enough gradients have been migrated.", "@theflofly, thanks for the work you've done so far with gradients and optimizers. GradientDescentOptimizer was very useful in my project as there is no optimizer implementation in current tensorflow c++ SDK. In the future we would like to have AdamOptimizer in our software as it has proven to be more robust in our case. What are your plans on implementing that one?", "@GedisJ For now I am working on migrating the gradients from Python to C++ because one Operation without gradient in the graph means an untrainable network.\r\nBecause the node `\u200bApplyAdam` already exists I think that adding Adam is not that hard. I'll take a look and keep you updated.", "@theflofly this is true about gradients, I had to add some simple gradient operations myself, before they were introduced in 1.4 release.\r\nregarding Adam the main thing I guess is adding slots implementation to base Optimizer from what I've seen in Python code, because in C++ this seems to be missing too.", "Reviving this old thread in connection with #11377.\r\nIs there any plan / road map publicly available to keep track and contribute to this? ", "I would also be interested.\r\n\r\nThe work I began to do was in the end discarded and I never heard back from someone at Google about working on a design...", "Thanks for reviving this thread.\r\n\r\nThe sad news is that someone from Google hasn't worked on a design.\r\nThe good news is that we've gotten better in terms of establishing a process so that design changes can be made without constantly waiting on someone at Google to drive it!\r\n\r\nIf there is interest in pushing this, then it would be great for the person interested to drive the [TensorFlow RFC process](https://github.com/tensorflow/community/blob/master/governance/TF-RFCs.md), which involves writing a design proposal as a first step. This will allow the community and maintainers to see the full picture and make sure it's headed in the right direction, instead of having the design evolve through a series of one-off PRs.\r\n\r\nI personally feel the ideal outcome would be that we'd have a path so that individual optimizer classes implemented in Python can be _migrated_ to C++ so that the Python class becomes a trivial wrapper over the functionality exposed by the C API instead of having to duplicate logic in two languages. The work earlier this year to get the Python graph building libraries to be wrappers over the C API for graph construction should make this much easier than it would have been last year.\r\n\r\nSo, long story short: No plan/roadmap available yet, but contributions are welcome to make them! :)\r\n\r\nFYI @ewilderj ", "Thanks for your answer @asimshankar, in the case of a RFC process do you accept being the RFC sponsor?", "cc @karmel ", "@theflofly : I would have loved to have been, but I've since moved to other projects (and am no longer involved with TensorFlow on a daily basis) and am thus not a good person for this at the moment.\r\n\r\n@ewilderj / @karmel should be able to find an appropriate sponsor.", "Thanks @theflofly for keeping on us about this. If you put together an RFC for review, I can help as the sponsor to find appropriate reviewers, etc. (CC @alextp , @skye who might be good reviewers here)", "Thanks @asimshankar, @karmel.\r\n\r\nI will work on something then, but bear with me I am quite busy at the moment.", "Hi @theflofly , How is work doing? We also need the GradientDescentOptimizer in our project, but I didn't find it in the TF 2.0, hope your work merge into the TF."]}, {"number": 9705, "title": "atrous_conv2d does not support NCHW format", "body": "Any plans to support NCHW format for atrous_conv2d?  As per the performance guidelines on TensorFlow website,  ops using NCHW format is faster than NHWC format for GPUs.", "comments": ["That seems like a fine contribution to have if you're up for it.\r\n\r\n@gpapan @martinwicke ", "@drpngx Hi, I have check it in r1.3 python API, and find it haven't support NCHW format. I'm interesting in contrib it. I'm familiar with dilated/atrous conv use and principle, but I haven't contrib before in tensorflow. Is it a good issue for beginner contributor ?", "@martinwicke any suggestion?"]}, {"number": 9664, "title": "Run half-precision models on Android", "body": "Is it possible to run half-precision (float16) graphs on Android (arm64-v8a/AArch64 supports half-precision)? If so, what would be the approach to do that? Trying to run a float16 graph gives an exception saying that only INT32 and Float32 ops are supported.\r\n\r\nThank you in advance,", "comments": ["@andrewharp do you know how far we are from that?", "@petewarden can correct me if I'm off base here, but I don't believe float16 support is something we're currently targeting on mobile.\r\n\r\nMost of our attention on mobile is focused on quantized 8bit support, which should give you even better size gains and possibly performance as well. Support for quantizing graphs can be found in the [transform_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) tool.", "@andrewharp thank you for the feedback. I am familiar with the 8bit quantization tool. Unfortunately, it results in a major drop in performance in my case. Probably because the network uses a custom activation (not ReLU, but something similar to ELU) while the quantization tool is (I believe) optimized for the ReLU activation. Also, I would like to use a higher precision than 8-bit for my problem, hence my interest in the half-precision support.\r\nBut since Eigen supports half-precision, I believe half-precision support would be relatively easier to integrate than 8bit quantization since it does not require any fancy op mapping. ", "@Androbin Thank you for the feedback but I don't use batch normalization. I think the main problem with the quantization tool is that the activation should be ReLU (the rounding option works fine though). ", "@hicham-eyeem As this is an issue involving a custom op we don't really have the bandwidth to investigate right now, but if you want to propose a general solution (for the quantization approach) in a PR we'd be happy to take a look.", "(Triaging issues.) Since @andrewharp from the TF Mobile team is willing to take a look at PRs, I'm marking this as contributions welcome.", "@andrewharp thank you, I hope I will have time to take a look at it. I remember I have also tried to decompose ELU as a sum of ReLU and masked exp(input) - 1 , still produced the same results, so I think the problem comes from the exp function on quantized values. ", "I got the same problem. Is there any solution?", "Hi @hicham-eyeem ! Does above [PR](https://github.com/tensorflow/tensorflow/pull/53274) resolve this issue?"]}, {"number": 9661, "title": "[feature] Support Cross Compiling with tfcompile", "body": "Tensorflow (using XLA) is able to AOT compile a graph using `tfcompile`. There does not seem to be a way to, or it it not documented, cross compile the graph (ie compile on OS X for deployment on iOS). (Related [SO](http://stackoverflow.com/questions/43508105/using-tfcompile-to-aot-compile-tensorflow-graph-for-ios) question).\r\n\r\nI suggest adding a means of performing this cross compilation.", "comments": ["I don't think anyone is currently working on this, but maybe @tatatodd can correct me. I'm gonna mark as contributions welcome for now.", "Can you get what you want by changing the --target_triple option handed to tfcompile?  I don't have either target to see if the result works here...but from Linux, I can do:\r\n--target_triple=thumbv7-apple-macosx10.6.7\r\nand\r\n--target_triple=thumbv7-apple-ios\r\nwhich creates a Mach-O object (cross compiling from a Linux host to ios/macosx targets).\r\nOr do you mean something else?", "I can confirm that doing what @petecoup suggests does work. I'm able to cross-compile from linux amd64 targetting rpi3 hardware (though, I still lack proper tooling to set the `--target_triple=` properly, but changing by hand works fine so far).", "@petecoup @lissyx In which file do we need to make changes, tfcompile.bzl or tfcompile_main.cc \r\n\r\nI see the line ` \" --target_triple=\" + target_llvm_triple() +` in tfcompile.bzl     and      `flags.target_triple = \"x86_64-pc-linux\";` in tfcompile_main.cc\r\n\r\nI want to compile for both android and ios\r\n\r\nFor ios I tried first tried, just replacing in tfcompile_main.cc  with `flags.target_triple = \"arm64-apple-ios\";`. The bazel build completed successfully but when I include the .h and .o in ios app, it says .o is not built for arm64.\r\n\r\nNext, in addition to changes in tfcompile_main.cc, when I also made the change in tfcompile.bzl by replacing the line with ` \" --target_triple=\" + \"arm64-apple-ios\" +`, the bazel build itself failed with error \r\n\r\nERROR: /tensorflow/test_graph_build/BUILD:4:1: Linking of rule '//test_graph_build:test_graph_tfmatmul' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored /usr/bin/gcc -shared -o bazel-out/local-opt/bin/test_graph_build/libtest_graph_tfmatmul.so '-fuse-ld=gold' ... (remaining 7 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n/usr/bin/ld.gold: error: bazel-out/local-opt/genfiles/test_graph_build/test_graph_tfmatmul.o:1:1: invalid character\r\ncollect2: error: ld returned 1 exit status\r\nTarget //test_graph_build:test_graph_tfmatmul failed to build", "@ruppeshnalwaya1993 This is what I have in our code, inside the `tf_library()` section:\r\n```\r\ntfcompile_flags = select({\r\n        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\r\n--target_features=\"+sse4.1\" --target_features=\"+sse4.2\" --target_features=\"+avx\" --target_features=\"+avx2\" --target_features=\"+fma\"',\r\n        \"//conditions:default\": str('')\r\n    }),\r\n```\r\n\r\nSo basically your `BUILD` file would contain:\r\n```\r\n$ cat native_client/BUILD \r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\",\r\n     \"tf_library\")\r\n\r\ntf_library(\r\n    name = \"project_model\",\r\n    cpp_class = \"class::method\",\r\n    graph = \"model.pb\",\r\n    config = \"tfcompile.config.pbtxt\",\r\n    tfcompile_flags = select({\r\n        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\r\n        \"//conditions:default\": str('')\r\n    }),\r\n)\r\n\r\n```", "@lissyx Thanks for your reply! I want to build for iOS and not raspberry pie. Nevertheless, to make sure I understand the process correctly, I tried to build using the changes that you suggested. One thing I am not sure of is given \r\n```\r\ntfcompile_flags = select({\r\n        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\r\n--target_features=\"+sse4.1\" --target_features=\"+sse4.2\" --target_features=\"+avx\" --target_features=\"+avx2\" --target_features=\"+fma\"',\r\n        \"//conditions:default\": str('')\r\n    }),\r\n```\r\nhow do specify in the bazel build command in terminal to build for //tensorflow:rpi3 and not //conditions:default or anything else?\r\n\r\nSince I was unsure how to specify the target in the terminal command, I specified the default one to be also raspberry pie like this:\r\n```\r\ntfcompile_flags = select({\r\n        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\r\n--target_features=\"+sse4.1\" --target_features=\"+sse4.2\" --target_features=\"+avx\" --target_features=\"+avx2\" --target_features=\"+fma\"',\r\n        \"//conditions:default\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"')\r\n    }),\r\n```\r\nI tried building the test graph mentioned at [tensorflow aot page](https://www.tensorflow.org/performance/xla/tfcompile) by adding tfcompile_flags as written above in the BUILD file and running the command `bazel build test_graph_tfmatmul`. BUILD file is as follows kept in a directory `//test_graph_build`.\r\n```\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\ntf_library(\r\n    name = \"test_graph_tfmatmul\",\r\n    cpp_class = \"foo::bar::MatMulComp\",\r\n    graph = \"test_graph_tfmatmul.pb\",\r\n    config = \"test_graph_tfmatmul.config.pbtxt\",\r\n    tfcompile_flags = select({\r\n        \"//tensorflow:ios\": str('--target_triple=\"arm64-apple-ios\"'),\r\n        \"//conditions:default\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"')\r\n    }),\r\n)\r\n```\r\n\r\nBut after a lot of time compiling, this does throws an error as follows:\r\n```\r\nINFO: Reading 'startup' options from /etc/bazel.bazelrc: --batch\r\nWARNING: /tensorflow/tensorflow/core/BUILD:1634:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /tensorflow/tensorflow/tensorflow.bzl:911:30.\r\nINFO: Found 1 target...\r\nINFO: From Executing genrule //test_graph_build:gen_test_graph_tfmatmul:\r\n2017-08-29 20:20:29.702831: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n'+neon-fp-armv8' is not a recognized feature for this target (ignoring feature)\r\n'+neon-fp-armv8' is not a recognized feature for this target (ignoring feature)\r\n'+neon-fp-armv8' is not a recognized feature for this target (ignoring feature)\r\n'+neon-fp-armv8' is not a recognized feature for this target (ignoring feature)\r\n'+neon-fp-armv8' is not a recognized feature for this target (ignoring feature)\r\nERROR: /tensorflow/test_graph_build/BUILD:4:1: Linking of rule '//test_graph_build:test_graph_tfmatmul' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored /usr/bin/gcc -shared -o bazel-out/local-opt/bin/test_graph_build/libtest_graph_tfmatmul.so '-fuse-ld=gold' ... (remaining 7 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n/usr/bin/ld.gold: fatal error: bazel-out/local-opt/genfiles/test_graph_build/test_graph_tfmatmul.o: unsupported ELF machine number 40\r\ncollect2: error: ld returned 1 exit status\r\nTarget //test_graph_build:test_graph_tfmatmul failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 16.549s, Critical Path: 3.46s\r\n```\r\n\r\n\r\n\r\nWhat would be the right way to build this ?", "I'm not sure about your error. What does `file bazel-out/local-opt/genfiles/test_graph_build/test_graph_tfmatmul.o` says? It should be some ARM64/iOS stuff I guess. However, the error `\"+neon-fp-armv8' is not a recognized feature for this target (ignoring feature)` would indicate that you might be trying to issue some ARMv6.\r\n\r\nAccording to `tensorflow/tensorflow.bzl`, the `//tensorflow:ios` should be the proper way to check. The fact that you have those `+neon-fp-armv8` errors would indicate that it does not gets selected.\r\n\r\nMy best bet is that you should not be using ld.gold but your arch's ld. In our case, we disable the test and benchmark binaries with `gen_test=False, gen_benchmark=False`. You should make sure you have properly configured Tensorflow for cross-compiling for iOS (you should not see the warnings about `+neon-fp-armv8`). And then you may want to disable test and benchmark binaries, in case it is still unable to properly cross-build them.", "@lissyx I resolved the error by building tfcompile separately using command `bazel build tensorflow/compiler/aot:tfcompile` and then using tfcompile tool directly in terminal like this \r\n```\r\nbazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_build/test_graph_tfmatmul.pb --config=test_graph_build/test_graph_tfmatmul.config.pbtxt --cpp_class=foo::bar::MatMulComp --out_header=test_graph_build/test_graph_tfmatmul.h --out_object=test_graph_build/test_graph_tfmatmul.o --target_triple=armv64-apple-ios\r\n```\r\n\r\nThe above command generated  `test_graph_tfmatmul.h` and `test_graph_tfmatmul.o` for ios arm64 which I link to the in [simple ios example project ](https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/ios_examples). I copied the basic program to run the graph as given on [tensorflow aot page](https://www.tensorflow.org/performance/xla/tfcompile) and built the app. But for some reason some symbols are not found and gives error:\r\n```\r\nUndefined symbols for architecture armv7:\r\n  \"___xla_cpu_runtime_EigenMatMulF32\", referenced from:\r\n      _entry in test_graph_tfmatmul.o\r\n  \"tensorflow::tfcompile::runtime::FreeContiguous(void*)\", referenced from:\r\n      foo::bar::MatMulComp::~MatMulComp() in RunModelViewController.o\r\n  \"tensorflow::tfcompile::runtime::MallocContiguousBuffers(long const*, unsigned long, void**, bool)\", referenced from:\r\n      foo::bar::MatMulComp::MatMulComp(foo::bar::MatMulComp::AllocMode) in RunModelViewController.o\r\n  \"xla::ExecutableRunOptions::set_intra_op_thread_pool(Eigen::ThreadPoolDevice const*)\", referenced from:\r\n      foo::bar::MatMulComp::set_thread_pool(Eigen::ThreadPoolDevice const*) in RunModelViewController.o\r\nld: symbol(s) not found for architecture armv7\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nI had already built the [simple ios example project ](https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/ios_examples) once before, hence I am assuming all the header search paths and linker paths and libs are properly set in build settings of the project. But after additional adding the .h and .o generated from AOT,  with accompanying code to test it, the project it not building. What am I missing? Is there something additional I have to link?", "@ruppeshnalwaya1993 Ah, I hit the same kind of stuff, I just chose for now to build the related libs, link my binary against and share them. I am not convinced it is the proper way, I would have preferred they get statically linked by Bazel, but for now it's good enough for me. Targets are: `//tensorflow/compiler/aot:runtime //tensorflow/compiler/xla/service/cpu:runtime_matmul //tensorflow/compiler/xla:executable_run_options`, and the resulting shared objects are `-lruntime -lruntime_matmul -lexecutable_run_options` under `bazel-bin/tensorflow/compiler/[...]` (subdirectories).", "@lissyx What will the bazel build command to build say `//tensorflow/compiler/aot:runtime` for iOS or android target? Just running `bazel build tensorflow/compiler/aot:runtime` builds default `libruntime.a` which is neither for target android or iOS. ", "I don't know, I'm just documenting how I got things to work in my case.", "@lissyx Ok, np. Thanks for your help. :)\r\n@cancan101 Any update on cross-compilation documentation? Any comment from you will be very helpful.", "@lissyx @cancan101 I got some success. I ran the following command to build `lruntime` for Android and similarly built `-lruntime_matmul -lexecutable_run_options`\r\n\r\n`bazel build -c opt --cxxopt='-std=c++11' tensorflow/compiler/aot:runtime --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a`\r\n\r\nAnd got success in running the [test program](https://www.tensorflow.org/performance/xla/tfcompile#step_3_write_code_to_invoke_the_subgraph) with the test graph.\r\n\r\nNow I try running the frozen and optimised graph of pix2pix(https://github.com/affinelayer/pix2pix-tensorflow) and I get the output to be buffer with all values zero. I double checked that the passed values of input image are correct (floats between 0-1). Has anyone faces such an issue? Is there a way to debug aot generated code?", "@cancan101 @skye @petecoup  I also checked with a graph that has a just an input node and a following identity node as output. Even in this case the result from xla is buffer with all values either 0.000 or -nan", "@lissyx , I'm trying to do basically the same thing, cross-compile tfcompile binary to raspberrypi. However, I'm using the VGG16 network in Keras. How did you find the targets needed for your model ```//tensorflow/compiler/aot:runtime //tensorflow/compiler/xla/service/cpu:runtime_matmul //tensorflow/compiler/xla:executable_run_options ``` and how did you compile them for arm? ", "@ruppeshnalwaya1993 were you able to get pix2pix running on ios ? How did you debug it further ? ", "@powderluv yes, I was able to run pix2pix using XLA AOT compilation by I did not finally use it as there were no gains in speed as compared to regular tensorflow. \r\nAnyways, other than the things mentioned above, I remember that I had remove all the code and dependencies of normal tensorflow for xla to work. Seems both of them together can't function properly. I have not checked the recent development in xla though. ", "@ruppeshnalwaya1993 can you share your BUILD file to cross-compile your model to both android and ios? I managed to use aot to compile my model into a shared library and run on linux, but cannot find anything workable in android and ios. Thanks!", "> @ruppeshnalwaya1993 can you share your BUILD file to cross-compile your model to both android and ios? I managed to use aot to compile my model into a shared library and run on linux, but cannot find anything workable in android and ios. Thanks!\r\n\r\ncan u share something?"]}, {"number": 9645, "title": "Gather/Slice/StridedSlice Gradients Support in the C++ API", "body": "Hi,\r\n\r\nI noticed that there is currently no gradient support in the C++ API for the gather, slice, and strided slice ops. Would it be too difficult to add support for those gradient ops? I am asking because they are ops very frequently used in the context of machine learning models and without them building ML models can be unnecessarily complicated.\r\n\r\nThank you!\r\n\r\nP.S. I have noticed that the Python API implementation of the gather op gradient uses indexed slices, but I am not sure if that is entirely necessary and can thus probably also be supported in the C++ API.", "comments": ["@suharshs", "I think this issue deserves a discussion related to [indexed slices](https://github.com/tensorflow/tensorflow/issues/9644) and their usefulness. My impression is that they are only used in the \"gather\" op gradient and whenever used in other contexts, they are almost always converted to full tensors. Is that true? In that case, is it really useful having a special \"indexed slices\" data structure?", "@keveman Can you comment about indexedslices? Or cc someone who knows more about that?\r\nThanks!", "Adding @martinwicke (RE: C++ API needs for TensorFlow). It looks like `StridedSlice` and `Slice` are supported by the merged PRs above; is `Gather` still lacking?"]}, {"number": 9503, "title": "Ops for Reading from Cloud Spanner", "body": "Is there any plan to make F1 public (be it a service in Google Cloud or just open source) and make it possible to store TensorFlow tensors in F1? I ask because as far as I can tell (might be wrong), there isn't a \"native\" database for TensorFlow (meaning a C++ reader with direct connection to the DB), and F1 supports Protobuf columns which would seem like a natural fit for Tensorflow data.\r\n\r\nFrom [here](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41344.pdf)\r\n> The F1 data model is very similar to the Spanner data\r\n> model. In fact, Spanner\u2019s original data model was more like\r\n> Bigtable, but Spanner later adopted F1\u2019s data model. At\r\n> the logical level, F1 has a relational schema similar to that\r\n> of a traditional RDBMS, with some extensions including\r\n> explicit table hierarchy and columns with Protocol Buffer\r\n> data types.\r\n", "comments": ["We have a bigquery cloud reader in contrib.\r\n\r\nCC @jhseu ", "Any planned support for Spanner? A quick search doesn't show anything. Our data is structured. I ask because if you are planning open source support, my group might benefit from it so we could possibly help development.", "Bigquery supports structs (e.g. json).", "@tornato7 ^^^ and https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax", "I do think ops for reading from Cloud Spanner would be useful. Feel free to reopen if that's what you want.", "The initial sql op support is committed: https://github.com/tensorflow/tensorflow/commit/8dac0eb067662ba69a7eb72e640d8ff8d146e5c9\r\n\r\nIt's currently a bare minimum supporting sqlite with TEXT type only, but it should be able to serve as a starting point to support other sql engine.  ", "@aidan-plenert-macdonald \u2014 FWIW, we just added [initial read/write support for Cloud Bigtable][tf-bigtable] in [TF 1.10.0][tf-1.10], and you can store protocol buffers in Cloud Bigtable columns \u2014 in fact, you can store arbitrary bytes in Cloud Bigtable, since it doesn't enforce a typed schema. Hope this helps!\r\n\r\n[tf-bigtable]: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/bigtable\r\n[tf-1.10]: https://github.com/tensorflow/tensorflow/releases/tag/v1.10.0", "Many ops have been added to [`tensorflow/io`](https://github.com/tensorflow/io/tree/master/tensorflow_io) to support Google Cloud (as well as other cloud platforms!). \r\n\r\n@yongtang, do you know if Spanner has been added?", "@aidan-plenert-macdonald @dynamicwebpaige We don't have Cloud Spanner support in tensorflow/io yet (BigQuery/PubSub are already supported), though we could certainly add CloudSpanner as a feature in tensorflow/io.", "Opened an issue tensorflow/io#592  for tracking the Cloud Spanner support for tensorflow."]}, {"number": 9420, "title": "[Java] Distributed mode support", "body": "### Describe the problem\r\n\r\nIs there any plan to add distributed mode to the Java API? I checked the code and it seems to be doable (unless I missed something) so I was wondering if anyone is already working on it? I went through the issue tracker and PRs but couldn't find anything related.", "comments": ["The Java API is focused on inference, where distributed mode is usually not relevant.  Do you have an application?", "@girving I wanted to create and train the model using the Java API instead of having to train in Python, save the graph file and then load it using the Java API. Would make the whole workflow much easier if I could do it from the same tech stack.\r\n\r\nI can have a look at how hard it's to implement and make a PR (if it's doable), but my question is would such a PR have any chance of being accepted or are you set on only inference and keeping the Java API minimal?", "@asimshankar and @mrry: Can you comment?  My naive hunch is that distributed support is fairly straightforward; just a matter of adding another parameter to the `Session` constructor.  Except, of course, that one has to arrange for all the processes to exist and connect.  However, I'm not sure how useful it will be given the Java API's lack of graph construction support.", "Yes, it looks like the [bindings in `Session.java`](https://github.com/tensorflow/tensorflow/blob/28091dce41045570b7d92eb912eddce0079c0234/tensorflow/java/src/main/java/org/tensorflow/Session.java#L51) are missing the equivalent of a `target` argument. For a pure-Java solution, we'd also want some bindings for the equivalent of Python's `tf.train.Server`, which should be fairly easy to add&mdash;but right now they are part of the \"no stability guarantees\" C++ API, rather than the stable C API.", "Cool, thanks @girving and @mrry  - I was mostly worried that some parts of the distributed impl were only done in Python and I missed them. I'll put something together since I need it for myself and PR, then you can decide if you want it or not.", "If you might submit a PR, I'll reopen this and mark it contributions welcome.  It's good to have a place to track feature requests."]}, {"number": 9368, "title": "PreventGradients in SoftmaxCrossEntropyWithLogit ops", "body": "SparseSoftmaxCrossEntropyWithLogits cannot take second order gradients (It's not a beautiful hack, and I am not sure how much computational speed up it will bring). \r\nAdding PreventGradient node in the gradient graph seems to contaminate the computation graph structure, e.g. I am doing custom forward-mode automatic differentiation.\r\nReference: https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/nn_grad.py#L334", "comments": ["@renmengye I am not sure what you are asking? Is the node preventing from attempting autodiff?\r\n\r\nCC: @vrv ", "For example, I cannot take tf.gradients() on a gradient node to compute second order derivatives. \r\ny = sparse_softmax_cross_entropy_with_logits(logits=z, labels=y_)\r\ngrad_z = tf.gradients(y, z)\r\ngrad_grad_z = tf.gradients(y, grad_z) # This returns [None], thus no second order derivative can be taken.", "@girving any ideas?", "The solution is to port the trick from the nonsparse case (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L393).", "I'm trying to port the nonsparse solution to this case. Seems easy enough for my first contribution :)", "This had to be rolled back, unfortunately, because in eager mode the gradient computation accidentally inserts hessian gradients as it can't tell that the input op is a Zeros or ZerosLike.  We'll have to fix this first.", "We have an internal solution for better handling of second derivatives with `tf.custom_gradient`; will try modifying the non-sparse xent gradient to use that and report back here.", "Thanks, @ebrevdo! Any updates on second derivatives with `tf.custom_gradient`?"]}, {"number": 9260, "title": "Sampling from a categorical distribution without replacement", "body": "Both `tf.multinomial()` and `tf.contrib.distributions.Categorical.sample()` allow to sample from a multinomial distribution. However, they only allow sampling with replacement.\r\n\r\nIn constrast, Numpy's `numpy.random.choice()` has a `replace` parameter that allows sampling without replacement. Would it be possible to add a similar functionality to TensorFlow?\r\n\r\nOne use case is sampling examples from the dataset proportional to the model's last loss on them. When an example generates a very large loss, the next batch will mainly consist of that example. Using sampling without replacement, we can avoid this problem.\r\n\r\nI see that sampling with replacement can be parallelized and implemented in a vectorized way, but I don't think sampling speed is a bottleneck in most people's programs.", "comments": ["Sampling without replacement requires book-keeping, and, as you pointed out, in parallel requires some synchronization. I think we would implement per-process semantics, but that might still be slow.\r\n@aselle what do you think?", "@drpngx I think per-process semantics covers a reasonable fraction of the use cases.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "One relatively GPU-friendly way to do this is with a variant on the Gumbel-max trick (see https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/)\r\n\r\nIn the standard version (for sampling with replacement), you add i.i.d. Gumbel(0, 1) noise to the logits, then take the argmax index. If instead you want to sample K indices *without* replacement, you can just take the indices of the top K values (e.g. with tf.nn.top_k) instead of the single argmax index.", "@mjwillson haha, thank you, this works! I wrote the codes like:\r\n\r\nz = -tf.log(-tf.log(tf.random_uniform(tf.shape(logits),0,1)))\r\n_, indices = tf.nn.top_k(logits + z,K)\r\nreturn indices", "Is this equivalent to sampling without replacement or an approximation? ", "> Is this equivalent to sampling without replacement or an approximation?\r\n\r\ntotally equivalent", "That\u2019s fantastic. I can sort of intuit why that is, but you wouldn\u2019t happen to have a source for a derivation would you?", "> That\u2019s fantastic. I can sort of intuit why that is, but you wouldn\u2019t happen to have a source for a derivation would you?\r\n\r\nwell, I can't answer. I only use the sampling method in preprocessing steps rather than the main computation graph. first of all, what's the derivation of a random sample?", "Well, normal sampling without replacing would just be sampling, removing the element, sampling again, and so on. The distribution of that being equivalent to this is non-obvious to me, but I understand why this would at least approximate that.", "haven't prove it but I think the gumbel-max sampling is totally equivalent to a single multinomial sampling,  and the gumbel-top-k-max sampling is also equivalent to a multinomial sampling without replacement. hope someone can give the proof.", "The connection is explained here https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/ and cited from Efraimidis and Spirakis (2005)", "I actually tried the code with a small array, and the results are biased:\r\n```\r\ndef sample_without_replacement(logits, K):\r\n    \"\"\"\r\n    Courtesy of https://github.com/tensorflow/tensorflow/issues/9260#issuecomment-437875125\r\n    \"\"\"\r\n    z = -tf.log(-tf.log(tf.random_uniform(tf.shape(logits),0,1)))\r\n    _, indices = tf.nn.top_k(logits + z, K)\r\n    return indices\r\n\r\nnumber_of_flips = 3\r\ndata = tf.constant(list(range(10)), dtype=tf.int32)\r\nwith tf.Session() as sess:\r\n    data = tf.cast(data, dtype=tf.float32)\r\n    taken = sample_without_replacement(data, number_of_flips)\r\n    taken = tf_print(taken, [taken], \"Taken: \")\r\n    for i in range(10):\r\n        sess.run(taken)\r\n```\r\n\r\nThe output is:\r\n\r\n> Taken:  [9 8 7]\r\n> Taken:  [9 7 6]\r\n> Taken:  [7 9 6]\r\n> Taken:  [7 9 6]\r\n> Taken:  [7 9 8]\r\n> Taken:  [8 9 7]\r\n> Taken:  [9 8 7]\r\n> Taken:  [9 7 8]\r\n> Taken:  [7 9 8]\r\n> Taken:  [9 8 7]\r\n\r\nAnything that I'm doing wrong?", "Your logits are 0,1,2,3,4,...\nWhich says the most likely are those with the highest indices, which is\nconsistent with the samples. Try logits=tf.zeros([10]) for uniform, or\n[0]*5+[1]*5 for something interesting.\n\n\nOn Tue, Jan 8, 2019, 9:21 PM Lorenzo Riano <notifications@github.com wrote:\n\n> I actually tried the code with a small array, and the results are biased:\n>\n> def sample_without_replacement(logits, K):\n>     \"\"\"\n>     Courtesy of https://github.com/tensorflow/tensorflow/issues/9260#issuecomment-437875125\n>     \"\"\"\n>     z = -tf.log(-tf.log(tf.random_uniform(tf.shape(logits),0,1)))\n>     _, indices = tf.nn.top_k(logits + z, K)\n>     return indices\n>\n> number_of_flips = 3\n> data = tf.constant(list(range(10)), dtype=tf.int32)\n> with tf.Session() as sess:\n>     data = tf.cast(data, dtype=tf.float32)\n>     taken = sample_without_replacement(data, number_of_flips)\n>     taken = tf_print(taken, [taken], \"Taken: \")\n>     for i in range(10):\n>         sess.run(taken)\n>\n> The output is:\n>\n> Taken: [9 8 7]\n> Taken: [9 7 6]\n> Taken: [7 9 6]\n> Taken: [7 9 6]\n> Taken: [7 9 8]\n> Taken: [8 9 7]\n> Taken: [9 8 7]\n> Taken: [9 7 8]\n> Taken: [7 9 8]\n> Taken: [9 8 7]\n>\n> Anything that I'm doing wrong?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9260#issuecomment-452547193>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVJZI0-v5O9eic9duWcHQMOao7qjMucPks5vBVIhgaJpZM4M-6kX>\n> .\n>\n", "For those looking for the simplest case where all elements have the same probably to appear in the sample, i.e. uniform distribution, the formula is just:\r\n\r\n```\r\ndef random_choice(domain, sample, batch_shape=()):\r\n  \"\"\"Generate random samples without replacement.\r\n  \r\n  This would be roughly equivalent to:\r\n  \r\n      numpy.random.choice(domain, sample, replace=False)\r\n      \r\n  but also supports generating batches of samples.\r\n  \"\"\"\r\n  p = tf.random_uniform(batch_shape + (domain,), 0, 1)\r\n  _, indices = tf.nn.top_k(p, sample)\r\n  return indices\r\n```", "This would be a great feature add, but is outside of the scope of TensorFlow Core and would fall under the jurisdiction of TensorFlow Probability. @jvdillon, could you please transfer the issue?", "> I actually tried the code with a small array, and the results are biased:\r\n> \r\n> ```\r\n> def sample_without_replacement(logits, K):\r\n>     \"\"\"\r\n>     Courtesy of https://github.com/tensorflow/tensorflow/issues/9260#issuecomment-437875125\r\n>     \"\"\"\r\n>     z = -tf.log(-tf.log(tf.random_uniform(tf.shape(logits),0,1)))\r\n>     _, indices = tf.nn.top_k(logits + z, K)\r\n>     return indices\r\n> \r\n> number_of_flips = 3\r\n> data = tf.constant(list(range(10)), dtype=tf.int32)\r\n> with tf.Session() as sess:\r\n>     data = tf.cast(data, dtype=tf.float32)\r\n>     taken = sample_without_replacement(data, number_of_flips)\r\n>     taken = tf_print(taken, [taken], \"Taken: \")\r\n>     for i in range(10):\r\n>         sess.run(taken)\r\n> ```\r\n> \r\n> The output is:\r\n> \r\n> > Taken:  [9 8 7]\r\n> > Taken:  [9 7 6]\r\n> > Taken:  [7 9 6]\r\n> > Taken:  [7 9 6]\r\n> > Taken:  [7 9 8]\r\n> > Taken:  [8 9 7]\r\n> > Taken:  [9 8 7]\r\n> > Taken:  [9 7 8]\r\n> > Taken:  [7 9 8]\r\n> > Taken:  [9 8 7]\r\n> \r\n> Anything that I'm doing wrong?\r\n\r\nlogits = log(P), not P , not indices", "FWIW, TFP would be happy to host a tfp.math.random_choice though my gut feeling is that tf.random.choice is a more natural home, in keeping with other random samplers.  However, a sample with replacement distribution object (complete with sample, log_prob, mean, etc) should live in TFP since tf.contrib.distributions are now gone.", "Is there an open issue in TFP for that?", "I'm not aware of any issue open for this in TFP. Feel free to open one and\nsend a PR. This could perhaps sit alongside tfp.math.random_rademacher, or\nmaybe we would want to make a new 'random' pkg in TFP. Team would need to\ndiscuss.\n", "> @mjwillson haha, thank you, this works! I wrote the codes like:\r\n> \r\n> z = -tf.log(-tf.log(tf.random_uniform(tf.shape(logits),0,1)))\r\n> _, indices = tf.nn.top_k(logits + z,K)\r\n> return indices\r\n\r\nInteresting approach, but then how do you calculate gradients w.r.t indices?", "Integer-valued tensors don't have a gradient. If you wanted gradients, you'd either relax the categorical (see papers on \"concrete\") or use a REINFORCE (high variance) gradient. There is also the GO gradient, which can work when the categorical sample is a stochastic leaf."]}, {"number": 9258, "title": "Create support for a score threshold in NonMaxSuppression to skip over boxes with low score", "body": "Right now tensorflow::ops::NonMaxSuppression only prunes away boxes that have a high IOU overlap with previously selected boxes. It would be nice to also support a threshold on score so that the algorithm can skip over boxes that have a score below that threshold. We strongly believe this feature will speed up nms. Is there a plan to add this score threshold as a parameter?", "comments": ["We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n\r\n(FYI @gpapan @shlens in case they know of anyone working on this)", "You can just filter out those boxes (with `tf.boolean_mask`) before nms to speed up."]}, {"number": 9201, "title": "Tensorflow Still Trying to use CUDA even when Session Created with device_count={'GPU': 0}", "body": "### System Information\r\nUsing the `tensorflow/tensorflow:1.0.1-devel-gpu` Docker image.\r\n`('v1.0.0-65-g4763edf-dirty', '1.0.1')`\r\nHost: `Driver Version: 367.57`, `3.13.0-57-generic`\r\n\r\n### Issue\r\nIf I `Set compute mode to EXCLUSIVE_PROCESS` on the Nvidia device (`sudo nvidia-smi -c 1`), then even though I tell the `Session` not to use GPUs (`config=tf.ConfigProto(device_count={'GPU': 0})`), Tensorflow attempts to use the GPU resulting in an inability to create session:\r\n```\r\nInternalErrorTraceback (most recent call last)\r\n<ipython-input-1-cabf26c1451a> in <module>()\r\n      1 import tensorflow as tf\r\n      2 from tensorflow.python.framework import ops\r\n----> 3 with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\r\n      4     pass\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in __init__(self, target, graph, config)\r\n   1174 \r\n   1175     \"\"\"\r\n-> 1176     super(Session, self).__init__(target, graph, config=config)\r\n   1177     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\r\n   1178     self._default_graph_context_manager = None\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in __init__(self, target, graph, config)\r\n    550     try:\r\n    551       with errors.raise_exception_on_not_ok_status() as status:\r\n--> 552         self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n    553     finally:\r\n    554       tf_session.TF_DeleteSessionOptions(opts)\r\n\r\n/usr/lib/python2.7/contextlib.pyc in __exit__(self, type, value, traceback)\r\n     22         if type is None:\r\n     23             try:\r\n---> 24                 self.gen.next()\r\n     25             except StopIteration:\r\n     26                 return\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc in raise_exception_on_not_ok_status()\r\n    464           None, None,\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n    468     pywrap_tensorflow.TF_DeleteStatus(status)\r\n\r\nInternalError: Failed to create session.\r\n```\r\nThis can be demonstrated by running:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nwith tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\r\n    pass\r\n```\r\nwhen another process is using CUDA and the exclusive process mode is set.\r\n\r\nIf exclusive process mode is _not_ set, then the session is created but using `nvidia-smi`, I see that the process is using GPU ram (and CUDA):\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      2237    C   /usr/bin/python                                 61MiB |\r\n```\r\n\r\nThe issue seems limited to TF trying to lock the CUDA device (an allocate ~61MB memory). Subsequent computations do happen correctly on the CPU.", "comments": ["You want either `export CUDA_VISIBLE_DEVICES=` or alternatively a virtualenv with non-GPU TensorFlow. See also: https://github.com/tensorflow/tensorflow/issues/2175#issuecomment-215977173", "@jart: I'm not sure why the config  approach I outlined doesn't work and why the only suggestion is to set an env var. Setting the configuration as I did seems to partially work (ie prevent usage of the gpu for the graph) but not totally ie it locks device. This seems to violate \"principle of least astonishment\". It seems like this is either a documentation issue or an issue with how the config is used. \r\n\r\nThe environmental var approach is not ideal as:\r\n1. It is weird for a process to set this value it self.\r\n2. related to 1, but limits the ability to set use GPU vs not on a per Session basis.", "@jart Any thoughts on the above questions / comments?", "@zheng-xq Our friend @cancan101 believes it would be less astonishing for our users if `tf.ConfigProto(device_count={'GPU': 0})` also implied `export CUDA_VISIBLE_DEVICES=\"\"`. That doesn't sound unreasonable to me. What are your opinions on this feature request?", "I am experiencing the same issue with TF and I too believe `tf.ConfigProto(device_count={'GPU': 0})` should imply `export CUDA_VISIBLE_DEVICES=\"\"`. I'd like to be able to use my GPU for specific tasks without setting up a second env.", "I'm also have the same problem. Will be very happy if this could be supported.", "Why is this issue closed?", "@TimZaman \r\n\r\nIt's not, but it really isn't a priority as you can (I know it's ugly)\r\n\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\r\n\r\n    # Code that uses Tensorflow without GPU\r\n\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\r\n\r\nIf you want you could also wrap the whole thing in a decorator:\r\n\r\n    import os\r\n\r\n    def cpu_only():\r\n        def _method_wrapper(function):\r\n            def wrap(*args, **kwargs):\r\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\r\n                ret = function(*args, **kwargs)\r\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\r\n                return ret\r\n                wrap.__doc__ = function.__doc__\r\n                wrap.__name__ = function.__name__\r\n            return wrap\r\n        return _method_wrapper\r\n\r\nSomeone might work on it one day but I wouldn't hold my breath", "@Belval hehe yeah that makes me feel like I want to take a shower.\r\nBut I love how you wrapped that turd in a beautiful decorator! \ud83e\udd23  Fair enough for now, I can see how this is not top prio."]}, {"number": 9180, "title": "Make `py_func` accept `SparseTensor`", "body": "### Describe the problem clearly\r\nAccording to the [doc](https://www.tensorflow.org/api_docs/python/tf/py_func), `py_func` accepts `inp` as a list of tensors (or convertible to tensor). However `SparseTensor` is not one of them. Could we support `SparseTensor` as well? Semantically there is no reason to treat `SparseTensor` differently.\r\n\r\n### Source Code / Logs\r\n```\r\nmy_sparse_tensor = tf.SparseTensor(...)\r\ntf.py_func(my_py_func, [my_sparse_tensor, ...], [tf.float32])\r\n```\r\nWhat I got:\r\n\r\n`TypeError: Tensors in list passed to 'input' of 'PyFunc' Op have types [<NOT CONVERTIBLE TO TENSOR>, ...] that are invalid`.\r\n", "comments": ["@alextp : Any thoughts on this (since you recently touched the `py_func` codepaths)", "One could repurpose the code which makes sparse tensors feedable / fetchable to do that.\r\n\r\nRight now to unblock you simply pass st.value, st.indices, and st.shape to the py_func where you can make a SparseTensorValue out of them.", "I'll work on this.", "@jingjingwang , what does your `my_py_func` look like? I think the function passed to `tf.py_func` is purely \"numpy\"(in tf runtime, we first convert tensor to numpy array), so there must be a sparse object type in numpy that we can convert `tf.SparseTensor` to.", "A SparseTensor is just three tensorflow Tensors (indices, values, and\nshape). Ops which take or produce SparseTensor are just python wrappers\naround tf ops which pass/return these 3 tensors.\n\nOn Thu, May 18, 2017 at 11:28 PM, Ziming Dong <notifications@github.com>\nwrote:\n\n> @jingjingwang <https://github.com/jingjingwang> , what does your\n> my_py_func look like? I think the function passed to tf.py_func is purely\n> \"numpy\", so there must be a sparse object type in numpy that we can convert\n> tf.SparseTensor to.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9180#issuecomment-302620030>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRCQ0JkvPC57xOwPrdVx2U5Fo_Eqks5r7TaKgaJpZM4M8Xtl>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp , I think @jingjingwang want to pass a `SparseTensor` directly to a custom python function, not unpack it to three Tensors first.", "Directly converting to `scipy.sparse.coo_matrix` would be cool, actually :)", "Any progress, @suiyuan2009 ? I guess it should be pretty straightforward if you just convert the tf.SparseTensor to become a namedtuple or a dict, containing `values`, `indices` and `dense_shape`.\r\nAh actually, it doesn't seem to support dicts/namedtuples?\r\n\r\n> Directly converting to scipy.sparse.coo_matrix would be cool, actually :)\r\n\r\ncoo_matrix only seems to support 2 dimensions.", "Here's a workaround so pyfunc accepts nested structures, feel free to make an MR so it ends up in the codebase:\r\n\r\n```\r\ndef py_func_v2(func, inputs, Tout):\r\n    \"\"\"Wrapper around py_func to accept nested structured like dicts and tuples.\"\"\"\r\n    def _wrapper(*flat_inputs):\r\n        inputs_d = tf.contrib.framework.nest.pack_sequence_as(inputs, flat_inputs)\r\n        return func(inputs_d)\r\n    flat_inputs = tf.contrib.framework.nest.flatten(inputs)\r\n    return tf.py_func(func=_wrapper, inp=flat_inputs, Tout=Tout)\r\n```"]}, {"number": 9150, "title": "C API Tensors", "body": "I am currently using the C API and building a Scala API on top of it. It seems that what is done in the Python API and the Java API is that the tensors fed into sessions are being copied to buffers internal to the native library. I am also currently doing that in the Scala library but I was wondering if we can do the following:\r\n\r\nLet's assume we can share a pointer to the underlying data structure between C and Scala (through a Java NIO DirectMemoryBuffer for example). Then, is there any functionality to obtain a tensor \"view\" that is a slice of that tensor, directly using that buffer? I imagine that since the TF op kernels are implemented in C++, it should be possible to use the StridedSlice op directly on a tensor data structure (without needing to use a session). The same idea can be extended to other ops. So, first of all, is that true?\r\n\r\nSecondly, if it is, where is that functionality available in the C API (or exposed elsewhere) so that I can use it from within my Scala library? I currently do the indexing on the byte buffer myself, but that can be painful for arbitrary slices.\r\n\r\nOne main issue with sharing a pointer is how to deal with the Java garbage collector. I haven't figured that out yet, but even if I can't do that, the above comment still applies. How can I use op kernels directly in order to manipulate tensors outside of the symbolic graph? That is useful for languages other than Python, where a library as powerful as numpy is not available.\r\n\r\nThank you!", "comments": ["@josh11b @skye , could you provide some guidance?", "The primitive that is offered is `TF_TensorData`, which provides the raw (flattened) tensor content. You could use that to build other views (using `TF_NewTensor` and making the buffer deallocation function be a no-op). However, in general, no, being able to invoke kernels directly on tensors and avoiding the symbolic graph is not something that the C API (or the TensorFlow runtime code) currently makes convenient.\r\n\r\nAnd as you gathered, with the JVM, sharing buffers with the C API will be problematic because of the garbage collector. In the Java API, the private [`Tensor.buffer()`](https://github.com/tensorflow/tensorflow/blob/2acba51/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L472) method provides direct access to the contents of the tensor in the runtime, but using it correctly is tricky (as one must be sure that the garbage collector and C memory management don't interfere with each other). For this reason, it has not been made public.", "@alextp ", "Note that the C API now elides copies when it is safe to do so (feeding doesn't copy, passing parameters to and returning values from py_func doesn't copy, and fetching only copies when fetching Variables and other persistent state).\r\n\r\nCurrently there is no API in tf for running op kernels directly on Tensors (this is tricky since the Tensors can be on non-cpu devices, which have contexts which need to be carefully set up). We could easily expose the CPU kernels.", "@asimshankar Thank you for that response! That's what I am currently doing in the Scala library (sharing the buffer), and I manage the indexing separately on top of it. If you slice it, I return a TensorSlice object which simply contains the slice information in it along with the original Tensor reference, and adjusts the indexing appropriately to account for the slice. The reason I am not returning a new Tensor with a view to part of the buffer is that a slice does have to be contiguous in memory (except for slices along the first dimension with stride 1). This is quite inefficient in my opinion and that's why I was thinking of the possibility of using the kernel implementation.\r\n\r\n@alextp I see. That makes sense. I assume you do that [here](https://github.com/tensorflow/tensorflow/blob/97c6203bb3f3978ac67920c66b6234ef82051c57/tensorflow/python/client/tf_session_helper.cc#L546), where you create a tensor by providing a pointer to the numpy array data. I could do that using Java ByteBuffers in Scala. My only problem is the deallocator. Would it be reasonable to provide a no-op to the deallocator (as @asimshankar mentioned) and let the JVM GC handle memory? As long as it is only used for a session.run call, then the GC should not mess with it (given it's in scope).\r\n\r\n@alextp Would it be easy to expose the CPU kernels only for CPU Tensors? That would provide a numpy-like library effectively, that could be used from Java, Scala, or other languages (through the C API). The kernel implementations could be called using something equivalent of the TF_OperationDecsription that's currently being used for creating ops. That would be very convenient as currently I have to build support for a tensor library from scratch on the Scala side, manipulating byte buffers that have the exact same structure as the TF_Tensor buffers.", "@eaplatanios to provide an empty deallocator in this case would leak memory. You'd need somehow to let the java runtime know when TF is done with that buffer. I don't know enough about JNI to know how this is done.", "@alextp As long as I create a DirectByteBuffer on the Scala side, the GC will be able to deallocate its underlying buffer. Furthermore, as long as a reference to that DirectByteBuffer exists on the Scala side, the GC will not deallocate it and the C side will be able to use it (e.g., when you call session.run). So I think there would be no memory leakage since the GC will take care of that buffer. The only occasion where there might be an issue if the GC deallocates it and then the C library tries to use it. This should not happen though unless the C library tries to reuse the tensor you pass in session.run, after that function returns. Does that sound reasonable?\r\n\r\nIt only means that we leave memory management to the GC on the Scala side, rather than to the TensorFlow native library.", "There is a chance that the C library will reuse the Tensor you pass to\nsession.run after it returns (for example, if the Tensor is enqueued into a\nfifo queue we reserve the right to not copy it until it's time to dequeue).\n\nOn Thu, Apr 13, 2017 at 10:07 AM, Anthony Platanios <\nnotifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> As long as I create a\n> DirectByteBuffer on the Scala side, the GC will be able to deallocate its\n> underlying buffer. Furthermore, as long as a reference to that\n> DirectByteBuffer exists on the Scala side, the GC will not deallocate it\n> and the C side will be able to use it (e.g., when you call session.run). So\n> I think there would be no memory leakage since the GC will take care of\n> that buffer. The only occasion where there might be an issue if the GC\n> deallocates it and then the C library tries to use it. This should not\n> happen though unless the C library tries to reuse the tensor you pass in\n> session.run, after that function returns. Does that sound reasonable?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293962227>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxX-5lcLpShE_6vTCyUw1V_eDjPZHks5rvlZOgaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "I see. In this case, how about we allocate the DirectByteBuffer on the C side wrapping the TF_Tensor buffer? This way it cannot be deallocated by the GC (only the wrapping DirectByteBuffer object can be GCed, but the underlying memory will still be there for TF to manage), and will be managed by the TensorFlow native library. Should the Scala user ever have to explicitly delete Tensor objects in this case (i.e., through the C API), or will the TF library manage them?\n\nThis also comes down to the more basic question of how this is done in the Python API. You are passing the TF_Tensor constructor the numpy deallocator. Does this mean that the TF native library can delete a Tensor even though it might still be used in the Python side as a numpy array, referencing the same underlying memory? If not, then how and when are TF tensors deallocated for the Python API?\n\nI'm sorry for the series of questions, but given that I'm building a Scala API aimed to be as close as possible to the Python API, in terms of functionality, I'm trying to get a better understanding of how the interfacing with the native library works and how to do it efficiently, without unnecessary copies.\n\nOn Apr 13, 2017, 1:14 PM -0400, Alexandre Passos <notifications@github.com>, wrote:\n> There is a chance that the C library will reuse the Tensor you pass to\n> session.run after it returns (for example, if the Tensor is enqueued into a\n> fifo queue we reserve the right to not copy it until it's time to dequeue).\n>\n> On Thu, Apr 13, 2017 at 10:07 AM, Anthony Platanios <\n> notifications@github.com> wrote:\n>\n> > @alextp <https://github.com/alextp> As long as I create a\n> > DirectByteBuffer on the Scala side, the GC will be able to deallocate its\n> > underlying buffer. Furthermore, as long as a reference to that\n> > DirectByteBuffer exists on the Scala side, the GC will not deallocate it\n> > and the C side will be able to use it (e.g., when you call session.run). So\n> > I think there would be no memory leakage since the GC will take care of\n> > that buffer. The only occasion where there might be an issue if the GC\n> > deallocates it and then the C library tries to use it. This should not\n> > happen though unless the C library tries to reuse the tensor you pass in\n> > session.run, after that function returns. Does that sound reasonable?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293962227>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/AAATxX-5lcLpShE_6vTCyUw1V_eDjPZHks5rvlZOgaJpZM4M6yMu>\n> > .\n> >\n>\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub (https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293963686), or mute the thread (https://github.com/notifications/unsubscribe-auth/ABPCXGJtaeZVo4HCPoL0adq5IhTk63gnks5rvlgDgaJpZM4M6yMu).\n>\n>\n>\n\n", "Also, since in the Python API for session.run, you do not copy the numpy array but only pass a pointer to its buffer, couldn't the Python garbage collector also deallocate the array, even when the native library might still be using it? That is, how is that problem avoided in the Python API?", "For the python-c bridge we create at different points a numpy array which\npoints to TF-managed memory (and when the refcount of this numpy array goes\nto zero it decrements the refcount of the tf-managed memory) and a TF\nTensor which points to numpy-managed memory (and when the refcount of the\nTF Tensor goes to 0 it decrements the refcount of the numpy array). This\nway all interactions are correct.\n\nIf you can trigger unrefing of the scala object when TF calls the\ndestructor and trigger unrefing of the Tensor when scala is done with the\nbyte buffer all will be well I think.\n\nOn Thu, Apr 13, 2017 at 10:35 AM, Anthony Platanios <\nnotifications@github.com> wrote:\n\n> Also, since in the Python API for session.run, you do not copy the numpy\n> array but only pass a pointer to its buffer, couldn't the Python garbage\n> collector also deallocate the array, even when the native library might\n> still be using it? That is, how is that problem avoided in the Python API?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293969307>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTbsPO8WqREb9K_2lffABidhTvZlks5rvlzygaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "That sounds good! After looking further into JNI, I suggest the following:\r\n\r\n- Tensors are always backed by a direct ByteBuffer that is created on the Scala/Java side.\r\n- They have an \"asNative\" method that (through JNI) calls \"TF_NewTensor\" in the C API. It also the JNI \"NewGlobalRef\" method to create a global reference to the direct ByteBuffer object. This guarantees that as long as TensorFlow native uses that byte buffer, it won't be garbage collected by the JVM. As a deallocator function we provide a function that the JNI \"DeleteGlobalRef\" method on the previously created reference.\r\n\r\nAnd that's the only interaction between the native TensorFlow tensors API and the Scala/Java API. The rest of it (i.e., elements access, slicing, etc.) is all handled on the Scala side. The same could apply to the current Java API and void the copying.\r\n\r\nDoes that sound reasonable?", "Yes, sounds reasonable. I'd love to see something like this as a pull\nrequest for contrib, as there are other people interested in scala wrappers.\n\nOn Thu, Apr 13, 2017 at 2:32 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> That sounds good! After looking further into JNI, I suggest the following:\n>\n>    - Tensors are always backed by a direct ByteBuffer that is created on\n>    the Scala/Java side.\n>    - They have an \"asNative\" method that (through JNI) calls\n>    \"TF_NewTensor\" in the C API. It also the JNI \"NewGlobalRef\" method to\n>    create a global reference to the direct ByteBuffer object. This guarantees\n>    that as long as TensorFlow native uses that byte buffer, it won't be\n>    garbage collected by the JVM. As a deallocator function we provide a\n>    function that the JNI \"DeleteGlobalRef\" method on the previously created\n>    reference.\n>\n> And that's the only interaction between the native TensorFlow tensors API\n> and the Scala/Java API. The rest of it (i.e., elements access, slicing,\n> etc.) is all handled on the Scala side. The same could apply to the current\n> Java API and void the copying.\n>\n> Does that sound reasonable?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-294027074>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQ910C4Pw034Gb_yKa2qQNMH2Wlmks5rvpR9gaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "That could happen soon. Although, I'm not sure how to set the whole thing up with bazel. I currently use SBT as my build tool and I assume that libtensorflow.so is in \"LD_LIBRARY_PATH\".\r\n\r\nI will soon make my repository public and we can see how to go about integrating it in the main repository, then.", "@eaplatanios : We'd probably not want to integrate the scala bindings into the TensorFlow repository, but instead will encourage you to maintain your own. However, any contributions to the C API or the Java/JNI code will be great to see.\r\n\r\nOne thing to keep in mind, the Tensors created by Java/Scala can be backed by the DirectByteBuffer. However, there will also be tensors created by the TensorFlow runtime (e.g., tensors \"fetched\" from a session run). Not sure how the memory management of those will be handled.", "@asimshankar Yeah I am working on a solution for tensors created by the TensorFlow runtime. I'll update once I have a good solution. For now, in my code, I avoid copying when feeding or creating constant ops.\r\n\r\nI'm not sure if this is the right place to ask this, but I also have some confusion with respect to the reference data types. How are these handled internally and what data exactly does a \"int8_ref\" tensor hold exactly? The same question applies for the \"resource\" data type. What data does a \"resource\" data type hold and how should I be feeding these into a Session?\r\n\r\nThanks for the feedback so far! :)", "The reference data types are going away, so you don't have to worry about feeding / fetching those.\r\n\r\nThe resource data type is a tensor whose elements are ResourceHandle C++ protocol buffer objects. It should not be necessary to feed or fetch these tensors; instead simply connect ops which emit resources with ops which take resources and things will work.", "That sounds great, thanks! :)\r\n\r\nI have one last question regarding the memory structure of tensors, just to make sure I got everything clear. For tensors with data type String, how to you signify the end of each string? That is, what is the memory structure for String tensors?\r\n\r\nAs an example, let's say we have `Tensor(Tensor(\"abc\", \"defog\"), Tensor(\"1\", \"56\"))` which is `[1, 2]` shape tensor. How would that be represented in the TF_Tensor buffer?", "Use the C API functions TF_StringEncode and TF_StringDecode to access the\nstring content of TF_Tensor objects.\n\nOn Fri, Apr 14, 2017 at 11:54 AM, Anthony Platanios <\nnotifications@github.com> wrote:\n\n> That sounds great, thanks! :)\n>\n> I have one last question regarding the memory structure of tensors, just\n> to make sure I got everything clear. For tensors with data type String, how\n> to you signify the end of each string? That is, what is the memory\n> structure for String tensors?\n>\n> As an example, let's say we have Tensor(Tensor(\"abc\", \"defog\"),\n> Tensor(\"1\", \"56\")) which is [1, 2] shape tensor. How would that be\n> represented in the TF_Tensor buffer?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-294213498>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYfA5sobjlhcrzl1sZbOmvtd4mfSks5rv8DPgaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "I have worked out all the core parts except for the fetching tensors from sessions. @alextp you mentioned that if I can trigger unrefing of the scala object when TF calls the destructor and trigger unrefing of the Tensor when scala is done with the byte buffer all will be well I think. I have managed the first part, but I have some confusion regarding the second. I currently have no way of unrefing the TF tensor when Scala is done with the byte buffer, but I'm not sure if that's necessary. The C API documentation for session.run says this:\r\n\r\n> On success, the tensors corresponding to outputs[0,noutputs-1] are placed in\r\n> output_values[]. Ownership of the elements of output_values[] is transferred\r\n> to the caller, which must eventually call TF_DeleteTensor on them.\r\n\r\nDoes this mean that the TensorFlow native library is guaranteed to not use that tensor again? Because in that case, I could return a pointer to the underlying byte buffer, along with it's size and let the garbage collector delete the byte buffer when it's done using it. Is there anything else that TF_DeleteTensor deletes other than that byte buffer? And if so, could I delete that data manually without deleting the buffer?\r\n\r\nLooking at the Python API implementation, I think this is the relevant piece of code:\r\n\r\n```cpp\r\n  // 4. We now own the fetched tensors, so set up a safe container to\r\n  // delete them when we exit this scope.\r\n  Safe_TF_TensorVector tf_outputs_safe;\r\n  for (const auto& output : outputs) {\r\n    tf_outputs_safe.emplace_back(make_safe(output));\r\n  }\r\n\r\n  // 5. Convert the fetched tensors into numpy ndarrays. Store them in a safe\r\n  // container so that we do not leak\r\n  Safe_PyObjectVector py_outputs_safe;\r\n  for (size_t i = 0; i < output_names.size(); ++i) {\r\n    PyObject* py_array;\r\n    s = TF_Tensor_to_PyObject(std::move(tf_outputs_safe[i]), &py_array);\r\n    if (!s.ok()) {\r\n      Set_TF_Status_from_Status(out_status, s);\r\n      return;\r\n    }\r\n    py_outputs_safe.emplace_back(make_safe(py_array));\r\n  }\r\n\r\n  // 6. If we reach this point, we have successfully built a list of objects\r\n  // so we can release them from the safe container.\r\n  for (auto& output : py_outputs_safe) {\r\n    out_values->push_back(output.release());\r\n  }\r\n```\r\n\r\nand the `make_safe` function creates a unique_ptr that calls TF_DeleteTensor when unreferenced. My question is whether there is a way to avoid that call entirely and let the garbage collector deallocate the underlying byte array when necessary.\r\n\r\nI hope this makes sense.", "On Tue, Apr 18, 2017 at 1:48 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I have worked out all the core parts except for the fetching tensors from\n> sessions. @alextp <https://github.com/alextp> you mentioned that if I can\n> trigger unrefing of the scala object when TF calls the destructor and\n> trigger unrefing of the Tensor when scala is done with the byte buffer all\n> will be well I think. I have managed the first part, but I have some\n> confusion regarding the second. I currently have no way of unrefing the TF\n> tensor when Scala is done with the byte buffer, but I'm not sure if that's\n> necessary. The C API documentation for session.run says this:\n>\n> On success, the tensors corresponding to outputs[0,noutputs-1] are placed\n> in\n> output_values[]. Ownership of the elements of output_values[] is\n> transferred\n> to the caller, which must eventually call TF_DeleteTensor on them.\n>\n> Does this mean that the TensorFlow native library is guaranteed to not use\n> that tensor again?\n>\nNo, this means the native library will not delete the memory until you call\nTF_DeleteTensor on the tensor (but aliasing considerations involving\nvariables and queues and other stateful things can mean that the tf runtime\nmight still use that memory).\n\n\n\n> Because in that case, I could return a pointer to the underlying byte\n> buffer, along with it's size and let the garbage collector delete the byte\n> buffer when it's done using it. Is there anything else that TF_DeleteTensor\n> deletes other than that byte buffer?\n>\nThe current implementation of TF_Tensor is a struct with a pointer to a\nTensorBuffer structure (which has a pointer to the byte buffer), a dtype,\nand a shape. This structure also gets deleted by TF_DeleteTensor.\n\nBut the API makes no promises that other things won't have to be deleted in\nthe future.\n\n\n> And if so, could I delete that data manually without deleting the buffer?\n>\n> Looking at the Python API implementation, I think this is the relevant\n> piece of code:\n>\n>   // 4. We now own the fetched tensors, so set up a safe container to\n>   // delete them when we exit this scope.\n>   Safe_TF_TensorVector tf_outputs_safe;\n>   for (const auto& output : outputs) {\n>     tf_outputs_safe.emplace_back(make_safe(output));\n>   }\n>\n>   // 5. Convert the fetched tensors into numpy ndarrays. Store them in a safe\n>   // container so that we do not leak\n>   Safe_PyObjectVector py_outputs_safe;\n>   for (size_t i = 0; i < output_names.size(); ++i) {\n>     PyObject* py_array;\n>     s = TF_Tensor_to_PyObject(std::move(tf_outputs_safe[i]), &py_array);\n>     if (!s.ok()) {\n>       Set_TF_Status_from_Status(out_status, s);\n>       return;\n>     }\n>     py_outputs_safe.emplace_back(make_safe(py_array));\n>   }\n>\n>   // 6. If we reach this point, we have successfully built a list of objects\n>   // so we can release them from the safe container.\n>   for (auto& output : py_outputs_safe) {\n>     out_values->push_back(output.release());\n>   }\n>\n> and the make_safe function creates a unique_ptr that calls\n> TF_DeleteTensor when unreferenced. My question is whether there is a way to\n> avoid that call entirely and let the garbage collector deallocate the\n> underlying byte array when necessary.\n>\nYou need to make the garbage collector call TF_DeleteTensor instead of\ndeleting the byte array.\n\nSpecially because since that byte array wasn't allocated by the allocator\nattached to your garbage collector there's no way for your garbage\ncollector to know how to return it to the free memory pool.\n-- \n - Alex\n", "Great! Thanks for the quick response!\r\n\r\nI looked into ways for doing that and I'm going to go ahead with the approach described [here](https://dzone.com/articles/letting-garbage-collector-do-c) and used [here](https://github.com/frohoff/jdk8u-dev-jdk/blob/master/src/share/classes/sun/java2d/Disposer.java), which basically consists of a thread running in the background and checking whether all references to an object have gone away, using the Java PhantomReference. When all references go away, it will call the `TF_DeleteTensor` function. This could also apply to the Java API.", "That makes sense.\n\nOn Tue, Apr 18, 2017 at 3:44 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> Great! Thanks for the quick response!\n>\n> I looked into ways for doing that and I'm going to go ahead with the\n> approach described here\n> <https://dzone.com/articles/letting-garbage-collector-do-c> and used here\n> <https://github.com/frohoff/jdk8u-dev-jdk/blob/master/src/share/classes/sun/java2d/Disposer.java>,\n> which basically consists of a thread running in the background and checking\n> whether all references to an object have gone away, using the Java\n> PhantomReference. This could also apply to the Java API.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-295005670>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXbwM6puC3fmFrakHoGBFFtfA2tpks5rxTzHgaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp Given that reference data types are going away, what input should I feed to ops such as `StridedSliceAssign` that have a `Ref(T)` input type?\r\n\r\nThe op registration in the C++ code looks like this:\r\n\r\n```cpp\r\nREGISTER_OP(\"StridedSliceAssign\")\r\n    .Input(\"ref: Ref(T)\")\r\n    .Input(\"begin: Index\")\r\n    .Input(\"end: Index\")\r\n    .Input(\"strides: Index\")\r\n    .Input(\"value: T\")\r\n    .Output(\"output_ref: Ref(T)\")\r\n    .Attr(\"T: type\")\r\n    .Attr(\"Index: {int32, int64}\")\r\n    .Attr(\"begin_mask: int = 0\")\r\n    .Attr(\"end_mask: int = 0\")\r\n    .Attr(\"ellipsis_mask: int = 0\")\r\n    .Attr(\"new_axis_mask: int = 0\")\r\n    .Attr(\"shrink_axis_mask: int = 0\")\r\n```\r\n\r\nAnd similarly, what is the output data type of such an op?", "I haven't ported StridedSliceAssign to new variables yet, so now you should\nfeed it the output of a VariableV2 op.\n\nOn Wed, Apr 19, 2017 at 4:48 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> @alextp <https://github.com/alextp> Given that reference data types are\n> going away, what input should I feed to ops such as StridedSliceAssign\n> that have a Ref(T) input type?\n>\n> The op registration in the C++ code looks like this:\n>\n> REGISTER_OP(\"StridedSliceAssign\")\n>     .Input(\"ref: Ref(T)\")\n>     .Input(\"begin: Index\")\n>     .Input(\"end: Index\")\n>     .Input(\"strides: Index\")\n>     .Input(\"value: T\")\n>     .Output(\"output_ref: Ref(T)\")\n>     .Attr(\"T: type\")\n>     .Attr(\"Index: {int32, int64}\")\n>     .Attr(\"begin_mask: int = 0\")\n>     .Attr(\"end_mask: int = 0\")\n>     .Attr(\"ellipsis_mask: int = 0\")\n>     .Attr(\"new_axis_mask: int = 0\")\n>     .Attr(\"shrink_axis_mask: int = 0\")\n>\n> And similarly, what is the output data type of such an op?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-295497238>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxT_876EI20PohW70qe6OGPMz6s1zks5rxp02gaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "Are there many other ops for which resource variables are not supported yet? If that's the case then maybe I should go about supporting the current variable API and wait until there is more complete support for resource variables. What do you think is the best approach for now?", "Maybe supporting both is a good idea, depends on what do you want to use\nthis for.\n\nOn Wed, Apr 19, 2017 at 5:05 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> Are there many other ops for which resource variables are not supported\n> yet? If that's the case then maybe I should go about supporting the current\n> variable API and wait until there is more complete support for resource\n> variables. What do you think is the best approach for now?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-295502321>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcvnBsjv_e5HqpbA-IZ17m413JWZks5rxqFJgaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "One important requirement is that my API supports control flow ops. Do resource-based variables support \"Switch\", \"Enter\", etc., with their underlying value op, or do we need something like \"RefSwitch\", \"RefEnter\", etc.?", "I believe Switch, Enter, etc. support resources, and you should not need the RefSwitch etc. versions. Also note that you'll need to use the TF_NewWhile/TF_FinishWhile methods to create while loops, which don't currently support creating Ref* ops anyway.", "I was working on porting the control_flow_ops module from the Python API because the TF_NewWhile/TF_FinishWhile methods are described as experimental and subject to change. Is there any specific reason that I should avoid porting the Python API approach, other than the implementation overhead? Thanks!", "You won't be able to create the cyclic back edge in the while loop using the TF_Operation methods (since you can't add or change the inputs to an operation after constructing it, unlike in Python), so you'll have to use the while loop methods instead. (While loops are the only graph structure that may contain a cycle, thus the API limitation.) Those methods shouldn't be marked experimental though, where did you see that?", "I see. I hadn't gotten to that part yet. And yes, you're right; the experimental flag has been removed or I simply remembered wrong. In that case, my question is whether I can add gradient computation code on top of the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the way in which gradients are computed in Python, but is that possible or does that also require adding inputs to ops after constructing them? Thanks!", "Also, what about the CondContext? Because I see that it also uses the \"op._update_input\" method in the Python code.", "I believe the gradients for loops in python add loop variables and stacks\nand other complicated things to the forward pass so I doubt you will be\nable to replicate the same code.\n\nOn Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I see. I hadn't gotten to that part yet. And yes, you're right; the\n> experimental flag has been removed or I simply remembered wrong. In that\n> case, my question is whether I can add gradient computation code on top of\n> the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the way\n> in which gradients are computed in Python, but is that possible or does\n> that also require adding inputs to ops after constructing them? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296819142>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp I was able to implement support for stacks and other things used in the Python code, but I'm not sure what to do about the \"op._update_input\" method that's used quite a lot apparently  for control flow ops. I wonder how difficult it would be to support that method in the C API given that constructed ops are supposed to be immutable. I imagine it shouldn't be too hard, but I don't know if you're willing to expose such functionality. If not, is there another way I could replicate it without constructing the proto defs in the Scala side, like the Python API does?", "It is possible to implement cond without rewiring (though the python\nimplementation uses rewiring to capture inputs).\n\nOn Mon, Apr 24, 2017 at 1:58 PM, Alexandre Passos <notifications@github.com>\nwrote:\n\n> I believe the gradients for loops in python add loop variables and stacks\n> and other complicated things to the forward pass so I doubt you will be\n> able to replicate the same code.\n>\n> On Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios <\n> notifications@github.com\n> > wrote:\n>\n> > I see. I hadn't gotten to that part yet. And yes, you're right; the\n> > experimental flag has been removed or I simply remembered wrong. In that\n> > case, my question is whether I can add gradient computation code on top\n> of\n> > the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the\n> way\n> > in which gradients are computed in Python, but is that possible or does\n> > that also require adding inputs to ops after constructing them? Thanks!\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 9150#issuecomment-296819142>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu>\n> > .\n> >\n>\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296820045>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfsDKB8OPrmhYh3ELApDpUSjJfAHks5rzQ0HgaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "It is not hard to expose the functionality right now (since it exists) but\nit will be very easy to misuse, which is why we do not really want to\nsupport it.\n\nIt might make sense to special-case your scala API though since you're\nalready doing the work. It's not my decision to make, though.\n\nOn Mon, Apr 24, 2017 at 2:02 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I was able to implement support for stacks and other things used in the\n> Python code, but I'm not sure what to do about the \"op._update_input\"\n> method that's used quite a lot apparently for control flow ops. I wonder\n> how difficult it would be to support that method in the C API given that\n> constructed ops are supposed to be immutable. I imagine it shouldn't be too\n> hard, but I don't know if you're willing to expose such functionality. If\n> not, is there another way I could replicate it without constructing the\n> proto defs in the Scala side, like the Python API does?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296821092>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeyctm9NWAMCmmrBqQIVfbqCF34dks5rzQ30gaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n", "Yeah I understand the misuse argument and I agree.\r\n\r\nOne potential solution might be to expose \"dangerous\" functionality in the C API using some convention (e.g., naming `Unsafe...`) and to clearly state that such functionality is unsafe, might change in the future, and that it is recommended not to use it.\r\n\r\nIf there is an alternative way to implement control flow ops and gradients using the current C API that's fine, but I think that missing support for something so important is quite limiting. While loops are very useful for RNNs and I would like to support them in the Scala API.\r\n\r\nThe rest of the functionality needed (e.g., stacks, etc.) for computing gradients over loops is exposed through the data flow ops. I haven't found something that's not supported yet on that front, but if I do I'll mention it.", "As a side note, my current plan for the Scala API is to port most of the Python API functionality and I'm slowly getting there. I have already added support for variables, op creation contexts (e.g., name scopes, defaults graph, colocation ops, etc.), sharing the underlying memory with TF session, among other things. And I have also developed a nice custom DSL for things like slicing and overriding operators for tensors, etc. So, it would be really helpful if I had access to all the necessary building blocks to support most of the functionality of the Python API.", "I'm in favor of temporarily exposing an experimental _update_input C method until we get the full while loop functionality working. @asimshankar and I discussed doing this as a similar stopgap to porting the Python API to use the C API\r\n\r\nMy original vision was for it to go in a separate header, maybe [c_api_internal.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_internal.h), and having projects that need it manually include the separate header so we don't need an additional package (too gross? :)). This will hopefully add more impedance to people using it unnecessarily. I'm also OK with putting it in the main header file though.\r\n\r\n@asimshankar what do you think? I can spin up a patch if this sounds ok.", "I like the idea of putting it in [c_api_internal.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_internal.h). I was thinking of it after my last comment.", "I think we also need access to \"op._add_input\" and to \"op._add_control_inputs\" to achieve the control flow functionality. Please correct me if I'm wrong!", "Actually, I think what will be easiest to implement in the C API is a function similar to TF_AddInput that takes the name of a (possibly nonexistent) operation instead of the TF_Operation* itself (AddNamedInput?). You can then create the TF_Operation* with that name later in order to create the cycle. We use something like this in the C API implementation of while loops, so it'll definitely be sufficient for the forward pass, although I haven't looked closely at adding the gradients yet. If you're interested, [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L1746) is where we use it in the C API.\r\n\r\nIt'll be trickier and probably more invasive to create an API like in Python where you can modify an already-created Operation (I don't think the underlying C++ API supports modification, and the operation creation code won't like partial inputs). It looks like _add_input isn't used in control_flow_ops.py, and if we end up needing a way to add control inputs to a TF_Operation* that should be easier. Let's start with just AddNamedInput for now though. @eaplatanios sound reasonable?", "@skye that sounds reasonable. Actually, I'm a bit confused as to everything involved in creating a \"cond\" or a \"while_loop\" in a graph. I get the high-level idea, but the Python code seems overly complicated to me for something like that and I'm wondering why. Would it be too much to ask for a brief summary of what ops get added to the graph for a \"cond\" and for a \"while_loop\" and how gradients should be computed for them? Then, I can make a fresh implementation of them using the named input idea you mentioned, without having to port the Python API code.", "Unfortunately I haven't fully figured out the while loop gradients logic myself... I talked to @asimshankar offline about it and we concluded that given the complexity of re-implementing this logic, it'll be better to wait until this functionality lands in the C API instead of re-implementing it in the Scala API (along with the additional downside of introducing the temporary API method). I'm currently working on switching the Python API to use the C API, and implementing the full while loop functionality will be a prerequisite for that, so it's on our roadmap and will hopefully not block you for long!", "@skye Sorry for the delayed response but I was traveling. That sounds reasonable, although it raises two questions: 1) should I go with the C API for the gradient support too, and 2) if I do so, when is it expected for the C++ gradients to be exposed to the C API and for the complete control flow functionality to be made available (i.e., with gradients support)?\r\n\r\nIn particular, I wrote some bindings for the C API \"AddGradients\" function but I have no idea how to import the C++ gradients from tensorflow/cc/gradients/*_grad.cc. How should I do that from an external project, without using bazel? And why are these not used by the C API by default, since they are available?", "I think @suharshs is working on exposing C++ gradients in the C API, and can comment on the timeline.", "Working on a change to include them into libtensorflow.so by default.", "@suharshs In case it helps, I found out that adding `\"//tensorflow/cc:cc_ops\"` and `\"//tensorflow/cc:grad_ops\"` to the `deps` of the `libtensorflow.so` cc_binary in the Bazel BUILD file does it.", "Thanks :) I think we currently link cc_ops in via the c_api dependencies so I am just adding grad_ops as a dependency to that.", "when will the TF_NewWhile be incorporated into a newer version of libtensorflow-cpu-windows-x86_64? The current version libtensorflow-cpu-windows-x86_64-1.2.0-rc0 has no TF_NewWhile. Thnx", "@alextp Going back to the original reason for this issue, would it be easy to expose the CPU kernels for tensors in order to allow for a numpy-like interface for languages that do not offer a numpy-equivalent library? I'm kind of stuck on this with the Scala API in that there isn't really a library out there that I could use for manipulating tensors on the Scala side (at least not as far as I know).", "@skye  Like @eaplatanios, I'm attempting to write a library (Clojure) on top of TF's C API. I'm at the point of trying to implement while loops and something like the `AddNamedInput` function you described would be very helpful. Has anything like that been added to the C API? I don't see it, but I'm hoping I just missed it.\r\n\r\nIf not, are there plans for that?\r\nOr, have there been any other advances relevant to implementing while loops?\r\n\r\nthanks", "Hi @bpiel, please use the TF_NewWhile/TF_FinishWhile functions to create while loops. The logic for while loops is quite tricky, especially when it comes to gradients, so we'd like all language bindings to use a shared implementation. We're currently working on getting while loop gradients implemented in the C API as well, stay tuned!", "@JimSEOW is the TF_NewWhile symbol still missing from the windows library?", "@skye Thanks for the quick reply! Unfortunately, the lack of support for gradients makes `TF_NewWhile`/`TF_FinishWhile` a show stopper for my needs (RNN/LSTM). I was hoping for, at least, some kind of stopgap. Also, even after the while-loop gradients are available, I may not be able to use them because I've found `TF_AddGradients` to be insufficient (I understand improvements are planned). As a result, I've already ported many gradients.\r\n\r\nI'm willing to put in extra work now to get something out the door -- even though it may (hopefully, it will) get torn out as more functionality is exposed by the C API. But in this case, it seems like I'm being blocked from filling in the gaps myself. **Please let me know if I'm misunderstanding the situation.**\r\n\r\nPlease don't read this too negatively. I appreciate all of the work going into TF. I want to help it succeed. It's just disappointing to run into this kind of obstacle after having spent a couple of (very fun) months working on this.\r\n\r\nthanks", "Your frustration is quite understandable! We are indeed blocking you, and I apologize -- hopefully this doesn't discourage you from continuing to contribute! I'm really impressed with the language bindings we're seeing from the community, and I wish we were able to keep up better.\r\n\r\nInstead of re-implementing the work we've done and are doing, would you be willing to help us with the C/++ implementations instead? (This is what I suggested to @eaplatanios as well, who's working on the Scala API.) You mentioned that TF_AddGradients is insufficient, so maybe this is something you could work on in the C++ API? Or something else. I'd be happy to help you get started with C API development, let me know if this works for you and we can talk more about concrete tasks/next steps.\r\n\r\nAlso regardless of whether you wanna fix it, what were your issues with TF_AddGradients?", "@skye Thanks for the invitation. Seriously. My only (but, large) hesitation is that my C/++ is very rusty. I've only gotten back into a little recently to mess with the TF JNI code. I need to think about this.\r\n\r\nI had two issues with `TF_AddGradients`. \r\n1. missing gradient implementations\r\n2. not handling the case where a variable is used by more than one op node\r\n", "Writing gradient functions might be a good way to get back into C++, and we could really use help on them. I think they should be strictly easier than dealing with JNI ;) Like I said, I'm happy to provide support. Think about, and maybe send me an email if you're potentially interested so we don't blow up this thread too much (my email should be on my github profile, I'm not sure I fully understand the visibility rules...).", "@skye I just wrote you."]}, {"number": 9142, "title": "Make bounding box operators consistent", "body": "Right now, the bounding box operators (tf.image.draw_bounding_boxes, tf.image.non_max_suppression and tf.image.sample_distorted_bounding_box) expect bounding boxes in the form of [ymin, xmin, ymax, xmax], with the origin (0,0) being the lower left corner of the image. But images themselves are tensors, and the pixel with index [0,0] in the tensor is in the top left, so bounding box coordinates are the opposite in the y direction to tensor indices.\r\n\r\nAdditionally, the operations tf.image.pad_to_bounding_box and tf.image.crop_to_bounding_box take coordinates in the form of [ymin, xmin, height, width], with the origin being the top-left corner, so the coordinates are inconsistent even within the image ops themselves (plus the parametrization of the bounding boxes is different, too).\r\n\r\nAnd the tf.image.crop_and_resize op doesn't specify what origin it uses (though I think it's bottom left too)\r\n\r\nI feel like this sort of inconsistency is unnecessarily confusing and a high risk for introducing errors.\r\n\r\nIt's especially bad since, if you supply bounding boxes the wrong way around to draw_bounding_boxes, it'll still draw them correctly (\r\n\r\nAll bounding box operators should use the same coordinate system and preferably the same parametrization, and preferably the coordinates should be consistent with image tensor indexing.", "comments": ["@shlens : Do you have any thoughts about this?", "I think this is a great proposal. It would be great to aim for a consistent API with regard to bounding box coordinates. I suspect that the default origin of top-left corner would make sense but I am open to suggestions.\r\n\r\nIf we do agree upon this then, we would need to update the API for `tf.image.draw_bounding_boxes`, `tf.image.non_max_suppression` and `tf.image.sample_distorted_bounding_box` in a manner that avoids (silently) breaking all users.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Marking this as \"Contributions Welcome\" in case someone wants to pick this up (and any API changes can be reviewed in the pull request). Thanks."]}, {"number": 8926, "title": "Image Distortions should be able to be applied to batches. [Feature Request?]", "body": "The tf.image distortion functions only accept a single image as an input whereas it would be much more useful to be able to place the distortion within your graph flow so any batches of images that pass through it get distorted too. I believe a current solution is as suggeested by @mrry [here on Stack Overflow](http://stackoverflow.com/questions/38920240/tensorflow-image-operations-for-batches) however it feels like, much like the other ops for images work, being able to take batches would be much more useful.\r\n\r\nIs the solution proposed by @mrry the accepted method or should the distortion functions be taking batches?", "comments": ["I made a similar changed to `resize_image_with_crop_or_pad `in https://github.com/tensorflow/tensorflow/pull/7369.", "@cancan101 that would be great, something like that. The map_fn method is looking promising for the mean time (as i'm short on time to go and patch TF myself) but I had some PIL/Pillow tools built already in Python so might look at trying to feed in and out of that (although would much rather prefer an in-graph solution!)", "@mrry, since you commented on the Stackoverflow, could you comment on this?", "I think it makes sense to make the image API more consistent (in a similar manner to what @cancan101 did), but I'll defer to anyone who feels more ownership over the image API (e.g. @martinwicke, @shlens?).", "See also #1029, #521. @cancan101 did #7369 fix #521?", "And https://github.com/tensorflow/tensorflow/issues/8465?", "Just to chime in, it seems that some image ops (e.g. `tf.image.random_brightness()`, `tf.image.random_contrast()`) allow batches, while others (e.g. `tf.image.random_saturation()`, `tf.image.random_hue()`) do not \u2013 it would make sense to either make all the image ops accept batches, or at least note on the documentation that they don't.\r\n\r\n(also, does the \"awaiting tensorflower\" tag mean a TF dev is working on this, or is this open to contributions?) ", "Stale tags. Fixed now.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "@itsmeolivia It is a little bit strange to close an issue with tag awaiting tensorflower. Tensorflower arrived to close just close the issue \ud83d\ude04? And now only the issue owner or a tensorflower could reopen it.", "@bhack Hey! I don't see what's strange.  It was not awaiting a tensorflower as per the tags.  It hasn't been commented on in almost two months so my original statement, \"Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you\"  still makes sense, especially in light of 1.2 coming out today \ud83d\ude04  ", "Was not commented cause we was awaiting a tensorflower tagged by a tensorflower. Cause is old I don't know if the owner (@jubjamie) is still interested on this topic. If he is not active anymore only  a tensorflower could reopen this issue.", "@martinwicke Can we considered this to be solved by [tf.map_fn()](https://stackoverflow.com/questions/38920240/tensorflow-image-operations-for-batches)?", "Nah, last comment was April 19, I removed awaiting label April 20, marked contributions welcome, no activity since then. \r\n\r\nmap_fn is not the perfect solution, but it's a solution. If you think it's valuable to leave open as contributions welcome, we can leave it open. There's not enough demand to prioritize this though, and nobody's lining up to do it (although it would be a fairly good entry-level contribution).", "You a right I've missed the contributions welcome label. I think that this issue was linked to many other issue about batch in image api some of that are closed in this closing run. Can we leave one issue opened for all realted to batching in image api?", "I think also an unique issue doesn't make sense cause resize_image_with_crop_or_pad in image api now support batch", "I'm very much still active! However I don't have time to work on the contribution. I raised it as an inconsistency that I thought the Devs would want to fix but if it's being left as contributions only for this long with no response then I guess we'll just have to wait until someone else has the same issue but with more time than me!", "```\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.2.0'\r\n>>> x = tf.placeholder(tf.float32, [None, 32, 32, 3])\r\n>>> tf.image.random_brightness(x, max_delta=0.125)\r\n<tf.Tensor 'adjust_brightness/Identity_1:0' shape=(?, 32, 32, 3) dtype=float32>\r\n>>> tf.image.random_contrast(x, lower=0.5, upper=1.5)\r\n<tf.Tensor 'adjust_contrast/Identity_1:0' shape=(?, 32, 32, 3) dtype=float32>\r\n>>> tf.image.random_flip_left_right(x)\r\n...\r\nValueError: 'image' (shape (?, 32, 32, 3)) must be three-dimensional.\r\n>>> tf.image.random_flip_up_down(x)\r\n...\r\nValueError: 'image' (shape (?, 32, 32, 3)) must be three-dimensional.\r\n>>> tf.image.random_saturation(x, lower=0.5, upper=1.5)\r\n...\r\nValueError: Shape must be rank 3 but is rank 4 for 'adjust_saturation/Slice' (op: 'Slice') with input shapes: [?,32,32,3], [3], [3].\r\n>>> tf.image.random_hue(x, max_delta=0.2)\r\n...\r\nValueError: Shape must be rank 3 but is rank 4 for 'adjust_hue/Slice' (op: 'Slice') with input shapes: [?,32,32,3], [3], [3].\r\n```\r\nNot all image ops support batch still, can we reopen? I raised a separate issue about making the image module API more consistent (#9318) but closed it because this issue existed. This has been on the back of my mind to contribute to because it would benefit a model I have but I haven't had time to look into it recently.", "Let's keep this open to track progress on this issue. ", "I know this has been briefly mentioned, but I've had great results just using ```tf.map_fn```.\r\n\r\n```\r\nhue = lambda x: tf.image.random_hue(x, 0.5)\r\nhue_imgs = tf.map_fn(hue, input_imgs)\r\n```\r\n\r\nThis seems to work on the whole batch. \r\n\r\n+1 to the fact that it is annoying how some ops (random_brightness) work on batches, but others (random_hue) do not :-/\r\n", "tf.image.random_flip_left_right() on batch would be fine indeed ...", "I would like to volunteer to fix these functions:\r\n- `flip_left_right`\r\n- `flip_up_down`\r\n- `random_flip_left_right`\r\n- `random_flip_up_down`\r\n\r\nThere are two I'm not familiar with but will also try if I think I can manage:\r\n- `transpose_image`\r\n- `rot90`", "The batch support for `adjust_hue` should have been enabled through PR #14187. And the batch support for `adjust_saturation` will likely be addressed if PR #14794 is accepted.", "I'm on Tensorflow 1.4.1 and I still got \"Shape must be rank 3 but is rank 4 for 'adjust_hue/Slice' (op: 'Slice') with input shapes: [?,256,256,3]\".", "@martin-gorner From the commit history https://github.com/tensorflow/tensorflow/commit/d5f0634312e9426414687c0b718c931a256a082f\r\n\r\nIt seems that this commit is only part of the `v1.5.0-rc1` and `v1.5.0-rc0` (not 1.4 or 1.4.1). I think the fix will be there once 1.5 is released.\r\n", "Hi,\r\nWould it be possible to make the distortion functions applicable not only on the entire batch, but also with a different seed per batch element ? This would lead to batches that have both the original and distorted examples, hence more diversity.\r\n\r\nSomething like:\r\n`inputs = tf.map_fn(lambda item: tf.image.random_flip_left_right(item[1], seed=item[0]), enumerate(inputs))`\r\nalthough it doesn't work this way, as there's an enum-to-tensor conversion.\r\n\r\nI was also thinking about computing the seed as a function of the image (e.g. sum of all pixels), but it doesn't work with seed as Tensor, only int."]}, {"number": 8884, "title": "8-bit quantized atrous conv2d op not supported", "body": "It looks like that 8-bit quantized atrous conv2d op is not supported. As per the latest API,  there are only 4 quantized ops supported namely quantized_conv2d, quantized_relu_x, quantized_max_pool and quantized_avg_pool.  Any target date by which 8-bit atrous conv2d op will be available?\r\n\r\n\r\n", "comments": ["@keveman, are there any plans for this feature in the short term. If not, please mark it contributions welcome."]}, {"number": 8820, "title": "Feature Request: Accelerate TensorFlow core on FPGA - How?", "body": "Consider the two following hardware scenarios:\r\n1) Linux running on x86 w/ FPGA fabric connected via PCIe\r\n2) Linux running on Arm A53 with AXI i/f to FPGA fabric (Think Xilinx Zynq)\r\n\r\nHow could the tensorflow core be accelerated for these scenarios? \r\nFPGA vendors do offer OpenCL binaries for running OpenCL APIs to parallelize computations.\r\n\r\nForgive me, I am a bit ignorant, still learning in this area, but I am intrigued by the future possibility of this, and would love to help in anyway I can.", "comments": ["This would be a great question for StackOverflow! I'm not aware of people who have talked about how to do this already, but that would be the best place to look. Thanks!", "Andrew,\n\nIt was more of a feature request than a question... Can you make sure that\nit makes it to someone at Google? Although, they may have already thought\nabout it, or be doing it behind the scenes...\n\nOn Fri, Mar 31, 2017 at 11:19 AM, Andrew Selle <notifications@github.com>\nwrote:\n\n> Closed #8820 <https://github.com/tensorflow/tensorflow/issues/8820>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8820#event-1024541926>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO_Gp5LYl7TPRGgMDbhyzSOzi6PSUQBtks5rrUOfgaJpZM4Mtp4A>\n> .\n>\n", "I'm interested in this too. Maybe it would be better to implement an IP in fpga optimized for tensorflow, rather then a generic OpenCL IP.", "I'm also interested, but from the aspect of creating an accelerator and then using the hooks in Tensorflow to access it.", "FYI, Xilinx has DNN accelerators now. Currently they are integrated with Caffe. I'm sure tensorflow is on the roadmap. \nhttp://www.datacenterknowledge.com/archives/2016/11/14/xilinx-unleashes-fpga-accelerator-stack-supporting-caffe-openstack/\n\n\nSent from my iPhone\n\n> On Jun 9, 2017, at 3:04 PM, alexshirley <notifications@github.com> wrote:\n> \n> I'm also interested, but from the aspect of creating an accelerator and then using the hooks in Tensorflow to access it.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Hi @aselle ,\r\n\r\nHugh Perkins has created [Coriander](https://github.com/hughperkins/coriander-dnn) that could run NVIDIA\u00ae CUDA\u2122 code on OpenCL 1.2 devices including FPGA devices. You might want to take a look if that suits your need to connect your Deep Learning software to OpenCL 1.2 devices. Kindly attribute his name and his contribution in case if you plan to use his work.", "@aselle Tensorflow is always lauded for it's portability. However, even if one was to use @viper7882's solution they'd still be constrained to using gpus, traditional cpus, and commonly used FPGAs with OpenCL vendor libraries. This is not true general portability.\r\n\r\nWould it be possible to add a custom device feature/interface that allows us to add unique devices, treating the tensor forward and backward computations within the context as a giant black box, that can be fed values via grpc serialization. Or the ability to pass a custom compiled executable/binary and link it in to the rest of the graph (not just a tensorflow lambda func). This would greatly help the applied deep learning community so we can stop pulling values out of the runtime graph, processing them, and reinserting it into the computation graph.\r\n\r\nHere are just several use cases based on projects I'm working on now:\r\n* Integration of tensorflow into high throughput/low latency real time systems where we want to define custom caching logic for only a few specified ops to meet unique run time criteria.\r\n* Usage of tensorflow to interface with custom ASICs without OpenCL libraries\r\n  * usage of ASICs specific to performing fast neural memory access in NTMs/NDCs/etc\r\n  * usage of analog devices with a digital interface such as an Analog neural network (sub)graph or analog transmitter/receiver \r\n* Easy ability to use tensorflow autodifferentiation with more abstract concepts in NN theory (such as deep function machines)\r\n* Easy ability to add distributed (something like MPI) computation for a specific tensor. FireCaffe/DeepScale have the functionality.\r\n\r\nMy basic understanding is that the device contexts solely serve to define compile logic of the tensors in the context and properly link the compiled nodes with the rest of the graph. I'd implement a PR myself, but I only have a good grasp on the grpc serialization logic. \r\n\r\nWish I understood the graph compilation portion better :(", "Would it be feasible to swap out specific parts of TensorFlow and accelerate it e.g. with an FPGA implementation? Like creating FPGA-accelerated ops as @kinsumliu suggests under #12538?\r\n\r\nWe've built [Hastlayer](https://hastlayer.com), which automatically transforms .NET programs into equivalent FPGA implementations. What we could thus do is to see how implementing some TF features with Hastlayer on an FPGA would work.", "Google designed an ASIC with which they've done this called the Tensorflow Processing Unit (TPU). There's a fair amount of information on it that has been publicly released.\r\n\r\n* https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view\r\n* https://www.nextplatform.com/2017/04/05/first-depth-look-googles-tpu-architecture/\r\n* https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu\r\n\r\nYou could use this as a starting point to design soft IP for an FPGA that could support a handful of tensorflow Ops you'd like to speed up, then write the TF device handlers and ops.\r\n\r\nYou could potentially even design the software interface with Xilinx's OpenCL macros (and maybe even the IP) to make the tensorflow Ops simpler to implement so you don't need to write HDL manually. There's also reconfigure.io (program an FPGA with Go). It sounds like Hastlayer that @Piedone mentiond does something similar as well.", "Yes, Hastlayer works in a very similar way @stevefox .\r\n\r\nWhat do you think, which part of TF would be most interesting to swap out with an accelerated implementation? It seems to me that the most important piece of the TPU is the matrix multiplier, a somewhat similar feature to GPUs.", "I was able to get Tensorflow Lite compiled for the Zynq and did a quick benchmark. I typed up my results here: https://github.com/nlbutts/tflite_zynq\r\n\r\nMy next step was looking at accelerating the convolution operation, which is where the software spends most of it's time when using Mobilenet. ", "@nlbutts did you make any progress? Is anyone aware of a solution that interfaces Tensorflow with FPGAs?\r\n\r\nThere are some old attempts to add new devices to Tensorflow, but I did not see anything about FPGAs in particular.", "To those interested. I would like to share some thoughts, and some info.\n\nFull disclosure, I opened this feature request about 1.5 years ago, back\nwhen I was more clueless. Also, I have worked at Xilinx for the past 4\nyears. Just 3 months ago I started as a \"Machine Learning Engineer\", so now\nI am very aware of Xilinx's approach in this space.\n\nMachine Learning/Computer Vision is a very fast paced market. It seems like\nevery year there is a new state of the art network architecture. Given that\nan FPGA design cycle can take 6 months to a year, especially when a company\nis lacking FPGA know-how, Xilinx provisioned a hardware and software team\nto design a \"soft\" neural network accelerator core. The core is meant to be\ngeneral purpose, in that it can accelerate most network architectures. We\nare calling it \"XDNN\". The software is getting tagged as \"xfDNN\". Xilinx\nFast DNN. We are so great at coming up with names!\n\nThe core is designed specifically for accelerating inference, and it takes\nadvantage of fixed point arithmetic to squeeze more compute into the FPGA\nfabric/DSPs. This means trained networks must undergo an offline\nquantization process, to go from float32 to int8. There is various research\nshowing that this is an effective method to achieve faster inference with\nminimal loss in accuracy (1-2%).\n\nThe core accelerates convolutions, pooling (Max/Average), eltwise add, and\nReLU. We run the majority of the network on the FPGA, and leave the final\nfully connected, or region layers to the CPU. (There was no gain in adding\nhardware support for these final layers, since FC layers are being used\nless and less, and region layers can be quite custom).\n\nCurrently, we only support cloud: \"Amazon AWS EC2 F1\", and on-premise:\n\"VCU1525\". However we are working on bringing the core into Zynq\nUltraScale+ devices, as well as other cloud providers such as Nimbix.\n\nCheck out our repo if you are interested: https://github.com/Xilinx/ml-suite\n\nAlso, you can kick the tires directly at:\nhttps://aws.amazon.com/marketplace/pp/B077FM2JNS\n\nRight now, the documentation on github isn't great, but we are about to\npush a new release with much improved docs. There is an EA branch that\nshows a lot of our software. My job right now is to enhance our\ndocumentation, and also add usability features (Ease of use Python stuff).\nSo feel free to complain to me, it is my job. We have some really nice\njupyter notebook tutorials coming. I can be reached at\nb.lozano.havoc@gmail.com or bryan.lozano@xilinx.com.\n\nNow we come to framework support...\n\nInitially, we did some experiments, enabling caffe to directly call our\naccelerator. We got it working, but it was clear that there would be a lot\nof work to add support for various networks, and to support this across\nCaffe/Tensorflow/MXNET. Tensorflow was still evolving when we started. We\nsimply didn't have enough software people to support that.\n\nWhat we settled on is a compiler frontend that takes your network\ndefinition, and weights. The compiler does some optimization (layer\nmerging), then generates a schedule of commands to accelerate a given\nnetwork. This way, the majority of our software can be framework agnostic,\nbut the compiler will have to \"multi-lingual\". The compiler reads prototxt,\nor frozen tensorflow graphs for example.\n\nI would still love it, if tensorflow had hooks to directly run on the FPGA,\nbut I don't see it happening anytime soon. It probably wouldn't be that\nbad, especially if Tensorflow can make OpenCL calls.\n\nAnyways, if you want more info, don't hesitate to reach out.\n\nAgain, I can be reached at b.lozano.havoc@gmail.com or\nbryan.lozano@xilinx.com\n\nThanks,\nBryan\n\nOn Fri, Jul 6, 2018 at 1:11 PM, GDG <notifications@github.com> wrote:\n\n> @nlbutts <https://github.com/nlbutts> did you make any progress? Is\n> anyone aware of a solution that interfaces Tensorflow with FPGAs?\n>\n> There are some old attempts to add new devices to Tensorflow, but I did\n> not see anything about FPGAs in particular.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8820#issuecomment-403134168>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO_Gp91PZcBHOvm6xGlQuTOMPtmufXmdks5uD8SLgaJpZM4Mtp4A>\n> .\n>\n", "Now, I add a fpga device in tensorflow, and I can call the kernel of fpga device to perform an addition operation\uff0cbut the fpga device is also a virtual device ,I do not realize API to collect the info of fpga, I will do it later.\r\nthe [code:](https://github.com/zhaohb/tensorflow/commit/7456ddb8d38e0313c90ff582d9a73ca5bdcaccc9)\r\nhttps://github.com/zhaohb/tensorflow/commit/7456ddb8d38e0313c90ff582d9a73ca5bdcaccc9"]}]