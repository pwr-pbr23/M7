[{"number": 28079, "title": "Moved strided slide ops from reference to its own.", "body": "This is to remove the gemmlowp dependency.", "comments": ["@karimnosseir , I am trying to port strided slice to Micro kernel for our HiSi board, For this i wanted to remove the  gemmlowp dependency first.\r\n\r\nRegards\r\nAmit", "@karimnosseir , can you please review the changes, a my other changes are dependent on these.\r\n\r\nRegards\r\nAmit", "@petewarden thanks for approving the PR, there was one sanity build failure as the names in the BUILD file was not sorted, i have fixed it, kindly approve it again. Sorry for the trouble.\r\n\r\nAlso i want to port strided slide along with mul and add (for batch norm) on TFlite micro, let me know your views on the same.\r\n\r\nRegards\r\nAmit", "@petewarden can you please re approve the PR.\r\n\r\nRegards\r\nAmit", "@petewarden , can you pls have alook at the PR and re approve it.\r\n\r\nRegards\r\nAmit", "@karimnosseir , sorry to bug you again, there was one check lint issue which is resolved now, can you please re-approve the PR.\r\n\r\nRegards\r\nAmit", "Changes are committed waiting for auto merge to happen.", "Seems automerge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 28078, "title": "Take a long time to initialize the model?", "body": "Hey, I met a strange problem when initializing a simple CNN model. The torsor is as below. Every time during initialization, the code will be stuck in the last row (code), which may cost 10-15 minutes.\r\n\r\nP.S. My machine owns 2 CPU and 10G memory. The version of TensorFlow is 1.8.0-cpu.\r\n\r\n![image](https://user-images.githubusercontent.com/7310697/56581827-6e6e3a80-6608-11e9-9036-d1f89c0472ac.png)\r\n\r\nAny ideas?", "comments": ["@JackieTseng Please fill out the issue template otherwise it will not be possible to look into your issue. \r\nA reproducible example would be very helpful as well.\r\n\r\nIf you think it could an issue in your code feel free to post on Stack Overflow under the `TensorFlow` tag."]}, {"number": 28077, "title": "Set appropriate library path for mpich", "body": "By default, `mpi.h, mpio.h, mpicxx.h` is placed under `/usr/include/mpi`\r\non Linux. Tested on Ubuntu 18.04 and 16.04.\r\n\r\nSigned-off-by: Yunjae Lee <lyj7694@gmail.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28077) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28077) for more info**.\n\n<!-- ok -->", "Looks like this varies a lot by distro. [Debian does not put files in /usr/include/mpi](https://packages.debian.org/buster/amd64/libmpich-dev/filelist), and it looks like a few others are similar.  And I'm guessing the current configure.py is tuned for installs from source.\r\n\r\nMPI_HOME doesn't make much sense if we're looking for system headers. So maybe the current from-source collection logic should stay, but we could add a second path for finding system headers starting from set_mpi_home.", "I see, forget about this request. I'll try to add the second path."]}, {"number": 28076, "title": "deprecation warning in confusion_matrix.py ", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nOS: Ubuntu 18.04.2\r\nTensorflow-gpu: 2.0.0alpha0\r\n\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0423 15:59:53.297839 140450503571264 deprecation.py:323] From <path>/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py:194: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0423 15:59:53.298299 140450503571264 deprecation.py:323] From <path>/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py:195: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.", "comments": ["These have been fixed on master, next update of alpha/tf2.0 will have them fixed. Thank you for notifying us about them"]}, {"number": 28075, "title": "TF2.0.0-alpha0  'Tensor' object has no attribute 'numpy'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):colab\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0.0-alpha0\r\n- Python version:3.6\r\n\r\n'Tensor' object has no attribute 'numpy' when use tf.data \r\n\r\nds_train = tf.data.Dataset.from_tensor_slices((train_x_paths,train_y_labels))\r\ndef readFile(path,label):\r\n  return tf.io.read_file(path).numpy(),label\r\n\r\nds_train.map(readFile)\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-94-7fc15313765a> in <module>()\r\n----> 1 ds_train.map(readFile)\r\n\r\n10 frames\r\n<ipython-input-92-a8cf88784b63> in readFile(path, label)\r\n      1 def readFile(path,label):\r\n----> 2   return tf.io.read_file(path).numpy(),label\r\n      3 \r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'", "comments": ["@happiness-ai Do you have eager execution enabled? This seems better suited for Stack Overflow", "@Carmezim TF2.0.0-alpha0 ", "This question is better asked on StackOverflw. Since it is not a bug or feature request. There is also a larger community that reads questions there. Thanks!", "@gadagashwini should the issue be closed then?", "@CarbonComputed @Carmezim Same issue. Since already tf2.0, why do say open eager execution? Does not it already support eager execution?", "Do you solve it? I have the same problem with eager execution already enabled. "]}, {"number": 28074, "title": "Metrics for multi-label classification for using with tf.keras", "body": "Top-K Metrics are widely used in assessing the quality of Multi-Label classification. `tf.metrics.recall_at_k` and `tf.metrics.precision_at_k` cannot be directly used with `tf.keras`! Even if we wrap it accordingly for `tf.keras`, In most cases it will raise **NaNs** because of numerical instability. Since we don't have out of the box metrics that can be used for monitoring multi-label classification training using `tf.keras`. I came up with the following plugin for `Tensorflow 1.X` version. This can also be easily ported to `Tensorflow 2.0`.\r\n\r\n```python\r\nimport tensorflow as tf\r\nK = tf.keras.backend\r\n\r\nclass MetricsAtTopK:\r\n    def __init__(self, k):\r\n        self.k = k\r\n\r\n    def _get_prediction_tensor(self, y_pred):\r\n        \"\"\"Takes y_pred and creates a tensor of same shape with 1 in indices where, the values are in top_k\r\n        \"\"\"\r\n        topk_values, topk_indices = tf.nn.top_k(y_pred, k=self.k, sorted=False, name=\"topk\")\r\n        # the topk_indices are along last axis (1). Add indices for axis=0\r\n        ii, _ = tf.meshgrid(tf.range(tf.shape(y_pred)[0]), tf.range(self.k), indexing='ij')\r\n        index_tensor = tf.reshape(tf.stack([ii, topk_indices], axis=-1), shape=(-1, 2))\r\n        prediction_tensor = tf.sparse_to_dense(sparse_indices=index_tensor,\r\n                                               output_shape=tf.shape(y_pred),\r\n                                               default_value=0,\r\n                                               sparse_values=1.0,\r\n                                               validate_indices=False\r\n                                               )\r\n        prediction_tensor = tf.cast(prediction_tensor, K.floatx())\r\n        return prediction_tensor\r\n\r\n    def true_positives_at_k(self, y_true, y_pred):\r\n        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)\r\n        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))\r\n        return true_positive\r\n\r\n    def false_positives_at_k(self, y_true, y_pred):\r\n        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)\r\n        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))\r\n        c2 = K.sum(prediction_tensor)  # TP + FP\r\n        false_positive = c2 - true_positive\r\n        return false_positive\r\n\r\n    def false_negatives_at_k(self, y_true, y_pred):\r\n        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)\r\n        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))\r\n        c3 = K.sum(y_true)  # TP + FN\r\n        false_negative = c3 - true_positive\r\n        return false_negative\r\n\r\n    def precision_at_k(self, y_true, y_pred):\r\n        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)\r\n        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))\r\n        c2 = K.sum(prediction_tensor)  # TP + FP\r\n        return true_positive/(c2+K.epsilon())\r\n\r\n    def recall_at_k(self, y_true, y_pred):\r\n        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)\r\n        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))\r\n        c3 = K.sum(y_true)  # TP + FN\r\n        return true_positive/(c3+K.epsilon())\r\n\r\n    def f1_at_k(self, y_true, y_pred):\r\n        precision = self.precision_at_k(y_true=y_true, y_pred=y_pred)\r\n        recall = self.recall_at_k(y_true=y_true, y_pred=y_pred)\r\n        f1 = (2*precision*recall)/(precision+recall+K.epsilon())\r\n        return f1\r\n```\r\n\r\n```python\r\n### Usage:\r\nmetrics = MetricsAtTopK(k=5)\r\n# model definition\r\n# ...\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \r\n                                                                     metrics.true_positives_at_k, \r\n                                                                     metrics.false_positives_at_k,\r\n                                                                     metrics.false_negatives_at_k,\r\n                                                                     metrics.recall_at_k,\r\n                                                                     metrics.precision_at_k,\r\n                                                                     metrics.f1_at_k,\r\n                                                                    ]\r\nmodel.fit(...)  # as usual\r\n \r\n```\r\n\r\nIs this something which we can integrate to Tensorflow? If so I will be glad to open up a **Pull Request**", "comments": ["@Abhijit-2592 I have opened a pull request already about adding multilabel classification, but i am not getting correct location to add those lines so that they can work properly", "Can you link the pull request?", "#28204 ", "@Abhijit-2592 Have you tried the new 2.0 metrics? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics.py#L1072 Please take a look and let me know if they would work for you. We have implementations of precision at K and recall at K. They are called `Precision` and `Recall` and `top_k` is accepted as a parameter in these metrics.", "Closing this issue for now, please feel free to re-open if this isn't resolved.", "@pavithrasv I'd like to reopen the issue if possible. My reasons are as follows:\r\n\r\n1. the metric has an annoying [control dependency](https://github.com/tensorflow/tensorflow/blob/6fc74b6c43c67bc2e387c9afa8189529b15d5a51/tensorflow/python/keras/utils/metrics_utils.py#L321):\r\n\r\n```python\r\nwith ops.control_dependencies([\r\n      check_ops.assert_greater_equal(\r\n          y_pred,\r\n          math_ops.cast(0.0, dtype=y_pred.dtype),\r\n          message='predictions must be >= 0'),\r\n      check_ops.assert_less_equal(\r\n          y_pred,\r\n          math_ops.cast(1.0, dtype=y_pred.dtype),\r\n          message='predictions must be <= 1')\r\n  ]):\r\n```\r\n\r\nwhy is this annoying? if using a sigmoid cross entropy for the loss (as multi-labels are independent), `tf` offers the function:\r\n\r\n```\r\ntf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\r\n```\r\nSince this function [applies](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/nn_impl.py#L193-L240) a sigmoid transformation to the logits (output of the model), then the user shouldn't have in their final layer a sigmoid activation. Otherwise it is `sigmoid(sigmoid(...))`. Since this is the case, values are not gaurenteed to be between `[0, 1]`, especially when training first begins. So `Precision` (and other similar `tf.keras.metrics`) are not even able to be included as this will throw an error ending training.\r\n\r\n2. in the [precision](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics.py#L1206) (and similar) metrics, the thing of interest `multi_label` is called with the default value (`False`):\r\n\r\n\r\n```python\r\n# inside Precision update_state\r\nreturn metrics_utils.update_confusion_matrix_variables(\r\n        {\r\n            metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\r\n            metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives\r\n        },\r\n        y_true,\r\n        y_pred,\r\n        thresholds=self.thresholds,\r\n        top_k=self.top_k,\r\n        class_id=self.class_id,\r\n        sample_weight=sample_weight) # <---- multi_label is not set to True, nor do we as users\r\n        # have the option to do so\r\n\r\n# function signature\r\ndef update_confusion_matrix_variables(variables_to_update,\r\n                                      y_true,\r\n                                      y_pred,\r\n                                      thresholds,\r\n                                      top_k=None,\r\n                                      class_id=None,\r\n                                      sample_weight=None,\r\n                                      multi_label=False, # <---- defaults to false\r\n                                      label_weights=None):\r\n```\r\n\r\n\r\ncollectively, this means that `multi_label` metrics are not supported via `tf.keras` api.\r\n\r\n\r\nWhile the `update_confusion_matrix_variables` function is great (and the associated structure), to an outsider who might want to contribute / track down why it doesn't work with their code, it is could be daunting.\r\n\r\nPerhaps `tf.keras.metrics` could single out multilabel metrics?\r\n\r\ne.g. based on the [subclassing Metric documentation](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric#add_weight):\r\n\r\n```python\r\nclass MultiLabelMacroSpecificity(tf.keras.metrics.Metric):\r\n    \r\n    def __init__(self, name='multi_label_macro_specificity', threshold=0.5, **kwargs):        \r\n        super(MultiLabelMacroSpecificity, self).__init__(name=name, **kwargs)\r\n        self.specificity = self.add_weight(name='mlm_spec', initializer='zeros')        \r\n        self.threshold       = tf.constant(threshold)\r\n\r\n        # replace this with tf confusion_matrix utils\r\n        self.true_negatives  = self.add_weight(name='fn', initializer='zeros')\r\n        self.false_positives = self.add_weight(name='fp', initializer='zeros')\r\n    \r\n    def update_state(self, y_true, y_pred):\r\n        \r\n        # Compare predictions and threshold.        \r\n        pred_is_pos  = tf.greater(tf.cast(y_pred, tf.float32), self.threshold)            \r\n        # |-- in case of soft labeling        \r\n        label_is_pos = tf.greater(tf.cast(y_true, tf.float32), self.threshold)                \r\n        label_is_neg = tf.logical_not(tf.cast(label_is_pos, tf.bool))\r\n        \r\n        self.true_negatives.assign_add(tf.reduce_sum(tf.cast(label_is_neg, tf.float32)))\r\n        self.false_positives.assign_add(\r\n            tf.reduce_sum(tf.cast(tf.logical_and(pred_is_pos, label_is_neg), tf.float32))\r\n        )\r\n        \r\n        tn = self.true_negatives\r\n        fp = self.false_positives\r\n        specificity = tf.div_no_nan(tn, tf.add(tn, fp))\r\n        self.specificity.assign(specificity)\r\n        return specificity\r\n    \r\n    def result(self):\r\n        return self.specificity        \r\n```", "Thank you, i see you have opened another issue for the pending features, will reply on that."]}, {"number": 28073, "title": "TensorFlow 2.0: Allow simple tensorboard summary usage", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0 alpha 0\r\n- Are you willing to contribute it (Yes/No): willing yes, but I don't have enough experience with the TF internals.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs to my current research and a [stackoverflow question on writing image summaries](https://stackoverflow.com/questions/55421290/tensorflow-2-0-keras-how-to-write-image-summaries-for-tensorboard), I think there is no user friendly way of writing arbitrary summaries from a TensorFlow 2.0 Keras model.\r\n\r\nIn more complex models, a developer might want to write scalars, histograms or simply images generated from tensors which are created in the middle of the network. With tf.layers and estimators, I could easily take such a tensor and write it to TensorBoard with tf.summary. At the moment there does not seem to be an equally easily usable way to do this with the Keras API.\r\n\r\nFrom the user perspective, I imagine a special TensorBoard logging layer, that can be chained into the model. This layer would then handle creating the summary writer and getting/maintaining the current model step. \r\nE.g. when using `Sequential`, it could look like:\r\n```\r\nmodel = keras.models.Sequential([\r\n\tkeras.layers.Flatten(input_shape=(28, 28), name=\"flatten\"),\r\n\tkeras.layers.Dense(128, activation='relu', name=\"dense1\"),\r\n\tTensorBoardLoggingLayer(\"my-logging-dir\", lambda input: tf.summary.scalar(\"first-value\", input[0]))\r\n\tkeras.layers.Dense(10, activation='softmax', name=\"dense2\")\r\n])\r\n```\r\n\r\nFurthermore, if the user creates a custom layer they could extend from this `TensorBoardLoggingLayer` and use its handling of the summary writer to easily write summaries of internal parts of their layer.\r\n\r\n**Will this change the current api? How?**\r\nIt would add a new layer but not effect other, existing API.\r\n\r\n**Who will benefit with this feature?**\r\nEverybody that wants to debug a model or get more insights into it. This is especially important when you create custom layers (e.g. implementing new stuff from/for papers) or if you want to debug e.g. images of inner results of your network.\r\n\r\n**Any Other info.**\r\nI think it is crucial to have an easy to use possibility to debug and track our networks with TensorFlow. This was/is possible with the tf.layers / tf.estimtators APIs. In my view it is a must to have this in the TF 2.0 ecosystem.", "comments": ["You can use `tf.keras.layers.Lambda` to inject arbitrary expressions as a Layer, and collect all the created summaries.", "Not really, how would I get the current step of the model. I need the step to be able to write a summary. My point is that there should be a layer handling the step and summary writer for me, so not everybody has to do it on their own.", "This feature is being worked on, but is currently blocked on some functionality from TensorBoard. @omalleyt12 will update the thread when we have made progress here.", "> You can use `tf.keras.layers.Lambda` to inject arbitrary expressions as a Layer, and collect all the created summaries.\r\n\r\nI was using tensor board conv model with tf.layers , now when I switch to tf.keras and try publishing weights to logs using summary, I am left with 2 options and both have issues \r\n\r\n1)Use call back to store weights - It's working upto storing weights but it has no provision of mentioning plugin name in it ( like my old model). Without plugin name, I can not identify data to be presented by mu custom plugin on tensorboard \r\n2) Use callback (on_test_batch_end) to call my summary method, with this I can get a chance to store my  plugin name but when  i try storing the data from summary in logs it gives an error \r\n\r\n'Tensor' object has no attribute 'value'\r\n\r\nDo we have any way to store weights using callback and also store with them the plugin name ?", "@andreas-eberle \r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999"]}, {"number": 28072, "title": "[TF 2.0] Inconsistency when timing operations", "body": "I have been trying to benchmark performance of the convolutions operations with speed-up such as depth-separable convolutions[1] and rank-separable convolutions[2]. I'm using the code pasted below:\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Define a scenario\r\nIMAGE_SIZE = 320\r\nCHANNELS_BATCH_SIZE = 2048  # channels * batch_size\r\nREPEATS = 200\r\nSKIP = 10        \r\n\r\n#Build various operations        \r\nclass build_normal_ops(tf.keras.Model):\r\n    def __init__(self, KERNEL_SIZE, channels):\r\n        super(build_normal_ops, self).__init__()\r\n        self.normal = tf.keras.layers.Conv2D(channels,KERNEL_SIZE,padding=\"same\")\r\n        \r\n    def call(self,x):\r\n        out = self.normal(x)\r\n        return out\r\n    \r\nclass build_rank_ops(tf.keras.Model):\r\n    def __init__(self, KERNEL_SIZE, channels):\r\n        super(build_rank_ops, self).__init__()\r\n        self.rs1 = tf.keras.layers.Conv2D(channels,(KERNEL_SIZE,1),padding=\"same\")\r\n        self.rs2 = tf.keras.layers.Conv2D(channels,(1,KERNEL_SIZE),padding=\"same\")\r\n        \r\n    def call(self,x):\r\n        out = self.rs1(x)\r\n        out = self.rs2(out)\r\n        return out    \r\n    \r\nclass build_depth_ops(tf.keras.Model):\r\n    def __init__(self, KERNEL_SIZE, channels):\r\n        super(build_depth_ops, self).__init__()\r\n        self.depthwise = tf.keras.layers.DepthwiseConv2D(KERNEL_SIZE,padding=\"same\")\r\n        self.pointwise = tf.keras.layers.Conv2D(channels,1,padding=\"same\")\r\n        \r\n    def call(self,x):\r\n        out = self.depthwise(x)\r\n        out = self.pointwise(out)        \r\n        return out\r\n       \r\ndef build_ops_all(channels, kernel_size):    \r\n    normal = build_normal_ops(kernel_size,channels)\r\n    rank = build_rank_ops(kernel_size,channels)\r\n    depth = build_depth_ops(kernel_size,channels)    \r\n    return normal, rank, depth \r\n\r\ndef time_ops(ops: tf.Operation):\r\n    # Benchmark operation\r\n    with tf.device(\"GPU\"):\r\n        image = tf.random.normal(shape=[batch_size, IMAGE_SIZE, IMAGE_SIZE, channels], dtype=tf.float32)\r\n        for i in range(REPEATS+SKIP):\r\n            if i == SKIP:\r\n                start = time.time() #Don't time initial runs\r\n            _ = ops(image)\r\n        end = time.time()\r\n        chk = np.round((end - start) / REPEATS * 1000,2)\r\n    return chk \r\n\r\nif __name__ == '__main__':\r\n    #Benchmark with various channel sizes\r\n    for channels in [64,128,256,512]:\r\n        # adjust batch_size so gpu doesn't run out of memory\r\n        batch_size = CHANNELS_BATCH_SIZE // channels        \r\n\r\n        #Benchmark with various kernel sizes\r\n        for param in [3,5,7]:                \r\n            normal, rank_separable, depth_separable = build_ops_all(channels, param)\r\n            print('Channels:', channels, 'kernel_size:', param)                   \r\n            \r\n            time_normal = time_ops(normal)\r\n            time_rank = time_ops(rank_separable)\r\n            time_depth = time_ops(depth_separable)\r\n\r\n            print(\"Normal method: {}ms \\t Rank-separable method: {}ms \\t Depth-separable method: {}ms \\n\".format(time_normal, time_rank, time_depth))\r\n        print('\\n')\r\n```\r\n\r\nThis results in the following output:\r\n```\r\nChannels: 64 kernel_size: 3\r\nNormal method: 5.31ms \t Rank-separable method: 22.44ms \t Depth-separable method: 0.59ms \r\n\r\nChannels: 64 kernel_size: 5\r\nNormal method: 35.56ms \t Rank-separable method: 27.95ms \t Depth-separable method: 0.65ms \r\n\r\nChannels: 64 kernel_size: 7\r\nNormal method: 39.56ms \t Rank-separable method: 33.5ms \t Depth-separable method: 0.63ms \r\n\r\n\r\n\r\nChannels: 128 kernel_size: 3\r\nNormal method: 13.7ms \t Rank-separable method: 30.75ms \t Depth-separable method: 0.6ms \r\n\r\nChannels: 128 kernel_size: 5\r\nNormal method: 16.85ms \t Rank-separable method: 41.63ms \t Depth-separable method: 0.54ms \r\n\r\nChannels: 128 kernel_size: 7\r\nNormal method: 61.79ms \t Rank-separable method: 46.81ms \t Depth-separable method: 0.58ms \r\n\r\n\r\n\r\nChannels: 256 kernel_size: 3\r\nNormal method: 12.75ms \t Rank-separable method: 47.77ms \t Depth-separable method: 0.56ms \r\n\r\nChannels: 256 kernel_size: 5\r\nNormal method: 137.72ms \t Rank-separable method: 64.87ms \t Depth-separable method: 0.57ms \r\n\r\nChannels: 256 kernel_size: 7\r\nNormal method: 159.38ms \t Rank-separable method: 64.92ms \t Depth-separable method: 0.61ms \r\n\r\n\r\n\r\nChannels: 512 kernel_size: 3\r\nNormal method: 23.17ms \t Rank-separable method: 93.72ms \t Depth-separable method: 0.57ms \r\n\r\nChannels: 512 kernel_size: 5\r\nNormal method: 86.4ms \t Rank-separable method: 119.05ms \t Depth-separable method: 0.55ms \r\n\r\nChannels: 512 kernel_size: 7\r\nNormal method: 537.36ms \t Rank-separable method: 130.35ms \t Depth-separable method: 0.55ms\r\n```\r\n\r\nMy concerns are\r\n1) Inconsistent running times with kernel size 5 and increasing channel size. \r\n2) Constant running times with depth-separable method.\r\nIn pytorch2 there are certain flags that need to be called/set to true to time operations. Has Tensorflow2 also introduced such flags?\r\n\r\nAlso, have TF developers managed to achieve the theoretical speedups claimed in [1] for depth-wise convolution?\r\n\r\n[1] MobileNets: Efficient Convolutional Neural Networks for Mobile VisionApplications. A. Howard et. al\r\n[2] Decomposeme: Simplifying convnets forend-to-end learning. J. Alvarez & L. Petersson", "comments": ["@ub216 When I tried to run the code, I did not see any inconsistency in time operation. Let us know, If the issue still persists. Thanks!", "@gadagashwini Thanks for getting back. I changed the *REPEATS* in my code to run the same operation more number of times (2000) and now my output changes to:\r\n```\r\nChannels: 64 kernel_size: 3\r\nNormal method: 18ms \t Rank-separable method: 19ms \t Depth-separable method: 12ms\r\n\r\nChannels: 64 kernel_size: 5\r\nNormal method: 27ms \t Rank-separable method: 28ms \t Depth-separable method: 19ms\r\n\r\nChannels: 64 kernel_size: 7\r\nNormal method: 31ms \t Rank-separable method: 37ms \t Depth-separable method: 26ms\r\n\r\n\r\n\r\nChannels: 128 kernel_size: 3\r\nNormal method: 24ms \t Rank-separable method: 33ms \t Depth-separable method: 14ms\r\n\r\nChannels: 128 kernel_size: 5\r\nNormal method: 31ms \t Rank-separable method: 50ms \t Depth-separable method: 21ms\r\n\r\nChannels: 128 kernel_size: 7\r\nNormal method: 53ms \t Rank-separable method: 58ms \t Depth-separable method: 28ms\r\n\r\n\r\n\r\nChannels: 256 kernel_size: 3\r\nNormal method: 55ms \t Rank-separable method: 60ms \t Depth-separable method: 18ms\r\n\r\nChannels: 256 kernel_size: 5\r\nNormal method: 129ms \t Rank-separable method: 86ms \t Depth-separable method: 25ms\r\n\r\nChannels: 256 kernel_size: 7\r\nNormal method: 150ms \t Rank-separable method: 87ms \t Depth-separable method: 33ms\r\n\r\n\r\n\r\nChannels: 512 kernel_size: 3\r\nNormal method: 106ms \t Rank-separable method: 105ms \t Depth-separable method: 27ms\r\n\r\nChannels: 512 kernel_size: 5\r\nNormal method: 422ms \t Rank-separable method: 113ms \t Depth-separable method: 34ms\r\n\r\nChannels: 512 kernel_size: 7\r\nNormal method: 526ms \t Rank-separable method: 122ms \t Depth-separable method: 41ms\r\n\r\n\r\n\r\nChannels: 1024 kernel_size: 3\r\nNormal method: 210ms \t Rank-separable method: 183ms \t Depth-separable method: 44ms\r\n\r\nChannels: 1024 kernel_size: 5\r\nNormal method: 842ms \t Rank-separable method: 199ms \t Depth-separable method: 51ms\r\n\r\nChannels: 1024 kernel_size: 7\r\nNormal method: 1673ms \t Rank-separable method: 215ms \t Depth-separable method: 59ms\r\n```\r\nThe average operation time should not change with more runs? Can you try this. Also could you paste your output for both REPEATS=200 & 2000?", "@ub216 \r\nCan you please verify on tf-nightly and let us know if the issue persist.", "Was able to reproduce the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/622fb7c220863e31c1560e32e9b6f1b4/untitled.ipynb)..Thanks !", "Was able to reproduce the issue in TF v2.6 ,please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/bd0c2bfddb14e33608ebffa925f1af8d/untitled.ipynb#scrollTo=SaYpR6xGi_lH)..Thanks !", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28072\">No</a>\n"]}, {"number": 28071, "title": "train_and_evaluate hang forever without message", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 3.10.0-693.11.6.el7.x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.6.0\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:CUDA Version 9.0.176\r\n- GPU model and memory:P100 12.0G\r\n\r\n**Describe the current behavior**\r\nwhen LD_LIBARARY_PATH don't include the libhdfs.so,and we set estimator RunConfig-model_dir to a hdfs path, it will hang at tf.train_and_evaluate forever .I think it can't operate tf.gfile.MkDir() ,but it gives no info or any message.\r\n\r\n**Describe the expected behavior**\r\nI expect that it will catch the exception and give the info about loss xx.so.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nclassifier = tf.estimator.Estimator(\r\n        model_fn=text_cnn_fn,\r\n        params={\r\n            'feature_columns': feature_columns,\r\n            'vocab_size': FLAGS.hash_size,\r\n            'embedding_size': FLAGS.embedding_size,\r\n            'filter_sizes': filter_sizes,\r\n            'num_filters': FLAGS.num_filters,\r\n            'sequence_length': FLAGS.sequence_length,\r\n            'n_classes': FLAGS.num_classes,\r\n            'learning_rate': FLAGS.learning_rate,\r\n        },\r\n        config=tf.estimator.RunConfig(model_dir=\"hdfs://default/tmp\")\r\n    )\r\n...data process\r\n tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28071\">No</a>\n"]}, {"number": 28070, "title": "tf2.0a0 tf.nn.ctc_loss with AttributeError: Tensor.op is meaningless when eager execution is enabled.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04/16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha\r\n- TensorFlow version (use command below): 2.0.0-alpha0 / v1.12.0-9492-g2c319fb\r\n- Python version: 3.7.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/fogotten\r\n- GPU model and memory:\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**When running: `python loss.py`, error raised, see the `error.txt` for details. Maybe there is some internal implementation error on the function namely `tf.nn.ctc_loss`...**\r\n\r\n**1. helper.py**\r\n```\r\n# Author: Jiarenyf ...\r\n# pylint: disable=invalid-name\r\n# pylint: disable=too-many-locals\r\n# pylint: disable=missing-docstring\r\n# pylint: disable=redefined-outer-name\r\n\r\nimport tensorflow as tf\r\n\r\n#########################################\r\n\r\n\r\ndef dense_to_sparse(tensor, eos_token):\r\n    eos_token = tf.constant(eos_token, tensor.dtype)\r\n    indices = tf.where(tf.not_equal(tensor, eos_token))\r\n\r\n    values = tf.gather_nd(tensor, indices)\r\n    shape = tf.shape(tensor, out_type=tf.int64)\r\n    return tf.SparseTensor(indices, values, shape)\r\n\r\n```\r\n\r\n**2. loss.py**\r\n```\r\n# Author: Jiarenyf ...\r\n# pylint: disable=invalid-name\r\n# pylint: disable=too-many-locals\r\n# pylint: disable=missing-docstring\r\n# pylint: disable=redefined-outer-name\r\n\r\nimport tensorflow as tf\r\nfrom helper import dense_to_sparse\r\n\r\n#########################################\r\n\r\n\r\ndef ctc_loss(label, logit, label_len, logit_len, classes):\r\n    prediction_sparse = tf.cast(tf.nn.ctc_greedy_decoder(\r\n        logit, logit_len, merge_repeated=True)[0][0], tf.int32)\r\n    prediction = tf.sparse.to_dense(prediction_sparse, classes)\r\n\r\n    label_sparse = dense_to_sparse(label, classes)\r\n    accuracy = 1.0 - tf.edit_distance(\r\n        prediction_sparse, label_sparse, normalize=True)\r\n    loss = tf.nn.ctc_loss(\r\n        label, logit, label_len, logit_len, blank_index=classes)\r\n\r\n    return loss, accuracy, prediction\r\n\r\n\r\n#########################################\r\n\r\n\r\nLOSS_DICT = {\r\n    'ctc': ctc_loss,\r\n}\r\n\r\n\r\n#########################################\r\n\r\nif __name__ == '__main__':\r\n    frames = 8\r\n    classes = 20\r\n    batch_size = 16\r\n    label_len = tf.ones(batch_size, tf.int32)\r\n    label = tf.ones((batch_size, 5), tf.int32)\r\n    logit_len = tf.zeros(batch_size, tf.int32)\r\n    logit = tf.zeros((frames, batch_size, classes+1))\r\n    print(ctc_loss(label, logit, label_len, logit_len, classes+1))\r\n\r\n```\r\n\r\n**3. error.txt**\r\n```\r\nTraceback (most recent call last):\r\n  File \"loss.py\", line 45, in <module>\r\n    print(ctc_loss(label, logit, label_len, logit_len, classes+1))\r\n  File \"loss.py\", line 22, in ctc_loss\r\n    label, logit, label_len, logit_len, blank_index=classes)\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py\", line 672, in ctc_loss_v2\r\n    name=name)\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py\", line 784, in ctc_loss_dense\r\n    return compute_ctc_loss(*args)[0]\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/function.py\", line 520, in __call__\r\n    ret, op = _call(self._signature, *args, **kwargs)\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/function.py\", line 1022, in _call\r\n    compute_shapes=False)\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3466, in create_op\r\n    input_ops = set([t.op for t in inputs])\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3466, in <listcomp>\r\n    input_ops = set([t.op for t in inputs])\r\n  File \"/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 934, in op\r\n    \"Tensor.op is meaningless when eager execution is enabled.\")\r\nAttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@shashvatshahi1998 @gadagashwini @ebrevdo \r\nCould you please help me, thank you.", "@jiarenyf refer to this issue #27739 for further updates as your code is showing same Attribute error.", "I found that changing the `tf.nn.ctc_loss` to `tf.compat.v1.nn.ctc_loss` is ok, but when would the error in `2.0.0-alpha` being fixed...\r\n\r\n```\r\n# Author: Jiarenyf ...\r\n# pylint: disable=invalid-name\r\n# pylint: disable=too-many-locals\r\n# pylint: disable=missing-docstring\r\n# pylint: disable=redefined-outer-name\r\n\r\nimport tensorflow as tf\r\nfrom helper import dense_to_sparse\r\n\r\n#########################################\r\n\r\n\r\ndef ctc_loss(label, logit, label_len, logit_len, classes):\r\n    prediction_sparse = tf.cast(tf.nn.ctc_greedy_decoder(\r\n        logit, logit_len, merge_repeated=True)[0][0], tf.int32)\r\n    prediction = tf.sparse.to_dense(prediction_sparse, classes)\r\n\r\n    label_sparse = dense_to_sparse(label, classes)\r\n    accuracy = 1.0 - tf.edit_distance(\r\n        prediction_sparse, label_sparse, normalize=True)\r\n    loss = tf.compat.v1.nn.ctc_loss(label_sparse, logit, logit_len)\r\n    # loss = tf.nn.ctc_loss(\r\n    # label, logit, label_len, logit_len, blank_index=classes)\r\n\r\n    return loss, accuracy, prediction\r\n\r\n\r\n#########################################\r\n\r\n\r\nLOSS_DICT = {\r\n    'ctc': ctc_loss,\r\n}\r\n\r\n\r\n#########################################\r\n\r\nif __name__ == '__main__':\r\n    num = 5\r\n    frames = 9\r\n    classes = 20\r\n    batch_size = 16\r\n    label = tf.zeros((batch_size, num), tf.int32)\r\n    label_len = tf.ones(batch_size, tf.int32) * num\r\n    logit = tf.concat([\r\n        tf.ones((frames//num, batch_size, 1)),\r\n        tf.zeros((frames//num, batch_size, classes)),\r\n    ], axis=-1)\r\n    for i in range(num-1):\r\n        tmp1 = tf.concat([\r\n            tf.zeros((frames//num, batch_size, classes)),\r\n            tf.ones((frames//num, batch_size, 1)),\r\n        ], axis=-1)\r\n        tmp2 = tf.concat([\r\n            tf.ones((frames//num, batch_size, 1)),\r\n            tf.zeros((frames//num, batch_size, classes)),\r\n        ], axis=-1)\r\n        logit = tf.concat([logit, tmp1, tmp2], axis=0)\r\n\r\n    logit_len = tf.ones(batch_size, tf.int32) * frames\r\n    loss, accuracy, prediction = map(\r\n        lambda r: r.numpy(),\r\n        ctc_loss(label, logit, label_len, logit_len, classes+1))\r\n    for l, a, p in zip(loss, accuracy, prediction):\r\n        print(f\"Loss: {'%.3f'%l}; Accuracy: {a*100}%; Prediction: {p}\")\r\n\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28070\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28070\">No</a>\n"]}, {"number": 28069, "title": "Conversion of frozen graph .pb to .tflite error", "body": "So I tried to convert a frozen graph that I got from [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](url) (the name of the model is \"faster_rcnn_resnet101_fgvc\"), to a tflite, by using the following command on google collab:\r\n\r\n!tflite_convert --graph_def_file=frozen_inference_graph.pb --output_file=optimized_graph.lite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,1024,1024,3 --input_array=image_tensor --output_array=Softmax\r\n\r\nI sort of guessed (probably incorrectly) the input and output arrays, and input_shape, because that information was not provided with the model. I got an error saying something like \"2019-04-23 09:30:48.751842: F tensorflow/lite/toco/tooling_util.cc:627] Check failed: dim >= 1 (0 vs. 1)\r\nAborted (core dumped)\". I am new to Tensorflow, so is there any way I can find out how that error occurred, and how I can resolve it?", "comments": ["Hi Mowghli,\r\n\r\nYou can use summarize_graph tool to inspect the input and outputs of the graph\r\nSee\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#inspecting-graphs\r\n\r\n", "So I got this:\r\n\r\n```\r\nAmoghs-MacBook-Pro:tensorflow amoghbhabal$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=frozen_inference_graph.pb\r\nFound 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3]) \r\nNo variables spotted.\r\nFound 4 possible outputs: (name=num_detections, op=Identity) (name=detection_classes, op=Identity) (name=detection_scores, op=Identity) (name=detection_boxes, op=Identity) \r\nFound 57548019 (57.55M) const parameters, 0 (0) variable parameters, and 65923 control_edges\r\nOp types used: 66369 Const, 8603 Reshape, 5794 StridedSlice, 5772 Shape, 5733 GatherV2, 5714 Fill, 2898 Add, 2895 Mul, 2892 Pack, 2890 Sub, 2884 Range, 2874 Slice, 2872 ConcatV2, 2870 Minimum, 2865 Less, 2865 Select, 2858 ZerosLike, 2855 NonMaxSuppressionV3, 61 Enter, 58 BiasAdd, 56 Conv2D, 49 Relu, 34 Identity, 27 RealDiv, 25 Cast, 22 TensorArrayV3, 22 Switch, 21 Merge, 20 NextIteration, 13 Maximum, 12 TensorArrayReadV3, 12 TensorArrayScatterV3, 12 Greater, 11 Split, 10 TensorArraySizeV3, 10 Squeeze, 10 TensorArrayWriteV3, 10 Tile, 10 Exit, 10 Unpack, 10 TensorArrayGatherV3, 9 ExpandDims, 7 Pad, 6 LogicalAnd, 6 Assert, 6 Transpose, 5 LoopCond, 5 Equal, 4 GreaterEqual, 4 Exp, 4 Size, 4 MaxPool, 4 TopKV2, 4 Round, 3 Where, 2 MatMul, 2 Softmax, 2 Sum, 2 Mean, 1 All, 1 Placeholder, 1 Max, 1 Relu6, 1 ResizeBilinear, 1 CropAndResize\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=-1,-1,-1,3 --output_layer=num_detections,detection_classes,detection_scores,detection_boxes\r\n```\r\n\r\nSo I found the input and output layers, but the input_layer_shape is an issue. I tried running:\r\n\r\n```\r\n!tflite_convert --graph_def_file=frozen_inference_graph.pb --output_file=optimized_graph.lite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=-1,-1,-1,3 --input_array=image_tensor --output_array=num_detections,detection_classes,detection_scores,detection_boxes --allow_custom_ops\r\n```\r\n\r\n\r\nand I got the error\r\n\r\n```\r\n2019-04-25 23:06:13.427794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-04-25 23:06:13.428096: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55705b2bac60 executing computations on platform Host. Devices:\r\n2019-04-25 23:06:13.428154: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 122, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 109, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py\", line 288, in from_frozen_graph\r\n    _set_tensor_shapes(input_tensors, input_shapes)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert_saved_model.py\", line 220, in set_tensor_shapes\r\n    raise ValueError(message)\r\nValueError: The shape of tensor 'image_tensor' cannot be changed from (?, ?, ?, 3) to [-1, -1, -1, 3]. Dimension -1 must be >= 0\r\n```\r\nIs there any way I can convert this model to .tflite format?\r\nAlso, are there any tensorflow lite sample models that perform object detection on animals and plants? I only found sample models trained on COCO dataset, and retraining them would involve using VMs, since the iNaturalist dataset is too large. ", "Is this resolved? facing the same issue.\r\nTIA", "@Mowghli  Is this still an issue?", "Sorry for the late reply. I asked the owner of the repository that has the model, regarding the same issues. He replied that he is not sure if conversion of FasterRCNN models to TF Lite is supported yet. He suggested performing transfer learning using a much smaller subset of the iNaturalist Dataset. He also gave the following links:\r\n\r\n1. [https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md)\r\n2. [https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n)\r\n\r\nThe Faster RCNN model is also not trained with quantization enabled, so it might become inaccurate when deployed, assuming you can convert it to TFLite.\r\nAlso, I tried converting this model to TFLite, with the following command (I tried guessing the shape, I am not sure if it is correct):\r\n\r\n```\r\n!tflite_convert --graph_def_file=frozen_inference_graph.pb --output_file=optimized_graph.lite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=0,1024,1024,3 --input_array=image_tensor --output_array=Softmax --allow_custom_ops\r\n```\r\n\r\nI got the following output (This is output is not complete, it's just the end of the output text I got):\r\n\r\n```\r\n2019-04-23 09:06:30.883905: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Round\r\n2019-04-23 09:06:30.883938: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Round\r\n2019-04-23 09:06:30.884003: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2019-04-23 09:06:30.884051: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2019-04-23 09:08:50.422889: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 67000 operators, 136448 arrays (0 quantized)\r\n2019-04-23 09:11:01.390182: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 502 operators, 749 arrays (0 quantized)\r\n2019-04-23 09:11:01.408409: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 502 operators, 749 arrays (0 quantized)\r\n2019-04-23 09:11:01.594021: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 207 operators, 447 arrays (0 quantized)\r\n2019-04-23 09:11:01.600658: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 207 operators, 447 arrays (0 quantized)\r\n2019-04-23 09:11:01.614120: F tensorflow/lite/toco/tooling_util.cc:627] Check failed: dim >= 1 (0 vs. 1)\r\nAborted (core dumped)\r\n```\r\nSeems like there were a lot of unsupported operations.\r\n\r\n\r\nEdit: In the end, I tried making my own dataset using a [google image crawler](https://github.com/hardikvasa/google-images-download) and [label tool](https://github.com/tzutalin/labelImg). I haven't tried transfer learning, though, as we shifted to implementing Wildlife classification using Google AutoML vision.", "@Mowghli looks you have found an alternative. Can we close this? ", "Yes. The issue is resolved.", "Glad it resolved. "]}, {"number": 28068, "title": "TypeError: 'Tensor' object is not callable when using tf.keras.optimizers.Adam, works fine when using tf.compat.v1.train.AdamOptimizer", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS \r\n- TensorFlow installed from (source or binary): binary with pip\r\n- TensorFlow version (use command below): 2.0 alpha0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nI am trying to migrate 1.12 code using estimator to TF 2.0 using everything from keras as it is suggested. Unfortunatley there is no example yet in TF 2.0 documentation. Here it is only partially using new keras functions:\r\nhttps://www.tensorflow.org/alpha/tutorials/distribute/multi_worker\r\n\r\n```\r\n    # Compute loss for both TRAIN and EVAL modes\r\n    ##loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\r\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, logits)\r\n\r\n    # Generate necessary evaluation metrics\r\n    ##accuracy = tf.compat.v1.metrics.accuracy(labels=tf.argmax(input=labels, axis=1), predictions=classes, name='accuracy')\r\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\r\n    accuracy.update_state(labels, logits)\r\n\r\n    eval_metrics = {'accuracy': accuracy}\r\n\r\n    tf.summary.scalar('accuracy', accuracy.result())\r\n\r\n    # Provide an estimator spec for `ModeKeys.EVAL`\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=eval_metrics)\r\n\r\n    # Provide an estimator spec for `ModeKeys.TRAIN`\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n\r\n        # crashing\r\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)\r\n\r\n        ##optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)\r\n\r\n        train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())\r\n\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op)\r\n```\r\n\r\n**Describe the expected behavior**\r\nreplacing:\r\n`optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)`\r\nby \r\n`optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)`\r\nit works without any issue.\r\n\r\nThere is so many changes with TF 2.0, that it could be that I am did a wrong implementation but I managed to migrate all old components to keras layer, loss, metrics ...\r\n\r\nI couldn't find an equivalent for \"tf.compat.v1.train.get_or_create_global_step()\" for TF 2.0 (without using the compatibility module v1)\r\n\r\n**Code to reproduce the issue**\r\n`# estimator model\r\ndef baseline_estimator_model(features, labels, mode, params):\r\n    \"\"\"\r\n    Model function for Estimator\r\n    \"\"\"\r\n    print('model based on keras layer but return an estimator model')\r\n\r\n    # gettings the bulding blocks\r\n    model = keras_building_blocks(params['dim_input'], params['num_classes'])\r\n\r\n    dense_inpout = features['dense_input']\r\n\r\n    # Logits layer\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        logits = model(dense_inpout, training=True)\r\n    else:\r\n        logits = model(dense_inpout, training=False)\r\n\r\n\r\n    # Compute predictions\r\n    probabilities = tf.nn.softmax(logits)\r\n    classes = tf.argmax(input=probabilities, axis=1, )\r\n\r\n    # made prediction\r\n    predictions = {\r\n        'classes': classes,\r\n        'probabilities': probabilities,\r\n    }\r\n\r\n    # to be tested\r\n    predictions_output = tf.estimator.export.PredictOutput(predictions)\r\n\r\n    #predictions_output = {\r\n    #    'classify': tf.estimator.export.PredictOutput(predictions)\r\n    #}\r\n\r\n    # Provide an estimator spec for `ModeKeys.PREDICT`\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=predictions,\r\n                                          export_outputs={tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output})\r\n\r\n    # Compute loss for both TRAIN and EVAL modes\r\n    ##loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\r\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, logits)\r\n\r\n    # Generate necessary evaluation metrics\r\n    ##accuracy = tf.compat.v1.metrics.accuracy(labels=tf.argmax(input=labels, axis=1), predictions=classes, name='accuracy')\r\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\r\n    accuracy.update_state(labels, logits)\r\n\r\n    #print(tf.argmax(input=labels, axis=1))\r\n    #print(classes)\r\n\r\n    eval_metrics = {'accuracy': accuracy}\r\n\r\n    tf.summary.scalar('accuracy', accuracy.result())\r\n\r\n    # Provide an estimator spec for `ModeKeys.EVAL`\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=eval_metrics)\r\n\r\n    # Provide an estimator spec for `ModeKeys.TRAIN`\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n\r\n        # crashing\r\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)\r\n\r\n        #optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)\r\n\r\n        print('step 7')\r\n        train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())\r\n\r\n        print('step 8')\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op)\r\n        print('step 9')`\r\n\r\nnotebook can be find here with the code, tf.dataset ...:\r\nhttps://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/TF_2.0/08-Mnist_keras_estimator.ipynb\r\n\r\n**Other info / logs**\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\n~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in train_and_evaluate(FLAGS, use_keras)\r\n    587                                       #exporters=exporter)\r\n    588 \r\n--> 589     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n    590 \r\n    591 def train_and_evaluate_old(FLAGS, use_keras):\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    471         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    472 \r\n--> 473   return executor.run()\r\n    474 \r\n    475 \r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)\r\n    611         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    612       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 613       return self.run_local()\r\n    614 \r\n    615     # Distributed case.\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)\r\n    712         max_steps=self._train_spec.max_steps,\r\n    713         hooks=train_hooks,\r\n--> 714         saving_listeners=saving_listeners)\r\n    715 \r\n    716     eval_result = listener_for_eval.eval_result or _EvalResult(\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    357 \r\n    358       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 359       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    360       logging.info('Loss for final step: %s.', loss)\r\n    361       return self\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1137       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1138     else:\r\n-> 1139       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1140 \r\n   1141   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1167       worker_hooks.extend(input_hooks)\r\n   1168       estimator_spec = self._call_model_fn(\r\n-> 1169           features, labels, ModeKeys.TRAIN, self.config)\r\n   1170       global_step_tensor = training_util.get_global_step(g)\r\n   1171       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1125 \r\n   1126     logging.info('Calling model_fn.')\r\n-> 1127     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1128     logging.info('Done calling model_fn.')\r\n   1129 \r\n\r\n~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in baseline_estimator_model(features, labels, mode, params)\r\n    440 \r\n    441         print('step 7')\r\n--> 442         train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())\r\n    443 \r\n    444         print('step 8')\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)\r\n    294     \"\"\"\r\n    295     grads_and_vars = self._compute_gradients(\r\n--> 296         loss, var_list=var_list, grad_loss=grad_loss)\r\n    297 \r\n    298     return self.apply_gradients(grads_and_vars, name=name)\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _compute_gradients(self, loss, var_list, grad_loss)\r\n    326     with backprop.GradientTape() as tape:\r\n    327       tape.watch(var_list)\r\n--> 328       loss_value = loss()\r\n    329     grads = tape.gradient(loss_value, var_list, grad_loss)\r\n    330 \r\n\r\nTypeError: 'Tensor' object is not callable\r\n", "comments": ["Same error message when I do it in a more native TF 2.0 way:\r\n\r\n`train_op = optimizer.minimize(loss,var_list=model.weights)`", "For using ```tf.train.get_or_create_global_step``` in TF 2.X ```tf.compat.v1.train.get_or_create_global_step``` is correct conversion. You will have to use v1 compatibility module to use that function.", "ok, for 'optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())'\r\nbut even using 'optimizer.minimize(loss,var_list=model.weights)' which is more native for TF 2.0 and doesn't use 'tf.train.get_or_create_global_step' it crashing so the issue is somewhere else I guess. Thanks for the follow up.", "Okay. I will close this issue for now. You can reopen when additional information is available or can file a new issue if it's not a related error. Thanks!", "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\r\nTypeError: 'NoneType' object is not callable", "train_step=tf.keras.optimizers.SGD(rate).minimize(loss,var_list=tf.compat.v1.trainable_variables())\r\nTypeError: 'Tensor' object is not callable\r\nthe same error. ", "I ran into the same error. For me the issue was that the loss has to be a callable that takes no inputs and returns the loss but, i was feeding the actual loss. Hope this helps someone", "Same issue:\r\n```\r\n    weights = K.variable(np.ones(5), dtype='float64')\r\n    chosen_action = K.argmax(weights, 0)\r\n\r\n    # inputs\r\n    reward = Input(shape=(1,), dtype='float64')\r\n\r\n    # current bandit's weight\r\n    resp_weight = K.gather(weights, chosen_action)\r\n\r\n    loss = K.log(resp_weight) * reward\r\n    optimizer = SGD(learning_rate = 0.001).minimize(loss, var_list=[weights])\r\n```\r\n\r\n`TypeError: 'Tensor' object is not callable`", "@whitish - your loss has to be a callable that takes no inputs and returns the loss", "def stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\r\n \r\n    number_of_features = X.shape[1]\r\n    # numpy array with 1 row and columns equal to number of features. In \r\n    # our case number_of_features = 3 (area, bedroom and age)\r\n    w = np.ones(shape=(number_of_features)) \r\n    b = 0\r\n    total_samples = X.shape[0]\r\n    \r\n    price_list = []\r\n    epoch_list = []\r\n    \r\n    for i in range(epochs):    \r\n        random_index = random.randint(0,total_samples-1) # random index from total samples\r\n        sample_x = X[random_index]\r\n        sample_y = y_true[random_index]\r\n        \r\n        y_predicted = np.dot(w, sample_x.T) + b\r\n    \r\n        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\r\n        b_grad = -(2/total_samples)*(sample_y-y_predicted)\r\n        \r\n        w = w - learning_rate * w_grad\r\n        b = b - learning_rate * b_grad\r\n        \r\n        price = np.square(sample_y-y_predicted)\r\n        \r\n        if i%100==0: # at every 100th iteration record the price and epoch value\r\n            price_list.append(price)\r\n            epoch_list.append(i)\r\n        \r\n    return w, b, price, price_list, epoch_list\r\nw_sgd, b_sgd, price_sgd, price_list_sgd, epoch_list_sgd = SGD(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\r\nw_sgd, b_sgd, price_sgd\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-56-7d0451465b94> in <module>\r\n----> 1 w_sgd, b_sgd, price_sgd, price_list_sgd, epoch_list_sgd = SGD (scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\r\n      2 w_sgd, b_sgd, price_sgd\r\n\r\nTypeError: 'SGD' object is not callable\r\n"]}, {"number": 28067, "title": "AttributeError: module 'tensorflow' has no attribute 'app'", "body": "<em>Upgraded the tensorflow for poets codelab to tf_2.*.alpha, returns the following on attempt to retrain</em>\r\n\r\npython3 -m scripts.retrain   --bottleneck_dir=tf_files/bottlenecks   --how_many_training_steps=500   --model_dir=tf_files/models/   --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\"   --output_graph=tf_files/retrained_graph.pb   --output_labels=tf_files/retrained_labels.txt   --architecture=\"${ARCHITECTURE}\"   --image_dir=tf_files/flower_photos\r\n[sudo] password for Beast: \r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/Beast/work/newAI/tensorflow-for-poets-2/scripts/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nAttributeError: module 'tensorflow' has no attribute 'app'", "comments": ["@AineKiraboMbabazi It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation. In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I'm having the same issue as well. TF2.0 Beta1.\r\nInstalled on Anaconda environment. Python 3.7.", "Same issue", "Can you solved this problem?", "Please open a new issue, filling in the template and providing minimal code reproducing the issue"]}, {"number": 28066, "title": "[TF2.0] SavedModel can't save output layer with multiple outputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n`tf.saved_model.save` does not work on keras model when the output layer produce multiple outputs.\r\n\r\n**Describe the expected behavior**\r\nSavedModel should produce same outputs as the original model.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\n\r\nclass TwoOutputs(tf.keras.layers.Layer):\r\n    def call(self, x):\r\n        return x + 1, x - 1\r\n\r\ninputs = Input([2], dtype=tf.int32)\r\noutputs = TwoOutputs()(inputs)  # subclass version is broken\r\n# outputs = Lambda(lambda x: (x + 1, x - 1))(inputs)  # functional version also broken\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nprint(model(tf.constant([[0, 1], [2, 3]])))  # works fine\r\n\r\ntf.saved_model.save(model, 'checkpoints/test')\r\nmodel = tf.saved_model.load('checkpoints/test')\r\ninfer = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nprint(infer.structured_outputs)  # wrong signatures\r\nprint(infer(tf.constant([[0, 1], [2, 3]])))  # wrong output\r\n\r\n```\r\n**Other info / logs**\r\nThe automatically generated saved_model signature relies on the name of outputs.\r\nKeras model generates `model.output_names` here \r\nhttps://github.com/tensorflow/tensorflow/blob/f0fba04cd274e7ffa24ab8e49f888ef6974e6f7a/tensorflow/python/keras/engine/network.py#L329\r\nFor outputs layers with multiple outputs, all outputs will have the same name!\r\nWhen creating saved_model, tensorflow uses the following _wrapped_model function to generate the ConcreteFunction signature\r\nhttps://github.com/tensorflow/tensorflow/blob/f0fba04cd274e7ffa24ab8e49f888ef6974e6f7a/tensorflow/python/keras/saving/saving_utils.py#L108\r\n\r\nIn our case the model has `outputs` [x1, x2] with `output_names` [layer, layer].\r\nthe _wrapped_model function produces signature `{layer: x2}` which is wrong!! \r\nx1 gets dropped from the output of saved model.\r\n", "comments": ["Thank you for the great bug report! Should be fixed in https://github.com/tensorflow/tensorflow/commit/d7cd03f1398cd33fc8579ce336b602350ac99259"]}, {"number": 28065, "title": "scatter_update of cache-enabled variable gives wrong output when sliced", "body": "Well, this looks a very weird bug to me. But this might be a serious one as it can give a \"wrong\" output.\r\n\r\n### System information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary, from pypi\r\n- TensorFlow version (use command below): tensorflow-gpu 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CUDA 10.0, cudnn 7.4 (but it happens to CPU only as well)\r\n- GPU model and memory: -\r\n\r\n### Code to reproduce the issue\r\n\r\nPlease take a look at the following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nK = tf.get_variable(\"K\", shape=[100, 32], dtype=tf.float32, trainable=False,\r\n                    initializer=tf.random_uniform_initializer(-0.0, 0.0),\r\n                    caching_device='/cpu:0')\r\n\r\n# update the variable (the first row of K, i.e. K[0])\r\ntf.scatter_update(K, [0], tf.ones([1, 32]))\r\n\r\n# These two lines should give the same answer\r\nprint(K[0, :].numpy() [:10])\r\nprint(K.numpy()[0, :] [:10])\r\n```\r\n\r\nThe output is:\r\n```\r\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\r\n```\r\n\r\nThe expected behavior:\r\n```\r\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\r\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\r\n```\r\n\r\n\r\n### Other information\r\n\r\n* A variable could be placed either on GPU or on CPU, giving the same behavior.\r\n\r\n* Other tensor manipulations such as `tf.reduce_sum(K)` will give the correct answer, so definitely the content of the tensor is well updated.\r\n\r\n* If you comment out `caching_device`, it works as expected. This means that this has something to do with **caching mechanism.** _Tensor slice operations seemingly are reading from invalidated caches._\r\n\r\n* When it comes to the **static graph mode** (i.e. not eager), it works as normal and expected:\r\n```python\r\nimport tensorflow as tf\r\nK = tf.get_variable(\"K\", shape=[100, 32], dtype=tf.float32, trainable=False,\r\n                    initializer=tf.random_uniform_initializer(-0.0, 0.0),\r\n                    caching_device='/cpu:0'\r\n                   )\r\nsess = tf.InteractiveSession()\r\nsess.run(K.initializer)\r\n\r\nupdate_op = tf.scatter_update(K, [0], tf.ones([1, 32]))\r\nsess.run(update_op)\r\n\r\n# gives [1. 1. ..... 1.]\r\nprint( sess.run(K[0, :]) )\r\n```", "comments": ["@wookayin I could reproduce the issue with TF1.13.1. Thanks!", "Yes, caching_device is dangerous, specially with eager execution.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28065\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28065\">No</a>\n", "Shouldn't we at least make a documentation or warning against it?", "Yes, I agree. Want to send a PR?\n\nOn Fri, Jun 7, 2019 at 12:00 PM Jongwook Choi <notifications@github.com>\nwrote:\n\n> Shouldn't we at least make a documentation or warning against it?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28065?email_source=notifications&email_token=AAABHRJA3L4F5RCC3DV34QDPZKV3TA5CNFSM4HHVBVCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXGWFXA#issuecomment-499999452>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNQ3EJYUF6ML2K4KKLPZKV3TANCNFSM4HHVBVCA>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 28064, "title": "unify keras.optimizer and tf.train.Optimizer behavior", "body": "when I using tf.train.Adam to replace kearas.optimizer.Adam the fit method rasize error about global_step not exits.", "comments": ["@zh794390558 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n"]}, {"number": 28063, "title": "keras ModelCheckpoint support period save by time or step, not only epoch.", "body": "**Describe the feature and the current behavior/state.**\r\nnow  ModelCheckpoint only save checkpoint after `period` epoch, which is not good for large data training.\r\n\r\n**Will this change the current api? How?**\r\nadd `period_type` for step, time, epoch\r\n\r\n**Who will benefit with this feature?**\r\ncan fine-grained control the checkpoint save behavior.\r\n\r\n**Any Other info.**\r\n", "comments": ["@zh794390558,\r\nSorry for the delayed response. Can you please specify the use cases where your Feature Request can be helpful? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 28062, "title": "Adding rhel community builds to readme", "body": "@gunan kindly review", "comments": []}, {"number": 28061, "title": "make tflite test error.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:lastest\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):7.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nmake tflite test target\r\nthen error occurs.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n- In file included from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:18:0:\r\n- ./tensorflow/lite/kernels/internal/common.h:48:10: fatal error: fixedpoint/fixedpoint.h: No such file or directory\r\n-  #include \"fixedpoint/fixedpoint.h\"", "comments": ["I can not find the code about fixedpoint.h at the code tree in repository.", "@licunhao\r\nMake sure compile binary source with latest gemmlowp pakage, make sure gemmlowp this package is exist in build.", "@Dayananda-V  \r\nThank you for reply.\r\nI will try it.", "@Dayananda-V \r\nI did what you suggest.\r\nIt works well.\r\nThank you. Now I close the issue."]}, {"number": 28060, "title": "TensorFlow/XLA reports an internal error \"Duplicate variable passed tp XLA cluster\" when build and run BERT", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nLinux Ubuntu 16.04.3\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n\r\nsource\r\n\r\n- TensorFlow version (use command below):\r\n\r\nToT\r\n\r\n- Python version:\r\n\r\n3.6\r\n\r\n- Bazel version (if compiling from source):\r\n\r\n19.04\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n5.4.0\r\n\r\n- CUDA/cuDNN version:\r\n\r\n10.0/7.4\r\n\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen XLA is off,  the BERT compiles fine.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThe following code pattern in the TF graph will expose the problem,\r\n\r\nop0:  Node \"VarHandleOp\"\r\nop1:  Node \"Switch\" op0, pred\r\nop2:  Node \"ReadVariableOp\" op0\r\nop3:  Node \"ReadVariableOp\" op1\r\n\r\nWhem op2 and op3 are clustered together in XLA,  the inputs op0 and op1 are\r\nconsidered resource arguments, and they happen to map the the same resource \r\nop0, and lead to the error  \"Duplicate variable passed to XLA cluster\" reported at \r\nline 135, xla_launch_util.cc.\r\n\r\nI am worarounding the problem by placing constraint on clustable nodes in\r\nmark_for_compilation_pass.cc, so that nodes with resource input from a \"Switch\"\r\nnode be rejected for clustering.  Some people suggest that it may be better to fix\r\nthe problem where the problem is reported.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is a current limitation of XLA (not allowing the same resource variable to flow into a cluster twice). There is a change in flight that will make XLA fall back to normal tensorflow execution but that probably is not what you are looking for.\r\nThe change you are proposing might fix the immediate issue at hands but does not really solve the problem. There are other ways that a resource variable might end up aliasing. But as a workaround for your problem it is possibly fine. \r\nI have to check up on XLA semantics around resource variables to see whether this is a limitation of the runtime or rather a conceptual mismatch with XLA semantics.", "Thanks for your quick response.  I would think fall back to TF-native is not a good option for the case,\r\nthough it is a way to avoid the problem going forward, but it may cause potential performance \r\nissue, too.\r\n\r\nI thought \"Switch\" may be a special case causing the problem, not sure if you could provide \r\nexamples of other cases with the same problem (duplicated resources passing to the same cluster), \r\n\r\nOn the workaround, do you think I should file a PR to get it into the ToT ?\r\n\r\n", "@jlebar ", "@sherhut works on XLA and appears to be on it?", "> The following code pattern in the TF graph will expose the problem,\r\n> \r\n> op0: Node \"VarHandleOp\"\r\n> op1: Node \"Switch\" op0, pred\r\n> op2: Node \"ReadVariableOp\" op0\r\n> op3: Node \"ReadVariableOp\" op1\r\n\r\nThis seems suspicious to me -- `op2` and `op3` should not be in the same cluster because they have different deadness.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28060\">No</a>\n"]}, {"number": 28059, "title": "load_model() Error: \"Unknown entries in loss dictionary\" in 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nTensorFlow installed from (source or binary):binary\r\nTensorFlow version (use command below): 2.0.0.dev20190417\r\nPython version: 3.6.5\r\n\r\n**Describe the current behavior**\r\nTried very simple transfer learning on Inception V3. Training finished, prediction works fine. Saved the model with model.save_model(). When trying to load it with model = load_model(), I get this error:\r\nValueError: Unknown entries in loss dictionary: ['class_name', 'config']. Only expected following keys: ['dense_73']\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\nbase_model = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False)\r\n\r\nx = base_model.output\r\ny = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nz = tf.keras.layers.Dense(1024, activation='relu')(y)\r\npredictions = tf.keras.layers.Dense(4, activation='softmax')(z)\r\n\r\n\r\nmodel = tf.keras.Model(inputs=base_model.input, outputs=predictions)\r\n\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\n\r\nmodel.compile(optimizer='rmsprop', loss=tf.keras.losses.CategoricalCrossentropy())\r\n\r\n\r\nmodel.fit(trainData, validation_data=(valData), shuffle=True)\r\n\r\nmodel.save(myPath)\r\n\r\noutValues = model.predict(testData)\r\n\r\nmodel = tf.keras.models.load_model(myPath) #error here\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I now see that the same problem was reported in #25938, so closing."]}, {"number": 28058, "title": "Where is the legacy_seq2seq of the tensorflow 2.0 version?", "body": "Where is the legacy_seq2seq of the tensorflow 2.0 version?\r\n", "comments": ["@flylonger It is [here](https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/contrib/legacy_seq2seq). Thanks!", "seq2seq is mentioned in this tutorial: https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention. But the link is broken", "@jasonzhang2022 Are you mentioning about the link here \"TensorFlow Neural Machine Translation (seq2seq) tutorial. \" I think that link is taking us to tutorial home page. Please let us know if there is any other links that are not working. If the link is taking you to \"error 404\" then please provide your location (ex. America, Europe, Asia, etc). Thanks!", "Yes, I mean that link. I expect the link taking me to the seq2seq tutorial\ninstead of tutorial home page.\n\nOn Mon, Apr 29, 2019, 9:10 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @jasonzhang2022 <https://github.com/jasonzhang2022> Are you mentioning\n> about the link here \"TensorFlow Neural Machine Translation (seq2seq)\n> tutorial. \" I think that link is taking us to tutorial home page. Please\n> let us know if there is any other links that are not working. If the link\n> is taking you to \"error 404\" then please provide your location (ex.\n> America, Europe, Asia, etc). Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28058#issuecomment-487815483>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABSBDRYYNTIYPVZ37RS6BSLPS7BCPANCNFSM4HHTTRXQ>\n> .\n>\n", "> @flylonger It is [here](https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/contrib/legacy_seq2seq). Thanks!\r\n\r\n@longer008,\r\nPlease check this comment and let us know if helps. Thanks!"]}, {"number": 28057, "title": "[Grappler] Support unknown shapes in SimplifyReduction", "body": "Allows reductions over unknown shapes to be replaced with Identity when the set of reduction indices is empty (which is common in the gradient graph). Previously this only worked if the input shape was known.\r\n\r\nExtends the tests to include this case.\r\n\r\nThis can have a big impact on the subsequent auto_mixed_precision pass, where the presence of reductions prevents optimizations.\r\n\r\nFYI @reedwm ", "comments": ["@benbarsdell thanks for this improvement. I only have a couple of nits.", "@benbarsdell thanks for the quick fix. Sorry it took a while to get to reviewing this."]}, {"number": 28056, "title": "1.14.0-rc0 cherry-pick request: Remove SparseConditionalAccumulator in 2.0.", "body": "PiperOrigin-RevId: 244751763\r\n\r\nOne more symbol needs to be removed from 2.0.", "comments": []}, {"number": 28055, "title": "Fix error message", "body": "", "comments": []}, {"number": 28054, "title": "misalignment in BahdanauAttention documentation", "body": "**System information**\r\n- TensorFlow version: 1.13\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention\r\n\r\nUnder the description for the __init__ function, in the Args, the memory_sequence_length should be in its own point and not part of memory.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** Yes, I can submit a PR for this.\r\n", "comments": ["Added PR #28267 for the fix."]}, {"number": 28053, "title": "Failed to get device properties, error code: 30", "body": "**Describe the current behavior**\r\nI commented under issue #26255 but the original poster closed the issue as his problem was solved by updating to tensorflow 2.\r\n\r\nI am opening a new issue because updating to the pre-release is not an option and I have no way to even trap this error to try to handle it, plus it is an unknown error code so no hint as to how to proceed.\r\n\r\nUnknown error and failure to initialize GPU. \r\n```\r\n2019-04-19 13:16:22.838705: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 30\r\nFailed to initialize GPU device #0: unknown error\r\n```\r\nMy configuration:\r\nWindows 10 Home\r\nTensorflow 1.13.1\r\nPython 3.5\r\nGTX 1060 Mobile Max-Q\r\n\r\nIt doesn't happen every time I run my program. I have localized it to running load_model from keras, before reaching that point I have imported tensorflow and verified that gpu is available.\r\n```python\r\n  if tf.test.is_gpu_available():\r\n        logger.debug(\"GPU is available\")\r\n```\r\nIs there a way to catch this error or check for it and recover/attempt to reinitialize?\r\n\r\nThanks\r\n**Describe the expected behavior**\r\nReport the actual error, provide mechanism to catch the exception and handle it.\r\n\r\n**Code to reproduce the issue**\r\nThe failure is intermittent\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@stevehawley : Could you be able to provide more details on the issue and its context. If possible, provide a code to reproduce the bug. Thanks!", "I have no idea what is causing it, other than it occurs when running load_model and it is an intermittent failure. It occurs essentially at startup, there isn't a variation in sequence of events that leads to the failure as far as I can see at this time.\r\n\r\nI would be happy to add any debug code you suggest and report back.", "I have an error like this:\r\n`2019-04-24 22:43:01.696579: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 30`\r\nand I don't know how to deal with it.", "@stevehawley : Sure that would help us to dig in easily. Thanks!", "@stephanwlee I have the **exact** same config as you and I am also running into this issue. I am using SSD_Mobilenet_v2 (ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03) for object detection. mobilenet v1 was working fine for me the other day so will probably just revert to that. I run into the same issue when running train.py and model_main.py. Not sure what details to provide that might help, but feel free to ask", "@stevehawley Could you provide any standalone code to reproduce the issue? thanks!", "@jvishnuvardhan I'm sorry, I cannot provide code that reproduces it as it is an intermittent issue. If you have debug code you would like me to add to the software to capture data for further debugging I would be happy to do that.", "Error code 30 is cudaErrorUnknown. There could be many reasons. It would be good if anyone can provide code and model/data that can consistently reproduce the problem.", "In the past where I have had a customer reporting an intermittent issue I would add better error reporting so the next time it occurs we would have the data we need to explain and maybe fix the issue. Can such a thing be added to tensorflow around this line of code? Could it capture more information from cuda? Maybe throw an exception so that the application can do something to handle the error? Currently the error is logged but there is no good way for the application to know a problem occurred and respond appropriately.", "@stevehawley I think this will be hard. Sometimes it may not be possible or very hard to recover from CUDA errors. Some of the errors are 'sticky'. Typically those are the kinds of errors that lead to the unexpected kernel termination, and the only way out is to completely reset GPU state with cudaDeviceReset(). And for some of them, rebooting the machine or resetting the GPU is the only way to recover. So this usually goes deeper than TF and can explain why it didn't bother to attempt recovering. If you have a better idea please let me know.\r\n\r\nIn addition, if you have a repro, even though it cannot reproduce the problem consistently, but as long as it can reproduce the problem say one at a hundred times, it would be very helpful and probably means there is a bug somewhere else, and we'll be happy to investigate.", "@nluehr do you have any ideas?", "Understand that it may not be recoverable, but the current behavior is that an error has occurred and the GPU is in an unknown state, the error is logged but the application is unaware and continues to try to use the GPU with no result. At a minimum I think tensorflow should throw an exception so that the application can inform the user and cleanup. \r\n\r\nIf there is an API for resetting the GPU the application could decide to try some number of times before giving up, then report the failure to the user, cleanup and exit.", "@stevehawley sorry I misunderstood that before. So you were saying to let it return an error status. @rmlarsen do you think that's something we can/should do?", "@aaroey   I have the same error message; see screenshot of conda prompt history below.\r\n![Anaconda Prompt history Capture](https://user-images.githubusercontent.com/28427504/58682955-e83ad600-83b5-11e9-8841-1fdc6e49dfa1.PNG)\r\n\r\nPC Details:\r\nWindows 10 Home\r\nDell 7559 with 16gb RAM, NVIDIA GTX960M (4GB)\r\n\r\nJupyter Notebook to re-produce the error is in ZIP file below.  You will need to download the Flickr8k dataset to run this code\r\n[GPU_failed_to_get_device_code.zip](https://github.com/tensorflow/tensorflow/files/3239765/GPU_failed_to_get_device_code.zip)\r\n.\r\nThe failure is not fatal, as code keeps processing (very slowly), presumably on the CPU, but not sure what \"Adapting to protcol v5.1....\" line on Anaconda prompt means??\r\nI have only run this code twice and it gave the same error message each time, with a full re-boot between runs.\r\nI have also seen the same conda prompt history message running other, similar keras based code in the last few weeks, so would love to find a solution.\r\n", "Hi, I get that same error message every time I call `session.run(something)`. Here's one simple call that triggers it:\r\n\r\n```\r\nPython 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2019-05-31 10:26:17.055157: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-05-31 10:26:18.052608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.30GiB\r\n2019-05-31 10:26:18.059187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-05-31 10:26:20.176475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-31 10:26:20.180563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-05-31 10:26:20.182798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n2019-05-31 10:26:20.185520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3011 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n>>> sess.run(tf.rank(tf.Variable([3], dtype = tf.int32)))\r\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-05-31 10:26:46.385633: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 30\r\n1\r\n>>>\r\n```\r\n\r\nIn case it matters, I'm on an XPS 15 9560 (so, Optimus) and have CUDA 10.0 and tensorflow-gpu 1.13.1 installed with pip. If you need more information I'd be happy to provide it, this makes computations quite slow for me.", "Did you find a fix? I would love to know how you solved the issue.", "@andyly would you mind to take a look at this? What will be the best way for Grappler to report the error?", "> Did you find a fix? I would love to know how you solved the issue.\r\n\r\nIf that can help, a few days ago I compiled TensorFlow 1.14 with CUDA 10.1 (not yet supported by the pip binaries), and I'm *reasonably sure* that I didn't have this error and the GPU was used.\r\n\r\nBut then I reinstalled Windows and now I'm getting build errors, so I can't check anymore. I will post here if I succeed in compiling it again.", "I compiled TensorFlow 1.14.0 for CUDA 10.1 on Windows 10, then ran a small script using it, and I got another error message: `E tensorflow/core/grappler/clusters/utils.cc:87] Failed to get device properties, error code: 999`. The GPU was used though, so the error doesn't seem important.", "Hi all, I have encountered this issue consistently while working through the TF tutorial for the fashion-mnist (https://www.tensorflow.org/tutorials/keras/basic_classification).\r\n\r\nMy rig is a Razor Pro laptop (Late 2016), win 10 pro with a Thunderbolt 3 connected egpu Razor Core X Chroma housing an Asus GeForce 2080 Ti\r\n\r\n```\r\nWARNING:tensorflow:From C:\\Users\\kuccello\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-07-21 09:57:23.404013: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2019-07-21 09:57:25.163980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 11.00GiB freeMemory: 9.03GiB\r\n2019-07-21 09:57:25.861157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.97GiB\r\n2019-07-21 09:57:25.861781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1\r\n2019-07-21 09:57:27.554340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-21 09:57:27.554600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 \r\n2019-07-21 09:57:27.554763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N \r\n2019-07-21 09:57:27.555168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N \r\n2019-07-21 09:57:27.555731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8699 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:07:00.0, compute capability: 7.5)\r\n2019-07-21 09:57:27.557220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 4716 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nEpoch 1/5\r\n2019-07-21 09:57:28.924614: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally\r\n```\r\n\r\nThe code executes and makes a prediction. The error code 30 seems to happen consistenly when calling\r\n\r\n```python\r\npredictions = model.predict(test_images)\r\n```\r\n\r\nSnipped output from log\r\n\r\n```\r\n...\r\n8928/10000 [=========================>....] - ETA: 0s - loss: 0.3602 - acc: 0.8694\r\n 9600/10000 [===========================>..] - ETA: 0s - loss: 0.3614 - acc: 0.8697\r\n10000/10000 [==============================] - 1s 87us/sample - loss: 0.3590 - acc: 0.8705\r\nTest accuracy: 0.8705\r\n2019-07-21 09:58:10.393072: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 30\r\nPredictions: [1.22570145e-05 3.14548632e-09 4.37460756e-07 1.29987212e-08\r\n 1.61830727e-07 2.23822240e-02 1.34041156e-05 1.87597424e-01\r\n 1.36709845e-04 7.89857388e-01]\r\nHighest Confidence: 9\r\n...\r\n```", "If it's any help - I was running a script for model compression, and started getting this error when I disconnected my laptop from the power and it started running on battery.\r\nAfter I reconnected it to the power, and restarted the script, the errors did not return.", "> I was running a script for model compression, and started getting this error when I disconnected my laptop from the power and it started running on battery.\r\n\r\nProbably your laptop is configured to shut the GPU off when on battery? This doesn't work for me, I always keep it plugged.", "Even I was facing the same problem while training my model using a GPU, then I updated my NVIDIA GPU driver using NVIDIA GeForce Experience App and the error stopped occurring.", "Updating my Nvidia driver worked for me too. Thanks @AbhishekKarthik ", "I had the same issue. It appears to be an issue with the save or load model. What I did was, I created the model again and saved and loaded only the weights. This solved it for me. Hope that helps.", "> I commented under issue #26255 but the original poster closed the issue as his problem was solved by updating to tensorflow 2. ...  I am opening a new issue because updating to the pre-release is not an option\r\n\r\nCan you check if upgrading to TF 2 helps now that TF2 is no longer pre-release?\r\n\r\n> but the current behavior is that an error has occurred and the GPU is in an unknown state, the error is logged but the application is unaware and continues to try to use the GPU with no result. At a minimum I think tensorflow should throw an exception so that the application can inform the user and cleanup.\r\n\r\nIf you need this behavior perhaps you could explicitly do `assert tf.test.is_gpu_available()`?", "I had the same issue. My work machine is a laptop.\r\nI solved this problem by lifting the electricity limit. And I turned on the cooling system at maximum power", "I have this problem because of a condition of two different processes concurring for initilizating tensorflow.\r\nI really did not solve the problem, but, I found some workaround for my case.\r\n\r\nIm using a Nvidia v100 + Nvidia tensorflow docker image 20.03\r\nI have a lib that wraps some Tensorflow models and an application that runs this lib + another models in tf too.\r\n\r\nIn the beginning of my app I start a Process (multiprocess.Process) and this process will include the lib that includes tensorflow during its initialization causing the Nvidia+tf stuff to be started.\r\nMeanwhile the main thread of my original process keeps its own bootstrapping and tries to initialize TF.\r\n\r\n\r\nWhen a call from the subprocess reaches a tf code it will freeze and will display the \"Failed to get device properties...\"\r\nI'm not sure why, but I noticed that TF does not work well when different processes from same root tries to use TF before. So I changed my code from multiprocessing.Process to threading.Thread and the error is gone.\r\n\r\nFor a proper solution I will refactor my code and avoid one of the inits because it isn't really necessary at that point.\r\nBut I thought to share my experience here in case it enlightens someone else.\r\n", "Hi\r\nwith NVIDIA  Geforce GT 525m i  have this error for this code device_lib.list_local_devices(session_config=None):\r\nRuntimeError: failed to get compute capability major for device: UNKNOWN ERROR (1); 0\r\nIs this because of my compute capability (2.1)?", "Hi, @fjsh84. TensorFlow requires a compute capability of at least 3.5 per https://www.tensorflow.org/install/gpu. The CUDA toolkit has also removed support for Compute Capability 2.x hardware. The specific error you note is more likely caused by the NVIDIA driver or CUDA runtime not being configured correctly. But that doesn't really matter given that the hardware wouldn't be supported anyway.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28053\">No</a>\n", "> @stephanwlee I have the **exact** same config as you and I am also running into this issue. I am using SSD_Mobilenet_v2 (ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03) for object detection. mobilenet v1 was working fine for me the other day so will probably just revert to that. I run into the same issue when running train.py and model_main.py. Not sure what details to provide that might help, but feel free to ask\r\n\r\nAlso had this issue when using keras `load_model` to load MobileNet in `tf.distribute.MirroredStrategy` scope. I got:\r\nFailed to initialize GPU device #0: initialization error.\r\nMy custom keras model didn't have this issue."]}, {"number": 28052, "title": "Fix layout", "body": "It seems that the layout had been introduced some bug within the Google internal commits.\r\n\r\nRelated to #25986", "comments": ["changes have been merged by commit: `ce1136f` , for some reason copybara did not merge this PR, closing this now."]}, {"number": 28051, "title": "Add feature to enable INT ops on GPU conditionally by environment", "body": "Tensorflow contains certain Fake-GPU ops i.e. ops that are marked as GPU ops but has inputs and outputs on the host and run host kernels. This pr adds a bazel configuration option to enable code path to register real GPU versions of Fake-GPU ops using an environment variable. \r\n\r\nIf \"--define using_gpu_int_ops=true\" added to bazel flags during building phase it will be possible to register GPU versions of Fake-GPU ops by setting an environment variable \"TF_USE_INT_TYPES_ON_GPU=1\"\r\n", "comments": ["tagging @azaks2, @aaroey  and @tfboyd ", "@rmlarsen  @alextp I think we shouldn't do it this way, and eventually just fix the PinToHost rewriter and move registration of all these ops on device.", "@tatianashp will be ready next time.", "@samikama could you please resolve the conflicts? Thanks !", "@samikama  gentle ping to resolve the conflicts. Thanks!", "@tatianashp @tfboyd Should I fix the conflicts for merging or are we going to discuss it further. I believe, since this touches many files it will have conflicts again if it is not going to be merged in near future and fixing these will be pointless.", "I'd also strongly prefer first fixing the PinToHostOptimizer and then always enabling int ops on GPUs.", "Let's put this PR on hold until we can get PinToHost optimizer to a point\nwhere it's always enabled; then we can enable non-host int kernels on GPUs\nby default.\n\nOn Thu, May 23, 2019 at 9:12 AM Alexandre Passos <notifications@github.com>\nwrote:\n\n> I'd also strongly prefer first fixing the PinToHostOptimizer and then\n> always enabling int ops on GPUs.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/28051?email_source=notifications&email_token=AANWFG3JWZVGMK7UBJ4IWFDPW27ARA5CNFSM4HHS3DI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWCXDZA#issuecomment-495284708>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG4ZDDN462ID7UM7SVLPW27ARANCNFSM4HHS3DIQ>\n> .\n>\n", "@tatianashp @tfboyd, as discussed in the meeting, compiling in the extra kernels increase the pip wheel size approximately 7 megabytes. filesize of the _pywrap_tensorflow_internal.so for gpu build with ci_build.sh increased from 530MB to 563MB.", "Thanks Sami.  I will try to figure out if 7MB (in the pip) is ok.  It seems ok to me but I am no the owner.  If not we can go back to the build flag and still move forward with trying to get this moving forward.", "@tatianashp   Please state the next direction.  Our last decision 22-MAY was to add this as optional so we can play with the feature.  ", "@samikama could you please resolve the conflicts? Thanks!", "@tatianashp  to get answer at next meeting.  Rasmus has been out and is vital to the review process."]}, {"number": 28050, "title": "Modify APPENDIX part", "body": "The PR is to correct the appendix of the LICENSE. ", "comments": []}]