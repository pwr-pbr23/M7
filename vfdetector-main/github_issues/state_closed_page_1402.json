[{"number": 10996, "title": "[Build] Reduces build time for SYCL target", "body": "", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 10995, "title": "Use native compute capabilities as default", "body": "Instead of using 3.5 and 5.2, would it be better to use compute capabilities of native GPUs?", "comments": ["Can one of the admins verify this patch?", "@meteorcloudy Could you rake a look to make sure all is clear on bazel-windows-GPU front?\r\nJenkins, test this please.", "Just realized `$CUDA_TOOLKIT_PATH/extras/demo_suite/deviceQuery` also exists on Linux, can you use this binary directly instead of compiling it as well? Like this:\r\n```\r\nfunction get_native_cuda_compute_capabilities {\r\n  device_query_bin=\"$CUDA_TOOLKIT_PATH/extras/demo_suite/deviceQuery\" # Also works on Windows without .exe\r\n  \"$device_query_bin\" |grep 'Capability'|grep -o '[0-9]*\\.[0-9]*'|sed ':a;{N;s/\\n/,/};ba'\r\n}\r\n```", "not done yet", "@meteorcloudy I think I'm done now. I test this configure on my Linux machine with two GPUs and a Windows machine without any GPU (therefore deviceQuery will fail) under msys2. Both works well. I don't have a windows device with GPU to test...", "I tested on a GPU machine, it also worked! Thanks!", "Jenkins, test this please."]}, {"number": 10994, "title": "DLL Problem with fresh tensorflow-gpu installation ", "body": "Hi,\r\n\r\nI installed tensorflow-gpu in an anaconda environment on my new notebook. It is a complete fresh installation. I can't get it to work. On my other machine it works fine but not on this new notebook :-(\r\n\r\nMachine:\r\n - Windows 10 Pro\r\n - i7 7700HQ\r\n - NVIDIA GTX 1060 - 6GB (newest device driver - **382.53**)\r\n - 16 GB RAM\r\n - Anaconda 3\r\n - CUDA  Toolkit 8.0 installed \r\n - CUDnn 5.1 for Cuda Toolkit 8.0 installed\r\n\r\nMy anaconda3 python environment uses Python35 (**3.5.3**).\r\n\r\n-  I installed \"tensorflow-gpu\" via pip.\r\n- I also tried: `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.2.0-cp35-cp35m-win_amd64.whl`\r\n\r\nI checked the DDL's. They are all there. I got the following list of DDLs from[ here](https://github.com/tensorflow/tensorflow/issues/5949#issuecomment-263708081)\r\n\r\nin C:\\windows\\system32, i can find:\r\n- KERNEL32.dll\r\n- WSOCK32.dll\r\n- WS2_32.dll\r\n- SHLWAPI.dll\r\n- python35.dll\r\n- MSVCP140.dll\r\n- VCRUNTIME140.dll\r\n\r\nin the anaconda folder, i can find:\r\n- api-ms-win-crt-runtime-l1-1-0.dll\r\n- api-ms-win-crt-heap-l1-1-0.dll\r\n- api-ms-win-crt-utility-l1-1-0.dll\r\n- api-ms-win-crt-stdio-l1-1-0.dll\r\n- api-ms-win-crt-string-l1-1-0.dll\r\n- api-ms-win-crt-math-l1-1-0.dll\r\n- api-ms-win-crt-convert-l1-1-0.dll\r\n- api-ms-win-crt-environment-l1-1-0.dll\r\n- api-ms-win-crt-filesystem-l1-1-0.dll\r\n- api-ms-win-crt-time-l1-1-0.dll\r\n\r\nwhen i want to run a mnist example (or when i just execute 'import tensorflow') I get this error message:\r\n\r\n```\r\n(tensorflow) c:\\bitbucket\\masterthesis\\anaconda test>python mnist_deep.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 919, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mnist_deep.py\", line 32, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 919, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n ", "comments": ["I reinstalled tensorflow-gpu via:\r\n- pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.2.0-cp35-cp35m-win_amd64.whl \r\n\r\n--> still doesn't work\r\n", "Let me guess do you have an Alienware ? I have a similar one which has 980 Nvidia graphic card and 7th generation of i-7 processor. Awesome machine, indeed? \r\n           There two ways you can use tensor flow effectively according my experience.\r\n\r\n1.    You can dual boot the `Ubuntu 17.04` on your SSD drive, not virtual box way. The Linux environment is naive to tensorflow and has `compatibility ` with the latest `Python 3 ` and `Nvidia GPU & drivers.` for effective and fast usage. Additionally, you can use Alienware graphic Amplifier as well if my guess is right?\r\n\r\n2.    `Windows` support is highly experimental.  If you want to stick to it than the following word by word instructions with [install tensor flow on windows](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso) link will help.\r\n\r\n \r\n\r\n- Note that the instructions over there recommend using specified versions of `Python, cuDNN, and CUDA`. Not the latest one. After you achieve it then may try to advance it with the latest versions I guess.", "You did not specify the cuDNN dll in your list of dll's. Have you added the bin folder of your cudnn installation to your PATH variable?\r\nAlso see [here] (https://www.tensorflow.org/install/install_windows#common_installation_problems) for a list of common list of installtion problems, which includes your \"No module named '_pywrap_tensorflow_internal\" error messages and links to a stackoverflow thread with possible causes and solutions.\r\nPlease note that the tensorflow guys \"are relying on Stack Overflow to document TensorFlow installation problems and their remedies\" [see here] (https://www.tensorflow.org/install/install_windows#common_installation_problems).", "Yes, you have to add it to the PATH variable.", "@printdhruv : No, no alienware :-) It's a Clevo Reseller Notebook.\r\n\r\nI'm using CUDA kit 8.0 and cudnn 5.1 for 8.0. And Python 3.5.3 like i mentioned in my first post. So I think that i got the right versions together.\r\n\r\nSry, I forgot to mention that I added the CUDA DLLs to environment variables.\r\n![image](https://user-images.githubusercontent.com/3481094/27472961-b2447fb4-57fd-11e7-8738-20acf28ba61e.png)\r\n\r\n\r\nand modified my PATH:\r\n![image](https://user-images.githubusercontent.com/3481094/27472384-6c27ac6a-57fb-11e7-993d-54a70af8eb23.png)\r\n![image](https://user-images.githubusercontent.com/3481094/27472982-bfafca1e-57fd-11e7-90bf-729b3fcfb70f.png)\r\n\r\n\r\nI included the CUDnn files directly into my CUDA like it is done [here](https://nitishmutha.github.io/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html).\r\n\r\nCould it be a problem with the newest NVIDIA display driver?\r\n\r\nI also run the tensorflow_self_check.py script from [here](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c).\r\n\r\nthe result is:\r\n``` \r\n[...]\r\n- All required DLLs are present [...]\r\n```", "Hi,\r\n       I am glad that you did it. You can try experimenting on top of this settings. If you are able to do configure it with the latest drivers and version in windows 10 than please share across community.", "Well, it is still not running. Still the same error.", "I am sure that you are missing a key somewhere. I would advise you to uninstall everything and follow stackoverflow link step by step .\r\nIf it won't work than would list all steps from my end.", "Can you try running [Dependency Walker](http://www.dependencywalker.com/) on the file `C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd`, and let us know what DLLs it reports as missing?", "I also have same problem, I try many way to fix it but still didn't work.\r\nAnd I run the [https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c]() main() function,\r\nand it give me  ERROR:Failed to import the TensorFlow module.\r\nWhat's going on? I have no idea..", "Hi,\r\n     Can you please state the exact issue with a log , OS and system information? I encourage you to follow the [common_installation_problems](https://www.tensorflow.org/install/install_windows#common_installation_problems). The link which you follow , Make sure to install recommended versions of python, Nvidia and cuDNN.\r\n\r\n      ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@printdhruv  Hi could some one help me out , i have installed cuda 9.1 with cuddn 7.1 and all the dlls are present in the folder yet when i try to use tensorflow gpu it give me \"ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\" but when i download the cuda 9.0 it again gives the same error saying cudart64_80.dll is not available .\r\n\r\nI have tired installing all the version of the tensorflow and cuda yet i get error i have checked the path variable to not sure why i am getting this error  on windows10", "Please read https://www.tensorflow.org/install/install_windows carefully for which versions of CUDA, cuDNN TF needs.", "I followed the instructions at https://www.tensorflow.org/install/install_windows and I get the same error:\r\n`Could not find 'cudart64_90.dll' [...]`.\r\nThis bug is blocking all my efforts at using Tensorflow on Windows 10.", "Same issue as @NicolaOrritos - I followed all the instructions, with a fresh CUDA and tensorflow-gpu install. I also get the `Could not find 'cudart64_90.dll' ...` message. Funnily enough, there is a file called `cudart64_92.dll` in the bin folder that is on my PATH. I just copied that and changed the name, now I get a different message:\r\n\r\n`...\r\nFile \"...\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: The specified module could not be found.`\r\n\r\nand `During handling of the above exception, another exception occurred:`\r\n\r\n`...\r\nFile \"...\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.`\r\n\r\nThere's a link to an FAQ page, but I couldn't find this issue there.", "Same issue over here I guess\r\nI also receive the message, that cudart64_90.dll couldn't be found, althought I checked it- it's in there.\r\n\r\nAs the path is mentioned, I checked my paths:\r\n![1](https://user-images.githubusercontent.com/40840044/42293587-b8831e74-7fdb-11e8-945f-4e4f217ef73b.JPG)\r\n![2](https://user-images.githubusercontent.com/40840044/42293591-bd270c42-7fdb-11e8-886a-4b674ac8c2cf.JPG)\r\n![3](https://user-images.githubusercontent.com/40840044/42293592-bed1345a-7fdb-11e8-9b05-b0503c48483d.JPG)\r\n\r\nSo, could you kindly confirm, it's correct?", "Working tensorflow-gpu on Windows 10 with native pip installation.\r\n\r\nGPU\r\nGeForce GTX 1070\r\n\r\npython --version (latest python version may work, not sure)\r\nPython 3.5.2\r\n\r\nVisual Studio Community 2017\r\nhttps://visualstudio.microsoft.com/downloads/\r\n\r\nCUDA Toolkit 9.0 (latest CUDA version may work, not sure)\r\nhttps://developer.nvidia.com/cuda-90-download-archive\r\n\r\ncuDNN v7.2.1 Library for Windows 10 (latest cuDNN matched with CUDA version may work, not sure)\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\n\r\nTensorflow 1.8.0 -> **latest tensorflow-gpu (1.10.1) causes the DLL load failed error with this config**\r\npip3 install tensorflow-gpu==1.8.0\r\n\r\n\r\n```\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('hello')\r\n>>> sess = tf.Session()\r\n2018-09-03 16:53:14.866096: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-09-03 16:53:15.196396: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.62GiB\r\n2018-09-03 16:53:15.201018: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-09-03 16:54:06.459722: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-03 16:54:06.463732: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:929]      0\r\n2018-09-03 16:54:06.464973: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:942] 0:   N\r\n2018-09-03 16:54:06.466436: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6394 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n>>> print(sess.run(hello))\r\nb'hello'\r\n>>> print(tf.__version__)\r\n1.8.0\r\n```", "If anyone else finds themself here, the combination of adding the dll from the Jozef Jarosciak's blog and setting the path in the second answer on [this](https://stackoverflow.com/questions/57528027/importerror-could-not-find-cudart64-100-dll) stack overflow seems to work for me. I am sure there is a better way than putting those lines of code in every tensorflow project so if you find the real issue I am overlooking, please @ me.\r\n"]}, {"number": 10993, "title": "[OpenCL] Fixes run_metadata_test for SYCL", "body": " This test is designed to test CUDA specific behavior", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10992, "title": "Feature request: RNN BeamSearchDecoder with Mixture Density Networks For Continuous Truth Values", "body": "For continuous data, there are 3 options to run a beam search when decoding:\r\n\r\n1. This is a workaround. Quantize continuous truth values into into either one-hot vector or embedding, effectively transforming the regression into a classification problem.\r\n2. Use a mixture density model with N Gaussian mixtures. When decoding, sample top `beam_width` mixtures.\r\n3. Use the SOTA method described in this paper: https://arxiv.org/pdf/1612.01474.pdf\r\n\r\nOf course, the preferred method is number 3.\r\n\r\nWith a graph similar to https://github.com/tensorflow/tensorflow/issues/10736, how can such a decoder be implemented?\r\n\r\nMy suggestion is to create a separate decoder subclass that specifically handle this situtation to avoid confusion.", "comments": ["@ebrevdo could you take a look at this request?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "This looks like a great subproject that belongs in its own github repository.  Since no one on the TensorFlow team would be able to maintain it, we can't accept PRs for this at the moment.  If your issue is a question of how to implement this, the best venue for discussions of that sort is StackOverflow.\r\n\r\nThanks!"]}, {"number": 10991, "title": "Seq2Seq Documentation is Broken", "body": "A good third of the documentation on https://www.tensorflow.org/api_guides/python/contrib.seq2seq is written about a `DynamicAttentionWrapper` class.\r\n\r\nThe link posted for an API results in a 404: https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/DynamicAttentionWrapper\r\n\r\nThis class is not mentioned in `__init__.py` at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/__init__.py\r\n\r\nCan someone please either update the documentation or add the class to git so we can actually use it?", "comments": ["Thanks for filing the issue @jmoney4769!\r\n\r\nAfter some digging, I believe (but am not positive) that `DynamicAttentionWrapper` was renamed to just `AttentionWrapper` starting in v1.2.\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/seq2seq/AttentionWrapper\r\n\r\n@lukaszkaiser can you confirm, and perhaps fix up the documentation?  Or suggest someone who could?  Thanks!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Looks like the documentation has been fixed."]}, {"number": 10990, "title": "run on gpu in tensorflow backend .", "body": "Hi,\r\nHow can  be sure that the tensorflow+keras  run on GPU?i use tensorflow+keras with backend   . \r\n\r\nAlthough in keras manual Written ;\"If you are running on the TensorFlow or CNTK backends, your code will automatically run on GPU if any available GPU is detected\"\r\n\r\nHow can this be checked?\r\nThanks.\r\n\r\n\r\n\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 10989, "title": "resize_images: Alignment of upsampled image seems incorrect with method=NEAREST_NEIGHBOR and align_corners=True", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.2.1511 (Core)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.1.0-0-g1ec6ed5', '1.1.0')\r\n- **Bazel version (if compiling from source)**: 0.4.5- (@non-git)\r\n- **CUDA/cuDNN version**: 7.5/5.1\r\n- **GPU model and memory**: M40, 12GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nMy understanding of the behaviour of `tf.image.resize_images` with `align_corners=True` is that it would resize\r\n```\r\n[A B C D] => [A x x B x x C x x D]\r\n```\r\nif the input size was 4 and the output size was 3*(4-1)+1=10, where the `x` values would be determined by the method of interpolation.\r\n(I will provide 1D examples, although I am actually working with images.)\r\n\r\nI see the expected behaviour with `method=tf.image.ResizeMethod.BILINEAR`:\r\n```\r\n[0 3 6 9] => [0 1 2 3 4 5 6 7 8 9]\r\n```\r\nhowever, with `method=tf.image.ResizeMethod.NEAREST_NEIGHBOR` the result is not what I expect:\r\n```\r\n[0 3 6 9] => [0 0 0 3 3 3 6 6 6 9]    # actual result\r\n[0 3 6 9] => [0 0 3 3 3 6 6 6 9 9]    # desired result\r\n```\r\nNote that the nearest neighbor of the third `0` in the \"actual result\" is the element with value `3`.\r\n\r\n### Source code / logs\r\nI have attached a simple example that demonstrates this effect: [nearest.py.zip](https://github.com/tensorflow/tensorflow/files/1095366/nearest.py.zip)", "comments": ["#6720\r\nSeems to be fixed already.", "Oh thanks! It is fixed in 1.2.0"]}, {"number": 10988, "title": "[OpenCL] Fixes CUDA specific test run on SYCL (#56)", "body": "The testBadParentValuesOnGPU should only be run on CUDA devices, as the\r\ntest checks for particular CUDA behaviour. We don't actually provide a\r\nSYCL kernel for GatherTree and so it's not a problem that the tests\r\ndon't target SYCL.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10987, "title": "[OpenCL] Registers SquaredDifference", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "> UnknownError: Could not start gRPC server\r\n\r\nIs that caused by our change set?", "Doesn't look like it. @tensorflow-jenkins test this please."]}, {"number": 10986, "title": "What should be my --output_node_names for freeze_graph.py ?", "body": "I am trying to freeze graph a .pb and .ckpt file of a trained Inception model. I cannot understand what shall by my output-node-names?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10985, "title": "Update initializers.py", "body": "According to [Understanding the difficulty of training deep feedforward neural networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) and the codes, it is sqrt(2. / (in + out))", "comments": ["Can one of the admins verify this patch?", "@sguada what do you think?", "Not clear to me the paper mentioned that.\n\nOn Jun 27, 2017 12:09 AM, \"drpngx\" <notifications@github.com> wrote:\n\n> @sguada <https://github.com/sguada> what do you think?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10985#issuecomment-311196225>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABr0fAAf9uRW59WXpzQgtIWFBIBprdhIks5sICwpgaJpZM4OCR42>\n> .\n>\n", "@sguada The paper mentioned that in (eq.12). \r\nThe variance of the weights is  2. / (in + out) , so the standard deviation should be sqrt(2. / (in + out)).\r\n", "ping @sguada", "The normalized initialization is defined in eq. (16) \r\n`sqrt(6) / sqrt(n_in + n_out)`\r\nwhich equivalent to \r\n`sqrt(3 / ((n_in + n_out)/2))`", "@sguada \r\nEq. (16) uses uniform distribution.  \r\nIf **W** is sampled from a uniform distribution in the range **[-sqrt(6) / sqrt(n_in + n_out), sqrt(6) / sqrt(n_in + n_out)]**, then it has a standard deviation of **sqrt(2) / sqrt(n_in + n_out)**.\r\nSo when **W** is sampled from a **normal distribution**, the standard deviation should be **sqrt(2) / sqrt(n_in + n_out)**. \r\n\r\nYou could check the code in **https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/layers/python/layers/initializers.py** .\r\nIf **mode == 'FAN_AVG'**, it uses a standard deviation of **sqrt(2) / sqrt(n_in + n_out)** for normal distribution. Because it is sampled from a **truncated normal distribution**, so it multiply the factor with 1.3 to keep the standard deviation at **sqrt(2) / sqrt(n_in + n_out)**.", "Can one of the admins verify this patch?", "@zhongzyd Thanks for the clarification, I miss understood your previous comment.", "@sguada Thanks.", "Jenkins, test this please."]}, {"number": 10984, "title": "Building Tensorflow on Windows with AVX2 enable not compiling", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorflow/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10. Intel Core i7-6600U\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:1.2.0-rc0\r\n- **Bazel version (if compiling from source)**:No\r\n- **CUDA/cuDNN version**:No\r\n- **GPU model and memory**:No\r\n- **Exact command to reproduce**:\r\n\r\n1. Set up toolchain for for 64-bit:\r\n` vcvarsall amd64`\r\n2. Invoked CMAKE\r\n `C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.12\\swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:\\Users\\sergio.murillo\\AppData\\Local\\Programs\\Python\\Python35/PYTHON.EXE -DPYTHON_LIBRARIES=C:\\Users\\sergio.murillo\\AppData\\Local\\Programs\\Python\\Python35\\libs\\python35.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2`\r\n3. To build the PIP package\r\n`MSBuild /p:Configuration=Release /filelogger tf_python_build_pip_package.vcxproj`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI'm opening a new issue as suggested in [issue 10199](https://github.com/tensorflow/tensorflow/issues/10199) to track AVX2 support on Windows.\r\n\r\nI followed the instructions to built tensorflow on Windows using [CMAKE](https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake) and wanted to enable AVX2, but when it was time to build with MSBuild it returned 550 errors all similar to this:\r\n\r\n`\"C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default target) (7) -> (ClCompile target) -> c:\\projects\\tensorflow\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\packetmathavx2.h(274): error C3861: '_mm256_extract_epi16': identifier not found (compiling source file C:\\Projects\\tensorflow\\tensorflow\\core\\framework\\allocator_registry.cc) [C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj] c:\\projects\\tensorflow\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\packetmathavx2.h(278): error C3861: '_mm256_extract_epi8': identifier not found (compiling source file C:\\Projects\\tensorflow\\tensorflow\\core\\framework\\allocator_registry.cc) [C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj]`\r\n\r\nAll errors with the same code `C3861: identifier not found` regarding ` _mm256_extract_epi16` and `_mm256_extract_epi8`  \r\nI do have` immintrin.h` in C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include but `_mm256_extract_epi8` and `_mm256_extract_epi16` are not defined in that file.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I've just switched to Visual Studio 2017 Community. Its `immintrin.h` has definitions for `_mm256_extract_epi8` and `_mm256_extract_epi16`", "Fix for Visual Studio 2015 support:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/10199#issuecomment-334743528", "Closing as this is resolved"]}, {"number": 10983, "title": " freeze_graph script error : google.protobuf.text_format.ParseError ", "body": "I am trying to run the freeze_graph script on my own .pb and .ckpt file. However I am getting this error.\r\n\r\n```\r\ngoogle.protobuf.text_format.ParseError: 2:1 : Message type \"tensorflow.GraphDef\" has no field named \"j\".\r\n```\r\nThe stack trace is as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/gabbar/ML/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 255, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/gabbar/ML/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/gabbar/ML/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 187, in main\r\n    FLAGS.variable_names_blacklist)\r\n  File \"/home/gabbar/ML/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 165, in freeze_graph\r\n    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\r\n  File \"/home/gabbar/ML/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in _parse_input_graph_proto\r\n    text_format.Merge(f.read(), input_graph_def)\r\n  File \"/home/gabbar/.cache/bazel/_bazel_gabbar/3ef5463937ccade414be63dae84521e3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 481, in Merge\r\n    descriptor_pool=descriptor_pool)\r\n  File \"/home/gabbar/.cache/bazel/_bazel_gabbar/3ef5463937ccade414be63dae84521e3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 535, in MergeLines\r\n    return parser.MergeLines(lines, message)\r\n  File \"/home/gabbar/.cache/bazel/_bazel_gabbar/3ef5463937ccade414be63dae84521e3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 568, in MergeLines\r\n    self._ParseOrMerge(lines, message)\r\n  File \"/home/gabbar/.cache/bazel/_bazel_gabbar/3ef5463937ccade414be63dae84521e3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 583, in _ParseOrMerge\r\n    self._MergeField(tokenizer, message)\r\n  File \"/home/gabbar/.cache/bazel/_bazel_gabbar/3ef5463937ccade414be63dae84521e3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 652, in _MergeField\r\n    (message_descriptor.full_name, name))\r\ngoogle.protobuf.text_format.ParseError: 2:1 : Message type \"tensorflow.GraphDef\" has no field named \"j\".\r\n```", "comments": ["Did you figure it out?", "Yeah you just have to specify --input_binary=True.", "@abhiML Hi Abhi,\r\n\r\nI am getting the same error.. i was wondering where you gave the --input_binary=True as a  parameter.. ", "while running the file from terminal."]}, {"number": 10982, "title": "python mnist_softmax_xla.py run failure", "body": "Hi,\r\nI run python mnist_softmax_xla.py and got below failure:\r\n\r\nlinux-swfm:~/workarea/test> python3 mnist_softmax_xla.py   \r\n...\r\n2017-06-22 20:02:52.685534: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (1): Tesla K40m, Compute Capability 3.5\r\n2017-06-22 20:02:57.741927: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user->operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7f30cc012550 vs. 0x7f30cc021490)\r\nAborted\r\n\r\nmy tensorflow version is tensorflow-1.1.0\r\ncuda sdk: 7.5\r\n ", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: SUSE Linux Enterprise Server 11 SP3  (x86_64)\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: tensorflow-1.1.0\r\n- **Bazel version (if compiling from source)**: bazel release 0.5.0- (@non-git)\r\n- **CUDA/cuDNN version**: CUDA 7.5/cuDNN 5.1\r\n- **GPU model and memory**: Tesla K40m\r\n- **Exact command to reproduce**: python3 mnist_softmax_xla.py\r\n\r\n### Describe the problem\r\nwhen run python3 mnist_softmax_xla.py, I met a assertion failure.\r\n\r\nmy build command is :\r\nbazel build --c dbg --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Source code / logs\r\n\r\nlinux-swfm:~/workarea> python3 mnist_softmax_xla.py\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-06-24 10:28:06.834688: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 10:28:06.834747: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 10:28:06.834762: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 10:28:06.834787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 10:28:06.834809: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 10:28:07.156861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: Tesla K40m\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:04:00.0\r\nTotal memory: 11.25GiB\r\nFree memory: 11.15GiB\r\n2017-06-24 10:28:07.157144: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x7ff0ac098f10\r\n2017-06-24 10:28:07.460466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \r\nname: Tesla K40m\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:83:00.0\r\nTotal memory: 11.25GiB\r\nFree memory: 11.15GiB\r\n2017-06-24 10:28:07.460571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 1\r\n2017-06-24 10:28:07.460613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 0\r\n2017-06-24 10:28:07.460672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \r\n2017-06-24 10:28:07.460692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y N \r\n2017-06-24 10:28:07.460712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   N Y \r\n2017-06-24 10:28:07.460751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0)\r\n2017-06-24 10:28:07.460770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40m, pci bus id: 0000:83:00.0)\r\n2017-06-24 10:28:07.838088: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices\r\n2017-06-24 10:28:07.838941: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 72 visible devices\r\n2017-06-24 10:28:07.852362: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x7ff0acde4930 executing computations on platform Host. Devices:\r\n2017-06-24 10:28:07.852407: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-06-24 10:28:07.852819: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices\r\n2017-06-24 10:28:07.853650: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 72 visible devices\r\n2017-06-24 10:28:07.866012: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x7ff0ad069f30 executing computations on platform CUDA. Devices:\r\n2017-06-24 10:28:07.866043: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\r\n2017-06-24 10:28:07.866056: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (1): Tesla K40m, Compute Capability 3.5\r\n2017-06-24 10:28:12.890410: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user->operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7ff0b4033ba0 vs. 0x7ff0b404d3e0)\r\nAborted\r\n", "This may be due to lack of python3 support. @tfboyd, care to take a look?", "I will take a look.  I have doubts python3 matters.  ", "Ping myself.  My test will be weak.  I am going to give this a try locally today with python2 and head just to make sure nothing is wrong.", "I know this is not super helpful although I had an idea at the end.  I just ran the following setup and the example worked.  I double checked the timeline to ensure XLA was 100% running:\r\n\r\n- CUDA 8\r\n- cuDNN 5.1\r\n- TF (head past 1.2):  3a64879a86e46908ad90a387efe56ad32be61e94\r\n- GTX 1080, which should not matter\r\n- Python2.7, I doubt python is the issue as the error is coming from the `op` but it would not be the first time I was really wrong.  \r\n- The AVX,SSE stuff also doesn't matter and we are working on a simpler warning.\r\n\r\nCompletely random thought.  I see you have two GPUs.  The code is not setup for multi-GPU, the example is also really basic but still I wonder if that is creating a problem.  You could try with one GPU and run something like this to try it out.  I have never tried the example on multi-GPU.      \r\n\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0 python mnist_softmax_xla.py\r\n```\r\n", "I tried with one GPU, but got the same failure.\r\n\r\nlinux-swfm:~/workarea> CUDA_VISIBLE_DEVICES=0 python3  mnist_softmax_xla.py\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-07-06 10:07:08.126795: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 10:07:08.126848: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 10:07:08.126862: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 10:07:08.126872: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 10:07:08.126886: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 10:07:12.868401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: Tesla K40m\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:04:00.0\r\nTotal memory: 11.25GiB\r\nFree memory: 11.15GiB\r\n2017-07-06 10:07:12.868480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-07-06 10:07:12.868498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-07-06 10:07:12.868534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0)\r\n2017-07-06 10:07:13.073789: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-07-06 10:07:13.074631: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 72 visible devices\r\n2017-07-06 10:07:13.086519: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x7f96e89c4d40 executing computations on platform Host. Devices:\r\n2017-07-06 10:07:13.086559: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-07-06 10:07:13.086943: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-07-06 10:07:13.087747: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 72 visible devices\r\n2017-07-06 10:07:13.099575: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x7f96e8c4a370 executing computations on platform CUDA. Devices:\r\n2017-07-06 10:07:13.099606: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\r\n2017-07-06 10:07:17.919040: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user->operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7f96e8c6dc70 vs. 0x7f96e8c9f6b0)\r\nAborted", "My final thought is to compile the latest version from head, which is where I started.  It is entirely possible there was an XLA issue in the 1.1 branch.  I would also use CUDA 8.  With XLA being experimental I try to stick with the most common settings and the most recent code.\r\n\r\nI know this can be frustrating and is why it is still marked as experimental.  If I think of anything else I will update this issue.  ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Adding @hpucha incase there is interest in fixing or enhancing the tutorial or close.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hpucha: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hpucha: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I retried the tutorial again locally (with TitanX, CUDA 8 and the latest Tensorflow version) and it worked. \r\n\r\nI'm going to close this because I couldn't reproduce this issue in the latest version. But please don't hesitate to reopen this if it's still not working for you. \r\n\r\nThank you!"]}, {"number": 10981, "title": "[OpenCL] REGISTER -> REGISTER6", "body": "Fixes compilation error for the SYCL target", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10980, "title": "python_configure.bzl: Add python import library on Windows", "body": "When building a Python C/C++ extension on Windows, we need to link to the python [import library](https://msdn.microsoft.com/en-us/library/windows/desktop/ms682592(v=vs.85).aspx) pythonXY.lib (eg. `C:\\Program Files\\Anaconda3\\libs\\python35.lib`)\r\nSee https://docs.python.org/3/extending/windows.html\r\n\r\nPreviously, TensorFlow relies on Bazel [adding ](https://github.com/bazelbuild/bazel/blob/master/tools/cpp/windows_cc_configure.bzl#L324)the python lib path `C:\\Program Files\\Anaconda3\\libs` into `LIB` environment variable, the build will break after Bazel stop doing so.\r\n\r\nTo solve this problem, we generate a python_import_lib rule. Ideally, it should be a `cc_library`, but when building a dll, Bazel add /WHOLEARCHIVE to `python35.lib` because it cannot tell it's a static library or import library(they have the same extension `.lib`). This will cause a linking error. As a workaround, we put it into `linkopts` if `python_headers`.\r\nSee more details at https://github.com/bazelbuild/bazel/issues/3237", "comments": ["http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/27/console", "It's broken all the tests, let me investigate...", "+CC @nlopezgi to make sure we are not undoing any of his changes.", "Turns out there are other lib files under `C:\\Program Files\\Anaconda3\\libs` that cannot be passed to linker, I've updated the change to only use `pythonXY.lib` as output of the genrule.\r\nTesting at http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/29/console", "@nlopezgi Does this change look good to you?", "sorry for the late reply, lgtm"]}, {"number": 10979, "title": "Not able to import tensorflow .", "body": "Os:  Windows 10.\r\nPython version:3.5.2\r\n\r\n**Installed the CPU-only version of TensorFlow:** and via native pip...not anaconda\r\n\r\nsuccesfully installed tensorflow:\r\n\r\nBut while importing tensorflow below error is coming:\r\n\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#2>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Santanu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nHow to resolve this?????", "comments": ["You posted the link to the common installation problems.  Your error is the last one of the three.  Did you follow the instructions to fix it on stack overflow? \r\n\r\nLink to question: http://stackoverflow.com/q/35953210", "@Santanu3dutta, as @jmoney4769 points out, this seems to be covered in the common_installation_problems link.\r\n\r\nAlso note for the future: this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10978, "title": "[OpenCL] Provides SYCL kernels for 3D pooling (#97)", "body": "* [OpenCL] Adds SYCL kernels for 3D pooling\r\n\r\nUses simple SYCL kernels to provide implementations for all 3D pooling\r\nops currently in use. These kernels pass the tests, but haven't really\r\nbeen optimized.\r\n\r\nThese need benchmarking to compare with Eigen and CPU kernels.\r\n\r\n* [OpenCL] Refactors SYCL kernels to use parameter struct\r\n\r\nMoves a lot of the functor parameters into a separate data struct, with\r\nthe aim of simplifying the functor code.\r\n\r\n* [OpenCL] Removes extra fetching of tensor dimensions\r\n\r\nWe already had the tensor dimensions passed into\r\nLaunchMaxPooling3dGradOP, so don't need to fetch them from the\r\ntensor.\r\n\r\n* [OpenCL] Renames SYCL 3D pooling kernels\r\n\r\nAdds '3D' to kernel names.\r\n\r\n* [OpenCL] Adds 3D pooling SYCL kernel documentation\r\n\r\n* [OpenCL] Adds guards around SYCLDevice typedef\r\n\r\n* [OpenCL] Use forward input for SYCL MaxPool3DGradGrad\r\n\r\nWhen we had a mix of SYCL and CPU kernels the forward_input would break\r\nand cause computation problems. Now that we have SYCL kernels for all 3D\r\npooling operations, this is not a problem.\r\n\r\n* [OpenCL] Reformats SYCL 3D pooling code\r\n\r\n* [OpenCL] Moves SYCL utils into separate header\r\n\r\n* [OpenCL] Simplifies SYCL Pool param contructors\r\n\r\nInstead of each constructor initialising the data, simplifies the\r\nconstructors to call the first constructor.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@mehdi-goli should be now in CLA ", "CLAs look good, thanks!\n\n<!-- ok -->", "Closing. We will open new PR with implementation that doesn't use atomics.", "Can one of the admins verify this patch?", "@benoitsteiner could you have a look at this updated version that does not contain atomics", "ping for @benoitsteiner ", "@benoitsteiner is that better?", "Jenkins, test this please."]}, {"number": 10977, "title": "Support for tf.nn.max_pool_with_argmax on CPU", "body": "Hello,\r\n\r\nRecently I trained a  model using **tf.nn.max_pool_with_argmax** on GPU and its working fine on GPU. I wanted to use the model on CPU but it seems that its not supported on CPU. How can I use this on CPU?\r\n\r\nWill there be any support for it in near future? Or any suggestions on how to use this on CPU would be great.\r\n\r\nThanks! ", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "Please reopen this issue", "Still an issue. Are there any technical pitfalls in CPU implementation of this demanded function? ", "@apatsekin \r\nI posted a patch as a workaround here:\r\nhttps://github.com/tensorflow/tensorflow/issues/6035\r\n\r\nIt might be helpful."]}, {"number": 10976, "title": "Adding new classes", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorflow/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.2.0rc2\r\n- **Bazel version (if compiling from source)**:0.5.0\r\n- **CUDA/cuDNN version**:--\r\n- **GPU model and memory**:---\r\n- **Exact command to reproduce**:--\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI want to retrain the inception model with 1003 classes where the first 1000 classes are same as imagenet. So I took with inception model and extracted the  final layer weights and added 3 more columns to it.I popped the final layer created another layer with 1003 classes and with the weights i have changed,as the weights of first 1000 classes remains same as inception but  while training the accuracy is starting from 0 which i didn't expect.What is going wrong..?\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10975, "title": "The issue template has an incorrect link.", "body": "Ironically, I think the issue template is wrong. I presume that point 3 below is meant to point to the tensorboard repo issue tracker `https://github.com/tensorflow/tensorboard/issues` but as you can see it just points back to the main repo issue tracker! Judging by the activity on that issue tracker perhaps this is on purpose but I doubt it!\r\n\r\n\r\n`3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorflow/issues).`\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go >>> [here](https://github.com/tensorflow/tensorflow/issues). <<<\r\n\r\nCheers", "comments": ["@jubjamie Haha, thanks for reporting the issue.  I just sent a PR to fix this.\r\n\r\nYeah, a real facepalm moment.  :)", "Closing this out, now that #11018 has been merged.", "No worries. Made me chuckle too :)"]}, {"number": 10974, "title": "Update WorkerCacheLogger::RecordRecvTensor", "body": "Update WorkerCacheLogger::RecordRecvTensor to let future improvements to tracing tools:\r\n* Add a placeholder for more details about a RecvTensor call.\r\n* Custom title to possibly distinguish between control and data flow RecvTensor calls.", "comments": ["@mrry @prb12 Will you take a look at this?", "Jenkins, test this please."]}, {"number": 10973, "title": "Update WorkerCacheLogger::RecordRecvTensor to let future improvements\u2026", "body": "\u2026 to tracing tools:\r\n\r\n* Add a placeholder for more details about a RecvTensor call.\r\n* Custom title to possibly distinguish between control and data flow RecvTensor calls.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@mrry @prb12 Can you look at this?", "Updated CLA!"]}, {"number": 10972, "title": "Tensorflow 1.2: FileWriter needs to be created after tf.text.summary ops?", "body": "The following works and creates a tf.text.summary which I can find via tensorboard:\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\nsummary_op = tf.summary.text('config/config', tf.convert_to_tensor('hello world'))\r\nsummary_writer = tf.summary.FileWriter('/tmp/tensorboard', sess.graph)\r\ntext = sess.run(summary_op)\r\nsummary_writer.add_summary(text, 0)\r\nsummary_writer.add_summary(text, 100)\r\nsummary_writer.add_summary(text, 200)\r\nsummary_writer.flush()\r\nsummary_writer.close()\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/6200749/27425155-c4a1199a-5737-11e7-89f8-9ae3bd4159b4.png)\r\n\r\nIf we change the order of the FileWriter and the summary_op above it does not log anything:\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\nsummary_writer = tf.summary.FileWriter('/tmp/tensorboard', sess.graph)\r\nsummary_op = tf.summary.text('config/config', tf.convert_to_tensor('hello world'))\r\ntext = sess.run(summary_op)\r\nsummary_writer.add_summary(text, 0)\r\nsummary_writer.add_summary(text, 100)\r\nsummary_writer.add_summary(text, 200)\r\nsummary_writer.flush()\r\nsummary_writer.close()\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/6200749/27425124-a2af2926-5737-11e7-9daa-53cda286cc67.png)\r\n", "comments": ["Does this happen with non-text summaries?", "The scalar data appears just fine as demonstrated here:\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\nsummary_writer = tf.summary.FileWriter('/tmp/tensorboard', sess.graph)\r\nsummary_op = tf.summary.text('config/config', tf.convert_to_tensor('hello world'))\r\nanother_summary_op = tf.summary.scalar('scalar/data', tf.convert_to_tensor(3.14))\r\ntext = sess.run(summary_op)\r\nscalar = sess.run(another_summary_op)\r\nfor data in [text, scalar]:\r\n    summary_writer.add_summary(data, 0)\r\n    summary_writer.add_summary(data, 100)\r\n    summary_writer.add_summary(data, 200)\r\nsummary_writer.flush()\r\nsummary_writer.close()\r\n```", "I had a similar problem. \r\nMy workaround: I created the `plugins/tensorboard_text` directory with the `tensors.json` file (containing the name of the tensor) and then it worked for me. So my educated guess is, that the `FileWriter` needs the information that summary contains a `tf.text.summary` to create the named file/directory.", "Both of the code-samples given by @hholst80 in the original report fail for me with the most recent git version of TF (v1.3.0-rc1-810-g349932f) -- the `/tmp/tensorboard` folder is created, and contains a tfevents file, but that is the only thing in the folder (i.e. there is no `plugins` subfolder, or anything else).\r\n", "Not sure if this helps any, but it turns out that by restoring some of the code that was present in the git tree at tag r1.2, the plugins subfolder and asset file is written.  Before the `return t_summary` line in `tensorflow/python/summary/text_summary.py` should be the following lines:\r\n\r\n    text_assets = plugin_asset.get_plugin_asset(TextSummaryPluginAsset)\r\n    text_assets.register_tensor(t_summary.op.name)\r\n\r\nThis does re-introduce the annoying \"WARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.\" message, but at least the plugins folder is created.\r\n\r\nBUT, even though the `config` header shows up in tensorboard now, it still doesn't show the `hello world` string when expanded -- only an empty `config/config` item.\r\n", "One other note:  I _am_ able to use tensorboard to see text summaries in tfevents files created from an older TF version (git commit 08ed32d).  I visually inspected two raw tfevents files (one whose text summaries can be viewed, and another whose summaries can't be viewed), and both contain the text-summary content (what _should_ be displayed in tensorboard), so the data is being written somehow to the tfrecord file in either case.\r\n\r\nSo, what this means is that there seem to be two issues:  \r\n  1. the missing `plugins` folder and contents, and\r\n  2.  a new (buggy?) way that the text data is being written to the tfrecord file which tensorboard doesn't like (doesn't show the text data).\r\n", "Since you guys are using 1.3, one thing to note is that we recently moved TensorBoard out of the TensorFlow repository.\r\n\r\n@chihuahua made some changes to text summaries recently, so he would be the best person to comment.", "> we recently moved TensorBoard out of the TensorFlow repository.\r\n\r\n\u2026and please note that using TensorBoard from source (GitHub master) requires the latest TensorFlow nightly. The `tensorflow-tensorboard` package does not require this.", "**A Brief Discourse on TensorBoard History**\r\nLong ago, as was the case in TensorFlow 1.2, text summaries wrote a separate `plugins` directory that TensorBoard would later use to compute relevant plugins. This \"plugin assets\" system for implementing plugins in TensorBoard exhibited some downsides and bugs (such as the original issue raised by @hholst80).\r\n\r\nWe thus deprecated the plugin assets system (and TensorFlow 1.3 stopped creating the now irrelevant `plugins` directory when text summaries are written) in favor of storing plugin data within tensor values written to disk. \r\n\r\nTensorBoard at HEAD (of the GitHub repo) can read both old-style (plugin assets) and new-style (tensor summary based) text summaries. However, the current release of TensorBoard (0.1.2) is too old to be able to read the new-style summaries.\r\n\r\n**All in all ...**\r\nThe problem noticed by @edgimar stems from how TensorFlow 1.3 writes new-style summaries, but TensorBoard 0.1.2 can't parse them.\r\n\r\nTensorBoard team is currently working on a 0.1.3 release of TensorBoard that will solve this problem.", "@hholst80 or @edgimar , could you please try running\r\n```\r\npip install tensorflow==1.3.0 tensorflow-tensorboard==0.1.4\r\n```\r\nand seeing if this fixes your problem? Note that TensorBoard 0.1.4 requires TensorFlow at least 1.3.0, which was released today.", "@wchargin With TF version 1.3.0 and TB 0.1.4, text summaries seem to be working.  @chihuahua Thanks for the explanation!\r\n", "Glad to hear it. Thanks for letting us know.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chihuahua: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chihuahua: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chihuahua: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chihuahua: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chihuahua: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Issue appears to be solved based on last comments. Closing this out."]}, {"number": 10971, "title": "TensorFlow 1.2.0 update still not supporting Python 3.6.1 on Windows 10", "body": "\r\n### System information\r\n- **Windows 10 (64-bit)**\r\n- **Python 3.6.1**\r\n\r\nEven with the new update the command:\r\n\r\n`pip install tensorflow_gpu-1.2.0-cp36-cp36m-win_amd64.whl` \r\n\r\nOR as in the official documentation\r\n\r\n`pip3 install --upgrade tensorflow`\r\n\r\nreturns\r\n\r\n> tensorflow_gpu-1.2.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\r\n\r\nAND\r\n\r\n> Could not find a version that satisfies the requirement tensorflow (from versions: )\r\n> No matching distribution found for tensorflow\r\n\r\nrespectively.", "comments": ["What output do you get when you run `pip --version` and `pip3 --version`?", "Looks like I had the 32-bit version of Python 3.6.1.\r\nMy sincere apologies.", "Don't worry about it - thanks for confirming!"]}, {"number": 10970, "title": "Please modify the tensorflow/workspace.bzl's eigen version to make Nvidia TX2 compilation successful", "body": "Hi,all\r\n   I just managed to compile Tensorflow 1.2 successfully in Nvidia TX2. Despite the patches made by this article: http://www.jetsonhacks.com/2017/04/02/tensorflow-on-nvidia-jetson-tx2-development-kit/ , the workspace.bzl file needs to be modified to use the latest eigen version.\r\nSpecifically, you need to change the following lines:\r\n\r\n>  native.new_http_archive(\r\n>       name = \"eigen_archive\",\r\n>       urls = [\r\n>           \"http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\",\r\n>           \"https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\",\r\n>       ],\r\n>       sha256 = \"ca7beac153d4059c02c8fc59816c82d54ea47fe58365e8aded4082ded0b820c4\",\r\n>       strip_prefix = \"eigen-eigen-f3a22f35b044\",\r\n>       build_file = str(Label(\"//third_party:eigen.BUILD\")),\r\n>   )\r\n\r\nto:\r\n\r\n>  native.new_http_archive(\r\n>       name = \"eigen_archive\",\r\n>       urls = [\r\n>           #\"http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\",\r\n>           #\"https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\",\r\n>           \"https://bitbucket.org/eigen/eigen/get/3.3.4.tar.gz\",\r\n>       ],\r\n>       sha256 = \"the sha256 value you used to verify\",\r\n>       strip_prefix = \"eigen-eigen-5a0156e40feb\",\r\n>       build_file = str(Label(\"//third_party:eigen.BUILD\")),\r\n>   )\r\n\r\nSince I'm not sure if the newest eigen works well in other platform, I don't have confidence to ask for a PR.\r\nI just post this issue in case some of you guys can't compile tensor flow 1.2 in nvidia TX2.", "comments": ["I think the best course forward here would be to propose this as a PR. We can then review it and see whether it makes sense to make this change. Thanks!"]}, {"number": 10969, "title": "Tensorflow Android how to set multi input for my own model?", "body": "Beside the images that I need to transform into the classifier, I also need to put another input named \"`phase_train`\" and set as  \"`False`\". How to make it?", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nIt sounds like you want to put a custom model in. Follow the comments in ClassifierActivity to see how you can put in an Inceptionv3 model and adjust the parameters to put your own model in.", "@jubjamie \r\nAny link is aviliable? I'm find no issue that meet my problem", "_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java#L45 gives you the intructions for using an inception model. You might be able to adjust this for your own model. You can enter a custom .pb model in the demo provided that you can get it into a form similar to the inception examples. This may require node renaming and stripping. This isn't a Tensorflow bug however and I see that you have opened a question on Stack Overflow so please close this issue. I can head over to Stack Overflow to try and help you further.", "@jubjamie \r\nThanks a lot"]}, {"number": 10968, "title": "bayesflow w/ higher order functions loops endlessly", "body": "**Environment**: TensorFlow v1.2.0-rc2-21-g12f033d 1.2.0 running on Mac OS X (v10.12.5)\r\n\r\n**Issue**:\r\nUse of `tensorflow.contrib.bayesflow.stochastic_tensor.StochasticTensor` in conjunction with higher order functions, such as `tf.map_fn` and `tf.while_loop`, results in a seemingly endless loop during construction of the associated stochastic graph's `surrogate_loss`. Within the source code, the aforementioned loop occurs [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/bayesflow/python/ops/stochastic_graph_impl.py#L85). A minimal example is provided below.\r\n\r\n**Example**:\r\n```\r\n# Dependencies\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.distributions import Bernoulli\r\nfrom tensorflow.contrib.bayesflow import\\\r\n\tstochastic_tensor as st, stochastic_graph as sg\r\n\r\n# Flags and variable declaration\r\n_int, _float = 'int32', 'float32'\r\nnum_logits = 0 #doesn't seem to matter\r\nlogits = tf.get_variable('logits', shape=[num_logits, 1], dtype=_float)\r\n\r\n# Pass `logits` to a distribution and then wrap with a StochasticTensor\r\ndist = Bernoulli(logits, dtype=_float)\r\nwith st.value_type(st.SampleValue()):\r\n\tsamples = st.StochasticTensor(dist)\r\n```\r\nBuilding off of the above, the following seem to loop endlessly. Using `tf.map_fn`,\r\n```\r\nlosses = tf.map_fn(lambda x: x, samples, _float)\r\nloss = sg.surrogate_loss([tf.reduce_mean(losses, axis=0)])# <- loops endlessly\r\n```\r\nUsing `tf.while_loop` in conjunction with `tf.TensorArray`,\r\n```\r\narray = tf.TensorArray(_float, num_logits).unstack(samples)\r\ndef cond(k, *args):\r\n\treturn tf.less(k, num_logits)\r\ndef body(k, losses):\r\n\treturn k+1, tf.concat([losses, array.read(k)], 0)\r\nloop_v = [0, tf.zeros([0], _float)]\r\ninvars = [tf.TensorShape([]), tf.TensorShape([None])]\r\nlosses = tf.while_loop(cond, body, loop_v, shape_invariants=invars)[1]\r\nloss = sg.surrogate_loss([tf.reduce_mean(losses, keep_dims=True)])# <- loops endlessly\r\n```\r\n", "comments": ["Hello and thanks for raising this issue.\r\n\r\nUnfortunately control flow primitives are difficult to support with StochasticTensors.  Currently, the best solution is to sample and calculate log_prob at every timestep within the while_loop and emit both; then calculate the loss yourself."]}, {"number": 10967, "title": "Fix typos", "body": "This PR fixes some typos: `intialized`, `be be`, `by by`, `in in`, `tranformation`, `new new`, `constaint`, and `mean_square_error`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}]