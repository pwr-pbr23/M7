[{"number": 18496, "title": "Issue with Colab Pytnotebook and Tensorflow hub", "body": "I was trying the Colab example with Tensorflow hub [here](https://www.tensorflow.org/tutorials/text_classification_with_tf_hub)\r\n\r\n```\r\n# Install the latest Tensorflow version.\r\n!pip install --quiet --upgrade --pre tensorflow\r\n# Install TF-Hub.\r\n!pip install tensorflow-hub\r\n!pip install tf-nightly\r\n```\r\n\r\nto have the nightly build, but when I try to install\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport pandas as pd\r\nimport re\r\nimport seaborn as sns\r\n```\r\n\r\nI get this version error\r\n\r\n```\r\nRuntimeErrorTraceback (most recent call last)\r\n<ipython-input-6-48e1bdaa8642> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 import tensorflow_hub as hub\r\n      3 import matplotlib.pyplot as plt\r\n      4 import numpy as np\r\n      5 import os\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_hub/__init__.py in <module>()\r\n     63 \r\n     64 # Comment/uncomment to skip checking the TensorFlow version.\r\n---> 65 _check_tensorflow_version(tf.VERSION)\r\n     66 \r\n     67 # Used by doc generation script.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_hub/__init__.py in _check_tensorflow_version(version)\r\n     60       \"TensorFlow Hub depends on 'tf-nightly' build after %s or \"\r\n     61       \"'tensorflow~=%s'. Found tf.VERSION = %s\" % (\r\n---> 62           _NIGHTLY_VERSION, _MAIN_VERSION, version))\r\n     63 \r\n     64 # Comment/uncomment to skip checking the TensorFlow version.\r\n\r\nRuntimeError: TensorFlow Hub depends on 'tf-nightly' build after 20180308 or 'tensorflow~=1.7'. Found tf.VERSION = 1.6.0\r\n```\r\n\r\nBut I have already installed\r\n\r\n```\r\nCollecting tf-nightly\r\n  Downloading tf_nightly-1.8.0.dev20180331-cp36-cp36m-manylinux1_x86_64.whl (48.6MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48.6MB 28kB/s \r\nRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nCollecting tb-nightly<1.9.0a0,>=1.8.0a0 (from tf-nightly)\r\n  Downloading tb_nightly-1.8.0a20180409-py3-none-any.whl (3.1MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 382kB/s \r\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tf-nightly)\r\nRequirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.9.0a0,>=1.8.0a0->tf-nightly)\r\nRequirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.9.0a0,>=1.8.0a0->tf-nightly)\r\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.9.0a0,>=1.8.0a0->tf-nightly)\r\nRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.9.0a0,>=1.8.0a0->tf-nightly)\r\nInstalling collected packages: tb-nightly, tf-nightly\r\nSuccessfully installed tb-nightly-1.8.0a20180409 tf-nightly-1.8.0.dev20180331\r\n```\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler updated.", "I solved the same issue by restarting the environment and then using:\r\n```\r\n!pip install tensorflow==1.7\r\n!pip install tensorflow_hub\r\n```\r\n\r\nHope that helps!", "@jure it works!", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18495, "title": "Add deprecation args decoration for tf.squeeze", "body": "This fix adds deprecation args decoration to `tf.squeeze`,\r\nwith deprecates `squeeze_dims` with `axis`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18494, "title": "Fix warnings in `nn.sampled_softmax_loss`", "body": "The `softmax_cross_entropy_with_logits` has been deprecated and replaced with `softmax_cross_entropy_with_logits_v2`.\r\n\r\nThis causes `nn.sampled_softmax_loss` to always generate a WANRING whenever called. This fix replaces `softmax_cross_entropy_with_logits` with `softmax_cross_entropy_with_logits_v2` and maintains the existing behavior to fix the warning.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18493, "title": "Tensorflow js returns NAN on large data set ", "body": "\r\n<html>\r\n--\r\n\u00a0 | <head>\r\n\u00a0 | <!-- Load TensorFlow.js -->\r\n\u00a0 | <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.9.0\"> </script>\r\n\u00a0 | \u00a0\r\n\u00a0 | <!-- Place your code in the script tag below. You can also use an external .js file -->\r\n\u00a0 | <script>\r\n\u00a0 | <!--var x=[];-->\r\n\u00a0 | <!--var y=[];-->\r\n\u00a0 | <!--for(var i=1;i<=100;i++){-->\r\n\u00a0 | <!--x.push(i);-->\r\n\u00a0 | <!--x.push(i+2);-->\r\n\u00a0 | <!--}-->\r\n\u00a0 | const model = tf.sequential();\r\n\u00a0 | model.add(tf.layers.dense({units: 1, inputShape: [1]}));\r\n\u00a0 | model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\r\n\u00a0 | const xs = tf.tensor2d([1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24], [24, 1]);\r\n\u00a0 | const ys = tf.tensor2d([3, 4, 5, 6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26], [24, 1]);\r\n\u00a0 | model.fit(xs, ys).then(() => {\r\n\u00a0 | model.predict(tf.tensor2d([11], [1, 1])).print();\r\n\u00a0 | });\r\n\u00a0 | </script>\r\n\u00a0 | </head>\r\n\u00a0 | \u00a0\r\n\u00a0 | <body>\r\n\u00a0 | </body>\r\n\u00a0 | </html>\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18492, "title": "Worker is failing with InvalidArgumentError>, \"A session is not created yet\"", "body": "System information\r\n------------------\r\n\r\n - Have I written custom code - Yes\r\n - OS Platform CentOS - 7.2.1511\r\n - TensorFlow installed from - Binary\r\n - TensorFlow version - 1.3.0\r\n - Python version - 2.7\r\n - Bazel version - N/A\r\n - CUDA/cuDNN version - N/A\r\n - GPU model and memory - N/A\r\n\r\nSource Code\r\n-----------\r\nSharing my code snippet below,\r\n\r\n    cluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n    \r\n    server = tf.train.Server(\r\n        cluster,\r\n        job_name=FLAGS.job_name,\r\n        task_index=FLAGS.task_index)\r\n       \r\n    if FLAGS.pipeline_id is None:\r\n        raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\r\n    \r\n    #print('job name '+FLAGS.job_name)\r\n    #print('task index name ' + FLAGS.task_index)\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    elif FLAGS.job_name == \"worker\":\r\n        print('only for worker')\r\n        # if not os.path.exists(FLAGS.train_dir):\r\n        #    os.mkdir(FLAGS.train_dir)\r\n        train_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/training'\r\n        eval_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/eval'\r\n        print 'Train logs path -- ', train_logs_path\r\n        print 'Eval logs path -- ', eval_logs_path\r\n    \r\n        tf.logging.set_verbosity(tf.logging.INFO)  # Set the verbosity to INFO level\r\n    \r\n        # First create the dataset and load one batch\r\n    \r\n        ##\r\n        ## replace flowers with FLAGS.pipeline_id in get_split()\r\n        ##\r\n        ##\r\n        labels_file = FLAGS.tf_records_dir + '/labels.txt'\r\n    \r\n        dataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\r\n        print 'num of classes ------------------->', dataset.num_classes\r\n        images, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\r\n                                       width=FLAGS.image_resize, is_training=True)\r\n    \r\n        # Get number of steps to decay\r\n        num_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\r\n        num_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\r\n        decay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\r\n    \r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                cluster=cluster)):\r\n       \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n            #======================= INITIATE TRAINING  =========================\r\n    \r\n    \r\n    \r\n    \r\n    \r\n            #Create the model inference\r\n            with slim.arg_scope(inception_v3_arg_scope()):\r\n                logits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\r\n    \r\n    \r\n    \r\n            exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\r\n            #exclude = get_variables_to_exclude()\r\n            for i in exclude:\r\n               print \"var to exclude -> \",i\r\n            variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\r\n    \r\n            #Perform one-hot-encoding of the labels\r\n            one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\r\n    \r\n            #Calculate loss\r\n            loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\r\n            total_loss = tf.losses.get_total_loss()\r\n    \r\n            #Create the global step\r\n            global_step = get_or_create_global_step()\r\n    \r\n    \r\n            #Define your exponentially decaying learning rate\r\n            lr = tf.train.exponential_decay(\r\n                learning_rate = FLAGS.initial_learning_rate,\r\n                global_step = global_step,\r\n                decay_steps = decay_steps,\r\n                decay_rate = FLAGS.learning_rate_decay_factor,\r\n                staircase = True)\r\n    \r\n            #Get optimizer as configured by user\r\n            optimizer = tf.train.AdamOptimizer(learning_rate = lr)\r\n            #optimizer = getOptimizer(learning_rate = lr)\r\n    \r\n            #Create the train_op.\r\n            variables_to_train = get_variables_to_train()\r\n            #for j in variables_to_train:\r\n              #print \"var to train \",j\r\n            #vn = tf.trainable_variables()\r\n            train_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\r\n    \r\n    \r\n            predictions = tf.argmax(end_points['Predictions'], 1)\r\n            probabilities = end_points['Predictions']\r\n            accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\r\n            metrics_op = tf.group(accuracy_update, probabilities)\r\n    \r\n    \r\n            #create all the summaries you need to monitor\r\n            tf.summary.scalar('losses/Total_Loss', total_loss)\r\n            tf.summary.scalar('accuracy', accuracy)\r\n            tf.summary.scalar('learning_rate', lr)\r\n            my_summary_op = tf.summary.merge_all()\r\n    \r\n            #Define train step to run training operation\r\n            def train_step(sess, train_op, global_step):\r\n    \r\n                start_time = time.time()\r\n                total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\r\n                time_elapsed = time.time() - start_time\r\n    \r\n    \r\n                logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\r\n    \r\n                return total_loss, global_step_count\r\n    \r\n        #Restore variables from checkpoint\r\n        saver = tf.train.Saver(variables_to_restore)\r\n        def restore_fn(sess):\r\n            return saver.restore(sess, FLAGS.checkpoint_path)\r\n    \r\n        #Create supervisor\r\n        writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\r\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\r\n                                 summary_writer=writer,\r\n                                 summary_op = my_summary_op, init_fn = restore_fn)\r\n    \r\n    \r\n        #Run the managed session\r\n        with sv.prepare_or_wait_for_session(server.target) as sess:\r\n            #writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\r\n            for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\r\n                #Log info at each epoch:\r\n                if step % num_batches_per_epoch == 0:\r\n                    logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\r\n                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])\r\n                    logging.info('Current Learning Rate: %s', learning_rate_value)\r\n                    logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\r\n    \r\n    \r\n                    logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\r\n                    print 'logits: \\n', logits_value\r\n                    print 'Probabilities: \\n', probabilities_value\r\n                    print 'predictions: \\n', predictions_value\r\n                    print 'Labels:\\n:', labels_value\r\n    \r\n                #Log the summaries every 10 step.\r\n                if step % FLAGS.steps_update_frequency == 0 and step != 0:\r\n                    loss, gs = train_step(sess, train_op, sv.global_step)\r\n                    summaries = sess.run(my_summary_op)\r\n                    if FLAGS.task_index == 0:\r\n                        print('Compute Summaries--------------')\r\n                        sv.summary_computed(sess, summaries)\r\n                        #writer.add_summary(summaries, gs)\r\n                        print \"**** SAVE THE MODEL ****\"\r\n                        print 'Train logs path -- ', train_logs_path\r\n                        sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\r\n                        #saver.save(sess, train_logs_path+'/model', global_step=gs)\r\n                        ##\r\n    \r\n                    checkpoint_path = tf.train.latest_checkpoint(train_logs_path)\r\n                    training_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\r\n                    current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\r\n                    print 'training_json -- ', training_json\r\n                    print 'sv.save_path', sv.save_path\r\n                    #dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\r\n    \r\n                    ##\r\n                    print('*************** Running eval at step = {0} *********************'.format(step))\r\n                    #cmd = 'python ./eval_image_classifier.py --pipeline_id='+FLAGS.pipeline_id+'  --eval_recording_step='+str(gs) + '  --tf_records_dir='+FLAGS.tf_records_dir + '  --checkpoint_dir='+train_logs_path + '  --eval_log_dir='+eval_logs_path + '  --dataset_name='+FLAGS.pipeline_id + '  --num_classes='+str(FLAGS.num_classes)  + '  --eval_num_epochs='+str(FLAGS.eval_num_epochs) + '  --eval_batch_size='+str(FLAGS.eval_batch_size) + '  --image_resize_method='+FLAGS.image_resize_method + '  --color_ordering='+str(FLAGS.color_ordering) + '  --saturation_contrast_lower_bound='+str(FLAGS.saturation_contrast_lower_bound) + '  --saturation_contrast_upper_bound='+str(FLAGS.saturation_contrast_upper_bound) + '  --brightness_max_delta='+str(FLAGS.brightness_max_delta) + '  --hue_max_delta='+str(FLAGS.hue_max_delta)\r\n                    #p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\r\n                    #out, err = p.communicate()\r\n                    print('***************Eval finished for step = {0}*********************'.format(step))\r\n    \r\n    \r\n                else:\r\n                    loss, _ = train_step(sess, train_op, sv.global_step)\r\n    \r\n            #We log the final training loss and accuracy\r\n            logging.info('Final Loss: %s', loss)\r\n            logging.info('Final Training Accuracy: %s', sess.run(accuracy))\r\n    \r\n            #Save the model on completing the training\r\n            logging.info('Finished training! Saving model to disk now.')\r\n            if FLAGS.task_index == 0:\r\n              sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\r\n\r\nProblem Description\r\n-------------------\r\n\r\nI am having three nodes Tensorflow cluster with one parameter server, two workers.\r\nOn machine A, I am running a parameter server and chief worker.\r\nOn machine B, I am running 2nd worker.\r\nMy tensorflow script expects one of the input parameter as the path to the directory containing tfrecord file.\r\n\r\nIssue 1\r\n-------\r\n\r\nParameter server and chief worker are running fine but 2nd worker fails with below error.\r\nI am running 2nd worker on machine B and input path to the tfrecords directory exists there.But I don't have any clue why it is looking for data under ps device=\"/job:ps/replica:0/task:0/cpu:0\" which is actually running on machine A where this path doesn't exist.\r\n\r\n    \r\n    INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, /home/mapr/mano/slim_data/flowers/slim_data_dir/flowers_train_00002-of-00005.tfrecord\r\n             [[Node: parallel_read/ReaderReadV2 = ReaderReadV2[_device=\"/job:ps/replica:0/task:0/cpu:0\"](parallel_read/TFRecordReaderV2, parallel_read/filenames)]]\r\n    \r\n\r\nIssue 2\r\n-------\r\n\r\nJust for my curiosity, I copied input tfrecord directory to machine A with the same path as on machine B.\r\nThen the previous error gone but now I am facing different error.\r\nI also checked no code has run within **sv.prepare_or_wait_for_session(server.target)**  block.\r\nIt seems to me 2nd worker waits for chief to initialize the session but somehow chief doesn't return session and thus worker fails.\r\n\r\n    INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, A session is not created yet....\r\n\r\nPlz help me here.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nExact command to reproduce", "Updated the requested information below,\r\n\r\nOS Platform & distribution - CentOS - 7.2.1511\r\nExact command to reproduce - \r\n\r\n1- Command to run ps job - python ./DTF_train_image_classifier.py \\\r\n--pipeline_id='TestPipeline_9' \\\r\n--logs_dir=./log_dtf \\\r\n--tf_records_dir=/home/mapr/mano/slim_data/flowers \\\r\n--dataset_name=flowers \\\r\n--num_classes=5 \\\r\n--training_num_epochs=1 \\\r\n--training_batch_size=32 \\\r\n--eval_num_epochs=1 \\\r\n--eval_batch_size=32 \\\r\n--checkpoint_path=hdfs://172.26.32.32:8020/user/mapr/deepak/inception_v3/inception_v3.ckpt \\\r\n--checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \\\r\n--steps_update_frequency=2 \\\r\n--job_name=\"ps\" \\\r\n--task_index=0\r\n\r\n2- Command to run worker 0 task- python ./DTF_train_image_classifier.py \\\r\n--pipeline_id='TestPipeline_9' \\\r\n--logs_dir=./log_dtf \\\r\n--tf_records_dir=/home/mapr/mano/slim_data/flowers \\\r\n--dataset_name=flowers \\\r\n--num_classes=5 \\\r\n--training_num_epochs=1 \\\r\n--training_batch_size=32 \\\r\n--eval_num_epochs=1 \\\r\n--eval_batch_size=32 \\\r\n--checkpoint_path=hdfs://ip1:8020/user/mapr/deepak/inception_v3/inception_v3.ckpt \\\r\n--checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \\\r\n--steps_update_frequency=2 \\\r\n--job_name=\"worker\" \\\r\n--task_index=0\r\n\r\n2- Command to run worker 1  task -  python ./DTF_train_image_classifier.py \\\r\n--pipeline_id='TestPipeline_9' \\\r\n--logs_dir=./log_dtf \\\r\n--tf_records_dir=/home/mapr/mano/slim_data/flowers/slim_data_dir \\\r\n--dataset_name=flowers \\\r\n--num_classes=5 \\\r\n--training_num_epochs=1 \\\r\n--training_batch_size=32 \\\r\n--eval_num_epochs=1 \\\r\n--eval_batch_size=5 \\\r\n--checkpoint_path=hdfs://ip2:8020/user/mapr/deepak/inception_v3/inception_v3.ckpt \\\r\n--checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \\\r\n--steps_update_frequency=2 \\\r\n--job_name=\"worker\" \\\r\n--task_index=1", "@cy89 - Plz help me resolving the issue.\r\nThanks.", "Dear,  I am still looking for an answer.\n\nThanks.\n\nOn Sat, 14 Apr 2018, 06:25 Alfred Sorten Wolf, <notifications@github.com>\nwrote:\n\n> Thank you for your post. We noticed you have not filled out the following\n> field in the issue template. Could you update them if they are relevant in\n> your case, or leave them as N/A? Thanks.\n> OS Platform and Distribution\n> Exact command to reproduce\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18492#issuecomment-381291754>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Aeth_9YK4E9j4ecWq4bbprwhxE17sNJNks5toUj4gaJpZM4TTY3L>\n> .\n>\n", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mrry would you PTAL? ", "Issue 1: It looks like you need a `with tf.device(\"/job:worker/task:%d\" % FLAGS.task_index):` around the code that creates the reader to prevent it from being placed on the PS task. Or you could just move it inside the existing `with tf.device(tf.train.replica_device_setter(...)):` block.\r\n\r\nIssue 2: The typical cause of an error like this is that the two workers don't agree on where the variables are placed. You might try printing the list of all variables and comparing their `device` property.", "(I'm going to close this issue as it doesn't seem like a bug in TensorFlow, but rather a usage question that could be better handled on Stack Overflow.)"]}, {"number": 18491, "title": "crf_log_likelihood become 2x slower after upgrade TensorFlow from 1.4 to 1.7", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.4.1708\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==1.4.0, pip install tensorflow-gpu==1.7.0\r\n- **TensorFlow version (use command below)**: 1.4 and 1.7\r\n- **Python version**:  2.7.5\r\n- **Bazel version (if compiling from source)**: N/A (not compiled from source)\r\n- **GCC/Compiler version (if compiling from source)**: N/A (not compiled from source)\r\n- **CUDA/cuDNN version**: CUDA8.0 + cuDNN6.0 for Tensorflow 1.4, CUDA9 + cuDNN7.0 for Tensorflow 1.7\r\n- **GPU model and memory**: GeForce GTX 1080 Ti, 11178MiB\r\n- **Exact command to reproduce**: python profile_crf.py\r\n\r\n### Describe the problem\r\nAfter upgrade TensorFlow from 1.4 to 1.7,  crf become 2x slower.\r\n\r\nWhen run in TF1.4, it cost about 19 seconds every 100 steps, but about 43 seconds every 100 steps in TF1.7, the source code are the same, see below.\r\n### Source code / logs\r\nthe source code are modified from \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\n# Data settings.\r\nnum_examples = 1000\r\nnum_words = 100\r\nnum_features = 1000\r\nnum_tags = 50\r\n\r\n# Random features.\r\nx = np.random.rand(num_examples, num_words, num_features).astype(np.float32)\r\n\r\n# Random tag indices representing the gold sequence.\r\ny = np.random.randint(num_tags, size=[num_examples, num_words]).astype(np.int32)\r\n\r\n# All sequences in this example have the same length, but they can be variable in a real model.\r\n# sequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)\r\nsequence_lengths = np.full(num_examples, num_words, dtype=np.int32)\r\n\r\n# Train and evaluate the model.\r\nwith tf.Graph().as_default():\r\n  with tf.Session() as session:\r\n    # Add the data to the TensorFlow graph.\r\n    x_t = tf.constant(x)\r\n    y_t = tf.constant(y)\r\n    sequence_lengths_t = tf.constant(sequence_lengths)\r\n\r\n    # Compute unary scores from a linear layer.\r\n    weights = tf.get_variable(\"weights\", [num_features, num_tags])\r\n    matricized_x_t = tf.reshape(x_t, [-1, num_features])\r\n    matricized_unary_scores = tf.matmul(matricized_x_t, weights)\r\n    unary_scores = tf.reshape(matricized_unary_scores,\r\n                              [num_examples, num_words, num_tags])\r\n\r\n    # Compute the log-likelihood of the gold sequences and keep the transition\r\n    # params for inference at test time.\r\n    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\r\n        unary_scores, y_t, sequence_lengths_t)\r\n\r\n    # Compute the viterbi sequence and score.\r\n    viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(\r\n        unary_scores, transition_params, sequence_lengths_t)\r\n\r\n    # Add a training op to tune the parameters.\r\n    loss = tf.reduce_mean(-log_likelihood)\r\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\r\n\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    mask = (np.expand_dims(np.arange(num_words), axis=0) <\r\n            np.expand_dims(sequence_lengths, axis=1))\r\n    total_labels = np.sum(sequence_lengths)\r\n\r\n    # Train for a fixed number of iterations.\r\n    seconds = 0\r\n    for i in range(500):\r\n      start_time = time.time()\r\n      tf_viterbi_sequence, _ = session.run([viterbi_sequence, train_op])\r\n      seconds += time.time() - start_time\r\n      if i>0 and i % 100 == 0:\r\n        print('time elapsed: {}'.format(seconds))\r\n        seconds = 0\r\n        correct_labels = np.sum((y == tf_viterbi_sequence) * mask)\r\n        accuracy = 100.0 * correct_labels / float(total_labels)\r\n        print(\"Accuracy: %.2f%%\" % accuracy)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Any progress on this?", "Any progress on this\uff1f", "This issue was started almost two months ago, i have no idea what's wrong with this still...\r\nAny progress on this?", "Hi\uff0cI also found the crf_log_likelihood from tf_addons is also slower than tf 1.4. any progress on this? appreciate your help!", "Hi @SimpleJian !\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Could you try again after changing your code base to [2.7](https://www.tensorflow.org/addons/api_docs/python/tfa) with addons? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18490, "title": "macOS: Debug fails when running though PyCharm", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.4\r\n- **TensorFlow installed from (source or binary)**: binary, pip\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.6.4 Anaconda, Inc.\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: Intel Iris Pro 1536 MB\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nTrying to debug a tf.Estimator script, using PyCharm as IDE. For this I use the tfdbg.LocalCLIDebugHook. When running the script through terminal debugging works fine, but through PyCharm i get a curses error:\r\n\r\n`_curses.error: setupterm: could not find terminal`\r\n\r\nI know this is more a PyCharm issue than a tf issue, but if there is anything simple you could do to mitigate this, it would be nice.\r\n\r\n### Source code / logs\r\nFile \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 903, in _train_model\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 546, in run\r\n    run_metadata=run_metadata)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1022, in run\r\n    run_metadata=run_metadata)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1113, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1098, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1162, in run\r\n    feed_dict, options)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1188, in _call_hook_before_run\r\n    request = hook.before_run(run_context)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/hooks.py\", line 106, in before_run\r\n    on_run_start_request)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 255, in on_run_start\r\n    self._prep_cli_for_run_start()\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 277, in _prep_cli_for_run_start\r\n    self._run_cli = ui_factory.get_ui(self._ui_type)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/debug/cli/ui_factory.py\", line 61, in get_ui\r\n    return curses_ui.CursesUI(on_ui_exit=on_ui_exit, config=config)\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 287, in __init__\r\n    self._screen_init()\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 400, in _screen_init\r\n    self._stdscr = curses.initscr()\r\n  File \"/Users/harald/anaconda3/envs/thesis/lib/python3.6/curses/__init__.py\", line 30, in initscr\r\n    fd=_sys.__stdout__.fileno())\r\n_curses.error: setupterm: could not find terminal\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes, it is.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "tfdbg requires curses it looks like. you may need to use the command line to do debugging. Assigning to the author of tfdbg, for additional info. @caisq could you provide more support.", "@hhusum @aselle Yes, this error occurs because tfdbg uses `curses` to manage the terminal UI by default. PyCharm is an environment that does not have a tty for curses to use. To work around this, set the `ui_type` kwarg to `readline` when using `LocalCLIDebugWrapperSession` or `LocalCLIDebugHook`."]}, {"number": 18489, "title": "Can device run tensorflow lite be used as a work task when performing distribute training?", "body": "Tensorflow lite is designed for mobile device, so can the lite version be used to do some training work. Like using the standard version tensorflow as parameter server, and the edge mobile device do some edge computing and work with the ps?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "the tensorflow version and platform are neither associate with this issue, it's just a common question, if there is a way or any plan to support it, please kindly share the info. Thanks.", "Training using TF Lite is not currently supported. There are no short-term plans to support it."]}, {"number": 18488, "title": "Compile error on couldn't find gunit.h", "body": "What's a `gunit.h` which is included in `tensorflow/stream_executor/cuda/cudnn_version_test.cc`? Since I can't find a `gunit.h` in tensorflow folder nor in the system\r\n\r\nSystem OS: Windows 10.0.16299.334", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code : N/A\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: VS compilation\r\nTensorFlow version: TAG e489b60\r\nBazel version: N/A\r\nCUDA/cuDNN version : v9.0\r\nGPU model and memory: TITAN X\r\nExact command to reproduce: Counldn't find `gunit.h`", "Nagging Assignee @rohan100jain: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18487, "title": "Fix functions in CRF when sequence_lengths contains zero", "body": "This fixes computation of `crf_log_norm` and `crf_sequence_score` when `sequence_lengths` contains one or more zero. \r\n\r\nThis PR addresses the similar problem to that  in https://github.com/tensorflow/tensorflow/pull/17755 ", "comments": ["The failing check seems to be irrelevant to this PR.", "Rebased this PR to master branch again."]}, {"number": 18486, "title": "Fix the error looking for libhdfs.so, Mac OS using libhdfs.dylib", "body": "When running the tensorflow on Mac OS to get the files from HDFS, with the following error:\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(libhdfs.so, 6): image not found\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "It looks like the CLA check hasn't passed. Can you double check that your e-mail on the commit matches what you used for the CLA? See the instructions in the above message.", "CLAs look good, thanks!\n\n<!-- ok -->", "@jhseu the CLA check has passed, could you take time to review? Thanks.", "Error is unrelated. Merging."]}, {"number": 18485, "title": "Can not download tensorflow C libray of macOS GPU version.", "body": "### Describe the problem\r\nCan not download tensorflow c libray of macOS GPU version.\r\n\r\n### Source code / logs\r\nI can't download that form the link :\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-darwin-x86_64-1.7.0.tar.gz\r\n\r\nAccording to https://www.tensorflow.org/install/install_c\r\n`TF_TYPE=\"cpu\" # Change to \"gpu\" for GPU support`\r\n `OS=\"linux\" # Change to \"darwin\" for macOS`\r\n` curl -L \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-${TF_TYPE}-${OS}-x86_64-1.7.0.tar.gz\" |`\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We should make this clearer on the \"Install C\" page, but since version 1.2 TensorFlow has not been providing GPU support on macOS.\r\n\r\nSee [release notes for 1.1.0](https://github.com/tensorflow/tensorflow/releases/tag/v1.1.0) and some [discussion on reddit](https://news.ycombinator.com/item?id=14577538).\r\n\r\nHope that helps.\r\n"]}, {"number": 18484, "title": "cannot add op with name conv2d/convolution as that name is already used", "body": "Here is my code : \r\n```python\r\nclass final_model:\r\n    # CNN encoder\r\n    encoder, preprocess_for_model = get_cnn_encoder()\r\n    #saver.restore(s, os.path.abspath(\"weights\"))  # keras applications corrupt our graph, so we restore trained weights\r\n    \r\n    # containers for current lstm state\r\n    lstm_c = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"cell\")\r\n    lstm_h = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"hidden\")\r\n\r\n    # input images\r\n    input_images = tf.placeholder('float32', [1, IMG_SIZE, IMG_SIZE, 3], name='images')\r\n\r\n    # get image embeddings\r\n    img_embeds = encoder(input_images)\r\n\r\n    # initialize lstm state conditioned on image\r\n    init_c = init_h = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\r\n    init_lstm = tf.assign(lstm_c, init_c), tf.assign(lstm_h, init_h)\r\n    \r\n    # current word index\r\n    current_word = tf.placeholder('int32', [1], name='current_input')\r\n\r\n    # embedding for current word\r\n    word_embed = decoder.word_embed(current_word)\r\n\r\n    # apply lstm cell, get new lstm states\r\n    new_c, new_h = decoder.lstm(word_embed, tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h))[1]\r\n\r\n    # compute logits for next token\r\n    new_logits = decoder.token_logits(decoder.token_logits_bottleneck(new_h))\r\n    # compute probabilities for next token\r\n    new_probs = tf.nn.softmax(new_logits)\r\n\r\n    # `one_step` outputs probabilities of next token and updates lstm hidden state\r\n    one_step = new_probs, tf.assign(lstm_c, new_c), tf.assign(lstm_h, new_h)\r\n```\r\n\r\nAnd I am getting following error : \r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-42-73f88badc13d> in <module>()\r\n----> 1 class final_model:\r\n      2     # CNN encoder\r\n      3     encoder, preprocess_for_model = get_cnn_encoder()\r\n      4     #saver.restore(s, os.path.abspath(\"weights\"))  # keras applications corrupt our graph, so we restore trained weights\r\n      5 \r\n\r\n<ipython-input-42-73f88badc13d> in final_model()\r\n     12 \r\n     13     # get image embeddings\r\n---> 14     img_embeds = encoder(input_images)\r\n     15 \r\n     16     # initialize lstm state conditioned on image\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\topology.py in __call__(self, inputs, **kwargs)\r\n    250     \"\"\"\r\n    251     # Actually call the layer (optionally building it).\r\n--> 252     output = super(Layer, self).__call__(inputs, **kwargs)\r\n    253 \r\n    254     # Update learning phase info.\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in call(self, inputs, mask)\r\n   1918     else:\r\n   1919       # Cache miss: actually apply the network graph to the new inputs.\r\n-> 1920       output_tensors, _, _ = self._run_internal_graph(inputs, masks)\r\n   1921       return output_tensors\r\n   1922 \r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in _run_internal_graph(self, inputs, masks)\r\n   2082                 if 'mask' not in kwargs:\r\n   2083                   kwargs['mask'] = computed_mask\r\n-> 2084               output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\r\n   2085               if hasattr(layer, 'compute_mask'):\r\n   2086                 output_masks = _to_list(\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py in call(self, inputs)\r\n    169     # TODO(agarwal): do we need this name_scope ?\r\n    170     with ops.name_scope(None, 'convolution', [inputs, self.kernel]):\r\n--> 171       outputs = self._convolution_op(inputs, self.kernel)\r\n    172 \r\n    173     if self.use_bias:\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in __call__(self, inp, filter)\r\n    833 \r\n    834   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n--> 835     return self.conv_op(inp, filter)\r\n    836 \r\n    837 \r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in __call__(self, inp, filter)\r\n    497 \r\n    498   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n--> 499     return self.call(inp, filter)\r\n    500 \r\n    501 \r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in __call__(self, inp, filter)\r\n    185         padding=self.padding,\r\n    186         data_format=self.data_format,\r\n--> 187         name=self.name)\r\n    188 \r\n    189 \r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\r\n    628         \"Conv2D\", input=input, filter=filter, strides=strides,\r\n    629         padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu,\r\n--> 630         data_format=data_format, name=name)\r\n    631     _result = _op.outputs[:]\r\n    632     _inputs_flat = _op.inputs\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789 \r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   2957     if compute_shapes:\r\n   2958       set_shapes_for_outputs(ret)\r\n-> 2959     self._add_op(ret)\r\n   2960     self._record_op_seen_by_control_dependencies(ret)\r\n   2961 \r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in _add_op(self, op)\r\n   2597       if op.name in self._nodes_by_name:\r\n   2598         raise ValueError(\"cannot add op with name %s as that name \"\r\n-> 2599                          \"is already used\" % op.name)\r\n   2600       self._nodes_by_id[op._id] = op\r\n   2601       self._nodes_by_name[op.name] = op\r\n\r\nValueError: cannot add op with name conv2d/convolution as that name is already used\r\n```\r\n\r\nHow can solve this issue ? Please help . THANKS.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi I had same problem with Jupyter-Notebook, and it was solved by restarting kernel", "@pranto00250 have you tried @salmanshkt 's suggestion?", "It has been 34 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18483, "title": "Fix the broken TFLite iOS example.", "body": "The demo app is only relying on CocoaPod now, but it's incorrectly\r\nconfigured to use the headers on Github. It crashes the app when\r\nthe header is different between Github and CocoaPod.", "comments": []}, {"number": 18482, "title": "Not able to import tensorflow", "body": "Ubuntu: 16.04.4\r\nTensorflow installed from Anaconda\r\nTensorflow version: 1.4.1\r\nPython: Python3\r\nCUDA: 8\r\nCudnn: 7.0.5\r\nGPU: GTX 1080 8GB\r\nCommand: import tensorflow on ipython notebook\r\nError:\r\n```\r\nImportError                               Traceback (most recent call last)\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     70 for some common reasons and solutions.  Include the entire stack trace\r\n     71 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 72   raise ImportError(msg)\r\n     73 \r\n     74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/sarvagya/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/sarvagya/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/sarvagya/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/sarvagya/anaconda3/envs/tf/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/sarvagya/anaconda3/envs/tf/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\n**UPDATE:**\r\nI installed normal tensorflow and I was able to import tensorflow. However, now, I am not able use the GPU, even though tensoeflorw-gpu is installed in the environment. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi,\r\nOS: Ubuntu 16.04.4\r\nCUDA: 8.03, Cudnn: 7.0.5\r\nGPU: 1080 8GB\r\nNo Bazel.\r\nCommand: No command. I am importing it in jupyter notebook and a conda env. So for import tensorflow, I get the error. ", "@tensorflowbutler, an Update:\r\nI created a new profile on Ubuntu, no NVIDIA libraries like CUDA, cudnn and other installed, installed anaconda and an env with `python 3`. I installed tf gpu using conda (` conda install -c anaconda tensorflow-gpu`)  and it installed `1.4` `cudatoolkit 8` and `cudnn 7`. Now, it's working. \r\n\r\nI even installed cuda 8 library after that. But I don't know why on the previous account it's not working. Can you help me? Is something going in some file I'm not aware of?", "**UPDATE**:\r\nI solved it by adding `/path/to/libcuda.so.1` in the `LD_LIBRARY_PATH`. I rebooted and it worked. ", "Closing since you solved the problem!", " File \"C:\\Users\\DMLaptopWindows1\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\DMLaptopWindows1\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\DMLaptopWindows1\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\DMLaptopWindows1\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message"]}, {"number": 18481, "title": "Fix crash when invalid dtype was passed", "body": "This fix tries to address the issue raised in #18474 where crash may happen if invalid dtype (e.g., `\"[,]\"`) is passed to `tf.constant(tf.string, \"[,]\")`. The crash happens during the comparision of `\"[,]\"` and numpy dtype candidate (e.g., `np.dtype([(\"qint8\", np.int8, 1)])`:\r\n```\r\n>>> import numpy as np\r\n>>> np.dtype([(\"qint8\", np.int8, 1)]) == \"[,]\"\r\nSegmentation fault: 11\r\n```\r\n\r\nThis fix adds a type check to make sure the type of the passed dtype is either numpy.dtype or type.\r\n\r\nThis fix fixes #18474.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["lgtm"]}, {"number": 18479, "title": "Configurable Custom graph optimizers for grappler", "body": "This PR is to start a discussion about passing configuration information to user implemented custom grappler optimization passes for @benoitsteiner, @zheng-xq  and @aaroey to take a look at. Please include other interested parties in the discussion.", "comments": ["@samikama: can you update the Init() method in meta_optimizer_test.cc and custom_graph_optimizer_registry_test.cc ? They now require an argument, and as the result the tests fail to build.", "@benoitsteiner @aaroey Sorry about the tests. I pushed a fixes.", "@samikama sorry I needed to revert the change in meta_optimizer.cc due to conflicts when I was merging internal changes. Please see https://github.com/tensorflow/tensorflow/pull/18714/commits/38dcc57681612c2321169367c8756bb218472dd7 for the revert and https://github.com/tensorflow/tensorflow/pull/18714/commits/4f8768319cfa56c25973cc66d920146ad454bd97 for the internal change that caused conflict.\r\nYou might want to create another PR after resolving the conflicts. Thank you and sorry for the inconvenience! ", "@yifeif Thanks for the notice, I'll create a new PR to fix that."]}, {"number": 18478, "title": "Tensorflow server doesn't pick version updates if pointed to shared model_config.conf file", "body": "I'm currently deploying tensorflow models on multiple servers in production but seem to have a issue while there is a model config file update.\r\n\r\n**Issue is -** \r\nOn updating the model config file located at a common shared location that all then tensorflow servers point to, not all the servers pick up the version updates!\r\n \r\nOn the contrary, the issue seems to be not occuring when each of the tensorflow server has its own model_config file, then it simultaneously updates with new versions.\r\n\r\nThis is a unwanted behavior, wanted to know if there is a work around or is it a bug which is existent in tensorflow serving.\r\n\r\nmodel_config looks something similar to this -\r\n\r\n```\r\nmodel_config_list: {\r\n  config: {\r\n    name: \"modelName1\",\r\n    base_path: \"/path to model1 files/\",\r\n    model_platform: \"tensorflow\"\r\n  },\r\n    config: {\r\n    name: \"modelName2\",\r\n    base_path: \"/path to model2 files/\",\r\n    model_platform: \"tensorflow\"\r\n  }\r\n}\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18477, "title": "tf.gather's gradient fails on GPU in eager mode", "body": "Tested with TensorFlow v1.7.0 on Ubuntu 16.06.\r\n\r\nConsider the following contrived example:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n    with tfe.GradientTape() as tape:\r\n        params = tfe.Variable(tf.zeros((1024,)))\r\n        indices = tf.constant(range(5))\r\n        output = tf.reduce_sum(tf.gather(params, indices))\r\n    print(tape.gradient(output, [params]))\r\n```\r\n\r\nWhen executed, it fails with:\r\n\r\n```\r\nTensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(), or transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: ToInt32\r\n```\r\n\r\nThis does not occur on the CPU.\r\n\r\nThe issue can be fixed by removing the `to_int32` call and just changing the `out_types` to `int32` here in [array_grad.py](https://github.com/tensorflow/tensorflow/blob/v1.7.0/tensorflow/python/ops/array_grad.py#L398-L399) as follows:\r\n```python\r\nparams_shape = array_ops.shape(params, out_type=ops.dtypes.int32) # int64 -> int32\r\n# params_shape = math_ops.to_int32(params_shape) [remove this line]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The root cause of this is that as of TensorFlow 1.7, by default we do not silently copy tensors between devices and since most operations on int32 tensors require the tensor to be placed on CPU, you get the error.\r\n\r\nAs the error message suggests, in TensorFlow 1.7 you can work around this by using:\r\n\r\n```python\r\ntf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_SILENT)\r\n```\r\n\r\nWith TensorFlow 1.8 we've done a bit more to make behavior in eager and graph more aligned - so you won't need the `with tf.device('/gpu:0')` block since GPUs will be used where available and tensors silently copied if needed by default. \r\n\r\nThus, for 1.7, try a different `device_policy` when enabling eager execution and with 1.8 the issue you described will go away.\r\n\r\nHope that helps.\r\n\r\nSome additional details for the record:\r\n- This silent copying could become a bottleneck in model performance, so you can revert to `tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT` to help identify the points at which copying happens.\r\n- The behavior between graph and eager when it comes to placement of operations won't always be _exactly_ the same as with graph execution the runtime has the benefit of seeing the complete computation to make more informed placement decisions, while with eager execution placement decisions have to be made greedily.\r\n\r\nClosing this out since the behavior is intentional, there is a workaround for 1.7, and this problem won't occur in 1.8. Please re-open if I've misunderstood. Thanks.\r\n", "It seems undesirable that this gradient operation will incur a cross device copy overhead when it seems avoidable (at least on non-Windows targets, based on my understanding of the comment above the cast to int32)", "Yes, it certainly is. \r\n\r\nThe Windows specific-ness of that is questionable See https://github.com/tensorflow/tensorflow/pull/9780#issuecomment-323547672.\r\n\r\nThough in practice this hasn't been a concern that has shown up much over the past year and hence resolving that hasn't been bumped up the priority list. If you do have some reason to believe that the int32 casting and the resulting copies is indeed causing trouble, please do file a bug with the details.\r\n\r\n(Keeping this particular one closed since this concern about int32 casting and copies isn't specific to the original description of the issue)\r\n\r\nHope that makes sense. Thanks."]}, {"number": 18476, "title": "Branch 192694244", "body": "", "comments": []}, {"number": 18475, "title": "Fix typos", "body": "This PR fixes some typos: `wihtin`, `seperately`, `paramaters`, `Additonal`, `transformaitons`, `varibale`, and `folow`.", "comments": []}, {"number": 18474, "title": "Segfault on bad input to tf.constant.", "body": "One can get a segfault providing some wrong data into a `tf.constant`:\r\n\r\n```python\r\n>> tf.constant(tf.string, \"[,]\")\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nWhile `tf.constant(tf.string, \"[]\")` and `tf.constant(tf.string, \",\")` both give the proper error (`dtype not understood`).\r\n\r\n", "comments": ["The issue seems to be caused in:\r\nhttps://github.com/tensorflow/tensorflow/blob/049dfd5e070cfa84c82eea71c6c746a70cba4a3f/tensorflow/python/framework/dtypes.py#L701\r\n\r\nwith crash happens in numpy. For example, the following will trigger a crash:\r\n```\r\n>>> import numpy as np\r\n>>> np.dtype([(\"qint8\", np.int8, 1)]) == \"[,]\"\r\nSegmentation fault: 11\r\n```\r\n\r\nAdded a PR #18481 to check the type of the `type_value` so that things like `\"[,]\"` is not compared.", "thanks"]}, {"number": 18473, "title": "Error when trying to use tf.contrib.distribute.MirroredStrategy in tf.estimator", "body": "**System information**\r\n- **OS Platform and Distribution**:  Linux Ubuntu 16.04.2\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.7\r\n- **Python version**: 3.5\r\n- **Bazel version**: 0.11.1\r\n- **GCC/Compiler version**: 5.4.0\r\n- **CUDA/cuDNN version**: 9/7\r\n- **GPU model and memory**: TITAN X (Pascal) 11170 MB memory\r\n\r\nI'm trying to add multi-gpu support to my tensorflow training code using tf.contrib.distribute.MirroredStrategy as a parameter to tf.estimator.RunConfig. \r\nI get the following error message: \r\n\r\n<!-- language: python -->\r\n\r\n    Traceback (most recent call last):\r\n      File \"python3.5/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n        yield\r\n      File \"python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\r\n        self, *merge_args, **merge_kwargs)\r\n      File \"python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 667, in _distributed_apply\r\n        reduced_grads = distribution.batch_reduce(\"sum\", grads_and_vars)\r\n      File \"python3.5/site-packages/tensorflow/python/training/distribute.py\", line 801, in batch_reduce\r\n        return self._batch_reduce(method_string, value_destination_pairs)\r\n      File \"python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 295, in _batch_reduce\r\n        value_destination_pairs)\r\n      File \"python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 169, in batch_reduce\r\n        raise ValueError(\"`value_destination_pairs` must be a list or a tuple of \"\r\n    ValueError: `value_destination_pairs` must be a list or a tuple of tuples of PerDevice objects and destinations \r\n\r\nThe following code produces the error (I omitted the code for parsing the tfrecord to image tensor as I don't believe this code effects the error, but I can add it if necessary):\r\n\r\n<!-- language: python -->\r\n    \r\n    import glob, os\r\n    import tensorflow as tf\r\n    slim = tf.contrib.slim\r\n    \r\n    # Initialization of args - arguments parser.\r\n    def init():\r\n        pass\r\n \r\n\r\n    def input_fn():\r\n    \r\n        dataset = tf.data.TFRecordDataset(glob.glob(os.path.join(args.train_data_dir, 'train*')))\r\n        dataset = dataset.map(\r\n                    lambda x: parse_and_preprocess_image(x, args.image_size),\r\n                    num_parallel_calls=2,\r\n        )\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size=4)\r\n        dataset = dataset.prefetch(1)\r\n    \r\n        return dataset\r\n    \r\n    \r\n    def model_fn(features, labels=None, mode=tf.estimator.ModeKeys.TRAIN, params=None):\r\n    \r\n        train_images_batch = features\r\n        res = slim.conv2d(inputs=train_images_batch, kernel_size=9, stride=1, num_outputs=3, scope='conv1')\r\n        loss = tf.reduce_mean((train_images_batch - res) ** 2)\r\n        optimizer = tf.train.AdamOptimizer(0.001)\r\n        train_op = slim.learning.create_train_op(loss, optimizer)\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=tf.estimator.ModeKeys.TRAIN,\r\n            loss=loss, train_op=train_op)\r\n    \r\n    \r\n    def train():\r\n    \r\n        init()\r\n    \r\n        distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=args.num_gpus)\r\n    \r\n        config = tf.estimator.RunConfig(\r\n            model_dir=args.log_dir,\r\n            train_distribute=distribution,\r\n        )\r\n    \r\n        estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n        estimator.train(\r\n                input_fn=input_fn,\r\n                max_steps=args.train_steps,\r\n            )\r\n    \r\n    \r\n    def main():\r\n        train()\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        main()\r\n\r\n\r\nThank you!\r\nAdva\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "What value did you pass for `args.num_gpus`? I got this error when passing `1`. Then I understood that 'Mirrored' naturally means that there are at least 2 devices involved.\r\n\r\nWhen you want to use only one device with DistributionStrategy, I think you have to use `OneDeviceStrategy` instead of `MirroredStrategy`.", "Yes, I passed `1` (I was hoping to debug it on a single gpu machine before test it on a multi-gpu one). Changing to `OneDeviceStrategy ` did solve the error in the single machine. I hope to test it on a multi-gpu machine soon. Thanks!", "Hi I tested on a multi-gpu machine and got a different error.\r\n\r\n<!-- language: python -->\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n        return fn(*args)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n        options, feed_dict, fetch_list, target_list, run_metadata)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n        run_metadata)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: Creating a partition for /device:CPU:7 which doesn't exist in the list of available devices. Available devices: /device:CPU:0,/device:GPU:0,/device:GPU:1,/device:GPU:2,/device:GPU:3,/device:GPU:4,/device:GPU:5,/device:GPU:6,/device:GPU:7\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"conditional_instance_norm/cin/bug.py\", line 80, in <module>\r\n        main()\r\n      File \"conditional_instance_norm/cin/bug.py\", line 76, in main\r\n        train()\r\n      File \"conditional_instance_norm/cin/bug.py\", line 68, in train\r\n        max_steps=args.train_steps,\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n        loss = self._train_model(input_fn, hooks, saving_listeners)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n        return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 977, in _train_model_distributed\r\n        saving_listeners)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1059, in _train_with_estimator_spec\r\n        _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 567, in run\r\n        run_metadata=run_metadata)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1043, in run\r\n        run_metadata=run_metadata)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1134, in run\r\n        raise six.reraise(*original_exc_info)\r\n      File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n        raise value\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1119, in run\r\n        return self._sess.run(*args, **kwargs)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1191, in run\r\n        run_metadata=run_metadata)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 971, in run\r\n        return self._sess.run(*args, **kwargs)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 900, in run\r\n        run_metadata_ptr)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n        feed_dict_tensor, options, run_metadata)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n        run_metadata)\r\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: Creating a partition for /device:CPU:7 which doesn't exist in the list of available devices. Available devices: /device:CPU:0,/device:GPU:0,/device:GPU:1,/device:GPU:2,/device:GPU:3,/device:GPU:4,/device:GPU:5,/device:GPU:6,/device:GPU:7", "me too ,   \r\nwhy ?", "@isaprykin WDYT?", "Nagging Assignee @isaprykin: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi!  Do you have any code that uses `tf.device`?\r\nIf not, then would it be practical for you to include your model code, please?\r\nIf the code above is your model, then my suspicion is using slim together with DistributionStrategy.  I wouldn't think it was intended that way.\r\n\r\nSomething went very wrong to result in \"Creating a partition for /device:CPU:7 which doesn't exist in the list of available devices. \".  ", "Hi Igor @isaprykin ,\r\n\r\nSeems like I have hit the same problem on stock tensorflow-gpu==1.8.0rc0 on a 8-GPU machine.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Creating a partition for /device:CPU:3 which doesn't exist in the list of available devices. Available devices: /device:CPU:0,/device:GPU:0,/device:GPU:1,/device:GPU:2,/device:GPU:3,/device:GPU:4,/device:GPU:5,/device:GPU:6,/device:GPU:7\r\n```\r\n\r\nI am using very straightforward multi-GPU setup. The relevant code fragments:\r\n\r\n```python\r\ndef get_available_gpus():\r\n    local_device_protos = device_lib.list_local_devices()\r\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\r\n\r\ndef build_subgraphs(net, filtered_uttids, subgraph_batch_size, chunk_size, examples, args, training, dependencies):\r\n    iterator_initializers = []\r\n    discriminative_objf_losses = []\r\n    kl_losses = []\r\n    top1_accuracies = []\r\n    valid_examples = []\r\n\r\n    gpu_devices = get_available_gpus()\r\n    for gpu_device_id, device in enumerate(gpu_devices):\r\n        READ_SIZE = subgraph_batch_size * PREFETCH_MULITPLIER\r\n        dataset = (tf.data.Dataset.from_tensor_slices(filtered_uttids)\r\n                   .shard(\r\n                       num_shards=len(gpu_devices),\r\n                       index=gpu_device_id)\r\n                   .repeat(args.num_epochs if training else 1)\r\n                   .batch(subgraph_batch_size)\r\n                   .map(map_func=lambda uttids_batch:\r\n                        vibetflib.read_examples(\r\n                            examples=examples,\r\n                            feature_dim=args.feature_dim,\r\n                            utterance_ids=uttids_batch,\r\n                            sequence_length=chunk_size,\r\n                            name='ceg_reader_chunk_{}_gpu_{}'.format(chunk_size, gpu_device_id)),\r\n                        num_parallel_calls=PREFETCH_MULITPLIER))\r\n\tif training:\r\n            dataset = (dataset\r\n                       .shuffle(\r\n                           buffer_size=READ_SIZE,\r\n                           reshuffle_each_iteration=True))\r\n\telse:\r\n            dataset = (dataset\r\n                       .cache())  # in memory                                                                                                                                                                      \r\n\r\n\titerator = dataset.make_one_shot_iterator() if training else dataset.make_initializable_iterator()\r\n        if not training:\r\n            iterator_initializers.append(iterator.initializer)\r\n        mats, targets, expected_output_seqlen = iterator.get_next()\r\n        mats.set_shape([None, chunk_size, args.feature_dim])\r\n        with tf.device(device):\r\n            discriminative_outputs, xent_outputs = net(mats, expected_output_seqlen, args, training=training)\r\n                discriminative_objf, _, posteriors = vibetflib.compute_loss(\r\n                    targets=targets,\r\n                    discriminative_outputs=discriminative_outputs,\r\n                    num_nnet_outputs=args.num_nnet_outputs)\r\n            ...\r\n```\r\n\r\nSo I basically just iterate over GPUs and create subgraphs. `net` is a subgraph template.\r\n\r\nThis problem didn't arise on 1-GPU and 2-GPU machines.\r\n\r\nMy case can be complicated by the fact that `vibetflib` is an in-house tensorflow extension which also uses GPU. However, it doesn't seem to be the case for @advaza -- so the problem could indeed lie within tensorflow itself.\r\n\r\nThanks!", "Update from my side: building tensorflow from source (`c8137f3a8e1a22b6e274d0ffc84013624523df59`, python 3.5, cuda 9.1, other settings at default) seemed to remove the problem.", "Hi Igor @isaprykin !\r\n\r\nI just changed the code and removed slim but still the same error occurs. \r\nI will update the configuration settings, code, and traceback.\r\n\r\n**System information**\r\n\r\n**OS Platform and Distribution:** Linux Ubuntu 16.04.3 LTS\r\n**TensorFlow installed from:** pip\r\n**TensorFlow version:** tensorflow-gpu==1.8.0rc0\r\n**Python version:** 3.5\r\n**Bazel version:** N/A\r\n**GCC/Compiler version:** N/A\r\n**CUDA/cuDNN version:** 9/7\r\n**GPU model and memory:** 8 GPUs of Tesla K80, memory 488, GPU memory 96\r\n**CPUs:** 32\r\n\r\nCode:\r\n```python\r\nimport glob, os\r\nimport tensorflow as tf\r\n\r\n\r\n# Initialization of args - arguments parser.\r\ndef init():\r\n    pass\r\n\r\n\r\ndef distribution_strategy(num_gpus):\r\n\r\n    if num_gpus == 1:\r\n        return tf.contrib.distribute.OneDeviceStrategy(device='/gpu:0')\r\n    elif num_gpus > 1:\r\n        return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\r\n    else:\r\n        return None\r\n\r\n\r\ndef input_fn():\r\n\r\n    with tf.device('/cpu:0'):\r\n\r\n        dataset = tf.data.TFRecordDataset(\r\n            glob.glob(os.path.join(args.train_data_dir, 'train*'))\r\n        )\r\n        dataset = dataset.map(\r\n                    lambda x: parse_and_preprocess_image(x, args.image_size),\r\n                    num_parallel_calls=2,\r\n        )\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size=4)\r\n        dataset = dataset.prefetch(1)\r\n\r\n        return dataset\r\n\r\n\r\ndef model_fn(features):\r\n\r\n    train_images_batch = features\r\n    res = tf.layers.conv2d(inputs=train_images_batch, filters=3, kernel_size=[9,9], \r\n                           name='conv1', padding='same')\r\n    loss = tf.reduce_mean((train_images_batch - res) ** 2)\r\n    optimizer = tf.train.AdamOptimizer(0.001)\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=tf.estimator.ModeKeys.TRAIN,\r\n        loss=loss, train_op=train_op)\r\n\r\n\r\ndef train():\r\n\r\n    config = tf.estimator.RunConfig(\r\n        model_dir=args.log_dir,\r\n        train_distribute=distribution_strategy(args.num_gpus),\r\n    )\r\n\r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n\r\n    if args.print_log:\r\n        tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n    estimator.train(\r\n            input_fn=input_fn,\r\n            max_steps=args.train_steps,\r\n        )\r\n\r\n\r\ndef main():\r\n\r\n    init()\r\n    train()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()    \r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Creating a partition for /device:CPU:5 which doesn't exist in the list of available devices. Available devices: /device:CPU:0,/device:GPU:0,/device:GPU:1,/device:GPU:2,/device:GPU:3,/device:GPU:4,/device:GPU:5,/device:GPU:6,/device:GPU:7\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"conditional_instance_norm/cin/bug.py\", line 90, in <module>\r\n    main()\r\n  File \"conditional_instance_norm/cin/bug.py\", line 86, in main\r\n    train()\r\n  File \"conditional_instance_norm/cin/bug.py\", line 78, in train\r\n    max_steps=args.train_steps,\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 977, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1059, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 567, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1043, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1134, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1119, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1191, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 971, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Creating a partition for /device:CPU:5 which doesn't exist in the list of available devices. Available devices: /device:CPU:0,/device:GPU:0,/device:GPU:1,/device:GPU:2,/device:GPU:3,/device:GPU:4,/device:GPU:5,/device:GPU:6,/device:GPU:7\r\n```\r\n\r\nLogging:\r\n```\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-05-24 15:29:03.599841: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-05-24 15:29:05.183779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:05.185180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:17.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:05.312107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:05.313326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:18.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:05.430032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:05.431248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:19.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:05.550047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:05.551240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 3 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1a.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:05.673337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:05.674512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 4 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1b.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:05.799709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:05.800834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 5 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1c.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:05.930817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:05.931936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 6 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1d.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:06.063970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-24 15:29:06.065079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 7 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-24 15:29:06.073436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2018-05-24 15:29:08.181500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-24 15:29:08.181554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 4 5 6 7\r\n2018-05-24 15:29:08.181568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y Y Y Y Y\r\n2018-05-24 15:29:08.181580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y Y Y Y Y\r\n2018-05-24 15:29:08.181590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y Y Y Y Y\r\n2018-05-24 15:29:08.181601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N Y Y Y Y\r\n2018-05-24 15:29:08.181613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 4:   Y Y Y Y N Y Y Y\r\n2018-05-24 15:29:08.181619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 5:   Y Y Y Y Y N Y Y\r\n2018-05-24 15:29:08.181630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 6:   Y Y Y Y Y Y N Y\r\n2018-05-24 15:29:08.181636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 7:   Y Y Y Y Y Y Y N\r\n2018-05-24 15:29:08.183932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 10761 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:17.0, compute capability: 3.7)\r\n2018-05-24 15:29:08.376594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 10761 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:18.0, compute capability: 3.7)\r\n2018-05-24 15:29:08.569150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 10761 MB memory) -> physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:00:19.0, compute capability: 3.7)\r\n2018-05-24 15:29:08.761061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 10761 MB memory) -> physical GPU (device: 3, name: Tesla K80, pci bus id: 0000:00:1a.0, compute capability: 3.7)\r\n2018-05-24 15:29:08.953221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:4 with 10761 MB memory) -> physical GPU (device: 4, name: Tesla K80, pci bus id: 0000:00:1b.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.145282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:5 with 10761 MB memory) -> physical GPU (device: 5, name: Tesla K80, pci bus id: 0000:00:1c.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.337138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:6 with 10761 MB memory) -> physical GPU (device: 6, name: Tesla K80, pci bus id: 0000:00:1d.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.529156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:7 with 10761 MB memory) -> physical GPU (device: 7, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\n2018-05-24 15:29:09.904275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2018-05-24 15:29:09.904611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-24 15:29:09.904631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 4 5 6 7\r\n2018-05-24 15:29:09.904638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y Y Y Y Y\r\n2018-05-24 15:29:09.904650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y Y Y Y Y\r\n2018-05-24 15:29:09.904661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y Y Y Y Y\r\n2018-05-24 15:29:09.904672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N Y Y Y Y\r\n2018-05-24 15:29:09.904687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 4:   Y Y Y Y N Y Y Y\r\n2018-05-24 15:29:09.904692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 5:   Y Y Y Y Y N Y Y\r\n2018-05-24 15:29:09.904703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 6:   Y Y Y Y Y Y N Y\r\n2018-05-24 15:29:09.904714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 7:   Y Y Y Y Y Y Y N\r\n2018-05-24 15:29:09.905948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10761 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:17.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.906051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10761 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:18.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.906144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10761 MB memory) -> physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:00:19.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.906222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10761 MB memory) -> physical GPU (device: 3, name: Tesla K80, pci bus id: 0000:00:1a.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.906309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10761 MB memory) -> physical GPU (device: 4, name: Tesla K80, pci bus id: 0000:00:1b.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.906395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10761 MB memory) -> physical GPU (device: 5, name: Tesla K80, pci bus id: 0000:00:1c.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.906490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10761 MB memory) -> physical GPU (device: 6, name: Tesla K80, pci bus id: 0000:00:1d.0, compute capability: 3.7)\r\n2018-05-24 15:29:09.906587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10761 MB memory) -> physical GPU (device: 7, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl and num_packs = 1\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-05-24 15:29:11.580453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2018-05-24 15:29:11.580794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-24 15:29:11.580813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 4 5 6 7\r\n2018-05-24 15:29:11.580825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y Y Y Y Y\r\n2018-05-24 15:29:11.580833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y Y Y Y Y\r\n2018-05-24 15:29:11.580838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y Y Y Y Y\r\n2018-05-24 15:29:11.580850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N Y Y Y Y\r\n2018-05-24 15:29:11.580864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 4:   Y Y Y Y N Y Y Y\r\n2018-05-24 15:29:11.580878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 5:   Y Y Y Y Y N Y Y\r\n2018-05-24 15:29:11.580888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 6:   Y Y Y Y Y Y N Y\r\n2018-05-24 15:29:11.580899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 7:   Y Y Y Y Y Y Y N\r\n2018-05-24 15:29:11.582151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10761 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:17.0, compute capability: 3.7)\r\n2018-05-24 15:29:11.582255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10761 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:18.0, compute capability: 3.7)\r\n2018-05-24 15:29:11.582341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10761 MB memory) -> physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:00:19.0, compute capability: 3.7)\r\n2018-05-24 15:29:11.582425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10761 MB memory) -> physical GPU (device: 3, name: Tesla K80, pci bus id: 0000:00:1a.0, compute capability: 3.7)\r\n2018-05-24 15:29:11.582521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10761 MB memory) -> physical GPU (device: 4, name: Tesla K80, pci bus id: 0000:00:1b.0, compute capability: 3.7)\r\n2018-05-24 15:29:11.582607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10761 MB memory) -> physical GPU (device: 5, name: Tesla K80, pci bus id: 0000:00:1c.0, compute capability: 3.7)\r\n2018-05-24 15:29:11.582689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10761 MB memory) -> physical GPU (device: 6, name: Tesla K80, pci bus id: 0000:00:1d.0, compute capability: 3.7)\r\n2018-05-24 15:29:11.582770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10761 MB memory) -> physical GPU (device: 7, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n```\r\n\r\nThanks!\r\n\r\n\r\n", "Your code seems reasonable, except I wouldn't put 'cpu' in the input_fn.\r\nAs per https://github.com/tensorflow/tensorflow/issues/18473#issuecomment-391728363, the latest code solves the problem.  Could you please try that too?", "Oh, I remembered this bug that we found during the 1.8RCs.\r\n\r\nIf you update to the latest stable release then the problem goes away.", "@isaprykin what is the location of `distribute` in this release? \r\nThanks!", "Are you asking about this? https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/distribute", "Yes, I just pip installed 1.7.1 and this module couldn't be found in `contrib`. Is it necessary to install from source?", "Could you please try 1.8?  That's the latest release.", "I just tried the latest and it is solved! Thanks"]}, {"number": 18472, "title": "Make changes", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 18471, "title": "Create Stuff.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 18470, "title": "Contrib MPI: Fix errors import", "body": "For logging, mpi_utils.h needs to import error.h", "comments": ["Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18469, "title": "Feature request: Use feature columns for labels", "body": "As you describe in the documentation\r\n> Think of feature columns as the intermediaries between raw data and Estimators. Feature columns are very rich, enabling you to transform a diverse range of raw data into formats that Estimators can use, allowing easy experimentation. [https://www.tensorflow.org/get_started/feature_columns](https://www.tensorflow.org/get_started/feature_columns)\r\n\r\nfeature columns are pretty handy to transform input data. I would like to see the same functionality for labels. Why can't I transform labels with the same \"data columns\" as features?\r\n\r\nHave I written custom code N/A\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Updated", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @Baschdl \r\nMost of the time complexity is around features. That's why we designed FeatureColumn. \r\nFor labels, most of the time main transformation is converting strings to indices (classification). For that we added 'vocabulary_keys' to pre-made estimators such as [DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier). \r\nFor other kind of transformations I would recommend to do that as part of `input_fn`"]}, {"number": 18468, "title": "Fix warnings in tf.distributions.Categorical", "body": "In tf.distributions.Categorical dimension was used with argmax.\r\nAs dimension has been deprecated this generates a warning.\r\nThis fix fixes the warning by changing to axis.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18467, "title": "Add missing semicolon", "body": "I missed semicolon in https://github.com/tensorflow/tensorflow/pull/18451#pullrequestreview-111513425", "comments": []}, {"number": 18466, "title": "Install absl before building", "body": "Install absl-py before building windows\\gpu\\cmake\\run_build.bat.", "comments": []}]