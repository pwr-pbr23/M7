[{"number": 14481, "title": "Tensorflow stops training on random epoch", "body": "When I run the following code on GPU, it trains well for some epoches and then just hangs.\r\nWhile hanged processes are still alive but the GPU usages become 0%.\r\nIn the below code I am using Dataset API from tf.contrib.data.Dataset. But I also tried using placeholder and feed dictionary approach which hangs as well on random epoch during training. \r\nI am struggling with the problem for last 2-3 weeks and cannot find a way out.\r\nI am running the code on a remote GPU cluster. Here are some information about cluster node,\r\nUsing tensorflow gpu version 1.4\r\n```\r\n   NodeName=node050 Arch=x86_64 CoresPerSocket=1\r\n   CPUAlloc=0 CPUErr=0 CPUTot=24 CPULoad=12.03 Features=Proc24,GPU4\r\n   Gres=gpu:4\r\n   NodeAddr=node050 NodeHostName=node050 Version=15.08\r\n   OS=Linux RealMemory=129088 AllocMem=0 FreeMem=125664 Sockets=24 Boards=1\r\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A\r\n   BootTime=2017-11-07T08:20:00 SlurmdStartTime=2017-11-07T08:24:06\r\n   CapWatts=n/a\r\n   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0\r\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\r\n```\r\nCODE\r\n\r\n```\r\ndat_split = np.load('data/dat_split2.npy')\r\nX_train = dat_split[0].astype(np.float32)\r\nX_test = dat_split[1].astype(np.float32)\r\ny_train = dat_split[2].astype(np.int32)\r\ny_test = dat_split[3].astype(np.int32)\r\n\r\nnum_epochs = 100\r\n\r\n\r\ntrain_data_len = X_train.shape[0]\r\ntest_data_len = X_test.shape[0]\r\nnum_joints = len(considered_joints)\r\nnum_classes = len(classes)\r\n\r\n\r\n############ taking batch_size even data##########\r\neven_train_len = (train_data_len//batch_size)*batch_size\r\neven_test_len = (test_data_len//batch_size)*batch_size\r\n\r\nX_train = X_train[:even_train_len]\r\nX_test = X_test[:even_test_len]\r\ny_train = y_train[:even_train_len]\r\ny_test = y_test[:even_test_len]\r\n\r\n\r\ntrain_dat = Dataset.from_tensor_slices((X_train, y_train))\r\ntrain_dat = train_dat.batch(batch_size)\r\n\r\ntest_dat  = Dataset.from_tensor_slices((X_test, y_test))\r\ntest_dat = test_dat.batch(batch_size)\r\n    \r\niterator = Iterator.from_structure(train_dat.output_types, train_dat.output_shapes)\r\n\r\ntrainig_iterator_init = iterator.make_initializer(train_dat)\r\ntest_iterator_init = iterator.make_initializer(test_dat)\r\n   \r\nif __name__ == '__main__':\r\n   \r\n    global_cell = GlobalLSTM(num_units=num_units_each_cell, num_joints=num_joints)   #GlobalLSTM is a subtype of RNNCell\r\n    next_element = iterator.get_next()\r\n    X_loaded2, Y_loaded = next_element\r\n    X_loaded = tf.where(tf.is_nan(X_loaded2), tf.zeros_like(X_loaded2), X_loaded2)\r\n    \r\n    init_state = global_cell.zero_state((batch_size), tf.float32)\r\n    rnn_ops, rnn_state = tf.nn.dynamic_rnn(global_cell, X_loaded, dtype=tf.float32)\r\n    \r\n    with tf.variable_scope('softmax__'):\r\n        W = tf.get_variable('W', [(num_joints)*num_units_each_cell, num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\r\n        b = tf.get_variable('b', [num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\r\n\r\n  \r\n      \r\n    final_logits = tf.matmul(rnn_state[1], W) + b       # taking h state of rnn \r\n    with tf.name_scope(\"loss_comp\"):\r\n        total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_logits, labels=tf.one_hot(Y_loaded, num_classes)))\r\n    with tf.name_scope(\"train_step\"):\r\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\r\n\r\n    with tf.name_scope(\"pred_accu\"):\r\n        predictions = tf.nn.softmax(final_logits)\r\n        pred2 = tf.reshape(tf.argmax(predictions, 1), [-1, 1])\r\n        correct_pred = tf.equal(pred2, tf.cast(Y_loaded, tf.int64))\r\n        accuracy_ = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n    \r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n  \r\n        tic = time.clock()    \r\n        for step in range(num_epochs):\r\n            sess.run(trainig_iterator_init)\r\n            batch_cnt = train_data_len//batch_size\r\n            epch_loss = 0.0\r\n            epch_acc = 0.0\r\n            for bt in range(batch_cnt):\r\n                _, loss_, acc = sess.run([train_step, total_loss, accuracy_])\r\n                epch_loss += loss_\r\n                epch_acc += acc\r\n            print ('loss after epoch, ', step,': ', epch_loss/batch_cnt, ' ## accuracy : ', epch_acc/batch_cnt)\r\n            \r\n        print (\"optimization finished, time required: \", time.clock()-tic)\r\n\t\t\r\n\t\t\r\n\t\t#############test accuracy##############\r\n        batch_cnt = test_data_len//batch_size\r\n        sess.run(test_iterator_init)\r\n        print ('testing accuracy on test data : batch number', batch_cnt)\r\n        epch_acc = 0.0\r\n        for bt in range(batch_cnt):\r\n            acc = sess.run(accuracy_)\r\n            epch_acc += acc\r\n        print ('testing accuracy : ', epch_acc/batch_cnt)  \r\n```\r\nHere are some screen shot of different hangs,\r\n**Hanged on an epoch**\r\n![hanged_epc](https://user-images.githubusercontent.com/15855504/32688711-1be4b622-c6a4-11e7-93eb-92f29ad8ee37.JPG)\r\n**GPU usage that time**\r\n![hanged](https://user-images.githubusercontent.com/15855504/32688718-3763037c-c6a4-11e7-9ed0-d89efd1663c6.JPG)\r\n**GPU usage while running (not hanged)**\r\n![running_gpuusage](https://user-images.githubusercontent.com/15855504/32688721-4b8d42cc-c6a4-11e7-9ebe-61d48ed2bcac.JPG)\r\n**Hanged on another eopch**\r\n![hanged2](https://user-images.githubusercontent.com/15855504/32688742-b35474de-c6a4-11e7-8492-493d9ce128cb.JPG)\r\n\r\n\r\nThis type of random hanging behavior keeps repeating on each run. \r\nEach time it hangs on a random epoch. That's why I cannot figure out what is going wrong.\r\nBy looking at code or other set up can anybody please give me any idea about what is going wrong or how can I debug this out? Thanks \r\n\r\n\r\n\r\n", "comments": ["Any idea where it is hanging? Can you get the stack traces?", "Hello drpngx,\r\nThanks for your reply.\r\nI am trying to get stack trace. Need to figure out how can I do that on hanged processes. What is confusing to me is that, sometimes it runs up to completion of training, but most of time it hangs somewhere in middle. After printing the epoch loss line it should be in the \"sess.run(trainig_iterator_init)\". Any guess about why its behaving like this? ", "You can do gdb attach. You will get many stack traces, for all hanging threads, but typically only one will but stuck in an rpc wait, then you can pop up from there. Not ideal.", "thanks for the advice. i will let you know", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Not tracking the problem.."]}, {"number": 14480, "title": "Dataset API batching is slow for numpy arrays", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.4rc1\r\n- **Python version**:\r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nNot sure, think 0.7?\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4\r\n- **CUDA/cuDNN version**:\r\n8/6\r\n- **GPU model and memory**:\r\ngtx980\r\n- **Exact command to reproduce**:\r\nsee attached\r\n\r\n### Describe the problem\r\nThe ```tf.data.Dataset.batch()``` functions seems to be slow when concatenating numpy arrays into batches.\r\n\r\nThe attached code basically pushes dummy data ([0, 1, 2, ... , NUM_ITEMS] resembling the output of ```range()```) through tensorflow by different approaches.\r\n\r\nThere are basically two ways to go about this:\r\n1. Use the ```tf.data.Dataset.range()```\r\n2. Use custom generators and ```tf.data.Dataset.from_generator()```\r\n\r\nAlso the data can be batched to increase throughput as it is usually done in deep learning. This can be done either in the generator itself or via ```tf.data.Dataset.batch()```. The latter appears to be ~30x slower for numpy arrays. Somehow this is not the case for 'native' tensorflow generation.\r\n\r\nI attached a benchmark to reproduce this. Please note that the behavior for larger amounts of data e.g. images is similar, resulting in only tens of images per second instead of hundreds.\r\n\r\n### Comments to the source code:\r\n```gen()``` is a python generator resembling ```range()```.\r\n```gen_batch()``` is also a python generator, but yields batches of 100 numbers instead of single numbers.\r\n\r\nThese two generators are benchmarked first, yielding millions of elements per second. They should not be a bottleneck.\r\n\r\nThen we define a few ```tf.data.Dataset```:\r\n```ds_range_single``` is similar to ```gen()```, using ```tf.data.Dataset.range()```\r\n```ds_range_batch``` is similar to ```gen_batch()```, using ```tf.data.Dataset.range().batch()```\r\n\r\n```ds_npy_single``` uses ```gen()``` with ```tf.data.Dataset.from_generator()```\r\n```ds_npy_batch``` uses ```gen()``` with ```tf.data.Dataset.from_generator().batch()```\r\n```ds_npy_single_batch``` uses uses ```gen_batch()``` and will perform much better than ```ds_npy_batch```. In fact it will be close to the 'native' ```ds_range_batch```.\r\n\r\n### Source code / logs\r\n```python\r\nfrom time import time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nNUM_ITEMS = 100000\r\nBATCH_SIZE = 100\r\n\r\n\r\ndef gen():\r\n    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64):\r\n        yield x\r\n\r\n\r\ndef gen_batch():\r\n    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64).reshape(NUM_ITEMS//BATCH_SIZE, BATCH_SIZE):\r\n        yield x\r\n\r\nstart = time()\r\nfor _ in gen():\r\n    pass\r\npy_single_generator_time = time() - start\r\n\r\nstart = time()\r\nfor _ in gen_batch():\r\n    pass\r\npy_batch_generator_time = time() - start\r\n\r\nds_range_single = tf.data.Dataset.range(NUM_ITEMS)\\\r\n    .make_one_shot_iterator().get_next()\r\n\r\nds_range_batch = tf.data.Dataset.range(NUM_ITEMS)\\\r\n    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()\r\n\r\nds_npy_single = tf.data.Dataset.from_generator(\r\n    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\\\r\n    .make_one_shot_iterator().get_next()\r\n\r\nds_npy_batch = tf.data.Dataset.from_generator(\r\n    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\\\r\n    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()\r\n\r\nds_npy_single_batch = tf.data.Dataset.from_generator(\r\n    gen_batch, output_types=tf.int64, output_shapes=tf.TensorShape([BATCH_SIZE]))\\\r\n    .make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n    start = time()\r\n    for _ in range(NUM_ITEMS):\r\n        sess.run(ds_npy_single)\r\n    npy_single_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\r\n        sess.run(ds_npy_batch)\r\n    npy_batch_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\r\n        sess.run(ds_npy_single_batch)\r\n    npy_single_batch_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS):\r\n        sess.run(ds_range_single)\r\n    range_single_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\r\n        sess.run(ds_range_batch)\r\n    range_batch_time = time() - start\r\n\r\n\r\nprint('Python single generator examples/s :', NUM_ITEMS/py_single_generator_time)\r\nprint('Python batch generator examples/s :', NUM_ITEMS/py_batch_generator_time)\r\nprint('tf npy single examples/s :', NUM_ITEMS/npy_single_time)\r\nprint('tf npy batch examples/s :', NUM_ITEMS/npy_batch_time)\r\nprint('tf npy single batch examples/s', NUM_ITEMS/npy_single_batch_time)\r\nprint('tf range single examples/s :', NUM_ITEMS/range_single_time)\r\nprint('tf range batch examples/s :', NUM_ITEMS/range_batch_time)\r\n```\r\n### Prints the following on my machine:\r\nPython single generator examples/s : 3779128.899140432\r\nPython batch generator examples/s : 137113566.52500817\r\ntf npy single examples/s : 2623.528591111384\r\ntf npy batch examples/s : 10184.987886595052\r\ntf npy single batch examples/s 308841.62594729604\r\ntf range single examples/s : 4673.512357184766\r\ntf range batch examples/s : 352137.31184393575", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "If it's *so* much slower than Python, there might be a bug. @jsimsa is looking into similar issues in #13101, and this test case is really useful, thanks!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "As per my comment on #13101, executing `tf.data` pipelines comes with overhead (moving data between Python and C++, scheduling TensorFlow ops, ...). I agree that the difference in throughput is surprising and will take a closer look at this. Having said that, the `tf.data` runtime performance is not optimized for this type of pipeline; it is optimized for pipelines that do non-trivial I/O and data transformations which can additionally benefit from parallel processing. \r\n\r\nFor instance, if the generator in your example would do work that requires one second of CPU time per element, then executing the non-`tf.data` pipeline for `1000` elements would take ~`1000` seconds irrespective of your hardware. In contrast, you can write a `tf.data` pipeline that does the work inside of `tf.data.Dataset.map`, which can be parallelized and on a machine with `n` CPU cores, the pipeline would take ~`1000/n` seconds to execute.\r\n\r\nIn other words, the objective of `tf.data` runtime is not to match the performance of sequential execution of Python code on trivial pipelines. Its objective is to efficiently utilize multicore architectures for non-trivial pipelines.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@jsimsa why was this issue closed? Im trying to debug my input pipeline, and to isolate problems with my dataset and reading from disk, I had a similar testcase where I was trying to generate random/dummy input data and noticed this huge delay. "]}, {"number": 14479, "title": "Wrong link in tensorflow-for-poets codelab", "body": "https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#3\r\n\r\nThere's a link for `Inception V3 model` pointing to https://github.com/tensorflow/models/tree/master/slim#pre-trained-models which 404's.\r\n\r\nI assume it can point to https://arxiv.org/abs/1512.00567 instead.\r\n\r\n<img width=\"375\" alt=\"screen shot 2017-11-11 at 06 37 47\" src=\"https://user-images.githubusercontent.com/287584/32687045-e22af624-c6aa-11e7-9773-a189f761a5ba.png\">\r\n\r\nI would've sent a PR fixing the link, but the codelab text doesn't seem to be available on anywhere Github. \ud83d\ude1e ", "comments": ["@MarkDaoust could you take a look?", "[Fixed](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models)"]}, {"number": 14478, "title": "Remove redundant code of slice_op.cc", "body": "In `slice_op.cc` line 252, when `input_dims` is 4, it is handle as a special case. \r\n\r\n```\r\nif (input_dims == 4) {\r\n        HandleCase4D(context, begin, size, result);\r\n} else {\r\n      HANDLE_DIM(1);\r\n      HANDLE_DIM(2);\r\n      HANDLE_DIM(3);\r\n      HANDLE_DIM(4); // Not necessary\r\n      HANDLE_DIM(5);\r\n      HANDLE_DIM(6);\r\n}\r\n````\r\nSo it is not necessary to handle the 4 case when `input_dims` is not  4.", "comments": ["Can one of the admins verify this patch?", "A test failed because of the test tool itself. @jhseu can you rerun the failed test and merge this PR?  ", "@tensorflow-jenkins test this please"]}, {"number": 14477, "title": "error message show while compiling tensorflow from source", "body": "bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgemm_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrsv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIdamin_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChpr2_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSscal_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemmBatched'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyr2_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDznrm2_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtpmv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsymm_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspmv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtbmv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnBatchNormalizationForwardTraining'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsyrk_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSger_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgbmv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIdamax_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftSetStream'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSasum_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDevicePrimaryCtxSetFlags'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecZ2D'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgbmv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrmm_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtpmv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCreate_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpr_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlanMany'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCher_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrsv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdotu_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasScasum_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCscal_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChemm_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateNormal'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIcamax_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardDataAlgorithm'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrmm_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrmv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgeru_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdrot_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlan3d'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyr2_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtpsv_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreate'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Smemory_Ustats_Cgen_Umemory_Ustats_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspr_v2'\r\n", "comments": ["It would be better if you post your system information as required. \r\n\r\n> ### System information\r\n> - **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n> - **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n> - **TensorFlow installed from (source or binary)**:\r\n> - **TensorFlow version (use command below)**:\r\n> - **Python version**: \r\n> - **Bazel version (if compiling from source)**:\r\n> - **GCC/Compiler version (if compiling from source)**:\r\n> - **CUDA/cuDNN version**:\r\n> - **GPU model and memory**:\r\n> - **Exact command to reproduce**:\r\n", "- I tried to compile tensorflow from source in jetson tk1,\r\ni use external sd card for booting (32 GB), ubuntu 14.04. already install cuda 7.0 with cudnn 4 (for dummy while compiling), python 2.7.6, gcc 4.8.5, bazel 0.5.4-dist, GPU Model K20A and memory of Jetson only 2 GB\r\n\r\nThe  steps are :\r\nhttps://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout rv1.4.0\r\ngrep -Rl \"lib64\"| xargs sed -i 's/lib64/lib/g'  bec ause jetson tk1 only support 32 bit\r\n./configure \r\n   chose jemalloc and cuda support,with cuda sdk 7.0 and cudnn 4\r\nbazel build --config=opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package \r\n\r\nand the result shown above after compiling file around 3000 files\r\n\r\nThx\r\n", "It looks like it's not linking to cublas correctly. The way to debug this is to get the build command, and look at the .so or .a file in cublas using nm to see what symbols are defined.", "I have same error.\r\n```\r\n$ nm /usr/local/cuda-9.1/lib64/libcudnn.so.7\r\nnm: /usr/local/cuda-9.1/lib64/libcudnn.so.7: no symbols\r\n```", "@mpkuse you can use `nm -D /usr/local/cuda-9.1/lib64/libcudnn.so.7`to see the stripped dynamic symbols.\r\nFor more information see https://unix.stackexchange.com/questions/282616/why-nm-shows-no-symbols-for-lib-i386-linux-gnu-libc-so-6", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 14476, "title": "IOS camera example load map_data  from in_pix is incorrect!", "body": "    \r\n//    for (int y = 0; y < wanted_input_height; ++y) {\r\n//        float *out_row = out + (y * wanted_input_width * wanted_input_channels); //0+height\r\n//        for (int x = 0; x < wanted_input_width; ++x) {\r\n//            const int in_x = (y * image_width) / wanted_input_width;\r\n//            const int in_y = (x * image_height) / wanted_input_height;\r\n//            tensorflow::uint8 *in_pixel =\r\n//            in + (in_y * image_width * image_channels) + (in_x * image_channels);\r\n//            float *out_pixel = out_row + (x * wanted_input_channels);\r\n//            for (int c = 0; c < wanted_input_channels; ++c) {\r\n//                out_pixel[c] = (in_pixel[c] - input_mean) / input_std;\r\n//            }\r\n//        }\r\n//    }\r\n\r\nfixed code \uff1arotoate 90 and resized\r\n\r\n   for (int y = 0; y < wanted_input_width; ++y) {\r\n        float *out_row = out + (y * wanted_input_width * wanted_input_channels); \r\n        for (int x = 0; x < wanted_input_height; ++x) {\r\n            const int in_x = (y * image_width) / wanted_input_width;\r\n            const int in_y =image_height - (x * image_height) / wanted_input_height;\r\n            tensorflow::uint8 *in_pixel =\r\n            in + (in_y * image_width * image_channels) + (in_x * image_channels);\r\n            float *out_pixel = out_row + (x * wanted_input_channels);\r\n            for (int c = 0; c < wanted_input_channels; ++c) {\r\n                out_pixel[c] = (in_pixel[c] - input_mean) / input_std;\r\n            }\r\n        }\r\n    }", "comments": ["@petewarden  can you please take a look? Feel free to reassign it to someone else.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 14475, "title": "Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain", "body": "### SYSTEM Infomation\r\nubuntu16.04\r\ncuda7.5\r\ncudnn5\r\ngtx1060\r\ntensorflow1.0.1\r\npython3.5\r\nmemory 15.6G ,used 5.3GB\r\n\r\n$ nvidia-smi\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 106...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 33%   38C    P2    27W / 120W |   6012MiB /  6071MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1536      G   /usr/lib/xorg/Xorg                           273MiB |\r\n|    0      3210      G   compiz                                       115MiB |\r\n|    0      3811      G   ...-token=C6D7A354DD6B35830E1B2860115A47BF    82MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n### run train script get this error:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nCalled with args:\r\nNamespace(cfg_file='./lstm/lstm.yml', gpu_id=0, max_iters=700000, network_name='LSTM_train', pre_train=None, randomize=False, restore=0, set_cfgs=None)\r\nUsing config:\r\n{'CHARSET': '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\r\n 'EXP_DIR': 'lstm_ctc',\r\n 'FONT': 'fonts/Ubuntu-M.ttf',\r\n 'GPU_ID': 0,\r\n 'IMG_SHAPE': [180, 60],\r\n 'LOG_DIR': 'lstm_ctc',\r\n 'MAX_CHAR_LEN': 6,\r\n 'MAX_LEN': 6,\r\n 'MIN_LEN': 4,\r\n 'NCHANNELS': 1,\r\n 'NCLASSES': 64,\r\n 'NET_NAME': 'LSTM',\r\n 'NUM_FEATURES': 60,\r\n 'POOL_SCALE': 2,\r\n 'RNG_SEED': 3,\r\n 'ROOT_DIR': '/srv/python/lstm_ctc_ocr_with_tf_1.0.1',\r\n 'SPACE_INDEX': 0,\r\n 'SPACE_TOKEN': '',\r\n 'TEST': {},\r\n 'TIME_STEP': 90,\r\n 'TRAIN': {'BATCH_SIZE': 32,\r\n           'DISPLAY': 100,\r\n           'GAMMA': 1.0,\r\n           'LEARNING_RATE': 0.001,\r\n           'LOG_IMAGE_ITERS': 100,\r\n           'MOMENTUM': 0.9,\r\n           'NUM_EPOCHS': 2000,\r\n           'NUM_HID': 128,\r\n           'NUM_LAYERS': 2,\r\n           'SNAPSHOT_INFIX': '',\r\n           'SNAPSHOT_ITERS': 2000,\r\n           'SNAPSHOT_PREFIX': 'lstm',\r\n           'SOLVER': 'RMS',\r\n           'STEPSIZE': 2000,\r\n           'WEIGHT_DECAY': 1e-05},\r\n 'VAL': {'BATCH_SIZE': 128,\r\n         'NUM_EPOCHS': 1000,\r\n         'PRINT_NUM': 5,\r\n         'VAL_STEP': 500}}\r\nOutput will be saved to `/srv/python/lstm_ctc_ocr_with_tf_1.0.1/output/lstm_ctc`\r\nLogs will be saved to `/srv/python/lstm_ctc_ocr_with_tf_1.0.1/logs/lstm_ctc/lstm_train/2017-11-11-11-13-49`\r\n/gpu:0\r\nTensor(\"data:0\", shape=(?, ?, 60), dtype=float32)\r\nTensor(\"conv4/BiasAdd:0\", shape=(?, ?, 30, 1), dtype=float32)\r\nTensor(\"time_step_len:0\", shape=(?,), dtype=int32)\r\nUse network `LSTM_train` in training\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/core/common_runtime/direct_session.cc:137] \r\nInternal: failed initializing StreamExecutor \r\nfor CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: \r\nCUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073709551615\r\nTraceback (most recent call last):\r\n  File \"./lstm/train_net.py\", line 89, in <module>\r\n    restore=bool(int(args.restore)))\r\n  File \"./lstm/../lib/lstm/train.py\", line 187, in train_net\r\n    with tf.Session(config=config) as sess:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1176, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 552, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n\r\n```\r\n\r\nI will be grateful to anyone for helping me\r\nThanks everyone\r\n\r\n", "comments": ["It looks like there is a problem with this:\r\n```\r\nfor CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: \r\nCUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073709551615\r\n```\r\n\r\nCan you try with:\r\n```\r\ngpu_fraction = 0.1\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing since the issue is stalled. Feel free to reopen if you have more information.", "It seems that there is no space in GPU, you would like to kill one process and re-run you code, or if you have  more than one gpu, set the gup number you want to use .", "I had an issue similar to this...\r\nI was trying to execute the tensorflow in a jupyter lab notebook.\r\n\r\nThe issue stopped after I ran jupyter with `sudo jupyter lab --allow-root`\r\n\r\nI hope this help...", "I had an issue similar to this when I used docker to run two tensorflow program using GPU.", "> It looks like there is a problem with this:\r\n> \r\n> ```\r\n> for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: \r\n> CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073709551615\r\n> ```\r\n> Can you try with:\r\n> \r\n> ```\r\n> gpu_fraction = 0.1\r\n> gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\r\n> sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n> ```\r\n\r\nI tried, but it still doesn' t work.", "`jupyter lab --allow-root` worked for me.", "Hi brothers\r\ni have 4 gpus on my PC. But i could utilize only one gpu. Others can not be used. Can you help me in this case. Actually i tried to use all gpus for dirrerent git repos.\r\n\r\nerror:\r\n\r\nAttempting to fetch value instead of handling error Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 12652838912", "@Harry-KIT ,\r\n\r\nYou are hitting OOM (CUDA_ERROR_OUT_OF_MEMORY). You can try forcing processes to use different GPUs in one of the following ways as per the context:\r\n\r\n- dlib: \r\n```\r\n  from dlib import cuda\r\n  cuda.set_device(<gpu number>\r\n```\r\n- OPENCV\r\n`   cv2.cuda.setDevice(<gpu number>`\r\n\r\n- More generic method\r\n```\r\ninstall numactl (sudo apt install numactl)\r\n   export CUDA_VISIBLE_DEVICES=<gpu_number>;numactl --cpunodebind=<gpu number> ./executable\r\n```\r\n\r\nHope this helps :+1: \r\n\r\nThank you.\r\n   "]}, {"number": 14474, "title": "Update docs for r1.4", "body": "", "comments": []}, {"number": 14473, "title": "Added checkpoint V1 test for SaveRestoreShardedTest", "body": "It seems that the V1 path of the SaveRestoreShardedTest test is not executed but the code is there. This patch will execute the V1 path.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14472, "title": "Merge internal changes", "body": "", "comments": ["@tensorflow-jenkins, test this please", "@tensorflow-jenkins, test this please\r\n", "@tensorflow-jenkins, test this please\r\n", "@jenkins test this please"]}, {"number": 14471, "title": "Branch 175324895", "body": "", "comments": []}, {"number": 14470, "title": "MKL: Adding Convolution implementation using the open source MKL-DNN Lib", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "What is the Ubuntu CC test that is preventing this PR from merging? It appears to be a Google Internal check. AFAICT none of these errors are related to the changes in this PR. Please merge."]}, {"number": 14469, "title": "Fix license declaration for Python headers", "body": "This appears to be a legacy package so we might want to consider deleting it at some point too.\r\n\r\ncc: @gunan ", "comments": []}, {"number": 14468, "title": "Added Ubuntu 16.04 Dockerfile with TF 1.4 optimized for CPU with Inte\u2026", "body": "\u2026l(R) MKL\r\nThis is an Ubuntu 16.04 Container running Tensorflow 1.4 with Intel MKL.\r\nTo test on a system with Docker installed, do this as root:\r\n\r\nBuild the container (Needs to be done only once; takes about 20 minutes)\r\n```\r\n# cd <tensorflow-root>tensorflow/tools/docker\r\n# docker build . -t intelaipg/tensorflow -f Dockerfile.devel-cpu-mkl\r\n```\r\nThen run the container as root. :\r\n\r\n```# docker run -it --mount type=bind,source=/root,target=/host intelaipg/tensorflow```\r\n\r\nThis will bind a directory in the container at /host to /root on the host machine.\r\n\r\nRun your tests and benchmarks.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "These failed checks don't seem to be related to this pull request. What do I need to do to get this PR merged?", "The PR is already approved. and the new file should not be covered by any of the failing tests.\r\nSo I will go ahead and merge."]}, {"number": 14467, "title": "Fix Python auto configure build support", "body": "7ccfbdf caused our Python autoconf to no longer be auto. This was likely due to the helper function _get_env_var() which has now been removed. I also used this change as an opportunity to apply DRY and remove some legacy documentation.\r\n\r\nThis change blocks tensorflow/tensorboard#719\r\n\r\nCC: @nlopezgi @gunan ", "comments": ["FAILURE\n \n", "FAILURE\n \n", "Also CC @case540 ", "@nlopezgi can you look at this too? I believe you made the original change.", "Hmm... why did the sanity checks abort?\r\n", "Jenkins, test this please.", "lgtm, iirc there was some configuration that did not set PYTHON_BIN_PATH properly, as long as that was fixed this should be fine."]}, {"number": 14466, "title": "Add named contacts to Code of Conduct", "body": "", "comments": ["Can one of the admins verify this patch?", "SUCCESS\n \n"]}, {"number": 14465, "title": "Fix a stray hyphen and add an SO link", "body": "To help address #14455.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "You probably should add your work email to your github account, and fix the email on the commit to be your work email too to make CLABot happy.", "SUCCESS\n \n", "CLAs look good, thanks!\n\n<!-- ok -->", "Oh, good, looks like it's been fixed.", "SUCCESS\n \n", "Were those failures a flake?", "Yeah, pretty sure your change wasn't the culprit. "]}, {"number": 14464, "title": "Revert \"Branch 175277161\"", "body": "Reverts tensorflow/tensorflow#14453\r\n\r\nThis push appears to have reverted a bunch of PRs.", "comments": ["SUCCESS\n \n", "FAILURE\n \n"]}, {"number": 14463, "title": "added CMake options to provide external zlib, GRPC, Eigen", "body": "Here are changes necessary to build tensorflow with different version of GRPC, Protobuf, Eigen, zlib, etc. It is not possible to compile project using two different versions of protobuf for example. To deal with that I've chosen approach similar to one in GRPC library. Also for some reason I don't see changes from https://github.com/tensorflow/tensorflow/pull/13867 in master, thus changes from that branch are also here, @mrry might know something. ", "comments": ["Can one of the admins verify this patch?", "I suspect #14464 has the answer to the missing PR code... I'd suggest to wait until that is merged and then rebase.", "thanks @mrry I'll ping you after then", "SUCCESS\n \n", "FAILURE\n \n", "@mrry rebased, I think it's ready for review now", "Please resolve conflicts. Thanks!", "@jhseu rebased, there is some problem with protobuf, but doesn't seem that it is related to this PR.", "Arg, it looks like there's a conflict with `tensorflow/contrib/cmake/CMakeLists.txt`. Can you please resolve, and we'll run the tests? Thanks!", "Here @mrry, should be ok now.", "Thanks for resolving the conflicts!\r\n\r\n@tensorflow-jenkins test this please.", "@mrry I see that some changes in master are reverted, is that correct and I should rebase onto it or just wait a bit?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I think the merge should be resolved now (indeed, it looks like `include(double_conversion)` was reverted).\r\n\r\n@tensorflow-jenkins test this please.", "@ whoever merges this, I think @googlebot is confused because I used the web tool to resolve the conflicts (and perhaps it doesn't have the right email address for my commit). AIUI we should still be fine to merge.", "Jenkins, test this please.", "@Slonegg could you take a look at the windows errors? Also, something went wrong on the CLA.", "@drpngx windows error seems to be \"Failed to checkout tag: 'b04e5cba356212e4e8c66c61bbe0c3a20537c5b9'\". I usually just fork all the dependencies to avoid problems like this, library developers often change tags and commit hashes. I think we have CLA problem because @mrry made one commit. ", "@Slonegg both commits appear to fail the CLA, according to our interface.\r\n`ba17124ccd4e5f8bec3db824ba06d95b1b48a3ec` is also broken.", "@drpngx probably something is wrong with the CLA system, mine is signed, just double-checked it right now", "Jenkins, test this please.", "@Slonegg you have to reply to this thread: \"I signed it!\" and it might get it unstuck.\r\n\r\nJenkins, test this please.", "I signed it!", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Slonegg we have you under the gmail address. Did you use that for the commit? Also, could you pull rebase and push again?", "@drpngx I've created a copy of this PR https://github.com/tensorflow/tensorflow/pull/16210 since this one seems to be impossible to fix", "Thanks!"]}, {"number": 14462, "title": "OpKernel errors after building tensorflow from sources", "body": "Instead of using pip to install pre-built tensorflow library, I downloaded the sources and manually built the library to support using my machine's CPU SIMD instructions (SSE4.1, SSE4.2, AVX, AVX2, FMA).\r\n\r\nI've installed tensorflow from sources using the official documentation (https://www.tensorflow.org/install/install_sources#PrepareMac)\r\n\r\nBut now I'm facing an array of OpKernel errors when I use tensorflow, here is a screenshot of my terminal...\r\n\r\n<img width=\"1264\" alt=\"screen shot 2017-11-10 at 9 39 24 pm\" src=\"https://user-images.githubusercontent.com/11311073/32675491-bd1047cc-c65f-11e7-97a9-6f936f2cc7da.png\">\r\n\r\nAlthough I ran the following commands, \r\n`pip3 install --upgrade '/tmp/tensorflow_pkg/tensorflow-1.4.0-cp35-cp35m-macosx_10_6_intel.whl'`\r\n\r\n`pip3 install --upgrade 'https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp35m-none-macosx_10_11_x86_64.whl'`\r\n\r\ntensorflow still raises the same errors when being imported in a session. Any help?\r\n\r\n**Operating System:** OS X El Captain 10.+\r\n\r\n**Versions of installed libraries**\r\nnltk (3.2.1)\r\nnumpy (1.13.3)\r\nolefile (0.44)\r\npandas (0.20.0)\r\npigar (0.7.1)\r\nPillow (4.0.0)\r\npip (9.0.1)\r\npipreqs (0.4.9)\r\nprotobuf (3.4.0)\r\npyparsing (2.1.10)\r\npython-dateutil (2.6.0)\r\npython-xlib (0.18)\r\npytz (2016.10)\r\nPyYAML (3.12)\r\nrequests (2.18.4)\r\nscikit-learn (0.18.1)\r\nscipy (0.18.1)\r\nsetuptools (36.7.0)\r\nsix (1.11.0)\r\nsklearn (0.0)\r\ntensorboard (1.0.0a5)\r\ntensorflow (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc2)\r\nTheano (0.8.2)\r\nurllib3 (1.22)\r\nWerkzeug (0.12.2)\r\nwheel (0.30.0)\r\nyarg (0.1.9)\r\n", "comments": ["This has been fixed in https://github.com/tensorflow/tensorflow/commit/f0b1e65b0ac9e587c117485a96a0eaf40675c518. If you pull the latest HEAD and build again, these warnings should not appear."]}, {"number": 14461, "title": "Sync Branch 175311543", "body": "@tensorflow-jenkins test this please", "comments": ["SUCCESS\n \n", "@gunan do you know where these tensorflow-jenkins \"SUCCESS\" messages come from? They're new, and confusing, because wrong.", "@tensorflow-jenkins test this please\r\n", "I am investigating, not sure what changed in our jenkins config.", "I made some changes on Jenkins config,\r\nLet me know if it helps or not with Jenkins spamming review threads.", "FAILURE\n \n", "I'll close this, we have to fix a merge accident first. I'll let you know.", "FAILURE\n \n"]}, {"number": 14460, "title": "Apparent incorrect behavior from resize_to_range() in models/preprocessor.py", "body": "From the comments for resize_to_range():\r\n\r\n  The output size can be described by two cases:\r\n  1. If the image can be rescaled so its minimum dimension is equal to the\r\n     provided value without the other dimension exceeding max_dimension,\r\n     then do so.\r\n  2. Otherwise, resize so the largest dimension is equal to max_dimension.\r\n\r\nThis logic would yield the wrong behavior for images with an aspect ratio near 1.  For instance, if we had a 1200x1200 image, and we had settings min_dimension = 600 and max_dimension = 1024, it seems that the desired behavior would be to rescale the image to 1024x1024.  Instead, following the logic above, the image would be rescaled to 600x600.\r\n\r\nAm I missing something?", "comments": ["@nealwu ", "Hi @MisterMorden, it looks like your issue is for the object_detection model in the [models repository](https://github.com/tensorflow/models). Please open an issue there and tag the owners of the object_detection model.", "Thanks for the point in the right direction."]}, {"number": 14459, "title": "MKL: Adding Batch Normalization implementation using the open source MKL-DNN Lib", "body": "", "comments": ["Can one of the admins verify this patch?", "SUCCESS\n \n", "@tensorflow-jenkins test this please.", "Jenkins, test this please."]}, {"number": 14458, "title": "MKL: Adding Concat op implementation using the open source MKL_DNN", "body": "", "comments": ["Can one of the admins verify this patch?", "SUCCESS\n \n", "@tensorflow-jenkins test this please.", "The Linux CPU Tests, Linux XLA, and ci.tensorflow.org failures don't seem to be related to this PR. Is this something that needs to be manually overridden to merge?", "What is the Ubuntu CC test that is preventing this PR from merging? It appears to be a Google Internal check. AFAICT none of these errors are related to the changes in this PR. Please merge.", "Jenkins, test this please."]}, {"number": 14457, "title": "MKL: Adding Relu implementation using the open source MKL-DNN", "body": "", "comments": ["Can one of the admins verify this patch?", "SUCCESS\n \n", "@tensorflow-jenkins test this please.", "What is the Ubuntu CC test that is preventing this PR from merging? It appears to be a Google Internal check. AFAICT none of these errors are related to the changes in this PR. Please merge. ", "Jenkins, test this please."]}, {"number": 14456, "title": "Windows native library (tensorflow_jni.dll) is dynamically linked to msvcp140.dll", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 SP1 x64\r\n- **TensorFlow installed from (source or binary)**: Downloaded from Maven repository\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n\r\n### Describe the problem\r\nI need to run TensorFlow on a Windows PC where I don't have admin rights. So I cannot install MS Visual C++ 2015 Redistributable package. And TensorFlow Windows native library (tensorflow_jni.dll) is dynamically linked to msvcp140.dll from that package.\r\n\r\nThus the only option to run TensorFlow for me is to download the DLL from some untrusted \"get any DLL\" site and put it somewhere in PATH.\r\n\r\nI believe it should not be the case. Please consider dropping dependency on MS Visual C++ 2015 Redistributable package.\r\nOr at least mention this dependency clearly in the \"Using TensorFlow with a Maven project\" section as well as \"Install on Windows\" here https://www.tensorflow.org/install/install_java\r\n\r\n### Source code / logs\r\nThe problem manifests itself as\r\n`Exception in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\Users\\...\\AppData\\Local\\Temp\\tensorflow_native_libraries-...-0\\tensorflow_jni.dll: Can't find dependent libraries`", "comments": ["@mrry ", "Asim, I assume you know how the JNI library is built, so can you suggest a solution here?\r\n\r\n(Since I don't think it's plausible to remove the dependency on `MSVCP140.dll`, perhaps we need to bundle it with the Maven package?)", "Thanks for the report. I'm not familiar enough with what the convention is when it comes to packaging programs that depend on it.\r\n\r\nI'll look into it.\r\n\r\nCCing @meteorcloudy @gunan in case they have any thoughts (they may not).\r\n\r\n`tensorflow_jni.dll` is built by [this script](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/tools/ci_build/windows/libtensorflow_cpu.sh#L53), that invokes bazel.  I see that the TensorFlow pip packages for Windows do not package `MSVCP140.dll` either. I'm not sure what the convention is when it comes to distributing it with a program, I'll try to dig into this.", "As far as I understand msvcp140.dll is a 'generic' library (iostream, containers, etc.) So it won't be needed if the project was built using MinGW, for example.\r\nAlternatively (if you prefer to stick to MS VC++ compiler) you could probably use `/NODEFAULTLIB` option and supply alternative (statically linked) implementation.\r\n\r\nThese are my \"out of thin air\" guesses though. So please ignore them if I don't know what I am talking about.\r\n\r\nP.S. Your build script applies some \"hacks\" with a comment that they can be removed once the following pull request is merged into Bazel:  https://github.com/bazelbuild/bazel/pull/2599\r\nWell, it was merged in March.", "Hi @sabi0,\r\n\r\nI guess your problem is how to get the dlls inside of the \"MS Visual C++ 2015 Redistributable\" package?  It can be easily solved if you have another computer: Download the package from microsoft.com , install it, then copy the dlls out.  AFAIK, build tensorflow with MinGW toolchains is no longer supported.  Please use VC instead. \r\n", "Thanks @snnn,\r\n\r\nI've already figured out where to get msvcp140.dll\r\nBut I believe the problem should be solved in a generic way. So that other users like me don't have to face it in the future.\r\nAt the very least \"MS Visual C++ 2015 Redistributable\" dependency should be clearly mentioned in the System Requirements as well as Installation documentation.", "Hi @asimshankar \r\nFor how to distribute , this article has some recommendations: \r\n https://blogs.msdn.microsoft.com/vcblog/2015/03/03/introducing-the-universal-crt/\r\n\r\n", "Bazel now use /MD to dynamically link to MSVCRT by default, but you can easily switch to static linking(/MT) by adding `--features=static_link_msvcrt` as a build option. (requires Bazel 0.7.0)\r\n\r\nBut be aware, using `/MT`  might cause problems like https://github.com/tensorflow/tensorflow/issues/12844#issuecomment-328034368", "msvcp140.dll is a \"Standard C++ Library for native code\", not a CRT.\r\nhttps://msdn.microsoft.com/en-us/library/8kche8ah.aspx\r\n\r\nAccording to MS it is possible (though not recommended) to bundle redistributable DLLs with the application. See https://msdn.microsoft.com/en-us/library/ms235299.aspx\r\n\r\nAnother option is to statically link the runtime libraries using `/MT` switch. This will likely blow the size of the tensorflow_jni.dll. So you might want to introduce a separate \"flavour\" of the Maven artifact: `<classifier>static</classifier>`. Keeping the current one (dynamic) as default.\r\n\r\nThe `/MT` option is described as statically linking _runtime_ libraries. So it might not work for standard C++ library. However it was mentioned as a solution to \"MSVCP140.dll missing\" question on StackOverflow: https://stackoverflow.com/questions/32998902/msvcp140-dll-missing\r\nSo I guess it does work for standard C++ library too.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@sabi0 Is this still a problem for you?", "I believe, this is a problem \"in general\", not just for me.\r\nAt the very least you should mention in the [installation instructions](https://www.tensorflow.org/install/install_java#install_on_windows) that there is a dependency on MS Visual C++ 2015 Redistributable package.\r\nBut an additional artifact (`<classifier>static</classifier>`) would have been a better solution, I believe.\r\n\r\nP.S. I've already obtained the msvcp140.dll and placed it into a folder in PATH. So everything works for me.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm going to go ahead and mark this as \"Contributions Welcome\" since it doesn't seem we have the required expertise within the TensorFlow team on Windows to work this out.\r\n\r\nHelp is certainly appreciated. Either in the form of a PR that updates the installation instructions, or ideally in the form of a PR that fixes the bazel BUILD rule. However, before sending any PR for the latter, perhaps describe the plan in a comment in this issue.\r\n\r\nThanks!", "Windows Server 2008 R1 64 bit\r\nJava JDK 8 64 bit / tensorflow 1.5.0-rc0.jar\r\n\r\nI was facing the same Error\r\n_java.lang.UnsatisfiedLinkError: tensorflow_native_libraries tensorflow_jni.dll: Can't find depedent libraries_\r\n\r\n@sabi0 , Thanks for your excellent debugging and RCA , I downloaded & installed Microsoft Visual C++ 2015 Redistributable on host machine and it worked for me. \r\n\r\n@asimshankar  Please update The Documentation at [Tensorflow Java Install](https://www.tensorflow.org/install/install_java), atleast it can be mentioned that tensorflow_jni.dll (libtensorflow_jni) depends externally on Microsoft Visual C++ 2015 as part of Quick Tips or Side note.", "@karankaw : That would be great. Could I trouble you (or any other volunteer) to send a PR? (I figured some information from folks who work in Windows more regularly than I do will be better).\r\n\r\nThe change would be to this file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_java.md \r\n", "@asimshankar I have raised PR https://github.com/tensorflow/tensorflow/pull/19719\r\nPlease approve, if its OK", "@sabi0, I have made some changes wrt mentioning dll dependency to \"Install on Windows\" section at https://www.tensorflow.org/install/install_java , wchich @asimshankar has approved/merged.\r\nI guess we can close this issue once it gets live on tensorflow site.", "I have followed all the instructions carefully here https://www.tensorflow.org/install/lang_java but i am still getting following error with TF1.12.0\r\n\r\njava -cp libtensorflow-1.12.0.jar;. -Djava.library.path=jni HelloTensorFlow\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n        at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)\r\n        at org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n        at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n        at org.tensorflow.Graph.<clinit>(Graph.java:361)\r\n        at HelloTensorFlow.main(HelloTensorFlow.java:9)\r\n\r\n\r\n@karankaw  can you please help.", "What version of tensorflow have people tried successfully on windows? ", "This is in eclipse ...\r\n\r\n\r\nException in thread \"Thread-4\" java.lang.UnsatisfiedLinkError: C:\\Users\\olelo\\AppData\\Local\\Temp\\tensorflow_native_libraries-1551676423743-0\\tensorflow_jni.dll: A dynamic link library (DLL) initialization routine failed\r\n\tat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n\tat java.lang.ClassLoader.loadLibrary0(Unknown Source)\r\n\tat java.lang.ClassLoader.loadLibrary(Unknown Source)\r\n\tat java.lang.Runtime.load0(Unknown Source)\r\n\tat java.lang.System.load(Unknown Source)\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:361)\r\n\r\n", "@sabi0 How did you figure out that it is msvcp140.dll which is missing?\r\nI am trying to run tensorflow on gpu and getting this generic `tensorflow_jni.dll: Can't find dependent libraries` error. How can I know what it missing?\r\nThanks in advance ;-)", "@patcher You need a tool like dumpbin or DLL Export Viewer to list the DLL imported functions.\r\n\r\nHere is what I have for tensorflow_jni.dll 1.5.0:\r\n```\r\nADVAPI32.dll\r\napi-ms-win-crt-convert-l1-1-0.dll\r\napi-ms-win-crt-environment-l1-1-0.dll\r\napi-ms-win-crt-filesystem-l1-1-0.dll\r\napi-ms-win-crt-heap-l1-1-0.dll\r\napi-ms-win-crt-locale-l1-1-0.dll\r\napi-ms-win-crt-math-l1-1-0.dll\r\napi-ms-win-crt-runtime-l1-1-0.dll\r\napi-ms-win-crt-stdio-l1-1-0.dll\r\napi-ms-win-crt-string-l1-1-0.dll\r\napi-ms-win-crt-time-l1-1-0.dll\r\napi-ms-win-crt-utility-l1-1-0.dll\r\nKERNEL32.dll\r\nMSVCP140.dll\r\nSHLWAPI.dll\r\nVCRUNTIME140.dll\r\n```", "Thanks a lot, I have similar picture in https://github.com/lucasg/Dependencies\r\nEverything seems fine, yet `Can't find dependent libraries` error.\r\nMy problem is actually only with GPU, like here https://github.com/tensorflow/tensorflow/issues/23656\r\nIn that issue the solution is to update python... But I don't use python, I am trying to run inference in java \u00af\\_(\u30c4)_/\u00af", "Did you check the 2nd level imports? (What's imported by MSVCP140.dll, etc.)\r\nIt is also important that JVM architecture (32 or 64 bits) matches that of loaded libraries. I.e. you cannot load a 64-bit DLL into a 32-bit JVM process and vice versa.", "Yes, but it is a lot. Since I have no experience with windows native development it gets overwhelming and does not tell me much.\r\nDependency Walker says that there are some deps missing, like API-MS-WIN-*, EXT-MS-WIN-* and IESHIMS.dll, but it is normal as I understood.\r\nI know about architectures, but I am not doing anything manually, only include tensorflow from maven in my gradle build. Anyway it must be doing the right thing since CPU version starts and seems to work. Unfortunately the network I've got is using tensor format that works only on GPU. And after swapping to `libtensorflow_jni_gpu` I get this `Can't find dependent libraries` error. Must be something with CUDA and friends then. I was hoping there is an easy way to figure out what exactly is missing ;-) ", "> Yes, but it is a lot. Since I have no experience with windows native development it gets overwhelming and does not tell me much.\r\n> Dependency Walker says that there are some deps missing, like API-MS-WIN-_, EXT-MS-WIN-_ and IESHIMS.dll, but it is normal as I understood.\r\n> I know about architectures, but I am not doing anything manually, only include tensorflow from maven in my gradle build. Anyway it must be doing the right thing since CPU version starts and seems to work. Unfortunately the network I've got is using tensor format that works only on GPU. And after swapping to `libtensorflow_jni_gpu` I get this `Can't find dependent libraries` error. Must be something with CUDA and friends then. I was hoping there is an easy way to figure out what exactly is missing ;-)\r\n\r\nHello, I have the same problem as you.On windows10 systems, the call to the tensorflow-cpu version using Java runs successfully, but replacing libtensorflow_jni with libtensorflow_jni_gpu will report an error in Can't find dependent libraries.Excuse me, have you solved it?", "@pacher Hello, I have the same problem as you.On windows10 systems, the call to the tensorflow-cpu version using Java runs successfully, but replacing libtensorflow_jni with libtensorflow_jni_gpu will report an error in Can't find dependent libraries.Excuse me, have you solved it?", "@swzaaaaaaa No, unfortunately not. It seems that the only way to get working tensorflow gpu on windows is to install it with Anaconda. So I ended up calling it through python using [Jep](https://github.com/ninia/jep). Jep is awesome, but to make it work on windows is also quite a challenge. It needs to compile native bits during installation using exactly the same C++ compiler as your python was compiled with. After lots of trial and error I succeed, but sadly forgot to write the exact steps down. Now I have a working installation which I am afraid to touch or update. \r\n\r\nFrom my notes back then, maybe it helps:\r\n\r\n> Jep install is easy: `pip install jep`, but you need RIGHT C++ compiler available.\r\n> On linux it's enough to `apt install build-essential` and `apt install python3-dev`\r\n> But on Windows it is quite a different story....\r\n> For python 3.6 you need Visual c++ 2015(!) command line build tools\r\n> This tools are even not visible on M$ download site\r\n> Working thing could be downloaded from:\r\n>      https://download.microsoft.com/download/5/f/7/5f7acaeb-8363-451f-9425-68a90f98b238/visualcppbuildtools_full.exe?fixForIE=.exe\r\n> or   https://download.microsoft.com/download/E/E/D/EEDF18A8-4AED-4CE0-BEBE-70A83094FC5A/BuildTools_Full.exe\r\n> but probably the first link\r\n> Then to install jep you have to:\r\n>      1. use one of the \"Visual C++ 2015\" (don't remember which one) shortcuts in start menu\r\n> to start command prompt\r\n>      2. From this prompt activate conda environment\r\n>      3. Finally `pip install jep`\r\n\r\n> And for python 3.7 you will probably need Visual C++ 2017...", "@pacher Thank you for your answer, but I have a question, you installed jep, is the language from Java back to python, so how to connect with javaweb?"]}, {"number": 14455, "title": "Tensorflow cannot be installed with default Windows Python 3.5 stack", "body": "After installing Python 3.5.0 using the Windows 64 bit installer (which includes pip in the install):\r\n\r\npip3 install --upgrade tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\nYou are using pip version 7.1.2, however version 9.0.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\r\n\r\nI tried on a different machine that worked, and found the only difference to be the pip version.  Updating to pip 9.0.1 solved the issue.\r\n\r\nIt's not explicitly stated anywhere that you need a newer version of pip.  When an old version of something is required to run something, people tend to ignore the messages indicating there is a newer version of it because that's exactly what they are expecting: \"yeah I know there's a newer version, I meant to do this\".\r\n\r\nIf this cannot be resolved for older version of pip (specifically, versions included with the required Python versions), could you please state this in the documentation.", "comments": ["In the PR above, I added a link on the Install Windows docs to include a link to a stack overflow page that covers this."]}, {"number": 14454, "title": "cuda_config.h is required to build non-CUDA release", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: NixOS 18.03.git.869485a (Impala)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nTensorFlow with CUDA support disabled doesn't build. CUDA support is disabled in `./configure`.\r\n\r\n### Source code / logs\r\n```\r\n./tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory\r\n #include \"cuda/cuda_config.h\"\r\n```", "comments": ["It seems that disabling MPI support allows TensorFlow to be built.", "Thanks for your feedback. \r\nWhat options you picked when running ./configure?", "```\r\n      export TF_NEED_GCP=1\r\n      export TF_NEED_HDFS=1\r\n      export TF_ENABLE_XLA=1\r\n      export TF_NEED_MPI=1\r\n      export TF_NEED_CUDA=0\r\n```\r\n\r\nOthers by default.", "@tfboyd @gunan ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 14453, "title": "Branch 175277161", "body": "", "comments": []}, {"number": 14452, "title": "Bug - freeze_graph producing invalid graph_def in tensorflow 1.4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: release 1.4\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: GTX 970, 4GB\r\n- **Exact command to reproduce**: \r\n\r\n1. Download [dbg.zip](https://github.com/tensorflow/tensorflow/files/1461735/dbg.zip) (contains `graph.pbtxt`, checkpoint and the resulting `frozen_model.pb` generated on my machine .)\r\n2. Unzip and open terminal in the unzipped folder\r\n3. Run <tensorflow_root>/python/tools/freeze_graph.py --input_graph=graph.pbtxt --input_binary=False --input_checkpoint=model.ckpt-1 --output_node_names=softmax_tensor --output_graph=frozen_model_test.pb --clear_devices=True\r\n4. Start python, attempt to import the frozen graph:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nwith gfile.FastGFile('frozen_model_test.pb','rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    tf.import_graph_def(graph_def, name='')\r\n```\r\n### Describe the problem\r\nTrying to import the graph, I get this error:\r\n> ValueError: graph_def is invalid at node 'IsVariableInitialized': Input tensor 'global_step:0' Cannot convert a tensor of type int64 to an input of type int64_ref.\r\n\r\nThe error is raised in `tf.import_graph_def(graph_def, name='')`. The dump of `str(graph_def)` can be seen in this text file: [graph_def_dbg.txt](https://github.com/tensorflow/tensorflow/files/1461768/graph_def_dbg.txt)\r\n\r\nThe error happens sine the upgrade to TF 1.4, with TF 1.3 the graph freezing and importing works as expected.\r\n", "comments": ["Looks like a bug in freeze graph, right @petewarden ?\r\n", "@GPhilo: Have you tried to blacklist IsVariableInitialized with the --variable_names_blacklist argument?\r\nI had the same issue with my graph, which were solved by blacklisting the variables.\r\nThe problematic variables were used in model to transfer the states of rnn cells from one iteration to another, these really had to remain variables and should not have been converted to constants.", "@asoehlke Just tried that, still the same error. If I also blacklist `global_step` though, the error goes away.\r\nThat however leaves global_step as variable and this breaks my further processing (I use the .pb in opencv's DNN module, which cannot handle variables)", "I have a similar problem with non-trainable variables (trainable=False) which are also modified within the model. A description of what I found out so far is below.\r\n\r\n@GPhilo: Can it be that the value of global_step is modified when processing the model? The name sounds like a kind of counter?\r\nIf it is modeled like this then I guess it needs to remain a variable, a const value could not be changed in the model.\r\n\r\n\r\nUnfortunately, blacklisting the variables seems not to be enough, when trying to use the model I now got the error that these variables are not initialized:\r\nAttempting to use uninitialized value rnn_state_c_0\r\n\t \\[\\[Node: rnn_state_c_0/read = Identity\\[T=DT_FLOAT, _class=[\"loc:@rnn_state_c_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn_state_c_0)]]\r\n\r\nSome further investigation: The init node of the model is gone in the frozen model. \r\nIn the attached files:\r\n- BachAccompanist.exported.pbtxt is the original exported graph\r\n- BachAccompanist.patched.pbtxt is a patched version of the exported graph, I manually copied the init node to a new node \"initVars\" and kept only the blacklisted variables as input.\r\n\r\nWhen I imported the patched frozen graph and ran the initVars node first, then the \"uninitialized variables\" error is gone.\r\nI reexported this model that I imported from the frozen model as BachAccompanist.frozen.reexported.pbtxt\r\n\r\nUnfortunatly I now seem to get only get a None result when running the model:\r\n output_step = sess.run(\"output\", {in_data : tick_data})\r\n print(output_step)\r\n-> None\r\nBut this might also be a different problem.\r\n\r\nSuggestions:\r\n- when freezing the model it would be good to have an option to only freeze the trainable variables. \r\n- the init node should remain for the blacklisted variables \r\n\r\n**System information**\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Studio 16.04\r\n    TensorFlow installed from (source or binary): source and binary\r\n    TensorFlow version (use command below): release 1.4.0\r\n    Python version: 3.6.3 (source installation), 3.4.5 (binary installation)\r\n    Bazel version (if compiling from source): 0.7.0\r\n    GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)\r\n    CUDA/cuDNN version: -\r\n    GPU model and memory: -\r\n    Exact command to reproduce:\r\nfreeze_graph --input_checkpoint BachAccompanist.pb.ckpt --input_graph BachAccompanist.exported.pbtxt --output_node_names output,state_update,initVars --input_binary False  --variable_names_blacklist rnn_state_h_0,rnn_state_c_0,rnn_state_h_1,rnn_state_c_1,rnn_state_h_2,rnn_state_c_2,rnn/multi_rnn_cell/cell_0/lstm_cell/w_f_diag,rnn/multi_rnn_cell/cell_0/lstm_cell/w_i_diag,rnn/multi_rnn_cell/cell_0/lstm_cell/w_o_diag,rnn/multi_rnn_cell/cell_1/lstm_cell/w_f_diag,rnn/multi_rnn_cell/cell_1/lstm_cell/w_i_diag,rnn/multi_rnn_cell/cell_1/lstm_cell/w_o_diag,rnn/multi_rnn_cell/cell_2/lstm_cell/w_f_diag,rnn/multi_rnn_cell/cell_2/lstm_cell/w_i_diag,rnn/multi_rnn_cell/cell_2/lstm_cell/w_o_diag --output_graph BachAccompanist.frozen.pb\r\n\r\nUsed models:\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/1468266/models.zip)\r\n\r\n\r\n \r\n", "Any updates on this?\r\n\r\nFrom what I could see, it seems `freeze_graph` fails to remove the `global_step/*` part of the graph. I believe this is due to the introduction of the initialization check for `global_step`, which introduces an `add` node whose control output is then input to many `Const` or `NoOp` further nodes in the graph.\r\n\r\nIt seems that the control input breaks `tensorflow.python.framework.graph_util_impl.convert_variables_to_constants()`. This is because, without the control output of the `add` node, the `global_step` subgraph is not connected to the inference graph output, so it gets pruned when calling `convert_variables_to_constants`. Since now there is a control input based on `add`, however, the function doesn't automatically prune the `global_step` and things break.\r\n\r\nThis is the only node in the frozen graph where the `^add` input is still preserved:\r\n```\r\nnode {\r\n  name: \"Reshape_1/shape\"\r\n  op: \"Const\"\r\n  input: \"^add\"\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          dim {\r\n            size: 2\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_INT32\r\n        tensor_shape {\r\n          dim {\r\n            size: 2\r\n          }\r\n        }\r\n        tensor_content: \"\\377\\377\\377\\377\\000\\006\\000\\000\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWhat is the purpose of this control input?", "Hello @GPhilo,\r\nI solved my issue now by defining the initial values of the problematic variables as additional placeholders.\r\nThis means I have to feed the initializing values additionally when using the model, but at least this got the model usable.\r\nThe initializers are the init_states variables in https://github.com/asoehlke/neuronal-music-accompanist-bach/blob/master/MusicAccompanistInferenceGraph.ipynb.\r\n\r\nWith this change, freeze_graph works without any blacklisted variables.", "@asoehlke for my use-case though, everything has to be made constant, since I won't be running inference in tensorflow.", "I also experienced this issue and got exactly the same error message. My model was also trained on tensorflow 1.4. Any updates on this issue?", "Same as @dratini6 .\r\nI've solved this issue by adding \"global_step\" in the black list on tf.graph_util.convert_variables_to_constants method.\r\n```python\r\noutput_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, # The session is used to retrieve the weights\r\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \r\n            output_node_names.split(\",\"), # The output node names are used to select the usefull nodes\r\n            variable_names_blacklist=['global_step']) \r\n```", "Thanks @Tauranis.  I tried your suggestion and it worked perfectly!", "<s>Also from me, thank you very much, this does solve the problem.</s> (see next comment)\r\n\r\nAs a comment, for those like me using directly `freeze_graph.py` as a script, add the argument `--variable_names_blacklist=global_step` to the call instead of modifying the python script itself.\r\n\r\nI won't close the issue yet (although, given the attention it received, that would make no difference..) since the original problem is IMO still there (the script should output a valid file without the need for users to know they have to exclude a variable to make it work).", "I believe the problem is that the global_step cannot be converted to a constant.\r\nMoreover, after freezing the graph some weird ops regarding global step appear on my graph, mainly on batch norm ops. And by then, the quantization could not be done. On issue #3628  I present a workaround that worked for me.", "What seemed to work yesterday actually still causes me problems when exporting frozen graphs.\r\nBlacklisting `global_step` effectively prevents it from being turned into a constant, but it is left in the .pb file as a variable.\r\nI need it to be either a constant or, better, removed altogether.\r\n\r\nIf I try to forcefully convert it to constant by adding `global_step` to `variable_names_whitelist`, I go back to the `ValueError` and invalid protobuf file problem described above.", "@GPhilo Yes, that occurs with me too. That's why I done the workaround presented on #3628 .\r\nBasically I create the graph from the source code (and not from **.meta** file), restore the weights and then save the model. That works perfectly to me.", "I found a workaround by deleting via script any `input: \"^add\"` line from the `graph.pbtxt` file before running `freeze_graph`. This way, the global_step branch of the graph is not connected anymore to the inference graph and is correctly removed by `convert_variables_to_constants`.", "/CC @petewarden can you take a look at this issue?", "Since there are workarounds and this is pretty old, closing this issue."]}]