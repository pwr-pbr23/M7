[{"number": 45472, "title": "No OpKernel was registered to support Op 'BroadcastTo' used by {{node gradients/Mean_grad/BroadcastTo}}with these attrs: [Tidx=DT_INT32, T=DT_FLOAT]", "body": "I have created a simple linear regression model and got a graph.pb file so i can used it in my android studio app. When I call  sess.runner().addTarget(\"init\").run(); I got this error: No OpKernel was registered to support Op 'BroadcastTo' used by {{node gradients/Mean_grad/BroadcastTo}}with these attrs: [Tidx=DT_INT32, T=DT_FLOAT]\r\n\r\nHere is the python model code:\r\n\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nx = tf.placeholder(tf.float32, name='input')\r\ny_ = tf.placeholder(tf.float32, name='target')\r\n\r\nW = tf.Variable(5., name='W')\r\nb = tf.Variable(3., name='b')\r\ny = tf.add(tf.multiply(W,x), b)\r\ny = tf.identity(y, name='output')\r\n\r\n           \r\nloss = tf.reduce_mean(tf.square(y - y_))\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\ntrain_op = optimizer.minimize(loss, name='train')\r\n\r\ninit = tf.global_variables_initializer()\r\nsaver_def = tf.train.Saver().as_saver_def()\r\nwith open('graph.pb', 'wb') as f:\r\n  f.write(tf.get_default_graph().as_graph_def().SerializeToString())\r\n", "comments": ["@alejandroaguileraalcalde-ing,\r\nI was able to run the code snippet on TF v2.3 without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3ccf00f3a3fb30ffde6b2157c22c7427/45472.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code along with the error log and also the TensorFlow version you are using. Thanks!", "Sorry for answering so late. It was a problem with the graph.pb file. I change a bit the mode codel and got a graph that was ok. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45472\">No</a>\n"]}, {"number": 45471, "title": "Failed to load native tensorflow runtime", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows Server 2012 R2 Standard\r\n- TensorFlow installed from : Binary (pip)\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7.2\r\n- Installed using : pip\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nTensorflow is installed without any issues. But when running my program I am getting the below error.\r\n\r\n> D:\\Tetherfi\\TRS\\TFaceAuthServer>python tfacecompare.py\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_hel\r\n> per\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\n> ion)\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", li\r\n> ne 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", li\r\n> ne 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"tfacecompare.py\", line 20, in <module>\r\n>     import tensorflow.keras\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\__init__.py\", line 41, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packag\r\n> es\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_hel\r\n> per\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\n> ion)\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", li\r\n> ne 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\kiolocal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", li\r\n> ne 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n\r\nI have tried installing visual c++ redistributable for 2015 (64 and 32 bit), but still the same. Reinstalling of python also didn't help.\r\nPlease guide and help to solve this.\r\n", "comments": ["@shreeraj04 \r\nCould you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/45435#issuecomment-739659395) and let us know if all the requirements are met as this is the common reason for the error.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45471\">No</a>\n"]}, {"number": 45470, "title": "Raspberry Pi 4 camera feed doesn't appear", "body": "Hi everyone,\r\n\r\n**System information**\r\n- OS Platform and Distribution : Raspbian\r\n- TensorFlow installed from : binary\r\n- TensorFlow version : v2.3.0-0-gb36436b087 2.3.0\r\n- Python version: Python 3.7.3\r\n- Model : Raspberry Pi 4 4gb ram\r\n\r\n**Describe the current behavior**\r\nWhen I'm using [image_classification example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/raspberry_pi) or [object_detection example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi) I didn't get any error but the camera feed doesn't appear so I couldn't check if it's working.\r\n\r\n**Other info / logs**\r\nI checked picamera and I could get the stream from the pi camera so my camera is working great.\r\nI build and install from setup.py in the main directory and downloaded the files in /tmp.\r\nEverytime I use ctrl+c to stop the program I got different writing like it is looping into the code. \r\n\r\nThanks for you help.\r\n\r\nI attached a picture with the terminal.\r\n<img width=\"1213\" alt=\"Capture d\u2019e\u0301cran 2020-12-08 a\u0300 11 37 08\" src=\"https://user-images.githubusercontent.com/45320002/101474746-d2638500-394b-11eb-9ad9-fcdbaa383bad.png\">\r\n\r\n", "comments": ["@Yosonix It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version TF 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45470\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45470\">No</a>\n"]}, {"number": 45469, "title": "Failed to build person_detection example test", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): All (presumably)\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): 4a11dfdd3bb\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): All (presumably), tested for ARC.\r\n\r\n**Describe the problem**\r\nAs I can see, now there is no longer common person detection, and int8 version became the [main one](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection). \r\nSo, actually I faced two issues, when I was trying to **build tests** for person detection.\r\n\r\n1. [This command](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/README.md#run-the-tests-on-a-development-machine) doesn't work at all, it says `make: *** No rule to make target 'test_person_detection_test'.  Stop.` and the right target actually is `test_person_detection_test_int8` as it used to be when it was an experimental version. Similarly, test generation works with `make -f tensorflow/lite/micro/tools/make/Makefile generate_person_detection_test_int8_make_project`.\r\n2. When test project is generated, it is stored in a following path: \r\n```\r\n\\gen\\[platform]\\prj\\person_detection_test_int8\\make\r\n```\r\nAnd build gives following error:\r\n```\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8/person_detect_model_data.cc', needed by 'tensorflow/lite/micro/tools/make/downloads/person_model_int8/person_detect_model_data.o'.  Stop.\r\n```\r\nThing is, it requires following project path (winthout _int8):\r\n```\r\n\\gen\\[platform]\\prj\\person_detection_test\\make\r\n```\r\nIf I change it manually, test can be built successfully. \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n1. `make -f tensorflow/lite/micro/tools/make/Makefile generate_person_detection_test_make_project`\r\n2.  \r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile generate_person_detection_test_int8_make_project\r\n*go into project folder*\r\nmake\r\n```\r\n", "comments": ["I've just run into this. Using `test_person_detection_test_int8` instead of `test_person_detection_test` works on macOS.\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_person_detection_test_int8\r\n```", "I'm working to update our documentation to always use person_detection_int8. At some point it may be worth renaming person_detection_int8 to person_detection since the original uint8 model is now gone, but in the meantime I'll make sure we use _int8 everywhere.\r\n\r\nI'm unable to replicate the makefile project generation issue above after trying the steps listed. Perhaps this issue is already fixed? Let me know if it persists for you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45469\">No</a>\n"]}, {"number": 45468, "title": "Strange Out Of Memory error when trying example Colab ", "body": "When I try to run the [officially provided Colab ](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer)\r\nnot changing any row, everything goes fine until the training `model = question_answer.create(train_data, model_spec=spec)` \r\nAs soon as training process started, after a while it begins to consume memory with big chunks (e.g. memory consumption increases by 2GB per 2-3 seconds) and shortly it crashes the Colab environment as it throws Out of Memory error. I think this is some sort of a bug, because otherwise this will not be official example, or it would be documented somewhere in the example that the free Colab environment with 16GB ram is not enough to run this example. \r\nI also tried to run the example with much smaller dataset, but the same problem in the same way is being reproduced. So I am almost sure that this is a bug in the library. \r\n", "comments": ["@ando0689 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nVan you share your colab gist where the error is present.", "Hi. I am sorry for that, but actually I don't know what to fill there.\r\nI opened this issue under the \"Other\" tag, as I guess it's not relevant what OS I use etc, because I just run the [**Official Tensorflow Lite Model Maker example Colab** ](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer) **without changing anything,** just running each cell one by one, and on the cell called \"**Customize the TensorFlow Model**\" I get out of memory error.\r\n\r\nIs it relevant who runs the example or on what machine etc. if I just run it in Google Colab?", "I didn't provide enough information in this issue, so I close this and provided the full information here: https://github.com/tensorflow/tensorflow/issues/45541"]}, {"number": 45467, "title": "TFlite fails to run on GPU with - ERROR: TfLiteGpuDelegate Prepare: No shader implementation for reduce_sum", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\ntf-nightly from source \r\n- TensorFlow version (or github SHA if from source):\r\n2.4 nightly\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nnon conversion issue but run time on mobile GPU\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Interpreter::UseNNAPI() is deprecated. Use tflite::NnApiDelegate() directly instead.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: TfLiteGpuDelegate Prepare: No shader implementation for reduce_sum\r\nERROR: Node number 51 (TfLiteGpuDelegate) failed to prepare.\r\n\r\ntflite model attatched.\r\n[MyModel2.zip](https://github.com/tensorflow/tensorflow/files/5657747/MyModel2.zip)\r\n\r\n", "comments": ["@shlomi-amitai \r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@shlomi-amitai Can you share your inference code? The logs mention both NNAPI & GPU (TFLite usually works best with only one of the two delegates).\r\nThe \"No shader implementation\" log line is probably a bug. CC'ing @impjdi who might know more.", "I couldn't spy which op triggers reduce_sum.  Can you pinpoint?\r\n\r\nAlso, the attached model is displaying something like:\r\n\r\n> 18 operations will run on the GPU, and the remaining 108 operations will run on the CPU.\r\n\r\nIt's possible that I'm using the wrong backend, but can you confirm that the attached model is a correct one?", "@shlomi-amitai Could you please try on latest stable version of tf 2.5 and let us know if this is still an issue.Thanks!", "@shlomi-amitai Is this still relevant?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45467\">No</a>\n"]}, {"number": 45466, "title": "Add op to convert numeric tensor to string faster", "body": "When we want to convert a tensor of numeric value to string type, there is `tf.as_string` available for us. However, its performance will become very poor along with the input tensor swelling, which is caused due to its implement by vsnprintf in my opinion. vsnprintf indeed provides a quite rich formatting features but it turns into a encumbrance for the case that we only want to convert number to string.\r\n\r\nTherefore, I proposed this pr trying to provide a high-performance and simple conversion method from numeric value to string type. **Performance improvement is the original intention of this pr**.\r\n\r\n### Benchmark\r\n\r\nI have made benchmarks against this new op and `tf.as_string` separately. For each method, I have tested both the case of float type and integer type, and the value of each element of input tensor is a random number from `-100,000` to `100,000`. For each input type, I have set up multiple gradually increasing inputs, the first of which is to measure the fixed overhead of the operation itself. In addition to that, each testing case is repeated 20 times and the average is taken as the result. \r\n\r\n*System environment: Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz*\r\n\r\nThe following benchmarks result (ms) is for input of float type:\r\n\r\n| tensor shape     | tf.number_to_string | tf.as_string        |\r\n| ---------------- | ------------------- | ------------------- |\r\n| (0)              | 0.453  | 0.359  |\r\n| (10,10)          | 0.458  | 0.403 |\r\n| (100,100)        | 1.413   | 1.705  |\r\n| (1,000, 100)     | 10.764    | 14.164  |\r\n| (10,000, 100)    | 314.907  | 1025.248  |\r\n| (100,000, 100)   | 2904.230  | 10173.129  |\r\n| (1,000,000, 100) | 28880.552  |  101203.889  |\r\n\r\n\r\nThe following benchmarks result (ms) is for input of integer type:\r\n\r\n| tensor shape     | tf.number_to_string | tf.as_string       |\r\n| ---------------- | ------------------- | ------------------ |\r\n| (0)              | 0.437  | 0.382 |\r\n| (10,10)          | 0.438  | 0.389 |\r\n| (100,100)        | 1.427  | 1.429 |\r\n| (1,000, 100)     | 9.590   | 9.814  |\r\n| (10,000, 100)    | 225.927  | 342.106   |\r\n| (100,000, 100)   | 2119.049   | 3255.322  |\r\n| (1,000,000, 100) | 21059.845   | 32273.443 |\r\n\r\nFrom the test results, we can see that since the fixed overhead of `tf.number_to_string` is slightly larger than that of `tf.as_string`, when the input size is small, the performance of former is slightly lower (but personally this should be an acceptable gap, right?). **Along with the input size becoming larger (maybe `(100,100)` is the watershed), `tf.number_to_string` gradually shows a quite significant advantage in performance**. Among them, the performance of integer conversion has been improved x1.5, and x3.5 for floating point conversion.\r\n\r\nRelated discussion #45406 . Could you please have a look? @vnvo2409", "comments": ["> Thank you for your PR. I am not a maintainer so let's wait for someone else to review this PR. In the meantime, in order to convince the maintainers, you should:\r\n> \r\n> * add a benchmark between `tf.as_string` and `tf.number_to_string`.\r\n> * add some tests ( probably not much since `fmt` are already well-tested but at least some simple tests. )\r\n> * suggest some benefits for the codebase. e.g if we replace `tf.as_string` with `tf.number_to_string` in a file or in a model, the performance will increase a lot.\r\n\r\n@vnvo2409 Thanks for your valuable suggestions. Sorry for the vagueness of my description. I have reorganized the description of pr and added performance benchmarks [here](https://github.com/tensorflow/tensorflow/pull/45466#issue-534189832).\r\n", "@firejq  Can you please check @mdanatg's comments and keep us posted ? Thanks!", "> Can you please check @mdanatg's comments and keep us posted ? Thanks!\r\n\r\n@gbaned Sorry for the late reply. In accordance with the amendments proposed by @mihaimaruseac [here](https://github.com/tensorflow/tensorflow/pull/45466#discussion_r539530149), I need to make considerable changes to the existing implementation to get rid of the dependence on third-party libraries `fmt`. As I have other tighter work on hand recently, I am going to close this pr first. Thanks again to all reviewers and if there are new developments in the future I will reopen it."]}, {"number": 45464, "title": "Build for TARGET_ARCH=fusion_f1 via reference implementation fallback.", "body": "This change adds reference fallbacks to the optimized xtensa kernels for the case when TARGET_ARCH is anything other than hifimini.\r\n\r\nThis sets the stage for a baseline from which we can incrementally optimize for architectures other than hifimini.\r\n\r\nThe goal is to have a starting point where all the unit tests pass for `TARGET_ARCH=hifimini` (which will use the optimized implementations) or any other `TARGET_ARCH` (with reference fallback).\r\n\r\nTested for `TARGET_ARCH=fusion_f1` with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=Google_F1 test\r\n```\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=Google_F1 test_keyword_benchmark\r\n\r\nInitializeKeywordRunner() took 239061 ticks (239 ms)\r\nKeywordRunNIerations(1) took 168564 ticks (168 ms)\r\nKeywordRunNIerations(10) took 1685111 ticks (1685 ms)\r\n```\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=Google_F1 keyword_benchmark BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1/bin/keyword_benchmark\r\n\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  48256\t  40132\t  24952\t 113340\t  1babc\ttensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1/bin/keyword_benchmark\r\n```\r\n\r\nAfter this change, we can:\r\n * add a continuous build for Hifi4\r\n * add optimizations for Hifi4 on a per kernelbasis and keep profiling the impact of these optimizations on the keyword benchmark cycles and binary size.\r\n\r\nAlso tested that `TARGET_ARCH=hifimini` is unaffected:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_keyword_benchmark\r\n\r\nInitializeKeywordRunner() took 1392788 ticks (1392 ms)\r\nKeywordRunNIerations(1) took 89195 ticks (89 ms)\r\nKeywordRunNIerations(10) took 891509 ticks (891 ms)\r\n```\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  46080\t  40204\t  24952\t 111236\t  1b284\ttensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving", "Internal checks were failing (while the external build was ok) because there is an automatic clang-format step prior to the code being imported into the google codebase. And my original commit was missing clang-format and an associated header.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/45464/commits/d2fd64fcfb5b2a51a91d1073d6c686299577205f fixes the issue."]}, {"number": 45463, "title": "Cannot ./configure Tensorflow on Ubuntu 18.04 LTS with CUDA 11.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: latest\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.7.1 \r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11.1/8\r\n- GPU model and memory: RTX TITAN (DUAL) 24 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nAttempting to configure tensorflow for using the ./compile script.\r\nCUDA is not found \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure \r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.6/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: y\r\nROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nCould not find any NvInferVersion.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\n        'local/cuda/extras/CUPTI/include'\r\nof:\r\n        '/lib'\r\n        '/lib/i386-linux-gnu'\r\n        '/lib/x86_64-linux-gnu'\r\n        '/lib32'\r\n        '/opt/ont/guppy/lib'\r\n        '/usr'\r\n        '/usr/lib'\r\n        '/usr/lib/i386-linux-gnu'\r\n        '/usr/lib/x86_64-linux-gnu'\r\n        '/usr/lib/x86_64-linux-gnu/libfakeroot'\r\n        '/usr/lib32'\r\n        '/usr/local/cuda'\r\n        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.1\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8\r\n\r\n\r\nPlease specify the TensorRT version you want to use. [Leave empty to default to TensorRT 6]: 7\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: \r\n\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /usr/local/cuda-11.1/targets/x86_64-linux/lib,/usr/local/cuda-11.1/targets/x86_64-linux/include,/usr/lib/x86_64-linux-gnu,/usr/include,/usr/local/cuda-11.1/bin/,/usr/local/cuda-11.1/nvvm/libdevice/              \r\n\r\nCould not find any libdevice*.10.bc in any subdirectory:\r\n        'nvvm/libdevice'\r\n        'share/cuda'\r\n        'lib/nvidia-cuda-toolkit/libdevice'\r\n        'local/cuda/nvvm/libdevice'\r\nof:\r\n        '/usr/include'\r\n        '/usr/lib/x86_64-linux-gnu'\r\n        '/usr/local/cuda-11.1/bin/'\r\n        '/usr/local/cuda-11.1/nvvm/libdevice/'\r\n        '/usr/local/cuda-11.1/targets/x86_64-linux/include'\r\n        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@cement-head \r\nCan you please try with CUDA 11.0? TF 2.4 (and nightly) is built and tested against CUDA 11.0, not 11.1.\r\nPlease refer to this issue and let us know : #43629", "Okay - this will take some time.", "Okay - same error with CUDA 11.0 and TensorFlow 2.5 (pulled from GitHub):\r\n\r\n```\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /usr/local/cuda-11.0/targets/x86_64-linux/lib,/usr/local/cuda-11.0/targets/x86_64-linux/include,/usr/lib/x86_64-linux-gnu,/usr/include,/usr/local/cuda-11.0/bin/,/usr/local/cuda-11.0/nvvm/libdevice/\r\n\r\n\r\nCould not find any libdevice*.10.bc in any subdirectory:\r\n        'nvvm/libdevice'\r\n        'share/cuda'\r\n        'lib/nvidia-cuda-toolkit/libdevice'\r\n        'local/cuda/nvvm/libdevice'\r\nof:\r\n        '/usr/include'\r\n        '/usr/lib/x86_64-linux-gnu'\r\n        '/usr/local/cuda-11.0/bin/'\r\n        '/usr/local/cuda-11.0/nvvm/libdevice/'\r\n        '/usr/local/cuda-11.0/targets/x86_64-linux/include'\r\n        '/usr/local/cuda-11.0/targets/x86_64-linux/lib'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:\r\n```\r\nJust to be explicit; the file is there: \r\n![Screenshot from 2020-12-08 09-59-19](https://user-images.githubusercontent.com/1056700/101500451-81986000-393c-11eb-86ce-f9d519b9ba86.png)\r\n\r\n", "BTW; Tensorflow is not finding the install CUDA, nor the cuDNN install automagically, which is in the path on this machine - all other CUDA functions are okay (on Ubuntu 18.04).\r\n\r\nThis does not appear to be a CUDA 11.0 vs 11.1 error/issue.", "I cannot reproduce this issue. Running Ubuntu 18.04 with CUDA 11.0 or 11.1, and they are detected just fine during `.configure`. It seems there is an issue with the submitter setup of CUDA.\r\n\r\nIs CUDA recognized using TF 2.4?", "Okay, turns out that TensorRT is already pre-built in the repos; I just installed that!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45463\">No</a>\n"]}, {"number": 45462, "title": "Pip install issue on windows 10 with error ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jc_mr.TSE-DEV64-2\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python38\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\aws\\\\aws-cpp-sdk-core\\\\include\\\\aws\\\\core\\\\client\\\\SpecifiedRetryableErrorsRetryStrategy.h'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please fill issue template", "Issue closed. Was due to a windows file long path management not activated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45462\">No</a>\n", "ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\mkays\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python38\\\\site-packages\\\\caffe2\\\\python\\\\serialized_test\\\\data\\\\operator_test\\\\collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip'\r\n\r\n\r\nGot this error while installing torch", "See following post to enable Long path in Windows : https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing#:~:text=Hit%20the%20Windows%20key%2C%20type,paths%20option%20and%20enable%20it."]}, {"number": 45461, "title": " Fixed error in the example of image_gradients documentation", "body": "image_gradients returns (dy, dx) rather than (dx, dy)", "comments": []}, {"number": 45460, "title": "Cannot convert model: Exception has occurred: ValueError", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): built from source\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\nHere you can find a **tf-trt, tflite converter** issue with model train code and build model - [Collab Notebook](https://colab.research.google.com/drive/1-dRG9O2B_NfvjmQz9du5A0oRQJaIC6dH?usp=sharing) \r\nHere is the converter error\r\n```\r\nInvalidArgumentError: Input 1 of node functional_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall was passed float from Func/functional_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/input/_1510:0 incompatible with expected resource.\r\n```\r\n**Also, please include a link to the saved model or GraphDef**\r\nHere is the link to the saved_model also - [saved_model.zip](https://drive.google.com/file/d/10olY_D9HcSOjDS6Ufrrd2Ci6HA5kRW4D/view?usp=sharing)\r\n\r\n**Failure details**\r\nTrying to convert model to fp16 using tflite or with tf-trt. \r\nTensorRT - version 7.2.1.6\r\nCuda - 11.1\r\nGPU - Nvidia GeForce 1060 6GB\r\nTensorflow as mentioned before - version 2.3.0 source build.\r\nAlso tried converting from saved_model directly but the problem is similar.\r\n\r\nCan you help me with issues please? As I see, there is a problem somewhere in Input but I don't understand where.\r\nBigger priority here is to use TF-TRT, but TF-Lite is ok too.\r\nAlso could you please clarify which issues, bottleneck I can meet with this mix of tf, tf-text, tflite, tf-trt?", "comments": ["@Ecclesiast Could you try conversion with V2 converter API, from_saved_model?", "If you mean \r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('MY-SAVED-MODEL-PATH')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\ntflite_model = converter.convert()\r\n```\r\nthen yes. I mentioned above, that it doesn't work. The error is a bit different, but I consider the main idea is the same.\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n    496         results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 497             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    498         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n\r\nInvalidArgumentError: Input 1 of node StatefulPartitionedCall/functional_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/functional_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/input/_1009:0 incompatible with expected resource.\r\n\r\nValueError: Input 1 of node StatefulPartitionedCall/functional_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/functional_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/input/_1009:0 incompatible with expected resource.\r\n```", "Was able to reproduce the issue with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/10da08af79ac01ee6d8b37460212db9c/45460.ipynb#scrollTo=AS3Lu3vY6fgz).\r\n\r\nHowever with TF v2.2 and TF-nightly, importing `tensorflow_text` throws an error stating\r\n```\r\nNotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow_text/python/metrics/_text_similarity_metric_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n```\r\nThanks!", "@Ecclesiast\r\n\r\nI tried to import the attached saved model through TensorFlow saved model loader. \r\n\r\ntensorflow/python/training/py_checkpoint_reader.py, in get_tensor\r\n    self, compat.as_bytes(tensor_str))\r\nIndexError: Requested 411640 bytes but only read 0.\r\n\r\nThe attached saved model may be not loadable because the checkpoint path is hard-coded somewhere in the saved model.\r\n\r\n```\r\nloaded = tf.saved_model.load(\"... saved model path ...\")\r\n```\r\n", "@amahendrakar tensorflow_text should be the same version as tf. Something with ABI, as I know. So if you are installing tf 2.2, then try to install tensorflow_text==2.2.0.", "@abattery I don't know actually what's the problem with this model, maybe something with compressing. \r\nPlease, go to attached collab notebook. Last cells contain trainer class and training code for one iteration. \r\nYou can execute them and save as a model.\r\nUPD: also there is a link to a folder with the same model I've tested - [Link](https://drive.google.com/drive/folders/1uiYkF_Fkyn7hoq9wwCqhWl9OWxZkJsp5?usp=sharing)", "@Ecclesiast\r\n\r\nI was able to convert the trainer in the colab notebook to the corresponding TFLite model via TFLite converter without any errors. I used tf-nightly version.", "@Ecclesiast\r\n\r\nI  would recommend training your model with the recent version of TF and TF text library and then convert your model with the same TF and TF text runtime. There were some updates for better supporting TF text library in TFLite in the recent version.", "@abattery thank you for your answer. Could you please specify which versions of these libraries I should use to convert models without errors?\r\nAlso is it a good solution to install tf-nightly side by side with tensorflow source build? As I remember, removing tf-nightly in future is a pain in the neck. And also tf-nightly as I understand is a developer builds of tf?\r\nWhen can I count on unbuggy tf, tf_text, tf_lite versions?", "@Ecclesiast I would say that there is a tf 2.4 released binaries. Could you use that version for your case?", "@abattery There is no version I've tested on Collab that resolves this issue. I've tried different versions of tf-nightly(2.5.0.dev20201210, 2.4.0.dev20200902).\r\nAlso tried tensorflow==2.4.0rc1 and tensorflow_text==2.4.0rc1 - nothing works fine. \r\nAlso this is important as the last stable release versions are 2.3.0 for both libraries and tensorflow-text is clarifying that versions of these libraries should be the same.\r\nIf you say, that everything is working fine for you - could you please share your tensorflow, tensorflow-text, tf-nightly versions?", "What I succeeded is the following conversion procedure. However, the shared saved model are based on version 2.2 so I would recommend you to create a new saved model based on tf 2.4.0 rc version.\r\n\r\n```\r\n!pip install tensorflow==v2.4.0-rc4\r\n!pip install tensorflow_text==2.4.0-rc1\r\n\r\ncls_trainer = ClassificationTrainer()\r\n\r\ntrain_dataset, valid_dataset = cls_trainer.convert2encoded()\r\n\r\ncls_trainer.train(train_dataset, valid_dataset)\r\ncls_trainer.save()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('PATH_TO_MODEL')\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n  tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nmodel = converter.convert()\r\n```\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45460\">No</a>\n"]}, {"number": 45458, "title": "tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda 4.9.1\r\n- Bazel version (if compiling from source): --\r\n- GCC/Compiler version (if compiling from source): --\r\n- CUDA/cuDNN version: cuda 10.1 / cudnn 7.6.5\r\n- GPU model and memory: Geforce GTX 1650Ti, 4 GB, 7.5 capability \r\n\r\n\r\n\r\n**Describe the problem**\r\nApplication outputs an error message (tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]). I have tensorflow-gpu 2.1.0 installed via conda. This error happened first when installing tensorflow via conda, and after I read other issues, some suggested installing via pip because of possible mismatch between conda installed cuda and cudnn. However, even with pip-installed tensorflow, I am still receiving this error. Error is persistent also for tensorflow 2.2.0. \r\n\r\nI have tried different version of cudnn (7.6.0, for cuda 10.1) but the error persists. I have system install of cuda and cudnn, and have set the environment variables as per [tensorflow guide](https://www.tensorflow.org/install/gpu).\r\n\r\n**Any other info / logs**\r\n`2020-12-09 18:25:30.256782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-12-09 18:25:31.215117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 Ti computeCapability: 7.5\r\ncoreClock: 1.485GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\r\n2020-12-09 18:25:31.215279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-09 18:25:31.218433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-09 18:25:31.220939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-09 18:25:31.221819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-09 18:25:31.225000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-09 18:25:31.227314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-09 18:25:31.240284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-09 18:25:31.240428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-12-09 18:25:31.240721: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-12-09 18:25:31.247163: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a1643df6a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-09 18:25:31.247314: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-09 18:25:31.247518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 Ti computeCapability: 7.5\r\ncoreClock: 1.485GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\r\n2020-12-09 18:25:31.247692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-09 18:25:31.247812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-09 18:25:31.247930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-09 18:25:31.248007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-09 18:25:31.248105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-09 18:25:31.248187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-09 18:25:31.248283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-09 18:25:31.248447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-12-09 18:25:31.669306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-09 18:25:31.669402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-12-09 18:25:31.669453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-12-09 18:25:31.669628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2913 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-09 18:25:31.672216: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a101d2a3a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-12-09 18:25:31.672324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1650 Ti, Compute Capability 7.5\r\n2020-12-09 18:25:33.789645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-09 18:25:34.661555: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-12-09 18:25:34.662100: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\nTraceback (most recent call last):\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 926, in conv2d\r\n    \"dilations\", dilations)\r\ntensorflow.python.eager.core._FallbackException: Expecting int64_t value for attr strides, got numpy.int32\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"A:/projekti/diplomski/train.py\", line 61, in <module>\r\n    run_train(yolo_model, num_epochs=2)\r\n  File \"A:/projekti/diplomski/train.py\", line 39, in run_train\r\n    y_preds = model(x_batch)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 968, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 719, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 888, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 968, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 207, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1106, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 638, in __call__\r\n    return self.call(inp, filter)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 237, in __call__\r\n    name=self.name)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 2014, in conv2d\r\n    name=name)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 933, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1022, in conv2d_eager_fallback\r\n    ctx=ctx, name=name)\r\n  File \"A:\\ProgramFiles\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]`\r\n\r\nThis is how i use convolution in code: \r\n\r\n`  \r\n\r\n    def conv_norm_lrelu(inputs, filters, kernel_size, stride=1, training=True, **conv_kwargs):\r\n\r\n        stride = tf.cast(stride, tf.int64)\r\n\r\n        conv = tf.keras.layers.Conv2D(filters, kernel_size, strides=(stride, stride), padding='same', **conv_kwargs)(inputs)\r\n\r\n        bn = tf.keras.layers.BatchNormalization(trainable=training)(conv)\r\n\r\n        return tf.keras.layers.LeakyReLU(alpha=0.1)(bn)\r\n\r\n`", "comments": ["We don't support ananconda here. Please use the default pip install in venv or try to get support at https://github.com/AnacondaRecipes/tensorflow_recipes", "@bhack If you read my post, you will see that I tried both pip and anaconda install ", "But pip inside a conda env?", "> But pip inside a conda env?\r\n\r\nYes, is that a problem? ", "Here we don't officially support conda also as env. It could work but It Is hard to get support in this repo for a conda setup.\n\nPlease try to make a clean install as in the official guide:\nhttps://www.tensorflow.org/install/pip", "The issue is that another application uses the cudnn services. So you cant run two instances of training and using the network because its using it. If you are running lc0 in the background it could be it. Try to close it", "So it's not a \"build/install\" because conventional layers are already implemented in cudnn. they use it without modifications so if another app uses it they cant.\r\n\r\nClose the issue I guess", "@wanusic \r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using.\r\n\r\nAlso, please go through issue #24828 with a similar error and let us know if it helps. Thanks!", "> @wanusic\r\n> \r\n> In order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using.\r\n> \r\n> Also, please go through issue #24828 with a similar error and let us know if it helps. Thanks!\r\n\r\nI have updated the stack trace. \r\n\r\nPossible issues:\r\n-  Incompatibility between conda installed version of cuda and cudnn. I have removed these from anaconda, so I only have system wide install. Also conda list cuda, and conda list cudnn returns empty. \r\n\r\n-  Tensorflow GPU guide uses cudnn 7.6.x version, i have tried both highest (7.6.5) and lowest (7.6.0) while still supported by cuda \r\n    10.1 (Some people in #24828 mentioned downgrading or upgrading cudnn have helped, but none seem to work for me)\r\n\r\n- Using config.gpu_options.allow_growth=True. I am using keras and tf.compat.v1.keras.backend.set_session doesn't take any arguments except session. I am not sure how to use this since I am not explicitly creating a tf.Session in my code (using functional api in keras). \r\n\r\n- I also have this error:  Expecting int64_t value for attr strides, got numpy.int32 which i don't know how to get rid of. Tried casting the stride value to tf.int64 but it didn't work. I am not sure if these are related. ", "In the end I think this is a memory issue. I tried using config.gpu_options.per_process_gpu_memory_fraction = 0.4 which eliminates the cudnn issue. I think that cudnn tries to allocate the memory for the entire neural network at once which exceeds limitations of my memory so cudnn cannot allocate handle. Still dynamically allocating memory does not work because it will still exceed memory limitation. In my case, I need a smaller network or more memory :(", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45458\">No</a>\n"]}, {"number": 45457, "title": "micro: copy operator FILL kernel from lite", "body": "This is a copy without modification of the kernel and test for\r\noperator FILL from tensorflow/lite/kernels at 67993f46cb9.\r\nAdaptations to micro and addition to the micro build to follow.\r\n\r\nThis is part of the work to port operator FILL as tracked in #45306.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45456, "title": "[ROCm] Removing --action_env=ROCM_ROOT... when building with --config=rocm", "body": "When we switched to ROCm 3.3 (first ROCm release with \"relocatable ROCm install\" support), some parts of the ROCm toolchain relied upon the value of env var ROCM_ROOT to determine the location of the ROCm install dir. This has been fixed, and all of the ROCM toolchain now only uses the ROCM_PATH value.\r\n\r\nThis commit updates `configure.py` to no longer set ROCM_ROOT, when building with --config=rocm\r\n\r\n-------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work \r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping"]}, {"number": 45454, "title": "keras model to a single pb file or valid graph", "body": "\r\nI use TensorFlow 2.3.1\r\n\r\ncurrently, you cant export a Keras model to a pb file without a folder that is a valid graph to use in the c++ api.\r\n\r\nI don't think this will change the api. just a feature for making a valid graph for the c++ api.\r\n\r\nI think that labs that work with c++ and other languages that use the c_api.h file for wrapping for the language (maybe python uses it too?) with tensorflow because that is what they've worked with.\r\n\r\nthank you, Im sorry if I just cant find it. I use the cppflow wrapper. is it that? can you assist me if its not your own and it *is* valid?\r\n", "comments": ["I tried now using the tutorial here https://medium.com/@sebastingarcaacosta/how-to-export-a-tensorflow-2-x-keras-model-to-a-frozen-and-optimized-graph-39740846d9eb. \r\nit does not say that its not a valid graph but it makes a problem. Exception thrown at 0x00007FFAF05B1F70 (tensorflow.dll) in a0xa.exe: 0xC0000005: Access violation reading location 0x0000000000000090. this is still in the properties of tensorflow because its your dll."]}, {"number": 45453, "title": "Suspected memory leak - when loading multiple models with tf.saved_model.load", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nbinary wheel via PyPI\r\n- TensorFlow version (use command below):\r\nv2.4.0-rc3-20-g97c3fef64ba 2.4.0-rc4\r\n- Python version:\r\n3.6\r\nBazel version (if compiling from source):\r\nNA\r\nGCC/Compiler version (if compiling from source):\r\nNA\r\n- CUDA/cuDNN version:\r\nCUDA 11.1\r\n\r\n- GPU model and memory:\r\nV100 16GB\r\n\r\n**Describe the current behavior**\r\nI'm suspecting a CPU memory leak when loading multiple models.\r\nWhen I'm running infinite loop that keeps loading the same model while using the same variable the memory (private bytes and working set) of the process keep increasing. At some points the working set seems to free some memory, but the trend is that the memory keeps on rising.\r\nFor example I used a simple model.\r\nFor our **current real** model the memory leak in tf 2.3 is 0.32MB per load model \r\nFor our **current real** model the memory leak in tf 2.4 is 0.08MB per load model it is still an issue since the model can be changed and our server serve different models 24/7 \r\n\r\nThis trend happens even though I call gc.collect() on every iteration and tf.keras.backend.clear_session().\r\n\r\n**Describe the expected behavior**\r\nThe memory shouldnt increase on each interation\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport gc\r\n\r\n\r\ndef build_and_save_own_model():\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dropout(0.2),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.save('my_model')\r\n    tf.keras.backend.clear_session()\r\n    del model\r\n    gc.collect()\r\n\r\n\r\ndef profile_load_model(path):\r\n    model = tf.saved_model.load(path)\r\n    tf.keras.backend.clear_session()\r\n    del model\r\n    gc.collect()\r\n\r\n\r\ndef run_model():\r\n    model_path = 'my_model'\r\n    build_and_save_own_model()\r\n    print(\"load model in loops:\")\r\n    c = 1\r\n    while True:\r\n        print(\"----------- iter\", c)\r\n        profile_load_model(model_path)\r\n        c += 1\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"*****************************************************\")\r\n    print(\"START LOADING MODEL\")\r\n    print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n    print(\"*****************************************************\")\r\n    run_model()\r\n```\r\n\r\n\r\nTF2_3 memory leak:\r\n![memleak_tf2_3](https://user-images.githubusercontent.com/27951762/101373173-5585cc00-38b5-11eb-8405-124b885033ab.PNG)\r\n\r\n\r\nTF2_4 memory leak:\r\n![memleak_tf2_4](https://user-images.githubusercontent.com/27951762/101373017-2bcca500-38b5-11eb-9b22-cc652036b455.PNG)\r\n\r\nthanks\r\n", "comments": ["@farotem \r\n\r\nCan you please share colab link or complete code to trace memory  allocations. Thanks!", "@ravikyram \r\nthe bug is for TF 2.3 & TF2.4 nightly \r\nhttps://colab.research.google.com/drive/1Vlc00zyfMEZ_8vXPN9v2_dp6eZ3K3NDz?usp=sharing\r\n", "@farotem \r\n\r\nPlease, grant me the access for the colab link. Thanks!", "please try again", "@farotem \r\n\r\nI tried in colab with TF 2.4, nightly version (`2.5.0-dev20201209`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/99ba4449987b586936b065b278d063ce/untitled572.ipynb).You are also seeing the same behavior?\r\n\r\nThanks!\r\n", "yes thank you, as you can see you started at 380.1 MiB\r\nfinished at 388.3 MiB \r\nafter 1024 iterations.", "@farotem Similar behavior was noticed with `tf.keras.models.load_model` also.\r\n```\r\n# model = tf.saved_model.load(path)\r\n    model = tf.keras.models.load_model(path)\r\n```\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/6f7d9fc4034138037ef0945e00fbfad3/untitled572.ipynb) is a gist for our reference. Thanks!", "Hi, I am not really sure based on my own experiments -\r\nis this memory leak still an issue with TF 2.4?", "@haimat I checked again with tf 2.4.1 and unfortunately still had the same memory leak.\r\nYou can look at the gist jvishnuvardhan  posted", "@haimat @farotem I just checked with `tf-nightly`. I don't see much leak. Please check the [gist](https://colab.research.google.com/gist/jvishnuvardhan/103a8bd18e46f988cef12a1ec9af4346/untitled572.ipynb). \r\n\r\nCan you please run the gist and tell me if you see any memory leak.\r\n\r\nI changed only one line as shown below\r\n\r\n```\r\n#model = tf.saved_model.load(path)\r\n    model = tf.keras.models.load_model(path)\r\n```\r\n\r\nPlease let me know how it progresses. Thanks!", "@haimat @farotem Please take a look at [similar issue](https://github.com/tensorflow/tensorflow/issues/40171) that was resolved recently. Thanks.\r\n\r\nPlease verify once and close the issue. Thanks!\r\n", "@jvishnuvardhan Looks good to me, thanks.\r\nBut I have not opened this issue, so it's not up to me to close it :)", "@farotem I think this was resolved in `tf-nightly`. I am closing this issue. \r\nPlease feel free to reopen if I am mistaken. Thanks!", "Thanks @jvishnuvardhan looks good!", "I have memory leaks cloning a loaded model with\r\n```\r\nmodel_copy =  tf.keras.models.clone_model(model)\r\nmodel_copy.build()\r\nmodel_copy.set_weights(model.get_weights())\r\n```\r\nusing Tensorflow 2.3.1. Does this fix affect the cloning as well?", "@OliverPfau Can you please open a new issue with a simple standalone code to reproduce your issue? Thanks! In the new issue, you could reference this issue. Thanks!", "Hi @jvishnuvardhan, so this issue has been fixed after tf2.5 version, is right?"]}, {"number": 45452, "title": "tflite quantized model unable to run on DSP", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\nBinary \r\n- Tensorflow version (commit SHA if source):\r\ntf-nightly \r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\nAndroid\r\n**Describe the problem**\r\nModel with full quantization. \r\nI get the following message:\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Interpreter::UseNNAPI() is deprecated. Use tflite::NnApiDelegate() directly instead.\r\nINFO: Hexagon delegate: 0 nodes delegated out of 51 nodes with 0 partitions.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nConverter configuration:\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT] # dynamic range quantization\r\nconverter.representative_dataset = representative_data_gen  # + float fallback quantization\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # +integer only quantization\r\nconverter.inference_input_type = tf.int8 # +integer only quantization\r\nconverter.inference_output_type  = tf.int8    \r\n\r\n\r\ntflite model is attatched\r\n[myModel.zip](https://github.com/tensorflow/tensorflow/files/5653707/myModel.zip)\r\n\r\n\r\n\r\n", "comments": ["Hello @shlomi-amitai is this same issue as\r\nhttps://github.com/tensorflow/tensorflow/issues/45902 \r\n\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45452\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45452\">No</a>\n"]}, {"number": 45451, "title": "tensorflow_estimator-2.4.0rc0.dist-info is missing METADATA file", "body": "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\users\\\\USER\\\\anaconda3\\\\envs\\\\tf-gpu-latest\\\\lib\\\\site-packages\\\\tensorflow_estimator-2.4.0rc0.dist-info\\\\METADATA'\r\n\r\nOnly the INSTALLER file is in the directory. \r\n\r\nEdit:\r\nReinstall it in a new environment and it now has those files, something must have gone wrong during pip installation. ", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45451\">No</a>\n"]}, {"number": 45450, "title": "all CUDA-capable devices are busy or unavailable", "body": "\r\n\r\n(tfv1) [root@bogon ~]# cat /etc/redhat-release\r\nCentOS Linux release 7.9.2009 (Core)\r\n\r\n\r\n\r\ntfv1) [root@bogon ~]# nvidia-smi\r\nMon Dec  7 22:21:25 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            Off  | 00000000:00:0C.0 Off |                    0 |\r\n| N/A   44C    P0    15W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n\r\n2020-12-07 22:03:48.390261: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.9.0\r\nTraceback (most recent call last):\r\n  File \"cpu.py\", line 91, in <module>\r\n    with tf.Session() as sess:\r\n  File \"/root/anaconda3/envs/tfv1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1570, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/root/anaconda3/envs/tfv1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 693, in __init__\r\n    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable\r\n\r\n\r\n\r\nThe CPU version of tensorflow can execute successfully, but GPU version of tensorflow fails. Why\uff1f\r\ntensorflow version \uff1a\r\n           conda install tensorflow=1.14.0=gpu_py37hae64822_0\r\n", "comments": ["@menkeyi \r\n\r\nPlease, try with the tested build configurations from [here](https://www.tensorflow.org/install/source#gpu). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45450\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45450\">No</a>\n"]}, {"number": 45449, "title": "Segmenation fault when saving StringLookup layer", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- MacOS \r\n- TensorFlow installed from binary\r\n- TensorFlow version: '2.3.1' \r\n- Python version: 3.7\r\n\r\n**Current behavior**\r\nA Model with just input and StringLookup layers is failing to save after loading it twice. It gives a segmentation fault.\r\n**Expected behavior**\r\nSaving without having a Segmentation fault\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nlookup = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=[\"a\", \"b\"])\r\n\r\ns = tf.keras.Input(1, dtype=tf.string)\r\nk = lookup(s)\r\n\r\nmodel = tf.keras.Model(s, k)\r\nmodel.compile()\r\n\r\ntf.keras.models.save_model(model, \"./out/dummy/\")\r\n\r\nload2 = tf.keras.models.load_model(\"./out/dummy/\")\r\ntf.keras.models.save_model(load2, \"./out/dummy2/\")\r\n\r\nload3 = tf.keras.models.load_model(\"./out/dummy2/\")\r\ntf.keras.models.save_model(load3, \"./out/dummy3/\")\r\n```\r\n", "comments": ["@moaradwan @mihaimaruseac,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/0ee0ef20c66aac2055b91a1924e77dcf/45449.ipynb#scrollTo=UWeRKkyRC2p9).\r\n\r\nHowever, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/1d88cbe6ececd4b5ee31b54d47a3374e/45449-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "It is likely this got fixed by my recent changes for a security vulnerability.\r\n\r\nCan you try testing with a nightly from November?", "> Can you try testing with a nightly from November?\r\n\r\n@mihaimaruseac,\r\nFails with nightly builds from November, until v2.5.0.dev20201207 ([gist](https://colab.research.google.com/gist/amahendrakar/0d2c5ba4fc5e547462fa888da5974fc0/45449-2-5-0-dev20201207.ipynb) for reference).\r\n\r\nIt was fixed with the 2.5.0.dev20201208 release.", "Awesome. Thanks for confirming. Then it will get fixed in the 2.4 release, it is due to a CVE I fixed recently.\r\n\r\nLet's keep this open until 2.4 gets released and then validate and close.", "@mihaimaruseac,\r\nThe issue persists with the stable version of TF v2.4 as well. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9735bdb13019780c8b26d680626c3008/45449-2-4.ipynb#scrollTo=UWeRKkyRC2p9). Thanks!", "Interesting. This means there is a different issue, I'll take a look.", "Have same issue when trying to load saved model to Hub KerasLayer (runned inside TestCase)\r\nTF 2.4.0\r\nHub 0.10.0\r\n\r\n```python\r\ntfwordvec/tests/test_hub.py::TestExportEncoders::test_char_cbow Fatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x000000010890bdc0 (most recent call first):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/handle_data_util.py\", line 32 in get_resource_handle_data\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/handle_data_util.py\", line 58 in copy_handle_data\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 592 in call\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1919 in _call_flat\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 895 in _call\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 828 in __call__\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 250 in resource_handle\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 395 in _get_tensor_from_node\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 339 in <listcomp>\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 339 in _setup_functions_captures\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 263 in _load_all\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 160 in __init__\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 890 in load_internal\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 859 in load\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module_v2.py\", line 106 in load\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\", line 423 in load_module\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\", line 152 in __init__\r\n  File \"/Volumes/HDD/Develop/semtech/tfwordvec/tfwordvec/tests/test_hub.py\", line 64 in test_char_cbow\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/unittest/case.py\", line 628 in run\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/unittest/case.py\", line 676 in __call__\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/unittest.py\", line 231 in runtest\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/runner.py\", line 135 in pytest_runtest_call\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/callers.py\", line 187 in _multicall\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 87 in <lambda>\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 93 in _hookexec\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/hooks.py\", line 286 in __call__\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/runner.py\", line 217 in <lambda>\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/runner.py\", line 244 in from_call\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/runner.py\", line 217 in call_runtest_hook\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/runner.py\", line 186 in call_and_report\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/runner.py\", line 100 in runtestprotocol\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/runner.py\", line 85 in pytest_runtest_protocol\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/callers.py\", line 187 in _multicall\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 87 in <lambda>\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 93 in _hookexec\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/hooks.py\", line 286 in __call__\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/main.py\", line 272 in pytest_runtestloop\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/callers.py\", line 187 in _multicall\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 87 in <lambda>\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 93 in _hookexec\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/hooks.py\", line 286 in __call__\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/main.py\", line 247 in _main\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/main.py\", line 191 in wrap_session\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/main.py\", line 240 in pytest_cmdline_main\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/callers.py\", line 187 in _multicall\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 87 in <lambda>\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/manager.py\", line 93 in _hookexec\r\n  File \"/usr/local/lib/python3.7/site-packages/pluggy/hooks.py\", line 286 in __call__\r\n  File \"/usr/local/lib/python3.7/site-packages/_pytest/config/__init__.py\", line 125 in main\r\n  File \"/usr/local/lib/python3.7/site-packages/ptr.py\", line 220 in run_tests\r\n  File \"/usr/local/lib/python3.7/site-packages/ptr.py\", line 209 in run\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/distutils/dist.py\", line 985 in run_command\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/distutils/dist.py\", line 966 in run_commands\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/distutils/core.py\", line 148 in setup\r\n  File \"/usr/local/lib/python3.7/site-packages/setuptools/__init__.py\", line 144 in setup\r\n  File \"setup.py\", line 55 in <module>\r\nzsh: segmentation fault  python3 setup.py test\r\n```\r\n\r\nI've added debug print `print(node_id, fn_name, obj)` in ` File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 395 in _get_tensor_from_node` and got such output:\r\n```python\r\n86 __inference__wrapped_model_2386 <tensorflow.python.ops.lookup_ops.MutableHashTable object at 0x14c5f3250>\r\n```", "That's a good indicator that the segfault here comes from a different place than originally thought. Thank you for the debug info.", "> The issue persists with the stable version of TF v2.4 as well. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9735bdb13019780c8b26d680626c3008/45449-2-4.ipynb#scrollTo=UWeRKkyRC2p9). Thanks!\r\n\r\nThe issue persists with fresh 2.4.1 release", "Thank you for checking. Will have to fully investigate and fix by 2.5 then, probably also backporting.", "@mihaimaruseac any updates about this?", "This no longer reproduces on master:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlookup = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=[\"a\", \"b\"])\r\n\r\ns = tf.keras.Input(1, dtype=tf.string)\r\nk = lookup(s)\r\n\r\nmodel = tf.keras.Model(s, k)\r\nmodel.compile()\r\n\r\ntf.keras.models.save_model(model, \"/tmp/dummy/\")\r\n\r\nload2 = tf.keras.models.load_model(\"/tmp/dummy/\")\r\ntf.keras.models.save_model(load2, \"/tmp/dummy2/\")\r\n\r\nload3 = tf.keras.models.load_model(\"/tmp/dummy2/\")\r\ntf.keras.models.save_model(load3, \"/tmp/dummy3/\")\r\n```\r\n\r\nAlso tried\r\n\r\n```python\r\nIn [14]: load = tf.keras.models.load_model(\"/tmp/dummy\")\r\nIn [15]: for i in range(1000):\r\n   ....:   tf.keras.models.save_model(load, f\"/tmp/x{i}\")\r\n   ....:   load = tf.keras.models.load_model(f\"/tmp/x{i}\")\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45449\">No</a>\n"]}, {"number": 45448, "title": "2.4.0-rc4: import keras error", "body": "This is likely to be a stupid question.\r\nI use an RTX 3090 and hence am very interested in the 2.4.0 version of Tensorflow.\r\n\r\nHowever, when I use the 2.4.0-rc4 and attempt to import keras...\r\n`from tensorflow import keras`\r\n\r\n... I get the error:\r\n```\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-9-256f42a18d91> in <module>\r\n     22 print('ok0')\r\n     23 \r\n---> 24 from tensorflow import keras\r\n     25 \r\n     26 print('ok1')\r\n\r\n~/anaconda3/envs/tsne/lib/python3.8/site-packages/tensorflow/keras/__init__.py in <module>\r\n     17 from . import callbacks\r\n     18 from . import constraints\r\n---> 19 from . import datasets\r\n     20 from . import estimator\r\n     21 from . import experimental\r\n\r\n~/anaconda3/envs/tsne/lib/python3.8/site-packages/tensorflow/keras/datasets/__init__.py in <module>\r\n     12 from . import cifar100\r\n     13 from . import fashion_mnist\r\n---> 14 from . import imdb\r\n     15 from . import mnist\r\n     16 from . import reuters\r\n\r\n~/anaconda3/envs/tsne/lib/python3.8/site-packages/tensorflow/keras/datasets/imdb/__init__.py in <module>\r\n      9 import sys as _sys\r\n     10 \r\n---> 11 from tensorflow.python.keras.datasets.imdb import get_word_index\r\n     12 from tensorflow.python.keras.datasets.imdb import load_data\r\n     13 \r\n\r\n~/anaconda3/envs/tsne/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py in <module>\r\n     23 import numpy as np\r\n     24 \r\n---> 25 from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n     26 from tensorflow.python.keras.utils.data_utils import get_file\r\n     27 from tensorflow.python.platform import tf_logging as logging\r\n\r\n~/anaconda3/envs/tsne/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/__init__.py in <module>\r\n     21 # TODO(mihaimaruseac): remove the import of keras_preprocessing and injecting\r\n     22 # once we update to latest version of keras_preprocessing\r\n---> 23 import keras_preprocessing\r\n     24 \r\n     25 from tensorflow.python.keras import backend\r\n\r\nModuleNotFoundError: No module named 'keras_preprocessing'\r\n```\r\n\r\nIs this an expected behaviour for this rc -- and I should have known that keras does not work with rc versions?\r\nIs there a simple way for me to get keras to work with 2.4.0-rc4?", "comments": ["We don't officially support conda here.\r\nSee if  this could help you https://stackoverflow.com/questions/64102020/modulenotfounderror-no-module-named-keras-preprocessing", "You may try https://stackoverflow.com/a/64104744/11127923", "@ymodak Is your different?", "My bad.. I didn't check your comment. I will retract. ", "Nevermind, thanks.", "Does it matter that I installed it with pip --just in a conda environment?", "See https://github.com/tensorflow/tensorflow/issues/45458#issuecomment-740199045", "Tested this in a virtualenv with `pip`-only and it works. So it seems that the fault lies in conda.", "Thank you, everyone!"]}, {"number": 45447, "title": "addv2", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@mihaibf \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share the colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "No template filled, closing"]}, {"number": 45445, "title": "Unable to succeed \"pip install tensorflow\" due to error \"Failed building wheel for grpcio\" in virtualenv", "body": "**System information**\r\n- OS Platform and Distribution Linux Ubuntu 16.04\r\n- TensorFlow installed from pip install\r\n- TensorFlow version:1.14.0\r\n- Installed using pip inside a virtual environment created using virtualenv command\r\n\r\n**When I was trying to install tensorflow in a virtual environment(created using `virtualenv` command, it is showing the following error at some point of installation:**\r\n```\r\nBuilding wheels for collected packages: grpcio\r\n  Running setup.py bdist_wheel for grpcio ... /\r\n\r\nFailed building wheel for grpcio\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nvirtualenv dd\r\nsource dd/bin/activate\r\npip install tensorflow\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/google/api/http.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/google/api/http.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/service/listener/v3/lds.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/service/listener/v3/lds.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python2.7 -c /tmp/tmpcOA4GA/a.c -o None/tmp/tmpcOA4GA/a.o\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/config/core/v3/address.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/config/core/v3/address.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/config/listener/v3/listener_components.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/config/listener/v3/listener_components.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/udpa/core/v1/resource_locator.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/udpa/core/v1/resource_locator.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/envoy/config/listener/v3/listener_components.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/envoy/config/listener/v3/listener_components.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/google/protobuf/any.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/google/protobuf/any.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv-fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/envoy/service/route/v3/rds.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/envoy/service/route/v3/rds.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/google/protobuf/descriptor.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/google/protobuf/descriptor.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/channel/connected_channel.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/channel/connected_channel.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/service/load_stats/v3/lrs.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/service/load_stats/v3/lrs.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/config/listener/v3/udp_listener_config.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/config/listener/v3/udp_listener_config.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/config/core/v3/backoff.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/config/core/v3/backoff.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/udpa/core/v1/resource_name.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/udpa/core/v1/resource_name.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/transport/inproc/inproc_plugin.cc -o python_build/temp.linux-x86_64-2.7/src/core/ext/transport/inproc/inproc_plugin.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/google/protobuf/descriptor.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/google/protobuf/descriptor.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/envoy/config/listener/v3/udp_listener_config.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/envoy/config/listener/v3/udp_listener_config.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-build-KkOhKw/grpcio/setup.py\", line 488, in <module>\r\n        cmdclass=COMMAND_CLASS,\r\n      File \"/usr/lib64/python2.7/distutils/core.py\", line 152, in setup\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/envoy/service/route/v3/srds.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/envoy/service/route/v3/srds.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n        dist.run_commands()\r\n      File \"/usr/lib64/python2.7/distutils/dist.py\", line 953, in run_commands\r\n        self.run_command(cmd)\r\n      File \"/usr/lib64/python2.7/distutils/dist.py\", line 972, in run_command\r\n        cmd_obj.run()\r\n      File \"/home/vagdevi4768/dd/lib/python2.7/site-packages/setuptools/command/install.py\", line 61, in run\r\n        return orig.install.run(self)\r\n      File \"/usr/lib64/python2.7/distutils/command/install.py\", line 563, in run\r\n        self.run_command('build')\r\n      File \"/usr/lib64/python2.7/distutils/cmd.py\", line 326, in run_command\r\n        self.distribution.run_command(command)\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n      File \"/usr/lib64/python2.7/distutils/dist.py\", line 972, in run_command\r\n        cmd_obj.run()\r\n      File \"/usr/lib64/python2.7/distutils/command/build.py\", line 127, in run\r\n        self.run_command(cmd_name)\r\n      File \"/usr/lib64/python2.7/distutils/cmd.py\", line 326, in run_command\r\n        self.distribution.run_command(command)\r\n      File \"/usr/lib64/python2.7/distutils/dist.py\", line 972, in run_command\r\n        cmd_obj.run()\r\n      File \"/home/vagdevi4768/dd/lib/python2.7/site-packages/setuptools/command/build_ext.py\", line 75, in run\r\n        _build_ext.run(self)\r\n      File \"/usr/lib64/python2.7/distutils/command/build_ext.py\", line 339, in run\r\n        self.build_extensions()\r\n      File \"/tmp/pip-build-KkOhKw/grpcio/src/python/grpcio/commands.py\", line 272, in build_extensions\r\n        \"Failed `build_ext` step:\\n{}\".format(formatted_exception))\r\n    commands.CommandError: Failed `build_ext` step:\r\n    Traceback (most recent call last):\r\n      File \"/tmp/pip-build-KkOhKw/grpcio/src/python/grpcio/commands.py\", line 267, in build_extensions\r\n        build_ext.build_ext.build_extensions(self)\r\n      File \"/usr/lib64/python2.7/distutils/command/build_ext.py\", line 448, in build_extensions\r\n        self.build_extension(ext)\r\n      File \"/home/vagdevi4768/dd/lib/python2.7/site-packages/setuptools/command/build_ext.py\", line 196, in build_extension\r\n        _build_ext.build_extension(self, ext)\r\n      File \"/usr/lib64/python2.7/distutils/command/build_ext.py\", line 498, in build_extension\r\n        depends=ext.depends)\r\n      File \"/tmp/pip-build-KkOhKw/grpcio/src/python/grpcio/_parallel_compile_patch.py\", line 59, in _parallel_compile\r\n        _compile_single_file, objects)\r\n      File \"/usr/lib64/python2.7/multiprocessing/pool.py\", line 250, in map\r\n        return self.map_async(func, iterable, chunksize).get()\r\n      File \"/usr/lib64/python2.7/multiprocessing/pool.py\", line 554, in get\r\n        raise self._value\r\n    CompileError: command 'gcc' failed with exit status 1\r\n\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/service/route/v3/rds.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/service/route/v3/rds.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/gpr/cpu_linux.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/gpr/cpu_linux.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/config/core/v3/base.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/config/core/v3/base.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/google/protobuf/duration.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/google/protobuf/duration.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/validate/validate.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/validate/validate.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/gpr/sync_abseil.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/gpr/sync_abseil.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/envoy/config/rbac/v3/rbac.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/envoy/config/rbac/v3/rbac.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upbdefs-generated/envoy/service/route/v3/srds.upbdefs.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upbdefs-generated/envoy/service/route/v3/srds.upbdefs.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/gpr/cpu_posix.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/gpr/cpu_posix.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/envoy/type/matcher/v3/metadata.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/envoy/type/matcher/v3/metadata.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/gprpp/mpscq.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/gprpp/mpscq.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/iomgr/endpoint_pair_uv.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/iomgr/endpoint_pair_uv.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/gpr/cpu_windows.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/gpr/cpu_windows.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/iomgr/gethostname_host_name_max.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/iomgr/gethostname_host_name_max.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/iomgr/poller/eventmanager_libuv.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/iomgr/poller/eventmanager_libuv.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/iomgr/endpoint_pair_windows.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/iomgr/endpoint_pair_windows.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/gpr/env_linux.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/gpr/env_linux.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/ext/upb-generated/envoy/config/route/v3/route.upb.c -o python_build/temp.linux-x86_64-2.7/src/core/ext/upb-generated/envoy/config/route/v3/route.upb.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/re2 -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Isrc/core/ext/upbdefs-generated -Ithird_party/zlib -I/usr/include/python2.7 -c src/core/lib/iomgr/gethostname_sysconf.cc -o python_build/temp.linux-x86_64-2.7/src/core/lib/iomgr/gethostname_sysconf.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    cc1: warning: command line option \u2018-std=c++11\u2019 is valid for C++/ObjC++ but not for C [enabled by default]\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    cc1plus: warning: command line option \u2018-std=gnu99\u2019 is valid for C/ObjC but not for C++ [enabled by default]\r\n    In file included from ./src/core/ext/filters/client_channel/service_config_parser.h:28:0,\r\n                     from ./src/core/ext/filters/message_size/message_size_filter.h:22,\r\n                     from src/core/ext/filters/message_size/message_size_filter.cc:19:\r\n    src/core/ext/filters/message_size/message_size_filter.cc: In function \u2018void recv_message_ready(void*, grpc_error*)\u2019:\r\n    src/core/ext/filters/message_size/message_size_filter.cc:207:56: error: no matching function for call to \u2018StrFormat(const char [45], uint32_t, int&)\u2019\r\n                                 calld->limits.max_recv_size)\r\n                                                            ^\r\n    ./src/core/lib/iomgr/error.h:158:71: note: in definition of macro\u2018GRPC_ERROR_CREATE_FROM_COPIED_STRING\u2019\r\n       grpc_error_create(__FILE__, __LINE__, grpc_slice_from_copied_string(desc), \\\r\n     ^\r\n    src/core/ext/filters/message_size/message_size_filter.cc:207:56: note: candidate is:\r\n                                 calld->limits.max_recv_size)\r\n                                                            ^\r\n    ./src/core/lib/iomgr/error.h:158:71: note: in definition of macro\u2018GRPC_ERROR_CREATE_FROM_COPIED_STRING\u2019\r\n       grpc_error_create(__FILE__, __LINE__, grpc_slice_from_copied_string(desc), \\\r\n     ^\r\n    In file included from src/core/ext/filters/message_size/message_size_filter.cc:24:0:\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note: template<class ... Args> std::string absl::lts_2020_09_23::StrFormat(absl::lts_2020_09_23::FormatSpec<Args ...>&, const Args& ...)\r\n     ABSL_MUST_USE_RESULT std::string StrFormat(const FormatSpec<Args...>& format,\r\n                                      ^\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note:   template argument deduction/substitution failed:\r\n    In file included from ./src/core/ext/filters/client_channel/service_config_parser.h:28:0,\r\n                     from ./src/core/ext/filters/message_size/message_size_filter.h:22,\r\n                     from src/core/ext/filters/message_size/message_size_filter.cc:19:\r\n    src/core/ext/filters/message_size/message_size_filter.cc:207:56: note:   mismatched types \u2018absl::lts_2020_09_23::FormatSpec<Args ...>\u2019 and \u2018const char [45]\u2019\r\n                                 calld->limits.max_recv_size)\r\n                                                            ^\r\n    ./src/core/lib/iomgr/error.h:158:71: note: in definition of macro\u2018GRPC_ERROR_CREATE_FROM_COPIED_STRING\u2019\r\n       grpc_error_create(__FILE__, __LINE__, grpc_slice_from_copied_string(desc), \\\r\n     ^\r\n    src/core/ext/filters/message_size/message_size_filter.cc: In function \u2018void message_size_start_transport_stream_op_batch(grpc_call_element*, grpc_transport_stream_op_batch*)\u2019:\r\n    src/core/ext/filters/message_size/message_size_filter.cc:269:48: error: no matching function for call to \u2018StrFormat(const char [41], uint32_t, int&)\u2019\r\n                         calld->limits.max_send_size)\r\n                                                    ^\r\n    ./src/core/lib/iomgr/error.h:158:71: note: in definition of macro\u2018GRPC_ERROR_CREATE_FROM_COPIED_STRING\u2019\r\n       grpc_error_create(__FILE__, __LINE__, grpc_slice_from_copied_string(desc), \\\r\n     ^\r\n    src/core/ext/filters/message_size/message_size_filter.cc:269:48: note: candidate is:\r\n                         calld->limits.max_send_size)\r\n                                                    ^\r\n    ./src/core/lib/iomgr/error.h:158:71: note: in definition of macro\u2018GRPC_ERROR_CREATE_FROM_COPIED_STRING\u2019\r\n       grpc_error_create(__FILE__, __LINE__, grpc_slice_from_copied_string(desc), \\\r\n     ^\r\n    In file included from src/core/ext/filters/message_size/message_size_filter.cc:24:0:\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note: template<class ... Args> std::string absl::lts_2020_09_23::StrFormat(absl::lts_2020_09_23::FormatSpec<Args ...>&, const Args& ...)\r\n     ABSL_MUST_USE_RESULT std::string StrFormat(const FormatSpec<Args...>& format,\r\n                                      ^\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note:   template argument deduction/substitution failed:\r\n    In file included from ./src/core/ext/filters/client_channel/service_config_parser.h:28:0,\r\n                     from ./src/core/ext/filters/message_size/message_size_filter.h:22,\r\n                     from src/core/ext/filters/message_size/message_size_filter.cc:19:\r\n    src/core/ext/filters/message_size/message_size_filter.cc:269:48: note:   mismatched types \u2018absl::lts_2020_09_23::FormatSpec<Args ...>\u2019 and \u2018const char [41]\u2019\r\n                         calld->limits.max_send_size)\r\n                                                    ^\r\n    ./src/core/lib/iomgr/error.h:158:71: note: in definition of macro\u2018GRPC_ERROR_CREATE_FROM_COPIED_STRING\u2019\r\n       grpc_error_create(__FILE__, __LINE__, grpc_slice_from_copied_string(desc), \\\r\n     ^\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc: In member function \u2018virtual std::string grpc_core::{anonymous}::GrpcLb::TokenAndClientStatsAttribute::ToString() const\u2019:\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:278:49: error: no matching function for call to \u2018StrFormat(const char [30], const string&, grpc_core::GrpcLbClientStats*)\u2019\r\n                                  client_stats_.get());\r\n                                                     ^\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:278:49: note: candidate is:\r\n    In file included from src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:69:0:\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note: template<class ... Args> std::string absl::lts_2020_09_23::StrFormat(absl::lts_2020_09_23::FormatSpec<Args ...>&, const Args& ...)\r\n     ABSL_MUST_USE_RESULT std::string StrFormat(const FormatSpec<Args...>& format,\r\n                                      ^\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note:   template argument deduction/substitution failed:\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:278:49: note:   mismatched types \u2018absl::lts_2020_09_23::FormatSpec<Args ...>\u2019 and \u2018const char [30]\u2019\r\n                                  client_stats_.get());\r\n                                                     ^\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc: In member function \u2018std::string grpc_core::{anonymous}::GrpcLb::Serverlist::AsText() const\u2019:\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:525:72: error: no matching function for call to \u2018StrFormat(const char [20], size_t&, std::string&, const char [50])\u2019\r\n                                           ipport, server.load_balance_token));\r\n      ^\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:525:72: note: candidate is:\r\n    In file included from src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:69:0:\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note: template<class ... Args> std::string absl::lts_2020_09_23::StrFormat(absl::lts_2020_09_23::FormatSpec<Args ...>&, const Args& ...)\r\n     ABSL_MUST_USE_RESULT std::string StrFormat(const FormatSpec<Args...>& format,\r\n                                      ^\r\n    third_party/abseil-cpp/absl/strings/str_format.h:338:34: note:   template argument deduction/substitution failed:\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:525:72: note:   mismatched types \u2018absl::lts_2020_09_23::FormatSpec<Args ...>\u2019 and \u2018const char [20]\u2019\r\n                                           ipport, server.load_balance_token));\r\n      ^\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc: In member function \u2018virtual std::string grpc_core::{anonymous}::GrpcLb::TokenAndClientStatsAttribute::ToString() const\u2019:\r\n    src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc:279:5: warning: control reaches end of non-void function [-Wreturn-type]\r\n         }\r\n         ^\r\n    src/python/grpcio/grpc/_cython/cygrpc.cpp: In function \u2018void __pyx_f_4grpc_7_cython_6cygrpc__unified_socket_write(int)\u2019:\r\n    src/python/grpcio/grpc/_cython/cygrpc.cpp:94016:46: warning: ignoring return value of \u2018ssize_t write(int, const void*, size_t)\u2019, declared with attribute warn_unused_result [-Wunused-result]\r\n       (void)(write(__pyx_v_fd, ((char *)\"1\"), 1));\r\n                                                  ^\r\n    src/python/grpcio/grpc/_cython/cygrpc.cpp: In function \u2018PyObject*__pyx_pw_4grpc_7_cython_6cygrpc_7Channel_9segregated_call(PyObject*, PyObject*, PyObject*)\u2019:\r\n    src/python/grpcio/grpc/_cython/cygrpc.cpp:30837:76: warning: \u2018__pyx_v_c_completion_queue\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n       __pyx_v_segregated_call->_c_completion_queue = __pyx_v_c_completion_queue;\r\n          ^\r\n    src/python/grpcio/grpc/_cython/cygrpc.cpp:30471:26: note: \u2018__pyx_v_c_completion_queue\u2019 was declared here\r\n       grpc_completion_queue *__pyx_v_c_completion_queue;\r\n                              ^\r\n    src/python/grpcio/grpc/_cython/cygrpc.cpp: At global scope:\r\n    src/python/grpcio/grpc/_cython/cygrpc.cpp:184131:1: warning: \u2018void __Pyx_PyAsyncGen_Fini()\u2019 defined but not used [-Wunused-function]\r\n     __Pyx_PyAsyncGen_Fini(void)\r\n     ^\r\n\r\n    ----------------------------------------\r\nCommand \"/home/vagdevik/dd/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-KkOhKw/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-vnkkCk-record/install-record.txt --single-version-externally-managed --compile --install-headers /home/vagdevik/dd/include/site/python2.7/grpcio\" failed with error code 1 in /tmp/pip-build-KkOhKw/grpcio/\r\nYou are using pip version 9.0.1, however version 20.3.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n", "comments": ["@vagdevik \r\nTF 1.14 is not supported, can you please try with 2.x and let us know.\r\nThere are siilar issues, please refer to them and let us know: [link](https://github.com/stan-dev/pystan/issues/470), [link1](https://stackoverflow.com/questions/56357794/unable-to-install-grpcio-using-pip-install-grpcio)", "> @vagdevik\r\n> TF 1.14 is not supported, can you please try with 2.x and let us know.\r\n> There are siilar issues, please refer to them and let us know: [link](https://github.com/stan-dev/pystan/issues/470), [link1](https://stackoverflow.com/questions/56357794/unable-to-install-grpcio-using-pip-install-grpcio)\r\n\r\nThanks, but adding a bit of important informtion here:\r\n\r\nHave even tried with this:\r\n```\r\npip install tensorflow==2.0.0a0\r\n```\r\nand faced the same issue.\r\nNevertheless, doing the following solved the issue:\r\n```\r\npip install --upgrade pip\r\npip install tensorflow\r\n```\r\n\r\nSo I realized that the problem was with my pip version.\r\nIf pip is of an older version(say 9.0), and if we install tensorflow directly, it is trying to install tensoflow 1.14 and is failing to succeed in installation. To make it successful, I relied on the above commands.\r\n\r\nFinally, once I upgraded pip, and just installed tensorflow (with/without the version provided), it worked smooth and installed tf2.\r\nI realized from the [docs](https://www.tensorflow.org/install) that the pip version should be greater than 19.0 for TensorFlow 2.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45445\">No</a>\n"]}, {"number": 45444, "title": "Did not get operators or tensors in subgraph 1. when using tf.lite.Interpreter", "body": "**tf version: 2.3.1** \r\n\r\nI saved a model of simple RNN and after quantitation, this model can not be reloaded by `tf.lite.Interpreter`\r\nI tried other types of model such as textCNN, it still does not work.\r\n\r\nIf I remove the `converter.optimizations = [tf.lite.Optimize.DEFAULT] `, it succeed.\r\nHowever, I cannot remove this command as it can significantly reduce the size of my model.\r\nAny hints ?\r\nThanks.\r\n```\r\ndef get_model_rnn():\r\n    inputs = layers.Input(shape=(maxlen,))\r\n    embedding = layers.Embedding(vocab_size, 128,  trainable=True)\r\n    title_embed = embedding(inputs)\r\n    title_ids_mask = layers.Masking(mask_value=0, name='mask')(title_embed)\r\n    title_gru = layers.Bidirectional(layers.GRU(128, return_sequences=False))(title_ids_mask)\r\n    outputs = layers.Dense(1, activation='sigmoid', name='mlp2')(title_gru) \r\n    model = keras.Model(inputs=inputs, outputs=outputs)\r\n    model.compile(\"Adam\", \"binary_crossentropy\", metrics=[\"binary_accuracy\"])\r\n    return model \r\nmodel = get_model_rnn()\r\nmodel.save('./model_rnn')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./model_rnn')\r\n#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT] # this caused the error\r\ntflite_quant_model = converter.convert()\r\nopen(\"model_rnn.tflite\", \"wb\").write(tflite_quant_model)\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"./model_rnn.tflite\")   \r\n```\r\n\r\n\r\nERROR==>\r\n```\r\nIn [27]: interpreter = tf.lite.Interpreter(model_path=\"./model_rnn.tflite\")\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-27-55e82bdec7e7> in <module>\r\n----> 1 interpreter = tf.lite.Interpreter(model_path=\"./model_rnn.tflite\")\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads)\r\n    196       self._interpreter = (\r\n    197           _interpreter_wrapper.CreateWrapperFromFile(\r\n--> 198               model_path, self._custom_op_registerers))\r\n    199       if not self._interpreter:\r\n    200         raise ValueError('Failed to open {}'.format(model_path))\r\n\r\nValueError: Did not get operators or tensors in subgraph 1.\r\n```", "comments": ["`converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]` , which has the same effect of compression, can also cause the error. \r\nthe size of the original model is **15M**\r\nif not using \r\n```\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n```\r\nthe quantization model size is 11M.\r\nIf use, the model will be 2.8M. A significant change.", "if not use `converter.optimizations = [tf.lite.Optimize.DEFAULT]`\r\n`tflite_quant_model = converter.convert()` info:\r\n\r\n> 2020-12-08 06:37:25.640712: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n> 2020-12-08 06:37:25.640753: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n> 2020-12-08 06:37:25.641019: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: ./model_rnn\r\n> 2020-12-08 06:37:25.701719: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n> 2020-12-08 06:37:25.701761: I tensorflow/cc/saved_model/loader.cc:250] Reading SavedModel debug info (if present) from: ./model_rnn\r\n> 2020-12-08 06:37:25.701954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-12-08 06:37:25.701962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]\r\n> 2020-12-08 06:37:25.924717: I tensorflow/cc/saved_model/loader.cc:215] Restoring SavedModel bundle.\r\n> 2020-12-08 06:37:26.177707: I tensorflow/cc/saved_model/loader.cc:199] Running initialization op on SavedModel bundle at path: ./model_rnn\r\n> 2020-12-08 06:37:26.395008: I tensorflow/cc/saved_model/loader.cc:319] SavedModel load for tags { serve }; Status: success: OK. Took 753990 microseconds.\r\n\r\n\r\nif use `converter.optimizations = [tf.lite.Optimize.DEFAULT]`\r\nThere are more lines of info:\r\n\r\n> 2020-12-08 06:38:08.069592: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n> 2020-12-08 06:38:08.069635: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n> 2020-12-08 06:38:08.069906: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: ./model_rnn\r\n> 2020-12-08 06:38:08.134263: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n> 2020-12-08 06:38:08.134305: I tensorflow/cc/saved_model/loader.cc:250] Reading SavedModel debug info (if present) from: ./model_rnn\r\n> 2020-12-08 06:38:08.134534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-12-08 06:38:08.134543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]\r\n> 2020-12-08 06:38:08.359398: I tensorflow/cc/saved_model/loader.cc:215] Restoring SavedModel bundle.\r\n> 2020-12-08 06:38:08.610223: I tensorflow/cc/saved_model/loader.cc:199] Running initialization op on SavedModel bundle at path: ./model_rnn\r\n> 2020-12-08 06:38:08.831016: I tensorflow/cc/saved_model/loader.cc:319] SavedModel load for tags { serve }; Status: success: OK. Took 761110 microseconds.\r\n> **2020-12-08 06:38:09.941434: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor std.constant13 because it has fewer than 1024 elements (256).\r\n> 2020-12-08 06:38:09.953622: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor arg6 that is not type float.\r\n> 2020-12-08 06:38:09.953653: I tensorflow/lite/tools/optimize/quantize_weights.cc:220] Skipping quantization of tensor arg7 because it has no allocated buffer.\r\n> 2020-12-08 06:38:09.954093: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor arg6 that is not type float.\r\n> 2020-12-08 06:38:09.954100: I tensorflow/lite/tools/optimize/quantize_weights.cc:220] Skipping quantization of tensor arg7 because it has no allocated buffer.**", "`2.5.0-dev20201207` is OK for this problem.\r\n", "I resolved this problem by running the optimization (quantization) script on the SavedModel format (according to https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) inside the TF docker container. My model was also inside the container (I've created a docker volume). ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45444\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45444\">No</a>\n", "Had the same problem with `tf version = 2.5.0`, could solve it by updating to `2.6` for the export.", "Update to my comment above: Seems it doesn't work with every model architecture ..."]}, {"number": 45443, "title": "error when load the model use keras.models.load_model", "body": "\r\n**System information**\r\n\r\n- OS Platform: Windows10\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.4\r\n- CPU\r\n\r\nI used AutoKeras generate a model and saved as 'tf' format.\r\nmodel.save('ak_best_model', save_format='tf')\r\nThen i use:\r\nkeras.models.load_model('ak_best_model')   in the same script\r\nIt works well...\r\n\r\nBut i open a new script and use Keras load the same model, error occured:\r\n\r\nRuntimeError: Unable to restore a layer of class Custom>MultiCategoryEncoding. Layers of class Custom>MultiCategoryEncoding require that the class be provided to the model loading code, either by registering the class using @keras.utils.register_keras_serializable on the class def and including that file in your program, or by passing the class in a keras.utils.CustomObjectScope that wraps this load call.\r\n\r\nhere is my script:\r\n\r\ndf = pd.read_excel('total.xlsx', dtype=np.float32)\r\n\r\ny = df[['Static_Fric', 'Rolling_Fric']].values\r\nx = df.drop(labels = ['Static_Fric', 'Rolling_Fric'], axis =1)\r\n\r\nx_train, x_test, y_train, y_test = train_test_split(\r\n    x, y,\r\n    test_size=0.2,\r\n    shuffle=True\r\n    )\r\nkeras_model = load_model(r'D:\\Program Files\\Desktop\\ML\\AutoKeras\\ak_best_model')\r\n\r\n\r\n\r\n", "comments": ["@Jiaxin8122,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: 'total.xlsx'`.\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. \r\n\r\nAlso, please go through issue [#44199](https://github.com/tensorflow/tensorflow/issues/44199#issuecomment-730297817) with a similar error and let us know if it helps. Thanks!", "here is the dataset:\r\n[total.xlsx](https://github.com/tensorflow/tensorflow/files/5652212/total.xlsx)\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nfrom keras.models import load_model\r\nfrom sklearn.model_selection import train_test_split\r\n\r\ndf = pd.read_excel('total.xlsx', dtype=np.float32)\r\n\r\ny = df[['Static_Fric', 'Rolling_Fric']].values\r\nx = df.drop(labels = ['Static_Fric', 'Rolling_Fric'], axis =1)\r\n\r\nx_train, x_test, y_train, y_test = train_test_split(\r\n    x, y,\r\n    test_size=0.2,\r\n    shuffle=True\r\n    )\r\n\r\nkeras_model = load_model(r'D:\\Program Files\\Desktop\\ML\\AutoKeras\\ak_best_model')\r\n\r\nBut my model based on training from AutoKeras...I saved it as 'tf' format.\r\n\r\nThe error:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 416, in _revive_layer_from_config\r\n    obj = layers_module.deserialize(\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 171, in deserialize\r\n    return generic_utils.deserialize_keras_object(\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 346, in deserialize_keras_object\r\n    (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 296, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown layer: Custom>MultiCategoryEncoding\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-5bbd59fbc60c>\", line 1, in <module>\r\n    runfile('D:/Program Files/Desktop/ML/Reinforcement/RL.py', wdir='D:/Program Files/Desktop/ML/Reinforcement')\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2020.2.3\\plugins\\python-ce\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2020.2.3\\plugins\\python-ce\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"D:/Program Files/Desktop/ML/Reinforcement/RL.py\", line 33, in <module>\r\n    keras_model = load_model(r'D:\\Program Files\\Desktop\\ML\\AutoKeras\\ak_best_model')\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 187, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 120, in load\r\n    model = tf_load.load_internal(\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 632, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 194, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 130, in __init__\r\n    self._load_all()\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 215, in _load_all\r\n    self._layer_nodes = self._load_layers()\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 315, in _load_layers\r\n    layers[node_id] = self._load_layer(proto.user_object, node_id)\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 341, in _load_layer\r\n    obj, setter = self._revive_from_config(proto.identifier, metadata, node_id)\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 359, in _revive_from_config\r\n    self._revive_layer_from_config(metadata, node_id))\r\n  File \"E:\\Pycharm\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 420, in _revive_layer_from_config\r\n    raise RuntimeError(\r\nRuntimeError: Unable to restore a layer of class Custom>MultiCategoryEncoding. Layers of class Custom>MultiCategoryEncoding require that the class be provided to the model loading code, either by registering the class using @keras.utils.register_keras_serializable on the class def and including that file in your program, or by passing the class in a keras.utils.CustomObjectScope that wraps this load call.\r\n\r\nbest wishes!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45443\">No</a>\n", "@Jiaxin8122  did you solve the problem.\r\nI am also facing the same problem\r\n"]}, {"number": 45442, "title": "Extract reference for operator DIV to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nThis PR is part of the work to port operator DIV from lite to micro,\r\nas tracked in Issue #45431.", "comments": []}, {"number": 45441, "title": "mixed_precision make train and predict very slow when only using cpu", "body": "tensorflow : 2.3\r\nHere is the [colab](https://colab.research.google.com/drive/1glypA7wC988kivuGe07Qfk_XHuq1j9aS?usp=sharing)\r\nYou can see that it costs 3s to train a epoch while costs 187s to train the same epoch using ```mixed_precision ```\r\n\r\n```\r\nEpoch 1/5\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.2968 - accuracy: 0.9134\r\nEpoch 2/5\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.1448 - accuracy: 0.9575\r\nEpoch 3/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.9678\r\nEpoch 4/5\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.0861 - accuracy: 0.9730\r\nEpoch 5/5\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.0734 - accuracy: 0.9769\r\n```\r\nvs\r\n```\r\nEpoch 1/5\r\n1875/1875 [==============================] - 187s 100ms/step - loss: 0.2936 - accuracy: 0.9141\r\nEpoch 2/5\r\n1179/1875 [=================>............] - ETA: 1:11 - loss: 0.1455 - accuracy: 0.9555\r\n```", "comments": ["@DachuanZhao \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [steps followed before you ran into this error or stand alone code to reproduce the issue faced, if possible colab gist with the error]", "> @DachuanZhao\r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [steps followed before you ran into this error or stand alone code to reproduce the issue faced, if possible colab gist with the error]\r\n\r\nI think this bug is a common bug which means it may exist in many versions . So I only give the tensroflow version . \r\nI have post a colab , and you can run the code in the colab . ", "Why using mixed precision with CPU? As said in the [doc](https://www.tensorflow.org/guide/mixed_precision), \r\n\r\n> As mentioned before, the mixed_float16 policy will most significantly improve performance on NVIDIA GPUs with compute capability of at least 7.0. The policy will run on other GPUs and CPUs but may not improve performance. For TPUs, the mixed_bfloat16 policy should be used instead.\r\n\r\nThis slowdown is pretty much expected.", "> This slowdown is pretty much expected.\r\n```it may not improve performance.```\r\n\r\nIn fact , it leads to worse performance on cpu ......", "I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.", "> I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.\r\n\r\nBut at least it should't make the performance  worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......", "> > I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.\r\n> \r\n> But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......\r\n\r\nThere is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision no matter using mixed precision or not.", "> > > I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.\r\n> > \r\n> > \r\n> > But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......\r\n> \r\n> There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.\r\n\r\nThe strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......\r\n", "I've clearly stated that mixed precision training has nothing to do with serving your model in full precision. Even in mixed precision training your model is saved in full precision.", "> > > > I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.\r\n> > > \r\n> > > \r\n> > > But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......\r\n> > \r\n> > \r\n> > There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.\r\n> \r\n> The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......\r\n\r\nIf you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.", "> > > > > I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.\r\n> > > > \r\n> > > > \r\n> > > > But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......\r\n> > > \r\n> > > \r\n> > > There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.\r\n> > \r\n> > \r\n> > The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......\r\n> \r\n> If you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.\r\n\r\nHave you run the code in my colab ?  It's acceptable that training a model with mixed_precision on cpu is 60 times slower than training it without mixed_precision  ? ", "> > > > > > I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.\r\n> > > > > \r\n> > > > > \r\n> > > > > But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......\r\n> > > > \r\n> > > > \r\n> > > > There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.\r\n> > > \r\n> > > \r\n> > > The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......\r\n> > \r\n> > \r\n> > If you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.\r\n> \r\n> Have you run the code in my colab ? It's acceptable that training a model with mixed_precision on cpu is 60 times slower than training it without mixed_precision ?\r\n\r\n\"The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\"\r\nMixed precision is not meant to accelerate training on CPU. It's expected to impact performance.", "> > > > > > I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.\r\n> > > > > \r\n> > > > > \r\n> > > > > But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......\r\n> > > > \r\n> > > > \r\n> > > > There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.\r\n> > > \r\n> > > \r\n> > > The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......\r\n> > \r\n> > \r\n> > If you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.\r\n> \r\n> Have you run the code in my colab ? It's acceptable that training a model with mixed_precision on cpu is 60 times slower than training it without mixed_precision ?\r\n\r\nfloat16 are accelerated by hardware, not software. Software enables such acceleration with supported hardware, while regular CPUs do not have fp16 capability. I am surprised it actually runs btw.", "@byronyi \r\nis this still an issue.", "> @byronyi\r\n> is this still an issue.\r\n\r\nI don't think so and this issue could be closed.", "Moving this to closed status with confirmation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45441\">No</a>\n", "> > @byronyi\r\n> > is this still an issue.\r\n> \r\n> I don't think so and this issue could be closed.\r\n\r\nWhy not ???\r\nYou say : \"I am surprised it actually runs btw.\" , but you don't think it's a bug ???", "> Moving this to closed status with confirmation.\r\n\r\nI suggest you run the code in my colab and you decide whether it's a bug or not by yourself ......", "> accelerate\r\n\r\nSo the conclusion is that \"It's acceptable that training or predicting a model with mixed_precision on cpu is slower than without mixed_precision\" ?", "You should really consult your CPU vendor. There is nothing TF can do to fix this for you except for disabling mixed precision on unsupported hardware all together.", "cc @reedwm ", "> You should really consult your CPU vendor. There is nothing TF can do to fix this for you except for disabling mixed precision on unsupported hardware all together.\r\n\r\nThis problem can reproduce at google colab . You think it may be a common problem on different types of CPUs so that TF can't fix it ?", "As @byronyi has stated, CPUs do not have hardware support for float16 and so will be slower with mixed_float16. I'll clarify in the tutorial that mixed precision can actually significantly slow down a model on CPUs, instead of just not speeding it up. \r\n\r\nAs for the TF serving issue: A float32 and a mixed_float16 `tf.train.Checkpoint` are identical for the same model, as checkpoints do not store the dtype of computations. On the other hand, a float32 and mixed_float16 SavedModel are different, as SavedModels store the graph of computations, which includes the dtype of computations. Using a mixed_float16 SavedModel with TF-Serving on a device that does not support mixed precision will be slow. As a workaround, checkpoints can be used instead, then when a SavedModel is required, the checkpoint can be loaded into a float32 model and a float32 SavedModel can be generated. I'll talk people working on SavedModel to work on a better solution.", "> As @byronyi has stated, CPUs do not have hardware support for float16 and so will be slower with mixed_float16. I'll clarify in the tutorial that mixed precision can actually significantly slow down a model on CPUs, instead of just not speeding it up.\r\n> \r\n> As for the TF serving issue: A float32 and a mixed_float16 `tf.train.Checkpoint` are identical for the same model, as checkpoints do not store the dtype of computations. On the other hand, a float32 and mixed_float16 SavedModel are different, as SavedModels store the graph of computations, which includes the dtype of computations. Using a mixed_float16 SavedModel with TF-Serving on a device that does not support mixed precision will be slow. As a workaround, checkpoints can be used instead, then when a SavedModel is required, the checkpoint can be loaded into a float32 model and a float32 SavedModel can be generated. I'll talk people working on SavedModel to work on a better solution.\r\n\r\n@reedwm \r\nHi. Is there any progress on that issue? it will be very helpful when using mixed precision for only \"training\" time, and serve model on CPU devices. We need something conenient \"save as dtype=float32\" method.", "> As @byronyi has stated, CPUs do not have hardware support for float16 and so will be slower with mixed_float16. I'll clarify in the tutorial that mixed precision can actually significantly slow down a model on CPUs, instead of just not speeding it up.\r\n> \r\n> As for the TF serving issue: A float32 and a mixed_float16 `tf.train.Checkpoint` are identical for the same model, as checkpoints do not store the dtype of computations. On the other hand, a float32 and mixed_float16 SavedModel are different, as SavedModels store the graph of computations, which includes the dtype of computations. Using a mixed_float16 SavedModel with TF-Serving on a device that does not support mixed precision will be slow. As a workaround, checkpoints can be used instead, then when a SavedModel is required, the checkpoint can be loaded into a float32 model and a float32 SavedModel can be generated. I'll talk people working on SavedModel to work on a better solution.\r\n\r\n@reedwm @byronyi Thanks for all your explanation ~~~"]}, {"number": 45440, "title": "Extract a function for parsing operator DIV", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nThis PR is part of the work to port operator DIV from lite to micro,\r\nas tracked in Issue #45431.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45440) for more info**.\n\n<!-- need_sender_cla -->", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "@googlebot I signed it!", "@ddavis-2015  Can you please resolve conflicts? Thanks!"]}, {"number": 45439, "title": "Flatbuffer_conversion changes for Transpose", "body": "Refactor transpose in flatbuffer_conversions.cc to prepare for porting kernel to micro.", "comments": ["@patriklaurell Can you please resolve conflicts? Thanks!", "Ready for review @petewarden "]}]