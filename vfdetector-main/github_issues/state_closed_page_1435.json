[{"number": 9921, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 9920, "title": "Freezing graphs with custom ops", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 and MacOS Sierra\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master (quite recent)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **Exact command to reproduce**: freeze_graph\r\n\r\n### Describe the problem\r\nI've asked my question on StackOverflow here:\r\nhttp://stackoverflow.com/questions/43880729/using-new-op-while-importing-graph-in-tensorflow\r\nI'm trying to freeze a model which contains a custom op. But `freeze_graph` gives the following error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 202, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in main\r\n    FLAGS.variable_names_blacklist)\r\n  File \"<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 99, in freeze_graph\r\n    _ = importer.import_graph_def(input_graph_def, name=\"\")\r\n  File \"<local path>/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 260, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named RoiPool in defined operations.\r\n```\r\nI was suggested on StackOverflow to build `freeze_graph` with my custom op as a dependency. I did that, but `freeze_graph` still gives the same error. \r\n\r\nIt was also suggested for me to open a **feature request** to make an easier-to-use interface for using freeze_graph with custom ops.\r\n\r\n### Source code / logs\r\nHere is the freeze_graph command I'm using: \r\n`bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=<local path>/data/graph_vgg.pb --input_checkpoint=<local path>/data/VGGnet_fast_rcnn_iter_70000.ckpt --output_node_names=\"cls_prob,bbox_pred\" --output_graph=<local path>/graph_frozen.pb`", "comments": ["@petewarden FYI", "@aselle could you chime in", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I guess we'd need for `freeze_graph` to load some binary to do that? I'm not sure how this would work.\r\n\r\n@rajatvikramsingh if you create a rule that's like `freeze_graph` but links with your binary, possibly loads a DSO if you have created one, would that work for you?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "hi\uff0cI have the same proble, how you solve it?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "See resolution above."]}, {"number": 9919, "title": "[DOCS] Updating PredictionType args definition", "body": "[DOCS] Updating PredictionType value for DynamicRNN", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 9916, "title": "tf.self_adjoint_eig doesn't behave the same way with float32 and float64", "body": "Here is an example to reproduce the problem\r\n\r\n```\r\nl = tf.constant([[10., -4., -4., -2.],\r\n                  [-4., 10., -2., -4.],\r\n                  [-4., -2., 6., 0.],\r\n                  [-2., -4., 0., 6.]], dtype=tf.float64)\r\ne, v = tf.self_adjoint_eig(tf.expand_dims(l, 0))\r\n```\r\n\r\nWhen I set `dtype=tf.float64` the output is\r\n\r\n```\r\n// Eigen values\r\n[[ -2.31986627e-15   5.52786405e+00   1.20000000e+01   1.44721360e+01]]\r\n\r\n// Eigen vectors\r\n[[-0.5        -0.16245985  0.5        -0.68819096]\r\n [-0.5         0.16245985  0.5         0.68819096]\r\n [-0.5        -0.68819096 -0.5         0.16245985]\r\n [-0.5         0.68819096 -0.5        -0.16245985]]\r\n```\r\n\r\nWhen `dtype=tf.float32` the output is\r\n\r\n```\r\n// Eigen values\r\n[[ -1.02379988e-06   5.52786446e+00   1.20000019e+01   1.44721375e+01]]\r\n\r\n// Eigen vectors\r\n[[ 0.49999985  0.16245979  0.49999985 -0.68819106]\r\n [ 0.5        -0.16246006  0.50000018  0.68819082]\r\n [ 0.5         0.68819106 -0.49999988  0.16246004]\r\n [ 0.49999991 -0.68819088 -0.50000012 -0.16245979]]\r\n```\r\nIn this case the sign of the second eigen vector changed.\r\n\r\nThe numpy equivalent of this code always give the same result (in float or float32) and is similar to the result I got with the tf.float64 version\r\n\r\n```\r\nL = np.array([[10., -4., -4., -2.],\r\n                  [-4., 10., -2., -4.],\r\n                  [-4., -2., 6., 0.],\r\n                  [-2., -4., 0., 6.]])\r\nD, V =np.linalg.eigh(L)\r\n```\r\n\r\n\r\n```\r\n// numpy eigen values\r\n[  1.11716192e-15   5.52786405e+00   1.20000000e+01   1.44721360e+01]\r\n\r\n// numpy eigen vectors\r\n[[ 0.5        -0.16245985  0.5        -0.68819096]\r\n [ 0.5         0.16245985  0.5         0.68819096]\r\n [ 0.5        -0.68819096 -0.5         0.16245985]\r\n [ 0.5         0.68819096 -0.5        -0.16245985]]\r\n```\r\n\r\n", "comments": ["Note that this matrix has rank 3, and the eigenvector in question corresponds to 0 eigenvalue.\r\n\r\nAlso note that both 0.5,0.5,0.5,0.5 and -0.5,-0.5,-0.5,-0.5 are valid normalized eigenvectors.\r\n\r\nSo, from the description of the function it seems the behavior is correct. I agree that it would be less surprising to have more deterministic behavior. Maybe an additional constraint to always have first component of eigenvector be non-negative? cc @rmlarsen ", "@yaroslavvb my problem here is not the first eigen vector but the second one. It goes from -0.16, 0.16, -0.68, 0.68 to 0.16, -0.16, 0.68, -68 with the same eigen value. And it changes every thing.", "@jrabary same thing here, they are both correct eigenvectors", "@yaroslavvb I understand. In fact, additional constraint will be help full here to get consistent result accross run. ", "I'm gonna mark this as a feature request, since it seems like this behavior is correct but potentially confusing.", "We could close this. I think that with nightly the signs are correct:\r\n```\r\ntf.Tensor([[-2.31986627e-15  5.52786405e+00  1.20000000e+01  1.44721360e+01]], shape=(1, 4), dtype=float64)\r\ntf.Tensor(\r\n[[[-0.5        -0.16245985  0.5        -0.68819096]\r\n  [-0.5         0.16245985  0.5         0.68819096]\r\n  [-0.5        -0.68819096 -0.5         0.16245985]\r\n  [-0.5         0.68819096 -0.5        -0.16245985]]], shape=(1, 4, 4), dtype=float64)\r\n\r\ntf.Tensor([[-2.06638583e-06  5.52786446e+00  1.20000000e+01  1.44721365e+01]], shape=(1, 4), dtype=float32)\r\ntf.Tensor(\r\n[[[-0.5        -0.16245994  0.49999976 -0.688191  ]\r\n  [-0.5000001   0.1624598   0.5000002   0.6881908 ]\r\n  [-0.49999988 -0.68819094 -0.5000001   0.16246   ]\r\n  [-0.50000006  0.688191   -0.49999994 -0.16245969]]], shape=(1, 4, 4), dtype=float32)\r\n\r\n```", "@jrabary,\r\n\r\nI tried in colab with `Tensorflow 2.8` and i didn't see any discrepancy in behave between `float32` and `float64`. Please, find the gist [here](https://colab.research.google.com/gist/chunduriv/e7e09e8f87d9b14a23308ce54deb1199/9916.ipynb).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 9915, "title": "problem importing tensorflow with tensorflow-gpu pip package and Nvidia PRIME", "body": "\r\n### System information\r\n- OS Platform and Distribution: Linux Ubuntu 16.10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: tensorflow-gpu-1.1.0\r\n- **CUDA/cuDNN version**:\r\n- GPU model and memory: GeForce 940MX 982MiB\r\n- Exact command to reproduce: import tensorflow\r\n\r\n\r\n== cat /etc/issue ===============================================\r\nLinux Lyn 4.8.0-51-generic #54-Ubuntu SMP Tue Apr 25 16:32:21 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.10 (Yakkety Yak)\"\r\nVERSION_ID=\"16.10\"\r\nVERSION_CODENAME=yakkety\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 6.2.0-5ubuntu12) 6.2.0 20161005\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux Lyn 4.8.0-51-generic #54-Ubuntu SMP Tue Apr 25 16:32:21 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.3.0)\r\ntensorflow-gpu (1.1.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\n2017-05-15 16:22:31.009080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-15 16:22:31.009102: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-15 16:22:31.009124: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-15 16:22:31.009131: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-15 16:22:31.009139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-15 16:22:31.119107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-15 16:22:31.119494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce 940MX\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 0.993\r\npciBusID 0000:01:00.0\r\nTotal memory: 982.12MiB\r\nFree memory: 675.25MiB\r\n2017-05-15 16:22:31.119518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-15 16:22:31.119526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-15 16:22:31.119542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0)\r\ntf.VERSION = 1.1.0\r\ntf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\ntf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon May 15 16:21:29 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce 940MX       Off  | 0000:01:00.0     Off |                  N/A |\r\n| N/A   43C    P0    N/A /  N/A |    262MiB /   982MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1017    G   /usr/lib/xorg/Xorg                             168MiB |\r\n|    0      1860    G   /usr/bin/compiz                                 41MiB |\r\n|    0      2324    G   ...el-token=2DD3BDBDD08C58317A0131100BC13BC1    52MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n------------------\r\ntf.GIT_VERSION\r\nv1.1.0-rc0-61-g1ec6ed5\r\n\r\ntf.VERSION\r\n1.1.0\r\n\r\n### Describe the problem\r\nI have a laptop with a dedicated Nvidia GPU. I use it only for prototyping my tensorflow code.\r\nBut dedicated GPUs drain a lot of energy and reduce the laptop's battery life.\r\nSo when I'm outside on battery (eg: in the library at university) I always set Nvidia PRIME to use the integrated card only (type nvidia-settings in a console to reach this setting).\r\nWith the previous versions of tensorflow-gpu (installedi via pip3 on Ubuntu) everything worked well.\r\nNow with the current release I can no longer use tensorflow-gpu while I have the Nvidia card disabled with PRIME.\r\nNow, to be able to work with tensorflow AND have enough battery to conclude my day, I have to install the pip package \"tensorflow\" (and not \"tensorflow-gpu\"). but that turns useless if, for some reason, need to test my code with GPU acceleration, turning back on the dedicated graphic card via Nvidia PRIME.\r\nIf I really want GPU acceleration I have to re-enable the dedicated card in PRIME, uninstall tensorflow and reinstall tensorflow-gpu. every time. that's a mess!\r\nTo me, there are two way to resolve this bug:\r\n1) make tensorflow-gpu again able to handle situation with all the Nvidia GPU are temporarily disabled.\r\n2) unify the tensorflow and tensorflow-gpu packages, including a smart logic inside them that enable the software to handle theese ibrid situations, which I think will be very common in the laptops in the near future\r\n\r\n### Source code / logs\r\nimport tensorflow\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 41   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     42   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n/usr/lib/python3.5/imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n/usr/lib/python3.5/imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: libnvidia-fatbinaryloader.so.375.39: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-a649b509054f> in <module>()\r\n----> 1 import tensorflow\r\n\r\n/home/federico/.local/lib/python3.5/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module>()\r\n     49 import numpy as np\r\n     50 \r\n---> 51 from tensorflow.python import pywrap_tensorflow\r\n     52 \r\n     53 # Protocol buffers\r\n\r\n/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     50 for some common reasons and solutions.  Include the entire stack trace\r\n     51 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 52   raise ImportError(msg)\r\n     53 \r\n     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/federico/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libnvidia-fatbinaryloader.so.375.39: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["Please can you clarify. You are unable to load tensorflow-gpu when your graphics card is off. But you used to be able to? And you can run tensorflow-gpu ok when the GPU is on? I'm almost certain this is intentional as Tensorflow won't be able to access the GPU and won't function. \r\n\r\nCan I suggest that you read through #9071 to see if it relates to you. I notice that you are on 16.10 however I have no issues using Tensorflow on that OS. The usual advice of checking drivers and dependencies applies, especially as you appear to be installing tensorflow-gpu frequently.\r\n\r\nI'm sure a Tensorflower will be able to clarify however in the mean time may **I suggest using some form of environment manager such as virtualenv or anaconda**.  This will allow you to have both tensorflow and tensorflow-gpu installed individually and allow you to swap between the two depending on whether you have your GPU on or not without having to uninstall and reinstall. This will allow you switch with relative ease. \r\n\r\nEDIT: I forgot to mention that you can also use `with tf.device('/cpu:0'):` to run code only on the CPU. I guess it depends on how your code is set up and if it's easier to just switch Tensorflow instances. But that's 2X storage and updates.", "1) yes: with the old releases I was able to load tensorflow-gpu even if my Nvidia card was shutted down via PRIME. there were only some warnings about the failed loading of some CUDA stuff (which I cannot remember now exactly), but I was able to do computations correctly, without the need to install the \"tensorflow\" package.\r\n2) yes: with the current release (and with only tensorflow-gpu installed) I'm able to do computations only when I resume the Nvidia card via PRIME. if the dedicated card is disabled it's impossible to import the tensorflow module: `import tensorflow` fails.\r\n3) my OS didn't change. all the dependencies are automatically tracked by pip and all of them are updated to the latest available version in pip. I'm using 375.66 Nvidia drivers and it didn't change. the only thing changed is the tensorflow-gpu release installed (upgraded). I'm not too much deep into technical stuff, but #9071 doesn't seem immediately related to me, but I can be wrong. I cannot downgrade my Ubuntu installation now to do some tests.\r\n4) I will surely try the virtual environment workaround soon, but I think that the tensorflow framework must work out of the box, so I hope this bug will gain attention. I don't think that it should have the `type:feature` label, because this bug is a huge handicap for all the people using Nvidia PRIME (and also Nvidia Optimus?) to try to have a decent battery life in their laptops.\r\n5) I cannot run `with tf.device('/cpu:0'):` because I cannot define `tf` at all: `import tensorflow as tf` returns the error posted above.\r\n\r\nthanks a lot for your time spent to answer me", "1/2. I'm quite surprised about this. Not sure that's intended. Perhaps tensorflow treated it as an insufficient card. It could be that an nVidia driver change now hides the card completely?\r\n\r\n3. Your OS shouldn't be an issue. I run on 16.10 with no issues. What version of CUDA/cuDNN are you using? I'm aware that some newer versions are not fully supported by tensorflow atm.\r\n\r\n4. I wouldn't quite call this a work around. This is a reccommend way of working with python and these kind of installs as it allows you to partially sandbox installs of things so that they dont break each other. Allows you to test a newer version of Tensorflow too and seeing if it will break anything.\r\nSaying that Tensorflow should work out of the box is a little bit tricky. Consider that Tensorflow is often compiled from source for certain platforms and having one binary that fits all might be a bit of a push. \r\n\r\nSorry this doesn't help you much but I think your solution is to use different versions of Tensorflow.", "@FedericoMuciaccia Could you try setting `LD_LIBRARY_PATH` to point to where your `CUDA` and `CUDNN` so files are located?\r\nYou can find out more on how to set these in NVIDIA's CUDA and CUDNN installation documentation.", "@jubjamie \r\n* cuda: 8.0.44\r\n* cudnn: 5.1.5\r\n\r\n@gunan \r\nsearching the web I've found that the correct way to set `LD_LIBRARY_PATH` in Ubuntu is this:\r\n* create a file `/etc/ld.so.conf.d/cuda_things.conf`\r\n* add the line `/usr/lib/x86_64-linux-gnu` inside it (where all my libs actually are)\r\n* run `sudo ldconfig`\r\n\r\nwhith all this done, the problem is still present", "Did you try also adding `/usr/lib/nvidia-375` to your `LD_LIBRARY_PATH`?\r\n\r\nA quick google search shows some people ran into the exact same issue you are, and was able to resolve it:\r\nhttps://stackoverflow.com/questions/42678439/importerror-libnvidia-fatbinaryloader-so-375-39-cannot-open-shared-object-file", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9914, "title": "Optimizing valid TensorFlow graph yields \"graph_def is invalid\" ValueError", "body": "`tensorflow.python.tools.optimize_for_inference_lib.optimize_for_inference` is producing an invalid graph definition. So far as I can tell this is not user error; optimizing a valid graph definition should produce a valid graph definition, so this appears to be a bug.\r\n\r\nThe following code demonstrates the problem. You will need the input graph definition [model.txt.gz](https://github.com/tensorflow/tensorflow/files/1001369/model.txt.gz). Running the code loads the graph definition, verifies it is valid (by importing it and printing the number of nodes) then calls `optimize_for_inference`. We then attempt to verify the resulting graph definition but get the error\r\n\r\n    ValueError: graph_def is invalid at node u'valid/valid_fed/model/rnn/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat/axis': More inputs specified ('valid/valid_fed/model/rnn/rnn/while/Switch:1') than the op expects..\r\n\r\nThe error originates from\r\n\r\n    tensorflow/python/framework/importer.py, line 362, in import_graph_def\r\n\r\nThe attached model definition has been manually altered to reduce the size of the (frozen) parameters but the same error occurs with the unmodified original. Attempts to reproduce this problem with a simpler graph failed. Simpler graphs can be optimized successfully. I don't know what it is about this graph that causes the failure.\r\n\r\n    import gzip\r\n    \r\n    import tensorflow as tf\r\n    from google.protobuf import text_format\r\n    from tensorflow.python.tools import optimize_for_inference_lib\r\n    \r\n    \r\n    def verify(graph_def):\r\n        with tf.Graph().as_default():\r\n            tf.import_graph_def(graph_def, name=\"\")\r\n            print(len(tf.get_default_graph().as_graph_def().node))\r\n    \r\n    \r\n    def read_graph_def(path):\r\n        graph_def = tf.GraphDef()\r\n    \r\n        with gzip.open(path, \"rb\") as input_file:\r\n            text_format.Merge(input_file.read(), graph_def)\r\n    \r\n        return graph_def\r\n    \r\n    \r\n    def optimize(input_graph_def):\r\n        output_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n            input_graph_def, [\"valid/valid_fed/model/input/x\"],\r\n            [\"valid/valid_fed/model/output/y\"], tf.int32.as_datatype_enum)\r\n        return output_graph_def\r\n    \r\n    \r\n    def main():\r\n        input_graph_def = read_graph_def(\"model.txt.gz\")\r\n        verify(input_graph_def)\r\n        output_graph_def = optimize(input_graph_def)\r\n        verify(output_graph_def)\r\n    \r\n    \r\n    if __name__ == \"__main__\":\r\n        main()\r\n\r\nEnvironment details:\r\n\r\n- Linux Ubuntu 14.04\r\n- TensorFlow installed from source\r\n- TensorFlow version: ('v1.1.0-0-g1ec6ed5', '1.1.0')\r\n- Bazel version: 0.4.5\r\n- No GPU\r\n", "comments": ["The team has recently released a tool called `graph_transforms`, which contains (possibly more complete?) similar functionalities that optimize for various deployment use cases: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\r\n\r\nCan you take a look at this newer tool to see if it works?", "Thanks @concretevitamin, that worked great. The only quibble I have is that the `TransformGraph` tool is not included in the TensorFlow pip package so I had to add it to the `PYTHONPATH`. Do you know if `optimize_for_inference_lib` is to be removed from the pip package and if `TransformGraph` is to be added to the pip package?", "Great to know it works!  Adding @petewarden to answer these questions.", "I'm hoping transform_graph will be added by default to pip in #9778. Since that looks like the only remaining issue, closing as a duplicate of that one. Let me know if I've missed something!"]}, {"number": 9913, "title": "[XLA] Add F16 support to the Literal protobuf and LiteralUtils class.", "body": "No support has been added to any public back-end, however the unit tests demonstrate that the literals can store and retrieve data correctly.\r\n\r\n[Note: Resize needed to be moved so it could be used in a subsequent function]\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@hawkinsp  It's all yours.", "@tensorflow-jenkins test this please"]}, {"number": 9912, "title": "Bazel Build fails with missing input file error @grpc", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. Stock script is used.\r\n- **OS Platform and Distribution**: CentOS Linux release 7.2.1511. \r\n- **TensorFlow installed from** : Source.\r\n- **Bazel version**: 0.4.5\r\n- **CUDA/cuDNN version**: 7.5\r\n- **Exact command to reproduce**:\r\n./bazel build --config=opt --config=cuda --spawn_strategy=standalone --genrule_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI am installing Tensorflow 1.1 on a system where I do not have root priviliges. Upon running Bazel command (mentioned above), I get the following error - \r\n\r\n**missing input file '@grpc//:include/grpc++/impl/codegen/string_ref.h'.**\r\n\r\nI need to include the path of gprc to the Bazel. However, I do not know how to find the location of these files and how to refer them to Bazel. I have tried to find these files in the filesystem and bazel downloads, but couldnt find them.\r\n\r\n#### Another insteresting thing that is happening is - \r\nEverytime I run the bazel command again, it gives me different error each time ( However, they all are related to \"@grpc//: \". Some of the other errors I got were - \r\n\r\nERROR: missing input file '@grpc//:include/grpc++/impl/service_type.h'.\r\nERROR: missing input file '@grpc//:LICENSE'.\r\nERROR: missing input file '@grpc//:include/grpc++/server_builder.h'.\r\netc.\r\n\r\n### Error Log\r\n`\r\nERROR: missing input file '@grpc//:include/grpc++/impl/codegen/string_ref.h'.\r\nERROR: /home_dir/xxxxx/nn/installation/tensorflow/tensorflow/core/BUILD:1308:1: C++ compilation of rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 43 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.\r\nERROR: /home_dir/xxxxx/.cache/bazel/_bazel_xxxxx/feba725fe17da6f4082aa7bd94139644/external/grpc/BUILD.bazel:1664:1: @grpc//:grpc++_unsecure: missing input file '@grpc//:include/grpc++/impl/codegen/string_ref.h'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home_dir/xxxxx/.cache/bazel/_bazel_xxxxx/feba725fe17da6f4082aa7bd94139644/external/grpc/BUILD.bazel:1664:1 1 input file(s) do not exist.\r\nINFO: Elapsed time: 13.184s, Critical Path: 2.25s\r\n`", "comments": ["Bazel clean solved this issue.\r\n\r\nThanks."]}, {"number": 9911, "title": "Estimator predict() fails after fit() or evaluate()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, I did write an `input_fn` based code based on the [Abalone](https://www.tensorflow.org/extend/estimators) example. Specifically, I replaced the `x` and `y` parameters with custom `input_fn` implementations loading images and applying `map_fn` to them before batching. This resulted in a batch of unknown tensor shape (`None`).\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.1.0-1-g10ec24a\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0, 5.1\r\n- **GPU model and memory**: GeForce 1080 Ti\r\n- **Exact command to reproduce**:  running the code\r\n\r\n### Describe the problem\r\n When calling `predict()` after calling `fit()` or `evaluate()` on an `Estimator` instance (an unlikely scenario, but shown in the example), the call crashes with \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mmayer/dev/everybag/orientation_network/orientation.py\", line 169, in <module>\r\n    main()\r\n  File \"/home/mmayer/dev/everybag/orientation_network/orientation.py\", line 35, in main\r\n    as_iterable=False)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 281, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 565, in predict\r\n    as_iterable=as_iterable)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 857, in _infer_model\r\n    infer_ops = self._get_predict_ops(features)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1187, in _get_predict_ops\r\n    self._labels_info)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py\", line 164, in create_placeholders_from_signatures\r\n    return signatures.get_placeholder()\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py\", line 89, in get_placeholder\r\n    shape=[None] + list(self.shape[1:]))\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 478, in __iter__\r\n    raise ValueError(\"Cannot iterate over a shape with unknown rank.\")\r\nValueError: Cannot iterate over a shape with unknown rank.\r\n```\r\n\r\nif `self._labels_info` of the `Estimator` has unknown shape (e.g. as an effect of using `map_fn` on images to obtain both feature and target from them. This is due to\r\n\r\n```python\r\n  def get_placeholder(self):\r\n    if self.is_sparse:\r\n      return array_ops.sparse_placeholder(dtype=self.dtype)\r\n    return array_ops.placeholder(dtype=self.dtype,\r\n                                 shape=[None] + list(self.shape[1:]))\r\n```\r\n\r\nin `tensor_signature.py`, since `self.shape[1:]` is `None`.\r\n\r\nThe call comes through\r\n\r\n```python\r\n  def _get_predict_ops(self, features):\r\n    labels = tensor_signature.create_placeholders_from_signatures(\r\n        self._labels_info)\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.INFER)\r\n```\r\n\r\nwhich uses the labels to construct the inference graph, when it probably shouldn't.\r\n\r\nThe `predict()` operation succeeds if it is called without prior calls to `fit()` or `evaluate()` as in this case the method\r\n\r\n```python\r\ndef create_placeholders_from_signatures(signatures):\r\n  if signatures is None:\r\n    return None\r\n  if not isinstance(signatures, dict):\r\n    return signatures.get_placeholder()\r\n  return {\r\n      key: signatures[key].get_placeholder()\r\n      for key in signatures}\r\n```\r\n\r\nearly exits.\r\n\r\n\r\n### Source code / logs\r\n\r\nExample code that triggers the problem; requires images in `img` directory. The problem can be resolved by explicitly setting the `indices` Tensor's shape in `input_fn` using a `tf.reshape` to `[-1]`, but given that it works for training and evaluation and that inference probably shouldn't use the targets at all (especially not from previous runs), I consider this a bug.\r\n\r\n```python\r\nimport os\r\nimport time\r\nfrom typing import Dict, Optional, Any\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.learn as tfl\r\nimport tensorflow.contrib.slim as slim\r\nfrom tensorflow.contrib.slim.python.slim.nets import inception_v3 as inception\r\n\r\n\r\ndef main():\r\n    model_params = {'learning_rate': 0.001,\r\n                    'batch_size': 16,\r\n                    'num_epochs': None}\r\n\r\n    nn = tfl.Estimator(model_fn=model_fn, params=model_params)\r\n    nn.fit(input_fn=lambda: input_fn('img', params=model_params), steps=10)\r\n    ev = nn.evaluate(input_fn=lambda: input_fn('img', params=model_params), steps=1)\r\n    predictions = nn.predict(input_fn=lambda: input_fn('img', params=model_params), as_iterable=False)\r\n\r\n\r\ndef model_fn(features, targets, mode, params):\r\n    training = (mode == tfl.ModeKeys.TRAIN)\r\n    num_classes = 5\r\n\r\n    image = features['image']\r\n    image.set_shape((None, 299, 299, 3))\r\n    targets.set_shape((None, num_classes))\r\n\r\n    with tf.contrib.slim.arg_scope(inception.inception_v3_arg_scope()):\r\n        _, end_points = inception.inception_v3(image, is_training=training, num_classes=num_classes)\r\n\r\n        logits = end_points['Logits']\r\n        predictions = end_points['Predictions']\r\n\r\n    predictions_dict = {'predictions': predictions}\r\n\r\n    loss = tf.losses.softmax_cross_entropy(targets, logits)\r\n    eval_metric_ops = {\r\n        'rmse': tf.metrics.root_mean_squared_error(targets, predictions)\r\n    }\r\n\r\n    train_op = tf.contrib.layers.optimize_loss(\r\n        loss=loss,\r\n        global_step=tf.contrib.framework.get_global_step(),\r\n        learning_rate=params['learning_rate'],\r\n        optimizer='Adam')\r\n\r\n    return tfl.ModelFnOps(\r\n        mode=mode,\r\n        predictions=predictions_dict,\r\n        loss=loss,\r\n        train_op=train_op,\r\n        eval_metric_ops=eval_metric_ops)\r\n\r\n\r\ndef input_fn(dataset_dir, params):\r\n    pattern = os.path.join(dataset_dir, '*.jpg')\r\n    filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(pattern),\r\n        num_epochs=params['num_epochs'])\r\n\r\n    image_reader = tf.WholeFileReader()\r\n    _, image_file = image_reader.read_up_to(filename_queue, 10)\r\n\r\n    image_batch = tf.train.shuffle_batch([image_file], params['batch_size'], 1000, 100,\r\n                                         num_threads=2,\r\n                                         enqueue_many=True,\r\n                                         allow_smaller_final_batch=True)\r\n    images, indices = tf.map_fn(image_fn, image_batch,\r\n                                dtype=(tf.float32, tf.int32),\r\n                                infer_shape=False)\r\n\r\n    images = tf.reshape(images, [-1, 299, 299, 3])\r\n\r\n    features = {'image': images}\r\n    targets = tf.one_hot(indices=indices, depth=5, dtype=tf.float32)\r\n    return features, targets\r\n\r\n\r\ndef image_fn(image_file):\r\n    image = tf.image.decode_jpeg(image_file, channels=3)\r\n\r\n    image = tf.expand_dims(image, 0)\r\n    image = tf.image.resize_bilinear(image, [299, 299])\r\n    image = tf.squeeze(image, [0])\r\n\r\n    k = 2\r\n    image = tf.image.rot90(image, k=k, name='rotate')\r\n    image = tf.cast(image, tf.float32)\r\n    image = tf.subtract(tf.divide(image, 128.), 1.)\r\n\r\n    return image, k\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nFull output:\r\n\r\n```\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpv1o9e6jk\r\n2017-05-15 11:55:54.834870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-15 11:55:54.835292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6325\r\npciBusID 0000:01:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 9.24GiB\r\n2017-05-15 11:55:54.835305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-15 11:55:54.835309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-15 11:55:54.835315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)\r\n2017-05-15 11:56:03.923926: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1546 get requests, put_count=1200 evicted_count=1000 eviction_rate=0.833333 and unsatisfied allocation rate=0.935317\r\n2017-05-15 11:56:03.923951: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\r\n2017-05-15 11:57:49.153141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\nWARNING:tensorflow:From untitled.py:18: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nTraceback (most recent call last):\r\n  File \"untitled.py\", line 97, in <module>\r\n  File \"untitled.py\", line 18, in main\r\n    predictions = nn.predict(input_fn=lambda: input_fn('img', params=model_params), as_iterable=False)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 281, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 565, in predict\r\n    as_iterable=as_iterable)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 857, in _infer_model\r\n    infer_ops = self._get_predict_ops(features)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1187, in _get_predict_ops\r\n    self._labels_info)\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py\", line 164, in create_placeholders_from_signatures\r\n    return signatures.get_placeholder()\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py\", line 89, in get_placeholder\r\n    shape=[None] + list(self.shape[1:]))\r\n  File \"/home/mmayer/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 478, in __iter__\r\n    raise ValueError(\"Cannot iterate over a shape with unknown rank.\")\r\nValueError: Cannot iterate over a shape with unknown rank.\r\n```", "comments": ["Hi @sunsided, thank you for pointing this out. Would you be able to provide a simpler a script that reproduces the error that does not depend on images?\r\n\r\nThank you,\r\nAli", "I can try.", "@sunsided any update on a simple script to reproduce the issue?", "@gunan Sorry, had no time to do this so far. ", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 9910, "title": "Issues with spark-tensorflow-connector", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.1.0 and 1.0.0\r\n- **Bazel version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:None\r\n- **GPU model and memory**:None\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen using **spark-tensorflow-connector**, I run the given example and get the the TFRecord test-output.tfr. Then I want to use it in tensorflow, but some errors occur.\r\nhttps://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector\r\n\r\n### Source code / logs\r\n\r\nscala code\uff1a\r\n```scala\r\nimport org.apache.commons.io.FileUtils\r\nimport org.apache.spark.sql.{ DataFrame, Row }\r\nimport org.apache.spark.sql.catalyst.expressions.GenericRow\r\nimport org.apache.spark.sql.types._\r\n\r\nval path = \"test-output.tfr\"\r\nval testRows: Array[Row] = Array(\r\nnew GenericRow(Array[Any](11, 1, 23L, 10.0F, 14.0, \"r1\")),\r\nnew GenericRow(Array[Any](21, 2, 24L, 12.0F, 15.0, \"r2\")))\r\nval schema = StructType(List(StructField(\"id\", IntegerType), \r\n                             StructField(\"IntegerTypelabel\", IntegerType), \r\n                             StructField(\"LongTypelabel\", LongType), \r\n                             StructField(\"FloatTypelabel\", FloatType), \r\n                             StructField(\"DoubleTypelabel\", DoubleType), \r\n                             StructField(\"name\", StringType)))\r\n                             \r\nval rdd = spark.sparkContext.parallelize(testRows)\r\n\r\n//Save DataFrame as TFRecords\r\nval df: DataFrame = spark.createDataFrame(rdd, schema)\r\ndf.write.format(\"tfrecords\").save(path)\r\n\r\n//Read TFRecords into DataFrame.\r\n//The DataFrame schema is inferred from the TFRecords if no custom schema is provided.\r\nval importedDf1: DataFrame = spark.read.format(\"tfrecords\").load(path)\r\nimportedDf1.show()\r\n\r\n//Read TFRecords into DataFrame using custom schema\r\nval importedDf2: DataFrame = spark.read.format(\"tfrecords\").schema(schema).load(path)\r\nimportedDf2.show()\r\n```\r\n\r\npython code:\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nfileNameQue = tf.train.string_input_producer([\"/home/wj/test-output.tfr\"])\r\nreader = tf.TFRecordReader()\r\nkey, value = reader.read(fileNameQue)\r\nfeatures = tf.parse_single_example(value, features={\r\n        'id': tf.FixedLenFeature([], tf.int64),\r\n        'IntegerTypelabel': tf.FixedLenFeature([], tf.int64),\r\n        'LongTypelabel': tf.FixedLenFeature([], tf.int64),\r\n        'FloatTypelabel': tf.FixedLenFeature([], tf.float32),\r\n        'DoubleTypelabel': tf.FixedLenFeature([], tf.float32),\r\n        'name': tf.FixedLenFeature([], tf.string),\r\n    })\r\n# with tf.Session() as sess:\r\nsess = tf.InteractiveSession()\r\n\r\ncoord=tf.train.Coordinator()\r\nthreads= tf.train.start_queue_runners(coord=coord)\r\n#for i in range(2):\r\n# a,b,c,d,e,f = sess.run([id,IntegerTypelabel,LongTypelabel,FloatTypelabel,DoubleTypelabel,name])\r\na = sess.run(features)\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n```\r\nlogs:\r\n```\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-13-889826d0ffbd> in <module>()\r\n      7 #for i in range(2):\r\n      8 # a,b,c,d,e,f = sess.run([id,IntegerTypelabel,LongTypelabel,FloatTypelabel,DoubleTypelabel,name])\r\n----> 9 a = sess.run(features)\r\n     10 \r\n     11 \r\n\r\n/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    963     if final_fetches or final_targets:\r\n    964       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 965                              feed_dict_string, options, run_metadata)\r\n    966     else:\r\n    967       results = []\r\n\r\n/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1013     if handle is None:\r\n   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1015                            target_list, options, run_metadata)\r\n   1016     else:\r\n   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1033         except KeyError:\r\n   1034           pass\r\n-> 1035       raise type(e)(node_def, op, message)\r\n   1036 \r\n   1037   def _extend_graph(self):\r\n\r\nFailedPreconditionError: /home/wj/test-output.tfr\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]\r\n\r\nCaused by op u'ReaderReadV2', defined at:\r\n  File \"/home/wj/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/home/wj/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 653, in launch_instance\r\n    app.start()\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-6-7fd6f1c21551>\", line 1, in <module>\r\n    key, value = reader.read(fileNameQue)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 272, in read\r\n    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 410, in _reader_read_v2\r\n    queue_handle=queue_handle, name=name)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/wj/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nFailedPreconditionError (see above for traceback): /home/wj/test-output.tfr\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]\r\n```", "comments": ["Is it possible that the `/home/wj/test-output.tfr\"/home/wj/test-output.tfr` is somehow not readable by the TF program?  Can you try changing its permission to 777?\r\n\r\n@jhseu ", "@concretevitamin It\u2019s still not work...", "@jhseu can you comment or redirect to someone who can? Thanks!", "Can you try with a known valid input file? There's not enough information to go on.", "@jhseu Actually,the work I'm working on is to test if the TFRecords file that the spark-tensorflow-connector  outputs is valid,but I get this error...", "@aselle @jhseu It looks like he was trying to run the example of [spark-tensorflow-connecor](https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector). I don't know whether the problem is caused by the code of Spark. \r\nDo you have some good idea to test the input TFRecords file?", "@wj1066 Could you upload your TFRecords file to a public gist?", "what was the resolution for this one ? I am running into similar issue where in TFR file written by this sparktensorflow connector is not being read by python code."]}, {"number": 9909, "title": "Fixed _linear(.) to use *batch* matrix multiplication.", "body": "In the `_linear` function (from the `tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl` module), there's relatively subtle bug. The reason that this bug is subtle is that it only affects tensors used by `_linear` internally.\r\n\r\nFor future reference, the signature and docstring of `_linear` are:\r\n```python\r\ndef _linear(args, output_size, bias, bias_start=0.0):\r\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\r\n\r\n  Args:\r\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\r\n    output_size: int, second dimension of W[i].\r\n    bias: boolean, whether to add a bias term or not.\r\n    bias_start: starting value to initialize the bias; 0 by default.\r\n\r\n  Returns:\r\n    A 2D Tensor with shape [batch x output_size] equal to\r\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\r\n\r\n  Raises:\r\n    ValueError: if some of the arguments has unspecified or wrong shape.\r\n  \"\"\"\r\n```\r\n\r\n\r\n**The Problem.**\r\nFrom the docsting of `_linear`, the expected behavior of the function is that it should return an output `y` such that `y = sum_i(args[i] * W[i])`, indicating a batch-wise `matmul` (formerly known as `batch_matmul`). This wasn't actually how `_linear` was implemented, though. Instead, all `args` were concatenated and the weights were chosen to have rank 2, i.e.\r\n```python\r\nx = concat(args, axis=1)  # x.shape == (batch_size, total_arg_size)\r\nw = Variable(...)         # w.shape == (total_arg_size, output_size)\r\ny = matmul(x, w)          # y.shape == (batch_size, output_size)\r\n```\r\nNow, the main problem here is that **`w` is not block-diagonal**, which means that it contains more entries than it should. There are two problems with this:\r\n1. The **number of weights is too large** by a factor of `num_args`.\r\n2. The extra \"off-diagonal\" weights yield **spurious connections** in the net architecture of e.g. LSTMCell.\r\n\r\n**The Solution.**\r\nThe solution is to use batch-wise `matmul` instead. In order to do this, we need `x` and `w` to be rank-3 (rather than rank-2) tensors, e.g.\r\n```python\r\nx = stack(args, axis=0)                    # x.shape == (num_args, batch_size, input_size)\r\nw = Variable(...)                          # w.shape == (num_args, input_size, output_size / num_args)\r\ny = matmul(x, w)                           # y.shape == (num_args, batch_size, output_size / num_args)\r\ny = transpose(y, [1, 0, 2])                # y.shape == (batch_size, num_args, output_size / num_args)\r\ny = reshape(y, [batch_size, output_size])  # y.shape == (batch_size, output_size)\r\n```\r\n\r\n**Tests.**\r\nI ran `tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_impl_test.py`, but it was failing all over the place. I expected this, because the `_linear` is used in many of the rnn cell classes. The tests were failing in more places than just the place that covered `_linear`. Let me know if I need to update the unit tests too. Also, the change in the PR put somewhat more stringent constraints on the shapes of the input `args`, i.e. they must all be the same. In the previous implementation, only the axis-0 sizes (`batch_size`) needed to be the same. We could use padding and slicing if we need to be able to handle different axis-1 sizes (`input_size`).", "comments": ["Can one of the admins verify this patch?", "@KristianHolsheimer could you rebase and push again?", "Hi @drpngx, I just rebased", "Jenkins, test this please.", "the documentation of `_linear` is not correct; but the implementation *is*.  we want to concatenate args along the columns, create a variable that's [num_columns, output_size], and do a matmul against this variable.\r\n\r\nif you'd like to clean this up; please update the documentation of `_linear` in a separate PR.  anyway, it's a hidden function so the documentation shouldn't really break anyone (no one outside of tensorflow core is supposed to use it).", "@ebrevdo The docstring isn't the thing that's wrong here. The reason is that it's used by e.g. `LSTMCell`, which requires the behavior mentioned in the docsting, *not* the implementation that you suggest. If we keep the implementation like this, we are diverting from the standard LSTM architecture in two ways:\r\n\r\n1. We use `len(args)` more weights\r\n2. We allow for mixing between input and hidden state at each time step\r\n\r\nI suspect that the real reason we're doing this concatenation thing is (time) optimization. So, if we're willing to have a non-standard definition of LSTM in tensorflow, we could decide to leave it like this, but *we need to be honest and make this choice consciously*, not as an unintended by-product of a low-level optimization trick.\r\n\r\nFor the record, my choice would be to use the definition of LSTM that is used by the rest of the community (as per this pull request).\r\n\r\n", "Also, I'm currently updating the unit tests that are failing because of this. As you can imagine, it touches quite a bit of code, so please give me a little bit of time.", "My reading of the equations is that the input and hidden state should be\nmixed in this way at each time step\n\nhttps://en.m.wikipedia.org/wiki/Long_short-term_memory\n\nWe also compare to cudnn implementation here and ensure we have identical\nvalues within machine precision:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\n\nFunction testCudnnCompatibleRnnCells (the lstm compatible cell is just a\nsubclass of LSTMCell with forget_bias=0).\n\nOn Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> The docstring isn't the thing\n> that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n> requires the behavior mentioned in the docsting, *not* the implementation\n> that you suggest. If we keep the implementation like this, we are diverting\n> from the standard LSTM architecture in two ways:\n>\n>    1. We use len(args) more weights\n>    2. We allow for mixing between input and hidden state at each time step\n>\n> I suspect that the real reason we're doing this concatenation thing is\n> (time) optimization. So, if we're willing to have a non-standard definition\n> of LSTM in tensorflow, we could decide to leave it like this, but *we\n> need to be honest and make this choice consciously*, not as an unintended\n> by-product of a low-level optimization trick.\n>\n> For the record, my choice would be to use the definition of LSTM that is\n> used by the rest of the community (as per this pull request).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316598058>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b>\n> .\n>\n", "Perhaps it's confusing because we are right multiplying by the parameters,\nunlike the typical format?  Tf data is row major and batch is the major\ndimension, which is why everything is transposed.\n\nOn Jul 19, 2017 10:28 PM, \"ebrevdo\" <notifications@github.com> wrote:\n\n> My reading of the equations is that the input and hidden state should be\n> mixed in this way at each time step\n>\n> https://en.m.wikipedia.org/wiki/Long_short-term_memory\n>\n> We also compare to cudnn implementation here and ensure we have identical\n> values within machine precision:\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\n>\n> Function testCudnnCompatibleRnnCells (the lstm compatible cell is just a\n> subclass of LSTMCell with forget_bias=0).\n>\n> On Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" <notifications@github.com>\n> wrote:\n>\n> > @ebrevdo <https://github.com/ebrevdo> The docstring isn't the thing\n> > that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n> > requires the behavior mentioned in the docsting, *not* the implementation\n> > that you suggest. If we keep the implementation like this, we are\n> diverting\n> > from the standard LSTM architecture in two ways:\n> >\n> > 1. We use len(args) more weights\n> > 2. We allow for mixing between input and hidden state at each time step\n> >\n> > I suspect that the real reason we're doing this concatenation thing is\n> > (time) optimization. So, if we're willing to have a non-standard\n> definition\n> > of LSTM in tensorflow, we could decide to leave it like this, but *we\n> > need to be honest and make this choice consciously*, not as an unintended\n> > by-product of a low-level optimization trick.\n> >\n> > For the record, my choice would be to use the definition of LSTM that is\n> > used by the rest of the community (as per this pull request).\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/pull/\n> 9909#issuecomment-316598058>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_\n> 3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316600605>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5norvEi8o3RDemFE3pwShzFiKC4ks5sPuWZgaJpZM4Naw8b>\n> .\n>\n", "Okay thanks, I'll have another look at it and come back to you.", "Wow, you're right, the same happens in `cudnn_rnn_ops`.  Okay, this is a bit bigger than I initially thought. I'll have to dig into it later this week.\r\n\r\nThanks again @ebrevdo ", "Okay, I did look into it in more detail now. I agree with your earlier comment. Now that I see it, I don't understand how I was confused about it...\r\n\r\nThanks @ebrevdo"]}, {"number": 9908, "title": "Make possible to use static libraries generated by tfcompile in MSVC.", "body": "Currently it is possible to generate MSVC-compatible static libraries from a\r\nLinux build of tfcompile using the target triple \"x86_64-pc-windows-msvc\".\r\n\r\nHowever, just linking such library does not work out-of-the-box because of a\r\ntrivial class -> struct issue that becomes a compiler error in MSVC and because\r\nmultiple other symbols are missing.\r\n\r\nThe files defining these symbols must be manually built into the MSVC project,\r\nbut many have minor incompatibilities that prevent them to be built.\r\n\r\nThis patch makes a few minor modifications to these files, making it possible\r\nto link and run Tensorflow graphs generated with tfcompile in MSVC in Windows.\r\n\r\nThe files that might need to be built into the MSVC project are as follows:\r\n\r\n1. For symbols required by the generated headers.\r\n - tensorflow/compiler/aot/runtime.cc\r\n - tensorflow/compiler/xla/executable_run_options.cc\r\n\r\n2. For symbols that provide custom XLA operations.\r\n - tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int32.cc\r\n - tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int64.cc\r\n - tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc\r\n - tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc\r\n\r\n3. For symbols required by SimpleResolver in xla/service/cpu/simple_orc_jit.cc.\r\n - tensorflow/compiler/xla/service/cpu/runtime_matmul.cc\r\n - tensorflow/compiler/xla/service/cpu/runtime_single_threaded_matmul.cc\r\n - tensorflow/compiler/xla/service/cpu/runtime_conv2d.cc\r\n - tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc\r\n - tensorflow/compiler/xla/service/cpu/cpu_runtime.cc\r\n\r\nAdditionally, the following files might also required by SimpleResolver, but are\r\nnot included in this patch because there is no simple alternative to\r\n__attribute__((vector_size(x))) and __attribute__((weak)) in MSVC.\r\n\r\n - tensorflow/compiler/xla/service/cpu/cpu_runtime_sse4_1.cc\r\n - tensorflow/compiler/xla/service/cpu/cpu_runtime_avx.cc\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/8310 for more information.", "comments": ["Can one of the admins verify this patch?", "Changes done. Sorry, I force-pushed them without knowing it would affect the review.", "Also fixed some tabs in this pull request that MSVC put into a couple of the files. Whitespace should be ok now.", "Jenkins, test this please.", "@tatatodd can you approve this?", "Thanks again for the fixes, and details like the whitespace!  :)"]}, {"number": 9907, "title": "Update input_fn.md", "body": "a simple, self-contained example for integrating an external python reader for use with tf.contrib.learn.Estimator", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@roi-f can you please sign the cla?", "CLA signed\nSorry for the delay \n\n\u2014-\u2014-\u2014-\u2014-\u2014\n\nPlease excuse typos and brevity due to touchscreen keyboard.\n\nSent from my iPhone\n\n> On May 19, 2017, at 9:12 PM, Rasmus Munk Larsen <notifications@github.com> wrote:\n> \n> @roi-f can you please sign the cla?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "CLAs look good, thanks!\n\n<!-- ok -->", "@roi-f could you address the comments?", "@roi-f ping.", "hi\r\nsorry for the delay... work :S\r\ni'll sit on it now and see how to address the comments.\r\nhowever, @frankchn  @drpngx : as someone who struggled a lot with the high level modules and the lacking documentation, I created this example to demonstrate many aspects which were not clear, including how to use the general estimator...\r\n\r\n\r\nRoi Frenkel\r\nAlgorithm Engineer\r\n", "@roi-f Closing this out because it's been a while since the last update. Please reopen or send a new pull request if you'd still like to make the change. Thanks!"]}, {"number": 9906, "title": "tf.import_graph_def() restricts the order of nodes in graph proto.", "body": "-----------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\ncommit 3bee923c9\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\ncuda 8.0/cudnn 5.1.5\r\n- **GPU model and memory**:\r\nTesla P40 \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI used auto parallel optimizer in grappler for cifar10 example, but grappler changes the order of nodes in graph proto. It causes the failure of import_graph_def, because ops.set_shapes_for_outputs(op), which is line 407 of tensorflow/tensorflow/python/framework/importer.py is failed. This is because all of the input nodes must be defined before to define a node. importer restricts the order of nodes for graph definition proto, but I think importer shoud be flexible to the order of nodes. \r\n\r\n### Source code / logs\r\n  ```\r\nwith tf.Graph().as_default() as graph:\r\n    #global_step = tf.contrib.framework.get_or_create_global_step()\r\n    global_step = tf.get_variable(\r\n        'global_step', [], dtype=tf.int32,\r\n        initializer=tf.constant_initializer(0), trainable=False)\r\n\r\n    # Get images and labels for CIFAR-10.\r\n    images, labels = cifar10.distorted_inputs()\r\n\r\n    # Build a Graph that computes the logits predictions from the\r\n    # inference model.\r\n    logits = cifar10.inference(images)\r\n\r\n    # Calculate loss.\r\n    loss = cifar10.loss(logits, labels)\r\n\r\n    # Build a Graph that trains the model with one batch of examples and\r\n    # updates the model parameters.\r\n    train_op = cifar10.train(loss, global_step)\r\n\r\n    init_op = tf.global_variables_initializer()\r\n\r\n    queue_runners = []\r\n    for qr in ops.get_collection(ops.GraphKeys.QUEUE_RUNNERS):\r\n      queue_runners.append(qr.to_proto())\r\n\r\n    mg = meta_graph.create_meta_graph_def(graph=graph)\r\n\r\n  #Auto-parallel\r\n  rewriter_config = rewriter_config_pb2.RewriterConfig()\r\n  rewriter_config.optimizers.append('autoparallel')\r\n  rewriter_config.auto_parallel.num_replicas = FLAGS.num_gpus\r\n\r\n  graph_def = tf_optimizer.OptimizeGraph(rewriter_config, mg)\r\n\r\n  with tf.Graph().as_default() as g:\r\n    tf.import_graph_def(graph_def=graph_def, name='')\r\n```\r\n\r\nThis is the log\r\n```\r\n File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow_rdag/examples/distributed/cifar10/cifar10_in_graph_auto_parallel.py\", line 157, in main\r\n    train()\r\n  File \"tensorflow_rdag/examples/distributed/cifar10/cifar10_in_graph_auto_parallel.py\", line 105, in train\r\n    tf.import_graph_def(graph_def=graph_def, name='')\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 404, in import_graph_def\r\n    ops.set_shapes_for_outputs(op)\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1723, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1673, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 653, in _call_cpp_shape_fn_impl\r\n    v = tensor_util.constant_value(op.inputs[idx])\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 710, in constant_value\r\n    ret = _ConstantValue(tensor)\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 676, in _ConstantValue\r\n    fill_value = constant_value(tensor.op.inputs[1])\r\n  File \"/cmsdata/ssd0/soojeong/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1443, in __getitem__\r\n    return self._op._inputs[i]\r\nIndexError: list index out of range\r\n\r\n```", "comments": ["@benoitsteiner: mind taking a look?  Do we have plans to modify `tf.import_graph_def()` for the desired effects?  ", "The easiest solution is probably to update the grappler optimizers to ensure that the final output is generated in topological order. The one difficulty is going to figure out where to break looks to make TensorFlow happy.  The natural place would be the NextIteration node.\r\n", "@sj6077, it's great/exciting to see you are using this feature! \r\n\r\nWhile we are still actively developing it and improving its usability and scalability, I hit the same order problem you described when applying it to the RNN PTB model. I'm working on a fix to sort the nodes of an optimized graph in topological order, which will be submitted soon.\r\n\r\nWe'd be glad to hear more of your feedback and suggestions later.", "Thank you. I'll look forward upgraded grappler optimizers!", "The change (https://github.com/tensorflow/tensorflow/commit/194b1644cba446d36c6d192771b11245847db7cc) is in. Please give it a try. ", "I tried, but there is another issue. I copied cifar10 model, but it doesn't utilize all the gpus. For example, conv2D is not copied for all gpus. I also got ` tensorflow/core/common_runtime/simple_placer.cc:668] Ignoring device specification /device:GPU:0 for node 'AutoParallel-Replica-0/GradientDescent' because the input edge from 'global_step_1' is a reference connection and already has a device field set to /device:GPU:2`\r\n\r\nCan you give an example?", "All transitive fanin nodes of target nodes should be copied/replicated for all GPUs. I think Conv2D is very likely to be in the transitive fanin and therefore should be replicated. Could you give more details on this and debug the AutoParallel code a bit to find out why it is not replicated?\r\n\r\nI'm aware of the simple_placer issue and will work on a fix. For now, a temporary workaround is\r\nconfig_proto = tf.ConfigProto(allow_soft_placement=True)\r\n\r\nYes, I will submit an example on ptb RNN model soon.", "I'm not sure about Conv2D issue now, I'll figure out it more. I'm now trying to initialize variables, but I couldn't find an easy way. There are `local3/weights/ExponentialMovingAverage/Assign_1 ` and `local3/weights/ExponentialMovingAverage/Assign`, I don't know what's the meaning.\r\n\r\nAnother issue is queue_runner is not made for new operations, so with `MonitoredTrainingSession`, it doesn't work. \r\n\r\nIs it possible for multi-machines and multi-gpus? I want to add tf.train.SyncReplicasOptimizer somewhere. \r\n\r\nI'll wait for your example, but can you let me know about 2 issues related with initializer and queue runner before?", "These two Assign nodes could be initializers for two variables. There are two collections in MetaGraphDef, \"trainable_variables\" and \"variables\", where you have a list of VariableDef including a field initializer_name for the corresponding initialization node. After you import the MetaGraphDef, these two collections should be preserved and you can run all initialization nodes in VariableDef.\r\n\r\nCould you be more specific about why queue_running doesn't work with MonitoredTrainingSession? Looks like MonitoredTrainingSession uses queue runner. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/monitored_session.py#L541\r\n\r\nYes, follow-up work will support multiple machines with both synchronous and async training; it may take some time for us to get there. For now, I'm not sure if using AutoParallel within a machine on multiple devices could be combined with a machine-level parallelization with SyncReplicasOptimizer; looks like they two should be compatible.", "Below is the code that I tried. The problem is collections in meta_graph is not updated for new graph from auto_parallel. So I can't access variables or queue_runner for new graph. Moreover, snapshot ops for variables, and operations of condition context collection are removed through auto_parallel.\r\n\r\n```\r\n with tf.Graph().as_default():\r\n    meta_graph_def = meta_graph_pb2.MetaGraphDef()\r\n    meta_graph_def.ParseFromString(meta_graph_def_str)\r\n    tf.import_meta_graph(meta_graph_def)\r\n\r\n    rewriter_config = rewriter_config_pb2.RewriterConfig()\r\n    rewriter_config.optimizers.append('autoparallel')\r\n    rewriter_config.auto_parallel.num_replicas = num_replicas\r\n\r\n    graph_def = tf_optimizer.OptimizeGraph(rewriter_config, meta_graph_def)\r\n    tf.import_graph_def(graph_def=graph_def, name='')\r\n\r\n    meta_graph_def = tf.train.export_meta_graph()\r\n```\r\n", "Yes, the collections are not updated and thus need manually updated currently. This is cumbersome and we will definitely think of ways to improve (will first get a working, proof-of-concept ptb example, and then improve/streamline the usage). In particular, you are exactly right that the snapshot name in VariableDef needs to be updated. For ptb, the names of all tensors (the states) that the model refer need to be updated (see the code below).\r\n\r\nI will just post the ptb_word_lm.py  code below (before I submit it) for your reference. Could you try follow its usage pattern used here?\r\n\r\nhttps://gist.github.com/zhangyaobit/08c0b776e4b7a4f786714d46e723ee8a\r\n", "Thank you for your example! It's really helpful. \r\n\r\nI tested your example, but it has no speed up acrroding to the number of gpus increases. Is it just a problem of log?\r\n```\r\nif verbose and step % (model.input.epoch_size // 10) == 10:\r\n      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\r\n            (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),\r\n             iters * model.input.batch_size / (time.time() - start_time)))\r\n\r\n```", "Yes, I believe it is a problem with logging. If you use tensorboard, you can compare global steps/sec and time to certain accuracy/loss.\r\n", "I found 2 issues related with auto-parallel.\r\nIt doesn't replicate queues and euqueue ops, but if input pipeline contains a work for data-preprocessing, gpus are utilized as serial orders.\r\nOne more thing is that apply gradient descent is a long time task, below is the tracing result, the computation time is less than 100ms but apply gradient descent takes more than 100ms. Is there any dpendencies between different apply gradient descent ops for same variables? \r\n\r\n![2017-05-29 16 43 25](https://cloud.githubusercontent.com/assets/2465713/26540790/f80d57b0-448d-11e7-999f-50e3c666a8dd.png)\r\n", "The queue and enqueue ops don't need to be replicated; only dequeue node is replicated, so that multiple GPUs could dequeue/work in parallel. If there is a lot of preprocessing for input data, you may consider using more threads for input preprocessing, so that it keeps up the GPU computations.\r\n\r\nThe time for applying gradients depends on the models. For small models, it may become a bottleneck; for larger models like the PTB one, it takes relatively a small portion of total time, and you can expect about 1.7x speedup using 2 GPUs. Could you label each row in the picture, and name/mark the related ops?\r\n\r\n", "Closing for now; feel free to reopen if needed.", "zhangyaobit,\r\nI know you seem to have fixed this issue with help from sj6077. I am seeing the same issue while running PTB example with tensorflow 1.2.1. Could you please confirm if these changes are already part of 1.2.1 release ? If not, what's the workaround ?\r\n", "I took a look. This change is not in 1.2.1; it is in 1.3.0. Are you able to use 1.3.0?\r\n\r\nWhat's the error message do you see (could you post the stack trace)?", "Hi zhangyaobit, I have tried with the ptb example with update collection utility,   I still get this error under version 1.4.1\r\n\r\n> Traceback\r\n> \r\n>  (most recent call last):\r\n>   File \"ppmodel.py\", line 558, in <module>\r\n>     tf.app.run()\r\n>   File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"ppmodel.py\", line 524, in main\r\n>     tf.train.import_meta_graph(metagraph)\r\n>   File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1810, in import_meta_graph\r\n>     **kwargs)\r\n>   File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 696, in import_scoped_meta_graph\r\n>     ops.prepend_name_scope(value, scope_to_prepend_to_names))\r\n>   File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3035, in as_graph_element\r\n>     return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n>   File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3077, in _as_graph_element_locked\r\n>     \"graph.\" % (repr(name), repr(op_name)))\r\n> KeyError: \"The name 'Train/TrainInput/StridedSlice_1:0' refers to a Tensor which does not exist. The operation, 'Train/TrainInput/StridedSlice_1', does not exist in the graph.\"\r\n>   "]}, {"number": 9905, "title": "Wrong hyperlink in web page tutorial", "body": "There is a wrong html link in text in the tutorial \"Vector Representations of Words\" at address https://www.tensorflow.org/tutorials/word2vec\r\n\r\nThe text is in the first paragraph after the bullet points in the Highlights section at the top of the page.\r\n\r\nThe link for:\r\n\r\n**tensorflow_models/tutorials/embedding/word2vec.py** \r\nhttps://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py\r\n\r\nis wrong, it should point to the following address:\r\n\r\nhttps://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\r\n\r\nI hope that this is helpful.", "comments": ["Thank you for the heads up, @Craigjw. We're on it.", "This is fixed in 1.2 and master already; this will be fixed at this location when 1.2 is made the current binary version.  We can cherrypick back to 1.1, but I don't think it's worth the trouble.\r\n\r\nAs a passing note, we should not link from 1.1 narrative docs to master code samples; this has caused confusion in the past, as the master code may not work with 1.1.  It should link to the 1.1 branch (which it's trying to do here, but there was a problem with the link format).\r\n\r\nAgain, this will be fixed very soon.", "This has propagated out to the root of tensorflow.org, so I'm closing.  Thanks."]}, {"number": 9904, "title": "BeamSearchDecoder cell state never changed ", "body": "tf version '1.1.0-rc2'\r\nIt looks BeamSearchDecoder never used next_cell_state, but always using the inital cell state.\r\nIn beam_search_decoder.py 423\r\n        next_cell_state = nest.map_structure(\r\n            self._maybe_split_batch_beams,\r\n            next_cell_state, self._cell.state_size)\r\nBut next_cell_state is never used later, since just pass   state to _beam_search_step function where state.cell_state is never changed \r\n    beam_search_output, beam_search_state = _beam_search_step(\r\n          time=time,\r\n          logits=cell_outputs,\r\n          beam_state=state,\r\n          batch_size=batch_size,\r\n          beam_width=beam_width,\r\n          end_token=end_token,\r\n          length_penalty_weight=length_penalty_weight)", "comments": ["I wrote the code using BeamSearchDecoder, and struggling to solve the issue that the first step's output looks quite OK but later steps' output seems strange (similar to first step's output). This seems to be the reason why.. ", "Also I think we need to gather new cell state according to next_beam_ids, if using attention also need to do this for attention", "Also there are still bugs if fixing above problem. For BeamSearchDecoder design, it is dynamic and stop when all facing EOS, but is it good for doing beam search by this way? I think done beams should not competing with non finished beams.  Take a small example, comparing the result of out graph beam search(beam_width 2) and beam search by BeamSearchDecoder:\r\nfor out graph beam search(models\\im2txt\\im2txt\\inference_utils\\caption_generator.py) I got:\r\n0 [936, 2] \u53e4\u4ee3/[EOS] 0.000512714519986 -7.57579 -7.57579 [-3.9758704, -3.599921]\r\n1 [1, 2] [UNK]/[EOS] 0.000276204156072 -8.19437 -8.19437 [-4.6221266, -3.5722442]\r\nfor BeamSearchDecoder, I print predicted_ids, parent_ids and sequence_lengths,  and I transpose final beam search output predicted_ids [0, 2, 1] to print the top 2 best paths.\r\npredicted_ids:  \r\n[[[936   1]\r\n  [  8   2]\r\n  [  2   2]]]\r\nparent_ids: \r\n[[[0 0]\r\n  [0 0]\r\n  [1 0]]]\r\nsequence_lengths:\r\n[[3 2]]\r\ntop2 best paths:\r\n[936   2   2] \u53e4\u4ee3/[EOS] 0.000512714 -7.57579140233\r\n[936   2  -1] \u53e4\u4ee3/[EOS] 0.000134506 -8.91390228447\r\n\r\nWhich is not as good as out graph result, you see the path [1, 2] will be ignored since we have \r\n[936, 8] [936, 2] occupy all 2 beam_width positions... (finished competing with non finished)\r\nAnd the final result has [936, 2, 2] [936, 2, -1] which is also not correct.\r\nI think should be [936, 2, -1] ([936,2,2]) and [936,8,2] with sequence_lengths [[2, 3]]\r\n\r\nIf considering length penalty, we can not assume early finished shorter path be better then longer ones.\r\nSo may be non dynamic version, just loop max_steps and return top N finished paths will be better.", "@ebrevdo mind taking a look?", "I seem to encounter the same problem. Some help would be much appreciated.", "Looking.", "Thanks for finding this rather egregious bug.  We've got a bugfix ready and\nhope to submit it today, and merge it into the TF1.2 branch.  We'll also be\nadding some better (functional) unit tests in the coming week.\n\nOn Thu, May 18, 2017 at 9:18 AM, ebrevdo <notifications@github.com> wrote:\n\n> Looking.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9904#issuecomment-302459998>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6k7v2RCe9nv4jEnLvL5srO8_0CPks5r7G9XgaJpZM4Nap4A>\n> .\n>\n", "Nagging Assignee @ebrevdo: It has been 444 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "closing this issue, seems eager mode can do better beam search.", "To be clear, we fixed this bug last year"]}, {"number": 9903, "title": "Mac nightlies blocked by failing test in contrib", "body": "Last Mac nightly is May 1st\r\n\r\nLooking at the last failure it seems it was caused by contrib\r\n//bazel_pip/tensorflow/contrib/batching:batch_ops_test                   FAILED in 1 out of 2 in 14.5s\r\n\r\nSince /contrib/ packages don't always have maintainers, should the \"Success\" criteria perhaps be relaxed to still release the pip package if some /contrib/ thing fail? @martinwicke \r\n\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/\r\n\r\n", "comments": ["The problem here is actually that all our Mac workers are offline. We'll\nmake new nightlies as soon as they come back.\n\n\nOn May 14, 2017 7:23 PM, \"Yaroslav Bulatov\" <notifications@github.com>\nwrote:\n\n> Last Mac nightly is May 1st\n>\n> Looking at the last failure it seems it was caused by contrib\n> //bazel_pip/tensorflow/contrib/batching:batch_ops_test FAILED in 1 out of\n> 2 in 14.5s\n>\n> Since /contrib/ packages don't always have maintainers, should the\n> \"Success\" criteria perhaps be relaxed to still release the pip package if\n> some /contrib/ thing fail? @martinwicke <https://github.com/martinwicke>\n>\n> https://ci.tensorflow.org/view/Nightly/job/nightly-\n> matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_\n> BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9903>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_bND8mH7XvYTem1CZtueSFFE34r-ks5r57c1gaJpZM4NamUQ>\n> .\n>\n"]}, {"number": 9902, "title": "Session#run method's feed_dict argument implicitly converts types", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes?\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.12.3\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: (see below)\r\n\r\n### Describe the problem\r\nHi TensorFlow humans! Thanks so much for making TensorFlow!\r\n\r\nRight now, if you feed a floating point array into a integral placeholder type, it will be converted implicitly. To my knowledge, most python operations will not implicitly convert.\r\n\r\nThe implicit conversion potentially creates convenience, but of course it also creates the opportunity for a hard-to-see bug. In my case, I lost about 1 day to find this bug and experienced great sadness. That probably says more about me than it does about TF.\r\n\r\nStill, my feeling is that it is a more sensible default to require the user to do the conversion explicitly. Alternatively, perhaps it would be logical to log a warning to the user.\r\n\r\nNote that, to my knowledge, TF operations like `tf.equal` require both tensors to have the same type. So users might have a belief that that TF and `Session#run` require them to be fairly explicit about types.\r\n\r\nIf it would be likely to be accepted, I am happy to write a patch for TF that warns the user when they feed a tensor of the wrong type.\r\n\r\nThanks for reading this issue!\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsession = tf.Session()\r\nconvert_implicitly = tf.placeholder(tf.int64, [None])\r\nfloat_input = np.random.uniform(size = 10)\r\nresult = session.run(\r\n    convert_implicitly,\r\n    feed_dict = { convert_implicitly: float_input }\r\n)\r\nprint(result)\r\n```\r\n\r\n**Output**\r\n\r\n```\r\n>> python test.py\r\n2017-05-14 18:20:22.524862: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-14 18:20:22.524890: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-14 18:20:22.524906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine andcould speed up CPU computations.\r\n[0 0 0 0 0 0 0 0 0 0]\r\n```\r\n", "comments": ["I would say this is a bug, silently turning floats into ints is surprising\r\n\r\nThere was a similar one fixed by @suharshs where he added some code [here](https://github.com/tensorflow/tensorflow/blob/a5b1fb8e56ceda0ee2794ee05f5a7642157875c5/tensorflow/python/framework/tensor_util.py#L391) to prevent python integers >2^31 being silently downcasted to int32.", "@martinwicke is changing our implicit conversion rules in placeholders breaking our API guarantees? I agree this is confusing and dangerous, but in theory some may rely on this behavior...", "@yaroslavvb, the fix was actually by @saxenasaurabh :)", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Yeah, this is likely to break people. We should add a warning if we can, and we will change this behavior in 2.0.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I will close this since we cannot do anything before 2.0. I noted this issue for 2.0."]}, {"number": 9900, "title": "DNNClassifier and GridSearchCV example", "body": "Hi , \r\n\r\nI am trying to run an gridsearch example in [https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/contrib/learn/python/learn/tests/test_grid_search.py](url)\r\nIf example inside testIrisDNN intend to be working ?\r\n\r\nI am getting \r\n\r\n> TypeError: __init__() got an unexpected keyword argument 'params'", "comments": ["This bug is against a temporary fork of the TensorFlow source code I did for development purposes unfortunately, so it's not going to be possible to debug what's going wrong here. Closing for now, but please reopen if you are able to reproduce this on the main https://github.com/tensorflow/tensorflow source code.", "Hi, I ran same code in grid_search_test.py and got the same error in TF 1.3.0, scikit-learn 0.19\r\nHave you any idea?\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import learn\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn import datasets\r\niris = datasets.load_iris()\r\nfeature_columns = learn.infer_real_valued_columns_from_input(iris.data)\r\nclassifier = learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3)\r\ngrid_search = GridSearchCV(classifier, {'hidden_units': [[5, 5], [10, 10]]}, scoring='accuracy', fit_params={'steps': [50]})\r\ngrid_search.fit(iris.data, iris.target)\r\n\r\n\r\n/Users/rickypark/anaconda/envs/handson-ml/lib/python3.6/site-packages/sklearn/model_selection/_search.py:583: DeprecationWarning: \"fit_params\" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the \"fit\" method instead.\r\n  '\"fit\" method instead.', DeprecationWarning)\r\n\\---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-10-995caf033e7d> in <module>()\r\n----> 1 grid_search.fit(iris.data, iris.target)\r\n\r\n~/anaconda/envs/handson-ml/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params)\r\n    622                                      n_candidates * n_splits))\r\n    623\r\n--> 624         base_estimator = clone(self.estimator)\r\n    625         pre_dispatch = self.pre_dispatch\r\n    626\r\n\r\n~/anaconda/envs/handson-ml/lib/python3.6/site-packages/sklearn/base.py in clone(estimator, safe)\r\n     60     for name, param in six.iteritems(new_object_params):\r\n     61         new_object_params[name] = clone(param, safe=False)\r\n---> 62     new_object = klass(**new_object_params)\r\n     63     params_set = new_object.get_params(deep=False)\r\n     64\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'params'"]}, {"number": 9899, "title": "Multiplicative Integration Recurrent Neural Networks", "body": "This is the same PR as #9286(closed) after I\r\n   - corrected my email address(because of CLA: No)\r\n   - resolved conflicts\r\n   - modified __init__.py\r\n\r\nI implemented Multiplicative Integration variants of recurrent neural networks\r\n(RNN, GRU and LSTM) proposed in\r\n\r\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\r\nOn Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\r\nhttps://arxiv.org/abs/1606.06630\r\n\r\nThe RNNs proposed in the paper are implemented as:\r\n\r\n    MultiplicativeIntegrationRNNCell\r\n    MultiplicativeIntegrationGRUCell\r\n    MultiplicativeIntegrationLSTMCell\r\n    _multiplicative_integration as a helper function\r\n\r\nin\r\n    tensorflow/contrib/rnn/python/ops/rnn_cell.py\r\n    tensorflow/contrib/rnn/__init__.py\r\n\r\nTest codes:\r\n    tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py\r\n", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I dont know why CLA is still no. So I close this PR.\r\nSorry.\r\n"]}, {"number": 9898, "title": "Support for float16 in tf.layers", "body": "It seems like `tf.layers.*` do not support `tf.float16`, because they rely on the default value handed to the `_Layer` superclass [here](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/layers/base.py#L64). It would be great if tf.float16 could be supported there.", "comments": ["@fchollet ", "You can always instantiate a layer with `dtype=tf.float16`, causing its weights to be created in fp16, making it callable on fp16 inputs. Any layer would work.", "The layers in `tf.layers` do not accept a `dtype` argument, that is my whole point.", "All layers support the `dtype` argument. But I see what you mean now. You mean \"the functional interfaces to core layers don't have the dtype argument in their signature\". That's right. I would suggest adding it.", "I am interested in taking up this issue. Should I send in a pull request?", "@skye This issue can be closed now because fp16 support has been already added by commit cc1a02d."]}, {"number": 9897, "title": "Fixed _linear(.) to use *batch* matrix multiplication.", "body": "In the `_linear` function (from the `tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl` module), there's relatively subtle bug. The reason that this bug is subtle is that it only affects tensors used by `_linear` internally.\r\n\r\nFor future reference, the signature and docstring of `_linear` are:\r\n```python\r\ndef _linear(args, output_size, bias, bias_start=0.0):\r\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\r\n\r\n  Args:\r\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\r\n    output_size: int, second dimension of W[i].\r\n    bias: boolean, whether to add a bias term or not.\r\n    bias_start: starting value to initialize the bias; 0 by default.\r\n\r\n  Returns:\r\n    A 2D Tensor with shape [batch x output_size] equal to\r\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\r\n\r\n  Raises:\r\n    ValueError: if some of the arguments has unspecified or wrong shape.\r\n  \"\"\"\r\n```\r\n\r\n\r\n**The Problem.**\r\nFrom the docsting of `_linear`, the expected behavior of the function is that it should return an output `y` such that `y = sum_i(args[i] * W[i])`, indicating a batch-wise `matmul` (formerly known as `batch_matmul`). This wasn't actually how `_linear` was implemented, though. Instead, all `args` were concatenated and the weights were chosen to have rank 2, i.e.\r\n```python\r\nx = concat(args, axis=1)  # x.shape == (batch_size, total_arg_size)\r\nw = Variable(...)         # w.shape == (total_arg_size, output_size)\r\ny = matmul(x, w)          # y.shape == (batch_size, output_size)\r\n```\r\nNow, the main problem here is that **`w` is not block-diagonal**, which means that it contains more entries than it should.\r\n\r\n**The Solution.**\r\nThe solution is to use batch-wise `matmul` instead. In order to do this, we need `x` and `w` to be rank-3 (rather than rank-2) tensors, e.g.\r\n```python\r\nx = stack(args, axis=0)  # x.shape == (num_args, batch_size, input_size)\r\nw = Variable(...)        # w.shape == (num_args, input_size, output_size / num_args)\r\ny = matmul(x, w)         # y.shape == (num_args, batch_size, output_size / num_args)\r\ny = reshape(transpose(y, [1, 0, 2]), [batch_size, output_size])\r\n```\r\n\r\n**Tests.**\r\nI ran `tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_impl_test.py`, but it was failing all over the place. I expected this, because the `_linear` is used in many of the rnn cell classes. The tests were failing in more places than just the place that covered `_linear`. Let me know if I need to update the unit tests too. Also, the change in the PR put somewhat more stringent constraints on the shapes of the input `args`, i.e. they must all be the same. In the previous implementation, only the axis-0 sizes (`batch_size`) needed to be the same. We could use padding and slicing if we need to be able to handle different axis-1 sizes (`input_size`).", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Okay, I messed up the CLA (pushed with my work email address).. let me create a new clean PR\r\n\r\nClosing this one."]}, {"number": 9896, "title": "Little issues in documentation", "body": "What is the proper forum for contributing / discussing documentation errors?\r\n\r\nThere are dead links on this page: https://www.tensorflow.org/tutorials/deep_cnn\r\n\r\nand in the \"Manual Device Placement\" section of this page https://www.tensorflow.org/tutorials/using_gpu\r\na, b and the result of the matmul should all be on the cpu, but the output in the docs shows that the matmul is done on the gpu.\r\n\r\n```\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K40c, pci bus\r\nid: 0000:05:00.0\r\nb: /job:localhost/replica:0/task:0/cpu:0\r\na: /job:localhost/replica:0/task:0/cpu:0\r\nMatMul: /job:localhost/replica:0/task:0/gpu:0\r\n[[ 22.  28.]\r\n [ 49.  64.]]\r\n\r\n```\r\n\r\nI'm working through a bunch of the tutorials today and if I find any issues I can let you know via the appropriate forum if you want. It's hard to keep up with the documentation!", "comments": ["We would love to see pull requests from the community to make TensorFlow better!  Please take a look at [this contributing guide](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md).  The doc you referred to reside in the [`docs_src` directory](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src).\r\n\r\nClosing this as this question has been answered - feel free to reference this issue if/when you open pull requests to fix the docs ;)"]}, {"number": 9895, "title": "XLA \"Aborted (core dumped)\"", "body": "OS: Ubuntu/Linux (16.04)\r\nTensorFlow: Compiled from source\r\nTensorFlow Version: r1.1\r\nBazel Version: 0.4.5\r\nCUDA/CuDNN Versions: 8.0/5.1\r\nGPU Model/Memory: TitanX/12Gb\r\n\r\nAfter turning on XLA JIT compiling, TF fails with a core dump.\r\n\r\n```\r\n2017-05-14 14:50:38.673877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:06:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.75GiB\r\n2017-05-14 14:50:38.673951: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x3cb6960\r\n2017-05-14 14:50:38.900484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:05:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.67GiB\r\n2017-05-14 14:50:38.901416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \r\n2017-05-14 14:50:38.901427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y \r\n2017-05-14 14:50:38.901430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y \r\n2017-05-14 14:50:38.901437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)\r\n2017-05-14 14:50:38.901441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)\r\n2017-05-14 14:50:38.979726: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices\r\n2017-05-14 14:50:38.979748: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 12 visible devices\r\n2017-05-14 14:50:38.981294: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4a7a7b0 executing computations on platform Host. Devices:\r\n2017-05-14 14:50:38.981305: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-14 14:50:38.981438: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices\r\n2017-05-14 14:50:38.981446: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 12 visible devices\r\n2017-05-14 14:50:38.982608: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4a42c80 executing computations on platform CUDA. Devices:\r\n2017-05-14 14:50:38.982619: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): TITAN X (Pascal), Compute Capability 6.1\r\n2017-05-14 14:50:38.982623: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (1): TITAN X (Pascal), Compute Capability 6.1\r\n2017-05-14 14:51:53.443289: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user->operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7f30bc938e70 vs. 0x7f30bcb3e340)\r\nAborted (core dumped)\r\n```\r\n", "comments": ["Thanks for reporting! Do you have a simple script to repro?\r\nCC: @tatatodd ", "I am implementing the [IRNN](https://arxiv.org/pdf/1504.00941v2.pdf) paper but the RNN layer runs really slow which prompted me to give XLA a try. I have done my best to simplify the code as much as possible. I hope this helps.\r\n\r\n```python\r\nimport os\r\nimport sys\r\nimport time\r\nimport urllib.request\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef fetch(destination, source):\r\n  if os.path.exists(destination):\r\n    return destination\r\n  target_dir = os.path.dirname(destination)\r\n  if not os.path.exists(target_dir):\r\n    os.makedirs(target_dir)\r\n  def progress(count, block_size, total_size):\r\n    bytes_downloaded = float(count * block_size)\r\n    percent_downloaded = int(bytes_downloaded / float(total_size) * 100.0)\r\n    sys.stdout.write('\\rDownloading from {} {}%'.format(\r\n      source, percent_downloaded\r\n    ))\r\n    sys.stdout.flush()\r\n  target_path, _ = urllib.urlretrieve(source, destination, progress)\r\n  target_info = os.stat(target_path)\r\n  sys.stdout.write(\r\n    '\\nSuccessfully downloaded {} {} bytes.\\n'.format(\r\n      target_path, target_info.st_size\r\n    )\r\n  )\r\n  sys.stdout.flush()\r\n  return target_path\r\n\r\ndef load_data():\r\n  path = fetch(\r\n    '/tmp/mnist.npz', 'https://s3.amazonaws.com/img-datasets/mnist.npz'\r\n  )\r\n  inputs = np.load(path)\r\n  try:\r\n    return (inputs['x_train'], inputs['y_train']), \\\r\n           (inputs['x_test'], inputs['y_test'])\r\n  finally:\r\n    inputs.close()\r\n\r\n\r\nepochs = 900\r\nbatch_count = 600\r\nbatch_sz = 100\r\n# Load the training data.\r\n(x_train, y_train), (x_test, y_test) = load_data()\r\n# Cast the examples.\r\nx_train = x_train.astype(np.float32)\r\nx_test = x_test.astype(np.float32)\r\n# Subtract the mean and scale the examples.\r\nx_train = (x_train - np.mean(x_train)) / \\\r\n          (np.max(x_train) - np.min(x_train))\r\nx_test = (x_test - np.mean(x_test)) / \\\r\n         (np.max(x_test) - np.min(x_test))\r\n# Reshape the inputs for the rnn layer.\r\nx_train = np.reshape(x_train, [60000, 784, 1])\r\nx_train = np.transpose(x_train, [1, 0, 2])\r\nx_test = np.reshape(x_test, [10000, 784, 1])\r\nx_test = np.transpose(x_test, [1, 0, 2])\r\n# Build the tensorflow graph.\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  x = tf.placeholder(tf.float32, [None, None, 1])\r\n  y = tf.placeholder(tf.int32)\r\n  with tf.variable_scope('rnn_1'):\r\n    weights = tf.get_variable('weights', initializer=tf.truncated_normal(\r\n      [1, 100], 0.0, 0.01, dtype=tf.float32\r\n    ))\r\n    state = tf.get_variable(\r\n      'state', \r\n      shape=[100, 100],\r\n      initializer=tf.constant_initializer(np.identity(100), dtype=tf.float32))\r\n    bias = tf.get_variable(\r\n      'bias', shape=[100], \r\n      initializer=tf.constant_initializer(0.0, dtype=tf.float32)\r\n    )\r\n    x_shape = tf.shape(x)\r\n    timesteps = x_shape[0]\r\n    batch_size = x_shape[1]\r\n    # Compute the linear transformation for every time step.\r\n    output = tf.reshape(x, [-1, 1])\r\n    output = tf.add(tf.matmul(output, weights), bias)\r\n    output = tf.reshape(output, [timesteps, batch_size, 100])\r\n    # Create a tensor array to hold the output of our rnn layer.\r\n    temp = tf.TensorArray(dtype=tf.float32, size=timesteps)\r\n\r\n    def step(c, t, h, s, i, o):\r\n      h = tf.nn.relu(tf.add(tf.matmul(h, s), i[c]))\r\n      o = o.write(c, h)\r\n      return [c + 1, t, h, s, i, o]\r\n\r\n    def step_condition(c, t, h, s, i, o):\r\n      return tf.less(c, t)\r\n\r\n    # Create a counter to track the number of timesteps.\r\n    count = tf.constant(0)\r\n    # Define the initial hidden state.\r\n    hidden = tf.zeros([batch_size, 100], dtype=tf.float32)\r\n\r\n    _, _, _, _, _, output = tf.while_loop(\r\n      step_condition,\r\n      step,\r\n      [count, timesteps, hidden, state, output, temp]\r\n    )\r\n\r\n    output = output.stack()\r\n  # We're only interested in the last output from the RNN layer.\r\n  output = output[-1, :, :]\r\n  with tf.variable_scope('ff_1'):\r\n    weights = tf.get_variable(\r\n      'weights',\r\n      shape=[100, 10],\r\n      initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float32)\r\n    )\r\n    bias = tf.get_variable(\r\n      'bias',\r\n      shape=[10], \r\n      initializer=tf.constant_initializer(0.0, dtype=tf.float32)\r\n    )\r\n    output = tf.add(tf.matmul(output, weights), bias)\r\n  predictions = tf.nn.softmax(output)\r\n  error = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n    logits=output, labels=y\r\n  )\r\n  error = tf.reduce_mean(error)\r\n  optimizer = tf.train.RMSPropOptimizer(1e-8)\r\n  gradients = optimizer.compute_gradients(error)\r\n  train_op = optimizer.apply_gradients(\r\n    [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\r\n  )\r\n# Train the model.\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nwith tf.Session(config=config, graph=graph) as session:\r\n  session.run(tf.global_variables_initializer())\r\n  for epoch in range(epochs):\r\n    epoch_duration = 0.0\r\n    epoch_error = 0.0\r\n    start_time = time.time()\r\n    for batch in range(batch_count):\r\n      batch_error, _ = session.run([error, train_op], {\r\n        x: x_train[:, batch * batch_sz:batch * batch_sz + batch_sz, :],\r\n        y: y_train[batch * batch_sz:batch * batch_sz + batch_sz]\r\n      })\r\n      epoch_error += batch_error\r\n    end_time = time.time()\r\n    epoch_duration = end_time - start_time\r\n    print('Epoch %d: loss = %.2f (%.3f sec)' % (\r\n      epoch + 1, epoch_error, epoch_duration\r\n    ))\r\n```", "Hey @tatatodd, could you take a look when you get a chance? (:\r\nThanks,\r\nAli", "Hi Guys,\r\n\r\nJust checking in :) Any progress on this issue?\r\n\r\nCheers,\r\nTom", "I'm also running into this problem.", "I'm running into this problem too when I run the mnist_softmax_xla.py.", "I'm also seeing this problem on Ubuntu 16.04, Linux ppc64le platform with tensorflow 1.1.0 built with XLA enabled.", "Any update on this? It's an easily reproduced issue that makes using XLA impossible, but there hasn't been anyone assigned to this ticket.", "Apologies on the long delay.  I'm taking a look at this now.", "BTW - if anyone who's running into this problem has a smaller repro script, that would be appreciated.", "I believe @wj1066 reported that he's having the same issue with [mnist_softmax_xla.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py). I tried to simplify the script I provided above but not sure I can make it any simpler. Thanks for looking into this issue.", "@thomasquintana I replicated your issue on TF 1.1.0 build with XLA enabled, using the python program you provided.  FYI the CHECK failure indicates a logical error in the compiler:\r\n```\r\n2017-05-14 14:51:53.443289: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user->operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7f30bc938e70 vs. 0x7f30bcb3e340)\r\n```\r\n\r\nI also tried the same thing on TF 1.2.0-rc2, and the failure goes away.  Looking through the commit history of algebraic_simplifier.cc, there have been many changes to the code.  I did not identify exactly which change fixed the problem.\r\n\r\nSo the solution is to use TF 1.2.0-rc2 or later.  As a reminder, different TF releases can be found here:\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags/\r\n\r\nI'm closing this out; if you feel this doesn't resolve the issue, just comment on this thread and I'll re-open.", "Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-09 16:58:39.308046: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416\r\n2019-03-09 16:58:39.308242: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)\r\n(tensorflow) deeplab@deeplab-X399-DESIGNARE-EX:~/Tracy/FasterRCNN/tf-faster-rcnn-master$ Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAttempting: command not found\r\n", "Deprecated in favor of operator or tf.math.divide.\r\n2019-07-11 12:42:06.524160: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-11 12:42:11.226602: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416\r\n2019-07-11 12:42:11.399046: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)\r\n\r\n\r\n\r\n\r\nthis is the error appearing need  help!!", "2019-10-31 14:55:52.525459: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-31 14:55:52.625544: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 4236312576\r\n2019-10-31 14:55:52.625882: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)"]}, {"number": 9894, "title": "Computing gradient when using tf.split generates console log", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Arch Linux\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.1.0\r\n- **CUDA/cuDNN version**: 8, 6\r\n- **GPU model and memory**: Titan X, 12 GB\r\n\r\n### Describe the problem\r\nWhen computing gradient and using tf.split with size_splits (not num_splits), tensorflow generates console log\r\n\r\n### Source code / logs\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nSIZE = 1\r\n\r\nx = tf.random_normal([1, 1])\r\nw = tf.get_variable(\"w\", [1, SIZE*2])\r\ng = tf.matmul(x, w)\r\ns1, s2 = tf.split(g, [SIZE, SIZE], 1) # CONSOLE MESSAGE\r\n# s1, s2 = tf.split(g, 2, 1) # NO CONSOLE MESSAGE\r\ny = s1 * s2\r\n\r\nloss = tf.reduce_mean(tf.square(x - y))\r\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(train_op)\r\n\r\n```\r\nConsole message looks like:\r\n[<tf.Tensor 'gradients/split_grad/concat:0' shape=(1, 2) dtype=float32>, None, None]", "comments": ["I have the same problem. Moreover, the gradients computed may be corrupted. Although I am not sure actually.", "@eddiepierce: can you paste the complete log?  Is this a harmless info log?", "@concretevitamin \r\n```\r\n>> python bug.py\r\n[<tf.Tensor 'gradients/split_grad/concat:0' shape=(1, 2) dtype=float32>, None, None]\r\n2017-05-15 12:02:08.328921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\r\npciBusID 0000:03:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 360.44MiB\r\n2017-05-15 12:02:08.328942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-15 12:02:08.328947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-15 12:02:08.328951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\r\n```\r\n\r\n\"Free memory\" is low because I'm running a separate model. \r\n\r\nYes, it's a harmless info log.", "@ekelsen: at a casual glance I didn't find any print statement or logging in `split`/`split_v`-related code (and their gradients).  Do you know where that gradient array is printed?", "I tried this myself, the print statement occurs when you run `train_op = tf.train.AdamOptimizer().minimize(loss)` (specifically the minimize() call). I'm digging into finding where the errant print is.", "It's here: https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/array_grad.py#L274\r\nIt looks like this is already fixed in master: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L271\r\n\r\nThanks for reporting!", "Sorry, I saw that this is closed but I have the save console log message.\r\nI really don't understand the reason of this message ...\r\nDo you think the retro propagation can be corrupted by this ? ", "@LucasMahieu what version of TensorFlow are you running?", "1.1\n\n-------------------------\nLucas MAHIEU\nLucas.mahieu@icloud.com\n\n> Le 21 juin 2017 \u00e0 16:18, Skye Wanderman-Milne <notifications@github.com> a \u00e9crit :\n> \n> @LucasMahieu what version of TensorFlow are you running?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "If you upgrade to 1.2 the message should go away. It has no other effect besides printing to the console though."]}, {"number": 9893, "title": "tf.where bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:cuda 8.0 cuDNN 5.0\r\n- **GPU model and memory**: 1060 6GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__ == '__main__':\r\n    bool_lists = np.array([[False, True, True, False],\r\n                           [False, False, True, True]])\r\n    k = tf.Variable(tf.zeros(shape=[2, 4], dtype=tf.int32))\r\n    where_val = []\r\n    bool_ops = []\r\n    k_opes = []\r\n    where_ops = []\r\n    for j in range(bool_lists.shape[0]):\r\n        for i in range(bool_lists.shape[1]):\r\n            bool_i = tf.constant(bool_lists[j, i], dtype=tf.bool)\r\n            bool_ops.append(bool_i)\r\n            where_val.append(k[j, i].assign(tf.where(bool_lists[j, i], i, k[j, i])))\r\n            tf_i = tf.constant(i, dtype=tf.int32)\r\n            where_ops.append(tf.where(bool_i, tf_i, k[j, i]))\r\n            k_opes.append(k[j, i])\r\n    with tf.control_dependencies(where_val):\r\n        k = tf.identity(k)\r\n```\r\n\r\nThe results of above code is:\r\n>[[0 1 2 3]\r\n [0 1 2 3]]\r\n[False, True, True, False, False, False, True, True]\r\n[0, 1, 2, 3, 0, 1, 2, 3]\r\n[0, 1, 2, 3, 0, 1, 2, 3]\r\n\r\nI think the right result of k's value should be:\r\n>[[0 1 2 0]\r\n [0 0 2 3]]\r\n\r\nI just update my tensorflow for 1.0 to 1.1.0. I remember  version 1.0 is right.\r\n\r\nThe following is results from tf 1.0:\r\n>[[0 1 2 0]\r\n [0 0 2 3]]\r\n[False, True, True, False, False, False, True, True]\r\n[0, 1, 2, 0, 0, 0, 2, 3]\r\n[0, 1, 2, 0, 0, 0, 2, 3]", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thanks for you reply!"]}, {"number": 9892, "title": "tf.while_loop runs very slow", "body": "I'm writing a simple RNN implementation using tf.while_loop but it runs incredibly slow. Any insights would be incredibly helpful.\r\n\r\nOS: Ubuntu/Linux (16.04)\r\nTensorFlow: Compiled from source\r\nTensorFlow Version: r1.1\r\nBazel Version: 0.4.5\r\nCUDA/CuDNN Versions: 8.0/5.1\r\nGPU Model/Memory: TitanX/12Gb\r\n\r\nHere's the implementation:\r\n```python\r\ndef forward(self, inputs):\r\n    inputs_shape = tf.shape(inputs)\r\n    timesteps = inputs_shape[0]\r\n    batch_size = inputs_shape[1]\r\n    # Compute the forward pass for every time step.\r\n    output = tf.reshape(inputs, [-1, self._inputs_dim])\r\n    output = tf.add(tf.matmul(output, self._weights), self._bias)\r\n    output = tf.reshape(output, [timesteps, batch_size, self._units])\r\n    # Create a tensor array to hold the output of our rnn layer.\r\n    temp = tf.TensorArray(dtype=self._dtype, size=timesteps)\r\n\r\n    def step(c, t, h, s, i, o):\r\n      h = self._activation(tf.add(tf.matmul(h, s), i[c]))\r\n      o = o.write(c, h)\r\n      return [c + 1, t, h, s, i, o]\r\n\r\n    def step_condition(c, t, h, s, i, o):\r\n      return tf.less(c, t)\r\n\r\n    # Create a counter to track the number of timesteps.\r\n    count = tf.constant(0)\r\n    # Define the initial hidden state.\r\n    hidden = tf.zeros([batch_size, self._units], dtype=self._dtype)\r\n\r\n    _, _, _, _, _, output = tf.while_loop(\r\n      step_condition,\r\n      step,\r\n      [count, timesteps, hidden, self._state, output, temp]\r\n    )\r\n\r\n    return output.stack()\r\n```\r\n\r\nIt takes ~5 minutes to complete one epoch of the mnist dataset.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9891, "title": "tf.random_crop  assert erroneously", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0\r\n- **GPU model and memory**:TitanX Geforce 16G \r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI use `tf.random_crop` for image augmentation, the original image was \"232x232x3\" rgb image. when I try to do random_crop it to \"224x224x3\",  the assert occurs  as following, seems very strange:\r\n\"InvalidArgumentError (see above for traceback): assertion failed: [Need value.shape >= size, got ] [232 232 3] [224 224 3]\"  \r\n\r\n### Source code / logs\r\n```python\r\n    def img_augmentation(self, image_tensor):\r\n        # resize image\r\n        resize_image = tf.image.resize_images(\r\n             image_tensor, [self.IMAGE_HEIGHT, self.IMAGE_WIDTH], method=0, align_corners=False)\r\n        padded_image = tf.image.pad_to_bounding_box(\r\n            resize_image, 4, 4, self.IMAGE_HEIGHT+8, self.IMAGE_WIDTH+8)\r\n        print \"padded_image shape:\", padded_image.get_shape()\r\n        # random crop image\r\n        distorted_image = tf.random_crop(\r\n            padded_image,  [self.IMAGE_HEIGHT, self.IMAGE_WIDTH, self.NUM_CHANNELS])\r\n```\r\n\r\nlogs:\r\n```\r\nCaused by op u'random_crop/Assert/Assert', defined at:\r\n  File \"model.py\", line 315, in <module>\r\n    m.start_train()\r\n  File \"model.py\", line 236, in start_train\r\n    ins = ImageLabelInputStreams(self.graph,self.config)\r\n  File \"/home/guoqingpei/Project/EXPERIMENT/model/inputPipeline.py\", line 62, in __init__\r\n    train_image = self.img_augmentation(train_image)\r\n  File \"/home/guoqingpei/Project/EXPERIMENT/model/inputPipeline.py\", line 132, in img_augmentation\r\n    padded_image,  [self.IMAGE_HEIGHT, self.IMAGE_WIDTH, self.NUM_CHANNELS])\r\n  File \"/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 303, in random_crop\r\n    [\"Need value.shape >= size, got \", shape, size])\r\n  File \"/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 121, in Assert\r\n    condition, data, summarize, name=\"Assert\")\r\n  File \"/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 39, in _assert\r\n    summarize=summarize, name=name)\r\n  File \"/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/guoqingpei/guoqp/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): assertion failed: [Need value.shape >= size, got ] [232 232 3] [224 224 3]\r\n\t [[Node: random_crop/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](random_crop/All/_11, random_crop/Assert/Assert/data_0, random_crop/Shape/_13, random_crop/size/_15)]]\r\n\t [[Node: random_crop/Assert/Assert/_18 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_42_random_crop/Assert/Assert\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9890, "title": "terminate called after throwing an instance of 'std::bad_alloc' what():  std::bad_alloc  Aborted (core dumped)", "body": "Hi everyone, I was learning the tutorials of mnist based on tensorflow.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py\r\n\r\nI ran the py file and got  this problem.\r\n\r\nplease give me a hand.\r\n\r\nMocayo, thanks.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Closing due to lack of activity. Please respond to @skye 's question and reopen if necessary.", "thank you all\r\nI changed my virtual machine form ubuntu14 to ubuntu16, and everything become well", "Thank you for sharing your solution to this issue!", "I'm using the Tensorflow 13.3.1 CPU version from source, with Python 3.5.3 x86_64, Ubuntu 17 and 16GB of memory. I compiled from source after getting this error using the default pip version of Tensorflow.\r\n\r\nI'm running the example here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py\r\n\r\naccuracy.eval(...) is throwing the following error when computing accuracy against the MNIST test set on line 167:\r\n\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\nwhat(): std::bad_alloc"]}, {"number": 9889, "title": "NADAM Optimizer", "body": "This adds the feature requested by #7715", "comments": ["Can one of the admins verify this patch?", "Thanks for the contribution!\r\n\r\nI think adding more booleans flags to the adam code is a little sad. How do you feel about making this a separate optimizer (sharing the op kernel is ok) in tensorflow/contrib/opt?", "@alextp I can definitely do that", "@alextp Is this what you were looking for?", "Jenkins, test this please.", "@alextp Oh I just saw your message here, my bad", "Oh, sorry, I think having nadam_test is good, with a basic test of the math (but no need to repeat all tests from adam_test since there are many)\r\n", "Probably why the checks are failing haha, which tests should I keep in the tester?", "testSparse and testBasic", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.\r\n\r\nEDIT: @alextp lol i tried... It should be working now, it compiled on my computer", "Jenkins, test this please.\r\n", "Jenkins, test this please.\r\n", "Can you fix the error in https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4287/console ?", "Jenkins, test this please.\r\n"]}]