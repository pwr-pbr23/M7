[{"number": 848, "title": "tf use all gpus in one machine, and killed my other training job, how to fix this?", "body": "Hi, all,\n\ni installed the gpu vision of tf which refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#virtualenv-installation, and when i run the third line of:\n\n``` python\nimport tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\n```\n\nthen it will start all of my k80s in one machine, because there is som traning job of some gpus with almost full memeory, then when tf starts, it killed all of the other job. so how to fix this probem?that to say, when i start tf, it use the gpu 0 default?\nthanks very much!\n", "comments": ["You can either set `CUDA_VISIBLE_DEVICES=0` or refer to [this thread](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/jw4FtKOivZE/gU84lQebCAAJ).\n", "thanks~\n"]}, {"number": 847, "title": "Ensure the code compiles when AVX2 instructions are enabled.", "body": "This fixes issue https://github.com/tensorflow/tensorflow/issues/580\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 846, "title": "Improved the performance of spatial convolutions on CPU:", "body": "Moved some checks out of inner loops\nSplit the mapper in 2: a base mapper, and a sub-mapper. This reduces the number of variables that are contained in the base mapper and helps reduce register spills\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 845, "title": "Improved the performance of convolutions on CPU", "body": "", "comments": ["Can one of the admins verify this patch?\n", "can you squash your commits?\n", "@tensorflow-jenkins: test this please.\n"]}, {"number": 844, "title": "Fix typo in framework.md", "body": "typo\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it! #cla\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Can you also fix it here: https://github.com/tensorflow/tensorflow/blob/2e7ea3d21be1284ae6470e2815b2664ef22bb884/tensorflow/python/framework/ops.py#L146\n\nand then squash the commits?\n", "Merged\n"]}, {"number": 843, "title": "build issue: Bazel paths", "body": "As of 0db986d25335a767855e92fc5748038f6fb585c4, I'm getting the error below when I try to build from source. Is my Bazel out of date or something?\n# \n\nERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:11:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..\nERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:12:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..\nERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:11:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.\nERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:12:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.\nERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:21:13: Traceback (most recent call last):\n        File \"/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD\", line 14\n                cc_library(name = \"cc_op_gen_main\", srcs = [\"ops/cc_op_gen.cc\", \"ops/cc_op_gen_main.cc\"], hdrs = [\"ops/cc_op_gen.h\"], copts = tf_copts(), deps = [\"//tensorflow/core:framework\"])\n        File \"/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD\", line 21, in cc_library\n                tf_copts\nname 'tf_copts' is not defined.\nERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:28:1: name 'tf_gen_op_wrappers_cc' is not defined.\nERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:66:13: Traceback (most recent call last):\n        File \"/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD\", line 63\n                cc_binary(name = \"tutorials_example_trainer\", srcs = [\"tutorials/example_trainer.cc\"], copts = tf_copts(), linkopts = [\"-lpthread\", \"-lm\"], deps = [\":cc_ops\", \"//tensorflow/core:kernels\", \"//tensorflow/core:tensorflow\"])\n        File \"/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD\", line 66, in cc_binary\n                tf_copts\nname 'tf_copts' is not defined.\nERROR: no such target '//tensorflow/cc:tutorials_example_trainer': target 'tutorials_example_trainer' not declared in package 'tensorflow/cc' defined by /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD.\nINFO: Elapsed time: 2.232s\n# \n\nbazel version\nBuild label: 0.1.1\nBuild target: bazel-out/local_linux-fastbuild/bin/src/main/java/bazel-main_deploy.jar\nBuild time: Thu Oct 15 20:15:14 2015 (1444940114)\nBuild timestamp: 1444940114\nBuild timestamp as int: 1444940114\n", "comments": ["As of current HEAD, you now have to upgrade to bazel 0.1.4.  Let us know if you still have problems after upgrading.\n", "Worked, thanks!\n", "Please update the documentation on the website to reflect this: https://www.tensorflow.org/versions/master/get_started/os_setup.html#installation-for-linux\n", "Yeah, we updated here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#install-bazel \n\nbut we haven't updated the website from that.  @martinwicke \n", "Done.\n\nOn Sun, Jan 24, 2016 at 10:10 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> Yeah, we updated here:\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#install-bazel\n> \n> but we haven't updated the website from that. @martinwicke\n> https://github.com/martinwicke\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/843#issuecomment-174413099\n> .\n"]}, {"number": 842, "title": "Tool request: Deep Visualization Toolbox for TensorFlow", "body": "I would like TensorFlow to have an official tool similar to this https://github.com/yosinski/deep-visualization-toolbox.\n\nIt would be helpful to check if our CNNs were learning useful features, to debug, and to have better insights.\n\nCheck this video: https://www.youtube.com/watch?v=AgkfIQ4IGaM\n\nThoughts?\n", "comments": [":+1: \n", "+1\nIt would be amazing!\n", "+1\n", "This would be really cool to add! It's not on my roadmap for this quarter, but if anyone wants to implement it I'd be happy to discuss design and guide through the process.\n", "+1\n", "This quarter is nearly over, how about do it in Q2:-)\uff1f@danmane\n", "+1\n", "@shendiaomo - sorry, still not on my near or medium term agenda :)\n", "Quick workaround;\nI believe some of visualization features of this tool can be handled directly within tensorflow (and combined with tensorboard) with a bit of effort.\n\nIssues: \n1. the layout is not ideal (and there are issues with layout of images in general);\n2. extra code means an increased chance for mistakes;\n3. it seems to make the processing much slower :);\n4. it doesn't take into account max_pooling (or other spatial contracting operations);\n\nThis provides visualization of output images of each layer (which is part of the requirements - but is in-efficient  :-1: );\n\n```\nW_conv1 = weight_variable([3,3,1,128])\n#f_x,f_y,depth, number of filters\nb_conv1 = bias_variable([128])\ncnn_layer_1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\nW_conv2 = weight_variable([3,3,128,128])\nb_conv2 = bias_variable([128])\ncnn_layer_2 = tf.nn.relu(conv2d(cnn_layer_1, W_conv2) + b_conv2)\n\nW_conv3 = weight_variable([3,3,128,128])\nb_conv3 = bias_variable([128])\ncnn_layer_3 = tf.nn.relu(conv2d(cnn_layer_2, W_conv3) + b_conv3)\n\n#grab only the first element of the batch and 16 filters\nlayer1_image1 = cnn_layer_1[0:1, :, :, 0:16]\nlayer2_image1 = cnn_layer_2[0:1, :, :, 0:16]\nlayer3_image1 = cnn_layer_3[0:1, :, :, 0:16]\n\nlayer1_image1 = tf.transpose(layer1_image1, perm=[3,1,2,0])\nlayer2_image1 = tf.transpose(layer2_image1, perm=[3,1,2,0])\nlayer3_image1 = tf.transpose(layer3_image1, perm=[3,1,2,0])\n\nlayer_combine_1 = tf.concat(2, [layer3_image1, layer2_image1, layer1_image1])\nlist_lc1 = tf.split(0, 16, layer_combine_1)\nlayer_combine_1 = tf.concat(1, list_lc1)\n\ntf.image_summary(\"filtered_images_1\", layer_combine_1)\n# combine this summary with tensorboard (and you get a decent output);\n```\n\nLooks like (using MNIST):\n![tensorboard-tiled-filter-output](https://cloud.githubusercontent.com/assets/2761482/15947688/ba56104a-2e51-11e6-8dfc-69c9e3f4b1b3.png)\n\nThe other parts would requires a little bit more work (some involve backprop).\n", "Has anyone started working on this? I might take a shot at it if there are no ongoing efforts", "@dyelax Feel free to take a shot. If you build it as a standalone Polymer component that would make it easier to integrate into TensorBoard as a new tab. You could try forking one of the existing dashboards and adding the functionality you need.\r\n\r\nWe plan to make it possible for TensorBoard to request raw Tensor data directly from the backend (i.e. from a tf.summary.tensor_summary) which may be helpful.", "@dandelionmane would it make sense to leverage the existing image summaries for this instead of creating a new summary type?", "Hi!\r\nWe have done some work on this and implemented the algorithm for convolutional neural networks visualization described in http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf and something similar to https://github.com/yosinski/deep-visualization-toolbox but using Tensorflow instead of caffe. However, we are not yet familiar with the TF codebase/ API to integrate it into the ImageSummary tab of tensorboard. We would like to make it into a part of tensorboard. Could someone suggest a good starting point of how we go about doing this? \r\nHere's how far we've progressed: Given any CNN, we can get the reconstructed images from deconvolution for all the layers in the network as described in the paper above. ", "@BhagyeshVikani that sounds cool! wrapping what you have in a Polymer custom element might be a good strategy. Eager to hear an official answer from the TensorBoard team\r\n\r\nhttps://www.polymer-project.org/1.0/", "@BhagyeshVikani @dandelionmane did mention a future [TensorBoard plugin format](https://twitter.com/micahstubbs/status/831948453395599360) recently at #TFDevSummit.\r\n\r\nwhen that plugin format is published, I imagine that's the way to go\r\n", "We have developed a  cool new API for visualizing images reconstructed (as per [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)) from CNN layers using TensorFlow.  Results are written to TensorBoard Images tab as suggested above. Feel free to play around with it and contact us with bugs or hugs- https://github.com/InFoCusp/tf_cnnvis.\r\n\r\nPlease suggest next steps for integrating it with TensorFlow.\r\nA small glimpse of our results:\r\n![results](https://cloud.githubusercontent.com/assets/20687525/24232352/8ca77c06-0fb0-11e7-90ee-f1c98a2be708.jpg)\r\n", "We also added [Deep dream](https://github.com/InFoCusp/tf_cnnvis) to the arsenal:\r\n\r\n|   |   |   |   |\r\n| :-----------: | :-----------: | :-----------: | :-----------: |\r\n| <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/302562334-deep_999.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/586427523-deep_9.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/3658923710-deep_24.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/2708104439-deep_385.png\" width=\"150\" height=\"150\"> |\r\n| Carbonara | Ibex | Elephant | Ostrich |\r\n| <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/3678905801-deep_993.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/1049045916-deep_970.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/2473296459-deep_934.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/1596202689-deep_933.png\" width=\"150\" height=\"150\"> |\r\n| Cheese burger | Tennis ball | Fountain pen | Clock tower |\r\n| <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/3869149957-deep_738.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/4283505926-deep_915.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/2629248471-deep_14.png\" width=\"150\" height=\"150\"> | <img src=\"https://bitbucket.org/repo/Lyk4Mq/images/3971745292-deep_22.png\" width=\"150\" height=\"150\"> |\r\n| Cauliflower | Baby Milk bottle | Sea lion | Dolphin |", "Happy to see that this is such a popular request! It sounds like this would be a perfect fit to be implemented as a new plugin/dashboard (like the scalars dashboard or image dashboard). We're actively working on a new plugin system that will make it easy for external contributors to create such plugins, either to use for themselves or share back to TensorBoard core. Keep an eye out in the coming months!\r\n\r\nI've migrated this issue to our new repository at https://github.com/tensorflow/tensorboard/issues/81. Please feel free to continue discussion there.", "We have created a platform for real time visualization  of networks in keras/tensorflow https://github.com/cyberneuron/RT-CNN-Vis . It's similar in concept with deep-visualization-toolbox. It can be used with almost any keras/tensorflow model and it's extendable with other visualization algorithms."]}, {"number": 841, "title": "py_func() is not getting imported", "body": "", "comments": ["py_func isn't public yet, which is why it is not available.  Once it's ready, we'll import it, add documentation, etc.  We may additionally initially import it into an 'unsupported' module while it is being worked on.\n", "Actually, it looks like it is public (according to our documentation), but only at HEAD, not 0.6.0.\n", "@vrv can you please suggest someway to use it? I mean I tried to build the source but it is giving errors with gen_script_ops used in tensorflow/python/ops/script_ops.py\n", "Hi, aadih,\n\nI just verified the code from HEAD should work. I did the following:\n$ git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n$ bazel test -c opt //tensorflow/python:py_func_test\nthe test passes.\n\nSo, it's most likely your test code or your build environment is somehow different from ours. If you paste the exact error you see, we may be able to help you diagnose.\n", "It was problem with my bazel's version. Its working now. This feature is a great step forward :+1: \n"]}, {"number": 840, "title": "Added check for hidden files in list comprehension", "body": "When invoking os.listdir() on OS X the hidden file \".DS_Store\" will be included which breaks the check if there are as many folders as classes.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for catching this.\nYou've commented some things out in that cell that shouldn't be.\nAlso, your commit results in a huge diff that's hard to verify (I know that iPython Notebooks are a pain in that way). Can you try to make the diff minimal? I'm also happy to commit it on my end if the diff is only 'if not d.startswith(\".\")]'\n", "This patch is made obsolete by:\nhttps://github.com/tensorflow/tensorflow/pull/860\nThanks!\n"]}, {"number": 839, "title": "Sorting now runs in tensorboards image grid.", "body": "Some other changes from previous commits were also added to compiled file through `gulp vulcanize`. \n\nTo make `gulp vulcanize` work I had to do some other changes first, documented [here](https://github.com/panmari/tensorflow/commit/29045bc29fc94eadfa9cb3dc9f53ccb300752c37). Should I make another PR with these (and also add some documentation for `gulp vulcanize`)? Or would that break the internal build?\n", "comments": ["Can one of the admins verify this patch?\n", "ping for @dsmilkov or @danmane \n", "@dsmilkov @danmane any feedback on this? I'll rebase it if it's worth the effort.\n", "Can one of the admins verify this patch?\n", "Looks good to me :+1: Rebase please\n", "I'll rebase and merge.\n", "I take that back, someone more qualified please do the rebase.\n", "Im on it. The documentation for generating the distribution file is kinda\nlacking, i'll try to fix that in another PR.\n\nOn Wed, Feb 10, 2016, 03:19 Martin Wicke notifications@github.com wrote:\n\n> I take that back, someone more qualified please do the rebase.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/839#issuecomment-182170203\n> .\n", "@martinwicke rebased and ran `gulp vulcanize` again.\n", "merged. Thanks!\n"]}, {"number": 838, "title": "fully_connected_preloaded.py on GPU trains slower then on CPU", "body": "while running `examples/how_tos/reading_data/fully_connected_preloaded.py` on CPU the speed of training is the following:\n\n```\nStep 0: loss = 2.31 (0.586 sec)\nStep 100: loss = 2.22 (0.012 sec)\n```\n\nBut, when I run this code on GPU, it hangs, seems like some blocking occurs:\n\n```\nStep 0: loss = 2.30 (3.961 sec)\nStep 100: loss = 2.13 (2.797 sec)\n```\n\nTimings are meaningless here, since actually I wait more than ~3 sec for next output.\n", "comments": ["It's common for a non-optimized network to run slower on GPU than on CPU. For instance, CPU has 5x smaller kernel launch overhead, so models with lots of small operations will run faster on CPU. Stack-overflow is a good place for these kinds of questions (ie, I could elaborate on debugging/profiling technique there). In the particular case, it seems that the data is stored at a 600MB constant node on CPU, so you have a 600MB CPU->GPU transfer at every step\n", "Actually, this has an easy fix, put \"input_images\", \"input_labels\" block into \"with tf.device(\"/cpu:0\"):\" block. The problem was that constant nodes were placed on GPU, but the ops that come after it are only supported on CPU, hence the data transfers\n", "I have this fix in a pending CL, this issue should get closed when it's pushed\n", "Fixed by  https://github.com/tensorflow/tensorflow/commit/ebe109b76d3a8fbdfadee8fa1e6e642563c88034\n", "Hey @yaroslavvb . Thanks for the detailed explanation, I also came up with this fix latter on. \nadding `with tf.device(\"/cpu:0\")`\nleads to the following speed:\n\n``` python\nStep 0: loss = 2.32 (0.112 sec)\nStep 100: loss = 2.20 (0.020 sec)\nStep 200: loss = 1.99 (0.018 sec)\nStep 300: loss = 1.76 (0.017 sec)\nStep 400: loss = 1.42 (0.019 sec)\n```\n\nIt solves the issue with timing. but using GPU here doesn't helps in performance which contradicts the whole purpose of GPU.\nI've added device placement logging\n`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`\n\nand I see that nodes related to input producing and queues  automatically placed on CPU.  Are there any other ways to get more detailed log to see memory flow CPU->GPU->CPU? \n\nI'm going to add simple logging, with warnings when the list of available devices contains GPU but looks Looks like it's enough to add logging only in this source:\n`https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/simple_placer.cc#L298`\n\nIs there any public lists of ops which doesn't have GPU kernels? \n", "After the patch I get 30ms per step on my CPU (HP Z420) and 140ms per step on GPU (K40c). Note that the patch only pins 2 constant ops to the CPU. It looks like there's a \"Cast\" op which is part of input pipeline that ends up on GPU, so that slows things down a lot. In general I found it best to pin the whole input pipeline to CPU manually.\n\nHowever, even if you fixed this, this network is too small to benefit much from GPU. Using CPU-only version, the \"QueueDeqeueMany\" op takes 25ms out of 30ms total, so you have 5 ms spent on \"actual computation\", so even if you made the computation run 5x faster, it would have negligible effect on step size.\n\nthird_party/tensorflow/models/image/mnist/convolutional is much better example for utilizing GPU, I get 8x speed-up when using GPU. Also, cifar10_train example has a better tuned input pipeline\n\nRegarding other questions:\nif you turn on verbose logging in bazel, you'll see some messages every time tensors get copied between devices here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L41\n\nOne way to find if an op has GPU implementation is to search for \"REGISTER_KERNEL_BUILDER\" macros, like here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc#L217\n"]}, {"number": 837, "title": "reading_data/fully_connected_reader.py VERY slow relative to fully_connected_feed.py", "body": "I noticed that when using a data reader to provide minibatches of examples to a model that performance is greatly reduced relative to just supplying the examples via `feed_dict`. For instance, when running `reading_data/fully_connected_reader.py` with the following flags::\n\n```\n--hidden1 512 --hidden2 512 --batch_size 128\n```\n\nit takes 28.7 seconds to process 600 minibatches with a GPU utilization of 13%. If I edit the code so that `num_threads=16` (instead of `num_threads=2`) when `shuffle_batch` is called, these numbers improve to 14.9 seconds and 23% GPU utilization. However, training the same model via `fully_connected_feed.py` takes only 2.63 seconds and achieves a GPU utilization of 55%. This is hardly rigorous, but it seems that the overhead involved in reading the Example protos from the TFRecords file, putting them into a queue, etc is much higher than I would expect. \n\nThese numbers were compiled using 039981f5a382ce9dc1e97dc3bd25aeba7fd82ade and running on a Titan X card with no other background processes running.\n", "comments": ["Related to #551, #763 ?\n", "I was able to get 68x68 images reading/decoding from TF-examples fast enough to saturate my K40 with input pipeline using 6 threads/1 CPU. These tutorials have not been tuned to work efficiently on GPU, so there could be some small ops that are placed on GPU suboptimally and are causing unnecessary data transfers -- try pinning your input pipeline to CPU manually. See #838 for an example of suboptimal placement making GPU version run 100x slower\n", "Re: \"Pinning the pipeline to the CPU\":\n\nhere's how I would optimize it -- pin everything to CPU (export\nTF_MIN_GPU_MULTIPROCESSOR_COUNT=800), and remove all the non-reading ops.\nTweak your pipeline design/number of threads until you get maximum\nthroughput. Then re-enable GPU, and use manual pinning to make sure that\nyour input pipeline throughput is unchanged. Then attach your processing\nops (on GPU)\n\nOn Fri, Jan 22, 2016 at 2:59 PM, nryant notifications@github.com wrote:\n\n> Pinning the pipeline to the CPU helps somewhat, but still is worse than I\n> would expect. For num_threads=2 the time reduces to 12.6 seconds with 13%\n> GPU utilization and for num_threads=16 to 9.4 seconds with 18% utilization\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/837#issuecomment-174081084\n> .\n", "This actually fixes #838. Pinning the pipeline to CPU for\n`fully_connected_reader.py` helps somewhat, but performance still lags\n`fully_connected_feed.py`. I did some benchmarking this afternoon before\nleaving the office and most of the remaining peformance gap seems to be\nfrom the fact that reading an epoch's worth of images takes 20-25x as long\nusing a reader (.2 seconds vs about 5 seconds; sorry, out of office and\ndon't have the precise timings with me). For a more realistically sized\nnetwork, this additional overhead wouldn't be such an issue, so this\nprobably should be closed after `fully_connected_reader.py` has been\nmodified.\n\nOn Friday, January 22, 2016, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Closed #837 https://github.com/tensorflow/tensorflow/issues/837 via\n> ebe109b\n> https://github.com/tensorflow/tensorflow/commit/ebe109b76d3a8fbdfadee8fa1e6e642563c88034\n> .\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/837#event-523634581.\n\n## \n\nNeville Ryant\nLinguistic Data Consortium\nUniversity of Pennsylvania\n", "oops, off-by-1 error on my part.\nWhen you are using a reader, there's more work done at the beginning because of prefetching, could it be that the extra time is due to it filling up a queue of examples?\n", "@ebrevdo: Could you take a look since it's queue related? \n", "@josh11b could this be due to lack of caching in the readers?  not sure if this bug is still relevant given the changes that have been pushed between when this bug report and now.\n", "I believe there is now the ability to read batches from a reader that can reduce overhead, assuming there is no problem with the examples having different dimensions.  Also I recall someone is working on ParseExample performance improvements?\n", "Yes; the ParseExample work will hopefully get checked in w/in a week or two.\n\nOn Wed, Aug 10, 2016 at 10:31 AM, josh11b notifications@github.com wrote:\n\n> I believe there is now the ability to read batches from a reader that can\n> reduce overhead, assuming there is no problem with the examples having\n> different dimensions. Also I recall someone is working on ParseExample\n> performance improvements?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/837#issuecomment-238941329,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim6qIclGyr6l59aA8O2h8x9g4HCFOks5qegrVgaJpZM4HKB-s\n> .\n", "I'm assuming that this is checked in. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 836, "title": "tensorboard not able to read large event file (~600MB)", "body": "I am training a network on 4 GPUs. The event file is too large to be parsed by tensorboard. How can I increase the limit?\n\n```\nRNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path /home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://0.0.0.0:6006)\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\nException in thread Thread-2:\nTraceback (most recent call last):\n  File \"/home/pc/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/home/pc/anaconda2/lib/python2.7/threading.py\", line 1073, in run\n    self.function(*self.args, **self.kwargs)\n  File \"/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/event_accumulator.py\", line 242, in Update\n    self.Reload()\n  File \"/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/event_accumulator.py\", line 175, in Reload\n    for event in self._generator.Load():\n  File \"/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/impl/directory_watcher.py\", line 82, in Load \n    for event in self._loader.Load():\n  File \"/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/impl/event_file_loader.py\", line 53, in Load \n    event.ParseFromString(self._reader.record())\nDecodeError: Error parsing message\n```\n", "comments": ["So big file, \nthe protocol Buffers are not designed to handle large messages.\nI am not sure about this solution but try to edit CodedInputStream::SetTotalBytesLimit() in /src/google/protobuf/io/coded_stream_inl.h\nthe Doc is here\nhttps://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.io.coded_stream#CodedInputStream.SetTotalBytesLimit.details\n", "look at this \nhttps://github.com/BVLC/caffe/issues/279\n", "Do you know why your protobuf is so big? Is it storing a very large GraphDef? Also, can you share the events file that is causing this issue?\n", "@danmane I was training an RNN on 4 GPUs. The weights was defined on CPU. Then I got a large graph. I am not sure if I am using RNN correctly in this multiple-GPU case. Do you know how to train RNN on 4 GPUs? I was using rnn_cell.BasicLSTMCell and rnn_cell.MultiRNNCell() to create an RNN.\n", "I'm not sure about RNN usage, sorry. (Maybe @martinwicke can advise or point in the right direction.)\n\nIt would be helpful to find out what message is so large that it's violating the protobuf message size limit - that would let us determine whether we should increase the message size limit, or if we should find a way to prevent such large messages from getting generated in the first place. Can you dig into the protobuf and find what it is, or send us the event file?\n", "I got the same problem with a 230MB event file. I copied the event file to another VM with larger memory, then opened Tensorboard without any problem.", "I have migrated this issue to tensorflow/tensorboard#49 because TensorBoard has moved to a new repository (outside of tensorflow/tensorflow). Lets continue discussion there, and sorry about how this issue is still open."]}, {"number": 835, "title": "Move spawn_strategy args from install docs to bazelrc.", "body": "`--spawn_strategy=standalone` is pretty much always required to avoid problems with hermetic builds excluding system files, so we make it the default in the bazelrc file. This simplifies the installation instructions.\n", "comments": ["merged.\n"]}, {"number": 834, "title": "Optimizing over only some parameters", "body": "Currently the optimizers in TensorFlow appear to act simultaneously upon all tf.Variables in the function graph. However, commonly, it's useful to optimize only over certain Variables. For example, given a pre-trained network, I might only want to optimize wrt a new set of output weights. Am I misreading the examples and optimizer code or is this presently not supported?\n", "comments": ["`optimizer.minimize(loss, var_list=[your variables])` does this if I understand correctly.\n\nAlso, take a look at: http://stackoverflow.com/questions/34477889/holding-variables-constant-during-optimizer/34478044#34478044\n", "Thanks!\n", "@zackchase: in the future I would use StackOverflow for these types of questions.\n", "optimizer.minimize(loss, var_list=[your variables]) will optimize over the list of variables. \r\n\r\nBut is there a way to optimize over a subset of a given variable while treating the rest of it as constants? What I mean is : say I have a 2D variable named W_fc of size 2x2. I want to optimize over W_fc[0][0] and W_fc[1][1] and treat W_fc[0][1] and W_fc[1][0] as constants. ", "@amohant4 I am also looking for this. Seems [scatter_update](https://www.tensorflow.org/api_docs/python/tf/scatter_update) may work. My concern is its computation efficiency.", "@amohant4 have you got a solution for the case above? \r\n\r\nI want to get gradients w.r.t. a subset of a variable, like tf.gradients([loss], [variable[i]] but this returns None as slicing returns a new tensor. This is closely related to this [stackoverflow question](https://stackoverflow.com/questions/41889531/slice-of-a-variable-returns-gradient-none).", "@thangbui I found an answer to your question [here](https://stackoverflow.com/questions/49048622/tensorflow-minimise-with-respect-to-only-some-elements-of-a-variable).", "You can use `tf.stop_gradient(x)` if you'd like to treat `x` as a constant when performing gradient descent. I found this API especially useful when implementing Q networks in RL where your target (the Bellman backup)  and loss function (the Q network) share the same weights. `stop_gradient` freezes the Bellman backup weights so the target is treated as a constant.", "I wonder what exactly it means 'to optimize with respect to some parameters'. Does this mean only the weights in `[var_list]` are updated? Derivatives only wrt variables in `[var_list]` are calculated and the rest are set to 0? In the code I use [var_list] is in fact a layer (output), not a set of weights.  "]}, {"number": 833, "title": "conv2d_transpose cannot be used in network with variable batch size", "body": "I'm building a network that requires an approximate deconvolution using conv2d_transpose and I'm running into this error when constructing the network:\n\n```\n...\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 80, in conv2d_transpose\n    output_shape_ = ops.convert_to_tensor(output_shape, name=\"output_shape\")\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 530, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 195, in _tensor_shape_tensor_conversion_function\n    \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\nValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 40, 40, 32)\n```\n\nIt appears that the operation doesn't support shape objects with none values as they cannot be converted to tensors. Is there a workaround? Or should I simply explicitly set my batch size for now?\n\nedit: It appears that reshape has the same issue.\n", "comments": ["For reshape, you can have one unknown dimension if you set it to -1.  For conv2d_transpose, I'm going to defer to Geoffrey.\n", "conv2d_transpose does need to know the exact shape, but that shape can be computed with tensorflow ops at runtime.  Presumably you have some variable whose first dimension is batch size, so you can do something like\n\n```\nbatch_size = tf.shape(something_or_other)[0]\ndeconv_shape = tf.pack([batch_size, 40, 40, 32])\nconv2d_transpose(..., output_shape=deconv_shape, ...)\n```\n", "Awesome, that worked! Thanks!\n", "I tried to to it exactly the same way as described here. But Unfortunately, it returns a tensor with unknown dimensions:\n\n```\nbatch_size = tf.shape(something_or_other)[0]\ndeconv_shape = tf.pack([batch_size, 40, 40, 32])\nconv2d_transpose(..., output_shape=deconv_shape, ...)\n```\n\nReturns: A  tensor with shape (?, ?, ?, ?).\n\n`conv2d_transpose(..., output_shape=[64, 40, 40. 32], ...)`\nReturns: A  tensor with shape (64, 40, 40, 32).\n\nIt even looks like it's unrelated to the variable batch-size, but to tf.pack():\n\n```\ndeconv_shape = tf.pack([64, 40, 40, 32])\nconv2d_transpose(..., output_shape=deconv_shape, ...)\n```\n\nReturns: A  tensor with shape (?, ?, ?, ?).\n\nSo, as soon as i use two conv2d_transposed() operations with variable batch-size one after the other, it is not working for me.\nDoes anyone have the same issues having a unknown tensor-shape. Is is there a way to set the tensor-shape manually?\n\nHere is the full method I use to simplify the call to conv2d_transpose:\n\n```\ndef conv2d_transpose(name_or_scope, x, n_filters,\n           k_h=5, k_w=5,\n           stride_h=2, stride_w=2,\n           stddev=0.02, bias=0.1,\n           activation=lambda x: x,\n           padding='SAME',):\n\n    with tf.variable_scope(name_or_scope):\n        input_shape = x.get_shape().as_list()\n        # extract batch-size like as a symbolic tensor to allow variable size\n        batch_size = tf.shape(x)[0]\n        w = tf.get_variable(\n            'W', [k_h, k_w, n_filters, input_shape[3]],\n            initializer=tf.truncated_normal_initializer(stddev=stddev))\n\n        assert padding in {'SAME', 'VALID'}\n        if (padding is 'SAME'):\n            out_h = input_shape[1] * stride_h\n            out_w = input_shape[2] * stride_w\n        elif (padding is 'VALID'):\n            out_h = (input_shape[1] - 1) * stride_h + k_h\n            out_w = (input_shape[2] - 1) * stride_w + k_w\n\n        out_shape = tf.pack([batch_size, out_h, out_w, n_filters])\n        # out_shape = tf.pack([batch_size, 14, 14, 32])\n\n        convt = tf.nn.conv2d_transpose(\n            x, w, output_shape=out_shape,\n            strides=[1, stride_h, stride_w, 1], padding=padding)\n        b = tf.get_variable(\n            'b', [n_filters],\n            initializer=tf.constant_initializer(bias))\n        convt += b\n    return activation(convt)\n```\n\n**EDIT:**\nExecuting this in the session, the result actually has the proper shape, but not during graph construction:\n\n```\nBATCH_SIZE = 64\n# ...\nconv3t = tt.network.conv2d_transpose(\"Deconv1\", encoder_out,\n                                         32, 5, 5, 2, 2,\n                                         stddev=0.02, bias=0.1,\n                                         activation=tf.nn.relu)\n# ...\nsess.run(tf.initialize_all_variables())\nfeed = {x: np.ones([BATCH_SIZE, 28 ** 2])}\nconv3tres = sess.run([conv3t], feed_dict=feed)\nprint(\"ct\", conv3t)\nprint(\"conv3tres-shape\", np.shape(conv3tres))\n```\n\nOutput:\n('ct', < tf.Tensor 'Decoder/Relu:0' shape=(?, ?, ?, ?) dtype=float32 >)\n('conv3tres-shape', (64, 14, 14, 32))\n\n**Solution:**\nI could fix it by myself. After checking out a working solution from [GitHub](https://github.com/pkmital/tensorflow_tutorials/blob/master/python/09_convolutional_autoencoder.py), I realized that the return value of shape (?, ?, ?, ?) seems to be normal. What I mixed up was the difference between tf.shape(...) and Tensor.get_shape(...). Here is the working code. Feel free to use it...\n\n```\ndef conv2d_transpose(name_or_scope,\n                     x, n_filters,\n                     batch_size,\n                     k_h=5, k_w=5,\n                     stride_h=2, stride_w=2,\n                     stddev=0.02, bias=0.1,\n                     activation=lambda x: x,\n                     padding='SAME',):\n    with tf.variable_scope(name_or_scope):\n        static_input_shape = x.get_shape().as_list()\n        dyn_input_shape = tf.shape(x)\n\n        # extract batch-size like as a symbolic tensor to allow variable size\n        batch_size = dyn_input_shape[0]\n\n        w = tf.get_variable(\n            'W', [k_h, k_w, n_filters, static_input_shape[3]],\n            initializer=tf.truncated_normal_initializer(stddev=stddev))\n\n        assert padding in {'SAME', 'VALID'}\n        if (padding is 'SAME'):\n            out_h = dyn_input_shape[1] * stride_h\n            out_w = dyn_input_shape[2] * stride_w\n        elif (padding is 'VALID'):\n            out_h = (dyn_input_shape[1] - 1) * stride_h + k_h\n            out_w = (dyn_input_shape[2] - 1) * stride_w + k_w\n\n        out_shape = tf.pack([batch_size, out_h, out_w, n_filters])\n\n        convt = tf.nn.conv2d_transpose(\n            x, w, output_shape=out_shape,\n            strides=[1, stride_h, stride_w, 1], padding=padding)\n        b = tf.get_variable(\n            'b', [n_filters],\n            initializer=tf.constant_initializer(bias))\n        convt += b\n    return activation(convt)\n```\n", "@bsautermeister Yes, that's expected behavior.  We currently don't do constant folding through partially known tensors such as the `tf.pack` here.  Hopefully that will be fixed at some point in the future.\n", "The proposed solution still wouldn't work in case of stacking `conv2d_transpose` filters because, in the example, the static shape is calculated from the input tensor, but the previous filter would produce the dynamic shape, i.e. `(?,?,?,?)` which should brake at the point of initializing weights since they can take static shapes with known values only (the channel dimension would be unknown). The only fix would be to fix shapes produced by the `conv2d_transpose` filter. At least the channel dimension should be known no matter what.\n", "@amolchanov86 The proposed solution works fine for stacked filters, since it has no dependence on static shapes.  If you want assistance getting your model working, please ask questions on StackOverflow.\n", "**Workaround:**\r\nI had this issue when I wanted to use batch_norm right after a conv2d_transpose. The conv2d_transpose loses all shape information about the output tensor. batch_norm requires the number of channels to be statically known. I found a workaround by using tf.reshape() right after the conv2d_transpose. This regains the static shape information.", "I'm using Python3.6 with TensorFlow 1.5.1 and my issue is\r\n[Report Error]ValueError: Incompatible shapes between op input and calculated input gradient. conv2d_transpose\r\nany idea how to solve it?"]}, {"number": 832, "title": "upgrade bazel in docker containers to 0.1.4", "body": "", "comments": []}, {"number": 831, "title": "upgrade bazel in docker containers", "body": "", "comments": []}, {"number": 830, "title": "fully_connected_preloaded.py fails with exception on python3.5", "body": "There are some errors while handling exception `tf.errors.OutOfRangeError`:\nOn python2 no issues with handling `tf.errors.OutOfRangeError`.\n\nDuring handling of the above exception, another exception occurred.\nlog:\n\n``` python\nTraceback (most recent call last):\n  File \"fully_connected_preloaded.py\", line 158, in <module>\n    tf.app.run()\n  File \"/Users/home/tensorflow_python3/lib/python3.5/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"fully_connected_preloaded.py\", line 154, in main\n    run_training()\n  File \"fully_connected_preloaded.py\", line 120, in run_training\n    _, loss_value = sess.run([train_op, loss])\n  File \"/Users/home/tensorflow_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/Users/home/tensorflow_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 428, in _do_run\n    target_list)\nSystemError: <built-in function delete_Status> returned a result with an error set\n```\n", "comments": ["Geoffrey is our Python 3 expert, perhaps he can help.\n", "Please split this issue into two: the GPU issue is not related to the Python 3 issue if it also happens on Python 2.\n", "For the Python 3 error, I'm trying to find the relevant function now, but I may need some way of reproducing the problem unless something obvious arises.\n", "Yeah, I think this is some sort of horrible swig issue, so I'll need a way to reproduce.  Can you make a minimal test case that causes the exception?\n", "Hey @girving , here is minimal test case:\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nimport sys\n\nx = tf.constant(np.array([1, 2, 3, 4]))\ny = tf.constant(np.array([1, 2, 3, 4]))\n\nsingle_x, single_y = tf.train.slice_input_producer([x, y], num_epochs=3)\nbatch_x, batch_y = tf.train.batch([single_x, single_y], batch_size=1)\n\nsum_op = batch_x + batch_y\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n\n\ntry:\n    step = 0\n    while not coord.should_stop():\n        res = sess.run(sum_op)\n        step += 1\n        print(res, step)\n        sys.stdout.flush()\nexcept tf.errors.OutOfRangeError:\n    print(\"Exception hear\")\nfinally:\n    coord.request_stop()\n\ncoord.join(threads)\n```\n\nOk, I'm splitting the issue.\n", "Thanks!  I'll take a look.\n", "Apologies for taking forever to get to this.  Unfortunately, I am unable to reproduce the problem: even on Python 3, I hit the exception every time (out of 1101 test runs).\n", "I can reproduce it. In fact, it's a pervasive problem whenever you run with python3.5. See #1033.\n", "The culprit seems to be this: https://bugs.python.org/issue23571\n", "I think we're suffering from this: https://github.com/swig/swig/issues/567, and the solution may be this: https://github.com/swig/swig/pull/560\n", "I'll try installing a fresh swig.\n", "@martinwicke: Was this fixed? \n", "Yes. For posterity: Upgrade to swig >=3.0.8.\n"]}, {"number": 829, "title": "GradientDescentOptimizer requires explicit variable listing after being called once in a different session", "body": "When I run \n[debugger.txt](https://github.com/tensorflow/tensorflow/files/99486/debugger.txt), I get a TypeError because the operation gradstep yields an empty list. However, when I comment out the first attempt to gather a gradient, or add the argument `var_list=[W,b]` to the second attempt, this error does not occur. It seems like somehow there's an issue with identifying the var_list implicitly, after having run (and closed) the previous session. \n", "comments": ["Hm....can't reproduce this at head. Closing/reopening the session doesn't seem to affect the list of trainable variables. Can you check at which point you are losing the trainable variables? That's the list that optimizer uses to identify the variables implicitly --\n[v.name for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]\n", "I think we fixed this some time in December but after our last 0.6.0 release.  We're hoping to release a newer version soon, but it should also be fixed at HEAD if you have the ability / patience to install from sources.\n\nFeel free to re-open if it's still broken at HEAD.\n"]}, {"number": 828, "title": "Changed non-public ops to public ones in tutorials", "body": "Replace non-public `tf.nn.relu_layer` and `tf.nn.xw_plus_b` by a combination of `tf.nn.relu`, `tf.matmul` and `tf.add`.\n\nThis PR fixes #811.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Our CI is currently broken, will pick this back up when we fix it. Looks good to me though. Can you squash the commits?\n", "My PR has only one commit. What do you mean by squash the commits?\n", "@martinwicke: the merge commits are dropped when we rebase, so this is actually fine as is.\n", "Maybe I shouldn't press the button `Update branch` in the Github UI next time? Or it doesn't matter?\n", "@tensorflow-jenkins, test this please\n\n@cesarsalgado: shouldn't matter as long as there are no conflicts with the base branch :)\n", "Merged.\n"]}, {"number": 827, "title": "Making a composite assignment + optimization op", "body": "I want to make an op that, when run, changes a `Variable`'s value and then updates it by optimizing some dependent scalar `Tensor`.\n\nI came up with this, which runs successfully:\n\n``` python\nv = tf.Variable(tf.zeros([3]), name='v')\nv_input = tf.placeholder(tf.float32, [3], name='v_input')\nloss = tf.reduce_sum(tf.square(v))\n\nwith tf.control_dependencies([v.assign(v_input)]):\n  op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n\nsession = tf.Session()\nsession.run(tf.initialize_all_variables())\nsession.run(op, feed_dict={v_input: [1., 2., 3.]})\n```\n\nHowever, if I change `GradientDescentOptimizer` to `AdagradOptimizer` (or `RMSProp`, `Adam`, ...), it fails in the variable-initialization (second-to-last) line, complaining that `v_input` must be fed:\n\n```\nW tensorflow/core/common_runtime/executor.cc:1076] 0x15cb160 Compute status: Invalid argument: You must feed a value for placeholder tensor 'v_input' with dtype float and shape dim { size: 3 }\n```\n\nObviously to actually execute the training op I need to feed in a value for `v_input` due to the control dependency. But just to initialize variables? I don't understand why I would need to feed anything in...\n\nI guess since `AdagradOptimizer` creates its own `Variable`s (accumulated gradient) when you call `minimize`? I really only want the optimizer's update step to be dependent on the assignment op I create, but I'm not sure how to do that. Any help would be appreciated!\n", "comments": ["Which version are you running? I would have expected this to be fixed by https://github.com/tensorflow/tensorflow/commit/bc624aa8d9460dca794fde6d5534f1d3e8054016, which avoids adding control dependencies to optimizers' slots. However that didn't make it into the 0.6.0 release, so you may need to build from source (or patch the appropriate lines in `optimizer.py`).\n", "Awesome. Looking forward to the next release then! Thanks.\n\nIs giving finer-grained control over control dependencies on the roadmap, by any chance? Only being able to do it with a context manager is a bit limiting sometimes.\n"]}, {"number": 826, "title": "Add --spawn_strategy=standalone to os_setup.md", "body": "It seems to solve more problems than it causes. Also fixes some formatting/grammar nits.\n\nFixes #771.\n", "comments": ["Another eye, please.\n"]}, {"number": 825, "title": "Fix typos in comments.", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 824, "title": "Feature parity of tf.diag with np.diag", "body": "np.diag can\n\n1) take 2d array and produce 1d vector of diagonal entries\n2) take 1d array and produce diagonal 2d array (inverse of case 1)\n\ntf.diag only supports 2) and the behavior for case 1) is undocumented\n\nCurrently to do 1) you need to do something like this:\n\n```\ndef identity_matrix(n):\n  \"\"\"Returns nxn identity matrix.\"\"\"\n  # note, if n is a constant node,\u00a0this assert node won't be executed,\n  # this error will be caught during shape analysis \n  assert_op = tf.Assert(tf.greater(n, 0), [\"Matrix size must be positive\"])\n  with tf.control_dependencies([assert_op]):\n    ones = tf.fill(n, 1)\n    diag = tf.diag(ones)\n  return diag\n\ndef extract_diagonal(tensor):\n  \"\"\"Extract diagonal of a square matrix.\"\"\"\n\n  shape = tf.shape(tensor)\n  n = shape[0]\n  assert_op = tf.Assert(tf.equal(shape[0], shape[1]), [\"Can't get diagonal of \"\n                                                       \"a non-square matrix\"])\n\n  with tf.control_dependencies([assert_op]):\n  return tf.reduce_sum(tf.mul(tensor, identity_matrix(n)), [0])\n```\n", "comments": ["I believe tf.diag is only intended to support 2).  It would be reasonable to create a new op for 1).\n", "Yeah, having the same op arbitrarily be its own one-sided inverse seems pretty strange.  Two ops sound better to me.\n", "Besides numpy, this also how it's done in Theano, Torch, Matlab, ViennaCL\n", "In my defense, that's four different things copying Matlab's arguably sketchy choice, but I see your point.  I'm okay if we bow to peer pressure.\n", "@girving This issue may be closed?\n", "Yep, closing.\n", "@chemelnucfin: For future use, you can include a line \"Fixes #824\" in the commit message in order to automatically close the issue when it's merged. \n"]}, {"number": 823, "title": "Give a convenient way to retrieve shadow variables created by ExponentialMovingAverage.", "body": "At first I was under the impression, the variables returned by `tf.moving_average_variables()` would be shadow variables created, but that does not seem to be the case (they didn't save/restore correctly for me). \n\nNow that I'm reading the code, that seems to be the intention, though? Shouldn't [this line](https://github.com/tensorflow/tensorflow/blame/f2bd0fc399606d14b55f3f7d732d013f32b33dd5/tensorflow/python/training/moving_averages.py#L227) read \n\n```\nops.add_to_collection(ops.GraphKeys.MOVING_AVERAGE_VARIABLES, avg)\n```\n\n? \n\nIf not, it would be great to save the averages under some other key and make them easily retrievable for the saver in a similar way as `tf.moving_average_variables()`, maybe as  `tf.moving_average_shadow_variables()`.\n", "comments": ["That could very well be a bug.  Sherry, could you look at this?\n", "Ping @sherrym, could you please give your opinion?\n", "HI there,\n\nSorry about the delay.\n\nWhat did you mean by \"they didn't save/restore correctly for me\"?\n\nWe have introduced variables_to_restore() (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L327) to return a map from the moving_average_names to the variables mappings, such that you can restore the moving_average values into the corresponding variables. Would that do what you want? If not, would you please elaborate what you would like to restore?\n\nThanks,\nSherry\n", "Hi,\n\nI guess I understand what @panmari says because I have found the same problem.\n\nThere are no way to obtain the list of shadow variables, for example, for just save them.\n\nAfter read the documentation, tf.moving_average_variables() seems to do the work. But it isn't because tf.moving_average_variables() returns the variables which moving average is applied not its shadow variables.\n\nThanks\n", "@martinwicke, @aselle: I think we may need backup czars, since Sherry is out at the moment. \n", "@sherrym \n", "Hi, is there any progress already?\n\nI'm currently facing the same issue. When I save and restore my trained model as explained [here](https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html) and I would like to continue my training, I was always wondering why the restored model always performs slightly worse that before saving it. I have the impression that restore() is not actually restoring the shadow-variables.\n\nIn case we have to save/restore them manually, that would be OK for me, but could you tell an example. Because I cannot use mae.variables_to_restore(), because I cannot access the ExponentialMovingAverage instance, which has been written in a different module.\n\nBeside ExponentialMovingAverage, I have the same problem for the shadow-variables within the Optimizers (such as Adam).\n\n**EDIT:**\n\nI found an example of its usage here in the CIFAR10 eval script:\nhttps://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_eval.py\n\nUnfortunately, when I do the same in my script, I still get worse results than before saving and restoring the model. Additionally, I cannot continue with my training when I do it like in the example:\n\n```\n# Restore the moving average version of the learned variables for eval.\nvariable_averages = tf.train.ExponentialMovingAverage(\n    cifar10.MOVING_AVERAGE_DECAY)\nvariables_to_restore = variable_averages.variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore)\n```\n\nIs there no simple way to save and restore **exactly** in the same state?\n\n**EDIT 2:**\n\nFor whatever reason, when I now do a simple restore like this, I can cantinue on training and get the same (better) results as before saving it:\n\n```\n# Restore the moving average version of the learned variables for eval.\nsaver = tf.train.Saver(variables_to_restore)\nsaver.restore(sess, \"/path/to/file.ckpt\")\n```\n\nIs there a rule of thumb when I should use these moving averages and when not?\nAdditionally, within the CIFAR-10 example of TensorFlow, I don't get the following thing:\nThe exp. moving averages are tracked during training, but are these values always just copies lying next to the variables actually getting trained? So when I do not restore a model and explicitely load these moving average values, these values would never get used during the evaluation? Because in case I get worse results on these averaged values, I could skip the computation of them during training and get (slightly) better training-performance, right?\n\n**EDIT 3:**\n\nIgnore everything above :) After doing more training, restoring the exp. moving averages for evaluation actually give better results! I'm happy for at least right now! ;-)\n", "So that would mean this bug is done? Or is there an issue still hidden here? If so, please comment to reopen, or file a new, specific issue.\n", "The issue is that there is no function to retrieve only the shadow variables. Something like tf.moving_average_shadow_variables(). Because tf.moving_average_variables() returns the variables which moving average is applied.\n", "Is there any best practice yet to retrieve shadow variable mappings for inference time?\r\n\r\nHere's a custom function I'm using to get shadow variable mappings instead of regular ones (returns regular variable mapping if shadow variable doesn't exist)....\r\n\r\n```python \r\ndef get_shadow_variables(decay_r):\r\n    names_set, shadow_vars = set(), {}\r\n\r\n    # assumption: shadow variable names appear after\r\n    # regular variable names in alphabetical list\r\n    sorted_items = sorted(tf.train.ExponentialMovingAverage(\r\n        decay_r).variables_to_restore().items(), reverse=True)\r\n\r\n    for key, val in sorted_items:\r\n        if val.name not in names_set:\r\n            names_set.add(val.name)\r\n            shadow_vars[key] = val\r\n\r\n    return shadow_vars\r\n```", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Is this still valid? `tf.moving_average_variables` Is a TF v1 only API now.", "I guess this is by now obsolete."]}, {"number": 822, "title": "Often PR are closed instead of merged", "body": "@vrv In the majority of the cases seems that your are following [this merging process](http://stackoverflow.com/questions/23015168/how-to-close-a-github-pull-request-with-a-commit-comment). This alter the github pulse and other stats. Is there a way in your process to let this PRs result as merged?\n", "comments": ["Are you merging something via web interface and other things with command line?\n", "We're using a command line, not the green button, and it inconsistently reports some as merged and some as closed, using the same set of command lines on different PRs.  This seems like a github bug to me, unless someone can tell me what github uses to tell whether something is merged.\n", "It's really infuriating -- any advice would be appreciated. I'd much prefer\nif we could have reliable statistics on merged vs. closed, but I also want\nthe power our scripts give us.\n\nOn Wed, Jan 20, 2016 at 1:20 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> We're using a command line, not the green button, and it inconsistently\n> reports some as merged and some as closed, using the same set of command\n> lines on different PRs. This seems like a github bug to me, unless someone\n> can tell me what github uses to tell whether something is merged.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173362529\n> .\n", "I don't know your script X and what git commands executes but I see the Y effects. E.g. When merged I see:\n\n```\nvrv\u00a0merged commit\u00a09d5d3d0\u00a0into\u00a0tensorflow:master\u00a06 hours ago\n```\n\nWhen closed I see:\n\n```\nvrv\u00a0closed this in\u00a090fb80a\u00a06 hours ago\n```\n\nSo seems to me that the second case was closed only by commit message special syntax like \"Closing\".\n", "Interestingly, all the PRs merged via script have that message (Closes #XXX). Sometimes they show up closes and sometimes merged. May be a race condition what process first does the close/merge. I'll remove the \"Closes ...\" from the message to see if that fixes it.\n", "Then the PR sometimes remains open. Give it a shot though\n", "I repeat I don't know your exact git commands sequence execution but @vrv declared that the sequence was always the same. So I suspected the race condition.\n", "I just merged #826 without the \"Closes #826\" part and that seemed to work.\nI hope we don't get PRs staying open this way again.\n\nOn Wed, Jan 20, 2016 at 4:14 PM bhack notifications@github.com wrote:\n\n> I repeat I don't know you exact git command sequence execution but @vrv\n> https://github.com/vrv declared that the sequence was always the same.\n> So I suspected the race condition.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173409660\n> .\n", "@martinwicke If you are interested this is an customizable little Kanban style project board for Github Tensorflow https://waffle.io/tensorflow/tensorflow.\n", "Thanks! That looks useful.\n\nOn Thu, Jan 21, 2016 at 2:46 AM bhack notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke If you are interested this\n> is an opensource customizable little Kanban style project board for Github\n> Tensorflow https://waffle.io/tensorflow/tensorflow\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173534271\n> .\n", "This has worked but have the \"Closes\" syntax in the commit message: https://github.com/tensorflow/tensorflow/pull/832\n", "Yeah, I was using a stale script, I've updated since then.  I'm about to\nmerge another change so we'll see if that removes the race.\n\nOn Thu, Jan 21, 2016 at 11:36 AM, bhack notifications@github.com wrote:\n\n> This has worked but have the \"Closes\" syntax in the commit message: #832\n> https://github.com/tensorflow/tensorflow/pull/832\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173684773\n> .\n", "https://github.com/tensorflow/tensorflow/commit/b11337ea5719a8799f279a41c2cd5c9e75a8acf7\n\nDidn't merge the PR.  Sigh.\n", "My guess is that because we rebase, it doesn't think we merged the PR.  When the existing PR is already synced to current HEAD, it will say \"Merged\".  I don't see a way around this -- basically github expects you to click their merge button.  I'd rather have the PRs automatically closed: at least their closing statement mentions the commit it was in.\n", "Agreed. We'll have this script do more things in the future (like\nsquashing, upon request, and testing), so the merge button just won't cut\nit. Sad for record-keeping, but otherwise shouldn't be a big problem.\n\nOn Thu, Jan 21, 2016 at 11:46 AM Vijay Vasudevan notifications@github.com\nwrote:\n\n> My guess is that because we rebase, it doesn't think we merged the PR.\n> When the existing PR is already synced to current HEAD, it will say\n> \"Merged\". I don't see a way around this -- basically github expects you to\n> click their merge button. I'd rather have the PRs automatically closed: at\n> least their closing statement mentions the commit it was in.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173687973\n> .\n", "Generally other projects ask to rebase and squash to the contributor before merging. If this will be the policy I think the merge by script will be safe. What do you think?\n", "We do ask contributors to squash, but if we require our clients to rebase, every time we accept another PR we'll have to ask the client to re-rebase :).  Since we own the merging process, it makes more sense for us to do the rebasing.\n", "What do you think of http://stackoverflow.com/questions/27740369/pull-requests-merged-manually-after-a-rebase-dont-show-as-merged-on-github?\n", "And also http://blog.differential.com/best-way-to-merge-a-github-pull-request/\n", "Yup, the section under \"Catch Feature Up with Master by Rebasing, then merge --no-ff\" is exactly what our script is doing.  The point is that we're the ones doing that, rather than the author.\n\nIt seems the best solution would be for github to have special magic like \"Closes #PR\" but for \"Merges #PR\".\n", "@vrv It seems someone have contacted github support https://github.com/nodejs/node/issues/4176\n", "@vrv Found the (half)official ticket https://github.com/isaacs/github/issues/2.\n", "Thanks for finding that!  Doesn't lend confidence in that getting implemented any time soon though. :/\n"]}, {"number": 821, "title": "String input producer creates trainable variable.", "body": "Using tf.train.string_input_producer function, creates trainable variable. In addition its tensor of int type. Example:\n\nfilename_queue = tf.train.string_input_producer([\"test\"], num_epochs=1, name='input_producer_test')\nassert tf.trainable_variables()[0].dtype.is_integer\n\nIt becomes problem, when uses variable_averages.apply(...), which require, that each trainable variable must be float type.\n", "comments": ["Is it something that was fixed recently? I only see \"limit_epochs\" as a variable made by string_input_producer, and it's regular, not trainable variable -- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/input.py#L76\n", "Sorry. Yes it is.\n"]}, {"number": 820, "title": "ci_build: fix bash array usage", "body": "- fixes the ci_build part of python3 build\n", "comments": []}, {"number": 819, "title": "./configure: line 26: ./util/python/python_config.sh: no such file or directory ", "body": "While running ./configure and then comes to an issue, ./configure: line 26: ./util/python/python_config.sh: no such file or directory. Could somebody give me a little help?\n", "comments": ["That's mysterious. The file is clearly in the repo ([here](https://github.com/tensorflow/tensorflow/blob/master/util/python/python_config.sh)). Can you make sure that you checked everything out properly?\n", "Closing due to staleness / lack of reproducibility.  Please request to re-open if it's still a problem.\n", "was working on a different fork - it seems the file isn't there for me. although there's a BUILD file. \r\n<img width=\"797\" alt=\"screen shot 2017-06-08 at 10 58 56 am\" src=\"https://user-images.githubusercontent.com/289994/26935270-80e478f4-4c39-11e7-900a-f1a09fea11f0.png\">\r\n"]}]