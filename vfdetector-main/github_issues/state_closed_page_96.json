[{"number": 52262, "title": "ERROR: Using member tf.contrib.layers", "body": "Hi, I converted my code from v1.x to v2.x and now I have the following erros:\r\n\r\nERROR: Using member tf.contrib.layers.l1_l2_regularizer in deprecated module tf.contrib. tf.contrib.layers.l1_l2_regularizer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n\r\nand\r\n\r\nERROR: Using member tf.contrib.layers.apply_regularization in deprecated module tf.contrib. tf.contrib.layers.apply_regularization cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n\r\nfor the following lines of code:\r\n\r\n ` l1_l2_regularization = tf.contrib.layers.l1_l2_regularizer(scale_l1=self.h['l1_const'], scale_l2=self.h['l2_const'], scope=None)`\r\n` vars_ = tf.compat.v1.trainable_variables()`\r\n` regularization_penalty = tf.contrib.layers.apply_regularization(l1_l2_regularization, vars_)`\r\n\r\nCan someone help me how to convert the above lines of code to accommodate v2.x?\r\n\r\nAny help\r\nThanks", "comments": ["@Edicient ,\r\nCan you please take a look at this links [link1](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers), [link2](https://github.com/keras-team/keras/blob/v2.6.0/keras/regularizers.py#L46-L207), [link3](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/regularizers/l1_l2) which provides the information on regualrizers.It helps.Thanks", "I managed to convert the following line:\r\n`l1_l2_regularization =tf.contrib.layers.l1_l2_regularizer(scale_l1=self.h['l1_const'], scale_l2=self.h['l2_const'], scope=None)`\r\nwith this line:\r\n`l1_l2_regularization = tf.keras.regularizers.L1L2(self.h['l1_const'], self.h['l2_const'])`\r\n\r\nBut I'm sure if both lines produce similar outputs. \r\n\r\nI'm not familiar with the original functions `tf.contrib.layers.l1_l2_regularizer` and `tf.contrib.layers.apply_regularization` and their expected outputs.\r\nCan you please tell me what is the original outputs so I can figure out how to replace them with the new version?\r\nThanks\r\n", "@Edicient ,\r\nPlease take a look at this [link1](http://tensorflow.biotecan.com/python/Python_1.8/tensorflow.google.cn/api_docs/python/tf/contrib/layers/apply_regularization.html) and [link2](https://docs.w3cub.com/tensorflow~1.15/contrib/layers/l1_l2_regularizer) which provides the information on tf.contrib.layers.apply_regularization and tf.contrib.layers.l1_l2_regularizer.It helps. Also please post  in either stackoverflow or tf discussion forum as it is not feature or bug.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52262\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52262\">No</a>\n"]}, {"number": 52261, "title": "Update MklMatMulPrimitiveFactory to support Arm Compute Library backend", "body": "Related to issue #47415 and PR #47775. Adding support for caching matmul primitives.\r\nUpdates onednn_acl_primitives.patch to include matmul primitives.", "comments": ["@penpornk thanks!\r\nIf this can be cherry-picked onto 2.7 rls that would be great!"]}, {"number": 52260, "title": "No gradient is provided to any variable. This occurs only when I want to use lambda layer for keras.argmax function and keep a model non trainable", "body": "`def define_gan(g_model, d_model,timestep):\r\n  \r\n    discriminator.trainable = False\r\n    generator.trainable= True\r\n    \r\n    a = Input(shape=(timestep,))\r\n    b = g_model(a)\r\n    c=K.argmax(b,axis=-1)\r\n    output_d = d_model(c)\r\n    model = Model(inputs=a,outputs=output_d)\r\n    \r\n    # compile model\r\n    opt = Adam(lr=0.0002, beta_1=0.5)\r\n    model.compile(loss=['binary_crossentropy'], optimizer=opt,metrics=['accuracy'])\r\n    model.summary()\r\n    return model`", "comments": ["Also When I keep discriminator.trainable= True. The gan model works fine and learns.", "Interestingly if the generator is kept non-trainable and the discriminator keeps trainable then also it works. Only for the case when generator trainable and discriminator non-trainable produce the issue. In that case, is it the lamda layer causing the issue? Generator learns outside the gan model properly.\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/66197947/136146755-afffd33c-696e-44b0-9ea2-f8508bc5d1d9.png)\r\n", "Hi @Mizanur4E ! Sorry for the late response. Could you please share a sample stand-alone  code to reproduce this issue  as Gist for reference? Feel free to go through similar issues l[ink1](https://stackoverflow.com/questions/41689451/valueerror-no-gradients-provided-for-any-variable),[link2](https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+is%3Aopen+No+gradient+is+provided+to+any+variable.). Thanks!", "https://gist.github.com/Mizanur4E/0223074ea9f45b33b42357155b3a5b46\r\nplease have a look", "@mohantym  have found the solution?", "https://gist.github.com/Mizanur4E/32bb5a65943ea6b2a0f8e04cd153719c\r\n\r\nCheck this public gist", "Hi @Saduf2019! Could you please look into this issue ? Replicating in  Keras [2.4.3](https://colab.research.google.com/gist/mohantym/9f93bd9a7f49c33b2d1839968da93ab4/github_52260.ipynb#scrollTo=rWr6NlpfvSbB) and [2.6](https://colab.research.google.com/gist/mohantym/6e40fdb0c8df1de92f53d80a4972c8e2/github_52260.ipynb#scrollTo=rWr6NlpfvSbB)", "@Mizanur4E \r\nThe training code is incorrect.\r\nRequest you to refer to below GAN example:\r\nhttps://www.tensorflow.org/tutorials/generative/dcgan ", "@Saduf2019 will you please  tell me where the code is incorrect. But have you noticed code works well if we make both generator and Discriminator trainable in GAN model. ", "@Mizanur4E \r\nCan you please create this issue in tf discussion forum as this is not a bug, there is a larger community to support.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52260\">No</a>\n"]}, {"number": 52257, "title": "[Go] Perform validation on slices when converting Go values to tensors", "body": "PR adds some validation in NewTensor such that it errors prior to building a tensor in the case of dimension mismatch.  960121c has the side effect of reintroducing the segfaults addressed by #50508, but the approach taken in this latter PR is no longer viable. ", "comments": ["> This LGTM but can we also add a unit test that would result in segfault if this gets reverted/broken again?\r\n\r\n@mihaimaruseac please take a look.  This seems to be very effective and test takes about 500ms here.  Probably this could be replaced by some smarter / less brute approach at a later time."]}, {"number": 52256, "title": "[Go] Fix flaky test of NewTensor", "body": "PR fixes a flaky test of NewTensor where `tensor` is flagged by GC and its finalizer is executed before comparison to the expected/test value, resulting in occasional failures when the test is run.", "comments": []}, {"number": 52255, "title": "Model Maker Object Detection Tutorial Bug", "body": "I ran the Model Maker Object Detection Tutorial via Colab.\r\n(https://colab.research.google.com/drive/1DhxMGuQ9ep9mrfDBrFBx47zmOeEOn9_W#scrollTo=qhl8lqVamEty)\r\n\r\nHowever, a problem occurred in\r\n `model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)`.\r\n\r\n```\r\nEpoch 1/50\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-5-187f39c1697e> in <module>()\r\n----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n\t [[Func/cond/then/_3378/input/_6828/_56]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_96849]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n```\r\n\r\nPlease solve this problem.", "comments": ["Hi @stist1111! \r\nCould you please the issue template ? it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "Thanks for the reply @mohanty\r\n\r\n\r\nThe issue template is as follows.\r\n\r\n`https://colab.research.google.com/drive/1DhxMGuQ9ep9mrfDBrFBx47zmOeEOn9_W#scrollTo=qhl8lqVamEty`\r\n\r\nWe inadvertently did not allow access.\r\nSince access is allowed, you can check the error by entering the link.\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------------\r\na problem occurred in\r\n\r\n`model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data).`\r\n\r\n```Epoch 1/50\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-5-187f39c1697e> in <module>()\r\n----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n\t [[Func/cond/then/_3378/input/_6828/_56]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_96849]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function```", "Hi @mohantym \r\nthere's something I haven't said\r\nAn error occurs when running on gpu in colab, but no error occurs when running the same code through cpu", "Ok @stist1111! Thanks for confirming that. Could you please look at  these similar issues .[ Link1](https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in) .Could you please confirm the Tensorflow version,Cuda and Cudnn version too as It seems to be a Cuda mismatch issue while running  in GPU.Tested configurations can be found from [here](https://www.tensorflow.org/install/source#gpu). Thanks!", "I have the same problem\r\n\r\n`UnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n\t [[cond_5/then/_3428/batch_learning_rate/ReadVariableOp/_94]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_96848]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function`", "Could you please confirm  Tensorflow version ,CUDA and CuDNN version too?", " Thank you for answer. @mohantym\r\nI use colab.\r\nIs it possible to change the cuda and cudnn versions within colab?\r\nI did a Google search and couldn't find it.\r\n\r\n\r\ntf version : 2.5.0\r\n\r\nResults using !nvidia-smi\r\n\r\n```\r\nThu Oct  7 10:27:23 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nResults using  !nvcc --version\r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Mon_Oct_12_20:09:46_PDT_2020\r\nCuda compilation tools, release 11.1, V11.1.105\r\nBuild cuda_11.1.TC455_06.29190527_0\r\n```\r\n\r\n", "Hi @stist1111 , I replicated this issue in [TF 2.5](https://colab.research.google.com/gist/mohantym/80f2d61ed8da3807d11942edf47ce527/model-maker-object-detection-tutorial.ipynb#scrollTo=CtdZ-JDwMimd) and resolved in [TF 2.6](https://colab.research.google.com/gist/mohantym/ea4213967e99463b798da29d65342b52/model-maker-object-detection-tutorial.ipynb#scrollTo=-BzCHLWJ6h7q) . \r\nReference - https://www.tensorflow.org/lite/tutorials/model_maker_image_classification  ,Thanks!", "Thank you for answer. @mohantym\r\nUsing TF 2.6 can train without any errors \r\nBut when the training is complete and export model by this code \r\n\r\n```\r\nmodel.export(export_dir='.', export_format=[ExportFormat.TFLITE, ExportFormat.LABEL])\r\n```\r\nand run \r\n\r\n```\r\nmodel.evaluate_tflite('model.tflite', test_data)\r\n```\r\nit caused an exception\r\n```\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: required broadcastable shapes [Op:Mul]\r\n```\r\n\r\nmy code template (https://colab.research.google.com/drive/16YkVjuacsjbgpCSd1IJhU4twX_fSRlGF?authuser=1)", "Hi @sachinprasadhs ! Could you please look at this issue . It's replicating in [2.5](https://colab.research.google.com/gist/mohantym/80f2d61ed8da3807d11942edf47ce527/model-maker-object-detection-tutorial.ipynb#scrollTo=CtdZ-JDwMimd) ,[nightly ](https://colab.research.google.com/gist/mohantym/bb13f9f0e32f55f38006b8e0a7a707d9/model-maker-object-detection-tutorial.ipynb#scrollTo=CgCDMe0e6jlT) and  was getting different error in [2.6](https://colab.research.google.com/gist/mohantym/ea4213967e99463b798da29d65342b52/model-maker-object-detection-tutorial.ipynb#scrollTo=vn61LJ9QbOPi).", "Thanks, Can you look at this template too? @mohantym \r\nhttps://colab.research.google.com/drive/1piFet0zmbPKbsHbmyn5Yb7o3DASo6DND?authuser=1#scrollTo=8xmnl6Yy7ARn\r\nIt's running on TPU.\r\nNo error occurs when train.\r\nThe problem is when I was going to export using this code\r\n```\r\nmodel.export(export_dir='.', export_format=[ExportFormat.TFLITE, ExportFormat.LABEL])\r\n```\r\nand this is the error.\r\n```\r\nUnimplementedError: File system scheme '[local]' not implemented (file: '/tmp/tmpnck6hl5i/variables/variables_temp/part-00000-of-00001')\r\n\tEncountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors.\r\n```\r\n\r\nCould you please provide the proper export method for me?\r\n\r\n", "Okay, I solved this issue. @stist1111 , @tykwon97.\r\nI ran on GPU, Trained, Evaluate, Export and test on my Flutter project. I worked perfectly!!\ud83d\ude04\r\nJust change TF 2.5.0 to TF 2.4.3\r\n```\r\n!pip install -q tensorflow==2.4.3\r\n```\r\n\r\nthis is my code : https://colab.research.google.com/drive/1SP85b6fRNWbraxOxdTkUZHSEk1nKlveU#scrollTo=qhl8lqVamEty\r\n", "I was able to solve it thanks to @noxhsxrk \r\nthank you!", "I had the same problem, and @noxhsxrk's solution fixed it. Thanks! ", "@mohantym @noxhsxrk Please find the problem with my code! It runs when it is set to CPU...\r\n\r\nhttps://colab.research.google.com/drive/1CIGbbRqEyPEJYGnXY2TLBuj4etWLgN6Z?usp=sharing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52255\">No</a>\n"]}, {"number": 52254, "title": "issue with tf.saved_model.save", "body": "https://github.com/tensorflow/tensorflow/issues/51587\r\n\r\nI cannot understand why this serious issue is not paid any attention for such a  long time", "comments": ["Duplicates #51587\r\n\r\nPlease don't open duplicate issues to raise awareness to other other as this wastes time. Issues are triaged and will be resolved pending time / questions will be asked on the original issue as needed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52254\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52254\">No</a>\n", "Many of my friends turn to pytorch as this kind of situation always happens. I can now understand. So low the development efficiency and communication efficiency !!!!!!!!!!!! Very disappointed !!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, {"number": 52253, "title": "Incorrect Links under API Reference ", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry : \r\nhttps://www.tensorflow.org/community/contribute/docs#api_reference\r\n\r\n\r\n## Description of issue (what needs changing): \r\n\r\n### Clear description\r\n\r\nUnder the description of  [\"API Reference\",](https://www.tensorflow.org/community/contribute/docs#api_reference) there is a link to a google doc for [Tensorflow 2 API Docs Advice](https://docs.google.com/document/d/1e20k9CuaZ_-hp25-sSd8E8qldxKPKQR-SkwojYr_r-U/preview#). This doc is having some links which are not working and may create confusion for any beginner who wants to start contributing to docs. \r\nLinks like - \r\n1. link of [tensorflow docs task tracker](https://docs.google.com/spreadsheets/d/1p3vqbocbKmcZQGrlxk9m2jHuleu8yr-XRbfKum0IDD8/edit?usp=sharing) - this doc is not currently under consideration so this (along with its related info) should be removed to avoid any confusion. \r\n2.  [docstring links](https://www.tensorflow.org/community/documentation) - this link is not working (needs to be correctly linked)\r\n3.  links given for [examples , guide & tutorials](https://github.com/tensorflow/docs/tree/master/site/en/r2) - incorrect link\r\n4. link for [python API documentation](https://www.tensorflow.org/community/documentation#generating_python_api_documentation) - incorrect link\r\n\r\nSo, overall from a beginner's point of view, I can say that this FAQ doc for API is not much useful & helpful So I'll suggest either update it or remove it! \r\n", "comments": ["I can update this doc if this issue seems to be a correct doc-bug and I will surely make one PR in no time if assigned! I just need approval! @lamberta  \r\n\r\n[I don't know who is assigned  & why , so can you please change assignee as well]", "Thanks @robotjellyzone , added you to the assignees", "Thanks, @lamberta ! I just need one suggestion , should i update the sheet or directly remove  this **API DOCS ADVICE** link  from the site ? cause i feel that removing directly the link (& that whole statement ) from the site would be fine!\r\n![image](https://user-images.githubusercontent.com/36916536/136125240-ea4281c2-ae19-4dd4-ace5-bd298880f2a2.png)\r\nAsking this as if you go through the doc, you will find mostly the doc is talking about that **\"task tracker sheet\"** which you mentioned me that it is not under consideration any more !\r\n![image](https://user-images.githubusercontent.com/36916536/136125917-0b2c21c6-0307-40ad-b201-00f539316bed.png)\r\n", "@robotjellyzone ,\r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made.Thanks!", "@robotjellyzone Hmm. I think there's a lot of good advice in that doc ... but, yeah, the tracker links and timeframe are out of date. Ideally, the \"Assessing API Documentation Quality\" section could be extracted and published on the site and then we could just remove the link.\r\n\r\nWhat do you think about taking that section and moving it to the end of this page? https://www.tensorflow.org/community/contribute/docs_ref\r\n@bhack WDYT?\r\n", "I think it is ok if the WEB page will not be too long to scroll. I don't see the source/button in the website (we know it is markdown) but the source for the PR is https://github.com/tensorflow/docs/blob/master/site/en/community/contribute/docs.md\r\n\r\nIn the \"starting from github\" experience we are pointing to the same Docs: https://github.com/tensorflow/docs/blob/master/CONTRIBUTING.md\r\n\r\nMore in general I still think that it is better to have, for every repository, some technical details directly in the community files on github as I've proposed in:\r\nhttps://github.com/tensorflow/community/pull/384\r\n\r\nThan we could add a reference to the community files from the website.\r\n", "Yeah, I see your point about adding to the doc length. And, really, most of that \"Assessing API Documentation Quality\" section is a big code sample. The advice for correct links, description, and example could be added to the [Contribute to the API documentation](https://www.tensorflow.org/community/contribute/docs_ref) page [[here in github](https://github.com/tensorflow/docs/blob/master/site/en/community/contribute/docs_ref.md)].", "Yes what I meant generally (it isn't only for the Docs repository) is to find a balance between the website infos and something technical that could let you to quickly contribute a PR just starting from the community file on the repo and without going to duplicate the source of truth or request to go forth and back too much from Github to the website.", "so @lamberta will it be good to directly remove that statement from the site containing that **\"API DOCS ADVICE LINK\"**? i mean this statement -> \r\n![image](https://user-images.githubusercontent.com/36916536/136247998-6de44cc2-62c5-432d-b890-2841cdcaa509.png)\r\n\r\nthough I feel the same that this doc is having much useful info that we can add at some other part of the website but then that would be another issue I guess or maybe a different pr with the different issue would be better for it. what do you think?", "> will it be good to directly remove that statement from the site containing that \"API DOCS ADVICE LINK\"\r\n\r\nI would say either: 1. keep the link and clarify what folks should look at in that doc, or 2. remove the link and move the useful content into one of the website pages (probably `dic_ref.md`)", "Ok Got the idea of what exactly to do now. I'll try to do it first according to my understanding and then will wait for your suggestions of improvements if any!", "The issue will move to closed status once the PR is merged.", "Hi @lamberta \ud83d\udc4b\ud83c\udffb ! I have added my pr at [#1954](https://github.com/tensorflow/docs/pull/1954) so can you please review that PR & provide me with improvement pointers if any !", "Hi, @tilakrayal  the #[pr1954](https://github.com/tensorflow/docs/pull/1954) for this issue is already merged so can you please close this issue by linking that pr to this issue !", "@robotjellyzone ,\r\nPlease feel free to close this issue as PR has been merged.Thanks", "Ok no problem ! I am closing this issue . Pr - [#1954](https://github.com/tensorflow/docs/pull/1954)", "@robotjellyzone Next time you could automate this with: \r\nhttps://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue", "Thanks, @bhack linked the pr successfully! "]}, {"number": 52252, "title": "tf.data.service worker occasionally fails \"to send worker update\" and will be unrecoverable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Mostly congruent to https://github.com/tensorflow/ecosystem/blob/master/data_service/tf_std_data_server.py \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Image based on `https://gcr.io/deeplearning-platform-release/tf2-gpu.2-5`\r\n- TensorFlow installed from (source or binary): see image\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: 3.7 (see image)\r\n\r\n**Describe the current behavior**\r\nOccasionally, a third of my 10 workers will all report something like this about 30 times\r\n\r\n```\r\n2021-10-05 02:19:10.033421: W tensorflow/core/data/service/worker_impl.cc:311] Failed to send task updates to dispatcher: Not found: Failed to send worker update: Task 4003 not found\r\n```\r\n\r\nand then not serve any data.  This is confirmed by checking network egress metrics for the workers.  And throughput on the training node will collapse.\r\n\r\nThis seems to come from https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/data/service/worker_impl.cc;l=308-312;bpv=0;bpt=1\r\n\r\nThis happens maybe every 3 times I use the service.  It does not seem to recover by itself.  I have to manually restart each worker that is experiencing this to recover throughput.\r\n\r\nIt is okay that this fails, but it does not seem to exit code as a failure and just sits there.\r\n\r\nNone of these workers have high mem pressure or cpu usage.\r\n\r\nNOTE: I am not trying to consume multiple datasets.  there is only 1 job and 1 dataset but multiple consumers (using `strategy.distribute_dataset`)\r\n\r\n**Describe the expected behavior**\r\nEither workers report this an error and exit code properly or retry until success.\r\n\r\n", "comments": ["Hi @sanatmpa1 ! Could you please look at this issue .Attaching [Gist f](https://colab.research.google.com/gist/mohantym/4712a0805340cebc0501c9d5db8aa357/github_52252.ipynb)or reference , Process is not ending once started.", "It would also be ideal if I could understand what causes this failure so I can perhaps construct my dataset to avoid it in the first place.  Thanks!", "@rllin,\r\n\r\nCan you share the stand-alone code or colab gist to reproduce the issue and expedite the trouble-shooting process? Thanks!", "Unfortunately I'm not certain I can give you a reproduction easily as this requires multiple separate machines.  The closest I can recommend would be probably to follow the ecosystem link I pasted to stand things up in GKE (pretty much exactly how we deploy the workers/dispatchers but on tf 2.5).  And then we consume elements aggressively on MirroredStrategy, e.g.:\r\n\r\n```\r\n      from tensorflow.python.data.experimental.ops.data_service_ops import _from_dataset_id, _register_dataset\r\n      # NOTE: use underlying ops directly to avoid compression.\r\n      dataset_id = _register_dataset(\r\n        service=dispatcher_name,\r\n        dataset=dataset,\r\n        compression=None,\r\n      )\r\n      def dataset_fn(input_context):\r\n        del input_context\r\n        return _from_dataset_id(\r\n          processing_mode=\"parallel_epochs\",\r\n          service=dispatcher_name,\r\n          dataset_id=dataset_id,\r\n          element_spec=dataset.element_spec,\r\n          job_name=job_name,\r\n          compression=None,\r\n        ).prefetch(prefetch)\r\n      dataset = strategy.experimental_distribute_datasets_from_function(dataset_fn)\r\n      \r\n...\r\n@tf.function\r\ndef train_step(inputs):\r\n  return tf.identity(inputs)\r\n  \r\nfor element in dataset:\r\n  strategy.run(train_step, element)\r\n  \r\n```", "Thanks @rllin for reporting the issue. It looks like the worker and dispatcher are out-of-sync. Do you know if the dispatcher has been restarted in the problematic jobs? If the dispatcher has been restarted, you can try passing in a `work_dir` and set `fault_tolerant_mode` to `True`. For example:\r\n\r\n```\r\nserver = tf.data.experimental.service.DispatchServer(\r\n    tf.data.experimental.service.DispatcherConfig(\r\n        port=FLAGS.port,\r\n        protocol=\"grpc\",\r\n        work_dir=\"<a path which can be shared by the dispatcher and worker servers>\",\r\n        fault_tolerant_mode=True))\r\n```\r\n", "@yangustc07 the dispatchers were not restarted during these failures afaict (is obvious when looking at pod restarts)\r\n\r\nthis happens very quickly when i first start all the requisite deployments/services and the workers do not recover even if i force restart them it seems.\r\n\r\ni will give that a shot though.  ~i assume this will not require much disk space to write to the log for tolerance?~ looks like i can give it a gs:// path", "Hi @rllin, thanks for trying this. I'm following up to see if using the fault tolerant mode solves the issue? Thanks.", "thanks for checking in @yangustc07 \r\n\r\ntolerant mode does not solve the issue unfortunately.  i verified that journaling was on as my gs:// path was being written to and the dispatcher logged things.\r\n\r\nmore observations:\r\n\r\n- if i use 10 workers, I never get issues, all workers have network egress.\r\n- if i use 30 workers, I will get < 10 workers actually with network egress.\r\n- it looks like some workers that do serve data can still output that log; so that log is not indicative of workers being unable to serve data.\r\n- if a worker is not serving data, it will never serve data (even if i delete that pod and it gets recreated as a new pod)\r\n\r\n\r\ni am consuming data quite aggressively pulling ~5 GB/s of uncompressed data onto a single node with 8 V100 devices", "Sorry for the late reply, I have shared the issue with my team and got some feedback. Here are some suggestions:\r\n\r\n- You can try compressing the data. By default compressing is enabled.\r\n- If you have a reproducible job, please share the instructions with us. That will make it more feasible to debug. For example, can you reproduce it in a unit test?\r\n- You can share the dispatcher journal with us. It will contain information about whether `Task 4003` is ever added to the tf.data service and what other events are happening with it.\r\n\r\nThanks! cc @aaudiber \r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52251, "title": "Remove tf-nightly step from the Use XLA with tf.function guide (jit_compile is stable)", "body": "@cheshire \r\n\r\nThis PR removes the `tf-nightly` step, since `jit_compile` has been stable since TF v2.5. The Colab has been run without `tf-nightly` (which is v2.7 now?).\r\n\r\nHope this helps \ud83d\udc4d ", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/52251\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>"]}, {"number": 52250, "title": "Compiling TF 2.6 in debug mode on Windows env. (hude pdb file)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r2.6.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.3/8\r\n- GPU model and memory: GTX 1060\r\n\r\n**Describe the current behavior**\r\nI try to compile my example program that utilises TF:\r\n// tensorflow/cc/example/example.cc\r\n\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\nint main() {\r\n  using namespace tensorflow;\r\n  using namespace tensorflow::ops;\r\n  Scope root = Scope::NewRootScope();\r\n  // Matrix A = [3 2; -1 0]\r\n  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });\r\n  // Vector b = [3 5]\r\n  auto b = Const(root, { {3.f, 5.f} });\r\n  // v = Ab^T\r\n  auto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\r\n  std::vector<Tensor> outputs;\r\n  ClientSession session(root);\r\n  // Run and fetch v\r\n  TF_CHECK_OK(session.Run({v}, &outputs));\r\n  // Expect outputs[0] == [19; -3]\r\n  LOG(INFO) << outputs[0].matrix<float>();\r\n  return 0;\r\n}\r\n\r\nAfter patching few files to allow debug compilation (https://github.com/tensorflow/tensorflow/issues/51799#issuecomment-912567685) all tensorflow libs compile, but the example linkage still fails.\r\n\r\nbazel build --local_ram_resources=HOST_RAM*.7 --config=dbg --config=windows --copt=/FS --copt=-nvcc_options=disable-warnings --linkopt=/DEBUG:FULL --strip=never --define=no_tensorflow_py_deps=true -s --verbose_explanations --subcommands=pretty_print //tensorflow/cc/example:example\r\n\r\nThe compilation fails with exception:\r\nLINK : fatal error LNK1201: error writing to program database 'E:\\tensorflow_r2.6_cpp_debug_cpu\\output\\xpduztnu\\execroot\\org_tensorflow\\bazel-out\\x64_windows-dbg\\bin\\tensorflow\\cc\\example\\example.pdb'; check for insufficient disk space, invalid path, or insufficient privilege\r\nTarget //tensorflow/cc/example:example failed to build\r\n\r\nexample.pdb has size of 4.41 GB (4,742,363,136 bytes)", "comments": ["@AndreyPlotkinOr \r\n\r\nApparently this is an issue of visual Studio. Please take a look at this thread for workarounds\r\nhttps://software.intel.com/en-us/forums/intel-visual-fortran-compiler-for-windows/topic/362147\r\nThanks!\r\n\r\nYou could also refer to these links and let us know if it helps: [link](https://github.com/tensorflow/tensorflow/issues/31610#issuecomment-782466060),#32366, #38084\r\n", "Unfortunately, we don't fully support a debug compilation. In all operating systems, this would result in binaries that are extremely bloated.\r\n\r\nYou could try using per file options to only add debug info to some objects.", "First of all let me describe the situation: there's a C++ solution with 50+ projects (with several prime projects utilising the others) One of them is using TF. Once compiled, the solution is delivered to clients to their on-premises environments. \r\n\r\nUp till TF 1.15 I used cmake to compile the TF in 2 modes: RELEASE and DEBUG. Release version was used for deliveries, debug version - for debugging needs. Starting from TF 2.0 a cmake compilation approach was voided (in favour to bazel).\r\n\r\nInability to compile TF in debug mode results in inability to compile the project (and thus the whole solution) in debug mode. As a result, there's a solution where developers can NOT use IDE debugging features. So there's an immediate need in TF in debug mode.\r\n\r\nNow, if I use the following command, a TF compilation succeeds:\r\nbazel build -c fastbuild --config=opt --config=windows --copt=/FS --copt=-nvcc_options=disable-warnings --linkopt=/DEBUG:NONE --strip=never --define=no_tensorflow_py_deps=true -s //tensorflow/cc/example:example\r\n\r\nSo a fastbuild is ok. Although the exemple.exe is big (~2GB), is works. \r\nThe example.exe-2.params ends with these flags:\r\n/DEBUG\r\n/OPT:REF\r\n/OPT:ICF\r\n/DEBUG\r\n/OPT:REF\r\n/OPT:ICF\r\n/DEBUG:NONE\r\n/MACHINE:X64\r\n/DEBUG:FASTLINK\r\n/INCREMENTAL:NO\r\n\r\nBut it uses /MD to compile TF internal libraries. I still can't link these libs with my solution, because the solution uses /MDd instead.\r\n\r\nFollowing command fails, because example.pdb is too big:\r\nbazel build -c dbg --config=opt --config=windows --copt=/FS --copt=-nvcc_options=disable-warnings --linkopt=/DEBUG:NONE --strip=never --define=no_tensorflow_py_deps=true -s //tensorflow/cc/example:example\r\n\r\nParams file ends with:\r\n/DEBUG\r\n/OPT:REF\r\n/OPT:ICF\r\n/DEBUG\r\n/OPT:REF\r\n/OPT:ICF\r\n/DEBUG:NONE\r\n/MACHINE:X64\r\n/DEBUG:FULL\r\n/INCREMENTAL:NO", "@AndreyPlotkinOr \r\nCan you please refer to [link](https://github.com/tensorflow/tensorflow/issues/52332#issuecomment-942498254) and move this to closed status.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52250\">No</a>\n"]}, {"number": 52249, "title": "Error compilation NVCC for TF 2.5 from source using Clang-11 for Debian 11", "body": "Hi all!\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Debian 11, kernel 5.10.0-8-amd64 \r\n- Mobile device : No\r\n- TensorFlow installed from: try compile from source \r\n- TensorFlow version: 2.5.1\r\n- Python version: 3.8.12\r\n- Installed using: virtualenv\r\n- Bazel version: 3.7.2\r\n- Compiler version: Debian Clang version 11.1.0-++20210804031632+1fdec59bffc1-1~exp1~20210804132251.11 installed from https://apt.llvm.org/\r\n- CUDA version:  Driver Version: 460.32.03  /  CUDA Version: 11.2 **installed from run**\r\n- cuDNN version: Cudnn_v8.1.1.33\r\n- GPU model and memory: GeForce GTX 1050 2GB RAM single\r\n- NCCL:  2.8.4, for CUDA 11.2, February 03,2021\r\n- RAM:  16GB\r\n\r\n\r\n**Problem description**\r\nPermanently  when I trying  compile TF 2.5.1 from source using Clang-11 raising  error compilation.\r\n**Before compile TF 2.5.1 with Clang-11 I successful compiled  TF 2.5.1  for verification  from source using standard  Debian gcc compiler version 10.2.1-6 - it was compiled without problems** but with a **permanenty  warning** in the IPython console \r\n`: I tensorflow / compiler / mlir / mlir_graph_optimization_pass.cc: 176] None of the MLIR Optimization Passes are enabled\r\n`\r\n\r\n\r\n\r\n**1. Configure TF 2.5.1  screen  before compilation:**\r\n```\r\ntf251) mvg@debian:~/tensorflow$ ./configure\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/mvg/tf251/bin/python3]: \r\n\r\nFound possible Python library paths:\r\n  /home/mvg/tf251/lib/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/mvg/tf251/lib/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.2 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: y\r\nClang will be used as CUDA compiler.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify which clang should be used as device and host compiler. [Default is /usr/lib/llvm-11/bin/clang]: \r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\n\r\n**2. Run compilation command and their console output:**\r\n```\r\n(tf251) mvg@debian:~/tensorflow$ bazel build --config=opt --copt=-march=native --copt=-Wno-sign-compare \\\r\n--copt=-mavx --copt=-mfma  --copt=-mfma4  \\\r\n--copt=-msse4.1 --copt=-msse4.2 --jobs=4 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/mvg/tf251/bin/python3 --action_env PYTHON_LIB_PATH=/home/mvg/tf251/lib/python3.8/site-packages --python_path=/home/mvg/tf251/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-11/bin/clang --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /home/mvg/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/mvg/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:opt in file /home/mvg/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:linux in file /home/mvg/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/mvg/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Build option --action_env has changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (426 packages loaded, 36834 targets configured).\r\nINFO: Found 1 target...\r\n```\r\n\r\n**3. Error log:**\r\n```\r\nERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:54:17: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/nccl_2.8.4/lib:/usr/local/nccl_2.8.4/lib/pkgconfig:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/min_f16_reduce_scatter.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:43:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:47:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:378:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:61:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:436:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:122:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:452:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:125:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:460:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:130:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:133:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\n13 errors generated when compiling for sm_61.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/mvg/tensorflow/tensorflow/lite/python/BUILD:59:10 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/nccl_2.8.4/lib:/usr/local/nccl_2.8.4/lib/pkgconfig:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/min_f16_reduce_scatter.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 870.023s, Critical Path: 58.66s\r\nINFO: 11462 processes: 7400 internal, 4062 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**4. About compability CUDA  v11.2.2 and Clang-11**\r\n1.[ Official documentation](https://docs.nvidia.com/cuda/archive/11.2.2/cuda-installation-guide-linux/index.html) in the partition `Installation Guide Linux` Clang-11 support  declared.\r\n2.  file in the CUDA directory  '/usr/local/cuda-11-2/include/crt/host_config.h` explicitly next stated:\r\n```\r\n#if defined(__clang__) && !defined(__ibmxl_vrm__) && !defined(__ICC) && !defined(__HORIZON__) && !defined(__APPLE__)\r\n#if (__clang_major__ >= 12) || (__clang_major__ < 3) || ((__clang_major__ == 3) &&  (__clang_minor__ < 3))\r\n#error -- unsupported clang version! clang version must be less than 12 and greater than 3.2 . The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\r\n```\r\n3. File ` /home/mvg/tensorflow/.bazelrc` contains pointer for in the next tensofrlow distibution in the partiotion `Remote build execution options (only configured to work with TF team projects for now.)` : \r\n`build:rbe_linux_cuda_clang_base --crosstool_top=\"@ubuntu18.04-clang_manylinux2010-cuda11.2-cudnn8.1-tensorrt7.2_config_cuda//crosstool:toolchain\" `\r\nI have installed Clang-11 - hence following the infomation above I should compile normally TF 2.5.1 from source with Clang-11.\r\n\r\n\r\n**5. The appointment of a responsible specialist for this ticket**\r\nI ask for the solution of this problem to appoint responsible [Artem Belevich Artem-B](https://github.com/Artem-B) and not to football me like balls for various person who haven't knoweledge in the subject and turn the solution into an eternal groundhog day - since every time I retell a human problem, after a while it disappears, a new person appears , I will explain again, the opat disappeared and so on ad infinitum.\r\n\r\nBest regards, Vadim Maklakov.\r\n\r\nP.S. Information for Artem-B - I think that   problem is somehow related to nccl. It doesn't matter if nccl is installed or not - it is constantly mentioned in the compilation error log.\r\nP.P.S `/home/mvg/tf251/bin:` - path for Python 3.8.12 virtual environment whe installef all required modules how in [this ](https://www.tensorflow.org/install/source)instruction.\r\n\r\n\r\n", "comments": ["Hi all!\r\nUninstall nccl (reboot, remove reference in the `/etc/ld.so.conf.d/` for nccl library with *.so extensions and remove path to nccl in the .`bashrc`  )  and try compile again with Clang-11 without nccl with this command remain setting above:\r\n```\r\n(tf251) mvg@debian:~/tensorflow$  bazel build --config=opt --copt=-march=native --copt=-Wno-sign-compare \\\r\n--copt=-mavx --copt=-mfma  --copt=-mfma4  \\\r\n--copt=-msse4.1 --copt=-msse4.2 --jobs=4 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/mvg/tf251/bin/python3 --action_env PYTHON_LIB_PATH=/home/mvg/tf251/lib/python3.8/site-packages --python_path=/home/mvg/tf251/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-11/bin/clang --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /home/mvg/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/mvg/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:opt in file /home/mvg/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:linux in file /home/mvg/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/mvg/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/mvg/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/mvg/tensorflow/tensorflow/workspace0.bzl:105:34: in workspace\r\n  /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (426 packages loaded, 36834 targets configured).\r\n```\r\n\r\nand get this error log with again contained pointers to nccl:\r\n\r\n```\r\nERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:54:17: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-11/bin/clang \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/mvg/tf251/bin/python3 \\\r\n    PYTHON_LIB_PATH=/home/mvg/tf251/lib/python3.8/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/k8-opt/bin/external/nccl_archive/_objs/device_lib/functions.cu.d '-frandom-seed=bazel-out/k8-opt/bin/external/nccl_archive/_objs/device_lib/functions.cu.o' -iquote external/nccl_archive -iquote bazel-out/k8-opt/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -Ibazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-march=native' -Wno-sign-compare -mavx -mfma -mfma4 -msse4.1 -msse4.2 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c external/nccl_archive/src/collectives/device/functions.cu.cc -o bazel-out/k8-opt/bin/external/nccl_archive/_objs/device_lib/functions.cu.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:43:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:47:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:378:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:61:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:436:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:122:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:452:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:125:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:460:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:130:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:133:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\n13 errors generated when compiling for sm_61.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 889.248s, Critical Path: 59.41s\r\nINFO: 11776 processes: 8278 internal, 3498 local.\r\n```\r\n\r\n\r\n\r\n\r\n", "This appears to be the same issue as https://bugs.llvm.org/show_bug.cgi?id=47869\r\n\r\nIt's triggered by something defining `_FORTIFY_SOURCE` during compilation. \r\nCompinling CUDA with `-U_FORTIFY_SOURCE` or `-D_FORTIFY_SOURCE=0` may help to work around the issue.\r\n", "@Artem-B, \r\n thank you very much!\r\nTomorrow check yours hypothesis..\r\nI right understand that I must add these parameters `-U_FORTIFY_SOURCE` or `-D_FORTIFY_SOURCE=0` to `bazel build` command?\r\n", "You should pass them via `--copt=-D_FORTIFY_SOURCE=0` flag to bazel.\r\n\r\nI have no idea who/where/how sets _FORTIFY_SOURCE, so I can't tell whether it will work, but it's worth trying.\r\nLooks like it's part of https://wiki.debian.org/Hardening but it's not clear how it gets enabled for bazel builds and what's the right way to disable it. \r\n", "1. Check `--copt=-D_FORTIFY_SOURCE=0 `\r\nwith running  this command:\r\n\r\n```\r\n(tf251) mvg@debian:~/tensorflow$ bazel build --config=opt --copt=-march=native --copt=-Wno-sign-compare \\\r\n--copt=-mavx --copt=-mfma  --copt=-mfma4  \\\r\n--copt=-msse4.1 --copt=-msse4.2 --copt=-D_FORTIFY_SOURCE=0 --jobs=4 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\noutput for command above:\r\n```\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/mvg/tf251/bin/python3 --action_env PYTHON_LIB_PATH=/home/mvg/tf251/lib/python3.8/site-packages --python_path=/home/mvg/tf251/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-11/bin/clang --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /home/mvg/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/mvg/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:opt in file /home/mvg/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:linux in file /home/mvg/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/mvg/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (426 packages loaded, 36834 targets configured).\r\nINFO: Found 1 target...\r\n```\r\ncompilation failed with next messages:\r\n```\r\nERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:54:17: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_sendrecv.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:43:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:47:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:378:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:61:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:436:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:122:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:452:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:125:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:460:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:130:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:133:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\n13 errors generated when compiling for sm_61.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/mvg/tensorflow/tensorflow/lite/python/BUILD:59:10 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_sendrecv.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 1133.000s, Critical Path: 65.14s\r\nINFO: 3000 processes: 8 internal, 2992 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "When I add compilation option ` --copt=-D_FORTIFY_SOURCE=0` in the post above the  I noticed that this line of error log in the part of the first error :\r\n`/usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_sendrecv.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o)\r\n`\r\ncontains this values:\r\n`' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1'`\r\nmay be is here  problem because this option - `-D_FORTIFY_SOURCE=1` does't change value to  setting in bazel  instead required `-D_FORTIFY_SOURCE=0`?\r\ncompilation terminated  when near 11300 row compiled...  ", "Try compiled with  `U_FORTIFY_SOURCE=1`\r\n```\r\n(tf251) mvg@debian:~/tensorflow$ bazel build --config=opt --copt=-march=native --copt=-Wno-sign-compare \\\r\n--copt=-mavx --copt=-mfma  --copt=-mfma4  \\\r\n--copt=-msse4.1 --copt=-msse4.2 --copt=-U_FORTIFY_SOURCE=1 --jobs=4 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\ncompilation terminated when near 17400 row compiled...Error log\r\n```\r\nERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:54:17: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c external/nccl_archive/src/collectives/device/functions.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:43:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:47:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:378:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:61:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:436:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:122:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:452:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:125:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:460:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:130:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:133:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\n13 errors generated when compiling for sm_61.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/mvg/tensorflow/tensorflow/lite/python/BUILD:59:10 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c external/nccl_archive/src/collectives/device/functions.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 8057.579s, Critical Path: 115.60s\r\nINFO: 11662 processes: 554 internal, 11108 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nagain error log contains ' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' that may overwriting additional buzel compilation option  `-D_FORTIFY_SOURCE=0' `and links between `nccl` folder in the TF sources and  local CUDA...\r\nI tried install `nccl` but it didn't work correctly , didn't pass `ncccl-test` and if I have single card  I think that `ncc` doesn't required for my PC.  \r\n\r\n\r\n\r\n", "Summary - I  correct install and test  nccl  and tried compile TF with clang include under root.\r\nIn all cases result same - compilation terminated when beginning compilation  next sentences\r\n```\r\nERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:54:17: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-11/bin/clang -MD -MF \\ bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_sendrecv.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/max_f64_sendrecv.cu.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\n```\r\nArtem, may  exist other way compiling TF with clang from source using cmake like[ Build TensorFlow Lite with CMake](https://www.tensorflow.org/lite/guide/build_cmake)? \r\n Google  how compile TF pip packages  for Ubuntu using only clang...\r\n", "Well, one way or another something ends up running with `_FORTIFY_SOURCES` defined.\r\nI can't tell you why that happens. The fix has already been committed to clang, so if you were to use a nightly build (not sure if it has NVPTX back-end enabled, though) or build it yourself from sources, that should get you past this build issue.\r\n", "@Hi, Artem-B\r\nVersion of my installed clang-11  from debian repo bellow:\r\n\"\"\"\r\n clang-11       1:11.1.0~++20210804031632+1fdec59bffc1-1~exp1~20210804132251>\r\n\"\"\"\r\nPatch fixed this trouble added in the end of September this year\r\nTry using clang from LLVM repo", "clang-11 or any other release currently out there will not have the changes I've committed just few days ago. \r\n\r\nYou choices are:\r\n* move to linux distro which does not enable fortified sources by default, or\r\n* figure out how to disable fortified sources on your Debian machine, or\r\n* build and use your own clang from sources, or\r\n* try using nightly snapshot of prebuilt packages from https://apt.llvm.org/, or\r\n* apply the following diff to clang headers you have installed on your machine : https://github.com/llvm/llvm-project/commit/29e00b29f76adb15a51c1ccd6c1fdb6fce5f4d7b.diff\r\n", "@Artem-B, hi!\r\nI continue my kampf  :)\r\nAs they say in one country: `The rescue of drowning people is the work of the drowning people themselves`  :)\r\nI think that problem in the turning back hardening on on the time compilation because  TF sources  designed first of all for compilation with gcc...\r\nI can't install clang-11 because  some application package installed which part of llvm-11 from the official Debian repo and I will use llvm-12+clang from https://apt.llvm.org \r\nversion: Debian clang version 12.0.1-++20211011094445+fed41342a82f-1~exp1~20211011214930.5\r\nBefore compiling I used magic command for nvcc - export NVCC_PREPEND_FLAGS=\"--allow-unsupported-compiler\" and use it for compiling TF with clang-12. For test I successful compiled `nccl` and nccl-test  using CUDA and Clang-11.\r\nBefore compilation  I do next action:\r\n1. Set default cc, cpp and c++ from clang-12 using next commnads:\r\n```\r\nsudo update-alternatives --config c++\r\nThere are 2 choices for the alternative c++ (providing /usr/bin/c++).\r\n  Selection    Path                          Priority   Status\r\n------------------------------------------------------------\r\n  0            /usr/bin/g++                   60        auto mode\r\n  1            /usr/bin/g++                   60        manual mode\r\n* 2            /usr/lib/llvm-12/bin/clang++   10        manual mode\r\n\r\nsudo update-alternatives --config cpp\r\nThere are 2 choices for the alternative cpp (providing /usr/bin/cpp).\r\n\r\n  Selection    Path                            Priority   Status\r\n------------------------------------------------------------\r\n  0            /usr/bin/cpp-10                  60        auto mode\r\n  1            /usr/bin/cpp-10                  60        manual mode\r\n* 2            /usr/lib/llvm-12/bin/clang-cpp   10        manual mode\r\n\r\nsudo update-alternatives --config c++\r\nThere are 2 choices for the alternative c++ (providing /usr/bin/c++).\r\n\r\n  Selection    Path                          Priority   Status\r\n------------------------------------------------------------\r\n* 0            /usr/bin/g++                   60        auto mode\r\n  1            /usr/bin/g++                   60        manual mode\r\n  2            /usr/lib/llvm-12/bin/clang++   10        manual mode\r\n```\r\n\r\n2. Check hardgeng\r\n```\r\ndpkg-buildflags --dump\r\nCFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\nCPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2\r\nCXXFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\nDFLAGS=-frelease\r\nFCFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong\r\nFFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong\r\nGCJFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong\r\nLDFLAGS=-Wl,-z,relro\r\nOBJCFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\nOBJCXXFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\n```\r\n\r\n3. Turn off fortify and check\r\n```\r\nexport DEB_BUILD_MAINT_OPTIONS=hardening=-pie,-fortify\r\ndpkg-buildflags --dump\r\nCFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\nCPPFLAGS=-Wdate-time\r\nCXXFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\nDFLAGS=-frelease\r\nFCFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong\r\nFFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong\r\nGCJFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong\r\nLDFLAGS=-Wl,-z,relro\r\nOBJCFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\nOBJCXXFLAGS=-g -O2 -ffile-prefix-map=/home/mvg=. -fstack-protector-strong -Wformat -Werror=format-security\r\n```\r\nAs  see above fortify turnoff for these terminal session.\r\n\r\n**Compilation**\r\nConfigure with ./configure and run compilation\r\n```\r\nbazel build --config=opt --copt=-march=native --copt=-Wno-sign-compare \\\r\n--copt=-mavx --copt=-mfma  --copt=-mfma4  \\\r\n--copt=-msse4.1 --copt=-msse4.2 --jobs=5 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\nafter compilation ~ 20250 sources compilation terminated with this error\r\n```\r\nERROR: /home/mvg/.cache/bazel/_bazel_mvg/0b49a1d111ba9d5e8b50175749244e64/external/nccl_archive/BUILD.bazel:54:17: \\\r\n  C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/mvg/.cache/bazel/_bazel_mvg/0b49a1d111ba9d5e8b50175749244e64/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mvg/tf251/bin:/usr/local/mlir-14/bin: \\\r\n    /usr/lib/llvm-12/bin:/usr/local/cuda-11.2/extras/CUPTI/lib64: \\\r\n    /usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin: \\\r\n    /usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/lib/llvm-12/bin/clang -MD -MF bazel \\\r\n  -out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.d ' \\\r\n  -frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.o' \\\r\n  -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive \\\r\n  -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda \\\r\n  -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs \\\r\n  -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs \\\r\n  -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual \\\r\n  -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs \\\r\n  -isystem external/local_config_cuda/cuda \\\r\n  -isystem bazel-out/host/bin/external/local_config_cuda/cuda \\\r\n  -isystem external/local_config_cuda/cuda/cuda/include \\\r\n  -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include \\\r\n  -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' \\\r\n  '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' \\\r\n  -fstack-protector -Wall -Wno-invalid-partial-specialization \\\r\n  -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 \\\r\n  -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' \\\r\n  -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '\\\r\n  -Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '\\\r\n  --cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '\\\r\n  -maxrregcount=96' -c external/nccl_archive/src/collectives/device/functions.cu.cc \\\r\n  -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/functions.cu.o)\r\n\r\n```\r\nas see above magically fortify turn on `-U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' `\r\nopen file `functions.cu.d` in the source directory\r\n`/tensorflow/bazel-out/host/bin/external/nccl_archive/_objs/device_lib/`\r\nand see reference to our favorite hardening\r\n```\r\n112 /usr/include/string.h /usr/include/string_fortified.h \\\r\n113 /usr/include/x86_64-linux-gnu/bits/strings_fortified.h \\\r\n```\r\nThe  `string_fortified.h` and  `strings_fortified.h` contains check condition for turning off fortified\r\n```\r\n#ifndef _BITS_STRING_FORTIFIED_H\r\n#define _BITS_STRING_FORTIFIED_H 1\r\n```\r\nwhich check hardening and if it turn off override hardening to turn on state \r\nas we see from next part error log bellow:\r\n```\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:432:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:43:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:47:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:378:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:61:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:436:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:122:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:452:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:125:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:460:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:130:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:495:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:133:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\n13 errors generated when compiling for sm_61.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nI am not an expert in C ++ and ask the question is -  is it possible to somehow correct the fortify check in the  `string_fortified.h` and  `strings_fortified.h`   at compilation time?\r\nOr would it be easier to ask configs from your colleagues who compiling TF pip packages with clang   on the Ubuntu? \r\nIn the fact Ubuntu is  Debian testing with same hardeng.\r\nIn the attachment files `functions.cu.d`  , `string_fortified.h` and  `strings_fortified.h` \r\n[functions.cu.d.zip](https://github.com/tensorflow/tensorflow/files/7337367/functions.cu.d.zip)\r\nBest regards, Vadim Maklakov.\r\n", "Let's keep things simple. If you need it to work just on your machine, just find __clang_cuda_runtime_wrapper.h installed  by clang and make this change there: \r\n```diff\r\ndiff --git a/clang/lib/Headers/__clang_cuda_runtime_wrapper.h b/clang/lib/Headers/__clang_cuda_runtime_wrapper.h\r\nindex 98fb561ca51b5..33aa25fb2d73c 100644\r\n--- a/clang/lib/Headers/__clang_cuda_runtime_wrapper.h\r\n+++ b/clang/lib/Headers/__clang_cuda_runtime_wrapper.h\r\n@@ -41,6 +41,7 @@\r\n #include <cmath>\r\n #include <cstdlib>\r\n #include <stdlib.h>\r\n+#include <string.h>\r\n #undef __CUDACC__\r\n \r\n // Preserve common macros that will be changed below by us or by CUDA\r\n@@ -205,11 +206,6 @@ inline __host__ double __signbitd(double x) {\r\n #endif\r\n \r\n #if CUDA_VERSION >= 9000\r\n-// CUDA-9.2 needs host-side memcpy for some host functions in\r\n-// device_functions.hpp\r\n-#if CUDA_VERSION >= 9020\r\n-#include <string.h>\r\n-#endif\r\n #include \"crt/math_functions.hpp\"\r\n #else\r\n #include \"math_functions.hpp\"\r\n```\r\n\r\nIf you need the build to work for everyone on Debian, you'll either need to wait until fixed clang package is available. \r\nThey do seem to have recent enough versions already: https://packages.debian.org/sid/clang-14\r\n\r\nI do not know how to turn off `fortified` compilation on Debian, so I can't help you there beyond what you've already tried.", "@Artem-B\r\nI compile with clang-12 installed from nightly build https://apt.llvm.org\r\nclang version now: \r\n`12.0.1-++20211011094445+fed41342a82f-1~exp1~20211011214930.5\r\n`\r\nI check `__clang_cuda_runtime_wrapper.h` in the  `/usr/lib/clang/12.0.1/include/` but I don't changes from [commit](https://github.com/llvm/llvm-project/commit/29e00b29f76adb15a51c1ccd6c1fdb6fce5f4d7b.diff)\r\n\r\npart 1\r\n```\r\n41 #include <cmath>\r\n42 #include <cstdlib>\r\n43 #include <stdlib.h>\r\n46 #undef __CUDACC__\r\n45\r\n46 // Preserve common macros that will be changed below by us or by CUDA\r\n```\r\npart 2 \r\n\r\n```\r\n207 #if CUDA_VERSION >= 9000\r\n208 // CUDA-9.2 needs host-side memcpy for some host functions in\r\n209 // device_functions.hpp\r\n210 #if CUDA_VERSION >= 9020\r\n211 #include <string.h>\r\n212 #endif\r\n213 #include \"crt/math_functions.hpp\"\r\n214 #else\r\n215 #include \"math_functions.hpp\"\r\n216 #endif\r\n```\r\n\r\nA fairy tale is told quickly, but it takes a long time :)\r\nIsn't yet commit go to compilation binaries clang-12 :(\r\nI want use for single local machine, tomorrow check new release clang- 12 or patch file `__clang_cuda_runtime_wrapper.h` locally.", "@Artem-B , hi!\r\nThank you very much for your help and patch... After patching `__clang_cuda_runtime_wrapper.h` in the `/usr/lib/clang/12.0.1/include` TF successful compiled after 16870 sec.\r\nNow the output console doesn't warning  about  the lack of MLIR, but the output in the console in the IDE is still different when I checked issue [#51997](https://github.com/tensorflow/tensorflow/issues/51997). Therefore, when I   try will developing the  AI applications, I have to visually   check its in the Jupyter console. :) \r\n\r\nThank you again and I close this ticket.\r\nAt this moment this post  the our patch while  doesn't  appear  in the binaries clang-12 in the https://apt.llvm.org.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52249\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52249\">No</a>\n"]}, {"number": 52248, "title": "[TF:TRT] Enable TensorRT explicit precision (QDQ/QAT) support", "body": "Adds TensorRT QDQ support (\"explicit precision mode\"), also sometimes referred to as \"QAT support\", referring to the training algorithm where QDQ nodes are used. From here on, we refer to the existing non-explicit precision pathway in the code base as the \"dynamic range INT8\" mode (DR INT8) and the new mode as the QDQ INT8 mode.\r\n\r\nIn the new mode, TF `QuantizeAndDequantize` operations are converted to TensorRT quantization scaling layers. Both the new QDQ mode logic as well as existing DR mode logic (`ConvertQuantize`) are moved into the file `convert/ops/quantization_ops.cc`. \r\n\r\nIn addition, in QDQ mode it is necessary to prevent the existing Grappler optimizations invoked in `trt_convert.py` on the loaded SavedModel from folding frozen `QuantizeAndDequantizeV2` operations between weighted ops (Conv, Matmul ,etc) and the weight constants. Thus, we depend on the experimental Grappler rewriter config option `experimental_disable_folding_quantization_emulation` and will be affected if it is removed. The alternative is to allow Grappler folding of the QDQ and constant weights and inserting identity QDQ scale factors manually during TensorRT network construction, but the logic becomes extremely verbose .\r\n\r\nA test suite is added in `convert/ops/quantization_ops_test.cc`. It builds a variety of sub-graph patterns and tests for conversion success. Because TRT QDQ mode has evolved significantly in terms of robustness and features between TRT 7 and TRT8, a set of test waive/skip policies are added indicating which patterns of use are appropriate for TRT7 vs TRT8", "comments": ["@christopherbate Can you please resolve conflicts? Thanks!", "Conflicts resolved", "I didn't review the change for the existing for to add use_explicit_precision, assuming that will be gone.", "Made a slight rebase error, will push up small diff as new commit", "Please explicitly state when this PR is ready for review again. I did a quick check, there are unaddressed comments, and we still use the option use_explicit_precision.", "> Please explicitly state when this PR is ready for review again. I did a quick check, there are unaddressed comments, and we still use the option use_explicit_precision.\r\n\r\nI'm going through it now and marking comments resolved as I make corrections, so that I can keep track. I will post here when I have pushed the actual changes. You can see my last push was 4 days ago.", "I pushed an update to:\r\n- Removed changes from `trt_convert.py`\r\n- Address comments not marked \"resolved\"\r\n\r\nNeed to check whether INSTANTIATE_TEST_CASE_P will allow me to create the correct parameters with the logic present in the function I currently have to enumerate test case parameters.", "@bixia1 I am using \"INSTATITATE_TEST_CASE_P\"... that was deprecated and renamed to \"INSTATIATE_TEST_SUITE_P\", which you can see at the bottom of `quantization_ops_test.cc`. I don't think I can avoid writing a function to enumerate the test suite parameters, since there is some logic that needs to decide exactly how to combine them. It's not an exact Cartesian product of Param1Range x Param2Range x ... x ParamNRange.", "Updated PR description. All comments are addressed.", "rebased", "Pushed changes to add API changes in C++/Python, updated unit tests to use TrtConvert C++ API functions", "still working on fixing some tests after converting to the fixture", "All done. Let me know when you want me to squash.", "Completely disabled explicit QDQ tests and grappler options (disable_emulate_quantization_folding) for TRT < 8.0.", "@bixia1 Good to squash?", "@christopherbate Can you please resolve conflicts? Thanks!\r\n", "> @christopherbate Can you please resolve conflicts? Thanks!\r\n\r\nrebased, thanks", "@christopherbate please squash and I will approve.", "squashed"]}, {"number": 52247, "title": "PSv2: Disable parameter_server_strategy_v2_test on macos to unblock 2\u2026", "body": "\u2026.7 release.\r\n\r\nPiperOrigin-RevId: 400280224\r\nChange-Id: Iec94fb2cf05e8ed2fa1e2dac111470c32b75b60d", "comments": []}, {"number": 52246, "title": "Update README.md :Added TensorFlow: TensorFlow 2 for Deep Learning Specialization to resources tab", "body": "Added TensorFlow: TensorFlow 2 for Deep Learning Specialization  to resources tab. Extremely helpful specialization by Dr Kevin Webster for more in depth knowledge of TensorFlow.", "comments": ["@MarkDaoust  the PR failed a check ,can you suggest what went wrong ?"]}, {"number": 52245, "title": "model.fit() showing value error. I don't know where i am doing wrong.", "body": "Epoch 1/5\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-24-3875a67a7083> in <module>()\r\n     50 model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'] )\r\n     51 \r\n---> 52 history = model.fit(train_x, train_y, epochs=5)\r\n     53 \r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    992           except Exception as e:  # pylint:disable=broad-except\r\n    993             if hasattr(e, \"ag_error_metadata\"):\r\n--> 994               raise e.ag_error_metadata.to_exception(e)\r\n    995             else:\r\n    996               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *\r\n        return step_function(self, iterator)\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:789 train_step\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:201 __call__\r\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\r\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:141 __call__\r\n        losses = call_fn(y_true, y_pred)\r\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:245 call  **\r\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:1666 categorical_crossentropy\r\n        y_true, y_pred, from_logits=from_logits, axis=axis)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/keras/backend.py:4839 categorical_crossentropy\r\n        target.shape.assert_is_compatible_with(output.shape)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:1161 assert_is_compatible_with\r\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (None, 1) and (None, 10) are incompatible", "comments": ["Hi @neha13rathore! Could you please fill the template from[ here ](https://github.com/tensorflow/tensorflow/issues/new/choose)as It helps expedite the issues . ", "Hi @neha13rathore ,@mohantym , I believe the error is arising out of incompatible shapes for categorical crossentropy.  I think the required  shapes are (?,1) and in the penultimate layer the input to the cross entropy loss is (?,10).  \r\n`y_true, y_pred, from_logits=from_logits, axis=axis)`\r\nHowever it is difficult to definitely provide a solution without a sample codebase . (Maybe looking at the output of the layer before cross entropy layer be of help) \r\n", "Hi @neha13rathore ,Could you check with @abhilash1910 's comments ?", "> Hi @neha13rathore ,@mohantym , I believe the error is arising out of incompatible shapes for categorical crossentropy. I think the required shapes are (?,1) and in the penultimate layer the input to the cross entropy loss is (?,10). `y_true, y_pred, from_logits=from_logits, axis=axis)` However it is difficult to definitely provide a solution without a sample codebase . (Maybe looking at the output of the layer before cross entropy layer be of help)\r\n\r\nthanks\r\nproblem has been solved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52245\">No</a>\n"]}, {"number": 52243, "title": "[Go] Update genop shell script to support module-aware installation ", "body": "PR updates genop/generate.sh for Go 1.17 compatibility: the use of `go get` to install executables is [deprecated in Go 1.17](https://golang.org/doc/go-get-install-deprecation).", "comments": ["@wamuir Can you please fix build failures ? Thanks!", "@gbaned Yes, however I cannot see details on the failing builds.  Did these builds run?  Can you provide or link to information on the failures? Thanks!", "> @gbaned Yes, however I cannot see details on the failing builds. Did these builds run? Can you provide or link to information on the failures? Thanks!\r\n\r\n@wamuir  I have triggered the build checks again and most of the are successfully completed. No action required from you at this moment.  Thank you!"]}, {"number": 52242, "title": "feat(base64image): Added Base64 String Image Support for Keras load i\u2026", "body": "## Added Base64 Image String types for easier use in web based api \r\n\r\n```python\r\n  image = tf.keras.preprocessing.image.load_base64_2_image(base64_string, color_mode=\"rgb\", target_size=\"nearest\")\r\n\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52242) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52242) for more info**.\r\n\r\n@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52242) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52242) for more info**.\r\n\r\n@googlebot I signed it!\r\n", "It looks like your PR relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. Thankyou.\r\n@fchollet, @qlzh727"]}, {"number": 52241, "title": "Add mlir-hlo unfuse-batch-norm-training pattern", "body": "Add mlir-hlo unfuse-batch-norm-training pattern.\r\nQuestion when adding dynamic shape: how can I convert `index` to `tensor<f32>`? I need to convert the dynamic reduction size to a `tensor` value. @joker-eph", "comments": ["> Add mlir-hlo unfuse-batch-norm-training pattern.\r\n> Question when adding dynamic shape: how can I convert `index` to `tensor<f32>`? I need to convert the dynamic reduction size to a `tensor` value. @joker-eph\r\n\r\nI use `std.index_cast` and `mhlo.convert` to cast a `index` to `tensor`.", "@qingyunqu Can you please check @jpienaar's comments and keep us posted ? Thanks!", "> @qingyunqu Can you please check @jpienaar's comments and keep us posted ? Thanks!\r\n\r\nYes, I will check and fix. Sorry for for the late reply. Thanks!", "@jpienaar gently ping, please review again.  ", "@qingyunqu Can you please resolve conflicts? Thanks!", "> @qingyunqu Can you please resolve conflicts? Thanks!\r\n\r\nDone, thanks.", "@jpienaar Can you please review this PR ? Thanks!", "@qingyunqu Can you please resolve conflicts? Thanks!", "> @qingyunqu Can you please resolve conflicts? Thanks!\r\n\r\ndone", "@qingyunqu Can you please resolve conflicts? Thank you!"]}, {"number": 52240, "title": "Op type not registered 'NormalizeUTF8' in binary running when reload model #51080", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): 2.6\r\n- TensorFlow version (use command below):\r\n- Python version: Python 3.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nActually , I am trying to convert the saved_model to tensorflowlite model and while using this code\r\nimport tensorflow as tf\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./translator') # path to the SavedModel directory\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nI am getting this error\r\n![image](https://user-images.githubusercontent.com/21074002/135761699-070a32cf-7387-48bf-a60a-cad371eb855a.png)\r\n", "comments": ["@jayaBalaR ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/49968#issuecomment-852677186) from the issue with similar error.It helps.Thanks!", "Yes NormalizeUTF8 's corresponding library seems to be missing while packaging . I am not sure where to include this issue for tflite. Please reassign to that github issues. thanks", "@jayaBalaR,\r\n\r\nCan you provide us a simple reproducible code snippet or colab gist of the issue to expedite the trouble-shooting process? Thanks!", "@sanatmpa1 \r\nI have used the code in this https://www.tensorflow.org/lite/convert/ \r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nto convert my saved_model to tflite model, and it fails in the first line.\r\nI have created the saved_model using\r\ntf.saved_model.save(translator, 'translator',\r\n                    signatures={'serving_default': translator.tf_translate})\r\n\r\n", "@jayaBalaR,\r\n\r\nI tried this example in the [link](https://www.tensorflow.org/lite/convert/#convert_a_keras_model_) you provided with a slight modification, and I don't see any errors. You can take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/7c153b94125e63066cfcdcf54cfd03e3/52240.ipynb). If this is not the right code, Please point me to the right example, or provide a colab gist of standalone code to reproduce the error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52240\">No</a>\n"]}, {"number": 52239, "title": "Shaky sidebar on the docs", "body": "## URL(s) with the issue: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWhen I scroll down in the docs, the left side of pages shakes very aggressively.\r\n\r\nhttps://user-images.githubusercontent.com/35577566/135758613-fbdf9612-e663-4dda-af53-aaae4d26b97a.mov\r\n\r\n\r\n", "comments": ["I am viewing this on Safari, Chrome seems much less shaky, but still a little jitter.", "Hi @logankilpatrick! @sanatmpa1!, I checked  the above link in Firefox,chrome,Edge Browser for Windows a for stability of above link , It seems to be working  fine though It was lagging  in Safari Browser for windows .Could you try again after updating Safari Browser and disconnecting VPN?", "I am not on a VPN and running the most up to dat safari version.", "@sanatmpa1 ,Could you please look at this issue?", "@logankilpatrick,\r\n\r\nWe've checked it in `Chrome(Version 94.0.4606.71 (Official Build) (64-bit)), Firefox(Version 78.14.0esr (64-bit)) and also in Safari(Version 14.1.2 (16611.3.10.1.6))`, but we don't see any such shaking issue that you have mentioned. \r\n\r\nCan you provide the version of safari you are using and other hardware details if possible? Also try using another device and update us if the problem exists in that as well? Thanks!", "I am on Safari: Version 14.1.2 (16611.3.10.1.6) with a MacBook Pro (13-inch, M1, 2020) on OS X BigSur 11.6 (20G165). I also tried a private browser window. I tried this on my non M1 MacBook Pro and did not see the issue, so perhaps it's specific to that processor? ", "@logankilpatrick,\r\n\r\nYes it could be related to the processor as it works fine on non M1 macbook, Also the issue is not present in different hardwares that we tested it on.", "Can you get access to a M1 Mac to test? I only have access to one. ", "@logankilpatrick,\r\n\r\nUnfortunately we don't have the M1 mac to test, Can you try opening an issue in this [repo](https://github.com/apple/tensorflow_macos) to see if this can be checked? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sanatmpa1 that repo is archives so I cannot open an issue there.", "I have updated to the newly release macOS version and no longer have any issues. Thanks for debugging with me!"]}, {"number": 52238, "title": "Revert \"Add one include file so that compile would succeed.\"", "body": "Reverts tensorflow/tensorflow#31372", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52238) for more info**.\n\n<!-- need_sender_cla -->", "@jessecantu Can you please sign CLA. Thanks!"]}, {"number": 52237, "title": "kerasTensor not behaving as expected, functional api incorrectly skipped", "body": "\r\n\r\n**System information**\r\nTf version:\r\nv2.6.0-rc2-32-g919f693420e 2.6.0\r\n\r\n**Problem**\r\nUsing `tf.keras.Input(4, 1)` doesn't work with all layers. I am using an op which does not support dispatching, which will break the use of `KerasTensor`s. Adding dispatching fixes this problem, and digging into the dispatching code I see it uses `tensorflow.python.keras.layers.KerasOpDispatcher` which just wraps my op in a `TFOpLambda` layer. Rather than add the dispatching I figure I can just use the TFOpLambda layer directly. But using the `TFOpLambda` layer directly does not work, because the check for whether the input to the layer is a keras tensor erroneously returns false, which means the layer is not built through the functional api.\r\n\r\n**Minimal example**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as layers\r\n\r\nfrom tensorflow.python.ops.gen_cudnn_rnn_ops import cudnn_rnn\r\nfrom tensorflow.python.util.dispatch import add_dispatch_support\r\nfrom tensorflow.python.keras.layers.core import TFOpLambda\r\n\r\n# explicitly adding dispatching works!\r\n# the call to cudnn_rnn_dispatch calls a tfoplayer, which is built CORRECTLY with the\r\n# functional API as the check _in_functional_construction_mode returns true this check\r\n# is done in keras.engine.base_layer and it returns true because we have a tensor t such that:\r\n# t is an instance of <class 'keras.engine.keras_tensor.KerasTensor'> \r\n# is_instance(t, keras_tensor.KerasTensor) -> True\r\nx = layers.Input([1, 4])\r\ncudnn_rnn_dispatch = add_dispatch_support(cudnn_rnn)\r\nout = cudnn_rnn_dispatch(x, x, 0, tf.zeros(128), rnn_mode='gru')\r\ny = out[0]\r\n\r\n# turning into a layer does not work!\r\n# it fails because in the base_layer now the _in_functional_construction_mode check\r\n# erroneuously returns False!\r\n# the _in_functional_construction_mode check now takes place in\r\n# tensorflow.python.keras.engine.base_layer\r\n# and we have the confusing:\r\n# t is an instance of <class 'keras.engine.keras_tensor.KerasTensor'> \r\n# is_instance(t, keras_tensor.KerasTensor) -> False\r\nx = layers.Input([1, 4])\r\ncudnn_layer = TFOpLambda(cudnn_rnn)\r\ntry:\r\n    out = cudnn_layer(x, x, 0, tf.zeros(128), rnn_mode='gru')\r\n    y = out[0]\r\n    print(y)\r\nexcept TypeError:\r\n    print('IM BROOOOOOOOOOOOOOOKEN')\r\n\r\n```", "comments": ["@ben-davidson-6 Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank You!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52237\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52237\">No</a>\n"]}, {"number": 52236, "title": "mixed_precision returns gradient zeros when the model input size is large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Noe\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.9\r\n- CUDA/cuDNN version: 11.1/8.1.1\r\n- GPU model and memory: RTX 3090/24GB\r\n\r\n**Describe the current behavior**\r\nWhen using mixed_precision policy described in [https://www.tensorflow.org/guide/mixed_precision](url) with large model input size, for example `(256, 368, 368,)`, the returned gradient are constantly ZEROS. However, if remove the mixed_precision policy, the returned gradient is normal with non-zeros numbers.\r\n\r\nMoreover, if we use small model input size, let's say `(16, 16, 16)`, the returned gradient is normal no matter the mixed_precision is allowed or not. \r\n\r\nMy model is a typical U-net like model.\r\n\r\n**Describe the expected behavior**\r\nWith large model input size like `(256, 368, 368)` used above, the returned gradient should be at least non-zeros. Otherwise the model won't be trained. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\nThe code below set the input shape to `[1, 256, 368, 368, 1]` and allows `mixed_precision.Policy('mixed_float16')`. It will return zeros gradients in the end (hence, no training at all). Setting `tf16_flag=False` will returns normal gradient behavior.\r\n\r\nAlso, by change `shape = [1, 16, 16, 16, 1]`, the gradient behaves normally no matter allows `mixed_precision.Policy('mixed_float16')` or not\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import mixed_precision\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\ntf16_flag = True\r\n\r\nif tf16_flag:\r\n    policy = mixed_precision.Policy('mixed_float16')\r\n    mixed_precision.set_global_policy(policy)\r\n\r\n\r\nshape = [1, 256, 368, 368, 1]\r\n# shape = [1, 16, 16, 16, 1]\r\n\r\n\r\ndef forward_conv(x, filters, kernels, name='forward', padding='same'):\r\n    i = 0\r\n    for flt, kernel in zip(filters, kernels):\r\n        x = layers.Conv3D(flt, kernel, activation='relu', padding=padding, dilation_rate=(1, 1, 1),\r\n                          use_bias=False, name=str(i) + '_' + name)(x)\r\n        x = layers.BatchNormalization(name=str(i) + '_bn_' + name)(x)\r\n        i += 1\r\n    return x\r\n\r\n\r\ndef part_one(ipt):\r\n    l1 = forward_conv(ipt, (4, 4), (3, 3), name='enc1')\r\n    d2 = layers.MaxPool3D(pool_size=(2, 2, 2))(l1)\r\n    l2 = forward_conv(d2, (4, 4), (3, 3), name='enc2')\r\n    return l1, l2\r\n\r\n\r\ndef part_inner(ipt1, ipt2):\r\n    l1 = forward_conv(ipt1, (4, 4), (3, 3), name='enc1')\r\n    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='enc2')\r\n    return l1, l2\r\n\r\n\r\ndef part_two(ipt1, ipt2):\r\n    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='dec2')\r\n    u1 = layers.UpSampling3D(size=(2, 2, 2))(l2)\r\n    r1 = forward_conv(ipt1 + u1, (4, 4), (3, 3), name='dec1')\r\n    return r1\r\n\r\ninitial = tf.ones(shape, dtype=tf.float16) if tf16_flag \\\r\n    else tf.ones(shape, dtype=tf.float32)\r\n\r\ntf.random.set_seed(1)\r\n\r\nwith tf.GradientTape() as g:\r\n    g.watch(initial)\r\n    l1_, l2_ = part_one(initial)\r\n    for _ in range(2):\r\n        l1_, l2_ = part_inner(l1_, l2_)\r\n    opt_ = part_two(l1_, l2_)\r\n    loss = tf.reduce_mean(l1_) + tf.reduce_mean(opt_)\r\n    gd = g.gradient(loss, initial)\r\n    print('-' * 100)\r\n    print(f'loss is {loss} and grad is {np.sum(gd)} with input shape {shape}')\r\n\r\n```\r\n", "comments": ["@sanatmpa1 ,\r\nI was able to reproduce the issue in tf v2.4 and v2.6 where as in nightly version the code is executed with different error.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/64331ba9914ff5c473afdfe7924dc4a1/52236.ipynb).", "@tilakrayal @sanatmpa1 thx for your response! I think you need to have at least 24GB gpu to run this code with input size `(256, 368, 368)' and mixed-precision disabled, otherwise OOM will happen. Note that, when `mixed-precision' disabled, even with `(256, 368, 368)` input size, the gradient would be correct, i.e., non-zeros", "Since this is a Keras issue, Could you please move this to https://github.com/keras-team/keras.", "@sachinprasadhs since this issue can be reproduced in tf2.4. I think it is still part of tf?", "@WingsOfPanda , Yes, since `mixed_precision` is under  `tensorflow.keras`  it would be good to move this issue under https://github.com/keras-team/keras so that we can track keras issues separately under the mentioned repository.", "Closing the issue here, since this issue is moved under keras repo, you can track the issue here https://github.com/keras-team/keras/issues/15479", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52236\">No</a>\n"]}, {"number": 52235, "title": "Declare go_package in status proto", "body": "PR declares `go_package` in status proto definition and resolves protoc-gen-go error (`unable to determine Go import path for \"tensorflow/core/protobuf/status.proto\"`)", "comments": ["@wamuir Can you please fix build failures ? Thanks!", "@gbaned Yes, however I cannot see details on the failing builds.  Did these builds run?  Can you provide or link to information on the failures? Thanks!", "> @gbaned Yes, however I cannot see details on the failing builds. Did these builds run? Can you provide or link to information on the failures? Thanks!\r\n\r\n@wamuir  I have triggered the build checks again and most of the are successfully completed. No action required from you at this moment.  Thank you!"]}, {"number": 52234, "title": "Normalization layer masking", "body": "**System information**\r\n- TensorFlow version (you are using): nightly (on colab) (also on my machine, but behind a few weeks (built upstream from source for ROCm support))\r\n- Are you willing to contribute it (Yes/No): Yes, once maintainers evaluate proposal\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.keras.layers.Normalization() does not ignore masked values.\r\n\r\n**Will this change the current api? How?**\r\nOnly additional parameters, should not contain any non-backwards compatible changes.\r\n\r\n**Who will benefit with this feature?**\r\nPeople working with ragged data who want to include a preprocessing layer to normalize in model.\r\n\r\n**Any Other info.**\r\n\r\nProposing:\r\n- add parameter to `.adapt()` method (mask_value=0.0 and/or boolean mask)\r\n- add `call(..., mask=None)` and tensor multiply to allow masking during model fitting/evaluation\r\n\r\n[Colab gist current behavior](https://colab.research.google.com/drive/1aKGeS0lJy-QEQx0jARyOAqAMFBEFRe--?usp=sharing)\r\n\r\nAn attempt at masked normalization (does **NOT** generalize to any input shape) \r\n```\r\nclass MaskedNormalization(tf.keras.layers.Layer):\r\n    \"\"\"custom masked normalization layer\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        mask_value: int = 999,\r\n        input_dims: tuple = (51, 3),\r\n        name: str = \"masked_normalizer\",\r\n    ):\r\n        super(MaskedNormalization, self).__init__()\r\n\r\n        self.mask_value = mask_value\r\n        self.supports_masking = True\r\n \r\n        self.normalization = tf.keras.layers.experimental.preprocessing.Normalization(axis=2)\r\n\r\n\r\n    def adapt(self, inputs):\r\n        \"\"\"redefined adapt to ignore masked values\"\"\"\r\n        if not self.normalization.built:\r\n            raise RuntimeError(\"layer must be \\\"built\\\" prior to calling `.adapt()`, see `model.build_graph()`\")\r\n\r\n        mask = inputs != self.mask_value\r\n\r\n        float_mask = tf.cast(mask, dtype=tf.dtypes.float32)\r\n\r\n        mean = tf.reduce_sum(inputs * float_mask, axis=[0, 1]) / tf.reduce_sum(\r\n            float_mask, axis=[0, 1]\r\n        )\r\n        # sample variance (divide by: n-1)\r\n        var = tf.reduce_sum(((inputs - mean) ** 2) * float_mask, axis=[0, 1]) / (\r\n            tf.reduce_sum(float_mask, axis=[0, 1])\r\n            - tf.constant(1, dtype=tf.dtypes.float32, shape=(3,))\r\n        )\r\n        count = tf.experimental.numpy.count_nonzero(\r\n            float_mask\r\n        )\r\n\r\n        self.normalization.adapt_mean.assign(mean)\r\n        self.normalization.adapt_variance.assign(var)\r\n        self.normalization.count.assign(count)\r\n\r\n        self.normalization.finalize_state()\r\n        self.normalization._is_adapted = True\r\n\r\n    def call(self, inputs, mask=None):\r\n\r\n        # if mask provided mask output values (multiply by zero)\r\n        if mask is not None:\r\n                reshape_mask = tf.transpose(\r\n                    tf.expand_dims(tf.cast(mask, dtype=tf.dtypes.float32), axis=0),\r\n                    perm=[1, 2, 0],\r\n                )\r\n            return self.normalization(inputs) * reshape_mask\r\n\r\n        return self.normalization(inputs)\r\n\r\n    def compute_mask(self, inputs, mask=None):\r\n        \"\"\"return the input_mask directly\"\"\"\r\n        return mask\r\n```\r\n", "comments": ["Hi @ianzur !\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Moved issue to keras repo.\r\n\r\nClosing here."]}, {"number": 52233, "title": "Pin actions to a full length commit SHA", "body": "Pinning an action to a full length commit SHA is currently the only way to use an action as\nan immutable release. Pinning to a particular SHA helps mitigate the risk of a bad actor adding a\nbackdoor to the action's repository, as they would need to generate a SHA-1 collision for\na valid Git object payload.\n\nhttps://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions#using-third-party-actions\n\nhttps://github.com/ossf/scorecard/blob/main/docs/checks.md#pinned-dependencies\n", "comments": []}, {"number": 52232, "title": "TF2 suggests adding report_tensor_allocations_upon_oom to RunOptions while the option is not available", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.18.0-193.60.2.el8_2.x86_64\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8.0\r\n- GPU model and memory: Tesla V100-SXM2-32GB\r\n\r\n**Describe the current behavior**\r\nWhen OOM, the following message is displayed:\r\n`Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.`\r\nHowever, the suggested option is not available in tf2.\r\n\r\nThis (only!) works in tf1, e.g. as described in [this SO answer](https://stackoverflow.com/a/49675283/11611246).\r\ntf2 doesn't have `RunOptions`\r\n\r\n`run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom=True)`\r\nresults in:\r\n`TypeError: Invalid keyword argument(s) in 'compile': {'options'}`\r\n\r\n**Describe the expected behavior**\r\n\r\n1. Preferably; Implementation of the option `report_tensor_allocations_upon_oom` in tf2\r\n2. Removal of the misleading \"hint\" in tf2\r\n", "comments": ["@ManuelPopp \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Please do refer this [link1 ](https://www.tensorflow.org/hub/migration_tf2),[link2](https://www.tensorflow.org/api_docs/python/tf/compat/v1/RunOptions) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52232\">No</a>\n"]}, {"number": 52231, "title": "Addition of Resnet18 Model in Application", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.6.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** Addition of a new pre-trained model: Resnet18 in the application section.\r\n\r\n**Will this change the current api? How?** Yes. Addition of a new Resnet18 pre-trained model in the application section.\r\n\r\n**Who will benefit with this feature?** Researchers as well as individuals who want to fine-tune or train Resnet18 in Tensorflow without switching to PyTorch\r\n\r\n**Any Other info.** This will be a helpful implementation to Tensorflow as currently Resnet18 is present in PyTorch but becomes difficult for individuals to implement who are not familiar with Pytorch and have knowledge of Tensorflow.\r\n", "comments": ["Hey, I would like to work on this issue. I have some experience with open source, but I am a newcomer here and some guidance will be much appreciated.\r\n\r\nWhere can I find the implementation of other similar models in TensorFlow which I can look for reference?\r\n\r\nThanks!", "@MrinalTyagi ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!\r\n", "Addition of following code to the api:\r\n\r\n`tf.keras.applications.ResNet18(\r\n    include_top=True, weights='imagenet', input_tensor=None, \r\n    input_shape=None, pooling=None, classes=1000,\r\n    classifier_activation='softmax', **kwargs\r\n)`\r\n\r\nUse Case: This would allow individuals to fine-tune the ResNet18 model pre-trained on the imagenet dataset to be fine-tuned on their custom dataset. As the implementation of ResNet18 is already available in PyTorch and not in Tensorflow, sometimes it becomes difficult for developers with knowledge of Tensorflow and no prior knowledge of Pytorch to implement the following. Hopefully, this will allow better transfer learning for developers and reduce time in writing the ResNet18 model from scratch, then training it on the imagenet dataset and then fine-tuning it on their custom dataset. Let me know if this resolves the doubt or I need to provide more explanation. \r\n[Paper for ResNet18 introduction and architecture](https://arxiv.org/pdf/1512.03385.pdf)", "This issue is related to `Keras`, could you please move this issue under https://github.com/keras-team/keras", "Sure", "https://github.com/breadbread1984/resnet18-34  I finished coding and pretraining resnet18. you may try it."]}, {"number": 52230, "title": "Cant dynamically change the weights of a multi-output model loss functions during training", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.4\r\n- Are you willing to contribute it (Yes/No):yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI have a multi output model using the sub classing API where each output have a different loss function, and i want to change the loss functions weights dynamically in the training  process ( as shown : https://arxiv.org/pdf/1711.02257.pdf  and more papers). \r\nIt is reasonable to have the option to alter these weights using a callback\r\n ( very much like the learning rate scheduler https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)  \r\n\r\nhere is the call() method that my sub-class model uses: \r\nNOTE : the ges_out is a classifier output \r\n```\r\n    def call(self, inputs, training=False, **kwargs):\r\n\r\n        # intro block1\r\n        x = self.conv1(inputs)\r\n        x = self.spatial_dropout1(x, training=training)\r\n        x = self.conv2(x)\r\n        x = self.spatial_dropout2(x, training=training)\r\n        # x = self.avgpool1(x)\r\n        x = self.conv3(x)\r\n        intro_out = self.spatial_dropout3(x, training=training)\r\n        # intro_out = self.avgpool2(x)\r\n\r\n        # classification block\r\n\r\n        g = self.conv_ges1(intro_out)\r\n        g = self.spatial_dropout_ges1(g)\r\n        g = self.maxpool_ges1(g)\r\n        g = self.conv_ges2(g)\r\n        g = self.spatial_dropout_ges2(g)\r\n        g = self.maxpool_ges2(g)\r\n        g = self.flatten_ges(g)\r\n        g = self.fc_ges1(g)\r\n        g = self.dropout_ges1(g, training=training)\r\n        g = self.fc_ges2(g)\r\n        g = self.dropout_ges2(g, training=training)\r\n        ges_out = self.fc_ges_out(g)\r\n\r\n        # segmentation block\r\n        s = self.conv_seg1(intro_out)\r\n        s = self.spatial_dropout_seg1(s)\r\n        s = self.avgpool_seg1(s)\r\n        s = self.conv_seg2(s)\r\n        s = self.spatial_dropout_seg2(s)\r\n        s = self.avgpool_seg2(s)\r\n        s = self.flatten_seg(s)\r\n        s = self.fc_seg1(s)\r\n        s = self.dropout_seg1(s, training=training)\r\n        seg_out = self.fc_seg_out(s)\r\n\r\n        return {\"gesture_out\": ges_out, \"segmentation_out\": seg_out}\r\n```\r\n\r\nand here is how i use model_compile(...):\r\n  ```\r\n  def model_compile_and_summary():\r\n        metrics = {\"gesture_out\": [\"accuracy\"], \"segmentation_out\": [\"mean_squared_error\"]}\r\n        loss_functions = {\"gesture_out\": keras.losses.CategoricalCrossentropy(from_logits=True),\r\n                              \"segmentation_out\": keras.losses.MeanAbsoluteError()}\r\n        loss_weights = {\"gesture_out\": 1.0, \"segmentation_out\": 0.0001}\r\n        optimizer_name = \"Adam\"\r\n        optimizer = get_optimizer_from_name(optimizer_name)\r\n        MyModel.model.compile(optimizer=optimizer, loss=loss_functions, metrics=metrics, loss_weights=loss_weights)\r\n```\r\n  \r\n**Will this change the current API? How?**\r\nthe current API have the option to pass constant loss functions weights in the model.compile(...) method.  \r\ni assigned each output to each loss using a dictionary   \r\n**Who will benefit with this feature?**\r\n i think all the researchers/engineers  that currently deal with multi-task learning. \r\n", "comments": ["Hi @Ram-WD! \r\nPlease post this feature request on [keras-team/keras repo](https://github.com/keras-team/keras/issues) too .\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]