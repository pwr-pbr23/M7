[{"number": 17547, "title": "Exclude kafka on Windows", "body": "I tried to enable kafka on Windows, but found it depends on `boringssl/decrepit/bio/base64_bio.c` which is not available in boringssl's Bazel build. I filed an internal issue for the boringssl team.\r\n\r\nFor now, let's excluded kafka on Windows to fix the Windows build.\r\n\r\n@gunan \r\nFYI @yongtang ", "comments": ["LGTM, Thanks for the fix!"]}, {"number": 17546, "title": "fix some compilation errors on MSVC if IS_SLIM_BUILD", "body": "Using MSVC (2017), the record_reader.cc can not be compiled successfully if `IS_SLIM_BUILD` is defined, so this PR fixes it.", "comments": []}, {"number": 17545, "title": "add reflexive method for Dimension", "body": "Fix #17482", "comments": ["This needs to be slightly more complicated because we can't safely cast every other type to a Dimension, only integers/None.\r\n\r\nFor example, currently Tensor + Dimension -> Tensor:\r\n```\r\n>>> tf.constant(1) + tf.Dimension(2)\r\n<tf.Tensor 'add:0' shape=() dtype=int32>\r\n```\r\nThis existing behavior would break with this patch.\r\n\r\nOn the other hand, Dimension + Tensor is currently an error. But it should probably return a Tensor instead (at least if the Dimension has known size).\r\n\r\nTo fix this, special methods like `__add__` and `__radd__` need to return `NotImplemented` when they encounter a type they don't recognize, instead of trying to cast everything to a Dimension. Something like:\r\n```\r\ndef __add__(self, other):\r\n  if other is not None and not isinstance(other, int):\r\n    return NotImplemented\r\n  # existing implementation\r\n```", "@shoyer I think for Tensor + Dimension -> Tensor:\r\n`Tensor.__add__` is invoked before `Dimension.__radd__`, hence the patch won't break its behavior. I'd like to add more tests to confirm it.", "@frankchn It seems that the build logs has been cleared. Could you restart all tests so I can check the log about failure on MacOS python2?", "I'll close the PR since 5fd341d3 has fixed it."]}, {"number": 17544, "title": "Register half in some ops which support all floating point types", "body": "Add 'half' to the acceptable data types for the depthwise convolution gradients, and some pooling ops.\r\n\r\nI've just added 'half', although it seems that these ops should be able to operate on any real value.  in general there isn't a lot of consistency in the op constraints...\r\n\r\nConv2D doesn't accept double, but DepthwiseConv2D does. BiadAdd works on any numbertype, but would typically follow a Convolution. Conv3D supports bfloat16, but its back prop versions do not.\r\n\r\nPerhaps there is a bigger job to be done in the future, to replace all of the explicit types with classes of types (real, int, etc).\r\n", "comments": ["hi - all checks passed - wonder if we could consider this for merging?\r\n", "thanks :)", "hi - could we get this merged before merge conflicts cause a problem?  cheers."]}, {"number": 17543, "title": "Revert \"Update external protobuf codebase version for Windows cmake b\u2026", "body": "\u2026uild\"\r\n\r\nThis reverts commit 07bec47ba5db4c2f2e33ecb49f23253a371bfbbe.\r\n\r\nI will run a cmake GPU test before we merge.", "comments": ["Windows GPU test build at http://ci.tensorflow.org/view/TF%20pull%20requests/job/tf-pr-win-cmake-gpu/23/", "Are you trying to fix the test failures in http://ci.tensorflow.org/job/tensorflow-master-win-cmake-py/ ?\r\nI think you should upgrade the version of the protobuf pip package on Jenkins slaves. Currently, the kokoro slaves are using a different version as the jenkins slaves. If you revert this change, tests will fail on kokoro.\r\n\r\nThat would also fix the test failures in the Bazel build.\r\nSee https://github.com/tensorflow/tensorflow/issues/16656", "triggered again http://ci.tensorflow.org/view/TF%20pull%20requests/job/tf-pr-win-cmake-gpu/24/"]}, {"number": 17542, "title": "Feature map in tensorflow source code", "body": "OS Platform and Distribution : ubuntu16.04\r\nTensorFlow installed from : source\r\nBazel version : 0.9.0\r\nCUDA/cuDNN version : 8 / 6\r\nGPU model and memory: nvidia 1080ti\r\nExact command to reproduce ; n/a\r\n\r\nI try to learn tensorflow source code. When doing forward propagation, each layer will produce a feature map. I want to know after each layer is completed.Where is the feature map will be stored ? \r\nHow can find this part? I hope i can know it.\r\n\r\nI find the this part (conv2d):\r\n\r\n    OP_REQUIRES_OK(context,\r\n                   GetWindowedOutputSize(input_rows, filter_rows, stride_rows,\r\n                                         padding_, &out_rows, &pad_rows));\r\n    OP_REQUIRES_OK(context,\r\n                   GetWindowedOutputSize(input_cols, filter_cols, stride_cols,\r\n                                         padding_, &out_cols, &pad_cols));\r\n    TensorShape out_shape =\r\n        ShapeFromFormat(data_format_, batch, out_rows, out_cols, out_depth);\r\n\r\n    // Output tensor is of the following dimensions:\r\n    // [ in_batch, out_rows, out_cols, out_depth ]\r\n    Tensor* output = nullptr;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\r\n\r\nIs the \"context\" saving the feature map after forward propagation ?\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17541, "title": "Keras/TF1.6.0 does not support CuDNNLSTM/CuDNNGRU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow) None **:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Linux Ubuntu 16.04**:\r\n- **TensorFlow installed from (source or binary) binary**:\r\n- **TensorFlow version (use command below) 1.6.0**:\r\n- **Python version 3.6.3 **: \r\n- **Bazel version (if compiling from source) NOT USED**:\r\n- **GCC/Compiler version (if compiling from source) NOT USED**:\r\n- **CUDA/cuDNN version 9.0**:\r\n- **GPU model and memory Quadro P4000 8116MiB **:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nTF1.6.0/Keras does not support CuDNNLSTM/CuDNNGRU.\r\n\r\nWhich is already supported on Keras 2.1.5.\r\n  https://keras.io/layers/recurrent/\r\n\r\n### Source code / logs\r\nI replace from LSTM to CuDNNLSTM. and get following error.\r\n  https://github.com/keras-team/keras/blob/2.1.5/examples/lstm_text_generation.py\r\n\r\n---------------------------------------------------------------------------`\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-19-54f42e5ce79a> in <module>()\r\n     12 from tensorflow.python.keras.models import Sequential\r\n     13 from tensorflow.python.keras.layers import Dense, Activation\r\n---> 14 from tensorflow.python.keras.layers import LSTM, CuDNNLSTM\r\n     15 from tensorflow.python.keras.optimizers import RMSprop\r\n     16 from keras.utils.data_utils import get_file\r\n\r\nImportError: cannot import name 'CuDNNLSTM'\r\n\r\n", "comments": ["The version of Keras bundled in Tensorflow  is 2.1.3. Importing from Keras proper works: `from keras.layers import CuDNNLSTM`", "Thank you for you comments.\r\nYour commenting package is original Keras package (not TensorFlow inclusion package).\r\n", "@sakaia (sorry for the slow reply) is this a bug or a feature request? Are you saying you want TF to have support, or that it should have had it but is missing it? ", "This is a feature request.\r\nIt should have had it but is missing it (since original Keras have this feature.)", "@fchollet could you please comment on feasibility or expectations for when we might get this?", "It seems that CuDNNGRU is in tf.keras at least since TF 1.9: https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/keras/layers/CuDNNGRU.\r\n\r\n```\r\nfrom tensorflow.keras.layers import LSTM, CuDNNLSTM, GRU, CuDNNGRU\r\n```\r\n\r\nworks for me as of TF 1.12.", "For me (tf-2.2.4) this worked \r\n> from tensorflow.python.keras.layers import LSTM, CuDNNLSTM\r\n\r\nand this did not\r\n\r\n> from tensorflow.keras.layers import LSTM, CuDNNLSTM", "@SyedAliZafar I tried what you suggested. The first import worked fine, but when I tried to fit the model, I got this error: \r\n\r\n```\r\nUnknownError: Fail to find the dnn implementation.\r\n\t [[{{node cu_dnnlstm/CudnnRNN}}]]\r\n\t [[GroupCrossDeviceControlEdges_0/training/SGD/SGD/Const/_65]] [Op:__inference_keras_scratch_graph_1483]\r\n```", "Hey,\n\nIt's some mulfunction of tensorflow. I also faced this problem. But I\nrestarted my computer and it's working fine now \ud83d\ude02\n\nBest regards,\n\nAli\n\nOn Wed, Mar 20, 2019, 19:35 Eric <notifications@github.com> wrote:\n\n> @SyedAliZafar <https://github.com/SyedAliZafar> I tried what you\n> suggested. The first import worked fine, but when I tried to fit the model,\n> I got this error:\n>\n> UnknownError: Fail to find the dnn implementation.\n> \t [[{{node cu_dnnlstm/CudnnRNN}}]]\n> \t [[GroupCrossDeviceControlEdges_0/training/SGD/SGD/Const/_65]] [Op:__inference_keras_scratch_graph_1483]\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17541#issuecomment-474973754>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AixfEMjkFZsSWwfrwhgnhCrNjcFpDEOQks5vYn95gaJpZM4SiO0W>\n> .\n>\n", "This is fixed with the latest version of TensorFlow (TF 1.14). Thanks!"]}, {"number": 17540, "title": "Protobuf size explosion for TFLite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tf 1.6.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: bazel release 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc 4.9.4\r\n- **CUDA/cuDNN version**: /\r\n- **GPU model and memory**: /\r\n- **Exact command to reproduce**: \r\n`bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/tmp/gru_rnn.pb --input_format=TENSORFLOW_GRAPHDEF  --input_types=FLOAT --output_format=TFLITE --output_file=/tmp/gru_rnn.tflite --inference_type=FLOAT --inference_input_type=FLOAT --output_arrays=model/softmax,model/rnn_states`\r\n\r\n\r\n### Describe the problem\r\nI successfully built a tflite model for mobile usage on November, 2017, and the size of .tflite was almost the same as .pb (no quantized), which was bout 950k.\r\n\r\nHowever, when I try to convert the same .pb to tflite model with tensorflow 1.6 master branch (HEAD commit f7acdf2ed5e0b9c50c1c5f4b80163255aa9e8073), I find that the size of the .tflite file is 17M, **which is 17x larger than before!!**\r\n\r\nSo I make a binary search of the commits in `tensorflow/contrib/lite` history, then I find out at which commit that thing changes: the commit 90ce80131a8b5213d9f3eb9649d63921db7874a4 can produce 950k .tflite model, but right after it 528d0c5e4d148655b797368fd55fe6304730fece will make the tflite model become 17M.\r\n\r\nHere are some additional information:\r\n  - Both of these two tflite models can be successfully run on mobile.\r\n  - I did not check the commits (if there have) between these two, since I only trace the `/tensorflow/contrib/lite` history.\r\n  - I guess the problem happened due to some of the operations I used in my frozen graph, in which I have 2 layer gru and some linear transforms. But I cannot upload the .pb file here...", "comments": ["Thanks! We are investigating.", "@nestle1993  A couple of questions for you:\r\n   - do you have a BatchMatMul in your pb?\r\n   - if so, what is its shape?", "@nestle1993 It maybe that the generated file has lots of zeros in it (something we are trying to fix). It would be nice to confirm this is the case. \r\n\r\nIf you looked at your flatbuffer in a hex editor and sees tons of 0s that would indicate that being the problem. Or zip it up - if it gets 17x smaller that's probably a good indicator too.\r\n\r\nthanks!", "@andrehentz \r\nI think I don't have BatchMatMul op in my frozen graph, at least I don't use it explicitly.\r\nI open the .pb file with the hex editor, there are 960+ thousand lines in total, and 40 thousand lines are 0s at the beginning of it.\r\nThen I open the .tflite file, there seems no many 0s.. \r\n\r\nTo facilitate debugging, I have uploaded these three files (original .pb, 950k .tflie and 17M .tflite) here:\r\nhttps://drive.google.com/file/d/13IJHGc9HJ7iS0_fRE9OYvuFoxBMjOYVW/view?usp=sharing\r\nhttps://drive.google.com/file/d/1mWWPRkA5z3KaQ2m-adE9k2HEGHs6Um5b/view?usp=sharing\r\nhttps://drive.google.com/file/d/1ETqhCC9Bwor65F6zkS23FCQrI8hdA_DZ/view?usp=sharing\r\n\r\nthanks!", "@andrehentz any update?", "hello? @andrehentz \r\n", "I also met this problem when I try to transfer lightrnn model which is protobuf format to tflite. And my original protobuf model's size is 9M, but in tflite format the model is bigger than 100M. ", "Sorry for the long delay. The fix for this issue will be made available in the next few days (likely early next week).", "This has been fixed.", "Thanks and I will have  a test on it.", "Closing it , please reopen if the issue persists", "@andrehentz  This has not been fixed actually. When I use the toco tool in tensorflow lite of last version, the model which is 18M in pb format converted into 194M in tflite. Could you check it again and make sure it has been fixed?", "I am also facing the same issue. created a stack overflow issue on the same. tensorflow lite model after conversion is around 245MB where as the original tensorflow mobile is around ~1MB. I am using tensorflow v1.10.\r\n\r\nhttps://stackoverflow.com/questions/52039880/tensorflow-lite-conversion-for-lstm-model", "Thanks for the report @jibanreloaded, we will look into this problem as well.\r\n", "Here are my results based on a recent build of tflite_convert, using the gru_rnn.pb file from above.\r\n```\r\n$ bazel run //tensorflow/lite/python:tflite_convert -- --graph_def_file /tmp/gru_rnn.pb --output_file /tmp/gru_rnn.tflite --output_arrays=model/softmax,model/rnn_states --input_arrays=model/frame_0,model/frame_1,model/frame_2,model/frame_3,model/frame_4,model/frame_5,model/frame_6,model/frame_7,model/frame_8,model/frame_9,model/frame_10,model/frame_11,model/frame_12,model/frame_13,model/frame_14,model/frame_15,model/frame_16,model/frame_17,model/frame_18,model/frame_19,model/frame_20,model/frame_21,model/frame_22,model/rnn_initial_state_0,model/rnn_initial_state_1\r\n\r\n$ ls -sh /tmp/gru_rnn.tflite\r\n848k /tmp/as.tflite\r\n\r\n```", "> Here are my results based on a recent build of tflite_convert, using the gru_rnn.pb file from above.\r\n> \r\n> ```\r\n> $ bazel run //tensorflow/lite/python:tflite_convert -- --graph_def_file /tmp/gru_rnn.pb --output_file /tmp/gru_rnn.tflite --output_arrays=model/softmax,model/rnn_states --input_arrays=model/frame_0,model/frame_1,model/frame_2,model/frame_3,model/frame_4,model/frame_5,model/frame_6,model/frame_7,model/frame_8,model/frame_9,model/frame_10,model/frame_11,model/frame_12,model/frame_13,model/frame_14,model/frame_15,model/frame_16,model/frame_17,model/frame_18,model/frame_19,model/frame_20,model/frame_21,model/frame_22,model/rnn_initial_state_0,model/rnn_initial_state_1\r\n> \r\n> $ ls -sh /tmp/gru_rnn.tflite\r\n> 848k /tmp/as.tflite\r\n> ```\r\n\r\n@andrehentz : which version of tensorflow are you using and did you try the same with toco?", "I'm building from HEAD. Here's the equivalent toco command line:\r\n\r\n`blaze run //tensorflow/lite/toco:toco -- --input_file=/tmp/gru_rnn.pb --output_file=/tmp/gru_rnn.tflite --output_arrays=model/softmax,model/rnn_states --input_arrays=model/frame_0,model/frame_1,model/frame_2,model/frame_3,model/frame_4,model/frame_5,model/frame_6,model/frame_7,model/frame_8,model/frame_9,model/frame_10,model/frame_11,model/frame_12,model/frame_13,model/frame_14,model/frame_15,model/frame_16,model/frame_17,model/frame_18,model/frame_19,model/frame_20,model/frame_21,model/frame_22,model/rnn_initial_state_0,model/rnn_initial_state_1`", "Closing due to inactivity."]}, {"number": 17539, "title": "remove annoying print of tensor for ValueError", "body": "I ran into this ValueError while using the `tf.data.Dataset`, and I had to wait for my (3e6, 2) shape Tensor to print out before I saw the error message.\r\n\r\nI think it would be better to get rid of this and just print the shape mismatch. Thanks.", "comments": ["This may have been because my data was in a python list, instead of numpy, which would print a shorter version.  So this might not be that bad."]}, {"number": 17538, "title": "strange error on mac", "body": "here is the code:\r\n```\r\nimport tensorflow as tf\r\na =tf.Variable(tf.float32,tf.zeros([1,1]))\r\n```\r\nsystem:\r\nosx 10.12.6\r\nmacbook pro 2017\r\n\r\nI installed tensorflow 1.6 using the following command:\r\n`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl`\r\n\r\nbelow is the debug info\r\n\r\n```\r\n/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/Users/long/dev/littleboy/tftest.py\", line 2, in <module>\r\n    a =tf.Variable(tf.float32,tf.zeros([1,1]))\r\n  File \"/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\r\n    constraint=constraint)\r\n  File \"/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 309, in _init_from_args\r\n    if trainable and ops.GraphKeys.TRAINABLE_VARIABLES not in collections:\r\n  File \"/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 614, in __bool__\r\n    raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n```", "comments": ["should place tf.float32 behind the initial value"]}, {"number": 17537, "title": "AVX512F is not compiled when running \"hello world\" test using Tensorflow 1.6 on Intel KNL", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo custom code\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary, follow the instructions from intel,\r\nhttps://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available#comment-1919528\r\n\r\nLinux KNL101-04 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"17.10 (Artful Aardvark)\"\r\nVERSION_ID=\"17.10\"\r\nVERSION_CODENAME=artful\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.2.0-8ubuntu3.2) 7.2.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux KNL101-04 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.5.2)\r\ntensorflow (1.6.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\n\r\nIt seems that AVX512F is not compiled into released tensorflow 1.6. (but other KNL specific AVX512 instructions are compiled!!!)\r\n\r\nI run simple \"hello world\" test on KNL after install the tensorflow 1.6 by following the steps mentioned here!!\r\n\r\nit give me the message as follows:\r\n\r\n2018-03-08 13:08:26.924344: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n\r\nBTW, i try to compile the tensorflow 1.6 from source code, and adding all of avxxxxx as -copt to bazel build cmd, after build tensorflow 1.6 package correctly, re-run above test program, it seems the \"AVX512F\" is compiled!!!!\r\n\r\n### Source code / logs\r\nSource code\r\nsimple program, just print \"hello tensorflow\" using tensorflow.\r\n\r\nlogs:\r\n2018-03-08 13:08:26.924344: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F", "comments": ["These are binaries built,  distributed and maintained by Intel.\r\nYou should reach out to intel with questions about those."]}, {"number": 17536, "title": "Branch 188272354", "body": "", "comments": ["note: revert change cl/188263046 forward as this change broke contrib build to unblock release.", "@saeta your changes should still be included."]}, {"number": 17535, "title": "R1.6", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 17534, "title": "Unable to use TF with a Maven project on Windows", "body": "I get the following errors when I tried to use tensorflow with maven project on windows:\r\n\r\n\"\"\"\r\nError injecting constructor, java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n\"\"\"\r\n\r\nAnd I use maven in project:\r\n\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow</artifactId>\r\n            <version>1.5.0</version>\r\n        </dependency>\r\n        <dependency>\r\n\r\nIt works well on OS but failed on windows. Anyone knows how to fix this? Massive thanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@Derekkk \r\nOnly x86_64 TensorFlow native library binaries are uploaded to Maven. (See `libtensorflow_jni` artifact)\r\nSo need 64-bit (x86_64) version of JVM to run TensorFlow. Not 32-bit (x86). \r\nIf that is not possible you could try compiling your own 32-bit TensorFlow binary for Windows.", "@Derekkk does that solve your problem?", "I'm going to close due to lack of activity but please reopen if there's a problem still."]}, {"number": 17533, "title": "tf1.6 error: tf.contrib.ffmpeg.decode_video", "body": "@yongtang  I use tensorflow 1.6on ubuntu and decodevideo.\r\n\r\n`with tf.Session() as sess:\r\n  movie_bin = tf.read_file('/home/xucl/app/data/bilibili/video/DongFangLieChe.mp4')\r\n  movie = tf.contrib.ffmpeg.decode_video(movie_bin)\r\n  movie_ev = movie.eval()\r\n  print(\"****\",len(movie_ev))`\r\n\r\nbut get an error\r\n\r\n`2018-03-08 10:48:23.000491: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:400] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [106, 0, 640, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_cPNCGW.err\r\n\u5df2\u653e\u5f03 (\u6838\u5fc3\u5df2\u8f6c\u50a8)\r\n`\r\n", "comments": ["I got this error too (and a couple other ones). At least in my case, it was caused by a different version of ffmpeg installed in my system, which produces slightly different output. The underlying problem is that this functionality relies on parsing the ffmpeg output to extract metadata (nb of frames and frame dimensions) but the output may differ between versions as well as between video files. Perhaps a more reliable way would be to use ffprobe to get the metadata.", "I will take a look, thanks for the report.", "@XuChunling Can you provide the ffmpeg version you use if possible.", "@yongtang the version of my ffmpeg\r\n\r\n#################################################################\r\n`\r\n$ ffmpeg\r\nffmpeg version 2.4.3-1ubuntu1~trusty6 Copyright (c) 2000-2014 the FFmpeg developers\r\n  built on Nov 22 2014 17:07:19 with gcc 4.8 (Ubuntu 4.8.2-19ubuntu1)\r\n  configuration: --prefix=/usr --extra-version='1ubuntu1~trusty6' --build-suffix=-ffmpeg --toolchain=hardened --extra-cflags= --extra-cxxflags= --libdir=/usr/lib/x86_64-linux-gnu --shlibdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --enable-shared --disable-stripping --enable-avresample --enable-avisynth --enable-fontconfig --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-opengl --enable-x11grab --enable-libxvid --enable-libx265 --enable-libdc1394 --enable-libiec61883 --enable-libzvbi --enable-libzmq --enable-frei0r --enable-libx264 --enable-libsoxr --enable-openal --enable-libopencv\r\n  libavutil      54.  7.100 / 54.  7.100\r\n  libavcodec     56.  1.100 / 56.  1.100\r\n  libavformat    56.  4.101 / 56.  4.101\r\n  libavdevice    56.  0.100 / 56.  0.100\r\n  libavfilter     5.  1.100 /  5.  1.100\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  0.100 /  3.  0.100\r\n  libswresample   1.  1.100 /  1.  1.100\r\n  libpostproc    53.  0.100 / 53.  0.100\r\nHyper fast Audio and Video encoder\r\nusage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}...\r\n\r\nUse -h to get full help or, even better, run 'man ffmpeg'\r\n`\r\n", "@XuChunling Is your system Ubuntu 14.04? Can you provide the details of the system, as was requested when you open the issue?\r\n\r\nIdeally the availability of the video file would be great as well. But if that is not possible, can you run ffmpeg with your video and show the output (of stdout and stderr)?", "The following information will be really helpful:\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n", "@yongtang  the complete message\r\n\r\nOS Platform and Distribution-->Linux Ubuntu 14.04\r\nTensorFlow installed from-->binary\r\nTensorFlow version -->1.6\r\nPython version--> 3.5.5\r\nGCC/Compiler version -->GCC 4.8.4\r\nCUDA/cuDNN version-->cuda9/cuDNN7\r\nGPU model and memory-->Tesla K20c/Tesla K20m\r\nffmpeg version-->2.4.3\r\n\r\ncode to reproduce:\r\n```\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    movie_bin = tf.read_file('/home/xucl/app/data/bilibili/video/DongFangLieChe.mp4')\r\n    movie = tf.contrib.ffmpeg.decode_video(movie_bin)\r\n    movie_ev = movie.eval()\r\n    print(\"****\",len(movie_ev))\r\n```\r\n\r\nget an error\r\n\r\n`2018-03-08 10:48:23.000491: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:400] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [106, 0, 640, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_cPNCGW.err \u5df2\u653e\u5f03 (\u6838\u5fc3\u5df2\u8f6c\u50a8)`\r\n\r\nbut, when i use ffmpeg directly, It can be exactly run well\r\n\r\n`$ ffmpeg -i /home/xucl/app/data/bilibili/video/DongFangLieChe.mp4 image-3%d.jpeg\r\nffmpeg version 2.4.3-1ubuntu1~trusty6 Copyright (c) 2000-2014 the FFmpeg developers\r\n  built on Nov 22 2014 17:07:19 with gcc 4.8 (Ubuntu 4.8.2-19ubuntu1)\r\n  configuration: --prefix=/usr --extra-version='1ubuntu1~trusty6' --build-suffix=-ffmpeg --toolchain=hardened --extra-cflags= --extra-cxxflags= --libdir=/usr/lib/x86_64-linux-gnu --shlibdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --enable-shared --disable-stripping --enable-avresample --enable-avisynth --enable-fontconfig --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-opengl --enable-x11grab --enable-libxvid --enable-libx265 --enable-libdc1394 --enable-libiec61883 --enable-libzvbi --enable-libzmq --enable-frei0r --enable-libx264 --enable-libsoxr --enable-openal --enable-libopencv\r\n  libavutil      54.  7.100 / 54.  7.100\r\n  libavcodec     56.  1.100 / 56.  1.100\r\n  libavformat    56.  4.101 / 56.  4.101\r\n  libavdevice    56.  0.100 / 56.  0.100\r\n  libavfilter     5.  1.100 /  5.  1.100\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  0.100 /  3.  0.100\r\n  libswresample   1.  1.100 /  1.  1.100\r\n  libpostproc    53.  0.100 / 53.  0.100\r\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from '/home/xucl/app/data/bilibili/video/DongFangLieChe.mp4':\r\n  Metadata:\r\n    major_brand     : isom\r\n    minor_version   : 512\r\n    compatible_brands: isomiso2avc1mp41\r\n    encoder         : Lavf57.71.100\r\n    description     : Packed by Bilibili XCoder v1.0(fixed_gap:False)\r\n  Duration: 00:10:24.45, start: 0.000000, bitrate: 480 kb/s\r\n    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 408 kb/s, 20 fps, 20 tbr, 16k tbn, 40 tbc (default)\r\n    Metadata:\r\n      handler_name    : VideoHandler\r\n    Stream #0:1(und): Audio: aac (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 65 kb/s (default)\r\n    Metadata:\r\n      handler_name    : SoundHandler\r\n[swscaler @ 0x1857c40] deprecated pixel format used, make sure you did set range correctly\r\nOutput #0, image2, to 'image-3%d.jpeg':\r\n  Metadata:\r\n    major_brand     : isom\r\n    minor_version   : 512\r\n    compatible_brands: isomiso2avc1mp41\r\n    description     : Packed by Bilibili XCoder v1.0(fixed_gap:False)\r\n    encoder         : Lavf56.4.101\r\n    Stream #0:0(und): Video: mjpeg, yuvj420p, 640x360 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 20 fps, 20 tbn, 20 tbc (default)\r\n    Metadata:\r\n      handler_name    : VideoHandler\r\n      encoder         : Lavc56.1.100 mjpeg\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\r\nPress [q] to stop, [?] for help\r\nframe=12488 fps=227 q=24.8 Lsize=N/A time=00:10:24.40 bitrate=N/A\r\nvideo:133937kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\r\n`", "@siekkine Which version of the ffmpeg do you use?", "@XuChunling I was trying to install ffmpeg with Ubuntu 14.04 but it looks like I couldn't find it in the default repo (in Ubuntu 16.04 ffmpeg is available on the default repo). In order to reproduce exactly the issue you encountered, can you specify the method you install ffmpeg on your Ubuntu 14.04?\r\n\r\nAlso, in TensorFlow the command to execute ffmpeg for video docoding is:\r\n```\r\nffmpeg -nostats -nostdin -i small.mp4 -f image2pipe -probesize 5000000 -loglevel info -hide_banner -vcodec rawvideo -pix_fmt rgb24 -y small.raw\r\n```\r\n\r\nwhere small.mp4 is the sample video and small.raw is the output file. Can you invoke the above command with the video files you have (DongFangLieChe.mp4) and post the output if possible?", "@yongtang  the output\r\n\r\n`$ ffmpeg -nostats -nostdin -i DongFangLieChe.mp4 -f image2pipe -probesize 5000000 -loglevel info -hide_banner -vcodec rawvideo -pix_fmt rgb24 -y small.raw\r\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from 'DongFangLieChe.mp4':\r\n  Metadata:\r\n    major_brand     : isom\r\n    minor_version   : 512\r\n    compatible_brands: isomiso2avc1mp41\r\n    encoder         : Lavf57.71.100\r\n    description     : Packed by Bilibili XCoder v1.0(fixed_gap:False)\r\n  Duration: 00:10:24.45, start: 0.000000, bitrate: 480 kb/s\r\n    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 408 kb/s, 20 fps, 20 tbr, 16k tbn, 40 tbc (default)\r\n    Metadata:\r\n      handler_name    : VideoHandler\r\n    Stream #0:1(und): Audio: aac (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 65 kb/s (default)\r\n    Metadata:\r\n      handler_name    : SoundHandler\r\nOutput #0, image2pipe, to 'small.raw':\r\n  Metadata:\r\n    major_brand     : isom\r\n    minor_version   : 512\r\n    compatible_brands: isomiso2avc1mp41\r\n    description     : Packed by Bilibili XCoder v1.0(fixed_gap:False)\r\n    encoder         : Lavf56.4.101\r\n    Stream #0:0(und): Video: rawvideo (RGB[24] / 0x18424752), rgb24, 640x360 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 20 fps, 20 tbn, 20 tbc (default)\r\n    Metadata:\r\n      handler_name    : VideoHandler\r\n      encoder         : Lavc56.1.100 rawvideo\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (h264 (native) -> rawvideo (native))\r\nframe=12488 fps=216 q=0.0 Lsize= 8429400kB time=00:10:24.40 bitrate=110592.0kbits/s\r\nvideo:8429400kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%`\r\n\r\nthis is the output file\r\n\r\n`$ ls\r\n`\r\n`DongFangLieChe.mp4  ff_help  jpg  small.raw  XML`", "Unfortunately, I cannot check the ffmpeg version at the moment, but if I remember correctly, I pulled it recently from github and compiled it myself to enable CUDA support. Here are the parsing issues I met in my case:\r\n\r\n- The aspect ratios ([SAR 1:1 DAR 16:9] above) outputted after the resolution confuses the code that parses height dimension. \r\n\r\n- Ordering of the output appears to differ between ffmpeg versions and, consequently, the parser code under \"in_mapping\" conditional may miss the \"frame=  \". \r\n\r\n- With sufficiently long video, ffmpeg may output the line with \"frame= \" to stderr multiple times overwriting the previous output (using carriage return). The parser will find the first one whereas it should extract the last one (e.g., use rfind instead of find). \r\n\r\nI gave ffprobe a try and replaced the parsing functionality with it. It turns out that the frame count may sometimes be off (by one) unless one tells ffprobe explicitly to count them (-count_frames) but that slows down the whole thing because it requires an extra pass over the video file. So, that's not a perfect solution either. I also replaced the use of temp files with pipes to avoid extra I/O operations and implemented a way to sample every nth frame (pass select filter option to ffmpeg). My code is messy but I can share it with you if you want.", "@XuChunling @siekkine  I am able to find out where the issue is, will have a fix shortly.\r\n\r\nBy the way, when posting code blocks, you could use three back ticks (\"```\") at the begging line and end line to enclose the block so that it will render correctly:\r\n```\r\n$ ffmpeg -nostats -nostdin -i DongFangLieChe.mp4 -f image2pipe -probesize 5000000 -loglevel info -hide_banner -vcodec rawvideo -pix_fmt rgb24 -y small.raw\r\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from 'DongFangLieChe.mp4':\r\n  Metadata:\r\n    major_brand     : isom\r\n    minor_version   : 512\r\n    compatible_brands: isomiso2avc1mp41\r\n    encoder         : Lavf57.71.100\r\n    description     : Packed by Bilibili XCoder v1.0(fixed_gap:False)\r\n  Duration: 00:10:24.45, start: 0.000000, bitrate: 480 kb/s\r\n    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 408 kb/s, 20 fps, 20 tbr, 16k tbn, 40 tbc (default)\r\n    Metadata:\r\n      handler_name    : VideoHandler\r\n    Stream #0:1(und): Audio: aac (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 65 kb/s (default)\r\n    Metadata:\r\n      handler_name    : SoundHandler\r\nOutput #0, image2pipe, to 'small.raw':\r\n  Metadata:\r\n    major_brand     : isom\r\n    minor_version   : 512\r\n    compatible_brands: isomiso2avc1mp41\r\n    description     : Packed by Bilibili XCoder v1.0(fixed_gap:False)\r\n    encoder         : Lavf56.4.101\r\n    Stream #0:0(und): Video: rawvideo (RGB[24] / 0x18424752), rgb24, 640x360 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 20 fps, 20 tbn, 20 tbc (default)\r\n    Metadata:\r\n      handler_name    : VideoHandler\r\n      encoder         : Lavc56.1.100 rawvideo\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (h264 (native) -> rawvideo (native))\r\nframe=12488 fps=216 q=0.0 Lsize= 8429400kB time=00:10:24.40 bitrate=110592.0kbits/s\r\nvideo:8429400kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\r\n```", "Added the fix in #17685."]}, {"number": 17532, "title": "Merge 1.6 back to master.", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 17531, "title": "contrib/lite: spelling/code tweaks", "body": "", "comments": ["will fix the test in a sec", "fixed test, rebased to match upstream", "@gunan removed issue introduced by rebase, please run test again \ud83c\udf89 "]}, {"number": 17530, "title": "Feature request:  Increase kMaxEagerParentSize or make it python version dependent", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nN/A (problem occurs on startup)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\ntip of tree\r\n- **Python version**: \r\n3.6 (from source)\r\n- **Bazel version (if compiling from source)**:\r\n0.10.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.8.4\r\n- **CUDA/cuDNN version**:\r\nunused\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\nimport tensorflow\r\n\r\n### Describe the problem\r\nWe wanted to evaluate our patches in Valgrind, so we made a custom build of python 3.6 from source (it's more valgrind-friendly), then compiled TensorFlow against that.\r\n\r\nWhen this done, 'import tensorflow' fails on a python exception because the size of the base class of EagerTensor in Python 3.6 is greater than kMaxEagerParentSize.\r\n\r\nIssue #16836 was opened on this a month ago, but closed by its creator.  When we encountered this problem, we found that we could simply set the value of kMaxEagerParentSize to 48 and everything would run with no obvious issues.\r\n\r\nIs there a reason that kMaxEagerParentSize must always equal 32, or can it be increased, or possibly even adjusted depending on which version of python TensorFlow is being compiled for?\r\n\r\n\r\n### Source code / logs\r\n```\r\ndiff --git a/tensorflow/python/eager/pywrap_tensor.cc b/tensorflow/python/eager/pywrap_tensor.cc\r\nindex 3ec2109..cc1c0f7 100644\r\n--- a/tensorflow/python/eager/pywrap_tensor.cc\r\n+++ b/tensorflow/python/eager/pywrap_tensor.cc\r\n@@ -163,7 +163,7 @@ PyObject* PyIntFromDataType(TF_DataType l) {\r\n \r\n extern \"C\" {\r\n \r\n-static const int kMaxEagerTensorParentSize = 32;\r\n+static const int kMaxEagerTensorParentSize = 48;\r\n \r\n // TODO(agarwal): store context handle in EagerTensor.\r\n typedef struct EagerTensor {\r\n```", "comments": ["You are compiling TensorFlow 1.6? or do you really mean Python 1.6? If the latter, we do not support less like python 2.x.\r\n\r\nAs far as I can tell, the underlying tensor implementation must be able to fit in the kMaxEagerParentSize. So in principal it can be increased with impunity. Somehow in your compile the base class tensor size is larger (probably alignment differences between compilers?)\r\n\r\n@alextp, do you have any insights?", "I meant Python version 3.6, sorry.  I got mixed up between TensorFlow and Python versions.  I've updated the ticket to correct my mistake.", "@agarwal-ashish I think it should be fine to increase the size, right?", "Yes, it should be ok to increase the size.  @RichDubielzig, do you want to submit your change ? ", "The same problem also happens when I try to compile python 3.6 with `--with-pydebug`\r\nFor some reason, that alone increase type size from 400 to 416", "kMaxEagerParentSize is now 64 bytes and should be available in nightly builds soon. Please reopen if this value still doesn't work.", "Thanks.  I was looking into ways to try and autodetect whether pydebug was on and got nowhere."]}, {"number": 17529, "title": "Add scan command to saved_model_cli to check for security sensitive ops.", "body": "", "comments": ["As mentioned in #17459, if we want to make saved_model_cli into a swiss army knife for SavedModel, we should consider adding a mode which lets you read a metagraph + checkpoint, plus potentially the names of inputs and output, and which produces a SavedModel. @yifeif what do you think?\r\n\r\nOtherwise this looks good, I'll merge."]}, {"number": 17528, "title": "Add scan command to saved_model_cli to check for security sensitive ops.", "body": "", "comments": []}, {"number": 17527, "title": "Enhancement with deprecated_argument_lookup", "body": "The tf.losses.cosine_distance deprecated dim and switched to axis. This fix adds the enhancement of using deprecated_argument_lookup, which is used in all other arguments deprecations.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 17526, "title": "NPE hardware acceleration support in Qualcomm chips", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android OS\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.6 TFLite\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: See below\r\n- **Exact command to reproduce**: N/A\r\n\r\nCan anyone clarify what chips will support NPE and have hardware acceleration for on device inference with tf lite?\r\n\r\nNPE SDK says 845, 820, 835, 625, 626, 650, 652, 653, 660, 630, 636, 450, 820Am.\r\n\r\nI was under the impression that only the 845 had NPE?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes, it has received no answer and I have reached out on Twitter and in the Qualcomm forums and no one has answered what the relationship is between TensorFlow Lite, NNAPI, sNPE and Qualcomm chips.", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I understand some details might have been tight lipped before the TF 2018 Dev Summit, but I gather from the talks that the Qualcomm Hexagon DSP driver support for NNAPI will come in Android P. Theres a lot of talk in the Dev Summit about things like NVIDIA TensorRT and Intel MKL-DNN etc as well as the ability to deploy models to platforms like Mobile and Raspberri Pi, so my guess is that these hardware specific optimisations will be supported through some higher level TensorFlow graph optimisation tools and for Android that will be NNAPI. So I guess the Hexagon Chip support will be up to Qualcomms provided NNAPI driver in Android P, but my guess is Flagships like 845, and then possibly the same line up above.\r\n\r\nAny word on the Pixel devices? Seems kind of cruel that the Google Pixel's 821 had the Hexagon DSP drivers disabled for sNPE?", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@madhavajay \r\nFor running neural nets on Qualcomm chips, you can have look at [MACE](https://github.com/xiaomi/mace), which is highly optimized for mobile phone performance. Here are some [benchmark results](https://github.com/XiaoMi/mace/issues/1).", "Wow thats really impressive stuff. Thanks for the notification! \ud83d\udc4d ", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "SNPE is Qualcomm's neural processing SDK for their chips.  It provides support for running TF-trained models on various Qualcomm Snapdragon SoC chips.  The SDK information page lists a long list of supported chips, as pointed out by @madhavajay.  Stock android, however, doesn't provide support for the underlying communication mechanism between the CPU and the DSP, so HVX acceleration via SNPE isn't supported on those platforms.  Android P, running on SDM845-based platforms, will accelerate inferences on the HVX via Qualcomm's NNAPI HAL driver.  I'm not certain about plans for NNAPI HAL support on platforms based on other Qualcomm chips, but this is definitely not the same thing as SNPE support for the underlying SoC chip.\r\n\r\nSorry for the complicated answer.\r\n", "@rockyrhodes thanks for the reply. Yes, I guess another way to phrase it would be, when will Android and TensorFlow support acceleration on Qualcomm chips that leverages the Hexagon DSP. It was mentioned in the TF Summit 2018 that it would come in Android P.", "@rockyrhodes sorry for replying to a closed issue, but just to clarify, will Android P on MSM8998 (Snapdragon 835) get HVX acceleration? It has the Hexagon 682 versus the 685 on the SDM845. Thanks!", "@reuben: I have no information about Qualcomm's plans for NNAPI HAL drivers for other chips, sorry.", "Check this.\r\nhttps://developer.qualcomm.com/docs/snpe/overview.html"]}, {"number": 17525, "title": "Build errors for non-Android build with lite proto runtime", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nMinor modifications to tensorflow/contrib/makefile/Makefile to allow for lite proto runtime \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Gentoo-based\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A (using Makefile)\r\n- **GCC/Compiler version (if compiling from source)**: clang++ 6.6.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n1. Patch Makefile to add `-DTENSORFLOW_LITE_PROTOS` to `CXXFLAGS`\r\n2. `make -f tensorflow/contrib/makefile/Makefile ...` (as in tensorflow/contrib/makefile/build_all_linux.sh)\r\n\r\n### Describe the problem\r\nA few files fail to compile because they rely on either:\r\n1. Lite runtime protos only being used in Android builds (i.e. when `__ANDROID__` is true), or\r\n2. All protos being `Message` objects (whereas they are `MessageLite `objects with the lite runtime).\r\n\r\n### Source code / logs\r\nThe specific errors stem from the use of `proto.DebugString()` in the following files:\r\n`core/framework/reader_base.cc`\r\n`core/grappler/costs/op_level_cost_estimator.cc`\r\n`core/grappler/costs/utils.cc`\r\n`core/grappler/grappler_item_builder.cc`\r\n`core/grappler/optimizers/constant_folding.cc`\r\n`core/grappler/optimizers/dependency_optimizer.cc`\r\n\r\nand the use of the `ReadTextProto` function in `core/grappler/inputs/utils.cc`.\r\n\r\nI will attempt to fix this problem myself, and will use this bug to track my progress and ask questions.", "comments": ["Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can we close this one  as we have not heard back from you ? ", "Yes, sorry. I solved this issue by switching from the Makefile build to Bazel."]}, {"number": 17524, "title": "Improvements for Android TV", "body": "To build for Android TV, user need to modify the orientation from \"portrait\" to \"landscape\"\r\nfor the four activities in tensorflow/examples/android/AndroidManifest.xml\r\n\r\n1. Show a toast when no camera is detected\r\n2. Game Controller L1 button and remote controller DRAP_Center also triggers debug output\r\n3. Fix the layout of Stylize app on Android TV. The style selector will be on right side\r\n4. User can use DPAD to navigate through styles in Stylize app", "comments": []}, {"number": 17523, "title": "Merge r1.6 back to master", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 17522, "title": "Branch 188235030", "body": "", "comments": []}, {"number": 17521, "title": "Making dockerhub the primary installation location.", "body": "", "comments": []}, {"number": 17520, "title": "New version 1.6 \"pip install --upgrade tensorflow \" installs tensorflow-gpu NOT tensorflow (cpu)....", "body": "1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n**BUG**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**WINDOWS 10.0**\r\n- TensorFlow installed from (source or binary):\r\n**pip (binary)**\r\n- TensorFlow version (use command below):\r\n**1.6**\r\n- Python version: \r\n**tried both python 3.5.2 & 3.6.4**\r\n- CUDA/cuDNN version: \r\nNO CUDA\r\n- GPU model and memory:\r\n**NO GPU**\r\n- Exact command to reproduce:\r\n**pip install --upgrade tensorflow**\r\n\r\n### Describe the problem\r\nimport tensorflow as tf \r\ngive the error (No module named \"pywrap_tensorflow\")\r\nIssue 42011070 on stack \r\nThere : it became clear that it is a cudannxx_x.dll , i.e. CUDA error.   \r\nI have tensorflow-gpu running flawlesly on NVIDIA GPU\r\n### Source code / logs\r\nNo module named _pywrap_tensorflow\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version", "There can be more root causes to your problem. I have verified that windows CPU package is indeed built for CPU.\r\nIt is possible that your CPU does not have AVX instruction set. Could you share your CPU model?", "potential duplicate of #17386", "I Tried it on processors : XEON W 3565  - Core i3 550 (both somewhat older models supposedly do not have avx) and on a Celeron 3930 : wikipedia Q1 2017 Kaby Lake supposedly having avx, no? (exactly same error _pywrap_tensorflow_internal // DLL load failed)\r\nI tried also on a AMD FX(tm)-6300 ... and .... YES : we have a winner : it is an AVX problem - duplicate of #17386 \r\nCeleron 3930 was a bit doubt ... but aparently has NO avx ... the AMD-FX-6300 clearly has (as stated on the webpage of the processor.  \r\nEt voila, thank you gunan for putting me on the right track.\r\nI have a bunch of Xeon's they gave me to play with, old ones, but still good enough to satisfy the guy in R&D with :-) .I'll use TF 1.5 or try the whl-solution mentioned in the other post\r\nYou should really document this thing, because it took me two days, just for nothing.", "Thanks for the feedback.\r\nWe did merge some updates to the installation guide, however we did not push it to the website yet.\r\nWe will look into getting it out there quickly.", "how to install a specific version of tensorflow in windows @anconda distribution\r\n"]}, {"number": 17519, "title": "Deploying Custom CNN implementation frameworks (fasterRCNN, maskRCNN etc) using TF-Serving", "body": "Running popular forks (fasterRCNN, maskRCNN), one finds they cannot be serialized as a GraphDef because of the usage of py_funcs. (Ref: Official tf documentation)\r\n<img width=\"800\" alt=\"screen shot 2018-03-07 at 4 27 31 pm\" src=\"https://user-images.githubusercontent.com/15898956/37120940-cfd0e972-2229-11e8-9db8-0a710ebb8f8c.png\">\r\n\r\n\r\nThe error observed is such: \r\n``2018-03-07 21:26:02.451945: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: saved_model version: 1} failed: Not found: Op type not registered 'PyFunc' in binary running on f39ddd6ef2c7. Make sure the Op and Kernel are registered in the binary running in this process``\r\n\r\nIs there any official fork/implementation for these popular frameworks or is there some documentation which can be used to convert py_funcs to native tensorflow operations?\r\n\r\nAlso, kindly let me know if any of the new tf betas in the works support/will support it since I am looking to deploy mine.\r\n\r\nThanks\r\n\r\n-----------------------------------------------\r\nSystem Info:\r\n  - Running in docker container (ubuntu/bash)\r\n  - tf 1.5.x", "comments": ["@chrisolston any input(s) would be appreciated, thanks!", "I got the same problem, while serving text-detection-cptn:\r\n2018-03-12 15:21:45.537829: E tensorflow_serving/core/aspired_versions_manager.cc:352] Servable {name: ctpn version: 1} cannot be loaded: Not found: Op type not registered 'PyFunc' in binary running on skyriver. Make sure the Op and Kernel are registered in the binary running in this process.\r\n", "@Cristiano-3 yes, this seems to be a limitation of Tensorflow Serving as of now. It would really make sense to deploy production scale ML code with tf-serving once either this is resolved and support is added or a workaround is found for the same since most of prod code I've worked uses some sort of custom framework(it's almost never as simple as deploying a super simple neural net without custom frameworks).\r\n\r\nI understand that this issue arises due to the fact that Tf is a library written for python in C++.\r\nWould love to get some direction and/or contribute to this process of adding pyfunc support to tensorflow serving.", "@sukritiramesh any comment here?", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@nfiedel any thoughts?", "In that case, actually I would recommend implementing `sinh` using tensorflow based on the [formula](https://en.wikipedia.org/wiki/Hyperbolic_function).", "@drpngx \r\nThanks for the input but I'm still looking to resolve the core issue, which is to actually get py_funcs working with tensorflow serving. This is majorly due to the vast resources available in terms of custom framework implementations like FasterRCNN.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So I think it's just a matter of linking the op in to your custom binary. Did you try that?", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Yes that does seem to work, thanks for your time @drpngx.", "Great to hear!"]}, {"number": 17518, "title": "tf.matrix_band_part uses int64 as default type for num_lower and num_upper", "body": "tf.matrix_band_part uses int64 as default type for num_lower and num_upper.\r\nBut it is weird, because most return from tf.shape() and tf.size() is int32.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "leave them as N/A", "@carltonwang I don't understand the issue. Is this a bug, a feature request, or a suggestion that we change the interfaces? What do you mean by the default type? The docs suggest that tf.matrix_band_part accepts either int32 or int64 for num_lower and num_upper. ", "It is a suggestion, because most methods accept float32 and int32 as default types, and tf.size() and tf.shape() also return int32 by default. But why tf.matrix_band_part() accept int64 by default? \r\nFor example, if I try tf.matrix_band_part(input, 0,1), both 0 and 1 will be treated as int64\r\nIf I try tf.matrix_band_part(input, 0, tf.shape(xxx)[xxx]), there will be an err, because 0 is treated as int64, but tf.shape() returns int32.\r\nUnderstand?", "I think I understand. The interface contract of tf.matrix_band_part says that num_upper \"Must have the same type as num_lower\", and you've complied, passing 0 (which you think of as an int32) and tf.shape(...)[...]. But this throws an error. \r\n\r\nCan I ask you to put together a very short reproduction script, please?", "```\r\nimport tensorflow as tf\r\na = tf.random_normal([5, 5])\r\nb = tf.random_normal([10, 10])\r\nc = tf.matrix_band_part(input=b, num_lower=0, num_upper=tf.shape(a)[0])\r\n\r\n```\r\nThe above code will cause a TypeError:\r\n\r\n> TypeError: Input 'num_upper' of 'MatrixBandPart' Op has type int32 that does not match type int64 of argument 'num_lower'.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@hawkinsp could I ask you to take a look? I think @carltonwang 's example is running afoul of the argument \"num_lower=0\" being passed as an int64, and I don't understand why Python chooses that. ", "The issue is because in ops registration the default value (see: `Tindex`) has been set as INT64:\r\nhttps://github.com/tensorflow/tensorflow/blob/64bb1de61377f12859a719448b65b452b03047a7/tensorflow/core/ops/array_ops.cc#L853-L859\r\n\r\nChanging the default to INT32 will address the issue and the code snippet should work fine. However, this is likely behavior-changing. Not sure if we should change it or not.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry, I didn't notice this issue was assigned to me until now. I changed the op from a hardwired `int64` to a choice of `int32` or `int64` in a previous PR. Previously, only `int64` was supported.\r\n\r\nThe error you are seeing is because:\r\na) tf.shape() returns an int32\r\nb) the types of num_lower and num_upper must be the same.\r\nc) the python `0` gets passed as an int64 by default.\r\n\r\nThere's two easy workarounds: either explicitly pass the 0 value as a numpy int32 value:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = tf.random_normal([5, 5])\r\nb = tf.random_normal([10, 10])\r\nc = tf.matrix_band_part(input=b, num_lower=np.int32(0), num_upper=tf.shape(a)[0])\r\n```\r\n\r\nor tell `tf.shape()` to return an int64 value:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = tf.random_normal([5, 5])\r\nb = tf.random_normal([10, 10])\r\nc = tf.matrix_band_part(input=b, num_lower=0, num_upper=tf.shape(a, out_type=tf.int64)[0])\r\n```\r\n\r\nAs to the larger usability question of \"should TensorFlow perform implicit promotions for integers\", it's probably better to open a new PR for that.\r\n", "(I originally had `int64` and `int32` transposed in my previous comment; corrected.)"]}]