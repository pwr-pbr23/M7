[{"number": 34605, "title": "tensorflow::Node*->out_edges() BUG ", "body": "I am writing my own graph optimization pass as a customized lib, which is register as follows: \r\n```\r\n\r\nREGISTER_OPTIMIZATION(OptimizationPassRegistry::PRE_PLACEMENT, 0,\r\n                      PartOptimization);\r\n```\r\n\r\nwhen I use the tensorflow::Node* -> out_edges() interfaces to rewrite the output nodes, I always get the number of output edges which is one less than the number tensorflow::Node* -> out_edges().size() gives me. This bugs me for quite a while. I am not sure if I use it the wrong way, or simply I register my pass in the wrong phase, or there is indeed some what bug (which I think is unlikely).\r\n\r\nAny insights will be appreciated! \r\n\r\nI am using tf-v.1.15\r\n```\r\nclass PartOptimization : public GraphOptimizationPass\r\n{\r\npublic:\r\n    Status Run(const GraphOptimizationPassOptions &options) override\r\n    {\r\n        Graph *g = options.graph->get();\r\n        for (auto n : g->op_nodes())\r\n        {\r\n            cout << n->DebugString() << endl;\r\n            cout << \"sizeof int edges: \" << n->in_edges().size() << endl;\r\n            for (const Edge *e : n->in_edges())\r\n                cout << e->DebugString() << endl;\r\n            cout << \"sizeof out edges: \" << n->out_edges().size() << endl;\r\n            for (const Edge *e : n->out_edges())\r\n            if(e)\r\n                cout << e->DebugString() << endl;\r\n            cout << endl;\r\n        }\r\n\r\n        return Status::OK();\r\n    }\r\n\r\nREGISTER_OPTIMIZATION(OptimizationPassRegistry::PRE_PLACEMENT, 0,\r\n                      PartOptimization);\r\n```\r\n\r\nmy toy model for testing:\r\n```\r\nimport tensorflow as tf\r\ntf.load_library('./tf_graph_partition.so')\r\n\r\na = tf.Variable(tf.zeros([10]))\r\nb = tf.Variable(tf.ones([10]))\r\nc = tf.add(a,b)\r\nd = tf.multiply(a, c)\r\ne = tf.add(a,d)\r\nf = tf.multiply(a, e)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    print(sess.run(f))\r\n\r\n```\r\n\r\nAnd the result is like:\r\n```\r\n{name:'save/Assign_1' id:23 op device:{} def:{{{node save/Assign_1}} = Assign[T=DT_FLOAT, _class=[\"loc:@w2\"], use_locking=true, validate_shape=true](w2, save/RestoreV2:1)}}\r\nsizeof int edges: 2\r\n[id=25 w2:0 -> save/Assign_1:0]\r\n[id=26 save/RestoreV2:1 -> save/Assign_1:1]\r\nsizeof out edges: 1\r\n\r\n{name:'w2' id:7 op device:{} def:{{{node w2}} = VariableV2[container=\"\", dtype=DT_FLOAT, shape=[2], shared_name=\"\"]()}}\r\nsizeof int edges: 1\r\n[id=34 _SOURCE:-1 -> w2:-1]\r\nsizeof out edges: 4\r\n[id=6 w2:0 -> w2/read:0]\r\n[id=17 w2:0 -> save/SaveV2:4]\r\n[id=25 w2:0 -> save/Assign_1:0]\r\n\r\n```\r\n\r\n\r\n\r\n@tensorflower-gardener ", "comments": ["Weird phenomenon appears as folllows: Every tensorflow::EdgeSet's ptrs' members  are explicitly initilized as nullptr, and also my demo involves no operations that might alter the last value of ptrs, but it shows nonzero here. \r\n```\r\n(gdb) p n->out_edges()\r\n$3 = (const tensorflow::EdgeSet &) @0x38b6aa8: {static kInline = 8, ptrs_ = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x38b14d0}, mutations_ = 59446464}\r\n```", "Thankfully, those graph/node APIs work if build my code into tensorflow package. Guess something wrong with the compiling flags. Hope someone can enlighten me the details."]}, {"number": 34604, "title": "How to quickly add extract_image_patch op support in tflite?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (or github SHA if from source):1.14\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport tensorflow as tf\r\nsaved_model_dir=\"./\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n#converter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\nopen(\"./converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, ELU, LOGISTIC, MAXIMUM, MINIMUM, MUL, RELU, RESHAPE, RESIZE_NEAREST_NEIGHBOR, REVERSE_V2, SOFTMAX, SPLIT, SQRT, SQUARE, STRIDED_SLICE, SUM, TANH, TRANSPOSE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.\r\nTraceback (most recent call last):\r\n  File \"/home/siju/.local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/siju/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/siju/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/siju/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/siju/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/siju/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, ELU, LOGISTIC, MAXIMUM, MINIMUM, MUL, RELU, RESHAPE, RESIZE_NEAREST_NEIGHBOR, REVERSE_V2, SOFTMAX, SPLIT, SQRT, SQUARE, STRIDED_SLICE, SUM, TANH, TRANSPOSE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n[saved_model.zip](https://github.com/tensorflow/tensorflow/files/3890254/saved_model.zip)\r\n\r\n\r\n**Failure details**\r\nI want to implement a block of contextual attenuation which contains op\r\n[extract image patches](https://www.tensorflow.org/api_docs/python/tf/image/extract_patches)\r\nThis op is not the first/last one. its in the middle so that i cannot do the processing outside the model.\r\n\r\n**Any other info / logs**\r\nNO\r\n\r\n", "comments": ["@gbaned Could you please help to assign one person to help me add this as a custom op? ", "> @gbaned Could you please help to assign one person to help me add this as a custom op?\r\n\r\nSure @siju-samuel  Thanks!", "@haozha111 any help is appreciated!\r\ni implemented as custom operator and now im facing another issue.\r\n\r\noutput shape of extract_image_patch is 1, 64, 85, 1536\r\nNext layer after extract_image_patch is reshape new_shape=[1,-1,4,4,96], where this error is coming.\r\n\r\n```\r\nERROR: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (1 != 0)\r\nERROR: Node number 150 (RESHAPE) failed to prepare.\r\n```\r\nIts quite a long time. please help!!\r\n", "Hi, @siju-samuel \r\nI've faced the same problem - implementation custom ops ExtractImagePatches. Could you share the way to solve the problem?", "You can try adding as a builtin-op instead of custom op. ", "Hi, @siju-samuel \r\nThanks for your fast reply.\r\nI already tried with it before, but there wasn't ExtractImagePatches ops in builtin-op list.\r\nHow could you add it as a builtin-op?", "@siju-samuel Hi, I facing the same problem when export DeepFill models. Haved you solved this problem, looking forward to chat with you!", "It was quite some time back.\r\nFirst i tried to implement ExtractImagePatches from the eigen. eigen already have api to support ExtractImagePatches.  You can add as a op in lite. For reference you can use [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/extract_image_patches_op.h) .\r\n\r\nEigen impl [link](https://github.com/libigl/eigen/blob/1f05f51517ec4fd91eed711e0f89e97a7c028c0e/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h#L151)\r\n\r\nBut for me there were some other issues as well. So i abandoned this method and i splitted the tflite graph into two and i did ExtractImagePatches outside tflite network.\r\n\r\nI strongly feel tflite must add support for this op. If its in middle of network, its difficult to process.", "@siju-samuel Thanks for your update!\r\nyes, ExtraImagePatches op is really important! I hold the same opinion with you!", "Hi\r\nDoes tflite support ExtraImagePatches op now?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34604\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34604\">No</a>\n"]}, {"number": 34603, "title": "Unable to save model using tf.saved_model when model outputs RaggedTensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 19.0.0: macOS Catalina\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nUsing `tf.function` seems to work fine for autographing functions that take a `RaggedTensor` as input and produce a `RaggedTensor` as the output. However, serializing such a module using `tf.saved_model.save()` seems to not work.\r\n\r\nWe run into\r\n```\r\n2019-11-25 20:39:57.800887: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-25 20:39:57.809936: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa1e014a930 executing computations on platform Host. Devices:\r\n2019-11-25 20:39:57.809946: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-25 20:39:58.145214: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nTraceback (most recent call last):\r\n  File \"/Users/raghavmehta/Developer/UCSD-NLP/blah.py\", line 26, in <module>\r\n    main()\r\n  File \"/Users/raghavmehta/Developer/UCSD-NLP/blah.py\", line 22, in main\r\n    tf.saved_model.save(my_module, 'model/', signatures=my_module.my_func)\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/UCSD-NLP-eKFj1XZo/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 893, in save\r\n    meta_graph_def, saveable_view, signatures, options.namespace_whitelist)\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/UCSD-NLP-eKFj1XZo/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 593, in _fill_meta_graph_def\r\n    signatures = _generate_signatures(signature_functions, resource_map)\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/UCSD-NLP-eKFj1XZo/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 468, in _generate_signatures\r\n    _tensor_dict_to_tensorinfo(outputs),\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/UCSD-NLP-eKFj1XZo/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 308, in _tensor_dict_to_tensorinfo\r\n    for key, value in tensor_dict.items()}\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/UCSD-NLP-eKFj1XZo/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 308, in <dictcomp>\r\n    for key, value in tensor_dict.items()}\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/UCSD-NLP-eKFj1XZo/lib/python3.7/site-packages/tensorflow_core/python/saved_model/utils_impl.py\", line 70, in build_tensor_info_internal\r\n    tensor_shape=tensor.get_shape().as_proto())\r\nAttributeError: 'RaggedTensor' object has no attribute 'get_shape'\r\n```\r\n\r\n**Describe the expected behavior**\r\n`tf.saved_model.save()` should work naturally for autographed functions even if they output a `RaggedTensor`.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyModule(tf.Module):\r\n    def __init__(self, name='my_module'):\r\n        super(MyModule, self).__init__(name=name)\r\n\r\n    @tf.function(input_signature=[\r\n        tf.RaggedTensorSpec(shape=(None, None), dtype=tf.string)\r\n    ])\r\n    def my_func(self, inputs):\r\n        return tf.ragged.constant([\r\n            ['Some', 'string'],\r\n            ['Some', 'other', 'string']\r\n        ])\r\n\r\n\r\ndef main():\r\n    my_module = MyModule()\r\n    my_module.my_func(tf.ragged.constant([['random', 'input']]))\r\n    # Returns <tf.RaggedTensor [[b'Some', b'string'], [b'Some', b'other', b'string']]> correctly!\r\n    tf.saved_model.save(my_module, 'model/', signatures=my_module.my_func)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n**Other info / logs**\r\nSaving a model that takes `RaggedTensor` as an input but returns a regular `Tensor` as the output using `tf.saved_model.save()` seems to work just fine.", "comments": ["@metarag \r\nI tried with recent nightly version (`!pip install tf-nightly==2.1.0dev20191126`) and i am not seeing any issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2fd53d05df0b24fe1899a8099b13567e/untitled412.ipynb).Thanks!\r\n", "Yes, the model serializes successfully but the loaded model doesn't work as expected\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    model = tf.saved_model.load('model/')\r\n    infer = model.signatures['serving_default']\r\n    infer(tf.ragged.constant([['random', 'input']]))\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\ngives the following error\r\n\r\n```\r\n2019-11-26 22:28:48.622481: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-26 22:28:48.631640: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcf63481720 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-26 22:28:48.631649: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/Users/raghavmehta/Downloads/untitled/blah2.py\", line 11, in <module>\r\n    main()\r\n  File \"/Users/raghavmehta/Downloads/untitled/blah2.py\", line 7, in main\r\n    print(infer(tf.ragged.constant([['random', 'input']])))\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/untitled-8kWb9AEQ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1551, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/Users/raghavmehta/.local/share/virtualenvs/untitled-8kWb9AEQ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1570, in _call_impl\r\n    ).format(self._num_positional_args, self._arg_keywords, args))\r\nTypeError: Expected at most 0 positional arguments (and the rest keywords, of ['inputs', 'inputs_1']), got (<tf.RaggedTensor [[b'random', b'input']]>,). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\r\n```", "I have tried on colab with TF version 2.1.0-dev20191126 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/502de891bf83c7607657e9b6ba7597a2/untitled412.ipynb). Thanks!\r\n", "In the repro gist, it looks like things work as long as I replace:\r\n\r\n    infer = model.signatures['serving_default']\r\n\r\nwith:\r\n\r\n    infer = model.my_func\r\n\r\nIs there a reason you can't just use model.my_func on the loaded model?", "@metarag could you please take a look at the above comment", "That works! \ud83d\ude00\r\n\r\nHowever, shouldn't `model.signatures['serving_default']` work too? It's based off of the official tutorials.", "@metarag,\r\nCan you please confirm if we can close this issue. Thanks! ", "Yes.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34603\">No</a>\n"]}, {"number": 34602, "title": "Add Label for XlaOp", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nAdd Label for XlaOp, because it may lead to fail to find XLAOpKernel, such as `DataFormatVecPermute`. @akuegel ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34602) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34602) for more info**.\n\n<!-- ok -->"]}, {"number": 34601, "title": "Relocate comment in resize_bilinear kernel", "body": "This comment applies to both branches, the older legacy code and the newer half-pixel code.\r\n\r\nI hope I'm not being too pedantic with pull-requests like this. I see something wrong, and I want to minimize confusion for anyone else who might pass this way.", "comments": []}, {"number": 34600, "title": "Rewrite SECURITY.md with technical writing", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n\r\nRewriting SECURITY.md with technical writing for better understanding.", "comments": ["I don't think `SECURITY.md` needs rewriting at this moment. Moreover, there is a separate internal process to handle this and some related changes."]}, {"number": 34599, "title": "Updated README.md", "body": "- Added a brief description of Tensorflow in README.md.\r\n- Rewrote parts of SECURITY.md with technical writing.\r\n\r\nFixes #34598\r\nFixes #34600 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34599) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34599) for more info**.\r\n\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34599) for more info**.\n\n<!-- ok -->", "These updates are not bringing up enough quality improvements to justify the hours of CI this would take to merge."]}, {"number": 34598, "title": "Description in README.md can be more descriptive", "body": "Description of Tensorflow in README.md can be more descriptive.", "comments": ["Thanks, but the readme description has been reviewed multiple times and I think we're good."]}, {"number": 34597, "title": "Always use real exp when scaling in linalg.expm", "body": "Previously, for matrices of complex type, the linalg.expm implementation\r\nwould compute 2**squarings as complex type (despite being an entirely\r\nreal computation). However, there's no GPU kernel for complex\r\nexponential.\r\n\r\nInstead, we can compute 2**squarings as a real type. We still do the\r\nsame number of casts as before (one), but now the cast happens after\r\nthe exponential instead of before.\r\n\r\nAs a result of this change, the entire linalg.expm computation can run\r\non GPU.", "comments": []}, {"number": 34596, "title": "Update windows release build script to create the correct pip package\u2026", "body": "\u2026 names.\r\n\r\nPiperOrigin-RevId: 282425067\r\nChange-Id: I13b05c37eb9dbeac2d9beea31b21898f12cfdc89", "comments": []}, {"number": 34595, "title": "Custom loss with VGG16 features works in keras and doesn't work in tf.keras", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab. Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using tensorflow.keras with TF 1.15 in Colab. I have custom loss function that loads VGG16 from tf.keras and does feature extraction from y_pred and y_true. This feature extraction works if I pass as arguments either y_true or y_pred or any expression without y_true. If I pass expression with y_true and something else it raises error: `You must feed a value for placeholder tensor 'dense_4_target' with dtype float and shape [?,?,?,?]\r\n\t [[{{node dense_4_target}}]]` The same code works when I change tensorflow.keras to just keras.\r\n\r\n**Describe the expected behavior**\r\n\r\nModel should compile without errors as in pure Keras\r\n\r\n**Code to reproduce the issue**\r\nPlease find gist [here](https://colab.research.google.com/gist/muxgt/c467f0a999f3a517264040465af4a5a0/untitled10.ipynb#scrollTo=GEBbaquGImyU)\r\n", "comments": ["Dear Team, any news on this?", "@muxgt Looks like we need to supply a numpy array or eager tensor for `extra_input`.\r\nWhen I changed `extra_input = Input((256, 256, 3))` to `extra_input = np.ones((256,256,3))`, it worked as expected. Thanks!\r\n\r\nAs error mentioned, you could create a placeholder and feed the value for the placeholder.\r\n\r\nFeel free to close the issue if this was resolved for you. Thanks!", "@muxgt I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34595\">No</a>\n", "TD\n\n\n\n\u5728 2020-03-27 09:45:13\uff0c\"Vishnuvardhan Janapati\" <notifications@github.com> \u5199\u9053\uff1a\n\nClosed #34595.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or unsubscribe.", "unsubscribe\n\n\n\nAt 2020-03-27 09:43:11, \"Vishnuvardhan Janapati\" <notifications@github.com> wrote:\n\n@muxgt I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or unsubscribe."]}, {"number": 34594, "title": "Reading/Writing via Azure blob storage APIs.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version 1.14:\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nSimilar to GCS FileSystem and S3 Filesystem, there could be a Azure FileSystem which could access data in azure blob storage using the azure blob storage APIs.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo. Only another implementation for a new storage system.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who runs tensorflow over files on Azure blob storage.\r\n\r\n**Any Other info.**\r\n", "comments": ["Found this already implemented here https://github.com/tensorflow/io/blob/master/RELEASE.md#release-070\r\n\r\n\r\nClosing PR."]}, {"number": 34593, "title": "Allows Keras TensorLikeDataAdapter to handle pandas series and datafr\u2026", "body": "\u2026ames.\r\n\r\nPiperOrigin-RevId: 282067215\r\nChange-Id: Ied0b0211ab38420639e00d2b03693c0330fffe8c", "comments": []}, {"number": 34592, "title": "TensorFlow SavedModel export fails with AttributeError", "body": "I'm following the tutorial exactly as it is here: https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\r\n\r\nFinally, if I want to export the trained model from this tutorial using model.save() I get this error message:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-19-768eb5acd4e8> in <module>()\r\n      3 \r\n      4 export_path = \"/tmp/saved_models/{}\".format(int(t))\r\n----> 5 model.save(export_path, save_format='tf')\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    984     \"\"\"\r\n    985     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n--> 986                       signatures, options)\r\n    987 \r\n    988   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     72   # default learning phase placeholder.\r\n     73   with K.learning_phase_scope(0):\r\n---> 74     save_lib.save(model, filepath, signatures, options)\r\n     75 \r\n     76   if not include_optimizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    905   # Note we run this twice since, while constructing the view the first time\r\n    906   # there can be side effects of creating variables.\r\n--> 907   _ = _SaveableView(checkpoint_graph_view)\r\n    908   saveable_view = _SaveableView(checkpoint_graph_view)\r\n    909 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py in __init__(self, checkpoint_view)\r\n    189           concrete_functions = [function]\r\n    190         for concrete_function in concrete_functions:\r\n--> 191           if concrete_function.name not in seen_function_names:\r\n    192             seen_function_names.add(concrete_function.name)\r\n    193             self.concrete_functions.append(concrete_function)\r\n\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```\r\n\r\nWhat's going on? Shouldn't it be possible to simply export this model to the `SavedModel` format? I'm trying with and without the `save_format='tf'` parameter.", "comments": ["Here you go: \r\n\r\n```\r\n!pip install tensorflow==2.0 \r\n!pip install -q tensorflow-hub\r\n!pip install -q tensorflow-datasets\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_datasets as tfds\r\n\r\nprint(\"Version: \", tf.__version__)\r\nprint(\"Eager mode: \", tf.executing_eagerly())\r\nprint(\"Hub version: \", hub.__version__)\r\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\r\n\r\n\r\n# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\r\n# for training, 10,000 examples for validation and 25,000 examples for testing.\r\ntrain_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\r\n\r\n(train_data, validation_data), test_data = tfds.load(\r\n    name=\"imdb_reviews\", \r\n    split=(train_validation_split, tfds.Split.TEST),\r\n    as_supervised=True)\r\n\r\nembedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\r\nhub_layer = hub.KerasLayer(embedding, input_shape=[], \r\n                           dtype=tf.string, trainable=True)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(hub_layer)\r\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\nhistory = model.fit(train_data.shuffle(10000).batch(512),\r\n                    epochs=2,\r\n                    validation_data=validation_data.batch(512),\r\n                    verbose=1)\r\n\r\nmodel.save('tmp')\r\n```", "Was able to reproduce the issue. Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/e7c994341d6d4f2b6757b03cc4359fd2/copy-of-text_classification_with_hub.ipynb).", "I am having the same issue\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-24-cb8b79bcc41d> in <module>\r\n----> 1 tf.saved_model.save(model, \"./spam/1/\")\r\n\r\n~/anaconda3/envs/orca/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    881   # Note we run this twice since, while constructing the view the first time\r\n    882   # there can be side effects of creating variables.\r\n--> 883   _ = _SaveableView(checkpoint_graph_view)\r\n    884   saveable_view = _SaveableView(checkpoint_graph_view)\r\n    885 \r\n\r\n~/anaconda3/envs/orca/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in __init__(self, checkpoint_view)\r\n    188           concrete_functions = [function]\r\n    189         for concrete_function in concrete_functions:\r\n--> 190           if concrete_function.name not in seen_function_names:\r\n    191             seen_function_names.add(concrete_function.name)\r\n    192             self.concrete_functions.append(concrete_function)\r\n\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```", "This issue is fixed in latest tf-nightly. Close this for now. Feel free to reopen it if needed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34592\">No</a>\n"]}, {"number": 34591, "title": "TensorFlow SavedModel export fails with AttributeError", "body": "I'm following the tutorial exactly as it is here: https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\r\n\r\nFinally, if I want to export the trained model from this tutorial using model.save() I get this error message:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-19-768eb5acd4e8> in <module>()\r\n      3 \r\n      4 export_path = \"/tmp/saved_models/{}\".format(int(t))\r\n----> 5 model.save(export_path, save_format='tf')\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    984     \"\"\"\r\n    985     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n--> 986                       signatures, options)\r\n    987 \r\n    988   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     72   # default learning phase placeholder.\r\n     73   with K.learning_phase_scope(0):\r\n---> 74     save_lib.save(model, filepath, signatures, options)\r\n     75 \r\n     76   if not include_optimizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    905   # Note we run this twice since, while constructing the view the first time\r\n    906   # there can be side effects of creating variables.\r\n--> 907   _ = _SaveableView(checkpoint_graph_view)\r\n    908   saveable_view = _SaveableView(checkpoint_graph_view)\r\n    909 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py in __init__(self, checkpoint_view)\r\n    189           concrete_functions = [function]\r\n    190         for concrete_function in concrete_functions:\r\n--> 191           if concrete_function.name not in seen_function_names:\r\n    192             seen_function_names.add(concrete_function.name)\r\n    193             self.concrete_functions.append(concrete_function)\r\n\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```\r\n\r\nWhat's going on? Shouldn't it be possible to simply export this model to the `SavedModel` format? I tried with and without the `save_format='tf'` parameter\r\n\r\n", "comments": ["duplicate of the issue #34592 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34591\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34591\">No</a>\n"]}, {"number": 34590, "title": "Make a TFLite test case run on big-endian machines", "body": "The test case `BasicInterpreter.ThreeStepAllocate` in `/tensorflow/lite/interpreter_test.cc` uses an array of little-endian binary data as one of its inputs. The little-endian data prevents this test from working properly on big-endian machines.\r\n\r\nThis pull request replaces the array of little-endian data with a struct, so that the embedded 32-bit fields will be in little-endian format on little-endian machines and in big-endian format on big-endian machines. After this change, the test case passes on both little-endian and big-endian machines.", "comments": ["Found a second test case in `/tensorflow/lite/string_util_test.cc` with the same issue and pushed a second commit to this branch to fix that test case too.", "@frreiss Can you please resolve conflicts? Thanks!", "Conflicts resolved.", "@nutsiepully Can you please take a look on this PR? Thanks!"]}, {"number": 34589, "title": "TANH/Sigmoid 16-bit activation functions using LUT", "body": "We think the reference functions for 16-bit activation are too complex for efficient implementation on resource constrained platforms and propose to replace the functions with a lookup table approach as follows:\r\n\r\nFirst rescale the input data to fixed range of -10.7 to +10.7. Then use a 256-entry lookup table for Sigmoid followed by linear interpolation to efficiently derive the result.\r\n\r\nThe Sigmoid LUT table is used for the TANH function, because tanh(x) = 2*sigmoid(2*x) -1 and we take into account the symmetry.\r\n\r\nThe proposed reference kernel implementation also has higher accuracy than the existing one.\r\nOn the current functions we measure a difference of up to 6.3 for sigmoid and 11.7 for tanh in quantized units compared to the floating point reference implementation over the 16-bit input range (representing -8.0 to +8.0). For the implementation of this patch we see the error reduced to less than 1.5 quantized units compared to floating point reference for both tanh and sigmoid.", "comments": ["can you share benchmark results showing how this compares to the current 16bit fixedpoint implementations on various ARM CPUs?\r\n\r\nbecause pure arithmetic implementations scale linearly with SIMD widths and lookup tables do not, we have typically found that despite looking expensive in source code, pure arithmetic implemenations can actually perform well. As ARM CPUs SIMD width increases, they keep looking better. For example, on a Cortex-A76 CPU (e.g. Pixel4) we can issue 2 128-bit multiplications each cycle, each doing 8 16-bit fixed-point multiplications, so each fixed-point multiplication effectively only costs one sixteenth of a cycle.\r\n\r\nas this varies greatly from one CPU to another, we don't expect one approach to consistently outperform the other (arithmetic vs LUT).  Instead, we have started to let both coexist in source code.  We still need to pick one to be used by default but at least having both in source code makes it easy to switch.  \r\n\r\ni don't remember the latest but some the the following people might: @renjie-liu @abattery @jianlijianli \r\n", "Thanks Elena for the pr. Like Benoit pointed out, lut is not necessarily desired from the performance point of view.\r\n\r\nAnother thing is I suspect the accuracy measurement is \"correct\", do you have any real model to benchmark the accuracy? Like you said, your method will use range from -10.7 to 10.7, but should that based on the real scenario?\r\n\r\nJaesung can probably comment more, but if you want to cover more range for your scenario, probably just add one more bits to the integer parts? (where currently it's 3 bits, so represents -8 to 8) for sigmoid?\r\n", "Hi, Elena.\r\n\r\nThank you for your contributions :)\r\n\r\nFor the performance, I have seen that LUT-table methods for Tanh/Sigmoid are practically faster than fixed point arithmetic based calculations in the modern mobile ARM CPUs for integer 8 bit cases. However, it's worth double-checking again for the integer 16 bit cases. Could you share the benchmark results before and after?\r\n\r\nIt seems that you are trying to increase accuracy based on the idea that it takes an absolute value since tanh function is symmetric and does LUT where the table covers little larger than 8-bit fixed point arithmetic range. In my first looking at it, it makes sense to me. However, As Renjie said, the Sigmoid case has the same expression range [-8, 8] with the current fixed point arithmetic kernels for Sigmoid. I am not sure that this approach improves Sigmoid case. Could you share your accuracy difference results for both Sigmoid and Tanh to us and how did you measure the accuracy?\r\n\r\nBest regards,\r\nJaesung", "Cache aliasing also plays key role in the performance of LUT and that is really hardware dependent. Seems to be the best option is for them to \"coexist\", like Benoit mentioned in his comment.", "Hi guys,\r\n\r\nThank you for the review @bjacob @abattery @jianlijianli @renjie-liu  ! \r\n\r\nI attached the file accuracy_test.cpp that demonstrates how the accuracy was tested:\r\nhttps://gist.github.com/wwwind/0b7413833cf8a7596bee084403224fa5\r\n\r\nThe test code reports the range and total sum of the error over the whole signed 16-bit integer range. Below are results of the testing.\r\n\r\n**For sigmoid:**\r\n\r\ntensorflow   sigmoid implementation error -6.392090 to 5.392578, error sum -32767.042969\r\nproposed     sigmoid implementation error -1.015625 to 1.015625, error sum -0.043635\r\n\r\n**For tanh:**\r\n\r\ntensorflow   tanh    implementation error -11.791016 to 11.791016, error sum 0.992188\r\nproposed     tanh    implementation error -1.476562 to 1.476562, error sum 0.992188 \r\n\r\nRegarding the performance benchmark, we do not have such measurements at hand. When we refer to resource constrained platforms, apart from the world of CPUs, MCUs and GPUs, we also consider neural network accelerators, the dedicated HW where small look-up table is likely to be a preferred implementation path. However, even just for CPUs, for the non-optimised reference code, we expect the proposed solution to be more efficient than the gemmlowp one due to the number of operations involved in the iterative approach used by the latter.\r\n\r\nIn the proposed solution, to compute a value, we need about 14 operations. The existing implementation is based on the iterative Newton-Raphson division algorithm to find a reciprocal, which takes around 12 operations + computation of exponent, which needs Taylor approximations with 4 terms. So, it looks a lot heavier, at least for a single result computation.\r\n\r\nWe agree that, for vectorised implementation, the performance advantage may or may not shift towards the iterative approach. Therefore, it seems that the approach proposed by @bjacob, @abattery and @jianlijianli , where both implementations will co-exist, is a sensible compromise. I would suggest that we keep a more accurate, LUT-based implementation, as a default one. Should I proceed to make the corresponding changes in this PR? What would be the best way to implement the switching mechanism between the two implementations? Could you please point to an example in the code ? \r\nThanks!", "Hi,\r\n\r\nthanks a lot for the testing! and thanks a lot for the work!\r\n\r\nI wonder should we sum the absolute value instead of the raw value? also the large sigmoid value makes me wonder it looks really look like a direct overflow: -32767.042969 really looks like -32768 plus some number (maybe we can just try with the extreme number?) or maybe just systematically shifted -0.5 for every value?\r\n\r\nalso given you're verifying the int16 value, judging from the number both have good results: +- 12/(1 << 15) for tanh\r\n\r\nI think it's fine to keep both approach, you can refer here for prepare: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc#L395-L422\r\n\r\nand here for eval: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc#L813-L824", "@wwwind Could you please check renjie-liu's comments and keep us posted. Thanks!", "Hi @renjie-liu,\r\n\r\nThanks a lot for your review. I pushed an updated version. Could you please take a look ?\r\nAlso, I think that this block is not relevant anymore for LUT approach https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc#L515 Should it be removed ?\r\n\r\nThanks,\r\nElena", "@wwwind Could you please check reviewer comments and keep us posted. Thanks!", "@renjie-liu, thanks for the review, while Elena is away, I replied where I could. Please, let us know what you think.", "@renjie-liu, @jdduke, following our off-line discussion, can we try to move this PR forward? As the proposed LUT-based INT16 implementation improves the accuracy, it seems reasonable to keep it for both kReference and kGenericOptimized paths. It would also make INT16 path in the corresponding Eval and Prepare calls symmetric to the current INT8 and UINT8 ones.", "Hi @jianlijianli I moved implementation of Tanh/Sigmoid into integer_reference_ops per discussion. Could you please review this PR ?\r\nThanks!", "Hi @jianlijianli , Could you please re-approve this PR ? I had to push a fix for the warning.\r\nThanks!", "Hi @gbaned, @liufengdb  ! I pushed a small fix for the error reported by buildifier.\r\nCould you please re-approve this PR ?\r\nThanks!", "Hi @rthadur ! This PR has been approved 10 days ago and all checks are green.\r\nIs it stuck internally with some errors ? Could you please take a look?\r\nThanks!", "@wwwind thanks for your patience , @jianlijianli is working internally on this , we will get back to you asap."]}, {"number": 34588, "title": "training with dataset cifar100fails: KeyError: 'conv2d_input", "body": "**System information**System information\r\n- OS Platform and Distribution: Arch Linux, 5.3.7-arch1-1-ARCH\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0.0\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: CUDA 10.1.243 / cuDNN 7.6.2.24\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\n**Describe the current behavior**\r\nexception thrown: \"KeyError: 'conv2d_input'\"\r\n\r\n**backtrace**\r\n\r\n> Epoch 1/10\r\n>       1/Unknown - 0s 70ms/stepTraceback (most recent call last):\r\n>   File \"cifar100.py\", line 35, in <module>\r\n>     verbose=1)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n>     total_epochs=epochs)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n>     batch_outs = execution_function(iterator)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n>     distributed_function(input_fn))\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n>     result = self._call(*args, **kwds)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n>     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n>     *args, **kwds))\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n>     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n>     graph_function = self._create_graph_function(args, kwargs)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n>     capture_by_value=self._capture_by_value),\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n>     func_outputs = python_func(*func_args, **func_kwargs)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n>     return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 66, in distributed_function\r\n>     model, input_iterator, mode)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 118, in _prepare_feed_values\r\n>     inputs = [inputs[key] for key in model._feed_input_names]\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 118, in <listcomp>\r\n>     inputs = [inputs[key] for key in model._feed_input_names]\r\n> KeyError: 'conv2d_input'\r\n\r\n**Describe the expected behavior**\r\ntraining the network doesn't fail\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\r\n\r\nds_train = tfds.load(name=\"cifar100\", split=tfds.Split.TRAIN)\r\nds_test = tfds.load(name=\"cifar100\", split=tfds.Split.TEST)\r\n\r\ninput_shape = (32,32, 3)\r\nnum_classes = 10\r\nepochs = 10\r\n\r\nmodel = tf.keras.Sequential([\r\n    Conv2D(32, 5, padding='same', activation='relu', input_shape=input_shape),\r\n    MaxPooling2D((2, 2), (2, 2), padding='same'),\r\n    BatchNormalization(),\r\n    Conv2D(64, 5, padding='same', activation='relu'),\r\n    MaxPooling2D((2, 2), (2, 2), padding='same'),\r\n    Flatten(),\r\n    Dense(1024, activation='relu'),\r\n    Dropout(0.4),\r\n    Dense(num_classes, activation='softmax')\r\n])\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(\r\n    loss=tf.keras.losses.categorical_crossentropy,\r\n    optimizer='adam',\r\n    metrics=['accuracy'])\r\n\r\nmodel.fit(ds_train,\r\n          epochs=epochs,\r\n          validation_data=ds_test,\r\n          verbose=1)\r\n```\r\n\r\n", "comments": ["Issue replicating for the given code, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/28b102fd0e6013a885cba6e51d5c944e/34588.ipynb) of colab.Thanks!", "@olk, \r\nThe error might be because the `Input Shape` which the Convolutional Layer expects and the Input Shape of tf.data.dataset doesn't match.  Please refer [this link](https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c), which has the complete code for Image Classification for Cifar Dataset. Thanks!", "`as_supervised=True` fixed the issue\r\n"]}, {"number": 34587, "title": "Fixing documentation issue on ctc_loss, this relates to the issue:", "body": "https://github.com/tensorflow/tensorflow/issues/34586", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34587) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34587) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed", "@googlebot I fixed", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34587) for more info**.\n\n<!-- ok -->", "Hi. Please make PR against master branch. We are only making PRs against release branches when we are preparing a release or a patch release. In either case, these PRs are only cherry-picks from master."]}, {"number": 34586, "title": "Documentation Error on nn.ctc_loss", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss\r\n\r\n## Description of issue (what needs changing):\r\nThe logits and logits_time_major parameters are ill defined.\r\n### Current on docs\r\n* logits: tensor of shape [frames, batch_size, num_labels], if logits_time_major == False, shape is [batch_size, frames, num_labels].\r\n* logits_time_major: (optional) If True (default), logits is shaped [time, batch, logits]. If False, shape is [batch, time, logits]\r\n\r\n### Clear description\r\n* logits: tensor of shape [time, batch_size, num_labels], if logits_time_major == True. Shape is [batch_size, frames, num_labels] if logits_time_major == False.\r\n* logits_time_major: (optional) If True (default), logits is shaped [time, batch_size, num_labels]. If False, shape is [batch_size, time, num_labels]\r\n\r\n### Submit a pull request?\r\nI'm preparing a PR\r\n", "comments": ["@leonardoaraujosantos \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34584, "title": "I have no idea how properly run pb file with \"serve\" tag do you have any idea?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["You can follow the example given in this [page ](https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple) where the model is saved as .pb file and served using Tensorflow serving. \r\n\r\nAlso plese post these questions in stackoverflow as this issue not related to bug/performance, build/install, feature request or docs related issues. Thanks!"]}, {"number": 34583, "title": "Filter outputs for available estimators", "body": "Hi,\r\n\r\nI have trouble standardizing outputs of Tensorflow estimators. This is achievable for pre-estimator `tf.session()` era and in custom estimators.\r\n\r\n1. Before `tf.Estimators()` came into play, I could play with the output of inference graph using `add_meta_graph_and_variables`. But I can't use this in estimators since it wraps intitalizing sessions from user.\r\n2. When building a custom estimator, I can specify `export_output` parameter and define the specific fields which I want to output.\r\n\r\nFor pre-made estimators, is there a way to pick specific fields out of the standard output?", "comments": ["@tejaslodaya, Could you provide the standalone code for us to analyze the reported issue. Also mention Tensorflow version. Thanks! ", "Hi @gadagashwini ,\r\n\r\nThe pip requirements are here:\r\n```tensorflow==2.0.0\r\ntensorflow-data-validation==0.14.1\r\ntensorflow-estimator==2.0.1\r\ntensorflow-metadata==0.14.0\r\ntensorflow-model-analysis==0.14.0\r\ntensorflow-serving-api==2.0.0\r\ntensorflow-transform==0.14.0\r\n```\r\n\r\nIt is not an issue. It is more like if that feature is available.\r\n\r\nLet me know specifically what you need from my end.", "@tejaslodaya, Just to verify, did you get a chance to go through this [link](https://www.tensorflow.org/guide/estimator#pre-made_estimators). Thanks", "Yes, I did. ", "@gadagashwini Any update on this would be appreciated?", "I figured a hacky way out, but I am still looking for a permanent \"clean\" solution from TF team.\r\n\r\n```py\r\nclass Wrapper(tf.estimator.Estimator):\r\n    def __init__(self, **kwargs):\r\n        dnn = tf.estimator.DNNLinearCombinedClassifier(**kwargs)\r\n    \r\n        def model_fn(mode, features, labels):\r\n            spec = dnn._call_model_fn(features, labels, mode, dnn.config)\r\n            export_outputs = None\r\n            \r\n            if spec.export_outputs:\r\n                all_output_tensors = spec.export_outputs[\"predict\"].outputs\r\n                export_outputs = {\r\n                    \"serving_default\": tf.estimator.export.PredictOutput({\r\n                        \"probabilities\": all_output_tensors[\"probabilities\"],\r\n                        \"classes\": all_output_tensors[\"classes\"]\r\n                    })}\r\n            copy = list(spec)\r\n            copy[5] = export_outputs\r\n            return tf.estimator.EstimatorSpec(*copy)\r\n\u200b\r\n        super(Wrapper, self).__init__(model_fn, kwargs[\"model_dir\"], dnn.config)\r\n```\r\nBuild a wrapper over \"canned estimators\" and treat them as \"custom estimators\". There won't be any latency implications because we're manually tweaking the graph according to our needs.\r\n\r\nBecause `DNNLinearCombinedClassifier` emits out many outputs, I am only concerned with probabilities and classes. I pluck them out of the existing graph and insert them in my graph.\r\n\r\nTo call the estimator -\r\n```py\r\nestimator = Wrapper(\r\n\tlinear_feature_columns=[payment_type],\r\n\tdnn_feature_columns=[fare, trip_start_hour],\r\n\tdnn_hidden_units=[100, 70, 50, 25],\r\n\tmodel_dir = MODEL_DIR)\r\n```\r\n\r\n@omalleyt12 any update on this?", "@omalleyt12 @gadagashwini @ymodak \ud83d\udc46 ", "@tejaslodaya,\r\n\r\nWe are checking to see if this is still an issue. Can you take a look at this [link](https://www.tensorflow.org/api_docs/python/tf/estimator/export/ExportOutput) and let us know if that is what you're looking for? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34583\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34583\">No</a>\n"]}, {"number": 34582, "title": "Correctly counting the number of params for TextVectorization", "body": "When calling a `model.summary()` using a `TextVectorization` layer, for every layer it is needed to compute the number of parameters via `layer.count_params()`. Currently, an exception is raised.\r\n\r\nTo accomplish a successful `tf.keras.Model.summary()`, internally followed by `TextVectorization.count_params()`:\r\n* `lookup_ops.MutableHashTable` should have a `shape` attribute\r\n* using `tensor_shape.TensorShape((None,))` on both weights (table and tf-idf) raises a `np.prod` exception - `tensor_shape.TensorShape((0,))` is required\r\n\r\n## Test\r\nWhen running `model.summary()`:\r\n\r\nBefore:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ntext (InputLayer)            [(None, 1)]               0         \r\n_________________________________________________________________\r\n---------------------------------------------------------------------------\r\nAttributeError: 'TrackableWeightHandler' object has no attribute 'shape'\r\n```\r\n\r\nNow:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ntext (InputLayer)            [(None, 1)]               0         \r\n_________________________________________________________________\r\ntext_vectorization_2 (TextVe (None, 400)               0         \r\n_________________________________________________________________\r\n...\r\n=================================================================\r\nTotal params: ...\r\nTrainable params: ...\r\nNon-trainable params: ...\r\n_________________________________________________________________\r\n```", "comments": ["@tanzhenyu shouldn't this be reviewed before tf 2.1.0?", "Hi @lagejoao - I don't think this fix will work as intended - specifically, we cannot change the IDF weight to have a (0) shape, since we rely on the fact that a (None) shape indicates a flexibly-sized weight (which is required, since we don't know the vocabulary size in all cases until after adapt() and adapt() necessarily happens after build() and init().) Also, we can't add the shape attr directly to the table object, we need to add it to the TrackableWeightHandler that owns its overall state.\r\n\r\nI'm taking a look at this internally and should have a fix very soon. Thank you for bringing this to our attention!", "Hi @markomernick, you are right.\r\nWhen testing my fix, I was mutating `TrackableWeightHandler` and not `MutableHashTable`. Where the first is a wrapper of the latter. Changing that logic to the `init()` I am mutating `MutableHashTable`.\r\n\r\nRelated with the TF-IDF case, I understand that it isn't the ideal fix.\r\n\r\nAppreciate if you could share your internal fix.", "Hi @lagejoao - I've submitted the code to the internal repo, and the fix should show up in the nightly soon. Thank you for your patience as we resolved this issue, and I hope you enjoy using TextVectorization!", "I'm having the same problem on colab, [here is notebook](https://colab.research.google.com/drive/1NCCl7QvT9jV0U9j3kheblAs0ehQwpYa3). I tried to install nightly with but I'm still seeing this same problem `pip install -q tf-nightly`\r\n```\r\n>>> tf.__version__\r\n'2.1.0'\r\n```", "> I'm having the same problem on colab, [here is notebook](https://colab.research.google.com/drive/1NCCl7QvT9jV0U9j3kheblAs0ehQwpYa3). I tried to install nightly with but I'm still seeing this same problem `pip install -q tf-nightly`\r\n> \r\n> ```\r\n> >>> tf.__version__\r\n> '2.1.0'\r\n> ```\r\n\r\nHmm...if you install nightly why the version would be 2.1.0? It should be something like dev20200225", "yeah this is also weird, I think it is because I tried to install tensorflow text after installing the nightly version. After reseting the colab and installing the nightly without TF Text, I've `'2.2.0-dev20200218'` and `model.summary()` is working \ud83c\udf89 ", "Versions of tf.Text are normally tied to specific TF versions. The main reason being that it is a shared library and minor releases can cause changes in TF's ABI which could cause runtime errors. Thus, the setup.py for tf.Text installation checks for the same major/minor version of TF being installed. (eg. Pip installing tf.Text 2.1.x will look for & force TF 2.1.x being installed).\r\n\r\nYou could reinstall the nightly version of TF after installing tf.Text to force the version you want, but there is the possibility of symlinking errors in the tf.Text ops."]}, {"number": 34581, "title": "Ignore \"_kernel\" attribute if it is set to \"host\" when using XLADevice", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nIgnore \"_kernel\" attribute if it is set to \"host\" when using XLADevice, because it may lead to fail to find XLAOpKernel. @akuegel ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34581) for more info**.\n\n<!-- need_sender_cla -->", "@Agoniii please sign CLA.", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34581) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34581) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 34580, "title": "Golang tensorflow no longer builds due to bad import in saved_model.go", "body": "A recent change (Change-Id: Iefdf75ed88f54d97a0a7d210f5a42f3123205bf2) has broken an import in file tensorflow/blob/master/tensorflow/go/saved_model.go\r\n\r\ntfpb \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework\"\r\n\r\n", "comments": ["@noj-richards, Please provide the exact sequence of commands / steps that you executed before running into the problem. Also include Tensorflow version. Thanks!", "Version is as 'master' with Change-id: Iefdf75ed88f54d97a0a7d210f5a42f3123205bf2\r\n\r\nTo reproduce simply try and import tf.SavedModel,\r\n\r\ne.g:\r\n\r\nimport (\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n)\r\n\r\n//Config Config\r\n\r\n//CNN CNN\r\ntype CNN struct {\r\n\tmodel               *tf.SavedModel\r\n\r\nThis will produce the error:\r\n\r\n../../../../../github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework\" in any of:\r\n        /usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework (from $GOROOT)\r\n        /Users/jonrichards/code/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework (from $GOPATH)", "Confirmed we're having the same prob. Had to revert to commit `36ef4264270e3815ea4b485a52e35e26e43e2112` in order to build", "is this also causing this failure?\r\n\r\n```\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework\" in any of:\r\n\t/usr/local/Cellar/go/1.13.4/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework (from $GOROOT)\r\n\t/Users/xxxxx/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework (from $GOPATH)\r\n\r\n```\r\n", "> is this also causing this failure?\r\n> \r\n> ```\r\n> go get github.com/tensorflow/tensorflow/tensorflow/go\r\n> \r\n> package github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework\" in any of:\r\n> \t/usr/local/Cellar/go/1.13.4/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework (from $GOROOT)\r\n> \t/Users/xxxxx/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework (from $GOPATH)\r\n> ```\r\n\r\nIndeed. That's a simpler minimal example of the prob!", "I've faced this issue too. During the building of the Docker image, got\r\n\r\n```\r\nStep 7/10 : RUN go get github.com/tensorflow/tensorflow/tensorflow/go\r\n ---> Running in aa0d0a73e640\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOROOT)\r\n\t/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOPATH)\r\n```", "What's the recommended way to install the go lang tensorflow binding now that this bug has been identified? Is there any fix in the works? \r\n", "any fix for this issue", "Wow this issue has been completely ignored. It's been almost 2 months! \ud83d\ude21 \r\nIt's an installation problem, ffs. Not some `nice to have` or `enhancement` feature. \r\nAnd the workaround is extremely hacky and not at all pleasant. ", "anyone can help please. trying go get \r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\ngetting an error \"cannot find package...\"\r\n\r\nwhat i'm doing wrong ?\r\n", "Fixed as of https://github.com/tensorflow/tensorflow/commit/4221d1aa4d20ada495771528bb13ca786d0bdbe0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34580\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34580\">No</a>\n", "(As a sidenote, please follow the instructions on the Go README which differs slightly from the documentation on tensorflow.org. I'll work on updating the official docs):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md", "I still have this issue:\r\n```\r\nStep 8/9 : RUN go get -d github.com/tensorflow/tensorflow/tensorflow/go\r\n ---> Running in c8a1c8c24a33\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\" in any of:\r\n\t/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOROOT)\r\n\t/root/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOPATH)\r\nThe command '/bin/sh -c go get -d github.com/tensorflow/tensorflow/tensorflow/go' returned a non-zero code: 1\r\n```", "Yes, Think this is still an issue:\r\n\r\n../../../../../github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:24:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\" in any of:\r\n        /usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOROOT)\r\n        /Users/xxxx/code/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOPATH)\r\n\r\n../../../../../github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:24:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\" in any of:\r\n        /usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOROOT)", "I have this issue too:\r\n\r\ngo: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\r\n../gopath/pkg/mod/github.com/tensorflow/tensorflow@v2.2.0+incompatible/tensorflow/go/saved_model.go:24:2: module github.com/tensorflow/tensorflow@latest found (v2.2.0+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\r\nmake: *** [release] Error 1", "Hi, \r\n\r\nI am experience the issue as well, any updates?\r\n\r\nRunning the following command:\r\n```\r\ngo get -d github.com/tensorflow/tensorflow/tensorflow\r\n```\r\n\r\nProduces the following output:\r\n\r\n```\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t/home/marios/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n```", "Go packages depends on the C lib which is using 1.15 Tensorflow version, go get is trying to get the last 2.2.0.\r\n\r\nThere is a workaround described here => https://github.com/tensorflow/tensorflow/issues/23257#issuecomment-433751410\r\n\r\nOn my side I use a go.mod file with \r\n`require github.com/tensorflow/tensorflow v1.15.0`\r\nand it worked \ud83d\ude4c\r\n", "Same problem here, checked my C lib is 2.2.0 but when try to use the go code:\r\n\r\n```\r\ngo: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\r\n/home/antonio/go/pkg/mod/github.com/tensorflow/tensorflow@v2.2.0+incompatible/tensorflow/go/saved_model.go:24:2: module github.com/tensorflow/tensorflow@latest found (v2.2.0+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\r\n\r\n```\r\n\r\nUnfortunatelly I can not dowgrade to TF 1.15", "**With Debian 10 and Tensorflow v2.2**\r\n\r\nSame problem here. Following command:\r\n`go get -d github.com/tensorflow/tensorflow/tensorflow/go`\r\n\r\n produce:\r\n`package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOROOT)\r\n\t/home/joceran/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOPATH)\r\n`\r\nFollowing command:\r\n`go generate github.com/tensorflow/tensorflow/tensorflow/go/op`\r\n\r\nproduce:\r\n`google/protobuf/any.proto: File not found.\r\ngoogle/protobuf/duration.proto: File not found.\r\ntensorflow/core/protobuf/autotuning.proto:10:1: Import \"google/protobuf/any.proto\" was not found or had errors.\r\ntensorflow/core/protobuf/autotuning.proto:11:1: Import \"google/protobuf/duration.proto\" was not found or had errors.\r\ntensorflow/core/protobuf/autotuning.proto:61:3: \"google.protobuf.Duration\" is not defined.\r\ntensorflow/core/protobuf/autotuning.proto:74:3: \"google.protobuf.Any\" is not defined.\r\n../genop/main.go:17: running \"bash\": exit status 1\r\ngo/src/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go:17: running \"go\": exit status 1\r\n`\r\n\r\nI tried with tensorflow r2.1 (git checkout r2.1) and the `go get -d...` error disappear. Unfortunately to test the subsequent `go generate` command I need to compile libtensorflow.so.2 against tensorflow v2.1.0 (mine is compiled against tensroflow 2.2.0 of course). \r\nEDIT: OK my bad, saw the warning on the C libtensorflow about compatibility. So no golang serving until the release of a compatible lib I guess.\r\n", "Is there any official progress on this?", "running into this error for TF v2.3.1. is there a way to generate this package?\r\n```\r\n\u2514\u2500 $ \u25b6 go test github.com/tensorflow/tensorflow/tensorflow/go\r\nsaved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t/home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/vendor/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (vendor tree)\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t/home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n```\r\n\r\nReverted to TF v1.15.0 and things are working fine:\r\ndownloaded c-lib from https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz "]}, {"number": 34579, "title": "Suspected memory leak - model.predict", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNA\r\n- TensorFlow installed from (source or binary):\r\nbinary wheel via PyPI\r\n- TensorFlow version (use command below):\r\nv2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nNA\r\n- GCC/Compiler version (if compiling from source):\r\nNA\r\n- CUDA/cuDNN version:\r\nNA\r\n- GPU model and memory:\r\nNA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm suspecting a memory leak on keras model.predict (**running on cpu only**).\r\nPerforming model.predict in an infinite loop - demonstrates memory leak trend (~400MB in 30min, please see image below).\r\nThis trend happens even though I call `gc.collect()` on every iteration.\r\nIn addition - using `gc.get_objects()` I can see that every iteration leaks exactly 1298 new objects. Using `objgraph` the leaked objects are:\r\n```bash\r\ntuple                          751101      +741\r\nlist                           202237      +197\r\ndict                           155046      +137\r\nTensor                          26665       +27\r\nTF_Output                       26621       +27\r\nOperation                       26686       +27\r\n_InputList                      26686       +27\r\nDimension                       15675       +16\r\nset                             18103       +15\r\nTraceableStack                  11629       +12\r\nTensorShape                     10845       +11\r\nbuiltin_function_or_method      10757        +9\r\nOrderedDict                      8799        +9\r\nweakref                         14739        +6\r\nTensorSpec                       3873        +4\r\nCondition                        2926        +3\r\ndeque                            2930        +3\r\n_local                           2925        +3\r\nObjectIdentitySet                2909        +3\r\nScopedTFGraph                    2909        +3\r\nGroupLock                        2909        +3\r\nFuncGraph                        2906        +3\r\nObjectIdentityWeakSet            2908        +3\r\n_EagerDefinedFunction            2904        +3\r\nScopedTFFunction                 2904        +3\r\n_EagerDefinedFunctionDeleter     2904        +3\r\n```\r\n**Describe the expected behavior**\r\nMemory shouldn't increase over time when calling `gc.collect()` nor should there be objects leaks per prediction.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nfrom collections import defaultdict\r\nimport gc\r\nimport objgraph\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\nmodel.evaluate(x_test,  y_test, verbose=2)\r\n\r\n\r\ndef mem_stat():\r\n    \r\n    objs = gc.get_objects()\r\n    print(\"total objects count\", len(objs))\r\n    \r\nc = 1\r\nwhile True:\r\n    print(\"----------- iter\", c)\r\n\r\n    model.predict(x_test)\r\n    \r\n    gc.collect()\r\n\r\n    print(\"mem stat after predict:\")\r\n    mem_stat()\r\n    objgraph.show_growth(limit=30)\r\n    c += 1\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n![image](https://user-images.githubusercontent.com/17004015/69528569-27b63980-0f77-11ea-8db3-ada1688c4d3b.png)\r\n\r\n", "comments": ["This seems to be related to [this Keras issue](https://github.com/keras-team/keras/issues/13118). It's hard to say though whether the bug is rooted in Keras or TensorFlow.", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191125 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/255d880fc37a666d928184da1579fb88/untitled410.ipynb). Thanks!", "Hi @ravikyram,\r\n\r\nI run your colab notebook and it seems that at least in term of objects leak - version 2.1.0dev is stable (I can't see addition of python objects per iteration).\r\n\r\nCan you please approve? If you believe it solved in 2.1.0 do you think there will be a fix in 2.0 too or should I wait for 2.1.0?\r\n\r\nThanks! ", "@awaizman1 you are right, the issues seems to be resolved now. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34579\">No</a>\n", "model.predict_on_batch fix for me."]}, {"number": 34578, "title": "Many problems when to convert pb to tflite", "body": " \r\n- OS Platform and Distribution ( Linux Ubuntu 18.04 **install on VMware Fusion **):\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (or github SHA if from source): 1.14.0\r\n\r\n\r\n> train model\r\nI download pet data source and train\r\n\r\n```\r\n python3 object_detection/model_main.py --pipeline_config_path=ssd_mobilenet_v1_pets.config  --model_dir=output\r\n```\r\n \r\n\r\n> use to get tflite_graph.pb and tflite_graph.pbtxt \r\n\r\n```\r\n python3 object_detection/export_tflite_ssd_graph.py  --pipeline_config_path=ssd_mobilenet_v1_pets.config  --trained_checkpoint_prefix=output/model.ckpt-0  --output_directory=output --add_postprocessing_op=true\r\n```\r\n\r\n> get tflite \r\n\r\n```\r\nbazel run -c opt tensorflow/lite/toco:toco --  --input_file=$TRAINHOME/tflite/tflite_graph.pb --output_file=$TRAINHOME/tflite/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n```\r\n\r\n`tensorflow/lite/toco:` is a directory which i download tensorflow source code from github\r\n \r\nerror message :\r\n\r\n```bash\r\n INFO: Writing tracer profile to '/home/kscorpio/.cache/bazel/_bazel_kscorpio/875b32c2c1b2a0876b2c7691115da3ca/command.profile.gz'\r\nERROR: Skipping 'tensorflow/lite/toco:toco': error loading package 'tensorflow/lite/toco': in /home/kscorpio/tensorflow/tensorflow.bzl: in /home/kscorpio/tensorflow/core/platform/default/build_config_root.bzl: Unable to find package for @local_config_remote_execution//:remote_execution.bzl: The repository '@local_config_remote_execution' could not be resolved.\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/lite/toco': in /home/kscorpio/tensorflow/tensorflow.bzl: in /home/kscorpio/tensorflow/core/platform/default/build_config_root.bzl: Unable to find package for @local_config_remote_execution//:remote_execution.bzl: The repository '@local_config_remote_execution' could not be resolved.\r\nINFO: Elapsed time: 0.183s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n\r\n```\r\n\r\nin addition , `bazel run xxxxx:toco` and `tflite_convert ` which is the right way ?\r\n \r\n \r\n\r\n\r\n \r\n", "comments": ["Looks like you are trying to serve a object detection model in tf lite. In that case following example may come in handy.\r\nSee https://www.tensorflow.org/lite/models/object_detection/overview\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/README.md\r\nFor [building a custom model](https://www.tensorflow.org/lite/models/object_detection/overview#customize_model) you may use transfer learning to train new data.\r\n", "I don't want to use a tflite object detection model in Android devices or iOS devices . It's to train a model with Tensorflow model - object detection ,and covert \"xxx.pb\" to \"xx.tflite\"\r\n\r\n> https://github.com/tensorflow/models/tree/master/research/object_detection\r\n\r\n> https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md \r\n\r\n\r\n"]}, {"number": 34577, "title": "tf.losses.cosine_similarity is a negative quantity between -1 and 0 ", "body": "In tensorflow website, it describes tf.losses.cosine_simialrity as follows:\r\nNote that it is a negative quantity between -1 and 0, where 0 indicates orthogonality and values closer to -1 indicate greater similarity. \r\nIn fact, the quantity is from 1. to -1., it just takes a negative from normal cosine_similarity.\r\nThe page is at\r\nhttps://tensorflow.google.cn/api_docs/python/tf/keras/losses/cosine_similarity", "comments": ["I can find the code in github.\r\n`\r\ny_true = nn.l2_normalize(y_true, axis=axis)\r\n\r\n  y_pred = nn.l2_normalize(y_pred, axis=axis)\r\n\r\n  return -math_ops.reduce_sum(y_true * y_pred, axis=axis)\r\n`", "@songs18, Could you post complete standalone code snippet for us to analyze the issue better?", "\u2018\u2019\u2018Python\r\ndef cosine_similarity(y_true, y_pred, axis=-1):\r\n  \"\"\"Computes the cosine similarity between labels and predictions.\r\n  Note that it is a negative quantity between -1 and 0, where 0 indicates\r\n  orthogonality and values closer to -1 indicate greater similarity. This makes\r\n  it usable as a loss function in a setting where you try to maximize the\r\n  proximity between predictions and targets.\r\n  `loss = -sum(y_true * y_pred)`\r\n  Args:\r\n    y_true: Tensor of true targets.\r\n    y_pred: Tensor of predicted targets.\r\n    axis: Axis along which to determine similarity.\r\n  Returns:\r\n    Cosine similarity tensor.\r\n  \"\"\"\r\n  y_true = nn.l2_normalize(y_true, axis=axis)\r\n  y_pred = nn.l2_normalize(y_pred, axis=axis)\r\n  return -math_ops.reduce_sum(y_true * y_pred, axis=axis)\r\n'''", "@songs18 Thank you for bringing this up. We assume that prediction is a value between 0 and 1 and labels are binary ie. 0 or 1. With these assumptions cosine similarity will be in positive space - [0, 1] and the loss function will be in [-1, 0].", "Closing this as the behavior is as expected. Please let us know if you have any questions. Thank you!", "poor thing,. we cant use it for generic cosine similarity loss functions..."]}, {"number": 34576, "title": "[tflite] Need custom implementantion for operators", "body": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, RESHAPE, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.", "comments": [" You can try [converting your model](https://www.tensorflow.org/lite/guide/ops_select#converting_the_model) using ```TFLITE_BUILTINS, SELECT_TF_OPS``` flags to reduce custom implementation of ops (only a subset of ops can be implemented by this method while others may still require custom implementation )\r\n\r\nWhat version of TF are you using?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34575, "title": "GPU support for tf.stack / tf.unstack with complex64 / complex128", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): Yes (although I don't know what's involved)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere currently don't seem to be GPU kernels for complex64 and complex128 `stack` and `unstack` operations. For example, the following code:\r\n```\r\nwith tf.Graph().as_default():\r\n    with tf.device('/gpu:0'):\r\n        x = tf.placeholder(shape=(1, ), dtype=tf.complex128)\r\n        y = tf.placeholder(shape=(1, ), dtype=tf.complex128)\r\n        z = tf.stack([x, y], axis=0)\r\n        with tf.Session() as s:\r\n            s.run(z, feed_dict={x: [1], y: [2]})\r\n```\r\nthrows an error:\r\n```InvalidArgumentError: Cannot assign a device for operation stack: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU, XLA_CPU] possible_devices_=[]\r\nPack: CPU XLA_CPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  stack (Pack) /device:GPU:0\r\n\r\nOp: Pack\r\nNode attrs: axis=0, N=2, T=DT_COMPLEX128\r\nRegistered kernels:\r\n  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='CPU'; T in [DT_QINT32]\r\n  device='CPU'; T in [DT_QUINT8]\r\n  device='CPU'; T in [DT_QINT8]\r\n  device='CPU'; T in [DT_VARIANT]\r\n  device='CPU'; T in [DT_RESOURCE]\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_BOOL]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nAnybody using TensorFlow with GPU for calculations involving complex numbers.", "comments": ["> Are you willing to contribute it (Yes/No): Yes (although I don't know what's involved)\r\n\r\nYou can start in the GPU kernel registration code for the [`Pack`](https://github.com/tensorflow/tensorflow/blob/b41fbcbf851bce1fcf462ad8f251756d9a6e166e/tensorflow/core/kernels/pack_op.cc#L158) and [`Unpack`](https://github.com/tensorflow/tensorflow/blob/b41fbcbf851bce1fcf462ad8f251756d9a6e166e/tensorflow/core/kernels/unpack_op.cc#L145) ops.  As a first step, maybe try adding `DT_COMPLEX64` and `DT_COMPLEX128` to the kernel registrations to see if that works?", "Added a PR #35375 to add complex support."]}]