[{"number": 7086, "title": "make weighted_cross_entropy consistent with sigmoid_cross_entropy.", "body": "Some relevant discussions in #6700.\r\nSince Python doesn't allow positional arguments to come after keyword arguments, this PR will enforce `pos_weight` to be passed as named arguments as well.\r\nBut maybe it's still better to throw error than to fail silently.", "comments": ["Can one of the admins verify this patch?", "While this is an improvement over what is currently in the repository it would break a lot of existing code, and we're sufficiently far along in the 1.0 release process that this kind of API-breaking change is probably not a great idea.\r\n\r\nI'm closing this for now, but it's a good idea to revisit it once things have stabilized more."]}, {"number": 7085, "title": "Add python 3.3 and 3.6 binary URLs to installation instruction.", "body": "", "comments": []}, {"number": 7084, "title": "Flush denormals to zero for Windows+GPU+cmake", "body": "Ideally this would apply to all platforms using nvcc+cmake, but the\r\nrelevant section seems to exist only for Windows.\r\n\r\n@mrry Does this look right to you?\r\n@gunan Can you test this on Windows GPU for me?", "comments": ["This will hopefully resolve the Windows GPU issues caused by https://github.com/tensorflow/tensorflow/pull/7016.", "Looks right to me (as far as I can tell, the flag has the same syntax on Linux and Windows).", "@gunan Over to you for testing.  I do love this style of programming: it's like being back in the days of batch processing with punchcards.", "Thanks for quickly preparing a fix.\r\nJust triggered the tests.\r\nhttp://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/11/\r\n", "Seems to pass.  I'll merge."]}, {"number": 7083, "title": "Feature request: simplify tf.train.write_graph() and tf.import_graph_def()", "body": "As can be seen in https://github.com/tensorflow/tensorflow/issues/616 the current mechanism is too painful. (Also #4044 etc )", "comments": ["Could you summarize what the problem is, perhaps with a snippet of code? Thanks.", "Nevermind, the old workaround of #616  has been completely streamlined and simplified in one single new function `tf.train.import_meta_graph`\r\n\r\n"]}, {"number": 7082, "title": "Disable graph transforms library in windows pip packages.", "body": "", "comments": ["Jenkins, test this please.", "http://ci.tensorflow.org/job/tf-pr-win-bzl/1/ running bazel windows tests.", "Build failed with:\r\nhttp://ci.tensorflow.org/job/tf-pr-win-bzl/1/console\r\n\r\nWill also do the same conditional include swig includes.", "Jenkins, test this please.", "all tests passed (android was failing before, now that is also passing."]}, {"number": 7081, "title": "Updates to install py3.5 packages. No more need for python 3.4, and new", "body": "dependency, werkzeug.", "comments": ["Do the mac and windows scripts need to updated accordingly as well?", "They have to be updated internally."]}, {"number": 7080, "title": "Fix some BatchToSpace and SpaceToBatch examples.", "body": "```\r\n>>> x = [[[[1],   [2],  [3],  [4]],\r\n...       [[5],   [6],  [7],  [8]],\r\n...       [[9],  [10], [11],  [12]],\r\n...       [[13], [14], [15],  [16]]]]\r\n>>> tf.space_to_batch_nd(x, [2, 2], [[0, 0], [0, 0]]).eval()\r\narray([[[[ 1],\r\n         [ 3]],\r\n\r\n        [[ 9],\r\n         [11]]],\r\n\r\n\r\n       [[[ 2],\r\n         [ 4]],\r\n\r\n        [[10],\r\n         [12]]],\r\n\r\n\r\n       [[[ 5],\r\n         [ 7]],\r\n\r\n        [[13],\r\n         [15]]],\r\n\r\n\r\n       [[[ 6],\r\n         [ 8]],\r\n\r\n        [[14],\r\n         [16]]]], dtype=int32)\r\n>>> x = [[[[1], [3]], [[9], [11]]],\r\n...      [[[2], [4]], [[10], [12]]],\r\n...      [[[5], [7]], [[13], [15]]],\r\n...      [[[6], [8]], [[14], [16]]]]\r\n>>> tf.batch_to_space_nd(x, [2, 2], [[0, 0], [0, 0]]).eval()\r\narray([[[[ 1],\r\n         [ 2],\r\n         [ 3],\r\n         [ 4]],\r\n\r\n        [[ 5],\r\n         [ 6],\r\n         [ 7],\r\n         [ 8]],\r\n\r\n        [[ 9],\r\n         [10],\r\n         [11],\r\n         [12]],\r\n\r\n        [[13],\r\n         [14],\r\n         [15],\r\n         [16]]]], dtype=int32)\r\n```\r\n", "comments": ["Can one of the admins verify this patch?", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  acmeideal@gmail.com\n    Domain biomassiv.es has exceeded the max emails per hour (102/100 (102%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext5.iad.github.net ([192.30.252.196]:33960 helo=github-smtp2a-ext-cp1-prd.iad.github.net)\n\tby chi-server32.websitehostserver.net with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.87)\n\t(envelope-from <noreply@github.com>)\n\tid 1cWmhr-0032I1-PP\n\tfor greg@biomassiv.es; Thu, 26 Jan 2017 10:15:27 -0600\nDate: Wed, 25 Jan 2017 15:46:43 -0800\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1485388003;\n\tbh=PgIM14L7+IWUPR7oI3nxwVIojUgr0fAt/dszWl7EwY8=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=L4eNLJrMmQRaB4FiKFVPatKJXWA9eoE0fMt8DF+fZs7lXT/Xv+TKjNcydyVI+g5xl\n\t vnjezRx6G+wkCLdFtP3y9ZAZn2BwcY5GvHWrBKbHyjj9XcQFL7pHhZ4oWa/2Z4RkWh\n\t I8GZPTJBxuFrSIez9+5GYzxYgu4YmaMCUREAM1oE=\nFrom: Tensorflow Jenkins <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/7080/c275268917@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/7080@github.com>\nReferences: <tensorflow/tensorflow/pull/7080@github.com>\nSubject: Re: [tensorflow/tensorflow] Fix some BatchToSpace and SpaceToBatch\n examples. (#7080)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_588938e373b85_53583fc9a1ab1134268282\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: tensorflow-jenkins\nX-GitHub-Recipient: biomassives\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0042d4e2402e52f45b6991f4c36b940da7e7b321fae4c2e692cf0000000114a0fae392a169ce0c1d72b8@reply.github.com>,\n <https://github.com/notifications/unsubscribe/AELU4psxoyp94KJbScZCTduwXn8XZxzIks5rV97jgaJpZM4LuJIa>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: greg@biomassiv.es\n\n\n----==_mimepart_588938e373b85_53583fc9a1ab1134268282\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nCan one of the admins verify this patch?\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/7080#issuecomment-275268917\n----==_mimepart_588938e373b85_53583fc9a1ab1134268282\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>Can one of the admins verify this patch?</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/7080#issuecomment-275268917\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/AELU4hnImiIkLNXkfA8bOK2B7cY5ld62ks5rV97jgaJpZM4LuJIa\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/AELU4mm33GhzlVdgDunBX9SO4UPvV7CTks5rV97jgaJpZM4LuJIa.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/7080#issuecomment-275268917\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@tensorflow-jenkins in #7080: Can one of the admins verify this patch?\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/7080#issuecomment-275268917\"}}}</script>\n----==_mimepart_588938e373b85_53583fc9a1ab1134268282--\n"]}, {"number": 7079, "title": "CudaRoot() returns the configured CUDA toolkit path.", "body": "Fixes the problem of XLA being unable to find libdevice files if not executed through bazel. For example,\r\nhttp://stackoverflow.com/questions/41729019/notfounderror-running-tensorflow-xla-example-libdevice-compute-35-10-bc/41800414#41800414", "comments": ["Can one of the admins verify this patch?", "The checks are still in the process of \"Waiting for status to be reported\". Any actions on my side to get them run and finish? ", "Jenkins, test this please.", "Are the tests stuck here?", "@martinwicke, can @hawkinsp trigger tests? @caisq to trigger tests", "@tensorflow-jenkins test this please", "It looks like this breaks the build. @wujingyue can you take another look?", "Yes. I'm looking at those failures. ", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@wujingyue it looks like the Android and Windows builds are still failing to find \"cuda/cuda_config.h\".\r\n\r\nERROR: /workspace/tensorflow/core/BUILD:798:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed: process-wrapper failed: error executing command /var/lib/jenkins/workspace/tensorflow-pull-requests-android/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/_bin/process-wrapper -1 5 - - ... (remaining 71 argument(s) skipped).\r\ntensorflow/core/platform/default/cuda_libdevice_path.cc:20:30: fatal error: cuda/cuda_config.h: No such file or directory\r\n #include \"cuda/cuda_config.h\"\r\n                              ^\r\ncompilation terminated.", "I re-spun this PR internally, because I can run tests more easily over there. \r\n\r\nIt has now landed as https://github.com/tensorflow/tensorflow/commit/c1238db44e3344b661ae3e6aa17ff74898569b11. So closing this PR. "]}, {"number": 7078, "title": "Replace all gate_gradients=1 with gate_gradients=GATE_OP in documenta\u2026", "body": "Replace all \"gate_gradients=1\" in documentation with \"gate_gradients=GATE_OP\" to make \r\nit very clear that GATE_OP is the default value. This matches what exists in the source code as well. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Is there something that I need to do to run the required checks above, or will they eventually run automatically?", "@slacy I'm not sure there is a way to preserve \"GATE_OP\" when auto-generating the docs. Assigning @xmbrst for advice.", "All the docs changed in the 1.0 release -- going to close this -- it's possible this got fixed with the new doc generators.  In any case, editing the .md files wouldn't be the right thing to do anyway :("]}, {"number": 7077, "title": "TensorBoard ImportError: No module named werkzeug", "body": "using the current HEAD of tensorflow, I have bumped into an issue when I execute tensorboard.\r\n\r\nVersion is reported as: 0.12.head\r\ngit rev-parse HEAD: a12c7dc3d83049e10c1dca8903d73cc71d3cb7b2\r\nLinux: Linux tensor 4.4.0-53-generic #74-Ubuntu SMP Fri Dec 2 15:59:10 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n> 2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\r\n2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\r\n2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\r\n2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 9, in <module>\r\n    load_entry_point('tensorflow', 'console_scripts', 'tensorboard')()\r\n  File \"/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 542, in load_entry_point\r\n    return get_distribution(dist).load_entry_point(group, name)\r\n  File \"/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2575, in load_entry_point\r\n    return ep.load()\r\n  File \"/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2235, in load\r\n    return self.resolve()\r\n  File \"/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2241, in resolve\r\n    module = __import__(self.module_name, fromlist=['__name__'], level=0)\r\n  File \"/home/greg/tensorflow/_python_build/tensorflow/tensorboard/tensorboard.py\", line 26, in <module>\r\n    from werkzeug import serving\r\nImportError: No module named werkzeug\r\n\r\n\r\nI see the werkzeug.BUILD file on my system so not sure why it cannot be found.\r\n", "comments": ["Just to be clear, you ran configure, built the whl file, installed, and running from there?\r\n\r\n/cc @dandelionmane ", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L38 also lists werkzeug, so it's not clear why pip installing the package would not install werkzeug too.", "Sorry,\r\n\r\nI ran configure and use TensorFlow from source, but don't build the whl, I am setup for \"Setting up TensorFlow for Development\".  So I use the following:\r\n\r\n    # To build with GPU support:\r\n    bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n    mkdir _python_build\r\n    cd _python_build \r\n    ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .\r\n    ln -s ../tensorflow/tools/pip_package/* .\r\n    python setup.py develop", "I am not super familiar with that style of development: I don't know why calling the setup.py with 'develop' mode doesn't install the required packages.\r\n\r\nYou should be able to get around this by `pip install werkzeug` manually.", "I can try the PIP but a little concerned it might cause problems later as the PIP should be bundled with the development install.\r\n\r\nHere is the link to the \"setting up with TensorFlow for Development\":\r\nhttps://www.tensorflow.org/get_started/os_setup#setting_up_tensorflow_for_development\r\n\r\nEDIT: That did fix the error but unsure why we need to manually install via \"pip\"", "If you are concerned about polluting your environment, you should run virtualenv. We need `werkzeug` to run.", "I understand, but wanted to report the break in the current HEAD development build.  This is the first include I have needed to PIP install that appears it should have been installed with bazel.  My only concern is I will not be able to detect when it is corrected in the future.", "Right, we are thinking that `setup.py` should have installed it, but for some reason didn't. Is it working for you now?", "Yes, I will remove the PIP package once you guys determine how to move forward as you could add the package as a prerequisite.", "Yes, it looks like it's a documentation issue. According to [stackoverflow](http://stackoverflow.com/questions/28509965/setuptools-development-requirements), the `develop` mode doesn't install the requirements. You have to install them separately. Closing a looping internally to update the doc.", "I also encountered the same problem, but I found the werkzeug package. But still did not solve the problem", "I have the same problem with TwistedW ,and still did not solve,too.", "I have the same problem and pip install does not solve it. ", "This is strictly a TB dependency.\r\nYou should reach out to tensorboard to look into fixing this dependency in tensorboard pip package."]}, {"number": 7076, "title": "[CMake] Fix the Python build with -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF.", "body": "The SWIG wrapper could not resolve the server-creation function, which\r\nis unconditionally referenced in server_lib.i, but not built when gRPC\r\nsupport is disabled. This change fixes the problem by building\r\nserver_lib.cc as part of tf_core_cpu, so it can always be resolved,\r\neven when gRPC support is disabled.\r\n\r\nFixes #7019. Thanks to @iantheconway for finding the bug.", "comments": []}, {"number": 7075, "title": "Generate docs for release.", "body": "", "comments": []}, {"number": 7074, "title": "Fix debian build.", "body": "Change: 144738033", "comments": []}, {"number": 7073, "title": "FreeBSD compatibility", "body": "Two commits\r\n\r\nOne with non breaking changes which should be able to be applied straight away\r\nOne with breaking changes as FreeBSD has libdl integrated into libc (so the -ldl linker option should probably be handled in another way but I have no idea how)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I've signed the CLA.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I've added the e-mail I commited with as alternate e-mail and verified it for my account as described in the contact info section so the CLA should be good now.", "CLAs look good, thanks!\n\n<!-- ok -->", "@blodan also GPU will work with this patch?", "By breaking change, do you mean breaking for existing configurations?\r\nInstead of those, we can conditionally set flags with the help of such macros:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L79", "@hholst80 i have not tried GPU mode, i know there is a binary nvidia driver for freebsd, but i don't know if the cudatoolkit is available on bsd", "@gunan yes, as the \"breaking change\" commit removes the \"-ldl\" link option everywhere compilation would probably break for linux\r\n\r\nHow would i go about writing such a macro for this line for example?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L1171\r\n\r\nIf you could give me an example for that one I can fix it for the rest of the -ldl places.", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L94\r\nHere is how most of our copts should be set. As you see, based on different OS and toolchains, we select different options.\r\nThe config settings that pick what to build are defined here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L47\r\n\r\nWe probably need a new config setting for freebsd.\r\n@damienmg @meteorcloudy How can we setup a new config setting for freeBSD in tensorflow BUILD files?", "OK, after reading a bit more:\r\nonce we add a new config value there, then we will need to set it from the command line when compiling for freeBSD.\r\nSo let's do this:\r\n\r\nin tensorflow/BUILD, we need a new config setting.\r\n```\r\nconfig_setting(\r\n  name = \"freeBSD\",\r\n  values = {\r\n    \"define\": \"os=freeBSD\"\r\n  },\r\n)\r\n```\r\nThen in this file https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L94\r\nWe can conditionally unset the linker flags.\r\n\r\nThen, when running bazel, you will need to add the flag `--define os=freeBSD`. \r\n\r\n@blodan Could you try the above out?", "@gunan @jart I'm trying to get this to work with your pointers but it doesn't pick up my settings I'm trying to implement.\r\n\r\nFirst off I've defined \r\n>config_setting(\r\n>    name = \"FreeBSD\",\r\n>    values = {\"cpu\": \"FreeBSD\"},\r\n>    visibility = [\"//visibility:public\"],\r\n>)\r\n\r\nin tensorflow/BULD\r\n\r\nThen I tried this on line 1153 in tensorflow/core/BUILD:\r\n>linkopts = select({\r\n>        \"//tensorflow:FreeBSD\": [],\r\n>        \"//conditions:default\": [\"-ldl\"],\r\n>})\r\n\r\nHowever, when trying to build it still adds -ldl to the compile options, shouldn't the above \"clear\" the settings for FreeBSD?", "Figured out that \r\n> values = {\"cpu\": \"freebsd\"},\r\n\r\nhad to be in lowercase, just going to build this on a fresh freebsd system to see that it actually works before commiting the changes.", "Okay so I've commited the changes now and this should be good to go if you don't want me to do any other changes @gunan @jart ", "@tensorflow-jenkins test this please", "Sorry I was OOO. /cc @aehlig who made Bazel FreeBSD port.\r\n\r\nI had only one comment but I see that @jart already gave the same :)", "@tensorflow-jenkins test this please", "@blodan it looks like the build files are still not quite there. Can you try to fix the problem?", "@rmlarsen from reading the output of jenkins it seems there was a failure during download/fetching some kind of repository? I don't think thats related to what I've changed.\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/2936/console\r\n\r\nOr am I looking at the wrong output?", "Jenkins, test this please.", "https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/2967/console\r\n\r\nIt failed again.\r\nBazel is not great at rreporting errors, but this usually means there is an error in BUILD of .bzl files.\r\nCould you test it out locally (or on docker)? Simply running `configure` should trigger the issue.", "Hi, I am interested to know what generate that stacktrace, Bazel should definitely never print a stack trace (means a bug to us)", "Which version of Bazel TensorFlow CI is using? 0.4.3? I  cannot repro the stack trace. However the error message I get is not really helpful either, it tries to load @local_config_cuda//cuda:cudart_static but cannot find @local_config_cuda//cuda build file. Looking at `$(bazel help output_base)/external/local_config_cuda/cuda` I can find a build file...", "Ok the problem is that it is looking for `@local_config_cuda//tensorflow:FreeBSD` which does not exists. The error message I get with Bazel 0.4.4 is definitely misleading:\r\n\r\n```\r\n.........\r\nERROR: /usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/e5cce820cc082410b4fcc604db349066/external/local_config_cuda/cuda/BUILD:48:1: no such package '@local_config_cuda//tensorflow': BUILD file not found on package path and referenced by '@local_config_cuda//cuda:cudart_static'\r\nERROR: /usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/e5cce820cc082410b4fcc604db349066/external/local_config_cuda/cuda/BUILD:48:1: no such package '@local_config_cuda//tensorflow': BUILD file not found on package path and referenced by '@local_config_cuda//cuda:cudart_static'\r\nERROR: Evaluation of query \"deps((//tensorflow/... - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure\r\n```\r\n\r\nThe stacktrace is probably coming from one of those case we fixed in recent release.", "Thanks for the more descriptive error @damienmg, that saved me a lot of time.\r\n\r\nI've now changed to a local config for the cuda BUILD.tpl as you seem to have done with darwin previously, it should build fine in jenkins now.", "@tensorflow-jenkins test this please", "@blodan it appears there are still formatting errors in the build files : \r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n1310,1313c1310,1313\r\n<             \"//tensorflow:FreeBSD\": [],\r\n<             \"//conditions:default\": [\"-ldl\"],\r\n<         }) + [\r\n<         \"-lm\"\r\n---\r\n>         \"//tensorflow:FreeBSD\": [],\r\n>         \"//conditions:default\": [\"-ldl\"],\r\n>     }) + [\r\n>         \"-lm\",\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n", "I've now removed one indentation as specified, hopefully it goes through now! :)", "Jenkins, test this please.", "I've now changed the config option to lowercase and fixed the last buildifer warning", "Jenkins, test this please.", "Looks like all tests are passing.\r\n@jart @damienmg could you take another look?", "@jart does this all look good to you?\r\n\r\n@tensorflow-jenkins test this please"]}, {"number": 7072, "title": "[Windows] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nOnly one, but it was not solved.\r\n### Environment info\r\nOperating System:\r\nWindows 10 (anaconda 4.3.8)\r\n`conda --version conda 4.3.8`\r\nInstalled version of CUDA and cuDNN: \r\n`nvcc --version nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Sat_Sep__3_19:05:48_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.44`\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n`pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n`I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally`\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nWhen I tried  [Single GPU computing example with tensorflow](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_MultiGPU/multigpu_basics.py) and get the following error:\r\n`Placeholder_1: (Placeholder): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] Placeholder_1: (Placeholder)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_10: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_10: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_11: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_11: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_12: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_12: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_13: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_13: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_14: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_14: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_15: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_15: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_16: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_16: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_17: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_17: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_18: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_18: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_19: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_19: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_2: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_2: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_3: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_3: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_4: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_4: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_5: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_5: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_6: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_6: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_7: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_7: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_8: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_8: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul_9: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul_9: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nAddN: (AddN): /job:localhost/replica:0/task:0/cpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] AddN: (AddN)/job:localhost/replica:0/task:0/cpu:0\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support`\r\n\r\n### What other attempted solutions have you tried?\r\nThen I tried a sample matrix multiplication:\r\n`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: Quadro M2000M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.137\r\npciBusID 0000:01:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.35GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\direct_session.cc:255] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0`\r\n\r\nand the following error:\r\n\r\n`a = tf.random_normal((100,100))`\r\n`b = tf.random_normal((100,500))`\r\n`c = tf.matmul(a,b)`\r\n`sess.run(c)`\r\n`random_normal_1/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal_1/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal_1/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal_1/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal_1: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal_1: (Add)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal: (Add)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal_1/stddev: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal_1/stddev: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal_1/mean: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal_1/mean: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal_1/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal_1/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal/stddev: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/stddev: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal/mean: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/mean: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support`\r\n`\r\n\r\nLike @kingtaurus and @menggangmark,\r\n\r\nI then copied the cudnn64_5.dll (cuda\\bin\\cudnn64_5.dll) from that zip archive into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\\;\r\n\r\ncuda\\include\\cudnn.h to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\include\\;\r\n\r\nand\r\n\r\ncuda\\lib\\x64\\cudnn.lib to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64\\\r\n\r\ncupti64_80.dll (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64) to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin;\r\nand cupti.lib(C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64) to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64\r\n\r\nWHERE C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0 is my install PATH for the CUDA toolkit.\r\nI had already added C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\\ to my PATH\r\n\r\n", "comments": ["Could you try with 0.12.1? There might have been some issues with 0.12rc.", "@drpngx  Thank you! Now the script works fine~", "Yay!", "I'm still getting this issue, has this actually been resolved?\r\n\r\nFor reference:\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\n``` \r\nproduces\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\n1.0.0\r\n```\r\n\r\nand \r\n```\r\nnvcc --version\r\n```\r\nproduces\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Mon_Jan__9_17:32:33_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.60\r\n```\r\n\r\nand finally the code I try is from the TF intro on the website:\r\n\r\n```\r\nimport tensorflow as tf\r\n# NumPy is often used to load, manipulate and preprocess data.\r\nimport numpy as np\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n#tf.Session(config=config)\r\n\r\n# Declare list of features. We only have one real-valued feature. There are many\r\n# other types of columns that are more complicated and useful.\r\nfeatures = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\r\n\r\n# An estimator is the front end to invoke training (fitting) and evaluation\r\n# (inference). There are many predefined types like linear regression,\r\n# logistic regression, linear classification, logistic classification, and\r\n# many neural network classifiers and regressors. The following code\r\n# provides an estimator that does linear regression.\r\nestimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\r\n\r\n# TensorFlow provides many helper methods to read and set up data sets.\r\n# Here we use `numpy_input_fn`. We have to tell the function how many batches\r\n# of data (num_epochs) we want and how big each batch should be.\r\nx = np.array([1., 2., 3., 4.])\r\ny = np.array([0., -1., -2., -3.])\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4,\r\n                                              num_epochs=1000)\r\n\r\n# We can invoke 1000 training steps by invoking the `fit` method and passing the\r\n# training data set.\r\nestimator.fit(input_fn=input_fn, steps=100)\r\n\r\n# Here we evaluate how well our model did. In a real example, we would want\r\n# to use a separate validation and testing data set to avoid overfitting.\r\nestimator.evaluate(input_fn=input_fn)\r\n```\r\n\r\nwhich produces the following output (error included):\r\n\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\WILLIA~1\\AppData\\Local\\Temp\\tmprwftardv\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 960\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.1775\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.64GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"D:\\Program Files\\Python35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(4, 1), b.shape=(1, 1), m=4, n=1, k=1\r\n         [[Node: linear/linear/x/matmul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](linear/linear/x/expand_dims, linear/x/weight)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".\\linear_contrib_learn.py\", line 30, in <module>\r\n    estimator.fit(input_fn=input_fn, steps=100)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 426, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 984, in _train_model\r\n    _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 462, in run\r\n    run_metadata=run_metadata)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 786, in run\r\n    run_metadata=run_metadata)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 744, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 891, in run\r\n    run_metadata=run_metadata)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 744, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(4, 1), b.shape=(1, 1), m=4, n=1, k=1\r\n         [[Node: linear/linear/x/matmul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](linear/linear/x/expand_dims, linear/x/weight)]]\r\n\r\nCaused by op 'linear/linear/x/matmul', defined at:\r\n  File \".\\linear_contrib_learn.py\", line 30, in <module>\r\n    estimator.fit(input_fn=input_fn, steps=100)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 426, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 934, in _train_model\r\n    model_fn_ops = self._call_legacy_get_train_ops(features, labels)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1003, in _call_legacy_get_train_ops\r\n    train_ops = self._get_train_ops(features, labels)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1162, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\linear.py\", line 166, in _linear_model_fn\r\n    scope=scope))\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 557, in weighted_sum_from_feature_columns\r\n    predictions = math_ops.matmul(tensor, variable[0], name='matmul')\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1855, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1454, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(4, 1), b.shape=(1, 1), m=4, n=1, k=1\r\n         [[Node: linear/linear/x/matmul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](linear/linear/x/expand_dims, linear/x/weight)]]\r\n```\r\n", "That seems like another error.\r\n\r\n@mrry is BLAS working well under windows?", "The tests for `tf.matmul()` pass on Windows, so I don't think this is a Windows-specific issue: perhaps the following lines are a clue to the root cause?\r\n\r\n```\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n```", "Any updates? @drpngx @finger563 ", "I think @mrry found the solution: you are running out of memory.", "I run into a similar problem. The task manager shows 60+% free memory (main memory), gpu has 6.4 GiB free memory when start. Got  CUBLAS_STATUS_ALLOC_FAILED. Is there a way to free the GPU memory?", "@mingrutar - use this instead:\r\ntf.Session(config=tf.ConfigProto(allow_growth=True))", "@mingrutar - Correction:\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)", "@shingte 's solution worked for me", "@shingte worked for me, too. thanks a lot.", "Hello, I just solved the problem coping my code from the E: drive and running it in C:. There seem to be problems to find the path. I hope it helps.", "Hello, the solution by @shingte doesn't solves this problem for me. Any more ideas on what to do?\r\n\r\nI am seeing\r\n2017-12-06 23:54:30.748791: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2017-12-06 23:54:30.751041: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2017-12-06 23:54:30.751215: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2017-12-06 23:54:30.751469: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2017-12-06 23:54:30.751658: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2017-12-06 23:54:30.751823: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2017-12-06 23:54:31.218032: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2017-12-06 23:54:31.218110: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:389] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2017-12-06 23:54:31.218691: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-12-06 23:54:31.218713: F C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\kernels\\conv_ops.cc:667] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)", "@shrimalmadhur I keep getting the same error. Were you able to solve your problem?", "I had this problem with a program that uses anaconda, keras, tensorflow. I tackled the problem by creating a new anaconda environment with keras and tensorflow. I don't know if this was related but I quickly ran into a bad numpy library (wrong hash). I uninstalled, removed numpy and theano. Reinstalled keras and tensorflow. pip updated them. Everything worked. Went back to the original environment and made sure the latest numpy was installed there. Everything worked. Wish I could be more specific. I wonder if this was all caused by a bad numpy package?", "I deleted everything and installed them again. Then, I used the code above, and it worked.", "Good to hear!", "I had a similar issue, what worked for me was freeing up hard disk space which was apparently getting full.", "I was running a game in the background.  Closing that cleared up the error.", "@shingte worked for me, also. thanks !!!", "At first, none of the above worked for me -- then I simply restarted my computer, and stopped getting the error (at least, I haven't gotten it yet). Earlier, I had force-killed a bunch of TF processes -- I'm wondering if GPU memory was not freed? TF always prints\r\n```\r\ntotalMemory: 4.00GiB freeMemory: 3.31GiB\r\n```\r\nat the start of each run, but could this have been wrong, or maybe it's referring to something else? I really don't know. Anyway, glad it's working for now.", "it worked for me. could you tell me how it works?", "If you're using PyCharm, don't enable `tf.enable_eager_execution()` in Python Console while testing your other .py file at the same time.", "had the same issue as @shrimalmadhur ( failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED)\r\nFixed it by following @BSalita 's hint, that there might be some issue with the numpy version. just updated the numpy package in my anaconda environment ( activate the environment => **pip install --upgrade numpy** ) \r\nThanks a lot!", "@juandarango This actually worked for me", "if you are using keras, just put this code before loading the model\r\n```\r\nimport tensorflow as tf\r\nfrom keras.backend.tensorflow_backend import set_session\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n                                    # (nothing gets printed in Jupyter, only if you run it standalone)\r\nsess = tf.Session(config=config)\r\nset_session(sess)  # set this TensorFlow session as the default session for Keras\r\n```", "> if you are using keras, just put this code before loading the model\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from keras.backend.tensorflow_backend import set_session\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\n> config.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n>                                     # (nothing gets printed in Jupyter, only if you run it standalone)\r\n> sess = tf.Session(config=config)\r\n> set_session(sess)  # set this TensorFlow session as the default session for Keras\r\n> ```\r\n\r\nHooo!! nice..this worked \r\nbut need to do this all the time prior to loading the code", "> if you are using keras, just put this code before loading the model\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from keras.backend.tensorflow_backend import set_session\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\n> config.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n>                                     # (nothing gets printed in Jupyter, only if you run it standalone)\r\n> sess = tf.Session(config=config)\r\n> set_session(sess)  # set this TensorFlow session as the default session for Keras\r\n> ```\r\n\r\nFinally, this solution worked for me, thanks", "Following as @gussmith23 suggested, a simple reboot did the trick for me. ", "hello there the error is still there with gtx 1650 also\r\n", "May be the version of tensorflow_gpu is incorrect.", "> Hello, I just solved the problem coping my code from the E: drive and running it in C:. There seem to be problems to find the path. I hope it helps.\r\n\r\nI solved my problem with running the program as an administrator", "I've had this too sometimes, and in my case it was related to GPU memory being used by another process. For instance, a game, NVidia RTX Voice, or just a pycharm debugging session that hadn't stopped cleanly to free the memory.\r\n\r\nYou can see the memory status of your nvidia gpu with the command `nvidia-smi` in a command prompt.\r\n\r\nRestarting your pc will help, closing the other program, or restarting pycharm.", "> if you are using keras, just put this code before loading the model\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from keras.backend.tensorflow_backend import set_session\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\n> config.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n>                                     # (nothing gets printed in Jupyter, only if you run it standalone)\r\n> sess = tf.Session(config=config)\r\n> set_session(sess)  # set this TensorFlow session as the default session for Keras\r\n> ```\r\n\r\nIf you are using TensorFlow 2 then change it to\r\n```python\r\nfrom tensorflow.compat.v1.keras.backend import set_session\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n                                    # (nothing gets printed in Jupyter, only if you run it standalone)\r\nsess = tf.compat.v1.Session(config=config)\r\nset_session(sess)  # set this TensorFlow session as the default session for Keras\r\n```", "> \r\n> \r\n> @mingrutar - Correction:\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = tf.Session(config=config)\r\n\r\nDidint work for tensorflow 2.0\r\n\r\nFound a different solution\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\r\n```", "> @mingrutar - Correction:\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = tf.Session(config=config)\r\n\r\nwhere should I enter that ?"]}, {"number": 7071, "title": "improved and fix bug of getting next batch in mnist", "body": "The original codes do not consider the situation that when the batch_size > num_examples (even though it is not a common event, if happens, no new data will be input at all).\r\n\r\nActually, a fully epoch means that all examples have been applied once, however, the original codes do not show it. If the batch_size sets just a little bit larger than half of the example number, nearly half of the examples cannot be used in one epoch.\r\n\r\nAlso, I set the shuffle to be a input variable.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@rmlarsen I've already normalized the format, thank you so much and pls check the new pr(43e7a40) again.", "@tensorflow-jenkins Follow by the agsignee's instruction, I've change some format of the code. Please test again . Thank you.", "@tensorflow-jenkins test this please", "Thanks for the update. I'll merge this once the  tests pass.", "@rmlarsen the jenkins test shows that Windows Cmake Tests failure, but I do not fully understand the reason from the output console.", "The cmake error is unrelated. Thanks for the contribution. I'll merge it now."]}, {"number": 7070, "title": "Fix breakages in Python 3.5 tests by using int type in indices (#7003)", "body": "Need to fix py3.5 tests in release :)", "comments": ["This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  acmeideal@gmail.com\n    Domain biomassiv.es has exceeded the max emails per hour (112/100 (112%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext2.iad.github.net ([192.30.252.193]:48604 helo=github-smtp2b-ext-cp1-prd.iad.github.net)\n\tby chi-server32.websitehostserver.net with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.87)\n\t(envelope-from <noreply@github.com>)\n\tid 1cWmiQ-0032ar-WB\n\tfor greg@biomassiv.es; Thu, 26 Jan 2017 10:16:03 -0600\nDate: Wed, 25 Jan 2017 11:08:48 -0800\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1485371328;\n\tbh=T/s+EFdVQPxpD01Y/8MnPWbFfmM0pp81+5C50eKTYj4=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=f1JHi+wV8u2WNAGeLPL9BlCohI4B+LRxvDzXtVHHqsrXS0vpV+hJtomkLgg5NbeB5\n\t srzbEPgjQKLdishEfo62S++e0+7CUvI3L7YQhJ7fnc5Xhfau1KX/v1pIX6f6mG+hQF\n\t 0UmexmiRsxZdVjo3w1zMa9v0H01XWq+S12YAPQmM=\nFrom: Shanqing Cai <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/7070/review/18477296@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/7070@github.com>\nReferences: <tensorflow/tensorflow/pull/7070@github.com>\nSubject: Re: [tensorflow/tensorflow] Fix breakages in Python 3.5 tests by\n using int type in indices (#7003) (#7070)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5888f7c05431e_31913fc73a4a71302020a6\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: caisq\nX-GitHub-Recipient: biomassives\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0042d4e20f5856e20b5873f7dd6093eed6dcaef62f8938bd92cf0000000114a0b9c092a169ce0c1c7ab3@reply.github.com>,\n <https://github.com/notifications/unsubscribe/AELU4m0ibieIbE5f9EUmDAkCzmQRphZfks5rV53AgaJpZM4Lt3xH>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: greg@biomassiv.es\n\n\n----==_mimepart_5888f7c05431e_31913fc73a4a71302020a6\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\ncaisq approved this pull request.\n\n\n\n\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/7070#pullrequestreview-18477296\n----==_mimepart_5888f7c05431e_31913fc73a4a71302020a6\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><b>@caisq</b> approved this pull request.</p>\n\n\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/7070#pullrequestreview-18477296\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/AELU4tuBIhmgJhHX6_U0mwxT-HNN2326ks5rV53AgaJpZM4Lt3xH\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/AELU4mQwn0qYPg6Rv5F3mQhLV9vw4JpSks5rV53AgaJpZM4Lt3xH.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/7070#pullrequestreview-18477296\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@caisq approved #7070\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/7070#pullrequestreview-18477296\"}}}</script>\n----==_mimepart_5888f7c05431e_31913fc73a4a71302020a6--\n"]}, {"number": 7069, "title": "error in installing tf from source", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I installed tf from source and I got following error when I wanna import it: \r\n\r\n> Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name pywrap_tensorflow\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n<\r\n\r\nI am installing tf 0.12 in ubuntu 14 with python2.7 with Cuda. I try to follow the same documentation in  https://www.tensorflow.org/get_started/os_setup. \r\nI appreciate any help.  Thanks", "Just to be clear, you built with cuda and built the pip and installed it on your machine?", "Yes, I follow instruction for installing from source from https://www.tensorflow.org/get_started/os_setup\r\nI made a wheel and then I install it by pip.\r\nIt seems some other guys have same problem here https://github.com/tensorflow/tensorflow/issues/2746. \r\nI am reading it if i can use their solution. \r\nThanks", "Unfortunately, it didn't help me. I uninstall six and install it again, but still i have same problem.  Any suggestion? Thanks", "It looks like it can't load the pywrap.\r\n\r\nCould you run this?\r\n\r\n```\r\nldd /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n```", "I found the solution! When I am running python inside of tensorflow directory I got that error. But if I run python from any other directory and then import tensorflow, everything is working fine. I am not sure uninstalling and installing six affect it. I am closing this issue.\r\nThanks. ", "Oh, ok, you should have seen a message about that.\r\n", "Hi, \r\nI think I have a similar problem. \r\n@amortazi how do you run python from a directory other than tensorflow ? \r\nThanks ! ", "@oumayb \r\nI think I have a similiar problem, too.\r\nBut I try to run `cd ~ && python`,  then everything is working well.", "The runnign directory is just a \"chance\" thing, this is not really a solution for those whose import is having actual problems. Here's mine in OSX El Capitan:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name 'pywrap_tensorflow'\r\n```", "It's probably missing some libraries. You could try\r\n```\r\nldd /System/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/*.so\r\n```\r\nto figure out which it might be."]}, {"number": 7068, "title": "Feature Request - support distributed Tensorflow on Sun Grid Engine job scheduler", "body": "Sun Grid Engine is commonly used to schedule jobs on scientific computing clusters / supercomputers.  Please consider adding support for distributed Tensorflow on SGE.", "comments": ["Thanks @stevendavis ! @jhseu FYI.", "Sorry, I don't think we'll be able to support this. If you end up with a solution, we'd be happy to link to it from tensorflow.org.", "@stevendavis We managed to run some models on the SGE, but it's tricky because you always run into walltimes and have to somehow handle them properly. Additionally the creation of the ClusterSpecs is not really trivial, as you submit to queues and not to hosts. But by making some tweaks you can actually do it.\r\n\r\nI didn't have too much time to make some proper write up, but you can skim [https://github.com/shoeffner/ann3depth](https://github.com/shoeffner/ann3depth) for how we made it. Since it was a project for a university seminar, there's also a small [project report](https://github.com/shoeffner/ann3depth/tree/master/docs/documentation.md) which explains some of the code we used in [tools/grid](https://github.com/shoeffner/ann3depth/tree/master/tools/grid). Most of it is not really well designed, some scripts take input via stdin, others via arguments etc., but it should give you some ideas.\r\n\r\nIn short these are the important ideas:\r\n\r\n- Open ports in your grid system. We had 5001 - 5007 available, you potentially just need one. Ask your admins about it.\r\n- One \"keepalive\" job which first checks what machines are available and distributes the workload to submit to hosts (This can be heavily improved but was enough for our proof of concept). The keepalive job is submitted by you, it will submit all jobs for worker and ps nodes.\r\n- The keepalive job checks if the jobs are still running and if not, resubmits the whole lot (this turned out to be easier than submitting individual jobs). It also restarts the whole thing before it hits its own walltime.\r\n- With the `-notify` flag you can let the SGE send USR1/USR2 signals to your jobs before they get killed. This gives you a window to store a checkpoint right before the walltime is reached. Unfortunately, as in our setup, the SGE can be configured to not send those signals. Then your best bet is to use ALRM signals which erupt before the walltimes are hit. We used a hook ([src/tfhelper.py#L160-L189](https://github.com/shoeffner/ann3depth/blob/1789366bbc7cd97bd70839bc5cff2db1e13e2a5d/src/tfhelper.py#L160-L189)) to handle this.\r\n- Save checkpoints every now and then, your jobs can fail to various reasons (sometimes shells were not found, sometimes some higher priority jobs can kill your job, even people running processes locally on machines without using the SGE resource manage can occur quite often and are troublesome).\r\n\r\nWhat you have to keep in mind is that the SGE is there to distribute computation time on specialized systems among several users for a fair sharing of resources. TensorFlow is not really designed for this use case, it suits designs using containerization where you can assume (almost) infinite computation power and resources. This makes things very tricky.", "@shoeffner Your scripts assume that there are enough available nodes in the SGE cluster, right? So if there are not enough, I guess it would break? Have you thought about queuing it all together, so that when it gets scheduled, you will get all the requested amount of nodes and resources? I'm currently looking into this. I think this probably can be done via the SGE parallel environment mechanism (see manpage sge_pe). I have seen OpenMPI setups using this `-pe` option.", "My scripts even rely on the exact hosts being available, so the situation gets worse.\r\nI though about job/task arrays, but it is fairly difficult to have distinguishable tasks in a way that the keepalive, as I was using it, can figure that out.\r\nAn alternative approach, which I did not implement, might be better suited for that: submit to random machines, let them register to a (known) chief machine and build the cluster from there.\r\n\r\nI am looking forward to seeing your solutions.\r\n\r\nNB: Don't use the pe option like that. It is there to access single machines with multiple CPUs, which might be useful for tensorflow, but is not necessarily needed. But that's why OpenMPI projects use it, they rely on multiple CPU cores. Instead, have a look at job arrays. ", "Here is a blog post that was recently done by Univa that outlines an example approach to integrating distributed TensorFlow to a Grid Engine Parallel Environment.  Hope this helps. Feedback and questions very welcome.\r\n\r\nhttps://blogs.univa.com/2018/04/integrating-distributed-tensorflow-with-grid-engine/\r\n\r\n"]}, {"number": 7067, "title": "Recurrent Spatial Transformer Network", "body": "i'm trying to replicate Recurrent Spatial Transformer Network implemented here (https://github.com/skaae/recurrent-spatial-transformer-code) , however the loss didn't decrease at all .\r\n\r\n\r\nthe configuration of the network is as follow:\r\n1 -  relu activations .\r\n\r\n2 - xavier weight initialization for weights , zero initialization for biases .\r\n\r\n3 - cost function is softmax_cross_entropy_with_logits .\r\n\r\n4 - optimizer is RMSProp (i tried 1e-6 ;1e-10 espilon) .\r\n\r\n5 - gradient clipping by value .\r\n\r\nso what should i try next ?", "comments": ["import tensorflow as tf\r\nfrom spatial_transformer import transformer\r\nfrom tensorflow.python.ops import rnn,rnn_cell\r\nimport numpy as np\r\nfrom tf_utils import weight_variable, bias_variable,  dense_to_one_hot\r\n\r\n# %% load data\r\nmnist_cluttered = np.load('data/mnist_sequence3_sample_8distortions_9x9.npz')\r\n\r\nX_train = mnist_cluttered['X_train']\r\ny_train = mnist_cluttered['y_train']\r\nX_valid = mnist_cluttered['X_valid']\r\ny_valid = mnist_cluttered['y_valid']\r\nX_test = mnist_cluttered['X_test']\r\ny_test = mnist_cluttered['y_test']\r\n\r\ny_train = np.reshape(y_train,[y_train.size,1])\r\ny_valid = np.reshape(y_valid,[y_valid.size,1])\r\ny_test = np.reshape(y_test,[y_test.size,1])\r\n\r\n# % turn from dense to one hot representation\r\nY_train = dense_to_one_hot(y_train, n_classes=10)\r\nY_valid = dense_to_one_hot(y_valid, n_classes=10)\r\nY_test = dense_to_one_hot(y_test, n_classes=10)\r\n\r\n\r\nY_train = np.reshape(Y_train,[y_train.size/3,3,10])\r\nY_valid = np.reshape(Y_valid,[y_valid.size/3,3,10])\r\nY_test = np.reshape(Y_test,[y_test.size/3,3,10])\r\n\r\n# %% Placeholders for 100x100 resolution\r\nx = tf.placeholder(tf.float32, [None, 10000])\r\ny = tf.placeholder(tf.float32, [None,3, 10])\r\n\r\n\r\nx_tensor = tf.reshape(x, [-1, 100, 100, 1])\r\n\r\ny_tensor = tf.reshape(y,[-1 ,10])\r\n\r\n#%% localizaton network\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\n\r\nl_pool0_loc = tf.nn.max_pool(x_tensor,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\r\n\r\nW_conv0_loc = weight_variable([3,3,1,20],'W_conv0_loc')\r\n\r\nb_conv0_loc = bias_variable([20],'b_conv0_loc')\r\n\r\nl_conv0_loc = tf.nn.relu(tf.nn.conv2d(l_pool0_loc,W_conv0_loc,strides=[1,1,1,1],padding='VALID')+b_conv0_loc)\r\n\r\nl_pool1_loc = tf.nn.max_pool(l_conv0_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')\r\n\r\nW_conv1_loc = weight_variable([3,3,20,20],'W_conv1_loc')   \r\n\r\nb_conv1_loc = bias_variable([20],'b_conv1_loc')\r\n       \r\nl_conv1_loc =  tf.nn.relu(tf.nn.conv2d(l_pool1_loc,W_conv1_loc,strides=[1,1,1,1],padding='VALID')+b_conv1_loc)\r\n\r\nl_pool2_loc = tf.nn.max_pool(l_conv1_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')\r\n\r\nW_conv2_loc = weight_variable([3,3,20,20],'W_conv2_loc')\r\n\r\nb_conv2_loc = bias_variable([20],'b_conv2_loc')\r\n\r\nl_conv2_loc = tf.nn.relu(tf.nn.conv2d(l_pool2_loc,W_conv2_loc,strides=[1,1,1,1],padding='VALID')+b_conv2_loc )\r\n\r\nl_conv2_loc = tf.reshape(l_conv2_loc,[-1 ,9*9*20 ])\r\n\r\n# Replicate input for Gated Recurrent Unit\r\nl_conv2_loc = tf.tile(l_conv2_loc,[1,3])\r\n\r\nl_conv2_loc = tf.split(1,3,l_conv2_loc)\r\n\r\n# Gated Recurrent Unit\r\n\r\ngru_cell = rnn_cell.GRUCell(num_units=256)\r\n\r\noutput, state = rnn.rnn(gru_cell,inputs=l_conv2_loc,dtype=tf.float32)\r\n\r\noutput = tf.reshape(output,[-1,256])\r\n\r\ninitial = tf.zeros([256,6]) \r\n\r\n\r\nW_fc1_loc = tf.Variable(initial_value=initial,name='W_fc1_loc')\r\n\r\n# Use identity transformation as starting point\r\ninitial = np.array([[1., 0, 0], [0, 1., 0]])\r\ninitial = initial.astype('float32')\r\ninitial = initial.flatten()\r\nb_fc1_loc = tf.Variable(initial_value=initial,name='b_fc1_loc')\r\n\r\n\r\nl_fc1_loc = tf.add(tf.matmul(output,W_fc1_loc), b_fc1_loc)\r\n\r\n\r\n# %% We'll create a spatial transformer module to identify discriminative patches\r\n\r\ndownsample = 3\r\n\r\nout_size = (100/downsample, 100/downsample)\r\n\r\n\r\nl_transform = transformer(tf.tile(x_tensor,[3,1,1,1]), l_fc1_loc, out_size)\r\n\r\n# %% Classification Network\r\n\r\n\r\nW_conv0_out = weight_variable([3,3,1,32],'W_conv0_out')                   \r\n\r\nb_conv0_out = bias_variable([32],'b_conv0_out')\r\n\r\nl_conv0_out = tf.nn.relu(tf.nn.conv2d(l_transform,W_conv0_out,strides=[1,1,1,1],padding='VALID')+b_conv0_out)\r\n\r\nl_pool1_out = tf.nn.max_pool(l_conv0_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\r\n                   \r\n#l_drp1_out = tf.nn.dropout(l_pool1_out,keep_prob)\r\n\r\nW_conv1_out = weight_variable([3,3,32,32],'W_conv1_out')  \r\n\r\nb_conv1_out = bias_variable([32],'b_conv1_out')\r\n               \r\nl_conv1_out = tf.nn.relu(tf.nn.conv2d(l_pool1_out,W_conv1_out,strides=[1,1,1,1],padding='VALID')+b_conv1_out)\r\n\r\nl_pool2_out = tf.nn.max_pool(l_conv1_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\r\n\r\n#l_drp2_out = tf.nn.dropout(l_pool2_out,keep_prob)\r\n\r\nW_conv2_out = weight_variable([3,3,32,32],'W_conv2_out')     \r\n\r\nb_conv2_out = bias_variable([32],'b_conv2_out')\r\n            \r\nl_conv2_out = tf.nn.relu(tf.nn.conv2d(l_pool2_out,W_conv2_out,strides=[1,1,1,1],padding='VALID')+b_conv2_out)\r\n\r\n\r\n\r\n# %% We'll now reshape so we can connect to a fully-connected layer:\r\nl_conv2_out_flat = tf.reshape(l_conv2_out, [-1, 4*4*32])\r\n\r\n# %% Create a fully-connected layer:\r\nn_fc = 400\r\n\r\nW_fc1 = tf.get_variable('W_fc1',shape=[4*4*32,n_fc],initializer=tf.contrib.layers.xavier_initializer())\r\n\r\n#W_fc1 = weight_variable([4*4*32,n_fc],'W_fc1')\r\n\r\nb_fc1=bias_variable([n_fc],'b_fc1')\r\n\r\n\r\nh_fc1 = tf.nn.relu(tf.add(tf.matmul(l_conv2_out_flat, W_fc1) , b_fc1))\r\n\r\n# %% And finally our softmax layer:\r\n\r\nW_fc2 = tf.get_variable('W_fc2',shape=[n_fc, 10],initializer=tf.contrib.layers.xavier_initializer())\r\n\r\n#W_fc2 = weight_variable([n_fc,10],'W_fc2')\r\n\r\nb_fc2=bias_variable([10],'b_fc2')\r\n\r\ny_logits = tf.add(tf.matmul(h_fc1, W_fc2) , b_fc2)\r\n\r\n\r\n\r\n# %% Monitor accuracy\r\n\r\n\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(y_logits, 1), tf.argmax(y_tensor, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\r\n\r\n\r\n# %% Define loss/eval/training functions\r\ncross_entropy = tf.reduce_mean(\r\n    tf.nn.softmax_cross_entropy_with_logits(y_logits,y_tensor))\r\n\r\nopt = tf.train.RMSPropOptimizer(0.0005,epsilon=1e-6)\r\n\r\n#opt = tf.train.AdagradOptimizer(0.01)\r\n#optimizer = opt.minimize(cross_entropy)\r\n\r\n\r\n\r\n\r\ngvs = opt.compute_gradients(cross_entropy)\r\n\r\ncapped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\r\n\r\noptimizer = opt.apply_gradients(capped_gvs )\r\n\r\n\r\n\r\n\r\n# %% We'll now train in minibatches and report accuracy, loss:\r\n\r\nnum_batches = 600\r\nn_epochs = 300\r\nbatch_size = 100\r\n\r\n\r\nwith tf.Session( ) as sess:\r\n\r\n     sess.run(tf.initialize_all_variables())\r\n    \r\n     for epoch_i in range(n_epochs):\r\n    \r\n    #print ('epoch: ' + str(epoch_i))\r\n         shuffle = np.random.permutation(X_train.shape[0])\r\n         avg_cost = 0.\r\n         for iter_i in range(num_batches - 1):\r\n             idx = shuffle[iter_i*batch_size:(iter_i+1)*batch_size]\r\n             batch_xs = X_train[idx]\r\n             batch_ys = Y_train[idx]\r\n       \r\n    \r\n             _,c=sess.run([optimizer,cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\r\n    \r\n             avg_cost += c / num_batches\r\n             print('iter: ' + str(iter_i) +' >> ' +' MiniBatch Cost: ' +str(c)) \r\n       \r\n     #   gr_print= sess.run([grads for grads,_  in gvs], feed_dict={x : batch_xs, y : batch_ys}) \r\n     #   print ('iter: '+str(iter_i))\r\n     #   for t in gr_print:\r\n      #      print np.linalg.norm(t)\r\n    \r\n       \t \r\n\r\nsaver = tf.train.Saver()\r\n\r\nsaver.save(sess,\"save/my-model\")\r\n\r\n`", "@vincentvanhoucke could you help me with that ?", "Thanks @seragENTp for the report. We are tracking feature improvements and bug reports on github, but for usage issues we believe that stackoverflow is a better venue. We monitor all questions tagged with \"tensorflow\". Please post there and cross-link here."]}, {"number": 7066, "title": "improved and fix bug of getting next batch in mnist", "body": "The original codes do not consider the situation that when the batch_size > num_examples (even though it is not a common event, if happens, no new data will be input at all).\r\n\r\nActually, a fully epoch means that all examples have been applied once, however, the original codes do not show it. If the batch_size sets just a little bit larger than half of the example number, nearly half of the examples cannot be used in one epoch.\r\n\r\nAlso, I set the shuffle to be a input variable.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 7065, "title": "pytorch 2.5x faster on VGG16", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nStarted on SO, and was told to post here ([SO post](http://stackoverflow.com/questions/41832779/tensorflow-2-5x-slower-than-pytorch-on-vgg16-architecture?noredirect=1#comment70901342_41832779))\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04 + Maxwell Titan X\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 8.0, cuDNN 5.1\r\n```python\r\n:~$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root    558720 Jan 25 08:23 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root        16 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root        19 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root    415432 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root    775162 Jan 25 08:23 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 1000 users       13 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 1000 users       17 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxrwxr-x 1 1000 users 79337624 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-rw-r-- 1 1000 users 69756172 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nInstalled from binary pip package :\r\n1. https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl with an Anaconda distribution\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:\r\n````\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.12.1\r\n````\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nUsing the following code to do a forward pass on a pretrained VGG16 :\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\nfrom tensorflow.contrib.slim import nets\r\n\r\ntf.reset_default_graph()\r\n# Use RNG to avoid the feed_dict argument\r\ninput_images = tf.random_uniform((16, 224, 224, 3), maxval=255)  \r\npreds = nets.vgg.vgg_16(input_images, is_training=False)[0]\r\nsaver = tf.train.Saver()\r\n\r\nconfig = tf.ConfigProto(log_device_placement=True)\r\nsess = tf.InteractiveSession(config=config)\r\nsaver.restore(sess, './vgg_16.ckpt')\r\n\r\n# With jupyter notebook magic\r\n%timeit sess.run(preds)\r\n```\r\n\r\nCompared to the pytorch version on the same machine :\r\n```python\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.models as models\r\nfrom torch.autograd import Variable\r\ntorch.backends.cudnn.benchmark = True\r\n\r\nnet = models.vgg16()\r\nnet.cuda()\r\n\r\n_in = Variable(torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda())\r\n\r\n# With jupyter notebook magic\r\n%timeit net(_in)\r\n```\r\n\r\nI get the following results by comparing the frameworks. Surprisingly, there is a small difference with the more complicated resnet-50 while I get a huge gap for the VGG16 architecture which (almost) just uses 3x3 convolutions.\r\n\r\n\r\nModel | TF | pytorch\r\n--- | --- | ---\r\nVGG16 | 160ms | 65ms\r\nresnet-50 | 58ms | 48ms", "comments": ["@sguada do you see anything that got added recently that could address performance gap? (maybe some new fused ops?)", "cc: @vincentvanhoucke in case he knows others working on VGG-like models", "Hi @yaroslavvb.  I am going to reproduce the result and get back to you.  Wanted to let you know we are looking at this issue.  ", "thanks\r\nps: @SeguinBe is the affected party here ", "One drive-by observation: the setup with\r\n\r\n```\r\ninput_images = tf.random_uniform(16, 224, 224, 3), maxval=255)\r\n```\r\n\r\n...might be slow because it's invoking the random number generator **for every batch**. In the PyTorch program, you run the RNG once, outside the timing loop (in the call to `np.random.randn()`), and reuse its results several times.\r\n\r\nThe following might be a fairer comparison:\r\n\r\n```\r\ninput_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))\r\npreds = nets.vgg.vgg_16(input_images, is_training=False)[0]\r\nsess.run(tf.global_variable_initializer())\r\n# ...\r\n```\r\n", "@mrry tried it already, it does not change the timing at all. \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\nfrom tensorflow.contrib.slim import nets\r\n\r\ntf.reset_default_graph()\r\ninput_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))\r\npreds = nets.vgg.vgg_16(input_images, is_training=False)[0]\r\nsaver = tf.train.Saver(var_list=[v for v in tf.global_variables() if 'vgg_16' in v.name])\r\ninit_op = tf.variables_initializer([input_images])\r\n\r\nconfig = tf.ConfigProto(log_device_placement=False)\r\nsess = tf.InteractiveSession(config=config)\r\nsaver.restore(sess, './vgg_16.ckpt')\r\nsess.run(init_op)\r\n```\r\n\r\nOn a side note, the resnet-50 timing in the end is more like 50ms so basically the same as pytorch.", "The tf version is using the slower NHWC data format. Changing it to NCHW will most likely speed things up.", "Can you try?\r\nwith slim.arg_scope([slim.conv2d], data_format='NCHW'):\r\n preds, _ = nets.vgg.vgg_16(input_images, is_training=False)", "```python\r\ninput_images = tf.Variable(tf.random_uniform((16, 3, 224, 224), maxval=255))\r\nwith slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):\r\n    preds, _ = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\r\ninit_op = tf.global_variables_initializer()\r\n\r\nconfig = tf.ConfigProto(log_device_placement=True)\r\nsess = tf.InteractiveSession(config=config)\r\nsess.run(init_op)\r\n```\r\n\r\nUsing `data_format='NHWC'` and size [X, 224, 224, 3] I still get **160ms** and with the `data_format='NCHW'` it is slightly better at **150ms**...\r\n\r\nI have to note that the fc6-fc7 are implemented as convolution in the TF version, will try if modifying them to pure matrix-multiplication changes anything", "So I think that was mainly the solution, the tensorflow definition of the network was using a convolution instead of the fully connected linear matrix multiplication for fc6 fc7 fc8 ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py#L116)). Did not think originally it would be a big problem but to recapitulate : \r\n\r\nModel | Timing\r\n--- | ---\r\nTF-slim default | 160ms\r\nTF-slim + NCHW | 150ms\r\nfc layers instead of conv | 94ms\r\nfc layers instead of conv + NCHW | 82ms\r\npytorch | 65ms\r\n\r\nThere is still a gap but it is definitely more acceptable, should we consider this as resolved?", "@SeguinBe are the models identical? One way of telling is initializing with same weights and running the computation through. Since they both rely on CuDNN they should get the same timing, 30% slower seems off.\r\n\r\nBTW, you can print out layers and their flops like this, this can sometimes help spot the difference\r\n\r\n```\r\n  tf.contrib.tfprof.model_analyzer.print_model_analysis(\r\n      tf.get_default_graph(),\r\n      tfprof_options=tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS)\r\n```", "Looped in by @sguada.\r\n\r\nThe discrepancy between using convolutional vs. fully connected layers should be fixed.\r\n\r\nThe convolution kernel correctly calls CuBlas gemm for 1x1 convolutions and NHWC format ([see here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L439)).\r\nUsing `conv2d` vs. `fully_connected` should not make a difference for 'fc7' or 'fc8' -- @SeguinBe can you verify that please?\r\n\r\nHowever, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when\r\nconvolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC", "@gpapan yeah it would be great if it works when the input size and the kernel size are the same and padding is 'VALID'.\r\n\r\nAlso, it seems that the optimization won't work with data_format='NCWH', but it should also work, isn't it?", "@gpapan Yes, setting fc7 or/and fc8 as 1x1 conv does not change the timing (both in NHWC and NCHW btw)\r\n\r\n@yaroslavvb It'd take a bit more time to properly transfer the weights from one framework to another. Though I think that I find the same proportional gap even in the 3x3 convolutional layers. I'll investigate more in the coming days.", "@SeguinBe Thanks, that's very informative, I will go ahead and submit a change to optimize the branch in which the filter and input activations have the same size.\r\n\r\n@sguada I think that cudnn natively supports the NCHW format and handles this special case internally. The last experiment by @SeguinBe also hints in the same direction.", "Can someone try it out and get timings with the latest head?", "@seguinbe can you please verify that the fix just pushed solves the conv2d vs. fully connected discrepancy issue? ", "Recent update of [Benchmarking State-of-the-Art Deep Learning Software Tools](https://arxiv.org/abs/1608.07249) shows some performance issues. For example, (see table 7) `AlexNet-R` is significantly (~ 10 times) slower in TF than in other frameworks, an it's even slower at GTX 980 than at GTX 1080. Also, ResNet-50 is ~5.5 times faster in MXNet. Those are most significant differences. \r\n\r\nIn addition, LSTM is around 3 times faster in CNTK, and ResNet-56 is twice faster in MXNet.\r\n\r\nVersion used was TensorFlow 0.11 (commit [47dd089](https://github.com/tensorflow/tensorflow/tree/47dd089db3cd16d76595791b2e8483e2fd0b0a25)) with CUDA 8.0 and cuDNN 5.1", "@Randl -- thanks, @annarev has been looking at those differences (ps, additional details should probably go to a different github issue since the problems there are different from vgg difference which is almost solved)", "Closing this item.  Comments can still be made.  While it does not always make a difference, you can also try adding `os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'`.  This seems to make a bigger difference on K80 and it is model dependent.  For Pascal I saw a 9% improvement when testing a Wide ResNet implementation.  Our [benchmark scripts](https://www.tensorflow.org/performance/performance_models) also include a VGG16 model, and I asked the team to check that implementation of VGG16 against the findings in this thread.  ", "So I'm coming back to some old issues and tried again this one:\r\n## Setup\r\n```\r\nconda create -n deepl python=3.6\r\nconda activate deepl\r\nconda install tensorflow-gpu=1.9 jupyter\r\nconda install pytorch torchvision -c pytorch\r\n    \r\nwget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\r\ntar -xvf vgg_16_2016_08_28.tar.gz\r\n```\r\nUbuntu 18.04 + Titan X Maxwell\r\nTF 1.9, Cuda 9.0, cuDNN 7.1.2 (have to update the drivers to go to 1.10, 9.2, and 7.2)\r\n\r\n## Commands\r\n#### Tensorflow\r\n```python\r\nimport os\r\n## Does not change anything\r\n#os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\nfrom tensorflow.contrib.slim import nets\r\nimport numpy as np\r\n\r\ntf.reset_default_graph()\r\nif False:  # 'NHWC' format\r\n    # Use RNG to avoid the feed_dict argument\r\n    input_images = tf.constant(np.random.randn(16, 224, 224, 3).astype(np.float32))\r\n    net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\r\n    conv_layer = net[1]['vgg_16/pool5']\r\n    preds = net[0]\r\nelse:  # 'NCHW' format\r\n    input_images = tf.constant(np.random.randn(16, 3, 224, 224).astype(np.float32))\r\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):\r\n        net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\r\n        conv_layer = net[1]['vgg_16/pool5']\r\n        preds = net[0]\r\nsaver = tf.train.Saver()\r\n\r\nconfig = tf.ConfigProto(log_device_placement=True)\r\nsess = tf.InteractiveSession(config=config)\r\nsaver.restore(sess, './vgg_16.ckpt')\r\n\r\n# With jupyter notebook magic\r\n%timeit sess.run(conv_layer)\r\n%timeit sess.run(preds)\r\n``` \r\n#### pyTorch\r\n```python\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.models as models\r\ntorch.backends.cudnn.benchmark = True\r\n\r\nnet = models.vgg16()\r\nnet.cuda()\r\n\r\n_in = torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda()\r\n\r\n# With jupyter notebook magic\r\n%timeit net.features(_in).data.cpu().numpy()\r\n%timeit net(_in).data.cpu().numpy()\r\n```\r\n\r\n## Results\r\n\r\n| Framework    | TF-NHWC | TF-NCHW | pyTorch |\r\n|--------------|---------|---------|---------|\r\n| pool5 output | 72.5    | 60.1    | 59.1    |\r\n| fc8 output   | 73.6    | 131.0   | 60.8    |\r\n\r\n### Conclusion\r\n\r\nPerformance is the same as long as the same data format is used.\r\n\r\nHowever, as we stated before the TF version is implementing the FC layers as convolutions instead of actual FC layers. For the NHWC case, an optimization makes this difference transparent but it is not the case for the NCHW layout, which explains the difference here. However, this is more about the networks being defined differently than actual performance issues.", "Hi all\r\n\r\nI just want to confirm if the below issue gotten fixed\r\n\"**However, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when convolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC**\" as I did estimate the time for 224*224 (input data size) fully convolutions layer and it still took more time than actual FC layers.\r\n\r\nI inference 10 pics by the below code\r\n\r\nFCN:\r\n```\r\n with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n            activation_fn=tf.nn.relu):\r\n            net = preprocessed_inputs\r\n            net = slim.conv2d(net,10, [224, 224], padding='VALID', scope='fc1')\r\n            net = tf.squeeze(net, [1, 2])   \r\n        prediction_dict = {'logits': net} \r\n```\r\n\r\nFC:\r\n```\r\n with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n            activation_fn=tf.nn.relu):\r\n            net = preprocessed_inputs\r\n            net = tf.contrib.layers.flatten(net)\r\n            net = slim.fully_connected(net, 10, activation_fn=tf.nn.relu, scope='fc6')\r\n        prediction_dict = {'logits': net} \r\n```\r\nResult_Time:\r\nFCN:0.001499s\r\nFC:0.000864s\r\n\r\nEnvironment\r\nCPU(s): Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz\r\nGPU: Nvidia GeForce RTX 2080 SUPER 8GB\r\nUbuntu: v18.04\r\nNvidia driver: v460\r\nCUDA: v11.2\r\nCuDNN: v8.1.1\r\n"]}, {"number": 7064, "title": "can't install tenserflow on ubuntu 16.04 via pip!!", "body": "Exception:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/wheel.py\", line 316, in clobber\r\n    ensure_dir(destdir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/utils/__init__.py\", line 83, in ensure_dir\r\n    os.makedirs(path)\r\n  File \"/usr/lib/python2.7/os.py\", line 157, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/tensorflow_gpu-0.12.1.dist-info'\r\n\r\n", "comments": ["Closing as a duplicate of #7063. Please don't send duplicate issues. If you want to edit the description, use the pencil icon on the upper right corner."]}, {"number": 7063, "title": "Can't install Tenserflow on ubuntu 16.04 via pip?", "body": "Exception:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/wheel.py\", line 316, in clobber\r\n    ensure_dir(destdir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pip/utils/__init__.py\", line 83, in ensure_dir\r\n    os.makedirs(path)\r\n  File \"/usr/lib/python2.7/os.py\", line 157, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/tensorflow_gpu-0.12.1.dist-info'\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["You'll want to run as root.\r\n\r\n```\r\nOSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/tensorflow_gpu-0.12.1.dist-info'\r\n```", "Did you use the `sudo pip`? It said that not permitted, so I don't think that maybe you use it", "Closing issue due to inactivity."]}, {"number": 7062, "title": "improved and fix bug of getting next batch in mnist", "body": "The original codes do not consider the situation that when the batch_size > num_examples (even though it is not a common event, if happens, no new data will be input at all). \r\n\r\nActually, a fully epoch means that all examples have been applied once, however, the original codes do not show it. If the batch_size sets just a little bit larger than half of the example number, nearly half of the examples cannot be used in one epoch.\r\n\r\nAlso, I set the shuffle to be a input variable.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 7061, "title": "Is there any GPU implementation of Queues?", "body": "what is the best way of pipelining the training samples in GPU to maximize its utilization?\r\n\r\nThe natural way of doing this seems to be using Queues, however it seems there is no GPU kernel for any type of Queue.", "comments": ["The queues are always on CPU. The CPU needs to spool, then the heavy duty processing is done on the GPU.\r\n\r\nThis is more a general question that's probably best answered on stackoverflow. We monitor issues with the tag \"tensorflow\" there.", "@mbz there's tracking issue in https://github.com/tensorflow/tensorflow/issues/5722 . In particular some queue-like ops with GPU support were added recently in https://github.com/tensorflow/tensorflow/commit/85acf52a"]}, {"number": 7060, "title": "Tensorboard: Please improve log-scale plots", "body": "Hi!\r\n\r\nTensorboard currently (version 0.12.1) has an option to plot graphs with a logarithmic log scale, but this functionality doesn't work well whenever plotting very small values (e.g. the ones that converge to zero, like loss values). For example, compare this plot in Tensorboard:\r\n\r\n![tensorboard_logplot](https://cloud.githubusercontent.com/assets/3627551/22293860/8abcdc40-e311-11e6-9e70-0796a503afac.png)\r\n\r\nWith the exact same data plotted using matplotlib/pandas using `logy=True`:\r\n\r\n    runs = sorted(os.listdir('tensorboard/logs/'))[:2]\r\n    d = [pd.read_csv('http://127.0.0.1:6033/data/scalars?run=%s&tag=loss&format=csv' % r)['Value'] for r in runs]\r\n    res = pd.DataFrame(d, index=runs).T\r\n    res.plot(ax=ax, logy=True)\r\n\r\n![matplotlib_logplot](https://cloud.githubusercontent.com/assets/3627551/22293890/9af8e568-e311-11e6-95ff-a803a39f5d27.png)\r\n\r\nClearly, the Tensorboard plot is much less informative (the \"logarithmic plot\" option doesn't do anything to improve the plot). It would be nice if Tensorboard could produce logarithmic plots similar to the ones in matplotlib.", "comments": ["@dandelionmane a request for logy plot. Is that something that we would like that Tensorboard doesn't have?", "We currently use a \"ModifiedLogScale\" scale from Plottable.js. It's designed to revert to being a linear scale for values close to 0, so that it can be defined at y=0.\r\n\r\nI can see that it's not working that well in this case. I also do want the plot to continue to be defined over the whole range of numbers, so that charts don't break if you toggle ito non values that intersect 0.\r\n\r\nHowever, we could move the linearity convergence to be arbitrarily small... e.g. it reverts to linearity in the range 10^-8 to 0. going to mark it contributions welcome for now.", "> However, we could move the linearity convergence to be arbitrarily small... e.g. it reverts to linearity in the range 10^-8 to 0. going to mark it contributions welcome for now.\r\n\r\nWe could also see if we can only use this modified scale when in fact there are negative* values. If we could manage that, then I'd be quite happy regardless of what the behavior is when there are negative values. That is, if you asked for a log plot when you have negative values, that's kind of your fault, but other people shouldn't be affected.\r\n\r\n(Sending zero to neginf is fine with me. Our plots already handle infinities well.)\r\n\r\nIn any case, I've migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/57. Please feel free to keep discussing or submit a PR there! :-)"]}, {"number": 7059, "title": "Issue to compile on macOS with unrecognized command line option", "body": "Hello,\r\n\r\nI do have issues to compile Tensorflow on my mac in order to use the Java version. Here some useful information about my environment:\r\n\r\n* macOS 10.12.3\r\n* gcc 6.3.0 (from MacPorts)\r\n* Bazel 4.3.0\r\n* Python 2.7\r\n* Tensorflow commit a12c7dc3d83049e10c1dca8903d73cc71d3cb7b2\r\n* CPU only\r\n\r\nOnce I have done the ```./configure``` I'm trying to compile the Java version and here the log I get:\r\n\r\n```\r\nbazel build -c opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nINFO: Found 2 targets...\r\nERROR: /private/var/tmp/_bazel_jplu/00c84c0db16fb633b58c75ed37aef199/external/nanopb_git/BUILD:8:1: C++ compilation of rule '@nanopb_git//:nanopb' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 34 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error: unrecognized command line option '-Wthread-safety'; did you mean '-fthread-jumps'?\r\ngcc: error: unrecognized command line option '-Wself-assign'; did you mean '-Wparam-assign'?\r\nINFO: Elapsed time: 6,013s, Critical Path: 0,05s\r\n```\r\n\r\nI also tried with a GCC 5.4 version and I get the exact same error.\r\n\r\nDid anyone have an idea of what is going wrong? Am I using a wrong version of GCC?\r\n\r\nThanks in advance.", "comments": ["Can you try this from the command-line?\r\n\r\n```\r\ngcc -v -c -xc++ /dev/null -o /dev/null\r\n```\r\n\r\nIt looks like this is requested here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL.tpl#L213\r\n\r\nCould you try removing that?", "Here the output I get:\r\n\r\n```\r\ngcc -v -c -xc++ /dev/null -o /dev/null\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nTarget: x86_64-apple-darwin16\r\nConfigured with: /opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_tarballs_ports_lang_gcc6/gcc6/work/gcc-6.3.0/configure --prefix=/opt/local --build=x86_64-apple-darwin16 --enable-languages=c,c++,objc,obj-c++,lto,fortran --libdir=/opt/local/lib/gcc6 --includedir=/opt/local/include/gcc6 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-6 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-6 --with-gxx-include-dir=/opt/local/include/gcc6/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-build-config=bootstrap-debug --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc6 6.3.0_0'\r\nThread model: posix\r\ngcc version 6.3.0 (MacPorts gcc6 6.3.0_0) \r\nCOLLECT_GCC_OPTIONS='-v' '-c' '-o' '/dev/null' '-mmacosx-version-min=10.12.3' '-asm_macosx_version_min=10.12' '-mtune=core2'\r\n /opt/local/libexec/gcc/x86_64-apple-darwin16/6.3.0/cc1plus -quiet -v -D__DYNAMIC__ /dev/null -fPIC -quiet -dumpbase null -mmacosx-version-min=10.12.3 -mtune=core2 -auxbase-strip /dev/null -version -o /var/folders/gc/mpn9gjgs2c9dxwmjm0xzkqg00000gn/T//cc2tp8Av.s\r\nGNU C++14 (MacPorts gcc6 6.3.0_0) version 6.3.0 (x86_64-apple-darwin16)\r\n\tcompiled by GNU C version 6.3.0, GMP version 6.1.2, MPFR version 3.1.4, MPC version 1.0.3, isl version 0.14 or 0.13\r\nGGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072\r\nignoring nonexistent directory \"/opt/local/lib/gcc6/gcc/x86_64-apple-darwin16/6.3.0/../../../../../x86_64-apple-darwin16/include\"\r\n#include \"...\" search starts here:\r\n#include <...> search starts here:\r\n /opt/local/include/gcc6/c++/\r\n /opt/local/include/gcc6/c++//x86_64-apple-darwin16\r\n /opt/local/include/gcc6/c++//backward\r\n /opt/local/lib/gcc6/gcc/x86_64-apple-darwin16/6.3.0/include\r\n /opt/local/include\r\n /opt/local/lib/gcc6/gcc/x86_64-apple-darwin16/6.3.0/include-fixed\r\n /usr/include\r\n /System/Library/Frameworks\r\n /Library/Frameworks\r\nEnd of search list.\r\nGNU C++14 (MacPorts gcc6 6.3.0_0) version 6.3.0 (x86_64-apple-darwin16)\r\n\tcompiled by GNU C version 6.3.0, GMP version 6.1.2, MPFR version 3.1.4, MPC version 1.0.3, isl version 0.14 or 0.13\r\nGGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072\r\nCompiler executable checksum: 7f53308891a2e11385f319c4215fc083\r\nCOLLECT_GCC_OPTIONS='-v' '-c' '-o' '/dev/null' '-mmacosx-version-min=10.12.3'  '-mtune=core2'\r\n /opt/local/bin/as -v -arch x86_64 -force_cpusubtype_ALL -o /dev/null /var/folders/gc/mpn9gjgs2c9dxwmjm0xzkqg00000gn/T//cc2tp8Av.s\r\nApple Inc version cctools-895, GNU assembler version 1.38\r\nCOMPILER_PATH=/opt/local/libexec/gcc/x86_64-apple-darwin16/6.3.0/:/opt/local/libexec/gcc/x86_64-apple-darwin16/6.3.0/:/opt/local/libexec/gcc/x86_64-apple-darwin16/:/opt/local/lib/gcc6/gcc/x86_64-apple-darwin16/6.3.0/:/opt/local/lib/gcc6/gcc/x86_64-apple-darwin16/\r\nLIBRARY_PATH=/opt/local/lib/gcc6/gcc/x86_64-apple-darwin16/6.3.0/:/opt/local/lib/gcc6/gcc/x86_64-apple-darwin16/6.3.0/../../../\r\nCOLLECT_GCC_OPTIONS='-v' '-c' '-o' '/dev/null' '-mmacosx-version-min=10.12.3'  '-mtune=core2'\r\n```\r\n\r\nNext, if I comment the two involved lines in the file you mention, it fails with the same error, but, because of an another file:\r\n\r\n```\r\nbazel build -c opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nINFO: Found 2 targets...\r\nERROR: /private/var/tmp/_bazel_jplu/00c84c0db16fb633b58c75ed37aef199/external/farmhash_archive/BUILD:12:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 36 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error: unrecognized command line option '-Wthread-safety'; did you mean '-fthread-jumps'?\r\ngcc: error: unrecognized command line option '-Wself-assign'; did you mean '-Wparam-assign'?\r\nINFO: Elapsed time: 17,390s, Critical Path: 0,31s\r\n```", "I'm not sure where these flags are coming from. @asimshankar any idea?", "I suspect that this happening because you're actually using `gcc` as opposed to `clang` which is the compiler that ships with xcode and is aliased to `gcc` on OS X. For example, on my Mac:\r\n\r\n```sh\r\n$ gcc --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/c++/4.2.1\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin16.3.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n\r\nShows that `/usr/bin/gcc` is actually clang. The toolchain configuration that @drpngx linked to is also [designed for clang and not gcc on OS X](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL.tpl#L172).\r\n\r\nGetting things to work with `gcc` on OS X will be some work, involving a review of the toolchain configuration in CROSSTOOL.tpl.\r\n\r\nIs it possible for you to use the default C compiler that ships with xcode? Or to put it differently, mind shedding some light on why you need to use a `gcc` from MacPorts instead of using the toolchain installed by xcode?", "We can also use `-Xclang-only=-Wthread-safety` to get it work for both (obviously disabling that on gcc).", "Hello,\r\n\r\nFirst of all, thanks a lot for you very clear explanation.\r\n\r\n> Is it possible for you to use the default C compiler that ships with xcode? Or to put it differently, mind shedding some light on why you need to use a gcc from MacPorts instead of using the toolchain installed by xcode?\r\n\r\nI can install the xcode toolchain and switch between both it is ok. The reason is that I don't use xcode at all to code, and I didn't want something that heavy installed for not being used at all. the second reason is that I'm used to use the GNU tools so I replaced the \"MacOS\" version of these tools for using the GNU ones.\r\n\r\n> We can also use -Xclang-only=-Wthread-safety to get it work for both (obviously disabling that on gcc).\r\n\r\nIf there is a possibility to make GNU GCC working, it would be super nice!\r\n\r\nI'm closing this issue as my problem has found a clear explanation. Thanks again.", "I am getting the same error with Clang 9.0.0\r\n\r\nkaos:tensorflow sam$ which g++\r\n/usr/bin/g++\r\nkaos:tensorflow sam$ g++ -v\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\nI have been considering a downgrade", "here is the exact error \r\nERROR: /private/var/tmp/_bazel_sam/7fbd006de80bc278719e770d66f90e2b/external/flatbuffers/BUILD:94:1: Linking of rule '@flatbuffers//:flatc' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-fobjc-link-runtime'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 22.061s, Critical Path: 12.49s\r\nFAILED: Build did NOT complete successfully", "downgraded still getting issue\r\n\r\nrotobuf_archive/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_sam/7fbd006de80bc278719e770d66f90e2b/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nERROR: /private/var/tmp/_bazel_sam/7fbd006de80bc278719e770d66f90e2b/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-darwin_x86_64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-darwin_x86_64' failed; build aborted\r\nINFO: Elapsed time: 16.684s\r\nFAILED: Build did NOT complete successfully (58 packages loaded)\r\n    currently loading: tensorflow/python ... (2 packages)", "Never mind I only downgraded my command line tools I think I need to downgrade all of XCode, initially I was trying to work with an anaconda based g++ 4.8.5 that didnt work out so well either."]}, {"number": 7058, "title": "what's the difference betwee tf.nn.seq2seq and tf.python.ops.seq2seq?", "body": "what's the difference betwee tf.nn.seq2seq and tf.python.ops.seq2seq?", "comments": ["There should not be a `tf.python.ops.seq2seq`. The public API defines `tf.nn.seq2seq`.", "but I found these two imports in two projects, this is why I got confused  ", "I think this maybe caused by the difference of the tensorflow version. In the version 12, the later is not available", "Thanks @KeyYD for checking. Right, to re-iterate, `tf.nn.seq2seq` is the valid one among the two.", "Is this right? It seems like tf.nn.seq2seq is again deprecated as well? What's the new API?", "We weren't confident that `seq2seq` was ready -- its API is still changing. it's in `tf.contrib.seq2seq`."]}, {"number": 7057, "title": "[PATCH] FreeBSD compatibility", "body": "Hi,\r\n\r\nI've managed to compile tensorflow in cpu mode so here's whats needed for FreeBSD compatibility.\r\n\r\nAs I've no clue how bazel really works I've mostly hacked the bazel stuff, so someone that knows what he's doing might want to make the BUILD changes in the proper place :)\r\n\r\n**Summary**\r\n1. -ldl needs to be removed from all linkopts for FreeBSD as libdl is integrated into libc  so FreeBSD does not have -ldl\r\n2. No code needs to be changed becides adding a few ifstatements for FreeBSD right next to APPLE statements and adding a missing header file\r\n3. external/protobuf/BUILD needs \"-lm\" in LINK_OPTS, I did not figure out how to tell bazel to do that during tar.gz extraction.\r\n", "comments": ["Could you submit that as a PR? That would be nice.", "I've added a PR for the changes I can make in the git repo\r\nhttps://github.com/tensorflow/tensorflow/pull/7073\r\n\r\nHowever, the protobuff BUILD file still needs to be patched after extraction somehow.", "Thanks! Could you submit the change to the protobuf repo? Then we can update the version.", "I've now submitted a PR there too\r\nhttps://github.com/google/protobuf/pull/2630", "Nice, thanks! Please ping back when approved.", "@drpngx the tensorflow PR has been merged now but no one has looked at the protobuf PR yet.\r\n\r\nPerhaps you know someone that works with protobuf you could ping?", "Thanks, let me ping on the thread first.", "Great, PR was approved on protobuf side. We'll probably want to wait for the next dot release to update the workspace.", "Sounds awesome!", "I think all the necessary work here was done.\r\nI am closing this issue due to inactivity.\r\nPlease reopen if there is more that needs to be done."]}]