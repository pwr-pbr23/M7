[{"number": 44536, "title": "Can't reproduce the same result: tensorflow/tensorflow/lite/micro/examples/person_detection_experimental/ model.", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18\r\n- TensorFlow installed from (source or binary): binary\r\n- Tensorflow version (commit SHA if source): 1.14\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33\r\n\r\n**Describe the problem**\r\nI am trying to reproduce the same result for a [person detection](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection_experimental) by following all instruction in this [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection_experimental/training_a_model.md) doc successfully. But, I can't get the expected result. \r\n\r\nHere is the output of the serial monitor:\r\n\r\n```\r\nStarting capture\r\nImage captured\r\nReading 3080 bytes from ArduCAM\r\nFinished reading\r\nDecoding JPEG and converting to greyscale\r\nImage decoded and processed\r\nPerson score: -1 No person score: 1\r\n\r\n```\r\nFor all capture frame, the model gives the same output` Person score: -1 No person score: 1` even there is a person on a frame.\r\n\r\nPlease help to resolve this issue.\r\n\r\nThanks\r\nBhavika\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44536\">No</a>\n"]}, {"number": 44535, "title": "tfp.stats.histogram does not print out that it has no gradients", "body": "tfp.stats.histogram does not print out that it has no gradients.  I would expect there would be a message indicating gradients aren't flowing like when you use tf.round().\r\n\r\nVerified no gradients via\r\n\r\n```\r\nx = tf.Variable([1,2,3,4,5,6,7,8])\r\nwith tf.GradientTape() as tape:\r\n    tmp = tfp.stats.histogram(x, [0,10])\r\ngrad = tape.gradient(tmp, x)\r\ngrad is None\r\n```", "comments": ["@isaacgerg,\r\nOn running the code with TF v2.3 and TF-nightly, `grad is None` returns `True`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2dcf867b0e18a620d001b6c3b0f5c179/44535.ipynb#scrollTo=gzgSd3M5szzo). \r\n\r\nCould you please confirm if this is the issue you're facing? Thanks!", "@amahendrakar   Yes that is what i also see.  When you do the same thing with tf.round, it prints a warning saying that none of the gradients are flowing down to earlier layers. I would think histogram should do the same thing.", "This seems to be the wrong repository for this bug - could you file it in TFP? (https://github.com/tensorflow/probability/issues)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44535\">No</a>\n"]}, {"number": 44534, "title": "Call dataset cardinality op on the dataset device.", "body": "Dataset cardinality op should be placed on the same host device as the dataset, otherwise TF will try to encode/decode the dataset to another host device, which is not possible.\n\nPiperOrigin-RevId: 339942622\nChange-Id: Ifbeb0aabdfc35f6c0e1248d69a8d22727e19c4a2", "comments": []}, {"number": 44533, "title": "Colab feature_columns.ipynb cannot save and reload Keras model", "body": "[Gist issue](https://colab.research.google.com/gist/Silb78dg/ec40d3472e6b67ad399c24320d671973/feature_columns.ipynb#scrollTo=RHNfDYLRv8o_) from [Colab feature_columns.ipynb](https://www.tensorflow.org/tutorials/structured_data/feature_columns)\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\nTF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n```\r\n2020-11-03 01:52:53.863478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nv2.3.0-0-gb36436b087 2.3.0\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nCannot reload a model saved in keras format\r\n```\r\nValueError: You are trying to load a weight file containing 4 layers into a model with 0 layers.\r\n```\r\n\r\n**Describe the expected behavior**\r\nBeing able to reload the previously saved model\r\n\r\n**Standalone code to reproduce the issue**\r\n[gist](https://colab.research.google.com/gist/Silb78dg/ec40d3472e6b67ad399c24320d671973/feature_columns.ipynb#scrollTo=RHNfDYLRv8o_)\r\nSteps:\r\n\r\n1. Run [Colab feature_columns.ipynb](https://www.tensorflow.org/tutorials/structured_data/feature_columns)\r\n2. Add one cell\r\n```\r\nmodel.save('pet_finder.h5', include_optimizer=False)\r\nreload_keras_model = tf.keras.models.load_model('pet_finder.h5')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-75-876b76fb8d70> in <module>()\r\n      1 model.save('pet_finder.h5', include_optimizer=False)\r\n----> 2 reload_keras_model = tf.keras.models.load_model('pet_finder.h5')\r\n      3 loss, accuracy = reload_keras_model.evaluate(test_ds)\r\n      4 print(\"Accuracy\", accuracy)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)\r\n    684                      'containing ' + str(len(layer_names)) +\r\n    685                      ' layers into a model with ' + str(len(filtered_layers)) +\r\n--> 686                      ' layers.')\r\n    687 \r\n    688   # We batch weight value assignments in a single backend call\r\n\r\nValueError: You are trying to load a weight file containing 4 layers into a model with 0 layers.\r\n```\r\n\r\nThank you.", "comments": ["@Silb78dg \r\nPlease share the tf version used", "It's in the description \r\n```\r\n2020-11-03 01:52:53.863478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nv2.3.0-0-gb36436b087 2.3.0\r\n```\r\ntf `2.3.0`. That the version in Colab.\r\nI can also reproduce locally with tf version `2.3.1`.\r\n\r\nThank you.", "@Silb78dg \r\nPlease share a simple stand alone code such that we can replicate the issue faced or if possible share a colab gist with the issue reported.", "@Saduf2019 Don't you have access to the gist I already shared?\r\nFirst line of the issue description.", "@Silb78dg\r\nWe ask for simple stand alone code such that we can replicate the issue faced, or if possible share a gist with the code and the error reported.", "@Saduf2019 I don't understand.\r\nHave you ever read the description? \r\nI have provided the gist link **twice**!\r\nCan you please tell me if you have access to the link I provided in the first line of the description (the gist's link not the colab's link)?\r\n\r\nJust in case, I can provided again: [https://colab.research.google.com/gist/Silb78dg/ec40d3472e6b67ad399c24320d671973/feature_columns.ipynb](https://colab.research.google.com/gist/Silb78dg/ec40d3472e6b67ad399c24320d671973/feature_columns.ipynb)\r\n\r\nWhat can be better than a bug reproduced using a tutorial? If we don't make the tutorial working properly, that is a really poor user experience.\r\n\r\n", "@ymodak \r\nI am able to replicate the error reported, Please find the [gist here](https://colab.research.google.com/gist/Saduf2019/9b5180b668f786b8ae929bf88d897154/untitled477.ipynb).", " Saving your model in default `tf` format  and loading is successful.\r\n```python\r\nmodel.save('pet_finder', include_optimizer=False)\r\nreload_keras_model = tf.keras.models.load_model('pet_finder')\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44533\">No</a>\n"]}, {"number": 44532, "title": "CONV_2d convert to DEPTHWISE_CONV when input depth=1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (or github SHA if from source):1.15.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\ntflite_convert  --output_file=centerfacembv2fix.tflite  \\\r\n                --graph_def_file=freeze.pb   \\\r\n                --inference_type=QUANTIZED_UINT8    \\\r\n                --input_arrays=input    \\\r\n                --input_shapes=1,320,320,1   \\\r\n                --std_dev_values=255 \\\r\n                --mean_values=0 \\\r\n                --output_arrays=scores,scores_1,scores_2,detector/wh_1,detector/reg_1,detector/angle_reg\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n![image](https://user-images.githubusercontent.com/30410113/97936501-a0905900-1db6-11eb-9bc9-33f1d0d74c09.png)\r\n\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n![image](https://user-images.githubusercontent.com/30410113/97936536-cae21680-1db6-11eb-9b08-1c6f9e33cadd.png)\r\nhere is my pb model \r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\nthe first layer is CON2D in pb,but converted to tflite it is depthwiseConv\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@sunzhe09 \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram I cannot paste  my code in colab \uff0cI have problem about prepare dataset in colab\uff1bI can upload the first two layers \u3002which is a subgraph to\uff0creproduce the problem\uff1bI think it\u2018s tflite_convert's problem,because my pb is a right layer\r\n[testmodel.zip](https://github.com/tensorflow/tensorflow/files/5479905/testmodel.zip)\r\n\r\n\r\n\r\n", "The Conv2D->DepthwiseConv2D shouldn't happen in the later TF versions.\r\nCan you please try with nightly may be or 2.4\r\n\r\nThanks", "@KabirKwatra I have tested with TF2.4 and TF-nightly,when I use this options:\r\ntflite_convert  --enable_v1_converter \\\r\n                --experimental_new_converter=True \\\r\nthe first conv was converted rightly,but my output seems to be wrongly converted.\r\n![image](https://user-images.githubusercontent.com/30410113/99865369-13d20180-2be4-11eb-8b85-456a937fbabd.png)\r\n\r\nthe original right output is like this \r\n![image](https://user-images.githubusercontent.com/30410113/99865387-3401c080-2be4-11eb-8a29-82e6a7845c36.png)\r\n", "Can you please share reproduce sample, so we can check it ?\r\nAlso, explain what does wrong mean here ?\r\nwrong result during inference ? wrong visualization ? something else.. Please explain.\r\n\r\nThanks", "I check the tflite model(tf2.4 converted) ,the output branch have many Equal layer during maxpool and select,but the output node is right.so my problem is the wrong visualization.", "@sunzhe09 That might be something missing to canonicalize to avoid this duplicates ? Do you have a sample reproduce for this, so i can check the reason exactly.\r\n\r\nThanks", "@karimnosseir \r\nhere is my command:\r\n#!/bin/bash\r\ntflite_convert  --enable_v1_converter \\\r\n                --experimental_new_converter=True  \\\r\n                --output_file=test.tflite  \\\r\n                --graph_def_file=freeze.pb   \\\r\n                --inference_type=QUANTIZED_UINT8    \\\r\n                --input_arrays=input    \\\r\n                --input_shapes=1,320,320,1   \\\r\n                --std_dev_values=255 \\\r\n                --mean_values=0 \\\r\n                --output_arrays=Select,Select_1,Select_2,detector/wh_1,detector/reg_1,detector/angle_reg\r\n\r\nyou can test with my pb\r\n\r\n[freezetest.zip](https://github.com/tensorflow/tensorflow/files/5636330/freezetest.zip)\r\n\r\n", "Thanks @sunzhe09 for providing the reproduce steps.\r\nI have a fix. Will be merged soon.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44532\">No</a>\n"]}, {"number": 44531, "title": "'list' object consisting of multiple inputs has no attribute 'ndim'", "body": "hey! guys.\r\nI have been in trouble, the error below was thrown when the model with double inputs predicted.\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"practice.py\", line 279, in <module>\r\n    action = np.argmax([0.1, 1, 0.2]*agent.get_qs(current_state))\r\n  File \"practice.py\", line 186, in get_qs\r\n    return self.model.predict(state)[0]\r\n  File \"C:\\Users\\liuzhen\\.conda\\envs\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1380, in predict\r\n    x, _, _ = self._standardize_user_data(x)\r\n  File \"C:\\Users\\liuzhen\\.conda\\envs\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 757, in _standardize_user_data\r\n    exception_prefix='input')\r\n  File \"C:\\Users\\liuzhen\\.conda\\envs\\python37\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 95, in standardize_input_data\r\n    data = [standardize_single_array(x) for x in data]\r\n  File \"C:\\Users\\liuzhen\\.conda\\envs\\python37\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 95, in <listcomp>\r\n    data = [standardize_single_array(x) for x in data]\r\n  File \"C:\\Users\\liuzhen\\.conda\\envs\\python37\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 30, in standardize_single_array\r\n    elif x.ndim == 1:\r\nAttributeError: 'list' object has no attribute 'ndim'\r\n```\r\nthe 'state' is a list of two nd-arrays there\r\n```python\r\nmodel = Model(inputs=[input1, input2], outputs=predictions)\r\n```\r\nI would really appreciate it if anyone is willing to give some  #tips\r\n\r\n_Originally posted by @tinmodeHuang in https://github.com/tensorflow/tensorflow/issues/20698#issuecomment-720506308_", "comments": []}, {"number": 44530, "title": "[ROCm] Fix for ROCm CSB breakage - 201102", "body": "The following commit introduces a new subtest ( `testThreeNestWithLists` within `//tensorflow/python/kernel_tests:while_v2_test_gpu` ), which fails with the following error on ROCm platform\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/07b75ffa453b1ec9189371244d21b6684503340b\r\n\r\n```\r\n======================================================================\r\nFAIL: testThreeNestWithLists (__main__.WhileV2Test)\r\ntestThreeNestWithLists (__main__.WhileV2Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/while_v2_test_gpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/while_v2_test.py\", line 291, in testThreeNestWithLists\r\n    self.assertAllClose(numerical, theoretical, rtol=1e-3)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/while_v2_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1236, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/while_v2_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2711, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/while_v2_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2651, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg))\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/while_v2_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2606, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/testing/nose_tools/utils.py\", line 1396, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.001, atol=1e-06\r\nMismatched value: a is different from b.\r\nnot close where = (array([0]), array([0]), array([0]))\r\nnot close lhs = [-0.16975401]\r\nnot close rhs = [-0.1699624]\r\nnot close dif = [0.00020839]\r\nnot close tol = [0.00017096]\r\ndtype = float32, shape = (1, 1, 1)\r\n(mismatch 100.0%)\r\n x: array([[[-0.169754]]], dtype=float32)\r\n y: array([[[-0.169962]]], dtype=float32)\r\n\r\n----------------------------------------------------------------------\r\nRan 70 tests in 16.568s\r\n\r\nFAILED (failures=1, skipped=2)\r\n================================================================================\r\n```\r\n\r\nThe commit fixes the subtest on the ROCm platform by slightly relaxing the error tolerance (1e-3 to 3e-3)\r\n\r\n\r\n-------------------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": ["@cheshire @chsigg gentle ping"]}, {"number": 44529, "title": "Tflite model inference bug", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from source\r\n- TensorFlow version 2.3.0\r\n\r\n**Describe the current behavior**\r\nI use the following code to test the model after tflite conversion\r\n```\r\ndef test_conversion(model_path):\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=model_path)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float64)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)  # <- ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT64 for input 0, name: input_1 \r\n\r\n    interpreter.invoke()\r\n\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(output_data)\r\n```\r\n\r\nWhile I print input_details I get the following:\r\n`[{'name': 'input_1', 'index': 0, 'shape': array([ 64, 224, 224,   3], dtype=int32), 'shape_signature': array([ 64, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float64'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]`\r\n\r\nDespite the fact, that the input_data dtype matches with expected type (np.float64) I still get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tflite.py\", line 55, in <module>\r\n    main()\r\n  File \"test_tflite.py\", line 51, in main\r\n    test_conversion(input_data)\r\n  File \"test_tflite.py\", line 30, in test_conversion\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 407, in set_tensor\r\n    self._interpreter.SetTensor(tensor_index, value)\r\nValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT64 for input 0, name: input_1 \r\n```\r\n", "comments": ["@Movisoto \r\nI ran the code shared and do not see any errors or output on tf-nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/0de7372b3b45b17b2f1363698b98f1da/untitled458.ipynb).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44529\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44529\">No</a>\n", "I ran into a very similar issue under tf 2.4.1. Workflow is fine for float32, but float64 raises the error described:\r\n\r\nhttps://colab.research.google.com/drive/1yS1bSIG_ygL4_vhXhjnMyqrvJf57cxYU?usp=sharing"]}, {"number": 44528, "title": "LocallyConnected1D layer description", "body": "The documentation of the [LocallyConnected1D layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected1D) mentions the following for the strides argument: \"Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\" \r\n\r\nI assume this sentence was just copied from the Conv1D layer and should be removed since there is no dilation_rate argument for the LocallyConnected1D layer?\r\n", "comments": ["I am closing this issue as the associated PR got merged. Thanks!"]}, {"number": 44527, "title": "Revert \"systemlibs: protobuf: fix merge conflict\"", "body": "Reverts tensorflow/tensorflow#44488", "comments": []}, {"number": 44526, "title": "Unable to parse stateful RNN in C API of Tensorflow 2.3.1", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc 8.4.0\r\n- CUDA/cuDNN version: CUDA 10.2, cuDNN 7.6\r\n- GPU model and memory: GTX 1050, 4GB\r\n\r\n**Describe the current behavior**\r\nI have created this RNN model:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\ninput = keras.layers.Input(shape=[12, 100], batch_size=1)\r\nx = keras.layers.Dense(20, activation='relu')(input)\r\nx = keras.layers.BatchNormalization()(x)\r\nfc1 = keras.layers.Dense(10, activation=None)(x)\r\ngru1 = keras.layers.LSTM(20, return_sequences=True, stateful=True)(fc1)\r\ngru2 = keras.layers.LSTM(20, return_sequences=True, stateful=True)(gru1)\r\nmodel = keras.Model(inputs=[input], outputs=[fc1, gru2])\r\n```\r\nWhen I try to use the C API of TensorFlow 2.3.1 (which I built myself with CUDA 10.2) to run inference on this model, I get the error:\r\n```\r\n2020-11-01 07:12:48.086124: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-11-01 07:12:48.086856: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\014\\000\\001\\002\\003\\004\\005\\006\\007\\010\\t\\n\\013\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\002\\010\\024\"\r\n}\r\n\r\n{{function_node __inference_standard_lstm_1329_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph}} {{function_node __inference_standard_lstm_1329_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph}} Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\014\\000\\001\\002\\003\\004\\005\\006\\007\\010\\t\\n\\013\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\002\\010\\024\"\r\n}\r\n```\r\nHere is the C code I used to serve the model:\r\n```\r\n    //********* Read model\r\n    TF_Graph* Graph = TF_NewGraph();\r\n    TF_Status* Status = TF_NewStatus();\r\n\r\n    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();\r\n    TF_Buffer* RunOpts = NULL;\r\n\r\n    const char* saved_model_dir = \"../model/\";\r\n    const char* tags = \"serve\"; // default model serving tag; can change in future\r\n    int ntags = 1;\r\n\r\n    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);\r\n    if(TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"TF_LoadSessionFromSavedModel OK\\n\");\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\",TF_Message(Status));\r\n    }\r\n\r\n    //****** Get input tensor\r\n    //TODO : need to use saved_model_cli to read saved_model arch\r\n    int NumInputs = 1;\r\n    TF_Output* Input = (TF_Output*)malloc(sizeof(TF_Output) * NumInputs);\r\n\r\n    TF_Output t0 = {TF_GraphOperationByName(Graph, \"serving_default_input_1\"), 0};\r\n    if(t0.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName serving_default_input_1\\n\");\r\n    else\r\n\tprintf(\"TF_GraphOperationByName serving_default_input_1 is OK\\n\");\r\n    \r\n    Input[0] = t0;\r\n    \r\n    //********* Get Output tensor\r\n    int NumOutputs = 1;\r\n    TF_Output* Output = (TF_Output*)malloc(sizeof(TF_Output) * NumOutputs);\r\n\r\n    TF_Output t2 = {TF_GraphOperationByName(Graph, \"StatefulPartitionedCall\"), 0};\r\n    if(t2.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName StatefulPartitionedCall\\n\");\r\n    else\t\r\n\tprintf(\"TF_GraphOperationByName StatefulPartitionedCall is OK\\n\");\r\n    \r\n    Output[0] = t2;\r\n\r\n    //********* Allocate data for inputs & outputs\r\n    TF_Tensor** InputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumInputs);\r\n    TF_Tensor** OutputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumOutputs);\r\n\r\n    int ndims = 3;\r\n    int64_t dims[] = {1,12,100};\r\n    float data[1*12*100];\r\n    for(int i=0; i< (1*12*100); i++)\r\n    {\r\n        data[i] = 1.00;\r\n    }\r\n    int ndata = sizeof(float)*1*12*100 ;\r\n\r\n    TF_Tensor* int_tensor = TF_NewTensor(TF_FLOAT, dims, ndims, data, ndata, &NoOpDeallocator, 0);\r\n    if (int_tensor != NULL)\r\n    {\r\n        printf(\"TF_NewTensor is OK\\n\");\r\n    }\r\n    else\r\n\tprintf(\"ERROR: Failed TF_NewTensor\\n\");\r\n    \r\n    InputValues[0] = int_tensor;\r\n    \r\n    // //Run the Session\r\n    TF_SessionRun(Session, NULL, Input, InputValues, NumInputs, Output, OutputValues, NumOutputs, NULL, 0,NULL , Status);\r\n\r\n    if(TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"Session is OK\\n\");\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\",TF_Message(Status));\r\n    }\r\n\r\n    // //Free memory\r\n    TF_DeleteGraph(Graph);\r\n    TF_DeleteSession(Session, Status);\r\n    TF_DeleteSessionOptions(SessionOpts);\r\n    TF_DeleteStatus(Status);\r\n```\r\nThe C code is a slightly modified version of https://github.com/AmirulOm/tensorflow_capi_sample/blob/master/main.c\r\nIt seems like the problem is with input = keras.layers.Input(shape=[12, 100], batch_size=1): if I change batch_size=1 to batch_size=None, then the inference runs fine. This lead me to suspect that maybe some settings or parameters may be incorrect when invoking the C API. \r\n\r\n**Describe the expected behavior**\r\nWhen I run the same model (with batch_size=1) in Tensorflow 2.3.1 using Python (3.6.9) API, everything works fine. Since The Python API calls the C/C++ code under the hood, I would expect the C API to be able to handle stateful LSTM mode.l  \r\n\r\n", "comments": ["It looks similar to https://github.com/tensorflow/tensorflow/issues/44428", "Please check a workaround patch in https://github.com/tensorflow/tensorflow/issues/44428", "Will be fixed by https://github.com/tensorflow/tensorflow/pull/47072, I believe ", "@DwayneDuane Could you please refer to the above comment and let us know if this issue can be fixed by #47072 ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44526\">No</a>\n"]}, {"number": 44525, "title": "The README file in this repo has a bad link - [404:NotFound]", "body": "The README file in this repo has a bad link - [404:NotFound]\r\n\r\nStatus code [404:NotFound] - Link: https://chat.stackoverflow.com/rooms/216694/tensorflow\r\n\r\nThis was found by an new experimental hobby project that I have just created: https://github.com/MrCull/GitHub-Repo-ReadMe-Dead-Link-Finder\r\nIf this has been in any way helpful then please consider giving the above Repo a Star.\r\n", "comments": ["Let's create a PR to remove the link.", "I removed the dead link but I am not able to merge \r\n#44757\r\nMight you be able to please recommend how to merge this\r\n", "It first needs to be approved by a person working at Google. Then there is some CI and if all is green it gets imported inside Google were it waits for a new review round and more CI. If all is good then there is automation that merges it.\r\n\r\nLooking at the Pr now."]}, {"number": 44524, "title": "Update list of systemlibs and add script to verify", "body": "I noticed that in addition to com_github_googleapis_googleapis being missing as a system lib (see #42361) the new typing_extensions_archive wasn't added to the list of valid options.\r\n\r\nThis simply removes the com_github_googleapis_googleapis, the alternative is to readd it as the new name com_google_googleapis and readd (and potentially update) the system build file but I can't verify this. Maybe  @perfinion may?\r\n\r\nIt also adds typing_extensions_archive as a valid option.\r\n\r\nI also made a script which points out those issues. I'd like to have that added to CI as this acts like a basic sanity check. Could anyone from the TF team guide me on how to do that?", "comments": ["I like this, thank you.", "@mihaimaruseac Could you tell me where I could add this so it is run by the CI scripts for PRs?", "> @mihaimaruseac Could you tell me where I could add this so it is run by the CI scripts for PRs?\r\n\r\nI talked with mihai at yesterday's SIG-Build meeting, we're not really sure about where yet. Lets get this merged first then maybe we can do a github action, the complicated part is most of their kokoro alerting is internal to google so there isnt a way to notify you and I. initially i'll try running a cronjob on my build machine until we figure out alerting", "Sorry, came to review GitHub stuff only now. We could try Github Actions at one point in the future, besides what perfinion@ said.", "I used GHA in the past and it is generally very easy to use. I could add one to run this to this PR or a follow up. It should be pretty trivial (checkout, setup python, run script) but needs maybe some bikeshedding about names and probably someone need to enable GHA for this repo. Also it will only run AFTER it is merged, so testing is a bit nasty. I could test it in my fork and link that in the PR though\r\nIMO running linters and such things like this are perfect for GHA as they are fast and easy to set up. Also that takes off pressure from the other tests\r\n", "Let's rebase this on master again, that should fix sanity build", "Rebased", "I'm not sure if im mis-reading the patches after the rebase or not?\r\nit looks like \"typing_extensions_archive\" is removed from syslibs_configure.bzl?\r\nThat got added in commit b7871f8a232536fcb6e388bb05a9fd7171ff3a96 so should stay in the valid list", "Good catch, thanks! Corrected and rebased to current master"]}, {"number": 44523, "title": "[tf.data] Add more precise processing accounting", "body": "The execution time of a synchronous captured function is currently approximated by the total elapsed time (including queueing time) rather than only measuring execution time. Time in queue is ~10ns per op or more and may increase under load. Mirroring the asynchronous captured function implementation, this commit pushes the accounting into the executor. Processing time now only measures time spent executing.", "comments": ["@jsimsa ", "gbaned@ could you please let us know what is this PR blocked on? thanks", "@jsimsa there was some error while migrating, now this is resolved ,can you please approve this internally."]}, {"number": 44522, "title": "Revert \"systemlibs: protobuf: Add missing headers\"", "body": "This reverts commit 62a71ef214d380847a72e2416a8f3e406ef15ab5.\r\n\r\nThe commit reintroducing the headers should not be required and existing headers were likely an artifact from a previous build. See https://github.com/tensorflow/tensorflow/pull/44489#issuecomment-720081499\r\n\r\n@perfinion Can you test this to verify this is working?\r\n\r\nIncludes the merge conflict fix also present in #44488", "comments": ["I am unsure what is the best resolution here. Waiting for @perfinion to validate that this will be working."]}, {"number": 44521, "title": "Formatting fix", "body": "Formatting fix in documentation of `zeros_like` in backend.py.", "comments": ["Is there any way to re-run failing tests?", "Please use testable docstrings instead of inline code blocks. See https://www.tensorflow.org/community/contribute/docs_ref", "> Please use testable docstrings instead of inline code blocks. See https://www.tensorflow.org/community/contribute/docs_ref\r\n\r\nNow I'm a bit confused... I had done that originally, but @fchollet asked me to use code block formatting instead, since \"these lines are not meant to be executed as part of our \"doctest\" suite\".\r\n\r\nSo should these lines be a doctest or not?\r\n\r\nCheers, Pedro", "You are right, my bad", "@mihaimaruseac Since you're here, is there any way to re-run the failing tests? Cheers!", "Adding the kokoro label retriggers the tests, but it has to be done by a googler.\r\n\r\nRight now the PR is waiting import into Google (`import/copybara`) and internal review. The MacOS CPU failed build will not block merge (since it is unrelated)"]}, {"number": 44520, "title": "tf.lite.Interpreter can not be used on the newest nightly version", "body": "I use the version tf-nightly==2.5.0-dev20201029 and tested the tflite model\r\n\r\nwhen I use the code below to test the performance of the fastspeech tflite model, the first input can be converted to audio correctly. However, the second would be converted to audio full of noise. Only when I reload the interpreter, the model can be used for the next input.\r\n\r\nthe next audio full of noise:\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\nfor samples in data_queue:\r\n            input_details = interpreter.get_input_details()\r\n            output_details = interpreter.get_output_details()\r\n            interpreter.resize_tensor_input(input_details[0]['index'], samples[\"input\"].shape)\r\n            interpreter.allocate_tensors()\r\n            interpreter.set_tensor(input_details[0]['index'], samples[\"input\"])\r\n            interpreter.invoke()\r\n            features = interpreter.get_tensor(output_details[0]['index'])\r\n            self.vocoder(features.numpy()) \r\n```\r\n\r\n\r\ncorrect:\r\n```\r\nfor samples in data_queue:\r\n            interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\n            input_details = interpreter.get_input_details()\r\n            output_details = interpreter.get_output_details()\r\n            interpreter.resize_tensor_input(input_details[0]['index'], samples[\"input\"].shape)\r\n            interpreter.allocate_tensors()\r\n            interpreter.set_tensor(input_details[0]['index'], samples[\"input\"])\r\n            interpreter.invoke()\r\n            features = interpreter.get_tensor(output_details[0]['index'])\r\n            self.vocoder(features.numpy()) \r\n```", "comments": ["@cookingbear Can you please share a simple standalone code to reproduce the issue? Thanks!", "> @cookingbear Can you please share a simple standalone code to reproduce the issue? Thanks!\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef generate_square_subsequent_mask(size):\r\n    \"\"\"  Generate a square mask for the sequence. The masked positions are filled with float(1.0).\r\n      Unmasked positions are filled with float(0.0).\r\n    \"\"\"\r\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\r\n    return mask\r\n\r\n\r\ndef create_multihead_mask(x, x_length, y, reverse=False):\r\n    r\"\"\" Generate a square mask for the sequence for mult-head attention.\r\n        The masked positions are filled with float(1.0).\r\n        Unmasked positions are filled with float(0.0).\r\n    \"\"\"\r\n    x_mask, y_mask = None, None\r\n    if x is not None:\r\n        x_mask = 1.0 - tf.sequence_mask(\r\n            x_length, tf.shape(x)[1], dtype=tf.float32\r\n        )\r\n        x_mask = tf.expand_dims(tf.expand_dims(x_mask, 1), 1)\r\n        if reverse:\r\n            look_ahead_mask = generate_square_subsequent_mask(tf.shape(x)[1])\r\n            x_mask = tf.maximum(x_mask, look_ahead_mask)\r\n        x_mask.set_shape([None, None, None, None])\r\n    if y is not None:\r\n        y_mask = tf.cast(tf.math.equal(y, 0), tf.float32)\r\n        y_mask = tf.expand_dims(tf.expand_dims(y_mask, 1), 1)\r\n        if not reverse:\r\n            look_ahead_mask = generate_square_subsequent_mask(tf.shape(y)[1])\r\n            y_mask = tf.maximum(y_mask, look_ahead_mask)\r\n        y_mask.set_shape([None, None, None, None])\r\n    return x_mask, y_mask\r\n\r\n\r\nclass PositionalEncoding(tf.keras.layers.Layer):\r\n    \"\"\" positional encoding can be used in transformer \"\"\"\r\n\r\n    def make_positional_encoding(self, position, d_model):\r\n        \"\"\" generate a postional encoding list \"\"\"\r\n\r\n        def get_angles(pos, i, d_model):\r\n            angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\r\n            return pos * angle_rates\r\n\r\n        angle_rads = get_angles(\r\n            np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model\r\n        )\r\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\r\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\r\n        pos_encoding = angle_rads[np.newaxis, ...]\r\n        return tf.cast(pos_encoding, dtype=tf.float32)\r\n\r\n    def __init__(self, d_model, max_position=800, scale=False):\r\n        super().__init__()\r\n        self.d_model = d_model\r\n        self.scale = scale\r\n        self.pos_encoding = self.make_positional_encoding(max_position, d_model)\r\n\r\n    def call(self, x):\r\n        \"\"\" call function \"\"\"\r\n        seq_len = tf.shape(x)[1]\r\n        if self.scale:\r\n            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n        x += self.pos_encoding[:, :seq_len, :]\r\n        return x\r\n\r\n\r\nclass ScaledPositionalEncoding(PositionalEncoding):\r\n    \"\"\" scaled positional encoding,\r\n        reference: https://arxiv.org/pdf/1809.08895.pdf\"\"\"\r\n    def __init__(self, d_model, max_position=800):\r\n        super().__init__(d_model, max_position, scale=False)\r\n\r\n    def build(self, _):\r\n        self.alpha = self.add_weight(\r\n            name=\"alpha\", initializer=tf.keras.initializers.constant(1)\r\n        )\r\n\r\n    def call(self, x):\r\n        seq_len = tf.shape(x)[1]\r\n        x += self.alpha * self.pos_encoding[:, :seq_len, :]\r\n        return x\r\n\r\n\r\nclass ScaledDotProductAttention(tf.keras.layers.Layer):\r\n    \"\"\"Calculate the attention weights.\r\n    q, k, v must have matching leading dimensions.\r\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\r\n    The mask has different shapes depending on its type(padding or look ahead)\r\n    but it must be broadcastable for addition.\r\n\r\n    Args:\r\n        q: query shape == (..., seq_len_q, depth)\r\n        k: key shape == (..., seq_len_k, depth)\r\n        v: value shape == (..., seq_len_v, depth_v)\r\n        mask: Float tensor with shape broadcastable\r\n          to (..., seq_len_q, seq_len_k). Defaults to None.\r\n\r\n    Returns:\r\n        output, attention_weights\r\n    \"\"\"\r\n    def __init__(self, unidirectional=False, look_ahead=0):\r\n        super().__init__()\r\n        self.uni = unidirectional\r\n        self.look_ahead = look_ahead\r\n\r\n    def call(self, q, k, v, mask):\r\n        \"\"\"This is where the layer's logic lives.\"\"\"\r\n        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\r\n\r\n        # scale matmul_qk\r\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n\r\n        if self.uni:\r\n            uni_mask = tf.ones(tf.shape(scaled_attention_logits))\r\n            uni_mask = tf.linalg.band_part(uni_mask, -1, self.look_ahead)\r\n            scaled_attention_logits += (1 - uni_mask) * -1e9\r\n        # add the mask to the scaled tensor.\r\n        if mask is not None:\r\n            scaled_attention_logits += mask * -1e9\r\n\r\n        # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n        # add up to 1.\r\n        # (..., seq_len_q, seq_len_k)\r\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\r\n\r\n        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\r\n\r\n        return output, attention_weights\r\n\r\n\r\nclass MultiHeadAttention(tf.keras.layers.Layer):\r\n    \"\"\" Multi-head attention\r\n\r\n    Multi-head attention consists of four parts: * Linear layers and split into\r\n    heads. * Scaled dot-product attention. * Concatenation of heads. * Final linear layer.\r\n    Each multi-head attention block gets three inputs; Q (query), K (key), V (value).\r\n    These are put through linear (Dense) layers and split up into multiple heads.\r\n    The scaled_dot_product_attention defined above is applied to each head (broadcasted for\r\n    efficiency). An appropriate mask must be used in the attention step. The attention\r\n    output for each head is then concatenated (using tf.transpose, and tf.reshape) and\r\n    put through a final Dense layer.\r\n    Instead of one single attention head, Q, K, and V are split into multiple heads because\r\n    it allows the model to jointly attend to information at different positions from\r\n    different representational spaces. After the split each head has a reduced dimensionality,\r\n    so the total computation cost is the same as a single head attention with full\r\n    dimensionality.\r\n    \"\"\"\r\n\r\n    def __init__(self, d_model, num_heads, unidirectional=False, look_ahead=0):\r\n        super().__init__()\r\n        self.num_heads = num_heads\r\n        self.d_model = d_model\r\n\r\n        assert d_model % self.num_heads == 0\r\n\r\n        self.depth = d_model // self.num_heads\r\n\r\n        self.wq = tf.keras.layers.Dense(\r\n            d_model,\r\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\r\n            input_shape=(d_model,),\r\n        )\r\n        self.wk = tf.keras.layers.Dense(\r\n            d_model,\r\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\r\n            input_shape=(d_model,),\r\n        )\r\n        self.wv = tf.keras.layers.Dense(\r\n            d_model,\r\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\r\n            input_shape=(d_model,),\r\n        )\r\n\r\n        self.attention = ScaledDotProductAttention(unidirectional, look_ahead=look_ahead)\r\n\r\n        self.dense = tf.keras.layers.Dense(\r\n            d_model,\r\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\r\n            input_shape=(d_model,),\r\n        )\r\n\r\n    def split_heads(self, x, batch_size):\r\n        \"\"\"Split the last dimension into (num_heads, depth).\r\n\r\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\r\n        \"\"\"\r\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, v, k, q, mask):\r\n        \"\"\" call function \"\"\"\r\n        batch_size = tf.shape(q)[0]\r\n\r\n        q = self.wq(q)  # (batch_size, seq_len, hiddn_dim)\r\n        k = self.wk(k)  # (batch_size, seq_len, hiddn_dim)\r\n        v = self.wv(v)  # (batch_size, seq_len, hiddn_dim)\r\n\r\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\r\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\r\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\r\n\r\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\r\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\r\n        scaled_attention, attention_weights = self.attention(q, k, v, mask)\r\n\r\n        # (batch_size, seq_len_q, num_heads, depth)\r\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\r\n\r\n        # (batch_size, seq_len_q, d_model)\r\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\r\n\r\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\r\n\r\n        return output, attention_weights\r\n\r\n\r\nclass TransformerEncoderLayer(tf.keras.layers.Layer):\r\n    \"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\r\n    This standard encoder layer is based on the paper \"Attention Is All You Need\".\r\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\r\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\r\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\r\n    in a different way during application.\r\n\r\n    Args:\r\n        d_model: the number of expected features in the input (required).\r\n        nhead: the number of heads in the multiheadattention models (required).\r\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\r\n        dropout: the dropout value (default=0.1).\r\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\r\n\r\n    Examples::\r\n        >>> encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)\r\n        >>> src = tf.random(10, 32, 512)\r\n        >>> out = encoder_layer(src)\r\n    \"\"\"\r\n\r\n    def __init__(\r\n            self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"gelu\",\r\n            unidirectional=False, look_ahead=0, ffn=None\r\n    ):\r\n        super().__init__()\r\n        self.self_attn = MultiHeadAttention(d_model, nhead, unidirectional, look_ahead=look_ahead)\r\n        # Implementation of Feedforward model\r\n        layers = tf.keras.layers\r\n        if ffn is None:\r\n            self.ffn = tf.keras.Sequential(\r\n                [\r\n                    layers.Dense(\r\n                        dim_feedforward,\r\n                        activation='gelu',\r\n                        kernel_initializer=tf.compat.v1.truncated_normal_initializer(\r\n                            stddev=0.02\r\n                        ),\r\n                        input_shape=(d_model,),\r\n                    ),\r\n                    layers.Dropout(dropout, input_shape=(dim_feedforward,)),\r\n                    layers.Dense(\r\n                        d_model,\r\n                        kernel_initializer=tf.compat.v1.truncated_normal_initializer(\r\n                            stddev=0.02\r\n                        ),\r\n                        input_shape=(dim_feedforward,),\r\n                    ),\r\n                    layers.Dropout(dropout, input_shape=(d_model,)),\r\n                ]\r\n            )\r\n        else:\r\n            self.ffn = ffn\r\n\r\n        self.norm1 = layers.LayerNormalization(epsilon=1e-8, input_shape=(d_model,))\r\n        self.norm2 = layers.LayerNormalization(epsilon=1e-8, input_shape=(d_model,))\r\n        self.dropout = layers.Dropout(dropout, input_shape=(d_model,))\r\n\r\n    def call(self, src, src_mask=None, training=None):\r\n        \"\"\"Pass the input through the endocder layer.\r\n\r\n        Args:\r\n            src: the sequnce to the encoder layer (required).\r\n            mask: the mask for the src sequence (optional).\r\n\r\n        Shape:\r\n            see the docs in Transformer class.\r\n        \"\"\"\r\n        out = self.self_attn(src, src, src, mask=src_mask)[0]\r\n        out = self.norm1(src + self.dropout(out, training=training))\r\n        out = self.norm2(out + self.ffn(out, training=training))\r\n\r\n        return out\r\n\r\n\r\nclass TransformerEncoder(tf.keras.layers.Layer):\r\n    \"\"\"TransformerEncoder is a stack of N encoder layers\r\n\r\n    Args:\r\n        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\r\n        num_layers: the number of sub-encoder-layers in the encoder (required).\r\n        norm: the layer normalization component (optional).\r\n\r\n    Examples::\r\n        >>> encoder_layer = [TransformerEncoderLayer(d_model=512, nhead=8)\r\n        >>>                    for _ in range(num_layers)]\r\n        >>> transformer_encoder = TransformerEncoder(encoder_layer)\r\n        >>> src = torch.rand(10, 32, 512)\r\n        >>> out = transformer_encoder(src)\r\n    \"\"\"\r\n\r\n    def __init__(self, encoder_layers):\r\n        super().__init__()\r\n        self.layers = encoder_layers\r\n\r\n    def call(self, src, src_mask=None, training=None):\r\n        \"\"\"Pass the input through the endocder layers in turn.\r\n\r\n        Args:\r\n            src: the sequnce to the encoder (required).\r\n            mask: the mask for the src sequence (optional).\r\n\r\n        Shape:\r\n            see the docs in Transformer class.\r\n        \"\"\"\r\n        output = src\r\n        for i in range(len(self.layers)):\r\n            output = self.layers[i](output, src_mask=src_mask, training=training)\r\n        return output\r\n\r\n\r\nclass FastSpeech(tf.keras.Model):\r\n    \"\"\"\r\n    Reference: Fastspeech: Fast, robust and controllable text to speech\r\n      (http://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf)\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.num_class = 314\r\n        self.eos = self.num_class - 1\r\n        self.feat_dim = 80\r\n        self.reduction_factor = 1\r\n\r\n        # for the x_net\r\n        layers = tf.keras.layers\r\n        input_features = layers.Input(shape=tf.TensorShape([None]), dtype=tf.int32)\r\n        inner = layers.Embedding(self.num_class, 384)(input_features)\r\n        inner = ScaledPositionalEncoding(384)(inner)\r\n        inner = layers.Dropout(0.1)(inner)\r\n        self.x_net = tf.keras.Model(inputs=input_features, outputs=inner, name=\"x_net\")\r\n        print(self.x_net.summary())\r\n\r\n        ffn_list = []\r\n        for _ in range(2):\r\n            ffn_list.append(tf.keras.Sequential(\r\n                [\r\n                    layers.Conv1D(\r\n                        filters=1536,\r\n                        kernel_size=3,\r\n                        strides=1,\r\n                        padding=\"same\",\r\n                        use_bias=False,\r\n                        data_format=\"channels_last\"),\r\n                    layers.ReLU(),\r\n                    layers.Dropout(0.1),\r\n                    layers.Conv1D(\r\n                        filters=384,\r\n                        kernel_size=3,\r\n                        strides=1,\r\n                        padding=\"same\",\r\n                        use_bias=False,\r\n                        data_format=\"channels_last\")\r\n                ]\r\n            ))\r\n        # define encoder form transform.py\r\n        encoder_layers = [\r\n            TransformerEncoderLayer(384, 2, 1536, 0.1, ffn=ffn_list[0])\r\n            for _ in range(6)\r\n        ]\r\n        self.encoder = TransformerEncoder(encoder_layers)\r\n\r\n        # define duration predictor\r\n        input_features = layers.Input(shape=tf.TensorShape([None, 384]),\r\n                                       dtype=tf.float32)\r\n        inner = input_features\r\n        for _ in range(2):\r\n            inner = layers.Conv1D(\r\n                filters=256,\r\n                kernel_size=3,\r\n                strides=1,\r\n                padding=\"same\",\r\n                use_bias = False,\r\n                data_format = \"channels_last\")(inner)\r\n            inner = layers.ReLU()(inner)\r\n            inner = layers.LayerNormalization()(inner)\r\n            inner = layers.Dropout(0.1)(inner)\r\n        inner = layers.Dense(1)(inner) # [batch, expanded_length, 1]\r\n        inner = tf.squeeze(inner, axis=-1)\r\n        self.duration_predictor = tf.keras.Model(inputs=input_features, outputs=inner,\r\n                                                 name=\"duration_predictor\")\r\n        print(self.duration_predictor.summary())\r\n\r\n        # for the y_net\r\n        input_features = layers.Input(shape=tf.TensorShape([None, 384]),\r\n                                      dtype=tf.float32)\r\n        inner = ScaledPositionalEncoding(384, max_position=3200)(input_features)\r\n        inner = layers.Dropout(0.1)(inner)\r\n        self.y_net = tf.keras.Model(inputs=input_features, outputs=inner, name=\"y_net\")\r\n        print(self.y_net.summary())\r\n\r\n        # define decoder\r\n        decoder_layers = [\r\n            TransformerEncoderLayer(384, 2, 1536, 0.1, ffn=ffn_list[1])\r\n            for _ in range(6)\r\n        ]\r\n        self.decoder = TransformerEncoder(decoder_layers)\r\n\r\n        # define feat_out\r\n        self.feat_out = layers.Dense(self.feat_dim * self.reduction_factor, use_bias=False,\r\n                                     name='feat_projection')\r\n        # define postnet\r\n        input_features_postnet = layers.Input(shape=tf.TensorShape([None, self.feat_dim]),\r\n                                              dtype=tf.float32)\r\n        inner = input_features_postnet\r\n        for _ in tf.range(5):\r\n            filters = 256\r\n            inner = layers.Conv1D(\r\n                filters=filters,\r\n                kernel_size=5,\r\n                strides=1,\r\n                padding=\"same\",\r\n                use_bias=False,\r\n                data_format=\"channels_last\",\r\n            )(inner)\r\n            inner = layers.BatchNormalization()(inner)\r\n            inner = tf.nn.tanh(inner)\r\n            inner = layers.Dropout(0.5)(inner)\r\n        inner = layers.Dense(self.feat_dim, name='projection')(inner)\r\n        self.postnet = tf.keras.Model(inputs=input_features_postnet, outputs=inner, name=\"postnet\")\r\n        print(self.postnet.summary())\r\n\r\n    def get_loss(self, outputs, samples, training=None):\r\n        \"\"\" get loss used for training \"\"\"\r\n        return None, None\r\n\r\n    def _feedforward_decoder(self, expanded_array, expanded_length, training: bool = None):\r\n        \"\"\"feed-forward decoder\r\n        Args:\r\n            expanded_array: expanded encoder outputs after length regulation\r\n                shape: [batch, y_steps, d_model]\r\n            expanded_length: corresponding lengths, shape: [batch, y_steps]\r\n            training: if it is in the training stage\r\n        Returns:\r\n            before_outs: the outputs before postnet calculation\r\n            after_outs: the outputs after postnet calculation\r\n        \"\"\"\r\n\r\n        expanded_mask, _ = create_multihead_mask(expanded_array, expanded_length, None)\r\n        expanded_output = self.y_net(expanded_array, training=training)\r\n        # decoder_output, shape: [batch, expanded_length, d_model]\r\n        decoder_output = self.decoder(expanded_output, expanded_mask, training=training)\r\n        batch = tf.shape(decoder_output)[0]\r\n        decoder_output = self.feat_out(decoder_output, training=training)\r\n        before_outs = tf.reshape(decoder_output, [batch, -1, self.feat_dim])\r\n        after_outs = before_outs + self.postnet(before_outs, training=training)\r\n        return before_outs, after_outs\r\n\r\n    def call(self, samples, training: bool = None):\r\n        return None\r\n\r\n    def synthesize(self, samples, alpha=1.0):\r\n        x0 = self.x_net(samples['input'], training=False)\r\n        _, input_mask = create_multihead_mask(None, None, samples['input'], reverse=True)\r\n        encoder_output = self.encoder(x0, input_mask, training=False) # [batch, x_steps, d_model]\r\n        duration_sequences = self.duration_predictor(encoder_output, training=False)\r\n        duration_sequences = tf.cast(tf.clip_by_value(tf.math.round(\r\n            tf.exp(duration_sequences) - 1.0),\r\n            0.0, tf.cast(100, dtype=tf.float32)), dtype=tf.int32)\r\n\r\n        phoneme_seq = encoder_output[0]  # [x_step, d_model]\r\n        duration_seq = duration_sequences[0]  # [x_step]\r\n        repeated_phoneme_seq = tf.repeat(phoneme_seq, repeats=duration_seq, axis=0)\r\n        repeated_phoneme_seq = tf.expand_dims(repeated_phoneme_seq, axis=0)\r\n        expanded_length = tf.reduce_sum(duration_sequences, axis=1)  # [batch]\r\n\r\n        _, after_outs = self._feedforward_decoder(repeated_phoneme_seq, expanded_length, training=False)\r\n        return after_outs\r\n\r\n\r\ndef tflite_convert():\r\n    \"\"\" restore the best model \"\"\"\r\n    model = FastSpeech()\r\n    checkpointer = tf.train.Checkpoint(model=model)\r\n    checkpointer.restore(tf.train.latest_checkpoint(\"tmp_ckpt/\"))\r\n\r\n    def inference(x):\r\n        samples = {\"input\": x}\r\n        outputs = model.synthesize(samples)\r\n        return outputs\r\n\r\n    model.inference_function = tf.function(inference, experimental_relax_shapes=True,\r\n                                           input_signature=[tf.TensorSpec(shape=[1, None], dtype=tf.int32)])\r\n    tf.saved_model.save(obj=model, export_dir=\"tmp_model\")\r\n    load_model = tf.saved_model.load(\"tmp_model\")\r\n\r\n    concrete_function = load_model.inference_function.get_concrete_function()\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                           tf.lite.OpsSet.SELECT_TF_OPS]\r\n    tflite_model = converter.convert()\r\n\r\n    with open('model.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.random.set_seed(1)\r\n    tflite_convert()\r\n```\r\n", "@jvishnuvardhan  I can reproduce the error using the code above, can you please look into this?\r\n", "I used the code above to create the tflite model which would be used to produce speech based on text input.\r\nAccording to my experiments, when I always feed the same input into the model, it works fine and always output the same speech. And the speech can also be produced correctly the first time after loading the tflite model.\r\nHowever, the tflite model would go wrong after one or two rounds of production and it would produce the audio with every sampling point sharing similar value. \r\nI just doubt that some operations and parameters may change even after interpreters have been created and loaded", "@terryheo @abattery can you take a look? I wonder if this is an issue with state (or tensor buffers?) not being properly reset.", "@jaeyoo Could you take a look? FYI, the issue is related to the fastspeech model.\r\n\r\n@cookingbear is this issue happening only with the tf nightly version or is it happening with the tf 2.3 and tf 2.4 versions as well?", "> @jaeyoo Could you take a look? FYI, the issue is related to the fastspeech model.\r\n> \r\n> @cookingbear is this issue happening only with the tf nightly version or is it happening with the tf 2.3 and tf 2.4 versions as well?\r\n\r\nbecause the model contains 'embedding' and 'dynamic size' and other 'special' layers, it can not be converted to the tflite model successfully in the tf2.3 or tf2.4 version.  so I only tried the tf nightly version. ", "@jaeyoo @abattery If there is any update or information needed to be provided, please let me know. Thanks very much!", "@cookingbear Could you share your checkpoint data to us if possible? If not, we need to find a way to verify your situation in our side.", "> @cookingbear Could you share your checkpoint data to us if possible? If not, we need to find a way to verify your situation in our side.\r\n\r\nSorry, the checkpoint currently can not be provided. However, I recently found that if I add the line in the __init__ function:\r\n\r\n```\r\n        self.position = np.load('position.npy')\r\n        self.position = tf.convert_to_tensor(self.position)\r\n```\r\nin which position.npy is a matrix of which the value can be randomly set. \r\nAnd replace the _feedforward_decoder function with the code below:\r\n```\r\n    def _feedforward_decoder(self, expanded_array, expanded_length, training: bool = None):\r\n        seq_len = tf.shape(expanded_array)[1]\r\n        expanded_output = expanded_array + 0.69105166 * self.position[:, :seq_len, :]\r\n        expanded_output = tf.concat([expanded_output, self.position[:, :seq_len, :]], axis=0)\r\n        return None, expanded_output\r\n```\r\nI convert this model into tflite, and after several rounds of calculations, I can see that the \"self.position\" value in \"expanded_output\" are all zeros.\r\nMeanwhile, if I use \r\n`converter.target_spec.supported_types = [tf.float32]`\r\nthe problems can be solved.\r\nif I use \r\n`converter.target_spec.supported_types = [tf.float16]`\r\nthe problem remains.", "@cookingbear what's the input samples looks like? Could you share one?\r\nI've tested this with simple input (such as integer array of [[1, 2, 3, 4, 5, 6, 7, 8]]) and it gives the same result with reused interpreter.\r\n\r\nFYI,  tf-nightly-2.5.0-dev20201029 looks bit unstable so I used tf-nightly-2.5.0.dev20201110", "Here is a checkpoint that can reproduce the error.\r\nhttps://drive.google.com/file/d/10INDBq-2ZvNmRc5UxJfgFOIXabWmzKHq/view?usp=sharing\r\n\r\nIf you test the converted tflite model using the inputs below:\r\n1). 294, 289, 159, 144, 70, 255, 301, 254, 280, 439, 389, 440, 320, 294, 1\r\n2). 294, 254, 88, 439, 390, 438, 209, 293, 150, 294, 1\r\n3). 294, 289, 159, 144, 70, 146, 371, 290, 301, 440, 300, 255, 301, 294, 1\r\n4). 294, 289, 159, 144, 70, 146, 371, 290, 301, 440, 300, 255, 301, 294, 1\r\n5). 294, 289, 159, 144, 70, 255, 301, 254, 280, 439, 389, 440, 320, 294, 1\r\n\r\nyou would find that although 1) and 5) share the same inputs, but the outputs are different. \r\nMeanwhile, if I use \r\n`converter.target_spec.supported_types = [tf.float32]`\r\nthe outputs of 1) and 5) are the same. \r\n@terryheo @abattery can you please have a look? \r\nby the way, tf-nightly-2.5.0.dev20201110 did not work", "@cookingbear thanks for sharing the data for reproducing. We will take a look at them and get back to you.", "> @cookingbear thanks for sharing the data for reproducing. We will take a look at them and get back to you.\r\n\r\ncan you reproduce the error? please let me know, it is kind of tricky problem to us... Thank you\r\n ", "@cookingbear I've reproduce the issue but I'm not sure if this is intended behavior or not.\r\nThe first operator is GATHER and the input is indices.\r\nSince the input matrix is 314 x 384, some values (439, 440, 389, 320) on your inputs are out of index.\r\n\r\nShouldn't you use those values? You might need some preprocessing to your inputs.", "> @cookingbear I've reproduce the issue but I'm not sure if this is intended behavior or not.\r\n> The first operator is GATHER and the input is indices.\r\n> Since the input matrix is 314 x 384, some values (439, 440, 389, 320) on your inputs are out of index.\r\n> \r\n> Shouldn't you use those values? You might need some preprocessing to your inputs.\r\n\r\nSorry for the mistake. The inputs mentioned before have some mismatch with the code above. You can also use the inputs below to reproduce the issue. thanks for your help.\r\n1)\t312, 223, 131, 117,  66, 200, 233, 199, 217, 308, 278, 309, 248, 312, 313\r\n2)\t312, 199,  77, 308, 279, 307, 173, 226, 122, 312, 313\r\n3)\t312, 223, 131, 117,  66, 119,   0, 224, 233, 309, 232, 200, 233, 312, 313\r\n4)\t312, 223, 131, 117,  66, 119,   0, 224, 233, 309, 232, 200, 233, 312, 313\r\n5)\t312, 223, 131, 117,  66, 200, 233, 199, 217, 308, 278, 309, 248, 312, 313", "I can't reproduce the issue with the given inputs.\r\nThe result of input 1 and result 5 is the same.\r\nThe result of input 3 and result 4 is the same.\r\n", "> I can't reproduce the issue with the given inputs.\r\n> The result of input 1 and result 5 is the same.\r\n> The result of input 3 and result 4 is the same.\r\n\r\nhave you sent the inputs into the model according to the given order?", "> have you sent the inputs into the model according to the given order?\r\n\r\nYes, I did.", "> > have you sent the inputs into the model according to the given order?\r\n> \r\n> Yes, I did.\r\n\r\nI just use the checkpoint and the code above to create the model.tflite, and run the code below:\r\n```\r\n        interpreter = tf.lite.Interpreter(\r\n            model_path=\"model.tflite\")\r\n        aa = [[312, 223, 131, 117, 66, 200, 233, 199, 217, 308, 278, 309, 248, 312, 313],\r\n              [312, 199, 77, 308, 279, 307, 173, 226, 122, 312, 313],\r\n              [312, 223, 131, 117, 66, 119, 0, 224, 233, 309, 232, 200, 233, 312, 313],\r\n              [312, 223, 131, 117, 66, 119, 0, 224, 233, 309, 232, 200, 233, 312, 313],\r\n              [312, 223, 131, 117, 66, 200, 233, 199, 217, 308, 278, 309, 248, 312, 313]]\r\n        for line in aa:\r\n            features = tf.convert_to_tensor([line])\r\n            input_details = interpreter.get_input_details()\r\n            output_details = interpreter.get_output_details()\r\n            interpreter.resize_tensor_input(input_details[0]['index'], features.shape)\r\n            interpreter.allocate_tensors()\r\n            interpreter.set_tensor(input_details[0]['index'], features)\r\n            interpreter.invoke()\r\n            features = interpreter.get_tensor(output_details[0]['index'])\r\n            continue\r\n```\r\nThen I can reproduce the issue. by the way, I tested it on centos-release-7-7.1908.0.el7.centos.x86_64 using CPU.\r\nThe output of 1) is \r\n[a.npy.zip](https://github.com/tensorflow/tensorflow/files/5567396/a.npy.zip)\r\nAfter the calculation of 2), 3) and 4), the output of 5) is always changing...\r\n", "@cookingbear, it works well for me.\r\n\r\nI wonder if it's a conversion issue or not. Could you generate tflite model with recent nightly build?\r\nFYI, your a.npy is different with mine.", "the tf nightly version I used is tf-nightly 2.5.0.dev20201119, the converted tflite model is \r\n[model.tflite.zip](https://github.com/tensorflow/tensorflow/files/5570814/model.tflite.zip)\r\n", "Now I can reproduce it with your model. Let me dig more.", "Did you use tf-nightly 2.5.0.dev20201119 to create the FP16 model?", "> Did you use tf-nightly 2.5.0.dev20201119 to create the FP16 model?\r\n\r\nI tested the tflite fp16 model using the code and the ckpt above with tf-nightly 2.5.0.dev20201119. It worked fine using the inputs above. ", "@cookingbear , did you mean the problem is solved with tf-nightly 2.5.0.dev20201119?", "> @cookingbear , did you mean the problem is solved with tf-nightly 2.5.0.dev20201119?\r\n\r\nThe tflite model in fp16 format would be more stable using tf-nightly 2.5.0.dev20201119 and the cases above can be solved. but when I have more tests and find that the fp16 model using tf-nightly 2.5.0.dev20201119 would still show the similar issues.", "Hmm. If you can provide a way to reproduce it, I'll continue to investigate.", "> Hmm. If you can provide a way to reproduce it, I'll continue to investigate.\r\n\r\nI'll try. but the tf-nightly 2.5.0.dev20201119 int8 model can still reproduce the issue using the cases above...", "Could you share the int8 model?", "> the tf nightly version I used is tf-nightly 2.5.0.dev20201119, the converted tflite model is\r\n> [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/5570814/model.tflite.zip)\r\n\r\n@terryheo this is the int8 model", "Oh, I thought it was a fp16 model. Got it.", "> Oh, I thought it was a fp16 model. Got it.\r\n\r\nso it is a conversion problem? how can I solve this?", "I could reproduce the issue with int8 hybrid quantized model.\r\nI'm still root-causing the issue to figure out culprit. I'll get back to you once I have an update.", "I've root caused the issue. Preparing a fix.", "Patch is merged. Nightly build tonight will have the fix.", "Let me close the issue. Please reopen if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44520\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44520\">No</a>\n"]}, {"number": 44519, "title": "need some help building on windows", "body": "**System information**\r\n- OS Platform and Distribution:Windows 7 x64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.4\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.26\r\n- GCC/Compiler version (if compiling from source): MSVC2017\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: K40m 12G\r\n\r\nim trying to compile TF using /LTCG but bazel's def_parser.exe crashes I guess he doesn't understand LTO data in the .lib files, I tried to build TF twice once with LTO and once without and copying the .def files manually but this doesn't prevent bazel from rerunning the def_parser.exe, if I delete def_parser.exe bazel gives an error saying that he is corrupt and will not build anything at all, same problem if I try to modify the def_parser.exe into other exe file :x", "comments": ["I also tried to use def_parser.exe from the newest bazel it didn't work :x", "see also: https://stackoverflow.com/questions/57082617/building-tensorflow-with-ltcg", "@fuckfrancisco,\r\nTensorFlow 1.x is not actively support. Could you please try building TensorFlow v2.3 and check if you are facing the same issue. Thanks!", "my python program don't work with 2 :x", "is it possible to build with cmake ?", "is it possible to build 2 with --config=v1 ?", "is there a file in the build system can allow to add /GL- only to the contrib ?", "cl.exe stucks on infinite loop when compiling file segment_reduction_ops in tensorflow/core/kernels with /arch:AVX or /arch:AVX2\r\nI built a version without AVX and put the .o file in there but bazel overwrites it, is there any way to prevent that ?", "> is it possible to build 2 with --config=v1 ?\r\n\r\nThe ` --config=v1` is used to build TensorFlow 1.x.\r\n\r\n> is it possible to build with cmake ?\r\n\r\nYou'll have to use Bazel to compile TensorFlow. For more information, please take a look at [this guide](https://www.tensorflow.org/install/source_windows). \r\n\r\n> my python program don't work with 2 :x\r\n\r\nIn this case, I'd suggest you to migrate your code to support TensorFlow 2.x as shown in [this guide](https://www.tensorflow.org/guide/migrate\r\n). Thanks!", "realy ? you can convert from v1 to v2 automatic with tf_upgrade_v2 ? that's nice... thank I will try this :)", "> realy ? you can convert from v1 to v2 automatic with tf_upgrade_v2 ? that's nice... thank I will try this :)\r\n\r\n@fuckfrancisco,\r\nAny updates regarding this? Is this still an issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44519\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44519\">No</a>\n"]}, {"number": 44518, "title": "Some variables are not restored only when MirroredStragy is used.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): official docker image <tensorflow/tensorflow:2.3.1-gpu>\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): I've tried every version (>=2.3.0) and I could reproduce this bug in each of them.\r\n- Python version: I could reproduce in 3.6, 3.7, and 3.8\r\n- GPU model and memory: Not relevant. Could reproduce this issue with and without GPU.\r\n\r\n**Description**\r\n`tf.keras.models.Model.load_weights` method doesn't restore the optimizer slot variables only when `MirroredStrategy` is used.\r\n\r\nHere's the minimum code to demonstrate this bug.\r\n```python3\r\nimport tensorflow as tf\r\n\r\n\r\ndef prepare_model():\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Dense(1),\r\n    ])\r\n    model.compile(optimizer='adam', loss='mse')\r\n    model.build([None, 1])\r\n    return model\r\n\r\n\r\ndef print_opt_weights(tag):\r\n    print(tag, list(map(lambda x: x.name, model.optimizer.weights)))\r\n    return\r\n\r\n\r\nlabels = inputs = tf.random.uniform([5, 1])\r\nmodel = prepare_model()\r\nprint_opt_weights('After model creation:')\r\nmodel.fit(x=inputs, y=labels, batch_size=1)\r\nprint_opt_weights('After training:')\r\nmodel.save_weights('save')\r\n\r\nprint('------------Without Distribute------------')\r\nmodel = prepare_model()\r\nprint_opt_weights('After model creation:')\r\nstatus = model.load_weights('save')\r\nprint_opt_weights('After load weight:')\r\n# model.fit(x=inputs, y=labels, batch_size=1)\r\n# print_opt_weights('After retraining:')\r\n# status.assert_consumed()\r\n\r\nprint('------------With Distribute------------')\r\nwith tf.distribute.MirroredStrategy().scope():\r\n    model = prepare_model()\r\n    print_opt_weights('After model creation:')\r\n    status = model.load_weights('save')\r\n    print_opt_weights('After load weight:')\r\n    # model.fit(x=inputs, y=labels, batch_size=1)\r\n    # print_opt_weights('After retraining:')\r\n    # status.assert_consumed()\r\n\r\n\r\n'''\r\nThe commented out lines will make optimizers in\r\n<without distribute> and <with distribute>\r\nhave the same number of weights.\r\n\r\nUpon `load_weights` call, all the slot variables in the optimizer\r\nwill be created when it's not using distribution strategy,\r\nwhile they are not created until the model is actually retrained\r\nwith a distribution strategy in use.\r\n'''\r\n```\r\nThe above snippet will produce the output like below:\r\n```\r\nAfter model creation: []\r\nAfter training: ['Adam/iter:0', 'Adam/dense/kernel/m:0', 'Adam/dense/bias/m:0', 'Adam/dense/kernel/v:0', 'Adam/dense/bias/v:0']\r\n------------Without Distribute------------\r\nAfter model creation: []\r\nAfter load weight: ['dense_1/kernel/m:0', 'dense_1/kernel/v:0', 'dense_1/bias/m:0', 'dense_1/bias/v:0']\r\n------------With Distribute------------\r\nAfter model creation: []\r\nAfter load weight: []\r\n```\r\n\r\nAs we can see from the output, the optimizer slot variables are correctly restored without `MirroredStrategy`, while they are not restored with `MirroredStrategy`.\r\nIn my code, I was calling `assert_consumed` method just after calling `load_weights` to make sure everything is loaded correctly before proceeding to any other operations. And due to this unexpected difference in the behavior, it was crushing when it uses `MirroredStrategy`.\r\n\r\nIn either case (with/without distribution strategy), all the variables are (probably) restored after retraining (calling `fit` again).\r\nSo the easiest solution will be to just ignore this difference, but I want to know what makes this difference and how we can fix this.\r\n\r\nThank you so much for taking the time to take a look at this issue.\r\nAny comments or suggestions will be very helpful.", "comments": ["@nikitamaia Hi! I would like to kindly ask you if you could have some time to take a look at this issue.", "This is (unfortunately) due to some historical constraints that optimizer slot variables are not immediately created in `load_weights` when a strategy is effective. This [comment](https://github.com/tensorflow/tensorflow/blob/bc01b08d2480fe25dec20dd8f307c4b8c1f1478b/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1308) sheds some details. This won't affect correctness as the slot variables will be restored after retraining. If you'd like to do some verification on loading, consider using `assert_existing_objects_matched`.\r\n\r\nI want to see if the constraint can be removed so the behavior is consistent w/ and w/o strategy, will post once there is a conclusion.\r\n\r\n\r\n", "The follow up, it turns out the behavior can't be changed. It is to allow use cases where `load_weights` is put outside distribution strategy scope, so if slot variables are immediately created, they will not be created within the distribution strategy scope. If using distribution strategy, all variables must be created under the scope. See https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#scope.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518\">No</a>\n", "Thank you for taking the time to investigate this.\r\nBut there is one thing that I don't see. What would be a reason to delay variable creation when `load_weights` is called inside a distribution strategy as I show in the example code? I think it is possible to create them immediately in this case and it should, as It would make the behavior consistent.\r\nWhat do you think?", "That's a good call. In fact giving it a second thought, we should be able to allow calling `load_weights` outside the strategy scope and meanwhile creating the slot variable immediately, because the optimizer is created under a scope already and the scope is captured inside the optimizer.\r\n\r\nI'll send out a fix for this soon.", "@ckkuang Glad to hear that! I would really appreciate the fix.", "This should have been fixed at tf-nightly. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518\">No</a>\n", "@ckkuang Thank you so much for the quick fix!!"]}, {"number": 44517, "title": "Custome loop distributed learning examples [proposed Label] comp:dist-start", "body": "Hello,\r\nI saw you provide the latest strategies to write customer loop for distributed learning in [this video](https://www.youtube.com/watch?v=jKV53r9-H14).\r\nHowever I am trying to find examples code in GitHub, but it seems these new style codes are not available.\r\n\r\nWould you please let me know if you have published those codes or not? or any similar ones?\r\n\r\nThank you ", "comments": ["@sakh251 \r\n\r\nPlease, refer this [tutorial](https://www.tensorflow.org/guide/distributed_training), [custom training](https://www.tensorflow.org/tutorials/distribute/custom_training) tutorial and see if it helps you.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44516, "title": "keras preprocessing data_utils.py not working for non-'inferred' labels", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5473994/tf_env.txt)\r\n\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ndata_utils works for labels= 'inferred', but not for any provided list of labels\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work. I changed it as per:\r\n\r\n[data_utils.txt](https://github.com/tensorflow/tensorflow/files/5474032/data_utils.txt)\r\nwhich works as far as I can tell.\r\nIf this hasn't already been fixed in a later version, and if the changes meet with whatever standards you have, feel free to adopt any part of it for future versions.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mabhags,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and the exact sequence of commands that you executed before running into the problem. Thanks!", "I attached the output from script above - detailing my site config.\r\n\r\nHere:\r\n\r\n[test_pp.txt](https://github.com/tensorflow/tensorflow/files/5480819/test_pp.txt)\r\nis a test file to see problem\r\nhere:\r\n![Screenshot from 2020-11-03 22-08-08](https://user-images.githubusercontent.com/73822769/97978581-db7ba680-1e21-11eb-8433-b0a50f5b8d6b.png)\r\nis a picture of the test data directory structure and contents - basically just 4 jpegs in the directory itself and another 4 ineach of two sub-directories\r\nhere:\r\nTensorflow version: 2.3.0\r\nTest using  labels='inferred'\r\nFound 8 files belonging to 2 classes.\r\n\r\nTest using label=[<list of class labels>]\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/mark/Documents/kaggle/input/test_pp.py\", line 38, in <module>\r\n    shuffle=False)\r\n\r\n  File \"/home/mark/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/image_dataset.py\", line 182, in image_dataset_from_directory\r\n    follow_links=follow_links)\r\n\r\n  File \"/home/mark/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/dataset_utils.py\", line 98, in index_directory\r\n    len(labels), len(filenames), directory))\r\n\r\nValueError: Expected the lengths of `labels` to match the number of files in the target directory. len(labels) is 12 while we found 8 files in ./samplesplit.\r\n: are the results of running with the dataset_utils.py\r\nhere:\r\n[dataset_utils.txt](https://github.com/tensorflow/tensorflow/files/5480854/dataset_utils.txt)\r\nis the adjusted dataset_utils I used (apologies but I attached the wrong file last night).\r\nhere:\r\nTensorflow version: 2.3.0\r\nTest using  labels='inferred'\r\nFound 8 files belonging to 2 classes.\r\n\r\nTest using label=[<list of class labels>]\r\nFound 12 files belonging to 5 classes.\r\n: are the results of running with this - which is what I would expect to get based on the input data to the call to image_dataset_from_directory.\r\n\r\nThe inferred results are the same in both cases. \r\n\r\nThe labels with list of classes results are not.\r\nThe main problem is the current dataset_utils is failing to read files from the parent directory - because the subdirs list compiled does not include the parent directory - which I believe it should - because potentially, if using a provided list of labels (and *not* 'inferred') there would be no subdirectories, just files in the (single) parent directory, although handling subdirecttoies too is a bonus just in case.\r\n\r\nThx - I hope this is enough to allow you to investigate now.\r\nCheers, Mark ", "oh - yes I know the REALs and FAKEs would not match between the two calls because of sorted subdir order - it doesn't matter for this test case.", "oops - now I know what Close with comment does...", "Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/2c92d5d9eb037ecd1833a59c7506d60f/44516.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/24cb18f80813d722532e220a78c08fc7/44516-tf-nightly.ipynb#scrollTo=TTmjGpjrkF5m). Please find the attached gist. Thanks!", "@mabhags,\r\nAs per the `labels` argument mentioned in the  [Arguments Section of image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory#arguments), it is mentioned as : \r\n\r\n\r\n> labels | Either \"inferred\" (labels are generated from the directory structure), or **a list/tuple of integer labels of the same size as the number of image files found in the directory**. Labels should be sorted according to the alphanumeric order of the image file paths (obtained via\u00a0**os.walk(directory)**\u00a0in Python).\r\n\r\nThe code works fine if we replace the line of code,\r\n\r\n`validation_labels = ['a','b','b','d','REAL','REAL','REAL','REAL','FAKE','FAKE','FAKE','FAKE']` \r\n\r\nwith\r\n\r\n`validation_labels = ['REAL','REAL','REAL','REAL','FAKE','FAKE','FAKE','FAKE']`\r\n \r\nIt is because, when we execute **`os.walk(val_dir)`** (as mentioned above):\r\n\r\n```python\r\nimport os\r\n\r\nfor EachFile in os.walk(val_dir):\r\n  print(EachFile)\r\n```\r\nIt will not list the Image Files which are not in Sub Folders. and will only shows the Image Files which are in the Sub Folders. It is shown below:\r\n\r\n```python\r\n('./samplesplit', ['REAL', 'FAKE'], ['test_image4.jpg', 'test_image2.jpg', 'test_image3.jpg', 'test_image1.jpg'])\r\n('./samplesplit/REAL', [], ['real_image3.jpg', 'real_image1.jpg', 'real_image4.jpg', 'real_image2.jpg'])\r\n('./samplesplit/FAKE', [], ['fake_image1.jpg', 'fake_image4.jpg', 'fake_image3.jpg', 'fake_image2.jpg'])\r\n```\r\nPlease find the [Gist of the Complete working code](https://colab.research.google.com/gist/rmothukuru/c4c1e1a7a9e39ca9bf0a7def05087d5e/44516.ipynb).\r\n\r\nHope it is clear now. Thanks!", "@mabhags,\r\nCan you please respond to the above comment. Thanks! ", "My interpretation of the documentation, and of the way os.walk works is that for the non-inferred option, all files in the specified directory are traversed, regardless of subdirectories. In a likely scenario of having source images not distributed into a set of class directories - which happens to be my situation - there is no reason for any directory structure beyond a single level. So in this case all 12 images would likely all be at the same directory level, there would be no REAL or FAKE subdirs - and the preprocessor as it stands would then pick up zero files - which is what happened at my first attempt - I only added the subdirs to try and figure out what it was up to.", "@mabhags,\r\nUnfortunately, that is not how it is designed. In your case, you can create a `Dataframe` with the \r\n\r\n1. `Paths of Images` as one Column and \r\n2. `Labels` as another column \r\n\r\nand then you can use **`ImageDataGenerator.flow_from_dataframe`**. \r\n\r\nThanks!", "OK, but either the documentation, or the code needs to change because currently they do not match each other.", "@mabhags,\r\nCan you please let us know what exactly in the documentation needs to be changed so that we can do it and close this issue? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44516\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44516\">No</a>\n"]}, {"number": 44514, "title": "Fix project name for GPU ubuntu builds", "body": "", "comments": []}, {"number": 44513, "title": "crashed at TfLiteInterpreterCreate ", "body": "- iPhone11 pro\r\n- Podfile\r\npod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'\r\npod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['Metal']\r\n\r\n- crashed at TfLiteInterpreterCreate.\r\nMy custom model is used at StyleTranfer demo project.\r\n\r\nguard let cInterpreter = TfLiteInterpreterCreate(model.cModel, cInterpreterOptions) else {\r\n--> Thread 8: EXC_BAD_ACCESS (code=1, address=0x0)\r\n\r\nThread 8 Queue : org.tensorflow.examples.lite.style_transfer (serial)\r\n#0\t0x00000001f0bce618 in _platform_memmove ()\r\n#1\t0x00000001007d406c in ___lldb_unnamed_symbol7040$$TFL Style Transfer ()\r\n#2\t0x00000001007c82e4 in ___lldb_unnamed_symbol6806$$TFL Style Transfer ()\r\n#3\t0x00000001007d0f90 in ___lldb_unnamed_symbol6968$$TFL Style Transfer ()\r\n#4\t0x00000001007c3744 in ___lldb_unnamed_symbol6746$$TFL Style Transfer ()\r\n#5\t0x00000001007c4304 in ___lldb_unnamed_symbol6748$$TFL Style Transfer ()\r\n#6\t0x00000001007b94bc in ___lldb_unnamed_symbol6574$$TFL Style Transfer ()\r\n#7\t0x0000000100429cfc in ___lldb_unnamed_symbol129$$TFL Style Transfer ()\r\n#8\t0x0000000100429924 in ___lldb_unnamed_symbol128$$TFL Style Transfer ()\r\n#9\t0x0000000100429678 in ___lldb_unnamed_symbol127$$TFL Style Transfer ()\r\n#10\t0x00000001007b9420 in ___lldb_unnamed_symbol6573$$TFL Style Transfer ()\r\n#11\t0x000000010042cc80 in ___lldb_unnamed_symbol169$$TFL Style Transfer ()\r\n#12\t0x00000001005638fc in ___lldb_unnamed_symbol2108$$TFL Style Transfer ()\r\n#13\t0x0000000100524554 in ___lldb_unnamed_symbol1773$$TFL Style Transfer ()\r\n#14\t0x000000010052431c in TfLiteInterpreterCreate ()\r\n#15\t0x000000010040e854 in Interpreter.init(modelPath:options:delegates:) at /Users/user/git/examples-master/lite/examples/style_transfer/ios/Pods/TensorFlowLiteSwift/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:98\r\n#16\t0x000000010040e144 in Interpreter.__allocating_init(modelPath:options:delegates:) ()\r\n#17\t0x00000001003fe3a0 in closure #1 in static Inpainting.newInstance(inpaintingModel:useMetalDelegate:completion:) at /Users/user/git/examples-master/lite/examples/style_transfer/ios/StyleTransfer/Inpainting.swift:97\r\n#18\t0x00000001003fee34 in thunk for @escaping @callee_guaranteed () -> () ()\r\n#19\t0x000000010096bb68 in _dispatch_call_block_and_release ()\r\n#20\t0x000000010096d5f0 in _dispatch_client_callout ()\r\n#21\t0x0000000100974fa8 in _dispatch_lane_serial_drain ()\r\n#22\t0x0000000100975cb4 in _dispatch_lane_invoke ()\r\n#23\t0x0000000100981e38 in _dispatch_workloop_worker_thread ()\r\n#24\t0x00000001f0bd4908 in _pthread_wqthread ()\r\n", "comments": ["@jucysoft Can you please create a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "this issue still persists. please see this project [Link](https://github.com/sdgbraincraft/Tensorflow-Image-Segmentation). I cloned the project from examples.", "When  a tflite model includes a TransposeConv node, this issue happens. Could you test it?", "This issue was resolved at nightly build version by khanh.", "Still it is crashing at my end. @jucysoft can you guide me how to resolve this?", "I just used the nightly build version of at that time. no other resolution.\n\nThank you,\n\n> 2021. 5. 22. \uc624\ud6c4 2:17, Nimish Patel ***@***.***> \uc791\uc131:\n> \n> \ufeff\n> Still it is crashing at my end. @jucysoft can you guide me how to resolve this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "It is crashing here.\r\n![image](https://user-images.githubusercontent.com/20941414/119216692-c8e16a00-baf2-11eb-9787-48cb43a69003.png)\r\n\r\n`pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'`\r\n\r\nI have used this. Is it fine @jucysoft?", "> this issue still persists. please see this project [Link](https://github.com/sdgbraincraft/Tensorflow-Image-Segmentation). I cloned the project from examples.\r\n\r\nDid you solve this issue?", "I ran the my old project with nightly version, but current nightly version reproduced this error. so, I changed pod file like following. It resolved this error.\n\npod 'TensorFlowLiteSwift', '~> 2.3.0', :subspecs => ['Metal']\n\n\n> 2021. 5. 22. \uc624\ud6c4 3:20, Nimish Patel ***@***.***> \uc791\uc131:\n> \n> \ufeff\n> this issue still persists. please see this project Link. I cloned the project from examples.\n> \n> Did you solve this issue?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "> I ran the my old project with nightly version, but current nightly version reproduced this error. so, I changed pod file like following. It resolved this error. pod 'TensorFlowLiteSwift', '~> 2.3.0', :subspecs => ['Metal']\r\n> [\u2026](#)\r\n> 2021. 5. 22. \uc624\ud6c4 3:20, Nimish Patel ***@***.***> \uc791\uc131: \ufeff this issue still persists. please see this project Link. I cloned the project from examples. Did you solve this issue? \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or unsubscribe.\r\n\r\nThanks. Let me try it right now.", "> I ran the my old project with nightly version, but current nightly version reproduced this error. so, I changed pod file like following. It resolved this error. pod 'TensorFlowLiteSwift', '~> 2.3.0', :subspecs => ['Metal']\r\n> [\u2026](#)\r\n> 2021. 5. 22. \uc624\ud6c4 3:20, Nimish Patel ***@***.***> \uc791\uc131: \ufeff this issue still persists. please see this project Link. I cloned the project from examples. Did you solve this issue? \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or unsubscribe.\r\n\r\n....\r\nhi, I'm facing the same [EXC_BAD_ACCESS error](https://github.com/tensorflow/tensorflow/issues/44513#issue-734202410)\r\n\r\nand I tried changing the pod file, but there is new error. Lines of \r\n\" TensorFlow Lite Error: Unsupported data type 13 in tensor\"  followed by \r\n\"Failed to create the interpreter with error: Failed to create the interpreter.\"\r\n\r\n<img width=\"483\" alt=\"Screenshot 2021-05-31 at 6 28 59 PM\" src=\"https://user-images.githubusercontent.com/1277150/120180227-1621b200-c23e-11eb-8fb0-b09d7f285722.png\">\r\n\r\nany help? did changing the pod file work you?", "> > I ran the my old project with nightly version, but current nightly version reproduced this error. so, I changed pod file like following. It resolved this error. pod 'TensorFlowLiteSwift', '~> 2.3.0', :subspecs => ['Metal']\r\n> > [\u2026](#)\r\n> > 2021. 5. 22. \uc624\ud6c4 3:20, Nimish Patel _**@**_.***> \uc791\uc131: \ufeff this issue still persists. please see this project Link. I cloned the project from examples. Did you solve this issue? \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or unsubscribe.\r\n> \r\n> ....\r\n> hi, I'm facing the same [EXC_BAD_ACCESS error](https://github.com/tensorflow/tensorflow/issues/44513#issue-734202410)\r\n> \r\n> and I tried changing the pod file, but there is new error. Lines of\r\n> \" TensorFlow Lite Error: Unsupported data type 13 in tensor\" followed by\r\n> \"Failed to create the interpreter with error: Failed to create the interpreter.\"\r\n> \r\n> <img alt=\"Screenshot 2021-05-31 at 6 28 59 PM\" width=\"483\" src=\"https://user-images.githubusercontent.com/1277150/120180227-1621b200-c23e-11eb-8fb0-b09d7f285722.png\">\r\n> \r\n> any help? did changing the pod file work you?\r\n\r\nNope. But I changed the approch of implementation. I was using custom code to classify. But now I am using TensorFlowLiteTaskText for Text classification in project like this,\r\n\r\n```\r\npod 'TensorFlowLiteSwift'\r\npod 'TensorFlowLiteTaskText', '~> 0.0.1-nightly'\r\n```\r\n\r\nTensorflow released TensorFlowLiteTaskText prereleased version.\r\nCheck this examples here\r\n[https://github.com/tensorflow/examples/tree/master/lite/examples](url)\r\nI hope this might be useful for you. \r\nThanks"]}, {"number": 44512, "title": "TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'", "body": "**System information**\r\n- OS Platform and Distribution :CentOS Linux release 7.7.1908\r\n-TensorFlow version:2.3.0\r\n\r\nI try to convert [the tensorflow offical image caption model ](https://www.tensorflow.org/tutorials/text/image_captioning?hl=en)to TFLite model.Here is the[ gist](https://colab.research.google.com/gist/DavidInWuhanChina/f83e3e11009211f3469436bbc069b18a/43753.ipynb).\r\n\r\nI try to convert the `tf.keras.Model `'s encoder and decoder model as following:\r\n\r\n```\r\nencoder_converter = tf.lite.TFLiteConverter.from_keras_model(encoder)\r\ndecoder_converter = tf.lite.TFLiteConverter.from_keras_model(decoder)\r\n\r\nencoder_model = encoder_converter.convert()\r\ndecoder_model = decoder_converter.convert()\r\n```\r\n\r\n\r\nbut the eorror is \r\n\r\n```\r\nINFO:tensorflow:Assets written to: /tmp/tmpgvx51gsa/assets\r\n\r\nINFO:tensorflow:Assets written to: /tmp/tmpgvx51gsa/assets\r\n\r\n---------------------------------------------------------------------------\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n<ipython-input-55-9f0e9ba5bb99> in <module>()\r\n      2 decoder_converter = tf.lite.TFLiteConverter.from_keras_model(decoder)\r\n      3 encoder_model = encoder_converter.convert()\r\n----> 4 decoder_model = decoder_converter.convert()\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    300   def wrapper(*args, **kwargs):\r\n    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 302       return func(*args, **kwargs)\r\n    303 \r\n    304   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\r\n\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ae5ce7da04942b487419b8bf565977a1/44512.ipynb#scrollTo=1nVb-qLe0GBL&line=3&uniqifier=1). Thanks!", "@abattery @karimnosseir to help take a look? Thanks", "Hi Can you try export as saved model and try the saved_model conversion?\r\n\r\nthanks", "> \r\n> \r\n> Hi Can you try export as saved model and try the saved_model conversion?\r\n> \r\n> thanks\r\nI have tried export as saved model.But the same problem came.[gist](https://colab.research.google.com/gist/DavidInWuhanChina/1d83802584a04e2f81ebebfbdfe763ba/44512.ipynb)\r\n\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\r\n", "This failure is happening at the the saved model exporter. @k-w-w @qlzh727 could you take a look at the keras model exporting failure?", "Is there anyone answer the question?", "@DavidInWuhanChina \r\nIs this still an issue", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44512\">No</a>\n"]}, {"number": 44511, "title": "TFLite memory not released", "body": "\r\n**Describe the current behavior**\r\n\r\nI am running a TFLite model on a edge device, and what we have observed is that when we invoke the first time, the RAM usage went up 30-40MB which is correct and that is the right amount of RAM needed to do inference. However, the memory does not get released after that. The second and many more times I run invoke, the overall RAM usage stays the same and there is no leak, but the overall RAM usage never goes down. Is there anything I need to do to release the RAM?\r\n\r\n", "comments": ["Do I need to call `ReleaseNonPersistentMemory` or something like that? ", "@yunchaogong \r\nPlease share simple stand alone code to replicate the issue reported along with the tf version.", "It is not really an issue. I suspect I might need to explicitly call something to release the memory? Or is it suppose to be like when I execute AllocateTensors() or Invoke(), it just allocates RAM and does not free it and reuse next time?", "In particular, if I have a stream of infinity number of images coming in, should I have \r\n```\r\n        std::unique_ptr<tflite::FlatBufferModel> mModel;\r\n        std::unique_ptr<tflite::Interpreter> mInterpreter;\r\n```\r\nThese two always allocated as global var, or should I allocate a new Interpreter each time when I see a new image?\r\n", "Updates?", "> These two always allocated as global var, or should I allocate a new Interpreter each time when I see a new image?\r\n\r\nIt depends on how frequently you need to run inference, and how sensitive you are to startup latency. There is a cost to constructing a new interpreter for each inference (often depending on the model size), which you should try measuring. If you destroy the model and interpreter, that will free up the memory, but if loading the model and interpreter is expensive, that may not be a desirable tradeoff, and it's better to keep those objects resident in memory for faster inference.", "> ReleaseNonPersistentMemory\r\n\r\nThis can free up memory, but you'll need to call AllocateTensors() again before you run inference, which again could have a non-trivial cost (which you'll want to measure).", "@yunchaogong  i have same problem. did you release memory, how can you do that", "@NALanhnt2 @yunchaogong did you solve this? I'm encountering it now. The issue for me is not whether to constuct and destruct each time or not. Rather, even if I just create and destroy once, my situation is described at the top of this issue."]}, {"number": 44509, "title": "```tf.keras.callbacks.EarlyStopping``` doesn't set correct mode when ```monitor=\"val_auc\",mode=\"auto\"```", "body": "**Describe the current behavior**\r\n[https://colab.research.google.com/drive/1y-gYAIMHYokd9K-OUDXuD3PGTh4qE1xN?usp=sharing](https://colab.research.google.com/drive/1y-gYAIMHYokd9K-OUDXuD3PGTh4qE1xN?usp=sharing)\r\n```\r\nearly_stopping = tf.keras.callbacks.EarlyStopping(\r\n    monitor='val_auc',\r\n    verbose=1,\r\n    )\r\n```\r\nis equivalent to\r\n```\r\nearly_stopping = tf.keras.callbacks.EarlyStopping(\r\n    monitor='val_auc',\r\n    verbose=1,\r\n    mode=\"min\"\r\n    )\r\n```\r\n**Describe the expected behavior**\r\nIt should be equivalent to\r\n```\r\nearly_stopping = tf.keras.callbacks.EarlyStopping(\r\n    monitor='val_auc',\r\n    verbose=1,\r\n    mode=\"max\"\r\n    )\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/afcaddf04d01641d4cbacb65b36ea51e/44509.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2a13236cc5f15233590e311ae622ef0f/44509-tf-nightly.ipynb). Please find the attached gist. Thanks!", "https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/callbacks.py#L1640-L1643\r\n\r\nLooks like for `acc` the `mode='max'` and for monitoring other quantities it is set to `'min'`.\r\nI think you have to manually set the `mode='max'` here for your case.", "> https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/callbacks.py#L1640-L1643\r\n> \r\n> Looks like for `acc` the `mode='max'` and for monitoring other quantities it is set to `'min'`.\r\n> I think you have to manually set the `mode='max'` here for your case.\r\n\r\nok\uff0cbut why not change \r\n```Python\r\n      if 'acc' in self.monitor:\r\n        self.monitor_op = np.greater\r\n      else:\r\n        self.monitor_op = np.less\r\n```\r\nto\r\n```Python\r\n      if 'acc' in self.monitor or 'auc' in self.monitor:\r\n        self.monitor_op = np.greater\r\n      else:\r\n        self.monitor_op = np.less\r\n```\r\n?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44509\">No</a>\n", "This is fixed with tf nightly version `2.5.0-dev20201029`  and also with TF 2.4\r\n(`'val_acc'` is monitored for max value)\r\nSee [gist](https://colab.research.google.com/gist/ymodak/f7925391d77c532b56795fe57173fbad/44509-tf-nightly.ipynb) for reference.", "> This is fixed with tf nightly version `2.5.0-dev20201029` and also with TF 2.4\r\n> (`'val_acc'` is monitored for max value)\r\n> See [gist](https://colab.research.google.com/gist/ymodak/f7925391d77c532b56795fe57173fbad/44509-tf-nightly.ipynb) for reference.\r\n\r\nI don't think you fix it . In fact , it stops at the first epoch because this [issue](https://github.com/tensorflow/tensorflow/issues/49227) . Since keras is moved to its own repo , I will try to raise a PR to solve it . "]}, {"number": 44508, "title": "Building TensorFlow Lite library with tensorflow ops failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to build a TensorFlow Lite C++ shared library that supports full tensorflow ops. I'm using cross-compile, buiding on PC and the target platform is arm64. But the build process failed. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nModify tensorflow/lite/BUILD to include \"//tensorflow/lite/delegates/flex:delegate\" in the last part: deps of tflite_cc_shared_object. \r\nThen run: \r\nbazel build --config=elinux_aarch64 --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/xxx/tensorflow/tensorflow/core/lib/strings/BUILD:62:1: C++ compilation of rule '//tensorflow/core/lib/strings:proto_text_util' failed (Exit 1)\r\nIn file included from external/com_google_protobuf/src/google/protobuf/stubs/port.h:49,\r\n                 from external/com_google_protobuf/src/google/protobuf/stubs/macros.h:34,\r\n                 from external/com_google_protobuf/src/google/protobuf/stubs/logging.h:34,\r\n                 from external/com_google_protobuf/src/google/protobuf/arena_impl.h:40,\r\n                 from external/com_google_protobuf/src/google/protobuf/arena.h:53,\r\n                 from ./tensorflow/core/platform/protobuf.h:33,\r\n                 from ./tensorflow/core/lib/strings/proto_text_util.h:22,\r\n                 from tensorflow/core/lib/strings/proto_text_util.cc:16:\r\nexternal/com_google_protobuf/src/google/protobuf/port_def.inc:74:2: error: #error PROTOBUF_DEPRECATED was previously defined\r\n #error PROTOBUF_DEPRECATED was previously defined\r\n  ^~~~~\r\n```\r\nThe full log is too long and I omit the middle part... And the last part is: \r\n```\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 24.871s, Critical Path: 24.02s\r\nINFO: 81 processes: 81 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nCould you help me analyze this? Many thanks. ", "comments": ["Hi, would anyone please have a look :)", "@bittergourd1224 Could you please try on latest stable version of TF 2.6.0 and refer to [this ](https://www.tensorflow.org/lite/performance/delegates)  documentations .Please let us know if this is still an issue?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44508\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44508\">No</a>\n"]}, {"number": 44507, "title": "Add an additional arg to tf.experimental.register_filesystem_plugin to take plugin function name", "body": "This PR proposes to add an additional arg `plugin_function` to\r\ntf.experimental.register_filesystem_plugin so that it is possible to load\r\na file system plugin with a different function name, i.e., a name other than\r\ndefault `TF_InitPlugin` (e.g., `TF_InitPlugin_AzureFileSystem`, `TF_InitPlugin_HTTPFileSystem`, etc).\r\n\r\nThere are several reasons for a need to have the flexibility of using a different function name.\r\nBut the biggest reason is that, by allowing passing function name as the parameter, it is possible\r\nto bundle several file systems into one shared object library and reduce the total size of the\r\npackage (e.g., one `.so` file with multiple file system support for Azure FS, HTTP FS, etc).\r\n\r\nSince thie PR will only add an optional args, users may still pass the `library_pathname`\r\nonly (and `TF_InitPlugin` function name will automatically be used).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Note though that you can load all filesystems in a single `TF_InitPlugin`, you don't need separate functions.\r\n\r\nThis will increase cognitive load on users, now they'll need to know which plugin function to call to register a filesystem instead of knowing that they just need to call the load python call and get all filesystems from the dso loaded.", "> Note though that you can load all filesystems in a single `TF_InitPlugin`, you don't need separate functions.\r\n\r\nThanks @mihaimaruseac for the pointers. I didn't know that before. Let me take a look and see if it works. If works I think this PR might potentially be closed.", "You control how many schemes are registered in a DSO: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/plugins/posix/posix_filesystem.cc;l=454-458;drc=f64e839687b6129dfe2a9e21ef965593c7f3a3c2\r\n\r\nFor each scheme you can then provide different operations. In the example they are the same, but there's no reason why some cannot change.", "Thanks @mihaimaruseac for the help. I will close this PR."]}, {"number": 44506, "title": "Training with TPU Out of range ... end of sequence at beginning of secund epoch", "body": "**System information**\r\n- Have I written custom code : Yes\r\n- Version of Tensorflow: 2.3\r\n- OS Platform and Distribution : Kaggle TPU\r\n- TPU model  :v3-8\r\n- dataset: [Raven_center_single](https://www.kaggle.com/datendnker/raven-center-single)\r\n\r\n**Describe the current behavior**\r\nMy model is training and evaluating the first epoch as expected. At the beginning of second epoch Error raises: OutOfRangeError \r\n\r\n**Describe the expected behavior**\r\nIt should train Without running out of data\r\n\r\nHere you can find the [Kaggle Notebook](https://www.kaggle.com/datendnker/notebook045427c7cb)\r\n\r\n\r\n**Complete error log** \r\n---------------------------------------------------------------------------\r\nOutOfRangeError                           Traceback (most recent call last)\r\n<ipython-input-11-c2c56d1c82d6> in <module>\r\n      5           validation_data=val_dataset,\r\n      6           validation_steps=(config.entrys * config.test_size)/ config.batch_size,\r\n----> 7           callbacks=[WandbCallback()])\r\n      8 \r\n      9 model.save_weights('./RAVEN-center-single_large_model.h5', overwrite=True)\r\n\r\n/opt/conda/lib/python3.7/site-packages/wandb/integration/keras/keras.py in new_v2(*args, **kwargs)\r\n    118             for cbk in cbks:\r\n    119                 set_wandb_attrs(cbk, val_data)\r\n--> 120         return old_v2(*args, **kwargs)\r\n    121 \r\n    122     training_arrays.orig_fit_loop = old_arrays\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1135           epoch_logs.update(val_logs)\r\n   1136 \r\n-> 1137         callbacks.on_epoch_end(epoch, epoch_logs)\r\n   1138         training_logs = epoch_logs\r\n   1139         if self.stop_training:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    413       else:\r\n    414         if numpy_logs is None:  # Only convert once.\r\n--> 415           numpy_logs = tf_utils.to_numpy_or_python_type(logs)\r\n    416         callback.on_epoch_end(epoch, numpy_logs)\r\n    417 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)\r\n    535     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n    536 \r\n--> 537   return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n    538 \r\n    539 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)\r\n    531   def _to_single_numpy_or_python_type(t):\r\n    532     if isinstance(t, ops.Tensor):\r\n--> 533       x = t.numpy()\r\n    534       return x.item() if np.ndim(x) == 0 else x\r\n    535     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1061     \"\"\"\r\n   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1065 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1029       return self._numpy_internal()\r\n   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1032 \r\n   1033   @property\r\n\r\n/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nOutOfRangeError: 9 root error(s) found.\r\n  (0) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[strided_slice_18/_202]]\r\n  (1) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[tpu_compile_succeeded_assert/_16956405043851590776/_5/_185]]\r\n  (2) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n  (3) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[Const_4/_212]]\r\n  (4) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[cond_14/switch_pred/_140/_90]]\r\n  (5) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[Pad_11/paddings/_154]]\r\n  (6) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[cond_15/switch_pred/_151/_92]]\r\n  (7) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[strided_slice_15/_192]]\r\n  (8) Out of range: {{function_node __inference_train_function_5573}} End of sequence\r\n\t [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]\r\n\t [[Shape_22/_124]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n", "comments": ["@datend3nker \r\n\r\nI tried in colab and could not able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b303727cf14babd875c76cff71f0dd95/untitled480.ipynb).Please, help with the reproducible code. Thanks!", "Simplified the code [here](https://colab.research.google.com/gist/datend3nker/e0448ed4bf48edd52e35ed35177f94bb/untitled480.ipynb). Colab returns more information about the error. Apparently I made a mistake with the steps per epoch. So case closed ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44506\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44506\">No</a>\n", "@datend3nker could I ask you how you managed to fix this error? I am experiencing this error on the kaggle TPUs aswell. I have my validation steps per epoch set to the ceiling of the number of validation steps / batchsize. This would be the number of batches sent through the system correct? or the erro in this code?", "@kmcguigan1, since it has been a long time, I am not really into this topic. I can give you a [notebook ](https://www.kaggle.com/code/datendnker/raven-simple-mlp), which does train the same model and does not fail. Accuracy in this case is not relevant. Good luck in finding the parts, that unbreak  your code "]}, {"number": 44505, "title": "tensorflow.keras.callback.TensorflowCallback turns off recording for tf.summary, and stops other callbacks from recording summaries", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (colab)\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3+\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n### Steps to reproduce\r\nAny custom keras callback that writes using tf.summary at batch_start or batch_end methods will have their `tf.summary` writing methods such as `tf.summary.scalar` not write anything to the protobuf.\r\n### Root cause\r\n`tensorflow.keras.callback.TensorflowCallback` modifies the private summary field of tensorflow.python.ops.summary_ops_v2:\r\n\r\n[summary_state = summary_ops_v2._summary_state  # pylint: disable=protected-access](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L2059)\r\n\r\n\r\n**Describe the expected behavior**\r\nTensorflowCallback should not impact the recording of other callbacks. Instead of updating the global summary_state, it should locally guard the writing of summaries.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1UPm1m3kNZCFaJZHHYT5rbx1Xcx0VZ7gR\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nCustom callback which can show the issue:\r\n```python\r\nclass CustomCallback(tf.keras.callbacks.Callback):\r\n    def __init__(self):\r\n      self.step = 1\r\n    \r\n    def on_train_batch_end(self, batch, logs=None):\r\n        if tf.summary.should_record_summaries():\r\n          print(f\"Warning!!!!!!!!!!!!: can only record summaries for step: {self.step}\")\r\n        self.step += 1\r\n\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq=100)\r\ncustom_callback = CustomCallback()\r\nmodel.fit(x=x_train, \r\n            y=y_train, \r\n            epochs=epochs, \r\n            validation_data=(x_test, y_test), \r\n            callbacks=[tensorboard_callback, custom_callback])\r\n\r\n```\r\nThe above code segment outputs:\r\n\r\n```bash\r\n  95/1875 [>.............................] - ETA: 7s - loss: 0.9137 - accuracy: 0.6734\r\nWarning!!!!!!!!!!!!: can record summaries for step: 100\r\n 191/1875 [==>...........................] - ETA: 7s - loss: 0.7903 - accuracy: 0.7202\r\nWarning!!!!!!!!!!!!: can record summaries for step: 200\r\n 292/1875 [===>..........................] - ETA: 6s - loss: 0.7092 - accuracy: 0.7464\r\nWarning!!!!!!!!!!!!: can record summaries for step: 300\r\n 396/1875 [=====>........................] - ETA: 6s - loss: 0.6668 - accuracy: 0.7633\r\nWarning!!!!!!!!!!!!: can record summaries for step: 400\r\n 493/1875 [======>.......................] - ETA: 5s - loss: 0.6315 - accuracy: 0.7762\r\nWarning!!!!!!!!!!!!: can record summaries for step: 500\r\n 595/1875 [========>.....................] - ETA: 5s - loss: 0.6107 - accuracy: 0.7834\r\nWarning!!!!!!!!!!!!: can record summaries for step: 600\r\n 692/1875 [==========>...................] - ETA: 4s - loss: 0.5928 - accuracy: 0.7891\r\nWarning!!!!!!!!!!!!: can record summaries for step: 700\r\n 792/1875 [===========>..................] - ETA: 4s - loss: 0.5800 - accuracy: 0.7931\r\nWarning!!!!!!!!!!!!: can record summaries for step: 800\r\n 892/1875 [=============>................] - ETA: 4s - loss: 0.5656 - accuracy: 0.7972\r\nWarning!!!!!!!!!!!!: can record summaries for step: 900\r\n 995/1875 [==============>...............] - ETA: 3s - loss: 0.5549 - accuracy: 0.8015\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1000\r\n1092/1875 [================>.............] - ETA: 3s - loss: 0.5439 - accuracy: 0.8059\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1100\r\n1194/1875 [==================>...........] - ETA: 2s - loss: 0.5351 - accuracy: 0.8083\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1200\r\n1293/1875 [===================>..........] - ETA: 2s - loss: 0.5289 - accuracy: 0.8101\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1300\r\n1388/1875 [=====================>........] - ETA: 2s - loss: 0.5231 - accuracy: 0.8121\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1400\r\n1490/1875 [======================>.......] - ETA: 1s - loss: 0.5148 - accuracy: 0.8151\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1500\r\n1591/1875 [========================>.....] - ETA: 1s - loss: 0.5102 - accuracy: 0.8163\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1600\r\n1686/1875 [=========================>....] - ETA: 0s - loss: 0.5060 - accuracy: 0.8174\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1700\r\n1796/1875 [===========================>..] - ETA: 0s - loss: 0.5002 - accuracy: 0.8193\r\nWarning!!!!!!!!!!!!: can record summaries for step: 1800\r\n```", "comments": ["@ashahab,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'model' is not defined`.\r\n\r\nAlso, I do not have access to the Colab notebook you have provided. Could you please provide the required permissions to view the files. Thanks! ", "@amahendrakar  thanks for looking, I've fixed the access issue in the colab notebook. Please try to run it. It is a toy example created to demonstrate the issue.", "@amahendrakar Have you been able to reproduce the issue?", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4b3d647a0cd6dfe8a8c64ccc6faa42ba/44505-tf-nightly.ipynb). Thanks!", "We see a 403 error on nightly [2.6.0-dev20210419], please find the [gist here](https://colab.research.google.com/gist/Saduf2019/3ec3ad6b49999b76ffc07a9a74a95ea4/untitled589.ipynb)", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/55f8fee139b555d5fc30f17de994736d/44505.ipynb). Thanks!", "Hi @ashahab! I could not replicate this issue in [TF 2.7](https://colab.sandbox.google.com/gist/mohantym/72530de7578e09655aa81aacde3f63ff/44505.ipynb#scrollTo=bjsyFTE6uTy0) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44505\">No</a>\n"]}]