[{"number": 53555, "title": "quantization model failed", "body": "### 1. System information\r\n\r\n- Windows10\r\n- TensorFlow 2.7\r\n- \r\n\r\n### 2. Code\r\n\r\nYOLOV5 provides  tensorflow int8 optimization failure\r\n\r\n\r\n```\r\n# keras_model I wrote is no problem \r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.target_spec.supported_types = [tf.float16]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nif int8:\r\n      dataset = LoadImages(check_dataset(data)['train'], img_size=imgsz, auto=False)  # representative data\r\n      converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib)\r\n      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n      converter.target_spec.supported_types = []\r\n      converter.inference_input_type = tf.uint8  # or tf.int8\r\n      converter.inference_output_type = tf.uint8  # or tf.int8\r\n      converter.experimental_new_quantizer = False\r\n      f = str(file).replace('.pt', '-int8.tflite')\r\n\r\ntflite_model = converter.convert()\r\nopen(f, \"wb\").write(tflite_model)\r\nLOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\r\n```\r\n\r\n\r\nTensorFlow Lite: export failure: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (16 != 1)Node number 3 (CONV_2D) failed to prepare.\r\n\r\n", "comments": ["Hi @prio158 ! Could you please check this[ thread](https://stackoverflow.com/a/64616086/11530462) for answer?", "> Hi @prio158 ! Could you please check this[ thread](https://stackoverflow.com/a/64616086/11530462) for answer?\r\n\r\nthank you!  That's not the answer I'm looking for, Can TensorFlow Lite quantize Depth separable convolution?", "Hi @sachinprasadhs ! Could you please look at this issue ?", "I found that whenever I used group convolution in the model, I got similar errors", "Group convolution for TFLite has not been implemented yet, you can follow [this](https://github.com/tensorflow/tensorflow/issues/40044) issue for more details and some workaround. Thanks!", "> Group convolution for TFLite has not been implemented yet, you can follow [this](https://github.com/tensorflow/tensorflow/issues/40044) issue for more details and some workaround. Thanks!\r\n\r\nThank you, I replace  \"keras.layers.Conv2D(out_channels , groups=out_channels )\"  with  \"keras.layers.DepthwiseConv2D\",  quantization  success !  but the accuracy of the quantized model is 0\u3002", "Refer [this](https://stackoverflow.com/questions/62015923/) issue for incorrect accuracy for the quanitized model.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53555\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53555\">No</a>\n"]}, {"number": 53553, "title": "Add folder pattern for AddOp and MulOp", "body": "Add folder pattern:\r\n* mhlo.add(%0, 0) -> %0\r\n* mhlo.mul(%0, 1) -> %0", "comments": ["> I was going to ask about the equivalent of https://llvm.discourse.group/t/canonicalization-of-x-0-0-in-tosa/4760 , but I see for HLOs we don't consider this.\r\n\r\nThere are some models lowered from tf dialect or torch_xla, they are not lowered from tosa. So I think these folders are meaningful."]}, {"number": 53551, "title": "[MHLO]: improve BroadcastInDim canonicalize", "body": "Simplify BroadcastInDim to Reshape\r\nSimplify BroadcastInDim to Transpose\r\nSimplify consecutive two BroadcastInDim to one", "comments": []}, {"number": 53550, "title": "I failed  to install   tflite-model-maker", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 18):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to install   tflite-model-maker  ,   i use \"pip3 install tflite-model-maker  \" or  \"pip install tflite-model-maker  \" \r\nprint:\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nspyder 3.3.6 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\r\nspyder 3.3.6 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\r\nconda 4.10.1 requires ruamel_yaml_conda>=0.11.14, which is not installed.\r\nastroid 2.3.1 requires typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\", which is not installed.\r\nspleeter 1.5.0 requires librosa==0.7.2, but you have librosa 0.8.1 which is incompatible.\r\nspleeter 1.5.0 requires pandas==0.25.1, but you have pandas 1.3.5 which is incompatible.\r\nspleeter 1.5.0 requires tensorflow==1.15.0, but you have tensorflow 2.7.0 which is incompatible.\r\nastroid 2.3.1 requires six==1.12, but you have six 1.16.0 which is incompatible.\r\nastroid 2.3.1 requires wrapt==1.11.*, but you have wrapt 1.13.3 which is incompatible.\r\n\r\n", "comments": ["\u90ae\u4ef6\u5df2\u6536\u5230\uff01", "Hi @chensisi0730! Could you try again with Python 3.8 ? Please make you sure your pip version is updated to latest version and you are using a virtual environment. Attaching relevant threads for reference. Ref [1](https://www.tensorflow.org/lite/guide/model_maker) , [2 ](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb#scrollTo=sr3q-gvm3cI8), [3](https://colab.sandbox.google.com/gist/mohantym/7cee04a31addf6a5cd4caa8b5b354df2/model-maker-image-classification-tutorial.ipynb) .Thank you!", "> Hi @chensisi0730! Could you try again with Python 3.8 ? Please make you sure your pip version is updated to latest version and you are using a virtual environment. Attaching relevant threads for reference. Ref [1](https://www.tensorflow.org/lite/guide/model_maker) , [2 ](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb#scrollTo=sr3q-gvm3cI8), [3](https://colab.sandbox.google.com/gist/mohantym/7cee04a31addf6a5cd4caa8b5b354df2/model-maker-image-classification-tutorial.ipynb) .Thank you!\r\n\r\nhi @mohantym  \r\npip3 install tflite-model-maker   or    conda install tflite-model-maker   ?", "Please try with `pip install tflite-model-maker`.", "hey @mohantym, I want to start contributing to tensorflow. I checked some of the issues tagged contribution welcome and good first issue but  they were too confusing at this earlier stage. Can you please suggest some issues I can start on. I will be highly obliged. Thanks", "\r\nRefer to tflite-model-maker  and https://github.com/hoitab/TFLClassify.git \uff0c\r\n Is it suitable for general picture classification \uff1f", "@chensisi0730 ! Above[ thread](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android#6) is a good example if you are looking for end to end deployment in android. You can explore more on android deployment from[ here.](https://www.tensorflow.org/lite/guide/android) .Thank you!", "@Nikhil-Mudgal ! Please check this [thread ](https://github.com/freeCodeCamp/how-to-contribute-to-open-source) as a pointer start  contributing in opensource libraries like Tensorflow. You can keep a tab on recurring feature requests ,bugs and pull requests to get an overall idea of Tensorflow ecosystem.  Feel free to visit  TF [forum](https://discuss.tensorflow.org/) for further assistance. Thank you!", "in vir_env , i can  not use gpu ,   i setup cuda10.0 , but report below: \r\nCould not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64::/usr/local/cuda-10.1/lib64:/usr/local/lib\r\n", "@chensisi0730 ! Could you please confirm your Tensorflow version? You can find suitable Cuda and CudNN version according to your Tensorflow version from [here](https://www.tensorflow.org/install/source#gpu)  . You can also[ set path](https://www.tensorflow.org/install/gpu) your cuda file using below command.  Ref [1](https://stackoverflow.com/questions/64193633/could-not-load-dynamic-library-libcublas-so-10-dlerror-libcublas-so-10-cann), [2](https://forums.developer.nvidia.com/t/tensorflow-fails-to-find-libcudart/154411/7), [3](https://github.com/tensorflow/tensorflow/issues/38578) .Thank you!\r\n```\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\extras\\CUPTI\\lib64;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include;%PATH%\r\nSET PATH=C:\\tools\\cuda\\bin;%PATH%\r\n```", "> @chensisi0730 ! Could you please confirm your Tensorflow version? You can find suitable Cuda and CudNN version according to your Tensorflow version from [here](https://www.tensorflow.org/install/source#gpu) . You can also[ set path](https://www.tensorflow.org/install/gpu) your cuda file using below command. Ref [1](https://stackoverflow.com/questions/64193633/could-not-load-dynamic-library-libcublas-so-10-dlerror-libcublas-so-10-cann), [2](https://forums.developer.nvidia.com/t/tensorflow-fails-to-find-libcudart/154411/7), [3](https://github.com/tensorflow/tensorflow/issues/38578) .Thank you!\r\n> \r\n> ```\r\n> SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin;%PATH%\r\n> SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\extras\\CUPTI\\lib64;%PATH%\r\n> SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include;%PATH%\r\n> SET PATH=C:\\tools\\cuda\\bin;%PATH%\r\n> ```\r\nwhen in  a virtual environment , using  \"pip install -q tflite-model-maker \" ,  automatic installation tensorflow2.7  , i  can not  select it .  \r\nI  setup cuda10.0  , i  do not have cuda11 \uff0c\r\n(base) tt@visiondev-SYS-4029GP-TRT:/data3/tools_work/CUDA/10.0$ ls /usr/local/cuda-10.\r\ncuda-10.0/ cuda-10.1/\r\n\r\n", "You can install respective Cuda and Cudnn files manually by referring guide from [here](https://www.tensorflow.org/install/gpu#software_requirements) . Please move this issue to closed status if it helped . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53550\">No</a>\n"]}, {"number": 53549, "title": "I FUCKING HATE TENSORFLOW", "body": "<spam removed>", "comments": ["imagine not having a fucking life", "I. FUCKING. HATE. BITCHES. LIKE. YOU.", "I'm sorry you feel about Tensorflow this way, but that doesn't mean you can open an issue and just trashtalk Tensorflow.", "> I. FUCKING. HATE. BITCHES. LIKE. YOU.\r\n\r\nok boomer", "> > I. FUCKING. HATE. BITCHES. LIKE. YOU.\r\n> \r\n> ok boomer\r\n\r\nI believe you miss understand. I hate people like that. I was calling them the bitch", "> <spam removed>\r\n\r\nThis man is exceedingly based", "The type of people who can't afford a GPU lmaooo", "> Don't ask me why. Because I don't want to talk about it anymore.\r\n\r\nvery constructive :D ", "When and why are you so mad at TensorFlow? Installing it?", "thanks for the feedback LGTM \ud83d\udc4d ", "\u00cb", "Why though?", "> > > I. FUCKING. HATE. BITCHES. LIKE. YOU.\r\n> > \r\n> > \r\n> > ok boomer\r\n> \r\n> I believe you miss understand. I hate people like that. I was calling them the bitch\r\n\r\nand I am calling you the boomer, what's your point", "Mindestens zwei Quellen f\u00fcr das Abitur", "Hell yeah brother", "what is this ? reddit ? ", "# not a bad opinion", "This is just too funny", "> This is just too funny\r\n\r\nits an odd series of events\r\n", "Lmao", "@sushreebarsa Can you lock this? I think this is going viral on twitter, you can only expect more inane comments", "See https://www.reddit.com/r/ProgrammerHumor/comments/rpkxg2/an_honest_guy/ for some explanation", "based"]}, {"number": 53548, "title": "Failed to load the native TensorFlow runtime.", "body": "Hi,\r\nI am having an issue when importing tensorflow into jupyter notebook. I installed tensorflow using pip install tensorflow using a command prompt opened via anaconda. I checked which versions of python and tensorflow are installed, the command line says I have python 3.8.5 and tensorflow 2.7.0. I believe they are compatible with eachother.\r\nThe error I get when importing it is attached below. \r\n![Issue](https://user-images.githubusercontent.com/96695861/147415587-9a175ade-f747-449b-b04e-01666ba43b08.PNG)\r\n\r\nHow can this be solved?\r\nMany thanks", "comments": ["@SLP-phys8 \r\nCan you please share information in text which will help any user with similar/same error instead of sharing a screenshot.\r\nAlso you may refer to below similar issues and let us know if it helps. also please try in venv to see if you face the error.\r\n\r\n#24419, [link](https://stackoverflow.com/questions/57214046/c-extension-not-working-in-python-3-7-says-importerror-dynamic-module-does-not?rq=1),[link2](https://github.com/pybind/pybind11/issues/2145)", "hey @Saduf2019 , I want to start contributing to tensorflow. I checked some of the issues tagged contribution welcome and good first issue but they were too confusing for me at this earlier stage. Can you please suggest some issues I can start on. I will be highly obliged. Thanks", "@Nikhil-Mudgal \r\ncan you open an issue on in discussion forum as there is a large community who can help and you can start addressing issues of your interest there, you can also filter labels here on github to issues of your choice and address them.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53548\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53548\">No</a>\n"]}, {"number": 53547, "title": "CHANGE IN README", "body": "CAHNGE IN README", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac", "This is also invalid, it does not make sense to capitalize that word"]}, {"number": 53546, "title": "Error when installing TF2.2 based on ROCm 3.3.0", "body": "``**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 8.4.0\r\n- ROCm version: 3.3.0\r\n- GPU model and memory:  AMD gfx906\r\n\r\n\r\n\r\n**Describe the problem**\r\nHello, everyone.\r\nI got the following error when compiling the C++interface of TF2.2 based on ROCm 3.3.0.\r\nInstall command: bazel build --config opt --config=rocm --verbose_failures //tensorflow:libtensorflow_cc.so --jobs 30\r\nError massage:\r\n`ERROR: /public/share/ac8zby7vk0/wankw/software/test1/tensorflow/tensorflow/BUILD:710:1: Linking of rule '//tensorflow:libtensorflow_cc.so.2.2.0' failed (Exit 1)\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<Eigen::half, float>::Run(Eigen::GpuDevice const&, Eigen::half const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<float, float>::Run(Eigen::GpuDevice const&, float const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<double, float>::Run(Eigen::GpuDevice const&, double const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<short, float>::Run(Eigen::GpuDevice const&, short const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'\r\nbazel-out/k8-opt/bin/external/com_github_grpc_grpc/_objs/gpr_base/string.pic.o:string.cc:function gpr_leftpad(char const*, char, unsigned long): warning: memset used with constant zero length parameter; this could be due to transposed parameters\r\nbazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/support/SmallPtrSet.pic.o:SmallPtrSet.cpp:function llvm::SmallPtrSetImplBase::Grow(unsigned int): warning: memset used with constant zero length parameter; this could be due to transposed parameters\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/batch_space_ops_gpu/spacetobatch_functor_gpu.cu.pic.o:spacetobatch_functor_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/batch_space_ops_gpu/spacetobatch_functor_gpu.cu.pic.o:spacetobatch_functor_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops_gpu/depthtospace_op_gpu.cu.pic.o:depthtospace_op_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops_gpu/spacetodepth_op_gpu.cu.pic.o:spacetodepth_op_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/diag_op_gpu/diag_op_gpu.cu.pic.o:diag_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/diag_op_gpu/diag_op_gpu.cu.pic.o:diag_op_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_1.cu.pic.o:where_op_gpu_impl_1.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_1.cu.pic.o:where_op_gpu_impl_1.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, unsigned long, char const*, char const*>(char const*, unsigned long, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_2.cu.pic.o:where_op_gpu_impl_2.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_3.cu.pic.o:where_op_gpu_impl_3.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_4.cu.pic.o:where_op_gpu_impl_4.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, char const*, std::string, char const*, std::string>(char const*, char const*, std::string, char const*, std::string): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::EventMgr::FreeMemory(absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> > const&): error: undefined reference to 'tensorflow::LogMemory::RecordRawDeallocation(std::string const&, long long, void*, tensorflow::Allocator*, bool)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<Eigen::half>: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<float>: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<double>: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<std::complex<float> >: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeOp<Eigen::GpuDevice, Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResize<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, float, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeGradImageOp<Eigen::GpuDevice, Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResizeBackpropImage<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<Eigen::half, 4, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeOp<Eigen::GpuDevice, float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResize<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, float, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeGradImageOp<Eigen::GpuDevice, float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResizeBackpropImage<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeOp<Eigen::GpuDevice, double>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResize<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<double const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, float, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeGradImageOp<Eigen::GpuDevice, double>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResizeBackpropImage<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 4, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/bincount_op_gpu/bincount_op_gpu.cu.pic.o:bincount_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*, char const*>(char const*, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/bucketize_op_gpu/bucketize_op_gpu.cu.pic.o:bucketize_op_gpu.cu.cc:function tensorflow::EventMgr::FreeMemory(absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> > const&): error: undefined reference to 'tensorflow::LogMemory::RecordRawDeallocation(std::string const&, long long, void*, tensorflow::Allocator*, bool)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/histogram_op_gpu/histogram_op_gpu.cu.pic.o:histogram_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*, char const*>(char const*, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPU<double, 3, 3, 1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPU<double, 3, 3, -1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<double, (tensorflow::DepthwiseConv2dDirection)0, 3, 3, 2, false>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<double, (tensorflow::DepthwiseConv2dDirection)0, 3, 3, 2, true>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*>(char const*, std::string, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_float.cu.pic.o:depthwise_conv_op_gpu_float.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*>(char const*, std::string, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/broadcast_to_op_gpu/broadcast_to_op_gpu.cu.pic.o:broadcast_to_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Unimplemented<char const*, std::string, char const*, std::string, char const*>(char const*, std::string, char const*, std::string, char const*): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, unsigned long, char const*, char const*>(char const*, unsigned long, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::EventMgr::FreeMemory(absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> > const&): error: undefined reference to 'tensorflow::LogMemory::RecordRawDeallocation(std::string const&, long long, void*, tensorflow::Allocator*, bool)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::DebugString(int) const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::SummarizeValue(long long, bool) const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::SummarizeValue(long long, bool) const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::DebugString(int) const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*, long long>(char const*, std::string, char const*, long long): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*, std::string>(char const*, std::string, char const*, std::string): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::variant_op_registry_fn_registration::UnaryVariantUnaryOpRegistration<tensorflow::data::OptionalVariant>::UnaryVariantUnaryOpRegistration(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::variant_op_registry_fn_registration::UnaryVariantBinaryOpRegistration<tensorflow::data::OptionalVariant>::UnaryVariantBinaryOpRegistration(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::UnaryOpVariant<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantUnaryOp, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::UnaryOpVariant<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantUnaryOp, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, tensorflow::VariantUnaryOp, char const*, std::string, char const*, std::string>(char const*, tensorflow::VariantUnaryOp, char const*, std::string, char const*, std::string): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterUnaryOpFn(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterUnaryOpFn(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function std::string absl::strings_internal::JoinAlgorithm<__gnu_cxx::__normal_iterator<tensorflow::Tensor const*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > >, tensorflow::data::OptionalVariant::DebugString() const::{lambda(std::string*, tensorflow::Tensor const&)#1}&>(__gnu_cxx::__normal_iterator<tensorflow::Tensor const*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > >, tensorflow::data::OptionalVariant::DebugString() const::{lambda(std::string*, tensorflow::Tensor const&)#1}&, absl::string_view, tensorflow::data::OptionalVariant::DebugString() const::{lambda(std::string*, tensorflow::Tensor const&)#1}&): error: undefined reference to 'tensorflow::Tensor::DebugString(int) const'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function void tensorflow::EncodeVariant<tensorflow::data::OptionalVariant>(tensorflow::data::OptionalVariant const&, std::string*): error: undefined reference to 'tensorflow::VariantTensorData::SerializeToString(std::string*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function bool tensorflow::DecodeVariant<tensorflow::data::OptionalVariant>(std::string*, tensorflow::data::OptionalVariant*): error: undefined reference to 'tensorflow::VariantTensorData::ParseFromString(std::string)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, unsigned long, char const*, unsigned long, char const*>(char const*, unsigned long, char const*, unsigned long, char const*): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::BinaryOpVariants<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantBinaryOp, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::BinaryOpVariants<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantBinaryOp, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterBinaryOpFn(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterBinaryOpFn(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::TransformFilter<Eigen::GpuDevice, double, int, 4>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 4, 1, int>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::ReverseTransformFilter<Eigen::GpuDevice, double, 4>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 4, 1, long>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::TransformFilter<Eigen::GpuDevice, double, int, 5>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 5, 1, int>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::ReverseTransformFilter<Eigen::GpuDevice, double, 5>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 5, 1, long>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function void tensorflow::EncodeVariant<tensorflow::TensorList>(tensorflow::TensorList const&, std::string*): error: undefined reference to 'tensorflow::VariantTensorData::SerializeToString(std::string*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function bool tensorflow::DecodeVariant<tensorflow::TensorList>(std::string*, tensorflow::TensorList*): error: undefined reference to 'tensorflow::VariantTensorData::ParseFromString(std::string)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterBinaryOpFn(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterUnaryOpFn(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 1571.556s, Critical Path: 229.26s\r\nINFO: 4832 processes: 4832 local.\r\nFAILED: Build did NOT complete successfully\r\n`\r\nHow do I solve it? Thanks.\r\n[nohup.zip](https://github.com/tensorflow/tensorflow/files/7776532/nohup.zip)\r\n\r\n", "comments": ["Hi @wankiwi ! Have you tried with latest version 2.7 yet?", "> Hi @wankiwi ! Have you tried with latest version 2.7 yet?\r\n\r\nNot yet, I selected the TF version based on the version of ROCm3.3.0 according to this [https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/rocm_docs/tensorflow-rocm-release.md](url) Should I try the TF2.7?", "@wankiwi \r\nPlease try in latest version 2.7 and let us know.", "> Hi @wankiwi ! Have you tried with latest version 2.7 yet?\r\nI tried the TF 2.7 and got the following error message\r\n`ERROR: /public/share/ac8zby7vk0/wankw/software/TF/tf-2.7.0/tensorflow/tensorflow/core/kernels/BUILD:719:18: error while parsing .d file: /public/home/wankaiwei/.cache/bazel/_bazel_wankaiwei/74b894e40687eb1a4e653de5840c4c0c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/split_lib_gpu/split_lib_gpu.cu.pic.d (No such file or directory)\r\n[4,727 / 11,155] 29 actions running\r\n    Compiling tensorflow/core/kernels/lookup_table_op.cc; 20s local\r\n    Compiling tensorflow/compiler/xla/service/cpu/runtime_single_thread\r\nclang-12: error: cannot find HIP runtime. Provide its path via --rocm-path, or pass -nogpuinc to build without HIP runtime.\r\n[4,727 / 11,155] 29 actions running\r\n    Compiling tensorflow/core/kernels/lookup_table_op.cc; 20s local\r\n    Compiling tensorflow/compiler/xla/service/cpu/runtime_single_thread\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 837.957s, Critical Path: 175.26s\r\nINFO: 3981 processes: 68 internal, 3913 local.\r\nFAILED: Build did NOT complete successfully\r\n`", "Hi @Saduf2019 ! Could you please look at this issue?", "Hi @wankiwi ! Could you try again using [pip](https://pypi.org/project/tensorflow-rocm/) command or instructions mentioned [here ](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream)?", "@wankiwi \r\nHave you tried with pip and verified the compatibility.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53546\">No</a>\n"]}, {"number": 53545, "title": "Cross compiling TFLite with XNNPACK=ON for ARM with CMake", "body": "**System information**\r\n- Linux Ubuntu 20.04\r\n- TensorFlow r2.8 (and master at the time of writing, being Dec 26th 2021)\r\n- CMake 3.16.3\r\n- gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf (as per [this](https://www.tensorflow.org/lite/guide/build_cmake_arm#download_toolchain_2) toolchain recommendation)\r\n\r\n**Describe the problem**\r\nI want to cross compile tensorflow lite (with xnnpack=on) for arm using cmake. \r\n\r\nThe build fails unless I force xnnpack to off.  I'm using the steps stated in the following articles:\r\n\r\n- https://www.tensorflow.org/lite/guide/build_cmake\r\n- https://www.tensorflow.org/lite/guide/build_arm\r\n- https://www.tensorflow.org/lite/guide/build_cmake_arm\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Install clean Ubuntu 20.04 and latest updates\r\n\r\n- Install Prerequisites \r\n```\r\nsudo apt-get install curl git cmake\r\n```\r\n\r\n- Install Toolchain (as per [this](https://www.tensorflow.org/lite/guide/build_cmake_arm#download_toolchain_2))\r\n```\r\ncurl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz\r\nmkdir -p ${HOME}/toolchains\r\ntar xvf gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz -C ${HOME}/toolchains\r\n```\r\n\r\n- Clone Tensorflow (as per [this](https://www.tensorflow.org/lite/guide/build_cmake#step_2_clone_tensorflow_repository))\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git ${HOME}/tensorflow_src\r\nmkdir ${HOME}/tflite_build && cd ${HOME}/tflite_build\r\n```\r\n\r\n- Run CMake (as per [this](https://www.tensorflow.org/lite/guide/build_cmake_arm#run_cmake_2)) noting that this defaults to using -DTFLITE_ENABLE_XNNPACK=ON\r\n```\r\nARMCC_FLAGS=\"-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations\"\r\nARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-\r\ncmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \\\r\n -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \\\r\n -DCMAKE_C_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n -DCMAKE_CXX_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\r\n -DCMAKE_SYSTEM_NAME=Linux \\\r\n -DCMAKE_SYSTEM_PROCESSOR=armv7 \\\r\n  ../tensorflow_src/tensorflow/lite/\r\n```\r\n\r\n- Build (as per [this](https://www.tensorflow.org/lite/guide/build_cmake#step_5_build_tensorflow_lite))\r\n```\r\ncmake --build . -j\r\n```\r\n\r\n**Any other info / logs**\r\nThis build crashes around halfway through as per the output below. I can make it work by adding -DTFLITE_ENABLE_XNNPACK=OFF, however that will disable XNNPACK and considerably slow down inference time.  My ARM hardware has the NEON capabilitiy, so really need this XNNPACK compile to work. Thanks in  advance!\r\n\r\n**Build Output**\r\n```\r\n[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/f32-ibilinear/gen/neonfma-c8.c.o\r\ncd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-gemm/gen/1x8s4-minmax-neonfma.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-gemm/gen/1x8s4-minmax-neonfma.c\r\ncd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-gemm/gen/6x8s4-minmax-neonfma.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-gemm/gen/6x8s4-minmax-neonfma.c\r\ncd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-ibilinear-chw/gen/neonfma-p8.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-ibilinear-chw/gen/neonfma-p8.c\r\n/home/tim/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c: In function \u2018xnn_f16_f32_vcvt_ukernel__neonfp16_x16\u2019:\r\ncd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-ibilinear/gen/neonfma-c8.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-ibilinear/gen/neonfma-c8.c\r\n/home/tim/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c:31:11: error: unknown type name \u2018float16x8_t\u2019\r\n     const float16x8_t vh0 = vreinterpretq_f16_u16(vld1q_u16(i)); i += 8;\r\n\r\n```\r\n", "comments": ["@TimRoadley \r\nCould you please try on stable version and let s know if this is still an issue, could you please try on 2.6/2.7 and let us know.", "Should be fixed with 6fcfc7c8a6d46b63e4e2febb1774f9f0865bd2f2", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53545\">No</a>\n"]}, {"number": 53544, "title": "How can I prevent CPU overflow while optimizing tensorflow with particle swarm optimization?", "body": "I am trying to optimize the following hyperparameters with pso for segmentation in modified Unet model.\r\n\r\nHyperparameters: Dropout, learning_rate, factor, min_lr\r\n\r\ntensorflow==2.0.0\r\n\r\nkeras==2.3.1\r\n\r\nPSO iterations number = 10\r\n\r\nPSO Particle number = 10\r\n\r\n10 models are running in each iteration (PSO iteration). This means a total of 100 models will run (model_1, model_2,...,model_100). But, when I go to the 2nd iteration, I get a memory overflow error on the 19th model. For the solution of this, I looked and applied similar problems on this platform, but the error did not go away. How can I avoid this memory overflow? I also tried the following code to solve this problem (for tensorflow 1.* version compatibility). First Soution Try:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import backend as K\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nconfig = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=36,inter_op_parallelism_threads=2, allow_soft_placement=True, device_count = {'CPU': 36})\r\nsess = tf.compat.v1.Session(config=config)\r\nK.set_session(tf.compat.v1.Session(config=config))\r\n```\r\n\r\n\r\nTHE CODE (Current State): The code block below is run on the calculation set with 36 Cores requested in the slurm script file.\r\n\r\n\r\n```\r\ndef objective_function(x):\r\n    _ = tf.Variable([1])\r\n    context._context = None\r\n    context._create_context()\r\n    tf.config.threading.set_intra_op_parallelism_threads(36)\r\n    tf.config.threading.set_inter_op_parallelism_threads(2)\r\n    Dropoutt = x[0]\r\n    Llearning_rate = x[1]\r\n    Factorr = x[2]\r\n    Mmin_lr = x[3]\r\n    with tf.device('/CPU:0'):\r\n        model = get_unet_mod(input_img, n_filters=16, dropout=Dropoutt, batchnorm=True)\r\n        model_isim = model.name\r\n        model.compile(optimizer=Adam(amsgrad=True, learning_rate=Llearning_rate), loss=jaccard_distance_loss, metrics=[\"accuracy\", dice_coef, f1])\r\n        Callbackss = [ReduceLROnPlateau(monitor=\"loss\",mode=\"min\",factor=Factorr, patience=10, min_lr=Mmin_lr, verbose=0)]\r\n        results = model.fit(X_train, y_train, batch_size=8, verbose=0, epochs=30, callbacks=Callbackss, validation_data=(X_test, y_test))\r\n\r\nPSO(objective_function, bounds, particle_size, iterations)\r\n```\r\nError:\r\n\r\nslurmstepd: error: step 8207716.4294967294 hit memory+swap limit at least once during execution. this may or may not result in some failure.\r\n", "comments": ["@furkanatlan ,\r\nWe see that you are using tf version 2.0,  please update to latest stable  v2.7 and let us know if you are using same issue.Thanks!", "The issueis not related to the version of tensorflow. The issue is that\nwhen optimizing keras model.fit with PSO, it requests more memory than\nallocated and the code doesn't run due to memory overflow. I would be very\nhappy if you could help me with this.\nBest regards\n\ntilakrayal ***@***.***>, 27 Ara 2021 Pzt, 06:51 tarihinde\n\u015funu yazd\u0131:\n\n> @furkanatlan <https://github.com/furkanatlan> ,\n> We see that you are using tf version 2.0, please update to latest stable\n> v2.7 and let us know if you are using same issue.Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/53544#issuecomment-1001317165>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKNNPB4LVAHOFXJKQNTUGY3US7PDFANCNFSM5KYGXDAQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "I ran the code shared and face a different error, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/55f75a374aea8d3a0fd21a7a3c8ee7f3/untitled167.ipynb).Please share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\n", " Its unlikely for TF 2.0 version to receive any bug fixes except when we have security patches. There is a high possibility that this was fixed with later TF versions. Perhaps you can use latest tf versions for your case. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53543, "title": "Improving error message on CUDA version search and specification during build `./configure`", "body": "This is from TF git version: 5e8cd1162cc8eeb9cf493d071293a621dcbb51f0 (Dec 22).\r\n\r\nWhen building from sources and when the `./configure` step fails to find a CUDA version with a message below, could it actually report what it found? The message below leaves things ambiguous:\r\n\r\n```\r\nCould not find any cuda.h matching version 'CUDA 11' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\n        'local/cuda/extras/CUPTI/include'\r\nof:\r\n        '/lib'\r\n        '/lib/i386-linux-gnu'\r\n        '/lib/x86_64-linux-gnu'\r\n        '/lib32'\r\n        '/usr'\r\n        '/usr/lib/x86_64-linux-gnu/libfakeroot'\r\n        '/usr/local/cuda'\r\n        '/usr/local/cuda/targets/x86_64-linux/lib'\r\n        '/usr/local/lib'\r\n\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:\r\n```\r\n\r\nWhen it says \"Could not find any cuda.h .... matching version ..\", could it report what exactly was found and whether it found anything?  From this prompt itself, it isn't immediately clear whether one should enter \"11\" or \"11.2\" (with minor version), or \"CUDA 11\". I do have a `/usr/local/cuda/include/cuda.h` and \r\n\r\n```console\r\n$ cat /usr/local/cuda/include/cuda.h  | grep CUDA_VERSION\r\n#define CUDA_VERSION 11020\r\n * \\defgroup CUDA_VERSION Version Management\r\n/** @} */ /* END CUDA_VERSION */\r\n```\r\n\r\nUpdate: Finally, specifying \"11.2\" worked.\r\n\r\n\r\n\r\n", "comments": ["Hi @bondhugula ! Did the issue get resolved after specifying 11.2 as Cuda Version?Tested configurations can be found from [here](https://www.tensorflow.org/install/source_windows).", "> Hi @bondhugula ! Did the issue get resolved after specifying 11.2 as Cuda Version?Tested configurations can be found from [here](https://www.tensorflow.org/install/source_windows).\r\n\r\nYes, specifying 11.2 does work. I now see the version table at https://www.tensorflow.org/install/source\r\nThanks.", "Ok @bondhugula ! Is this issue good to close then?", "> Ok @bondhugula ! Is this issue good to close then?\r\n\r\n@mohantym The issue really was on `./configure` not reporting what it actually found and whether it found something. Without that it wouldn't be clear to a user as to why. In any case, this may not be a high priority issue.", "Hi @Saduf2019! Could you please look at this issue?", "@bondhugula \r\nWe see that you are using the latest tf version, as per this [link]( https://www.tensorflow.org/install/source), the cuda version is already mentioned to be 11.2, hence there is no need to modify error message as its already specified which version is compatible. Kindly move this to closed status as you are able to use TF with the correct cuda version.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53543\">No</a>\n"]}, {"number": 53542, "title": "Tensorboard load saved_model.pb failed", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nWindows\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\ntag v2.6.0\r\n\r\n**Describe the current behavior**\r\n![image](https://user-images.githubusercontent.com/39380007/147376311-8278d9d7-1577-4d03-803b-e2e4931dbb49.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nShow saved_model.pb on tensorboard\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53542\">No</a>\n", "oh...I should parse saved_model.pb to SavedModel   not GraphDef"]}, {"number": 53541, "title": "Feature Request: Providing Gradients for `layer.set_weights()`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n- Are you willing to contribute it (Yes/No): No, I am not experienced enough.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe feature is to make `set_weights()` differentiable. The current behavior is that the `model.set_weights()` function breaks the gradient. Given a model named `learner` and a batch of data `batch`\r\n\r\n```\r\nl = tf.constant(0.1)\r\nwith tf.GradientTape() as gt:\r\n  gt.watch(l)\r\n  new_weights = [w * l for w in learner.weights]\r\n\r\n  learner.set_weights(new_weights)\r\n\r\n  y_pred = learner(batch[\"train\"][\"X\"])\r\n  loss = tf.keras.losses.categorical_crossentropy(batch[\"train\"][\"y\"], y_pred)\r\ngrads = gt.gradient(loss, l)\r\ngrads # returns None\r\n```\r\n\r\nThis example returns None for the gradients. In fact, the gradient tape can't calculate the gradient of any variable related to calculating the `new_weights`.\r\n\r\nSo I was wondering if it's possible to provide gradients for `set_weights()` or if there are any work-arounds to this problem.\r\n\r\n**Will this change the current api? How?**\r\n\r\nI don't see how this feature would change the current api. It is more a modification on an existing function.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nI believe adding this feature would make projects related to meta-learning easier to realize.\r\n\r\nIn my specific case, I was trying to implement [this paper](https://openreview.net/pdf?id=rJY0-Kcll) and translate the [pytorch implementation](https://github.com/markdtw/meta-learning-lstm-pytorch) to tensorflow, but I think this feature would generally benefit many projects that are related to the growing field of meta-learning.\r\n\r\n**Any Other info.**\r\nThanks and Merry Christmas!!!\r\n", "comments": ["@NuoWenLei ,\r\nPlease feel free to close this issue since it is already being tracked in Keras repo? Thanks!"]}, {"number": 53540, "title": "Tensorflow", "body": "### 1. System information\n\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- TensorFlow installation (pip package or built from source):\n- TensorFlow library (version, if pip package or github SHA, if built from source):\n\n### 2. Code\n\nProvide code to help us reproduce your issues using one of the following options:\n\n#### Option A: Reference colab notebooks\n\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.\n2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).\n\n```\n(You can paste links or attach files by dragging & dropping them below)\n- Provide links to your updated versions of the above two colab notebooks.\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\n```\n\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\n\n```\n(You can paste links or attach files by dragging & dropping them below)\n- Include code to invoke the TFLite Converter Python API and the errors.\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\n```\n\n### 3. Failure after conversion\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\n\n- Model produces wrong results and/or has lesser accuracy.\n- Model produces correct results, but it is slower than expected.\n\n### 4. (optional) RNN conversion support\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\n\n### 5. (optional) Any other info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Hi @winnie-bar ! Could you please point the exact issue?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53539, "title": "2.8 cherry pick request: Hide JPEG symbols on Darwin (to align with Linux) ", "body": null, "comments": []}, {"number": 53538, "title": "ModuleNotFoundError: No module named 'tensorflow.python.keras.applications'", "body": "<em>Please make sure that this is an issue related to keras.\r\ntag:keras_template</em>\r\n\r\nI am running a script that is trying to import this:\r\n\r\n``import tensorflow.python.keras.applications``\r\n\r\nbut it gives the bellow error:\r\n\r\n```ModuleNotFoundError: No module named 'tensorflow.python.keras.applications'```\r\n\r\nMy TensorFlow version is 2.8.0 and Keras version is 2.8.0.\r\nI'm trying to train an object detection model but I keep getting that error.\r\n\r\nHere is my pip list: https://pastebin.com/xjfTW28W", "comments": ["@hackermondev I was able to import this  `import tensorflow.python.keras.applications` on colab.\r\n Could you please find the gist [here](https://colab.sandbox.google.com/gist/sushreebarsa/7205e420b464689776e57e4a387e3fbe/53538.ipynb#scrollTo=zdITO1jmn9kH) for reference and confirm the same?Thank you!", "I was able to fix the problem by uninstalling TensorFlow and installing it back.\r\n\r\n```\r\npip uninstall tensorflow\r\npip install tensorflow\r\n```\r\n\r\nI'm not sure why I had to do this though.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53538\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53538\">No</a>\n"]}, {"number": 53537, "title": "[jax] Factor out GetJitArgumentStickyDevice function", "body": null, "comments": []}, {"number": 53536, "title": "Update gpu_backend_lib.cc", "body": "hcc removed for #53475", "comments": ["This was merged."]}, {"number": 53535, "title": "Update gemmlowp", "body": "Update gemmlowp version to grab fix https://github.com/google/gemmlowp/pull/209.", "comments": []}, {"number": 53534, "title": "Error in Hessian calculation using forward over backward propagation ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- TensorFlow installed from (source or binary):  both colab CPU and GPU mode\r\n- TensorFlow version (use command below):2.7.0\r\n- Python version: Python 3.7.12\r\n- GPU model and memory: Tesla P100 \r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to compute second derivative using the method provide by the [tutorial](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) using forward over backward propagation to calculate the hessian-vector-product. For memory reasons, we chose not to use the double backward method. In the code, we have nested loop and `tf.dynamic_partition`. Both the gradient and hessian works in the eager mode but when I try to decorate the function by `@tf.function` the error appears, and I found it's because the combination of using `tf.dynamic_partition` and `for loop` gives the error. \r\n\r\nAdditionally, a different error is given when I try to decorate the `hvp` function. Without decorator, there's a `TypeError `, with a decorator, it gives a `SystemError `\r\n\r\n- Briefly describe your candidate solution(if contributing): I have tried to find a workaround to use `tf.gather` or use a loop to replace the `tf.dynamic_partition`, different problems also prompt. \r\n\r\n**Standalone code to reproduce the issue**\r\nI have made a reduced dummy code here and also in [colab](https://colab.research.google.com/drive/1k2-zp6DIzt4C4f9lgrqIasXE0SjdUJ3P?usp=sharing)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n@tf.function # without the decorator, the function works fine in eager mode\r\ndef foo(mu):\r\n\r\n  partitions = tf.constant([1, 0, 0])\r\n  points = tf.dynamic_partition(mu, partitions, 2)[0]\r\n  block = points\r\n  # a dummy example of a loop\r\n  for j in tf.range(1): # without this loop, the function works fine\r\n    block =  points\r\n  \r\n  return block\r\n# dummy input data\r\nmu = tf.constant([[3.,2.,1.],[3.,2.,1.],[3.,2.,1.]])\r\nfoo(mu)\r\n\r\n# gradient calculation \r\n@tf.function\r\ndef grad(mu):\r\n\r\n    with tf.GradientTape(watch_accessed_variables=False) as t:\r\n        t.watch(mu)\r\n        property = foo(mu)\r\n    loss = t.gradient(property,mu)\r\n\r\n    return(loss)\r\ngradient = grad(mu)\r\n\r\n# hessian vector product\r\n@tf.function # with/without the decorator, different error prompt\r\ndef hvp(mu,tangents):\r\n    with tf.autodiff.ForwardAccumulator(mu, tangents) as acc:\r\n        with tf.GradientTape(watch_accessed_variables=False) as t:\r\n            t.watch(mu)\r\n            property = foo(mu)\r\n            print('tracing')\r\n            tf.print('executing')\r\n        loss = t.gradient(property,mu)\r\n    hess = acc.jvp(loss)\r\n    return(hess)\r\n  \r\ntangents = np.zeros(mu.shape)\r\ntangents[0]=1\r\ntangents = tf.convert_to_tensor(tangents,dtype=tf.float32)\r\nhess = hvp(mu,tangents)\r\n\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nStagingError                              Traceback (most recent call last)\r\n<ipython-input-6-f99c3f212345> in <module>()\r\n     14 tangents[0]=1\r\n     15 tangents = tf.convert_to_tensor(tangents,dtype=tf.float32)\r\n---> 16 hess = hvp(mu,tangents)\r\n     17 \r\n     18 \r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)\r\n   1127           except Exception as e:  # pylint:disable=broad-except\r\n   1128             if hasattr(e, \"ag_error_metadata\"):\r\n-> 1129               raise e.ag_error_metadata.to_exception(e)\r\n   1130             else:\r\n   1131               raise\r\n\r\nStagingError: in user code:\r\n\r\n    File \"<ipython-input-4-f99c3f212345>\", line 9, in hvp  *\r\n        loss = t.gradient(property,mu)\r\n\r\n    SystemError: PyEval_EvalFrameEx returned a result with an error set\r\n```\r\n", "comments": ["@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.5, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/c6cc071e8beb14621801edd9aa4398c8/53534.ipynb).", "Hi @Saduf2019, is there any update on this problem? Thanks", "@GeorgeLiang3 \r\nWe see that you are using python for loops inside a tf.function please avoid that, try using tf.while_loop instead.\r\nThis is not a bug its coding errors, can you move this to closed status in this repo as this is for feature requests and bugs, please open this issue in tf discussion forum as there is a larger community to help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53534\">No</a>\n", "@Saduf2019 \r\nThanks for the reply, but I got the exact same error using `tf.while_loop`\r\n\r\n```Python\r\n@tf.function # without the decorator, the function works fine in eager mode\r\ndef foo(mu):\r\n\r\n  partitions = tf.constant([1, 0, 0])\r\n  points = tf.dynamic_partition(mu, partitions, 2)[0]\r\n  block = points\r\n  # # a dummy example of a loop\r\n  # for j in tf.range(1): # without this loop, the function works fine\r\n  #   block =  points\r\n  i0 = tf.constant(0)\r\n  c = lambda i, m: i < 1\r\n  b = lambda i, m: [i+1,points]\r\n  block = tf.while_loop(c,b,[i0,block])\r\n  return block\r\n# dummy input data\r\nmu = tf.constant([[3.,2.,1.],[3.,2.,1.],[3.,2.,1.]])\r\nfoo(mu)\r\n```\r\n\r\nError message:\r\n```\r\nTypeError: in user code:\r\n\r\n\r\n    TypeError: `dtype` <dtype: 'variant'> is not compatible with 1 of dtype int64.\r\n```\r\n\r\n", "@GeorgeLiang3 \r\nKindly open this in discussion forum as this is not a bug in tensorflow, discussion forum is the appropriate repo for such issues. Thanks!"]}, {"number": 53532, "title": "eliminate broadcast_in_dim + transpose to broadcast_in_dim", "body": "transpose(broadcast_in_dim(X)) => broadcast_in_dim(X)", "comments": ["I fix the typo, please approve again.  @joker-eph ", "> LG, would the opposite fusion work as well? broadcast_in_dim(transpose(x)) -> broadcast_in_dim(x) ?\r\n\r\nYou didn't answer me here, WDYT?", "> > LG, would the opposite fusion work as well? broadcast_in_dim(transpose(x)) -> broadcast_in_dim(x) ?\r\n> \r\n> You didn't answer me here, WDYT?\r\n\r\nYes, I think `broadcast_in_dim(transpose(x)) -> broadcast_in_dim(x)` would work as well.  \r\nBut I think there maybe some pattern like `broadcast_in_dim(transpose(constant))`, should this pattern be simplified `broadcast_in_dim(transposed_constant)` or `broadcast_in_dim(constant)` ?", "Right now this PR seems to be failing `//tensorflow/compiler/mlir/hlo/tests/Dialect/mhlo/canonicalize:transpose.mlir.test`:\r\n\r\n```\r\ntensorflow/compiler/mlir/hlo/tests/Dialect/mhlo/canonicalize/transpose.mlir:51:62: error: undefined variable: ARG\r\n // CHECK: [[RET:%[a-zA-Z0-9]+]] = \"mhlo.broadcast_in_dim\"([[ARG]])\r\n                                                             ^\r\n```", "> But I think there maybe some pattern like broadcast_in_dim(transpose(constant)), should this pattern be simplified broadcast_in_dim(transposed_constant) or broadcast_in_dim(constant) ?\r\n\r\nSeems better to not touch the constant in general if we don't need to, but that's hard to control: the folding of `transpose(constant)` is likely to happen independently first, before `broadcast_in_dim(transpose(x)) -> broadcast_in_dim(x)` kicks in.\r\n", "Sorry for the delay: this change exposed a few bugs in other places and I had to fix them so that all of our testing would pass! :)"]}, {"number": 53531, "title": "[PluggableDevice] TF_RESOURCE is not deserializable since 2.7", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.9\r\n\r\n**Describe the current behavior**\r\nWhen calling `TF_TensorData` in `TF >= 2.7.0` on a tensor containing a `TF_RESOURCE`, the returned data is the resource handle itself instead of being a string serialization of it. This seems to have been caused by [this commit](https://github.com/tensorflow/tensorflow/commit/6f8c6dbdf7bf9155a931aaefb77fdbcff3b06e16) which removed the `TF_RESOURCE` serialization logic from TF 2.\r\n\r\n**Describe the expected behavior**\r\nCalling `TF_TensorData` on a tensor containing a `TF_RESOURCE` should return some kind of cross-abi serialization of the resource handle or the resource proto. For example, reverting [this commit](https://github.com/tensorflow/tensorflow/commit/6f8c6dbdf7bf9155a931aaefb77fdbcff3b06e16) produces the expected behavior from a plugin standpoint.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): I can if needed\r\n- Briefly describe your candidate solution(if contributing): \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```cpp\r\nvoid* tensor_data = TF_TensorData(tensor);\r\nsize_t tensor_size = TF_TensorByteSize(tensor)\r\nstd::string serialized_data(reinterpret_cast<const char*>(tensor_data), tensor_size);\r\nauto resource_handle = std::make_shared<tensorflow::ResourceHandleProto>();\r\n\r\nif (!resource_handle->ParseFromString(serialized_data)) {\r\n  std::abort();\r\n}\r\n```\r\n", "comments": ["@PatriceVignola \r\nIn order to expedite the trouble-shooting process,please provide a complete code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sushreebarsa I provided a code snippet above. All this code uses is the TF C API, the STD library and protocol buffers. I want to know how we can deserialize resource tensors from pluggable device code since the 2.7 change that broke it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53531\">No</a>\n"]}, {"number": 53530, "title": "First tensorflow operation on numpy or list data takes long time after tf.GradientTape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: 11.2 | 8.2.4.15-1+cuda11.4\r\n- GPU model and memory: RTX 2080 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nFirst tensorflow operation on numpy or list data after `with tf.GradientTape() as ...` takes much longer than subsequent operations, and is dependant on model size\r\n\r\n**Describe the expected behavior**\r\n\r\nOperation takes the same amount of time as all others. Note that in 2.6.0 both the first operation before, and the first operation after, would take a long time, but in 2.7.0 it is only the first operation after.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport time\r\nimport tensorflow as tf\r\nfrom keras import Input, Model\r\nfrom keras.layers import Conv2D\r\nimport numpy as np\r\n\r\n\r\ndef Backbone(filters):\r\n    x = Input((None, None, 3))\r\n    inp = x\r\n    x = Conv2D(filters=filters, kernel_size=(3, 3), activation='relu')(x)\r\n    x = Conv2D(filters=filters, kernel_size=(3, 3), activation='relu')(x)\r\n    x = Conv2D(filters=filters*2, kernel_size=(3, 3), activation='relu')(x)\r\n    x = Conv2D(filters=filters*2, kernel_size=(3, 3), activation='relu')(x)\r\n    x = Conv2D(filters=filters*4, kernel_size=(3, 3), activation='relu')(x)\r\n    x = Conv2D(filters=filters*4, kernel_size=(3, 3), activation='relu')(x)\r\n    return Model(inp, x)\r\n\r\n\r\nclass TrainModel(tf.keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super(TrainModel, self).__init__(*args, **kwargs)\r\n\r\n    def train_step(self, data):\r\n        ts1 = time.time()\r\n        test = tf.reduce_mean([1.0])\r\n        te1 = time.time()\r\n\r\n        ts2 = time.time()\r\n        test = tf.reduce_mean([1.0])\r\n        te2 = time.time()\r\n\r\n        with tf.GradientTape() as tape:\r\n            res = self(data)\r\n            loss = tf.reduce_mean(res)\r\n\r\n        ts3 = time.time()\r\n        test = tf.reduce_mean([1.0])\r\n        # test = tf.reduce_mean(np.random.randint(0, 100, (100, 100)))\r\n        te3 = time.time()\r\n\r\n        ts4 = time.time()\r\n        test = tf.reduce_mean([1.0])\r\n        te4 = time.time()\r\n\r\n        trainable_vars = self.trainable_variables\r\n        gradients = tape.gradient(loss, trainable_vars)\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n\r\n        print(f\"before: {te1 - ts1:.05f}s {te2 - ts2:.05f}s, after: {te3 - ts3:.05f}s {te4 - ts4:.05f}s\")\r\n\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\n\r\nprint(\"filters = 8\")\r\nmodel = Backbone(filters=8)\r\nmodel = TrainModel(model.inputs, model.outputs)\r\nmodel.compile(optimizer='adam', run_eagerly=True)\r\nmodel.fit(x=tf.random.normal([32,224,224,3]),\r\n          batch_size=4,\r\n          epochs=1,\r\n          verbose=0)\r\n\r\n\r\nprint(\"filters = 32\")\r\nmodel = Backbone(filters=32)\r\nmodel = TrainModel(model.inputs, model.outputs)\r\nmodel.compile(optimizer='adam', run_eagerly=True)\r\nmodel.fit(x=tf.random.normal([32,224,224,3]),\r\n          batch_size=4,\r\n          epochs=1,\r\n          verbose=0)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nOutput from above:\r\n\r\n```\r\nfilters = 8\r\nbefore: 0.00079s 0.00030s, after: 0.00082s 0.00050s\r\nbefore: 0.00042s 0.00034s, after: 0.00125s 0.00040s\r\nbefore: 0.00051s 0.00038s, after: 0.00115s 0.00029s\r\nbefore: 0.00039s 0.00035s, after: 0.00121s 0.00029s\r\nbefore: 0.00039s 0.00035s, after: 0.00124s 0.00035s\r\nbefore: 0.00039s 0.00040s, after: 0.00127s 0.00041s\r\nbefore: 0.00048s 0.00041s, after: 0.00125s 0.00030s\r\nbefore: 0.00048s 0.00047s, after: 0.00123s 0.00040s\r\nfilters = 32\r\nbefore: 0.00075s 0.00049s, after: 0.00600s 0.00043s\r\nbefore: 0.00049s 0.00042s, after: 0.01974s 0.00063s\r\nbefore: 0.00057s 0.00043s, after: 0.01396s 0.00036s\r\nbefore: 0.00043s 0.00036s, after: 0.01337s 0.00040s\r\nbefore: 0.00052s 0.00040s, after: 0.01359s 0.00040s\r\nbefore: 0.00062s 0.00039s, after: 0.01354s 0.00041s\r\nbefore: 0.00054s 0.00042s, after: 0.01387s 0.00043s\r\nbefore: 0.00060s 0.00062s, after: 0.01415s 0.00073s\r\n```\r\n", "comments": ["@geometrikal ,\r\nWe don't find any issue in the result.Can you please find the gist [here](https://colab.research.google.com/gist/tilakrayal/17aa98d4638ef865f67cc18c401cefb0/untitled153.ipynb).Also please let us know if we are missing anything here.Thanks!", "@tilakrayal Try again in colab using a GPU runtime and you will see the issue. \n\nThis is the output for me using the GPU runtime:\n\n```\nfilters = 8\nbefore: 0.01121s 0.00102s, after: 0.00118s 0.00111s\nbefore: 0.00148s 0.00093s, after: 0.00927s 0.00093s\nbefore: 0.00144s 0.00091s, after: 0.00958s 0.00095s\nbefore: 0.00151s 0.00085s, after: 0.00956s 0.00088s\nbefore: 0.00123s 0.00097s, after: 0.00987s 0.00083s\nbefore: 0.00130s 0.00098s, after: 0.00907s 0.00090s\nbefore: 0.00095s 0.00091s, after: 0.00988s 0.00088s\nbefore: 0.00101s 0.00089s, after: 0.00998s 0.00092s\nfilters = 32\nbefore: 0.00139s 0.00085s, after: 0.03691s 0.00091s\nbefore: 0.00191s 0.00085s, after: 0.07519s 0.00086s\nbefore: 0.00162s 0.00089s, after: 0.08111s 0.00089s\nbefore: 0.00150s 0.00089s, after: 0.08580s 0.00336s\nbefore: 0.00127s 0.00085s, after: 0.08641s 0.00179s\nbefore: 0.00136s 0.00084s, after: 0.08203s 0.00859s\nbefore: 0.00124s 0.00092s, after: 0.08265s 0.00129s\nbefore: 0.00113s 0.00098s, after: 0.07719s 0.00092s\n<keras.callbacks.History at 0x7fe0202ae910>\n```", "Further investigation reveals this is not a bug.\r\n\r\nThe tensoflow operations within `with tf.GradientTape() as tape` are running in parallel with the python code. When a tensorflow operation is performed on numpy or list data, it forces the two to synchronise, meaning that there is a delay while waiting for the previous code to complete. The delay is proportional to the size of the model, as the time taken to call a bigger model is longer.\r\n\r\nHope this helps someone who is trying to time their code in python and wondering why some basic operations take a long time.\r\n\r\n![image](https://user-images.githubusercontent.com/1187528/147297608-6ad00bf4-5ab4-4de0-af1a-b01a03957cb2.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53530\">No</a>\n"]}, {"number": 53528, "title": "Disable caching for ACL softmax primitives", "body": "Co-authored-by: Luke Ireland <luke.ireland@arm.com>", "comments": ["Hi @penpornk,\r\n\r\nThis PR replaces https://github.com/tensorflow/tensorflow/pull/53273 and only includes the changes needed for the Compute Library based softmax primitive from oneDNN 2.5.\r\n\r\nWould it be possible to cherry-pick this fix onto the 2.8 release branch please?"]}, {"number": 53527, "title": "[oneDNN] updating oneDNN to v2.5.1", "body": "This update fixes some performance regressions in benchmark tests.", "comments": []}, {"number": 53526, "title": "2.8-rc1 cherry-pick request: Add op-determinism changes to the version 2.8 release notes", "body": "/CC @mihaimaruseac ", "comments": []}, {"number": 53524, "title": "[MHLO-Linalg] Lower mhlo.convolution to linalg when padding is non zero.", "body": "Lower mhlo.convolution to linalg when padding is non zero.", "comments": []}, {"number": 53523, "title": "[TF-TRT] Fix a bug in getting the DType string representation for the conversion report.", "body": "Previously, we use DType.str method to get the string representation for a DType object. This PR fix this to use dtypes[DType object] to get the string representation for the object.", "comments": ["Thanks for fixing this! I modify the PR title and add a description. Please let me know whether they look good.", "@Kh4L Can you please check @bixia1's comments and keep us posted ? Thanks!", "> Previously, we use DType.str method to get the string representation for a DType object. This PR fix this to use dtypes[DType object] to get the string representation for the object.\r\n\r\nLGTM!"]}, {"number": 53522, "title": "line 123 of build_pip_package_with_cmake.sh to be adapted", "body": "**System information**\r\n- Linux Ubuntu 20.04 on WSL2\r\n- Cmake 3.16\r\n- Host system with i5-10210u processor, 4 cores\r\n\r\nLine 123 of the script as shown on\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh#L123\r\ncauses the script to crash as result of not limiting number of processors. \r\nError message: ERRO[0169] error waiting for container: invalid character \u2018u\u2019 looking for beginning of value\r\n\r\nIn a similar way, running the cmake build command directly on the Linux host (i.e. without using a docker container) causes compiler errors if the number of processors (4 in this case) is not specified correctly. (need to use \"cmake --build . -j 4\" as correct command)\r\n\r\nIf ${BUILD_NUM_JOBS} is replaced by the actual number of processors/cores available (in my case 4) then both the script to create the pip wheel as well as the basic cmake command perform as desired and complete without errors.\r\n\r\nSee [this thread](https://discuss.tensorflow.org/t/instructions-for-cmake-on-raspberry-pi-zero-are-inaccurate/6610/12) for further details.\r\n\r\n\r\n\r\n", "comments": ["You can easily override the value BUILD_NUM_JOBS as the following.\r\n\r\n```\r\nBUILD_NUM_JOBS=1 build_pip_package_with_cmake.sh \r\n```\r\nor\r\n```\r\nexport BUILD_NUM_JOBS=1\r\nbuild_pip_package_with_cmake.sh \r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53522\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53522\">No</a>\n", "> You can easily override the value BUILD_NUM_JOBS as the following.\r\n> \r\n> ```\r\n> BUILD_NUM_JOBS=1 build_pip_package_with_cmake.sh \r\n> ```\r\n> \r\n> or\r\n> \r\n> ```\r\n> export BUILD_NUM_JOBS=1\r\n> build_pip_package_with_cmake.sh \r\n> ```\r\n\r\nTrue, but without documenting this problem, bringing it to the user attention or resolving it in another way, many more users will run into the same issue. Just mentioning a possible solution here as a reply does not really prevent this from happening in the future and will leave the build script as vulnerable as before."]}, {"number": 53520, "title": "[TF:TRT]How to write my input_fn when I conver my tf model to trt? ", "body": "\r\nuse nvcr.io/nvidia/tensorflow:19.12-tf2-py3 in docker\r\nmy model is\r\n\r\nmax_batch_size: 512\r\ninput [{\r\n\tname: \"dense_input\"\r\n\tdata_type: TYPE_FP32\r\n\tformat: FORMAT_NONE\r\n\tdims: [-1]\r\n\tis_shape_tensor: false\r\n\tallow_ragged_batch: false\r\n}, {\r\n\tname: \"sparse_ids_input\"\r\n\tdata_type: TYPE_INT32\r\n\tformat: FORMAT_NONE\r\n\tdims: [-1]\r\n\tis_shape_tensor: false\r\n\tallow_ragged_batch: false\r\n}, {\r\n\tname: \"seq_input\"\r\n\tdata_type: TYPE_INT32\r\n\tformat: FORMAT_NONE\r\n\tdims: [-1, -1]\r\n\tis_shape_tensor: false\r\n\tallow_ragged_batch: false\r\n}, {\r\n\tname: \"sparse_wgt_input\"\r\n\tdata_type: TYPE_FP32\r\n\tformat: FORMAT_NONE\r\n\tdims: [-1]\r\n\tis_shape_tensor: false\r\n\tallow_ragged_batch: false\r\n}]\r\noutput: [{\r\n\tname: \"tf_op_layer_Sigmoid\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [1]\r\n\treshape: {\r\n\t\tshape: []\r\n\t}\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_pctr\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [1]\r\n\treshape: {\r\n\t\tshape: []\r\n\t}\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_dapan_action\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [1]\r\n\treshape: {\r\n\t\tshape: []\r\n\t}\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_pcvr_ctr\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [2]\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_pctcvr_1\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [1]\r\n\treshape: {\r\n\t\tshape: []\r\n\t}\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_delay_time\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [2]\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_pcvr\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [1]\r\n\treshape: {\r\n\t\tshape: []\r\n\t}\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_pctcvr\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [1]\r\n\treshape: {\r\n\t\tshape: []\r\n\t}\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}, {\r\n\tname: \"tf_op_layer_Sigmoid_1\"\r\n\tdata_type: TYPE_FP32\r\n\tdims: [1]\r\n\treshape: {\r\n\t\tshape: []\r\n\t}\r\n\tlabel_filename: \"\"\r\n\tis_shape_tensor: false\r\n}]\r\nI want to know how to write my input_fn when I conver it to trt, my code is blew but not ok :\r\n\r\n# -*- coding: utf-8 -*-\r\nimport os\r\n\r\nfrom tensorflow import make_tensor_proto\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\r\n\r\nif __name__ == \"__main__\":\r\n    input_saved_model_dir = \"./TF-recommend/1634309909\"\r\n    output_saved_model_dir = \"./TF-recommend-trt/\"\r\n\r\n    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS\r\n    conversion_params = conversion_params._replace(\r\n        max_workspace_size_bytes=(1 << 32))\r\n    conversion_params = conversion_params._replace(precision_mode=\"FP16\")\r\n    conversion_params = conversion_params._replace(\r\n        maximum_cached_engines=100)\r\n\r\n    converter = trt.TrtGraphConverterV2(\r\n        input_saved_model_dir=input_saved_model_dir,\r\n        conversion_params=conversion_params)\r\n\r\n    converter.convert()\r\n\r\n\r\n    def my_input_fn():\r\n        dense_input = np.ones([438]).astype('float32')\r\n        sparse_ids_input = np.ones([79]).astype('int32')\r\n        sparse_wgt_input = np.ones([79]).astype('float32')\r\n\r\n        req = {\r\n            \"dense_input\": make_tensor_proto(dense_input, shape=(dense_input.shape)),\r\n            \"sparse_ids_input\": make_tensor_proto(sparse_ids_input, shape=(sparse_ids_input.shape)),\r\n            \"sparse_wgt_input\": make_tensor_proto(sparse_wgt_input, shape=(sparse_wgt_input.shape)),\r\n        }\r\n        yield req\r\n\r\n\r\n    converter.build(input_fn=my_input_fn)\r\n    converter.save(output_saved_model_dir)", "comments": ["@GuoGuiRong \r\nPlease refer t below links: as this is not a feat req or a bug please create this issue in discussion forum.\r\n[link](https://github.com/tensorflow/tensorrt)\r\n[link1](https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.html),\r\n[link2](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html)", "```\r\nTraceback (most recent call last):\r\n  File \"/Users/guirong/venv/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 497, in _import_graph_def_internal\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 5 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.\r\n\r\n```\r\n\r\nIS this a bug that hasn't been solved until now?", "@GuoGuiRong \r\nCan you please run your code in google colab and share a gist as it is not indented, so we can help you.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53520\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53520\">No</a>\n"]}]