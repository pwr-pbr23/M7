[{"number": 51835, "title": "self.add_weight in custom Keras Layers breaks model saving in TF format", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Debian 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nI have a model that seems to fail in saving with TF format (.h5 is completely fine) when I call self.add_weight in a custom Keras layer:\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Layer, Input\r\nimport tensorflow as tf\r\nfrom model import load_gen\r\n\r\nclass ResAdd(Layer):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.res_gain = self.add_weight(shape=(), initializer='zeros', trainable=True)\r\n\r\n    def call(self, inputs):\r\n        res, skip = inputs\r\n        gain = tf.cast(self.res_gain, res.dtype)\r\n        out = res * gain + skip\r\n        return out \r\n\r\ninp = Input((32, 32, 3)) \r\nout = ResAdd()([inp, inp])\r\nmodel = Model(inp, out)\r\n\r\nmodel.save('model')\r\n```\r\n\r\nThe error states:\r\n```\r\nTraceback (most recent call last):\r\n  File \"bring.py\", line 23, in <module>\r\n    model.save('model')\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 2146, in save\r\n    signatures, options, save_traces)\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/keras/saving/save.py\", line 150, in save_model\r\n    signatures, options, save_traces)\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/keras/saving/saved_model/save.py\", line 91, in save\r\n    model, filepath, signatures, options)\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1228, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1399, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1349, in _build_meta_graph_impl\r\n    wrapped_functions)\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 200, in __init__\r\n    self._trace_all_concrete_functions()\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 289, in _trace_all_concrete_functions\r\n    for obj in self.checkpoint_view.list_objects():\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 460, in list_objects\r\n    trackable_objects, _, _ = self.objects_ids_and_slot_variables()\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 455, in objects_ids_and_slot_variables\r\n    self.objects_ids_and_slot_variables_and_paths())\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 443, in objects_ids_and_slot_variables_and_paths\r\n    object_names[obj] = _object_prefix_from_path(path)\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 64, in _object_prefix_from_path\r\n    for trackable in path_to_root))\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 64, in <genexpr>\r\n    for trackable in path_to_root))\r\n  File \"/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 57, in _escape_local_name\r\n    return (name.replace(_ESCAPE_CHAR, _ESCAPE_CHAR + _ESCAPE_CHAR)\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n```\r\n\r\nNote that I fixed this error by replacing:\r\n```\r\nself.res_gain = self.add_weight(shape=(), initializer='zeros', trainable=True)\r\n```\r\nwith\r\n```\r\nself.res_gain = tf.Variable(0.0, trainable=True)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThat the behavior of \r\n```\r\nself.res_gain = self.add_weight(shape=(), initializer='zeros', trainable=True)\r\n```\r\nis equal to \r\n```\r\nself.res_gain = tf.Variable(0.0, trainable=True)\r\n```\r\nwhen initializing a model and that saving the model in TF format won't raise an error", "comments": ["@BearNinja123 Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51835\">No</a>\n"]}, {"number": 51834, "title": "staged_predict for Boosted Tree classifier", "body": "<emThe issue related to the performance of the [BoostedTree classifier](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1932-L2101) in terms of accuracy.</em>\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 16.04, Windows 10\r\n- TensorFlow installed from (source or binary): pip install TensorFlow\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6\r\n\r\n** Describe the model**\r\nBoostedTree classifier is a model introduced by [N Ponomareva, T Colthurst, G Hendry.2017](https://ieeexplore.ieee.org/abstract/document/8257910).\r\nIt is built over the Xgboost idea and learning is done through building one layer of decision tree regressor over N boosting iteration.\r\n\r\n**Describe the current behavior**\r\nReturn the accuracy of the ensemble model with [n_tree](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1932-L2101)\r\n\r\n**Describe the expected behavior**\r\nI need to have the accuracy of each individual tree in the ensemble model.\r\nFor example, the Gradient Boosting of Sklearn has the [staged_predict method](https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/ensemble/_gb.py#L1270).\r\nIs there any similar method in tf.BoostedTree as well?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nYou may find an example of the model, [here](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding)", "comments": ["Hi @mihaimaruseac, May I ask for your help on this issue, please?\r\n\r\nI was thinking maybe I could use this [method](https://github.com/tensorflow/estimator/blob/35467a902bcdb4d774fff3d52c2daa5d4ca4488a/tensorflow_estimator/python/estimator/estimator.py#L1539) to give almost the same expected result, but after reading the method I understand it was a different method, so in any case, I have to modify this [method](https://github.com/tensorflow/estimator/blob/35467a902bcdb4d774fff3d52c2daa5d4ca4488a/tensorflow_estimator/python/estimator/estimator.py#L1655) if I want to predict each tree of one ensemble model. \r\n\r\nCould you please help me!", "Estimators are not receiving new feature development at this time. It's possible https://www.tensorflow.org/decision_forests (Tensorflow decision forests) either has the features that you need, or that you can request the features that you need."]}, {"number": 51833, "title": "ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]", "body": "I'm trying to build a neural network for colour recognition, I have 3 categories to sort\r\npart1 of my code is building the data set\r\n----------\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport os \r\nimport cv2\r\nimport random\r\nimport pickle\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D,MaxPool2D\r\n\r\n\r\nDataDIR = \"/Users/zhiranbai/Downloads/SmartSyringeSystem-master/notebooks\"\r\nCATES = [\"Advil\",\"Demazin\",\"Panadol\"]\r\n\r\ntf.get_logger().setLevel('ERROR')\r\n\r\nfor CATE in CATES:\r\n    path = os.path.join(DataDIR,CATE)  #path to medicine dir\r\n    for img in os.listdir(path):\r\n        img_array = cv2.imread(os.path.join(path,img))\r\n        #plt.imshow(img_array, cmap=\"gray\")\r\n        #plt.show()\r\n        break\r\n    break\r\nIMG_SIZE= 200\r\nnew_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\r\n#plt.imshow(new_array, cmap = 'gray')\r\n#plt.show()\r\n\r\n\r\ntraining_data = []\r\n\r\ndef create_traning_data():\r\n    for CATE in CATES:\r\n        path = os.path.join(DataDIR,CATE)\r\n        class_num = CATES.index(CATE)\r\n        for img in os.listdir(path):\r\n            try:\r\n                img_array = cv2.imread(os.path.join(path,img))\r\n                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\r\n                training_data.append([new_array, class_num])\r\n            except Exception as e:\r\n                pass\r\ncreate_traning_data()\r\n#print(len(training_data))\r\n\r\nrandom.shuffle(training_data)\r\n\r\nfor sample in training_data[:10]:\r\n    print(sample[1])\r\n\r\n\r\nx = []\r\ny= []\r\n# y= np.array(y)\r\nfor features, label in training_data :\r\n    x.append(features)\r\n    y.append(label)\r\n    # np.array((y,label))\r\nx = np.array(x).reshape(-1, IMG_SIZE, IMG_SIZE,3) #-1 fea num, 3 RGB\r\n\r\n\r\npickle_out = open(\"x.pickle\", \"wb\")\r\npickle.dump(x, pickle_out)\r\npickle_out.close()\r\n\r\npickle_out = open(\"y.pickle\", \"wb\")\r\npickle.dump(y, pickle_out)\r\npickle_out.close()\r\n-----------\r\npart 2 is try to train the NN\r\n-------\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.datasets import cifar10\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\n\r\nimport pickle\r\n\r\nX = pickle.load(open(\"x.pickle\",\"rb\"))\r\ny = pickle.load(open(\"y.pickle\",\"rb\"))\r\n\r\n#nomalize data --scale\r\n\r\nX = X/255.0\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(64,(3,3),input_shape = X.shape[1:]))\r\nmodel.add(Activation(\"relu\")) #activation layer rectify linear\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Conv2D(64,(3,3)))\r\nmodel.add(Activation(\"relu\")) #activation layer rectify linear\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\n\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\r\n\r\nmodel.fit(X,y,batch_size=32, epochs=1, validation_split=0.3) #valid--out of sample data\r\n----------------\r\nand I got this error\r\n------\r\npython3 roberttest3.py\r\n2021-09-04 22:52:41.530069: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File \"roberttest3.py\", line 34, in <module>\r\n    model.fit(X,y,batch_size=32, epochs=1, validation_split=0.3) #valid--out of sample data\r\n  File \"/usr/local/lib/python3.7/site-packages/keras/engine/training.py\", line 1121, in fit\r\n    (x, y, sample_weight), validation_split=validation_split))\r\n  File \"/usr/local/lib/python3.7/site-packages/keras/engine/data_adapter.py\", line 1479, in train_validation_split\r\n    \"arrays, found following types in the input: {}\".format(unsplitable))\r\nValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\r\n\r\n\r\n-------\r\n\r\n", "comments": []}, {"number": 51831, "title": "Add snappy compression support on Python API", "body": "Currently the the TFRecord Python API lacks support for snappy compression option, which is desirable for developers who prefer to use snappy for their training data.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51831) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 51830, "title": "Add support for fp16 GEMM BEF thunks", "body": "The corresponding TFRT PR is https://github.com/tensorflow/runtime/pull/79\r\n", "comments": []}, {"number": 51829, "title": "Don't constant-fold DT_RESOURCE constants.", "body": "PiperOrigin-RevId: 391803952\r\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51829) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 51828, "title": "Build fails with internal compiler error on file lstm_ops.cc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: 11.4/8.2.2\r\n- GPU model and memory: gtx 970 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild failes after 4~ hours with error message\r\n```\r\nERROR: C:/users/zaksm/source/repos/tensorflow/tensorflow/core/kernels/rnn/BUILD:42:18: C++ compilation of rule '//tensorflow/core/kernels/rnn:lstm_ops' failed (Exit -1): python.exe failed: error executing command\r\n  cd C:/users/zaksm/_bazel_zaksm/arvgy65w/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.4\r\n    SET GCC_HOST_COMPILER_PATH=C:/msys64/usr/bin/gcc.exe\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/zaksm/AppData/Local/Programs/Python/Python39/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/zaksm/AppData/Local/Programs/Python/Python39/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\zaksm\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.2\r\n    SET TMP=C:\\Users\\zaksm\\AppData\\Local\\Temp\r\n  C:/Users/zaksm/AppData/Local/Programs/Python/Python39/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_l_dnn_v1/src/cpu/x64/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /arch:AVX /std:c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -DNV_CUDNN_DISABLE_EXCEPTION -DGOOGLE_CUDA=1 -DNV_CUDNN_DISABLE_EXCEPTION -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL=1 /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/rnn/_objs/lstm_ops/lstm_ops.obj /c tensorflow/core/kernels/rnn/lstm_ops.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\nC:\\users\\zaksm\\_bazel_zaksm\\arvgy65w\\execroot\\org_tensorflow\\tensorflow\\core\\kernels\\rnn\\lstm_ops.cc(56) : fatal error C1001: Internal compiler error.\r\n(compiler file 'd:\\a01\\_work\\2\\s\\src\\vctools\\Compiler\\Utc\\src\\p2\\main.c', line 213)\r\n To work around this problem, try simplifying or changing the program near the locations listed above.\r\nIf possible please provide a repro here: https://developercommunity.visualstudio.com\r\nPlease choose the Technical Support command on the Visual C++\r\n Help menu, or open the Technical Support help file for more information\r\n\r\nTarget //tensorflow:tensorflow.dll failed to build\r\nINFO: Elapsed time: 13605.853s, Critical Path: 6621.50s\r\nINFO: 11659 processes: 3915 internal, 7744 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --config=opt --config=cuda tensorflow:tensorflow.dll\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@zipy124 ,\r\n\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu).In this case, can you please try installing TensorFlow v2.6 with compatible configurations and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51828\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51828\">No</a>\n"]}, {"number": 51827, "title": "Custom signatures not working with Subclasssed Keras Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nWhen saving keras_models (checked for Subclassed Model) and providing custom signatures, `keras_model.save(export_dir, signatures)`  does not work and the saved model does not have the exported signature. The custom signature however works if we are using `tf.saved_model.save(keras_model, export_dir, signatures)`.\r\n\r\n**Describe the expected behavior**\r\ntf.saved_model.save(keras_model, export_dir, signatures) and `keras_model.save(export_dir, signatures)` should have similar signatures.\r\n", "comments": ["Hi @sumitbinnani !Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Ok @sumitbinnani ,Feel free to close this one . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Tracking in keras-team/keras#15324", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51827\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51827\">No</a>\n"]}, {"number": 51824, "title": "Keras model with TPUStrategy get InternalError", "body": "When I fit a Keras Model with TPUStrategy, it got following errors:\r\n```\r\nInternalError: 7 root error(s) found.\r\n  (0) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1630677855.531027249\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3009,\"referenced_errors\":[{\"created\":\"@1630677855.531025671\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":398,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\nExecuting non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\t [[Pad_9/paddings/_152]]\r\n  (1) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1630677855.531027249\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3009,\"referenced_errors\":[{\"created\":\"@1630677855.531025671\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":398,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\nExecuting non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\t [[cond_14/switch_pred/_140/_90]]\r\n  (2) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1630677855.531027249\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3009,\"referenced_errors\":[{\"created\":\"@1630677855.531025671\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":398,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\nExecuting non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\t [[TPUReplicate/_compile/_10521736726166110026/_4/_194]]\r\n  (3) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n: ... [truncated]\r\n```\r\n\r\nTensorFlow Version: \r\nv2.6.0-0-g919f693420e 2.6.0\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe code used to reproduce this error can be found at https://drive.google.com/file/d/1ceB5VBwFd87Z0ANa_91vhvrOdG4SJQLY/view?usp=sharing\r\n", "comments": ["Hi @howl-anderson !We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "Hi  @mohantym, I have added the TF version and colab link to reproduce the issue. Steps followed before I ran into this error are also list in the colab link.", "Hi @Saduf2019 ,Could you look into this issue ! providing gist in TF [2.5](https://colab.research.google.com/gist/mohantym/0ed4686d38b613687b0e8d588953d496/layers_crf_with_tpu.ipynb?authuser=1#scrollTo=l3WJsqUBxpFX) ,[2.6 ](https://colab.research.google.com/gist/mohantym/961390e60a7b3c780a46ec2abc506802/layers_crf_with_tpu.ipynb?authuser=1#scrollTo=HKbmByByTVEP), [nightly ](https://colab.research.google.com/gist/mohantym/ce0ed003afee869053e792ef51bee3c7/layers_crf_with_tpu.ipynb?authuser=1#scrollTo=pFMeRQTrPI1B)for reference. ", "Thank you @mohantym for your swift response.", "@howl-anderson \r\nCould you please refer to [similar error](https://github.com/tensorflow/tensorflow/issues/44963) issue and try with 2.4, also [link](https://github.com/keras-team/keras-io/issues/150).\r\nIn case the issue still persists:\r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "Hi @Saduf2019, thank you for your help.  My code target is TF 2.6. So, it surely has `StringLookup` issue under TF 2.5.  The true issue will show under TF 2.6, you can reproduce it from this code: https://drive.google.com/file/d/1ceB5VBwFd87Z0ANa_91vhvrOdG4SJQLY/view?usp=sharing . Sorry for the confusion. And thank you for your reminder, I will move this issue to keras-team/keras repo. ", "Move this issue to keras-team/keras#15317", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51824\">No</a>\n"]}, {"number": 51823, "title": "[Tensorflow Executor Dialect]  ", "body": "**MLIR operation format is :**\r\noperation            ::= op-result-list? (generic-operation | custom-operation)\r\n                         trailing-location?\r\ngeneric-operation    ::= string-literal `(` value-use-list? `)`  successor-list?\r\n                         region-list? dictionary-attribute? `:` function-type\r\ncustom-operation     ::= bare-id custom-operation-format\r\nop-result-list       ::= op-result (`,` op-result)* `=`\r\nop-result            ::= value-id (`:` integer-literal)\r\nsuccessor-list       ::= `[` successor (`,` successor)* `]`\r\nsuccessor            ::= caret-id (`:` bb-arg-list)?\r\nregion-list          ::= `(` region (`,` region)* `)`\r\ndictionary-attribute ::= `{` (attribute-entry (`,` attribute-entry)*)? `}`\r\ntrailing-location    ::= (`loc` `(` location `)`)?\r\n\r\n\r\n**here is tf_executor.graph operation :**\r\n%fetches = tf_executor.graph : tensor<*xf32> {\r\n  // Operations in the current block execute when their inputs are ready,\r\n  // possibly concurrently.\r\n  // Only operations in the tf_executor dialect are expected here.\r\n  // Ops can return multiple outputs and a control token for control\r\n  // dependencies.\r\n  // We don\u2019t mention the control token in the return type here, it is implicit.\r\n  %0, %ctl0 = tf_executor.opA %feed#0, %feed#1 : tensor<*xf32>\r\n  %1, %ctl1 = tf_executor.opB : tensor<*xf32>\r\n  %2, %ctl2 = tf_executor.opC %1, %ctl0 : tensor<*xf32>\r\n  %3, %ctl3 = tf_executor.opD %2 : tensor<*xf32>\r\n  tf_executor.fetch %3 : tensor<*xf32>\r\n} // end of the \u201ctf_executor.graph\" operation/region\r\n\r\nAccording to Operation format,   {} stands for dictionary-attribute\r\nobviously, tf_executor.graph operation looks like abnormal case\r\nthe {} of tf_executor consists of several operations\r\n\r\ncould anyone explain and point out how to understand tf_executor.graph operation\r\nmany thanks\r\n", "comments": ["@FullZing Could you please have a look at the[ link1](https://www.tensorflow.org/mlir/includes/tf_passes), [link2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/ir/tf_executor_ops.td) and let us know if it helps ?", "Hi @sushreebarsa \r\n\r\nthank you for replying\r\nbut my problem still exists\r\n1.   i checked the Region defination in MLIR from \r\n https://mlir.llvm.org/docs/LangRef/#regions\r\nand it shows a sample with region \r\n  \"any_op\"(%a) ({ // if %a is in-scope in the containing region...\r\n\t // then %a is in-scope here too.\r\n    %new_value = \"another_op\"(%a) : (i64) -> (i64)\r\n  }) : (i64) -> (i64)\r\n\r\n2.  check tf_executor.graph Operation description from https://github.com/tensorflow/community/blob/master/rfcs/20190612-mlir-dialect.md\r\nand it shows a sample with region\r\n%fetches = tf_executor.graph : tensor<*xf32> {\r\n  // Operations in the current block execute when their inputs are ready,\r\n  // possibly concurrently.\r\n  // Only operations in the tf_executor dialect are expected here.\r\n  // Ops can return multiple outputs and a control token for control\r\n  // dependencies.\r\n  // We don\u2019t mention the control token in the return type here, it is implicit.\r\n  %0, %ctl0 = tf_executor.opA %feed#0, %feed#1 : tensor<*xf32>\r\n  %1, %ctl1 = tf_executor.opB : tensor<*xf32>\r\n  %2, %ctl2 = tf_executor.opC %1, %ctl0 : tensor<*xf32>\r\n  %3, %ctl3 = tf_executor.opD %2 : tensor<*xf32>\r\n  tf_executor.fetch %3 : tensor<*xf32>\r\n} // end of the \u201ctf_executor.graph\" operation/region\r\n\r\n\r\ncompared tf_executor.graph with any_op,   you can find parentheses for region is missing in tf_executor\r\nso i wonder what is his grammar \uff1f\r\n\r\n3.  from  https://mlir.llvm.org/docs/LangRef/,   failed to find any same usage\r\n\r\nplease check again\r\n ", "@FullZing \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51822, "title": "[PluggableDevice] TensorList support with DEVICE_DEFAULT", "body": "Use stream apis to enable the TensorList with DEVICE_DEFAULT including memzero, memset and memset32 and it's only enabled with pluggable device.", "comments": ["@penpornk @saxenasaurabh Could you please help to review this PR? It implements the TensorList with stream APIs instead of exposing C APIs when using pluggable device. Thanks."]}, {"number": 51821, "title": "ctc_loss: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.2\r\n- GPU model and memory: on CPU\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using ctc_loss with dense tensor, it functions correctly. But when using it with sparse tensor, although the result is the right but it raised a warning saying: \r\n```\r\nW ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.\r\n```\r\nI'm not sure if this is a real bug or something else. \r\n\r\n**Describe the expected behavior**\r\nIt shouldn't raise the warning. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nlabel = [\r\n\t[1, 2, 1, 0, 0],\r\n\t[1, 1, 0, 0, 0],\r\n\t[1, 1, 1, 1, 1],\r\n]\r\nlogits = [\r\n\t[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],\r\n\t[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],\r\n\t[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]],\r\n]\r\nlabels_length = [3, 2, 5]\r\nlogits_length = [5, 5, 5]\r\nlabels_tensor = tf.convert_to_tensor(label, dtype=tf.int32)\r\nlabels_tensor_sparse = tf.sparse.from_dense(labels_tensor)\r\nlogits_tensor = tf.convert_to_tensor(logits, dtype=tf.float32)\r\n\r\nlabels_length_tensor = tf.convert_to_tensor(labels_length, dtype=tf.int32)\r\nlogits_length_tensor = tf.convert_to_tensor(logits_length, dtype=tf.int32)\r\n\r\nloss_dense = tf.nn.ctc_loss(labels_tensor, logits_tensor, labels_length_tensor, logits_length_tensor, logits_time_major=False)\r\nprint(loss_dense.numpy()[0])\r\nloss_sparse = tf.nn.ctc_loss(labels_tensor_sparse, logits_tensor, None, logits_length_tensor, logits_time_major=False, blank_index=0)\r\nprint(loss_sparse.numpy()[0])\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n![image](https://user-images.githubusercontent.com/6693605/132005260-d30cd4e5-e7f9-4e52-afc5-7668fd22122b.png)\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51821\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51821\">No</a>\n", "NVM, it's my mistake and understand the thing wrongly"]}, {"number": 51820, "title": "Tensorflow sort changes values in output list to 0 when the tensor datatype is tf.float32 but not for tf.float64", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): M1 Macbook Air where is the issue is seen / Intel Mac where I don't see any issue\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nWhen n<=16 it sorts the values properly but when n>16 it sorts the list and then changes the values at \r\nposition 16 and higher to  value  -0.\r\n\r\nimport tensorflow as tf\r\n\r\nn=17\r\na = tf.random.uniform(shape=[n], dtype=tf.float32)\r\nprint(a)\r\nprint(tf.sort(a))\r\n\r\ntf.Tensor(\r\n[ 0.00088847  0.03644979  0.06494105  0.07945895  0.13768506  0.153288\r\n  0.27231824  0.39109504  0.4003389   0.41191268  0.48915362  0.528106\r\n  0.5799836   0.6125376   0.65293264  0.8135816  -0.        ], shape=(17,), dtype=float32)\r\n\r\non M1 Macbook Air ( tensorflow 2.5.0 ) and\r\n\r\ntf.Tensor(\r\n[0.41191268 0.48915362 0.65293264 0.6125376  0.00088847 0.03644979\r\n 0.13768506 0.528106   0.27231824 0.4003389  0.5799836  0.83420205\r\n 0.06494105 0.39109504 0.8135816  0.153288   0.07945895], shape=(17,), dtype=float32)\r\n\r\non Intel Mac ( tensorflow 2.4,3 )\r\n\r\n```\r\n**Describe the expected behavior**\r\n\r\n```\r\ntf.Tensor(\r\n[0.41191268 0.48915362 0.65293264 0.6125376  0.00088847 0.03644979\r\n 0.13768506 0.528106   0.27231824 0.4003389  0.5799836  0.83420205\r\n 0.06494105 0.39109504 0.8135816  0.153288   0.07945895], shape=(17,), dtype=float32)\r\n\r\n\r\n\r\n```\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): \r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @mohanr ,Please look at the [comment ](https://github.com/tensorflow/tensorflow/issues/51499#issuecomment-904821790)of the this similar[ issue ](https://github.com/tensorflow/tensorflow/issues/51499)  .", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51820\">No</a>\n"]}, {"number": 51819, "title": "Converting Hard Swish activation to TFLite efficiently", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nwhen using a hard swish activation:\r\n```\r\nx = 0.16667 * x * tf.nn.relu6(x+3)\r\n```\r\nwhat shows up in the Tensorflow Lite model is is just the computation graph of the above definition:\r\nx\r\n -> multiply by 0.167 \r\n -> add 3-> relu6 (those are fused together)\r\n         -> multiply results from above\r\n(see diagram [here](https://imgur.com/a/OZcWZWQ))\r\n\r\n ** Expected behavior ** \r\n\r\nTensorflow Lite has a hard swish operator already. Question is, how should I define the computation in Tensorflow such that when converted to TFLite, the output would be a single hard swish node for this activation?\r\n\r\n", "comments": ["I think if you do\r\n\r\n```\r\ntf.nn.relu6(x+3.0) * 0.1666666666666 * x\r\n```\r\n\r\nthe fusion should works", "Confirmed. It works. \r\n\r\nCan you please explain why \r\n```x = 0.16667 * x * tf.nn.relu6(x+3)``` \r\ndoes not work but\r\n```tf.nn.relu6(x+3.0) * 0.1666666666666 * x```\r\nworks? \r\nIs it a matter of types, or operator order or something else?", "you can check out our existing fusion patterns here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td#L253-L317\r\n\r\nsee two reasons your ops are not fused:\r\n\r\n* operator order\r\n* 0.1666666666666 does not match 0.16667", "Ok, that's great! Thank you."]}, {"number": 51817, "title": "Facing issue while initializing parameters in tensorflow.", "body": "![2](https://user-images.githubusercontent.com/26819449/131990627-7504f592-80e1-478f-ab3d-19a48d7b50b9.JPG)\r\n\r\n![1](https://user-images.githubusercontent.com/26819449/131990652-bc585eb8-0be2-4ace-b3c2-b88f05da8e9c.JPG)\r\n\r\nWhere i am lacking.\r\nI have already ready tensor flow documentation.\r\nJust tell me what should I do.\r\ntf_version_2.3.0\r\nThanks!\r\n", "comments": ["Hi @starboyvarun \r\nThis is not a bug or feature request, Please open these type of  issues in [tf discussion forum ](https://discuss.tensorflow.org/)as there is a larger community there.   \r\n\r\n```\r\ninitializer = tf.keras.initializers.GlorotNormal()\r\nvalues = initializer(shape=(2, 2))\r\n\r\nprint(values.numpy())\r\n\r\n```\r\nReference :https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal#examples_2\r\nThank you!", "@mohantym I already told you I have read the documentation.\r\nI asked where I am lacking.\r\nyou just copying and pasting the code here.\r\nThanks!.\r\n\r\n", "Hi @starboyvarun !This is not a bug or feature request, for any further queries you may open this issue in tf discussion[ forum](https://discuss.tensorflow.org/) as there is a larger community there.Thank you!"]}, {"number": 51816, "title": "Building TFlite for windows with flex delegate error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: None\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): MSVC 2019 (Not sure how to check more in detail)\r\n- CUDA/cuDNN version: 10.1/ 7.6.5\r\n- GPU model and memory: Nvidia GTX 1050\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to build tflite for windows with support for Tensorflow ops outside of standard tflite operation.\r\nSo, I followed this guide: https://www.tensorflow.org/lite/guide/ops_select#c\r\nAlthough it is ambiguous and since I am pretty new in this field, I looked around for other site and do the following.\r\n\r\nNote that I would like to build TF with no CUDA support. (Only using cpu)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Add \"//tensorflow/lite/delegates/flex:delegates\" to `deps` in `tensorflow/tensorflow/lite/BUILD`\r\n2. Using following command to build tflite\r\n`bazel build --config=monolithic -c opt --cxxopt='--std=c++11' //tensorflow/lite:libtensorflowlite.so`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere are some part of the result logs:\r\n```   \r\n SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\jobpa\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\jobpa\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params\r\nExecution platform: @local_execution_config_platform//:platform\r\nLINK : warning LNK4044: unrecognized option '/s'; ignored\r\nbuiltin_ops.lib(register.obj) : error LNK2005: \"public: __cdecl tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver(void)\" (??0BuiltinOpResolver@builtin@ops@tflite@@QEAA@XZ) already defined in builtin_ops_all_linked.lo.lib(register.obj)\r\nbuiltin_ops.lib(register.obj) : error LNK2005: \"public: virtual class std::vector<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)>,class std::allocator<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)> > > __cdecl tflite::ops::builtin::BuiltinOpResolver::GetDelegates(int)const \" (?GetDelegates@BuiltinOpResolver@builtin@ops@tflite@@UEBA?AV?$vector@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@V?$allocator@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@@2@@std@@H@Z) already defined in builtin_ops_all_linked.lo.lib(register.obj)\r\nbuiltin_ops.lib(register.obj) : error LNK2005: \"public: virtual class std::vector<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)>,class std::allocator<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)> > > __cdecl tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates::GetDelegates(int)const \" (?GetDelegates@BuiltinOpResolverWithoutDefaultDelegates@builtin@ops@tflite@@UEBA?AV?$vector@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@V?$allocator@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@@2@@std@@H@Z) already defined in builtin_ops_all_linked.lo.lib(register.obj)\r\nLINK : fatal error LNK1189: library limit of 65535 objects exceeded\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\n```\r\n\r\nAccording to this [post](https://github.com/tensorflow/tensorflow/issues/43367), I tried to add `--copt=/bigobj` but that does not help either.\r\n\r\nI'm not sure if I did something wrong, or if there is a limitation of adding flex delegate to tflite on windows...\r\n\r\n", "comments": ["Any update on this?", "Please check https://github.com/tensorflow/tensorflow/issues/43367#issuecomment-1021211279", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51816\">No</a>\n"]}, {"number": 51815, "title": "ValueError: Shape must be rank 1 but is rank 0 for ", "body": "Value Error: in user code:\r\n    <ipython-input-18-4013e653d9ce>:18 one_hot_matrix  *\r\n        one_hot = tf.reshape(tf.one_hot(label,depth,axis=0), (depth))\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:195 reshape\r\n        result = gen_array_ops.reshape(tensor, shape, name)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:8234 reshape\r\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\r\n        compute_device)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\r\n        op_def=op_def)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1975 __init__\r\n        control_input_ops, op_def)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](one_hot, Reshape/shape)' with input shapes: [6], [].\r\n\r\ntf version : '2.3.0'\r\n\r\nIs this a bug ?", "comments": ["@starboyvarun ,\r\nCan you please to execute your code in latest stable tf v2.6 and let us know if you are facing same issue.Thanks!"]}, {"number": 51814, "title": "Connection to pythong", "body": "I got folowing error using tensorflow in R\r\n```\r\n> library(tensorflow)\r\n> tf$constant(\"Hellow Tensorflow\")\r\nError in system2(python, stdout = TRUE, args = c(\"-c\", shQuote(\"import sys; import platform; sys.stdout.write(platform.architecture()[0])\"))) : \r\n  'CreateProcess' failed to run 'C:\\Python27\\ArcGIS10.8\\python.exe -c \"import sys; import platform; sys.stdout.write(platform.architecture()[0])\"'\r\n", "comments": ["Hi @mateusz1981 ! \r\nCould  you  please fill the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].  \r\n\r\nPlease check for the environment variable path  of python installations . Reference - https://github.com/rstudio/reticulate/issues/425\r\n\r\nIt also seem you are trying Python 2.7 ,Supported  versions are Python 3.6-3.9\r\nAlso check requirements and instructions  [here](https://www.tensorflow.org/install) for installing Tensorflow  .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51814\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51814\">No</a>\n"]}, {"number": 51809, "title": "Gradient accmulation with LSTM achieving lower accuracy on testset", "body": "Hi,\r\n\r\nSorry if this is not the right place. I'm really at a loss for my problems.\r\nI'm using TF 2.4, and I'm trying to do gradient accumulation with my model, which is three layers of LSTM connected to three Dense layers. I'm also using a \"sub-network\" trick-- in each layer of my model, the first x% neurons are taken to form a sub-network, with its weights shared with the full network. (For example, if the full net is two Dense layers with 10 and 20 neurons each, then a sub-network with x being 50% is just two Dense layers with 5 and 10 neurons each.) There are three sub-networks currently, each with a different value of said x (named width_mult in the following code). In the training, I add up the gradients for the full network and the three sub-networks, and then update the weights.\r\n\r\nThe problem is, by training in this way, I'm getting lower accuracy on testset (~82% or even lower) than direct training (~89%, i.e., without gradient accumulation and the sub-network thing). But it seems that the accuracies for both full and sub-network during training are high. Also, if I get rid of the LSTM layers, the testing accuracy goes up. So I'm wondering if there is something wrong with my implementation (especially in the gradients padding part, see below), but I can't spot a bug myself. Can someone help? Perhaps a glaring error that I overlooked?\r\n\r\n[original code deleted]", "comments": ["@hx1997,\r\n\r\nI see that you are using `TF 2.4`, I would suggest you can update to latest stable version i.e `2.6.0` and if the issue still persists, Please share a reproducible code snippet to expedite the trouble-shooting process. Thanks!", "Hi Sanat,\r\n\r\nThanks for replying! I'm a little busy recently, but I'll try your suggestions and share reproducible code if necessary.", "@hx1997,\r\n\r\nSure, Please let me know if it works, or if the issue still persists in `2.6.0`. Thanks!", "Hi,\r\n\r\nI (sadly) haven't got time to install TF 2.6.0 yet, but a minimized and reproducible version of my code can be found here (`grad_repro_train.py` is the training code, `utils_learning_rnn.py` is the model): [code withdrawn]\r\n\r\nNote that for some reason on my part, I'm not allowed to share the dataset I use to train the model, so I've replaced the input to the model with random numbers (as you can see in lines 170-174 in `grad_repro_train.py`). On my dataset, I got lower accuracy by training using this piece of code than using a plain `model.fit()`.\r\n\r\nI will try updating TF as soon as I got more time.", "Hi,\r\n\r\nI found out the cause of my problem. It was because I didn't enable dropout in the model used in this code, whereas in direct training, I enabled it. Setting `dropout` when creating LSTM layer AND specifying `training=True` when doing forward pass raised the accuracy to ~88%, solving the problem.\r\n\r\nThank you for your time and attention, Sanat!"]}, {"number": 51808, "title": "[Intel MKL] Bug fix for Pattern Matcher", "body": "This PR fixes a bug inside pattern matcher for grappler. The bug was due to not considering the nodes_to_preserve in the remapper use case.", "comments": []}, {"number": 51807, "title": "CUDA Graphs in XLA", "body": "Not intended to merge as is.", "comments": ["Please open against master, not release branches"]}, {"number": 51806, "title": "Fix abseil compile error using proper HWAES flag on aarch64.", "body": "This small PR fixes a compile error on aarch64.\r\n\r\n   - It backports a future [absl upstream fix](https://github.com/abseil/abseil-cpp/commit/2e94e5b6e152df9fa9c2fe8c1b96e1393973d32c) of the issue\r\n   -  Fixes compilation [reported in the issue #51750](https://github.com/tensorflow/tensorflow/issues/51750#issuecomment-910717534) \r\n   \r\nOther way to address the issue is to rise up [abseil requirement](https://github.com/tensorflow/tensorflow/blob/master/third_party/absl/workspace.bzl#L10) to the minimum of this [commit hash](https://github.com/abseil/abseil-cpp/commit/2e94e5b6e152df9fa9c2fe8c1b96e1393973d32c).\r\n\r\nThank you !", "comments": ["The PR pass a complete targeted build on [copr aarch64](https://copr.fedorainfracloud.org/coprs/rezso/ML/build/2681986/ )\r\n\r\n\r\n", "LGTM?", "Can someone check why it failed?", "Should be good now and land soon"]}, {"number": 51805, "title": "Can not install python3-pip", "body": "Hello, I am new to python. I was trying to install pip, but I get and error whitch I can not find a way to fix it\r\nCode that I am running:\r\n`sudo apt install python3 python3-pip`\r\nWhat I am getting:\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree... Done\r\nReading state information... Done\r\npython3 is already the newest version (3.9.4-1).\r\nYou might want to run 'apt --fix-broken install' to correct these.\r\nThe following packages have unmet dependencies:\r\n nodejs : Depends: libnode72 (= 12.21.0~dfsg-3ubuntu1) but it is not going to be installed\r\n python3-pip : Depends: python3-setuptools but it is not going to be installed\r\n               Depends: python3-wheel but it is not going to be installed\r\n               Depends: python-pip-whl (= 20.3.4-1ubuntu2) but it is not going to be installed\r\n               Recommends: python3-dev (>= 3.2) but it is not going to be installed\r\nE: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\r\n\r\n```\r\nAlso tried to do `sudo apt --fix-broken install` like statet in the last line, but got this:\r\n```\r\n\r\nUnpacking nodejs (16.8.0-deb-1nodesource1) over (12.21.0~dfsg-3ubuntu1) ...\r\ndpkg: error processing archive /var/cache/apt/archives/nodejs_16.8.0-deb-1nodesource1_amd64.deb (\r\n--unpack):\r\n trying to overwrite '/usr/share/doc/nodejs/api/process.html', which is also in package nodejs-do\r\nc 12.21.0~dfsg-3ubuntu1\r\ndpkg-deb: error: paste subprocess was killed by signal (Broken pipe)\r\nErrors were encountered while processing:\r\n /var/cache/apt/archives/nodejs_16.8.0-deb-1nodesource1_amd64.deb\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n\r\n\r\n```\r\n\r\nAny ideas on how to fix it?\r\nRunning Linux PopOs\r\nnpm -v: 6.14.15", "comments": ["Hi @Burtininkas69 , This does not look like a Tensorflow issue. You can have a look a the instructions[ here ](https://docs.python-guide.org/starting/install3/linux/) though in case you might have missed some steps. ", "@Burtininkas69  You can find the Solution [here](https://askubuntu.com/questions/148383/how-to-resolve-dpkg-error-processing-var-cache-apt-archives-python-apport-2-0)", "Not a TF issue, please open only TensorFlow related issues here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51805\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51805\">No</a>\n"]}, {"number": 51804, "title": "Hexagon Delegate crashes on max pooling operation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855 dev platform\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nAfter converting YOLOv4 from Tensorflow to TfLite, the model runs well on mobile device, on CPU.\r\nWhen running on DSP using hexagon delegate, I get the following error:\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: TfLiteHexagonDelegate delegate: 535 nodes delegated out of 568 nodes with 2 partitions.\r\nINFO: Replacing 535 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 5 partitions.\r\n\r\n----------------\r\nTimestamp: Sun May 30 16:21:13 2021\r\n\r\n\r\nLog\r\nhexagon/ops/src/op_maxpool_d32.c:567:insufficient width padding\r\nhexagon/src/execute.c:167:execute() failed on node id=689 err=-1\r\nhexagon/src/interface.c:1297:fail in execute_inner()\r\n\r\n----------------\r\nERROR: Failed: Failed to execute graph..\r\nERROR: Node number 568 (TfLiteHexagonDelegate) failed to invoke.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect it to run successfully. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nSee attached model. \r\n\r\n**Other info / logs** \r\nI've narrowed it down to a single max pool operation that is later concatenated. The operation in question takes a 1x13x13x512 input tensor and computes tf.nn.max_pool(input_data, ksize=13,padding = \"SAME\", strides=1). other instances of this operator with ksize = 9 and ksize =5 (all other parameters are the same) work just fine. \r\n\r\nAnother thing I noticed is that depending on what else is going on in the model, sometimes the converter emits a TfLite model that works. However, it is intermittent and I was unable to reproduce this behavior reliably \r\n\r\nSee model attached [here](https://drive.google.com/file/d/1sAh0i-hV2936tVGKliyX5sMcWbfxqU3-/view?usp=sharing).", "comments": ["In case you want replicate creating the quantized model:\r\n\r\nHere's a [zip file](https://drive.google.com/file/d/1J3kg2Ul0w0Dw9w29nU4MFRKD7tjv47B8/view?usp=sharing) with the TensorFlow saved_model (./checkpoints/yolov4-416), the original model definitions (./core), the representative dataset based on COCO (./data/dataset/val2017) and the conversion script: convert_tflite.py \r\nwhich you run like this:\r\npython convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416-int8.tflite --quantize_mode int8 --dataset ./data/dataset/val2017.txt\r\n\r\nyou can find the problematic max_pool being used at ./core/backbone.py, line 99. ", "> Another thing I noticed is that depending on what else is going on in the model, sometimes the converter emits a TfLite model that works. However, it is intermittent and I was unable to reproduce this behavior reliably\r\n\r\nThis is certainly weird. What do you mean by 'intermittent'? Does it:\r\n\r\n1. Not convert a model sometimes, with a conversion error? OR\r\n2. Converts a model that runs on Hexagon just fine?", "I meant the second option. Sometimes I get a model that runs fine, sometimes I get a model that crashes with the error reported above. One caveat - I haven't yet verified the correctness of the output when executed on Hexagon, just that execution terminates without runtime errors (when it runs fine). However, the same model runs fine on PC, always - and the output is correct as well. ", "Hi, are there any updates on this? Is there anything I can do to help debug this?", "Sorry for the delay, we had a long weekend in the US. If possible can you paste the models that 1) runs fine on Hexagon & 2) gives an error with Hexagon? If you only intermittently see Hexagon errors, then there might be some issue with the converter. I can compare the two models & see what is different between the two.", "I've tried many times to recreate a model that runs on Hexagon, but I couldn't. They all failed to run as I've described above. It's likely the intermittency I've seen is just confusion on my part. \r\n\r\nCan we discuss why max_pool with size (13,13) fails?\r\n\r\nIn the meantime, I've replaced the max_pool operator with kernel size (13,13) with a chain of two smaller max_pool operators that work fine - (9,9) and (5,5), and a multiplication in the following manner:\r\n```\r\n### replaced this:\r\n  input_data = tf.concat([tf.nn.max_pool(input_data, ksize=13, padding='SAME', strides=1), \r\n                                       tf.nn.max_pool(input_data, ksize=9, padding='SAME', strides=1),\r\n                                        tf.nn.max_pool(input_data, ksize=5, padding='SAME', strides=1), \r\n                                         input_data], axis=-1)\r\n\r\n### with this:\r\n    step1 = tf.nn.max_pool(input_data, ksize=9, padding='VALID', strides=1)\r\n    step2 = tf.nn.max_pool(step1, ksize=5, padding='VALID', strides=1)\r\n    step3=tf.math.multiply(tf.constant(1.0, shape=(1, 13, 13, 512), name='test'), step2)\r\n    \r\n    input_data = tf.concat([step3, tf.nn.max_pool(input_data, ksize=9, padding='SAME', strides=1)\r\n                            , tf.nn.max_pool(input_data, ksize=5, padding='SAME', strides=1), input_data], axis=-1)\r\n```\r\n\r\nThis works. But is there a way to resolve this without ugly workarounds? \r\n", "The error comes from within the Hexagon kernel ([see code here](https://github.com/XiaoMi/nnlib/blob/master/hexagon/ops/src/op_maxpool_d32.c#L566)), so you can probably follow the logic there (with your op's IO dimensions & arguments) to get a better idea of why this particular instance of the op is failing. I am a bit swamped at the moment, so might not have the time to dig myself. Could you help take a look?\r\n\r\n\r\nI am glad you atleast found a workaround for your use-case :-)", "For now, we have disabled accepting MaxPool operations if padding is SAME and filter size > 12 (which is where we see errors in our unit tests too). It is unfortunately not very likely that this will be fixed in the Hexagon SDK code very soon, but the TFLite delegate interface should work fine & run the rest of the graph correctly even if such ops are present.", "@yakovdan Can you retry from nightly / HEAD", "Closing the issue, as this shouldn't be happening now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51804\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51804\">No</a>\n"]}, {"number": 51802, "title": "how to use SparseTensor with ctc loss where the label's length varies within one batch", "body": "how to use SparseTensor with ctc loss where the label's length varies within one batch. \r\n\r\nIDK how to formulate a sparse tensor with different lengths and pass into the ctc loss. any example here? \r\n", "comments": ["Hi @ysyyork ! Please have a look at this[ link](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss) for answer. But We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "Hi @mohamedadaly \r\n\r\nThanks for your reply!\r\n\r\nSorry, i didn't follow the format because all I want is just some explanation but not a bug. \r\n\r\nBut the tf version is 2.4.0. I already read all the API doc about ctc loss but just that I'm a bit confused about how to make it work with SparseTensor. \r\n\r\nMy question is that, for example, if I have labels like below:\r\n```\r\n[\r\n  [1, 0, 1, 0, 1]\r\n  [1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\r\n  [1, 0, 1, 1, 1, 0]\r\n]\r\n```\r\nHow to indicate the different lengths with SparseTensor?\r\n\r\nFor dense labels, I can pad them all to the same length and then pass in the label_length vector to say the lengths are [5, 10, 6]. But if I pass in a SparseTensor, it didn't allow me to pass in the label_length (it has to be None). But to represent the above labels, I can only do \r\n```\r\nSparseTensor(\r\n    values=[...],\r\n    indices=[...],\r\n    shape=[3, 10]\r\n)\r\n```\r\nIf I decode this sparse tensor into a dense one, it becomes the below:\r\n```\r\n[\r\n  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0]\r\n  [1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\r\n  [1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\r\n]\r\n```\r\nThen the first and last labels are actually the same. The length info seems lost in this case. \r\n\r\nSo I just want to get a clearer SparseTensor example about how to formulate this SparseTensor object so that the length info is not lost. ", "Ok ! We will update the document with an example , Feel free to look at the [comment](https://github.com/tensorflow/tensorflow/issues/41353#issuecomment-658059572) of earlier similar[ issue](https://github.com/tensorflow/tensorflow/issues/41353).  Providing a sample [Gist ](https://colab.research.google.com/gist/mohantym/3a9847de1628a42d12a2d3d64a495d8c/ctc_loss.ipynb#scrollTo=nQX4zZx0eKJG)as an example. Thanks!", "@mohantym Thanks for this. this is helpful. Just to make sure. Does that mean instead of using 1 and 0 as the label, I should use tf.nn.log_softmax([1, 0, ..]) to represent the label and the 0 will finally becomes the blank label? \r\n", "Hi @ysyyork , Please update once after trying that , You can also look in this [issue ](https://stackoverflow.com/questions/64321779/how-to-use-tf-ctc-loss-with-variable-length-features-and-labels). \r\n For any further queries you may open this issue in tf discussion [forum](https://discuss.tensorflow.org/) as there is a larger community there as This is not a bug or feature request,Thank you!", "I have figured out myself. Thanks!\n\nOn Mon, Sep 6, 2021 at 11:37 AM mohantym ***@***.***> wrote:\n\n> Hi @ysyyork <https://github.com/ysyyork> , Please update once after\n> trying that , You can also look in this issue\n> <https://stackoverflow.com/questions/64321779/how-to-use-tf-ctc-loss-with-variable-length-features-and-labels>\n> .\n> For any further queries you may open this issue in tf discussion forum\n> <https://discuss.tensorflow.org/> as there is a larger community there as\n> This is not a bug or feature request,Thank you!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/51802#issuecomment-913313352>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABTCFZJ6OKC3O26W7OSB5M3UAQZO7ANCNFSM5DJK5H3Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "Ok! Could you please close this issue then ?", "sure", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51802\">No</a>\n"]}, {"number": 51801, "title": "When loading a saved model with multiple outputs, outputs order is not deterministic", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 11.2\r\n- GPU model and memory: Nvidia Quadro T1000\r\n\r\n**Describe the current behavior**\r\nI have a model that outputs several tensors, each with a different shape and dtype, let's call it model A.\r\nModel's A output tensors are the input of another model B.\r\nWhat I would like to do is to be able to save model A (using model.save(\"...\")), and then use it later by loading it.\r\nI found out that when loading model A and calling it, the outputs order in non deterministic, which causes a problem when I try to use those outputs as inputs to model B.\r\n\r\n**Describe the expected behavior**\r\nI'd like the model to maintain the original outputs order\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass A_Pow(keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(A_Pow, self).__init__(**kwargs)\r\n        self.input_spec = tf.keras.layers.InputSpec(shape=(1, 2), dtype=tf.float32)\r\n\r\n    def call(self, input):\r\n        return input * input\r\n\r\nclass B_AddX(keras.layers.Layer):\r\n    def __init__(self, x, **kwargs):\r\n        super(B_AddX, self).__init__(**kwargs)\r\n        self.x = x\r\n        self.input_spec = tf.keras.layers.InputSpec(shape=(1, 3), dtype=tf.int32)\r\n\r\n    def call(self, input):\r\n        return input + self.x\r\n\r\n\r\npow_input = tf.keras.Input(shape=(2), batch_size=1, dtype=tf.float32)\r\naddx_input = tf.keras.Input(shape=(3), batch_size=1, dtype=tf.int32)\r\npow_output = A_Pow()(pow_input)\r\naddx_output = B_AddX(2)(addx_input)\r\nmodel = tf.keras.Model(inputs=[pow_input, addx_input], outputs=[pow_output, addx_output])\r\n\r\nmodel_output = model([tf.constant([1,2], shape=(1,2), dtype=tf.float32), tf.constant([3,4,5], shape=(1,3), dtype=tf.int32)])\r\n\r\nmodel.compile()\r\nmodel.save(\"multiple_outputs_model\")\r\n\r\nloaded_model = tf.saved_model.load(\"multiple_outputs_model\")\r\nloaded_model = loaded_model.signatures[\"serving_default\"]\r\n\r\nloaded_model_output = loaded_model(input_1=tf.constant([1,2], shape=(1,2), dtype=tf.float32), input_2=tf.constant([3,4,5], shape=(1,3), dtype=tf.int32))\r\n\r\nprint(model_output)\r\nprint(loaded_model_output)\r\n\r\n# The order of the outputs in the loaded model is not determoinistic\r\n# one run output:\r\n#   [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>]\r\n#   {'b__add_x': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>, 'a__pow': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>}\r\n\r\n# another run output:\r\n#   [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>]\r\n#   {'a__pow': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, 'b__add_x': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>}\r\n\r\n# The second run has the valid output (a__pow before b_pow)\r\n\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@nitsanhasson \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "@sushreebarsa  Posted there, thanks", "@nitsanhasson Thanks for the update! Could you Please move this issue to closed status as you have posted the same in keras repo, you will get the right help there?Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51801\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51801\">No</a>\n"]}, {"number": 51800, "title": "Android GPU delegate gives same output from intermediate layers output ", "body": "Hi Team\r\n\r\nI have trained a mobilnetv2  model using the tfod API.  Along with the normal detector output (bbox, score,num_detection, confidences), I also wanted an intermediate layers output, so  while converting the model to tflite i specified the intemediate node from where i needed the output ('FeatureExtractor/MobilenetV2/expanded_conv_9/add') as illustrated below : \r\n\r\n`tflite_convert --graph_def_file=$1/tflite_graph/tflite_graph.pb --output_file=$1/tflite_graph/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3','FeatureExtractor/MobilenetV2/expanded_conv_9/add' --inference_type=FLOAT --allow_custom_ops\r\n`\r\n\r\nthe resulted graph is like this : \r\n\r\n<img width=\"1659\" alt=\"Screenshot 2021-09-02 at 7 38 20 PM\" src=\"https://user-images.githubusercontent.com/74127861/131858728-3b53a171-2e37-4848-ab57-2932f20d52b4.png\">\r\n\r\nWhen I run the tflite model on the android with the XNNPack or CPU, I am able to get the different output  of the intermediate layer which is coming at index 4 \r\ncode snippet : \r\n```\r\ntry {\r\n      Interpreter.Options options = new Interpreter.Options();\r\n      options.setNumThreads(NUM_THREADS);\r\n      options.setUseXNNPACK(true);\r\n      d.tfLite = new Interpreter(modelFile, options);\r\n      // 4th index for intermediate output\r\n      int[] shape = d.tfLite.getOutputTensor(4).shape();\r\n      d.tfLiteOptions = options;\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n\r\n```\r\n\r\nBut when i run the same model on GPU delegate, I am getting the same output for every different frame I am passing. The other output like (bbox, score,num_detection, confidences) changes but the intermediate output remains the same \r\n```\r\ntry {\r\n      Interpreter.Options options = new Interpreter.Options();\r\n      CompatibilityList compatList = new CompatibilityList();\r\n      GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();\r\n      GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);\r\n      options.addDelegate(gpuDelegate);\r\n\r\n      d.tfLite = new Interpreter(modelFile, options);\r\n      // 4th index for intermediate output\r\n      int[] shape = d.tfLite.getOutputTensor(4).shape();\r\n      d.tfLiteOptions = options;\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n```\r\n\r\n**System information**\r\n- Mobile device: OnePlus 6T, Mi Note5 pro\r\n- TFOD Api TensorFlow version : 1.15\r\n- Android Tensorflow Lite version: 2.4.0\r\n\r\nNeed help to figure out what may be going wrong\r\n\r\n[detect_10.tflite.zip](https://github.com/tensorflow/tensorflow/files/7099629/detect_10.tflite.zip)\r\n", "comments": ["@letdivedeep,\r\n\r\nI see that you have mentioned the TF version as `1.5` which is no more supported officially. Can you try updating to latest stable version i.e `2.6.0` and let us know if the issue still persists? Thanks!", "@sanatmpa1 Thanks for the response\r\n\r\n I mean to say I trained a model with tfod API with TensorFlow 1.15 version. But while inferencing the tflite on the android I used 'org.tensorflow:tensorflow-lite:2.4.0' version \r\n \r\n I even tired with 'org.tensorflow:tensorflow-lite:2.6.0' version same error", "@ymodak .. Just to update you on this part.\r\n\r\nI even tried to run the model on GPU delegate on the Linux platform, it gives the same issue \r\n", "Because GPU delegate reuses memory objects, you cannot dump intermediate tensors.  If you really wish to do so, you have to insert a fake node, e.g. ADD 1e-6, and declare the output of that op as graph output.", "@impjdi  thanks for the suggestion.  I will try this approach and let you knaow", "> Because GPU delegate reuses memory objects, you cannot dump intermediate tensors. If you really wish to do so, you have to insert a fake node, e.g. ADD 1e-6, and declare the output of that op as graph output.\r\n\r\n@impjdi this worked. thanks for the inputs", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51800\">No</a>\n"]}, {"number": 51798, "title": "tensorflow2 memory leak when using tf.function", "body": "I post this question here because no one is able to answer it for over a month. Hope someone here would give the solution.\r\nhttps://stackoverflow.com/questions/68379996/how-to-profile-tensorflow-memory", "comments": ["Please resubmit and pay attention to the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Please provide all the information it asks. Thank you.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51798\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51798\">No</a>\n"]}, {"number": 51797, "title": "Interpreter invocation error after input tensor resizing ", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0\r\n\r\nError: Post reshaping the input sizes of my tensors - the interpreter invocation fails with an input-output shape mismatch. [ Stack trace in the linked colab file ]. \r\n\r\n### 2. Code\r\n\r\n[Here](https://colab.research.google.com/drive/1Vsinmju5FsbiEtfD9BqBHWeKxM3LES-U?usp=sharing ) is the end-end flow of conversion. You can refer to the test files used to generate the test data ( for dummy input ) here: [dev.hi](https://drive.google.com/file/d/1HJtbqzF3X-51S01DFGn5XOUambvMQriO/view?usp=sharing ) and [dev.en](https://drive.google.com/file/d/1IKqg2c-SJiKmd2fsFmODxeB0af3rBATl/view?usp=sharing)\r\n\r\n### 3. Failure after conversion\r\n\r\nThe model gets converted ( and quantized ) successfully though despite setting the model inputs with a valid input sample ( during model.predict(), I have also explicitly tried using model._set_inputs() - that doesn't work either ) the input shapes of the graph are [1,1] for all the arguments. I am forced to reshape them hence, and that's when the error occurs. \r\n\r\nNote: In the colab file, I have shown the input details pre and post resizing - while the argument shape seems to be altered according to my resizing shapes - the 'shape_signature' does not change. Could this be the source of the error ? ( Is this expected behaviour ) \r\n\r\n### 5. (optional) Any other info / logs\r\n\r\nThe linked colab file has the exact stack traces. Note that if I do not reshape my tensors - the invocation runs but due to the [1,1] limitation - there is no meaningful input that I can provide to the model. ", "comments": ["If i am not mistaken, shape_signature [-1, 1]  means first dimension can be resized but not the second dimension. you are trying to resize second dimension which is invalid.\r\nIf you passed parameter strict=True to resize_tensor_input you should get invalid resize warning\r\n\r\n```\r\ninterpreter.resize_tensor_input(input_details[0]['index'], np.shape(batch['attention_mask']), strict=True)\r\n```", "> If i am not mistaken, shape_signature [-1, 1] means first dimension can be resized but not the second dimension. you are trying to resize second dimension which is invalid.\r\n> If you passed parameter strict=True to resize_tensor_input you should get invalid resize warning\r\n> \r\n> ```\r\n> interpreter.resize_tensor_input(input_details[0]['index'], np.shape(batch['attention_mask']), strict=True)\r\n> ```\r\n\r\nThanks for clarifying that. The documentation indicates the same - which is why I was asked if this was expected behavior - since I didn't get an error in the first place  ( when the tensors are resized, they appear to be converted to the altered tensor successfully ); \r\n\r\n[Side Note: This probably goes as a separate issue ] Additionally, in the issue description I have mentioned that the frozen graph size [1,1] is not expected behavior - the conversion process is altering ( in this case making the input size [1,1] ) the saved model's input format despite setting model inputs to a different value explicitly. \r\n", "You will need to set strict=True to get the error.\r\n\r\n> [Side Note: This probably goes as a separate issue ] Additionally, in the issue description I have mentioned that the frozen graph size [1,1] is not expected behavior - the conversion process is altering ( in this case making the input size [1,1] ) the saved model's input format despite setting model inputs to a different value explicitly.\r\n\r\nCan you share the saved model, and how you're converting it. Conversion shouldn't alter the model input. In the sample you shared above it looks like your model has [-1, 1] shape.", "> You will need to set strict=True to get the error.\r\n>\r\n> > [Side Note: This probably goes as a separate issue ] Additionally, in the issue description I have mentioned that the frozen graph size [1,1] is not expected behavior - the conversion process is altering ( in this case making the input size [1,1] ) the saved model's input format despite setting model inputs to a different value explicitly.\r\n> \r\n> Can you share the saved model, and how you're converting it. Conversion shouldn't alter the model input. In the sample you shared above it looks like your model has [-1, 1] shape.\r\n\r\n1. Noted. \r\n2. The sample specified above does mirror the exact conversion process I follow ( and how I generate the Saved Model). I edited the code so that it now specifies the explicit assignment of inputs to the model ( before saving, and that shape is not [1,1] ) as well.\r\n\r\nI was able to trace the inconsistent behavior ( of model.save() altering the outputs ) to a embedding resizing that I perform during the model training ( I train using the Hugging Face TFTrainer API). If I do not resize the model embeddings - and then save the model: the behavior is as expected i.e model.save() does not alter the inputs and the tflite file is generated with the inputs that it was saved with). Having said that - Even in that case: I cannot resize the tensors at runtime and get the same error. Kindly let me know if this out of scope for resolution ( due to the cross-platform dependency on HF ) and I shall close the issue for the time being.  \r\n\r\nEdit: I just noticed another AutoGraph Warning in the beginning of the stack trace; Kindly let me know If I should file a separate issue for that as indicated. ", "I am not familiar with HF API, but if you can make the shapes dynamic in a correct way before saving, then you can resize correctly after converting.\r\n\r\n> Edit: I just noticed another AutoGraph Warning in the beginning of the stack trace; Kindly let me know If I should file a separate issue for that as indicated.\r\n\r\nYes file another Issue.\r\n\r\nI am closing the issue since it is not a problem with TFLite. Feel free to file issues if you're having a problem.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51797\">No</a>\n"]}, {"number": 51796, "title": "Attributes section in Keras Model (documentation) is not generated properly", "body": "Model attributes section in the documentation has 2 issues:\r\n\r\n1. It is duplicated: [attributes](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#attributes) and [attributes_1](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#attributes_1). You might have to look at table of content to spot the difference.\r\n2. Some attributes are missing even though they have been documented and have @property decorator in the github code. E.g [metrics](https://github.com/bizzyvinci/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L635-L683) is missing but [metrics_name](https://github.com/bizzyvinci/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L685-L723) is included.\r\n\r\nNote that the documentation link is nightly and the code link is the current master. I guess there might be issue with how the attribute section is generated.\r\n", "comments": ["Hi @Saduf2019. Could you please look at this issue!", "@bizzyvinci \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "Ok. Thanks.\r\n"]}]