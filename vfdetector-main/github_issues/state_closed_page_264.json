[{"number": 46501, "title": "Sagemaker endpoint serving doesnt work for multiple timestep `and` multiple inputs (mulit input-output LSTM)", "body": "Update: Minimum Reproducible code\r\nhttps://colab.research.google.com/drive/1RJ2ou705xfrqjeFGImvMk02lTxfFPZek?usp=sharing\r\n\r\nI have a LSTM network that has 3 inputs and 3 outputs(built with [functional api][1] in Tf.keras) , that I am trying to deploy as sagemaker endpoint. I have input shape of (None,10,1) for each input/feature, which means 10 timesteps.(I later concatenate the embeddings, but its irrelevant here) \r\n\r\nEverything works fine on training time on sagemaker training jobs as well and training completes and artifacts are made successfully. But at time of invocation, endpoint is not working to predict `1 example, having 10 timesteps with 3 inputs` , I have tried multiple things but cant provide three inputs for prediction(input_1,input_2,input_1).\r\n\r\nAs I said that each input has 10 timesteps, so have shape (10,1). Endpoint only returns the output if I format my payload as below, but by doing so, it treats each time-step as separate example/instance and return 10 predictions for each output\r\n\r\n    {'inputs':{\r\n               'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\r\n               'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\r\n               'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\r\n              } # gives len(pred['output_1\"])) == 10\r\n\r\nThis is expected as it consider this request as 10 examples, but in my case it is one example with 10-timesteps for each feature (1,10,1). So I tried different things from the [documentation][2]. Like using instances.\r\n\r\n    {'instances': [\r\n                    {\r\n                          'input_1': [[0],[0], [0],[0],[2],[12],[11], [7], [7], [2]],\r\n                          'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\r\n                          'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]\r\n                    }\r\n                  ]\r\n    }\r\n\r\nBut it gives this error.\r\n\r\n>  transpose expects a vector of size 4. But input(1) is a vector of\r\n> size 3\\\\n\\\\t [[{{node transpose_1}}]]\\\\n\\\\t\r\n> [[functional_1/lstm/PartitionedCall]]\\\\n\\\\t\r\n> [[StatefulPartitionedCall/StatefulPartitionedCall]]\\\"\\n}\"}\"\r\n\r\nDocument also gives example and says\r\n\r\n> for models with multiple named inputs, just include all the keys in the input dict\r\n\r\nbut when I use this, I get error saying `Missing 'inputs' or 'instances' key\\\"\\n}\"`\r\n\r\n    {'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\r\n     'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\r\n     'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\r\n\r\n\r\nMy invocation code is below.\r\n\r\n    import boto3\r\n    import json\r\n    \r\n    sm = boto3.client('sagemaker-runtime')    \r\n    endpoint_name = \"tensorflow--------------------4\"\r\n    response = sm.invoke_endpoint(EndpointName=endpoint_name, \r\n                                  Body=json.dumps(payload),\r\n                                  ContentType='application/json')\r\n\r\n\r\n\r\nI am not sure how to solve this issue, looking forward for help\r\n\r\n\r\n  [1]: https://keras.io/guides/functional_api/\r\n  [2]: https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html\r\n\r\n\r\n\r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Sagemaker and Ubuntu 20\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI am not sure about the standalone code as sagemaker endpoint is confidential, and what should I share for this, i would be thankful for the suggestion on this too.", "comments": ["@abdulbasitds \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram  I am not sure how to provide TF serving code, but I have tried to reproduce the problem with minimum code possible here:\r\n\r\nhttps://colab.research.google.com/drive/1RJ2ou705xfrqjeFGImvMk02lTxfFPZek?usp=sharing", "@abdulbasitds \r\n\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here](https://github.com/tensorflow/serving/issues/new). Thanks!", "Have made this issue on this repo, thanks for letting me know", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46501\">No</a>\n"]}, {"number": 46500, "title": "[TFLM] Added support for optimized fully connected op for CEVA-DSP BX1 and SP500", "body": "As described in the github issue: https://github.com/tensorflow/tensorflow/issues/45607\r\n\r\nWe will be porting and adding a considerable number of operations to CEVA-DSP cores.\r\nStarted with Quantize (https://github.com/tensorflow/tensorflow/pull/46226) and this is fully connected, both int8 and float32 are supported.\r\n\r\n@advaitjain ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@yair-ehrenwald  Can you please resolve conflicts? Thanks!", "> I have taken a review pass at the makefile as well.\r\n> \r\n> Let me know once you pull in the changes from #46703 and address my earlier comments about sharing the code and structs from fully_connected_common.cc.\r\n> \r\n> Since this PR has already gone through a lot of changes, feel free to rebase if that makes it easier for you.\r\n\r\nI made some changes to the Makefiles and opened a new PR for them since this is getting a bit messy: \r\nhttps://github.com/tensorflow/tensorflow/pull/46841\r\nTook the opportunity to update the makefiles for our latest toolchain which should be required for building in the optimized files.", "@yair-ehrenwald Can you please check @advaitjain's comments and keep us posted ? Thanks!", "@yair-ehrenwald, just making sure that you are not waiting on me for this change. After the small changes that I requested in my most recent review, we can get this merged.", "> @yair-ehrenwald, just making sure that you are not waiting on me for this change. After the small changes that I requested in my most recent review, we can get this merged.\r\n\r\nShould be done now :)\r\n"]}, {"number": 46499, "title": "TfLite converter crashes when quantization is enabled", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A \r\n- GPU model and memory: N/A\r\n\r\nI have TF model in saved_model format. When converting to TFlite without quantization, everything works and I'm able to run inference, no problem. When converting using full integer quantization with a representative dataset, I get the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/REPOSITORIES/PoolNet2TFv2/tfliteInf.py\", line 93, in <module>\r\n    quantize(\"poolnet_640_tf\")\r\n  File \"C:/REPOSITORIES/PoolNet2TFv2/tfliteInf.py\", line 35, in quantize\r\n    quant_model = converter.convert()\r\n  File \"C:\\REPOSITORIES\\PoolNet2TFv2\\venv38local\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"C:\\REPOSITORIES\\PoolNet2TFv2\\venv38local\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 899, in convert\r\n    return super(TFLiteFrozenGraphConverterV2,\r\n  File \"C:\\REPOSITORIES\\PoolNet2TFv2\\venv38local\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 638, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"C:\\REPOSITORIES\\PoolNet2TFv2\\venv38local\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 450, in _calibrate_quantize_model\r\n    return calibrate_quantize.calibrate_and_quantize(\r\n  File \"C:\\REPOSITORIES\\PoolNet2TFv2\\venv38local\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 95, in calibrate_and_quantize\r\n    return self._calibrator.QuantizeModel(\r\nRuntimeError: Quantization not yet supported for op:\r\n\r\nsee [here](https://drive.google.com/file/d/19D3BuJSEXHsM0eH0L1Cad3gX-9JkkXYX/view?usp=sharing) a zip file containing the TF model (poolnet_640_tf), a folder with 3 images for the representative dataset (norm_images) and quantize.py, my conversion code. \r\n\r\nThank you\r\n\r\n\r\n", "comments": ["@daverim @teijeong @liufengdb @MeghnaNatraj  Could you take a look at this?", "@yakovdan,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/04fff092e402aab660fec5c2169c457e/46499-2-3.ipynb) and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/a7e65574fb185f0b05dbcfa38e46e491/46499.ipynb). \r\n\r\nHowever, I did not face any errors and was able to convert the model to `.tflite` with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2f27821ddb635d90702dadc3e5ab4b82/46499-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "This works for me as well with the latest TF-nightly. However, inference is now broken. See [here](https://drive.google.com/file/d/17lLtNZyJOCD8UCy4Bf-nAXB2gyuCs0IZ/view?usp=sharing) for code, model and input image. \r\n\r\nI get the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"infere.py\", line 56, in <module>\r\n    result = run_tflite_model(img,sys.argv[1]+\".tflite\").astype(np.uint8)[0,0,:,:]\r\n  File \"infere.py\", line 34, in run_tflite_model\r\n    interpreter.allocate_tensors()\r\n  File \"c:\\REPOSITORIES\\PoolNet2TFv2\\venv38local\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\", line 335, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: tensorflow/lite/kernels/pad.cc:118 op_context.input->type != op_context.constant_values->type (INT8 != FLOAT32)Node number 6 (PADV2) failed to prepare.\r\n", "In the folder [here](https://drive.google.com/file/d/17lLtNZyJOCD8UCy4Bf-nAXB2gyuCs0IZ/view?usp=sharing), I've downloaded the model 'poolnet_quant.tflite' and visualized it in [netron](https://lutzroeder.github.io/netron/). (refer to the image below)\r\n\r\n@daverim any idea why the node \"PadV2/constant_values;StatefulPartitionedCall/PadV2/constant_values\" still remains `float32` even though we have applied full integer quantization? \r\n\r\nHere's a [gist for reference](https://colab.research.google.com/gist/MeghnaNatraj/6471fa4e322dfd546a321b3f62dcccc6/46499-tf-nightly.ipynb) for model conversion and inference. The model converts successfully but when running inference, the code fails when we allocate tensors `interpreter.allocate_tensors()` which fails at [this line](https://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/lite/kernels/pad.cc#L116-L119) where you can see that if PADV2 op's \"constant_values\" attribute is defined (as it is in this model), it is expected to be `int8` but instead it is `float32`. (refer to the logs below)\r\n\r\n```\r\ninterpreter.allocate_tensors()\r\n...\r\n...\r\nTraceback (most recent call last):...\r\n...\r\nRuntimeError: tensorflow/lite/kernels/pad.cc:118 op_context.input->type != op_context.constant_values->type (INT8 != FLOAT32)Node number 6 (PADV2) failed to prepare.\r\n```\r\n\r\n![Screen Shot 2021-01-20 at 3 07 54 PM](https://user-images.githubusercontent.com/8790823/105156799-50fa3780-5b32-11eb-9360-3830c7f941fb.png)\r\n", "Hi, are there any updates on this?", "Are there any news on this? \r\n@MeghnaNatraj , @daverim \r\n", "@yakovdan we're currently looking into this issue and we'll let you know once we have an update.", "Hi,\r\n@MeghnaNatraj , it has been three weeks. Are there any updates on this or some plan to work on this?\r\n\r\nthank you,", "Hi @yakovdan,\r\n\r\nApologies for late update. With tf-nightly, you can specify `converter.experimental_new_quantizer = True` to enable new MLIR quantizer. That one quantized the constant padding correctly.\r\n\r\nHowever, as the constant value is really large, it will make all values after PadV2 op around zero. This is because the padding and input of PadV2 op should share the same scale. Rather than setting it to -FLT_MAX, consider giving it some reasonable value. It depends but if the dimensions can vary a bit, MaxPool2D with same padding would give similar result, too.", "Thank you for solving this. Can you explain why this large scale value is selected and how would I choose something more reasonable? \r\nI've noticed that most scale values are in the range [0,1.1] so I chose 0.5 as a test and I still got all zeros. ", "> so I chose 0.5 as a test and I still got all zeros.\r\n\r\nCan you elaborate what have you observed? Did you manually modified the scale for PadV2 op? you might also want to modify zero point, too.\r\n\r\nFrom the [quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec), the scale is determined by `(max - min) / quant_range` where `quant_range = 255` for int8 quantization. So when the range gets large like (1, -1000000000), scale becomes near zero, and zero point would be at 127 or so. The minimum value will be quantized to -128, but any meaningful value will likely be quantized to 127, and in turn which becomes zero.\r\n\r\nHow do you define your pad op in the original graph? you can assign more reasonable lower bound to the padded value for the quantization to work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46499\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46499\">No</a>\n"]}, {"number": 46496, "title": "Illegal instruction (core dumped) with tf <= that 1.15 and cpu with no AVX instructions", "body": "On a Intel Q9659 cpu that does not support AVX instructions, I have a `Illegal instruction (core dumped)`. So I have installed a previous version, but I'm still getting the error.\r\n\r\nThis Quad core cpu has the following flags\r\n\r\n```\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\n```\r\n\r\nand\r\n\r\n```\r\nloreto@ombromanto:~$ grep flags -m1 /proc/cpuinfo | cut -d \":\" -f 2 | tr '[:upper:]' '[:lower:]' | { read FLAGS; OPT=\"-march=native\"; for flag in $FLAGS; do case \"$flag\" in \"sse4_1\" | \"sse4_2\" | \"ssse3\" | \"fma\" | \"cx16\" | \"popcnt\" | \"avx\" | \"avx2\") OPT+=\" -m$flag\";; esac; done; MODOPT=${OPT//_/\\.}; echo \"$MODOPT\"; }\r\n-march=native -mssse3 -mcx16 -msse4.1\r\n```\r\n\r\n\r\n\r\nHere are detailed information about this system: \r\n\r\n`bash <(curl -s https://raw.githubusercontent.com/tensorflow/tensorflow/master/tools/tf_env_collect.sh) && cat tf_env.txt`\r\n\r\n```\r\n== check python ===================================================\r\npython version: 3.7.5\r\npython branch: \r\npython build version: ('default', 'Nov  7 2019 10:50:52')\r\npython compiler version: GCC 8.3.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020\r\nos release version: 5.4.0-42-generic\r\nos platform: Linux-5.4.0-42-generic-x86_64-with-Ubuntu-18.04-bionic\r\nlinux distribution: ('Ubuntu', '18.04', 'bionic')\r\nlinux os distribution: ('Ubuntu', '18.04', 'bionic')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='ombromanto', release='5.4.0-42-generic', version='#46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                1.19.5\r\nprotobuf             3.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\n...\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSun Jan 17 17:33:54 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 45%   23C    P8    N/A /  75W |    241MiB /  4033MiB |      1%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1053      G   /usr/lib/xorg/Xorg                 16MiB |\r\n|    0   N/A  N/A      1132      G   /usr/bin/gnome-shell               56MiB |\r\n|    0   N/A  N/A      1934      G   /usr/lib/xorg/Xorg                106MiB |\r\n|    0   N/A  N/A      2080      G   /usr/bin/gnome-shell               27MiB |\r\n|    0   N/A  N/A      3447      G   ...AAAAAAAAA= --shared-files       21MiB |\r\n|    0   N/A  N/A      6932      G   /usr/lib/firefox/firefox            1MiB |\r\n|    0   N/A  N/A      7286      G   /usr/lib/firefox/firefox            1MiB |\r\n|    0   N/A  N/A     10042      G   /usr/lib/firefox/firefox            1MiB |\r\n|    0   N/A  N/A     10159      G   /usr/lib/firefox/firefox            1MiB |\r\n|    0   N/A  N/A     10256      G   /usr/lib/firefox/firefox            1MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 1.14.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/loreto/.venv/lib/python3.7/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 5, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n\r\n```", "comments": ["@loretoparisi,\r\nStarting with v1.6, TensorFlow binaries use AVX instructions. For more information, please take a look at the [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements) for TensorFlow.\r\n\r\nIn this case, as an alternative you can either [build TensorFlow from source](https://www.tensorflow.org/install/source) or use [Google Colab](https://colab.research.google.com/). Thanks!", "@amahendrakar yes I know thank you. The problem here is that with binaries <= 1.5 (check the logs from `tf_env_collect.sh`) this issue comes out anyways. Why?", ">```\r\n> == check pips ===================================================\r\n> numpy                1.19.5\r\n> protobuf             3.14.0\r\n> tensorflow           1.14.0\r\n> tensorflow-estimator 1.14.0\r\n>```\r\nFrom the log, I see that you have installed TensorFlow 1.14 which is greater than TensorFlow 1.6. Please take a look at the [release history](https://www.tensorflow.org/versions) for more information. Thanks!", "@amahendrakar thank you, my fault. Last question. Is it technically possible to build from sources with bazel and deactivate AVX instructions explicitly?\nIf so, any guidance?\n\nThank you so much.\n\n\n\n\n\n\n\n\n", "@loretoparisi,\r\nYes, you can build TensorFlow for machines which do not support AVX instructions. \r\n\r\nPlease follow [this guide](https://www.tensorflow.org/install/source#tested_build_configurations) to build TensorFlow from source for your machine. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46496\">No</a>\n", "@amahendrakar thank you for your help! I have followed your suggestion and trying a `docker` gpu build from sources, but there are some errors while compiling.\r\nPlease have a look, thank you!\r\nhttps://github.com/tensorflow/tensorflow/issues/46845"]}, {"number": 46495, "title": "Converted models gives wrong outputs", "body": "**System information**\r\nRun on Colab; tensorflow-gpu==2.3.1 for the first part, then tf-nightly-gpu (after runtime restart).\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nColab link with the project:\r\nhttps://colab.research.google.com/drive/1V7fo4TgfAvMxme9AOBbdYggW-I7-tXTr?usp=sharing\r\n\r\n**The output from the converter invocation**\r\nOn Colab, no verbose output but model.tflite is created\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\nHere the link of the .pb model (which was obtained using onnx_to_tensorflow):\r\nhttps://drive.google.com/drive/folders/1RiwVVbzTNymzdmy_sRZxIWWSOGE3EwOx?usp=sharing\r\n\r\n**Failure details**\r\nI obtained a converted model.tflite file but the outputs are wrong (see Colab notebook for comparison)\r\n\r\n**Any other info / logs**\r\nAs explained in the notebook, the tflite coinversion works with tensorflow-gpu==2.3.1 but to run the model tf-nightly-gpu is required (after runtime restart). If I do differently, Colab crashes.\r\n\r\n", "comments": ["I tried to reproduce this on tf-nightly. When loading the attached saved model, the saved model loader does not stop forever. The loading problem will report this to the TensorFlow Core team.", "Is this still an issue with the latest version of TensorFlow? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46495\">No</a>\n"]}, {"number": 46494, "title": "tools:summarize_graph fails to open TFHub models due to parsing errors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: Python 3.8.6 (default, Oct  9 2020, 11:59:16) \r\n- Bazel version (if compiling from source): Bazelisk version: v1.6.1\r\n- GCC/Compiler version (if compiling from source): [GCC 9.3.0] on linux\r\n- CUDA/cuDNN version: not relevant\r\n- GPU model and memory: not relevant\r\n\r\n**Describe the current behavior**\r\n\r\n`summarize_graph` is not able to load downloaded and extracted models from TFHub e.g. https://tfhub.dev/deepmind/biggan-128/2 due to parsing errors.\r\n\r\n```\r\n$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph='/media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb'  --print_structure=true\r\n[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:324] Error parsing text-format tensorflow.GraphDef: 1:1: Invalid control characters encountered in text.\r\n[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:324] Error parsing text-format tensorflow.GraphDef: 1:2: Interpreting non ascii codepoint 154.\r\n[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:324] Error parsing text-format tensorflow.GraphDef: 1:2: Expected identifier, got: \ufffd\r\n2021-01-17 12:13:33.563477: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:320] Loading graph '/media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb' failed with Can't parse /media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb as binary proto\r\n\t (both text and binary parsing failed for file /media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb)\r\n2021-01-17 12:13:33.563504: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:322] usage: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe tool should load the model", "comments": ["@paulgavrikov \r\n\r\nThis issue is more suitable for TensorFlow Hub repo. Please post it on [hub repo](https://github.com/tensorflow/hub/issues/new) from here. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46494\">No</a>\n"]}, {"number": 46492, "title": "documentation missing, tf.keras.Model.fit shuffle argument is being ignored when passing a tf.data.dataset", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere's an issue with the shuffle argument description, it doesn't state that the \"shuffle\" argument is being ignored\r\nwhen the argument \"x\" is a tf.data.dataset in tf.keras.model.fit.\r\n\r\n### Clear description\r\n\r\nI was inspecting the source code in tf.keras.model.fit and found that when the input arg x to the [data handler](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/engine/training.py#L1050) is a tf.data.dataset, it ends up using the [dataset adapter](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/python/keras/engine/data_adapter.py#L671) which silently ignores the shuffle argument. \r\n\r\nThis issue is not clearly explained in the description of the shuffle argument below:\r\n\r\n_\"\"\"Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None.\"\"\"_\r\n\r\nI would recommend to write something like this instead \"_This argument is ignored when x is a generator or a tf.data.Dataset._\" \r\n\r\nI can contribute by fixing the docs if you agree that this should be done.\r\n\r\nThank you in advance for your review!\r\n\r\n\r\n \r\n\r\n\r\n", "comments": ["@juanma9613 Please feel free to raise a PR to update the docs. You check the following [resource](https://www.tensorflow.org/community/contribute/code) to contribute to improve the docs. Thanks!", "Done, I will add the PR at the end of the week. Thanks."]}, {"number": 46491, "title": "(Donkey Car: Epoch failure during train command) Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, uses Donkey Car train function.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): I am not sure, using Anaconda Prompt\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.4.0 (have also tried 2.3.0 and 2.3.2)\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GTX 1080 * GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen executing the Train command, it eventually fails with error: `W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. [[{{node PyFunc}}]]`\r\n\r\n**Describe the expected behavior**\r\n\r\n100 Epochs should be ran through.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n`(donkey) C:\\Users\\Kelvin>python C:\\Users\\Kelvin\\mycar\\train.py --tub C:\\Users\\Kelvin\\mycar\\data\\ --model C:\\Users\\Kelvin\\mycar\\models\\mypilot.h5`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nSorry if anything is unclear, I am fairly new to this. This is in reference to autorope/donkeycar#742 . Basically, we are trying to train a neural network and go through 100 Epochs, but run into this error. I have tried with 2.3.2 and 2.4.0 and updated Cuda 10.2 and updated my graphics drivers and have had no luck.\r\n\r\nHere is the full log when I run into the issue:\r\n\r\n[console with 2.4.0.txt](https://github.com/tensorflow/tensorflow/files/5825583/console.with.2.4.0.txt)\r\n\r\n", "comments": ["@kelvham \r\n\r\nWill it be possible to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram I am not sure exactly what code you'll need, as I was following a step-by-step guide that clones another github repo, the important pages of the instructions are [http://docs.donkeycar.com/guide/host_pc/setup_windows/](url) and [http://docs.donkeycar.com/guide/train_autopilot/](url) . \r\n\r\nNote that the instructions say version 2.2.0, but I have also tried with 2.3.2 and 2.4.0.", "@kelvham \r\n\r\nTensorFlow v2.4 is built and tested against CUDA 11.0 (not 11.1) and cuDNN 8\r\n\r\nCould you please check if you are facing the same issue with TF v2.4, CUDA 11.0 and cuDNN 8.Thanks!", "@ravikyram it failed instantly with that setup you outlined\r\n\r\n[TF 2.4, CUDA 11.0, cuDNN 8.txt](https://github.com/tensorflow/tensorflow/files/5836580/TF.2.4.CUDA.11.0.cuDNN.8.txt)\r\n", "Hi @kelvham ! I checked the log in above text file.  Your device does not seem to comply AVX instructions which might be causing OOM in first case.  Please refer these threads to  build Tensorflow in your device from source . You can also constrain [memory growth](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth) using to avoid OOM issue. Attaching relevant threads for reference .Ref [1](https://stackoverflow.com/questions/59265920/this-tensorflow-binary-is-optimized-with-intelr-mkl-dnn-to-use-the-following-c), [2](https://technofob.com/2019/06/14/how-to-compile-tensorflow-2-0-with-avx2-fma-instructions-on-mac/).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym Thank you for the reply, as you can see this thread is from over a year ago and I have since disassembled my bot. If I ever pursue the project again I will keep this in mind! \r\n\r\nThanks again!!", "Ok @kelvham ! Closing this issue for now. Feel free to re-open if you need further assistance . Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46491\">No</a>\n"]}, {"number": 46490, "title": "help me to solve this i am running a windows machine 8 gb ram python 3.8 cudnn 6.5 cuda 11 geforce 210", "body": "\r\n(base) C:\\Users\\ADMIN>python\r\nPython 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>                                                                                                                                                                     ", "comments": [" Your CPU model might be not supporting  AVX instruction sets.\r\n\r\nTry Google Colab to use TensorFlow", "@seviksindia,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check similar duplicate issue #36167 for reference. Thanks!", "You may want to try using TensorFlow CPU version since the cuda compute capability of your [graphics card](https://www.geeks3d.com/20100606/gpu-computing-nvidia-cuda-compute-capability-comparative-table/) is 1.2 which is less than required minimum cuda compute capability of 3.5\r\nSee https://www.tensorflow.org/install/gpu#hardware_requirements\r\nThanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46490\">No</a>\n"]}, {"number": 46489, "title": "Please Help Me To Sort This Out ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2\r\n- Python version:3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:6.5\r\n- GPU model and memory:geforce 210 \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@seviksindia please describe the problem properly", "duplicate https://github.com/tensorflow/tensorflow/issues/46490#issuecomment-761871554", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46489\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46489\">No</a>\n"]}, {"number": 46488, "title": "Keras saved model returns different result than original model using Batch Normalization on multiple GPUs (Distributed training)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary via pip3\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Geforce GTX 1080 Ti 4x11GB\r\n\r\nI got a different result when using evaluate function on a saved model when compare with the original model. This only happens when Batch Normalization is included in the model and when training on multiple GPUs with MirroredStrategy.\r\n\r\nHere is my model\r\n```\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.BatchNormalization(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n\r\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n```\r\n\r\nMultiple GPUs with MirroredStrategy\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n```\r\nOutput\r\n```\r\nNumber of devices: 2\r\n```\r\n\r\nEvaluate after training\r\n```\r\neval_loss, eval_acc = model.evaluate(eval_dataset)\r\n\r\nprint('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n```\r\n\r\nOutput\r\n```\r\n79/79 [==============================] - 0s 5ms/step - loss: 0.0424 - accuracy: 0.9884\r\nEval loss: 0.04239395260810852, Eval Accuracy: **0.9883999824523926**\r\n```\r\n\r\nSave model and evaluate again\r\n```\r\npath = 'saved_model/'\r\nmodel.save(path, save_format='tf')\r\nwith strategy.scope():\r\n  replicated_model = tf.keras.models.load_model(path)\r\n  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                           optimizer=tf.keras.optimizers.Adam(),\r\n                           metrics=['accuracy'])\r\n\r\n  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\r\n  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n```\r\nOutput\r\n```\r\n79/79 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9883\r\nEval loss: 0.04239019751548767, Eval Accuracy: **0.9883000254631042**\r\n```\r\n\r\n - Without BN\r\n\r\nOutput from evaluate\r\n```\r\n79/79 [==============================] - 0s 6ms/step - loss: 0.0450 - accuracy: 0.9837\r\nEval loss: 0.04498908668756485, Eval Accuracy: **0.9836999773979187**\r\n```\r\n\r\nSave model and repeat evaluate\r\n```\r\n79/79 [==============================] - 0s 4ms/step - loss: 0.0450 - accuracy: 0.9837\r\nEval loss: 0.04498908668756485, Eval Accuracy: **0.9836999773979187**\r\n```\r\n\r\nI have searched for similar issues and thought that this is because [BN computes differently during training and interference ](https://keras.io/api/layers/normalization_layers/batch_normalization/) but when I tried with one GPU, this issue didn't occur.\r\n\r\n[Code for reproduce results](https://colab.research.google.com/drive/14iv88UJwFSv1SaVzmEl9RMOa1EH8IiRG?usp=sharing)\r\n", "comments": ["Hi @Lucasbravo1, I've just run the code in TF2.4 with two GPUs and if I don't recompile the mode, I'm actually seeing the same accuracy for the BN model. Can you [run the code in this gist](https://colab.research.google.com/gist/nikitamaia/007bfe2a1213b9e9508c09be617b440f/github_issue_46488.ipynb#scrollTo=5Y04_YwwNnmY) on two GPUs and let me know if you see the same or if I'm missing something?\r\nI don't think you need to recompile the model, see the[ note in the docs here](https://www.tensorflow.org/tutorials/distribute/save_and_load) \"After restoring the model, you can continue training on it, even without needing to call compile() again, since it is already compiled before saving\"", "Hi @nikitamaia, thank you very much for your support. \r\n\r\nMaybe I found where is the problem. If I run your code in TF2.3.1, without recompile, the evaluation even generate [random results](https://drive.google.com/file/d/139mdSKbeb-uSnf0hbLWYexuMARS5_Qzo/view?usp=sharing), but [this works on TF2.4](https://colab.research.google.com/drive/1--AlIxAOm4UBXIML2j0CoCFnPWfzypf_?usp=sharing). Note that, in the first link, I reinstalled tensorflow from version 2.4.1 to 2.3.1 on Colab. I haven't checked multiple GPUs in TF2.4 yet because I have only CUDA versions 10.1 and 11.1. I think [tensorflow hasn't supported the latest CUDA version yet](https://www.tensorflow.org/install/source#gpu)?\r\n\r\n", "Yep, you can track support for 11.1 and 11.2 in #46093.", "Ok, thanks @nikitamaia. I will have a look. Hope this works with multiple gpus.", "I just ran some tests for the multi GPU case and there is a small difference in the loss values (accuracy is the same). \r\nAfter further investigation, it seems this is not totally unexpected. The model is saved as a non distributed model. Batch norm is a local statistic that happens on each replica, so when the model is reloaded the values are averaged across all of the replicas. You should not see this discrepancy if you use [SyncBatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization), although training might be slower.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46488\">No</a>\n", "Thanks much @nikitamaia "]}, {"number": 46487, "title": "typo / missing space in RuntimeError", "body": "I just signed up here to report this, sorry if I'm doing everything wrong or something.\r\nI just got the follwing:\r\n```\r\nRuntimeError: in user code:\r\n\r\n    <ipython-input-74-5015738e55b1>:22 fitting  *\r\n        grads = tape.gradient(value, model.trainable_weights)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py:1027 gradient  **\r\n        raise RuntimeError(\"A non-persistent GradientTape can only be used to\"\r\n\r\n    RuntimeError: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)\r\n```\r\nAnd I think there is a space missing in the last line's \"tocompute\".", "comments": ["Thanks for the issue. This was recently fixed with commit [d335bfd](https://github.com/tensorflow/tensorflow/commit/d335bfd43249c6c4b25ed179ba700b0794f86391) and should be part of next TF versions."]}, {"number": 46485, "title": "Tensorflow DOESNOT WORKS WITH GPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and provide all the information asked in issue template. Thank you.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46485\">No</a>\n"]}, {"number": 46484, "title": "TypeError: '<' not supported between instances of 'function' and 'str'", "body": "<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I'm training the model on google collaboratory\r\n\r\n**Describe the current behavior**\r\nI've trained and saved my model, on importing the model it gives this error on model evaluation, but model.predict() works perfectly\r\n\r\n**Testing Code**\r\n`model_path = \"/content/drive/MyDrive/Train Data/Models/text_block_model_new_batch.h5\"`\r\n`model = tf.keras.models.load_model(model1_path,custom_objects={'dice_coef':dice_coef,'dice_coef_loss':dice_coef_loss})`\r\n`results = model.evaluate(X_test,Y_test)`\r\n\r\n\r\n\r\n![Screenshot (40)](https://user-images.githubusercontent.com/54077406/104814641-16448680-5836-11eb-8057-9bcd29159d9f.png)\r\n\r\n", "comments": ["@divyanshjoshi,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Alternatively, you can share the link of the Colab notebook you are running. \r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/46249#issuecomment-756605409) from a similar issue and let us know if it helps. Thanks!", "Currently I'm working on tensorflow 2.4.0\r\nI can't upload my colab notebook publicly, so is there any other way I can reach you? \r\nAlthough I can upload the training portion of my notebook, if that's possible, Thanks", "Check if you have any missing values in your data and remove them if any.", "TypeError: '<' not supported between instances of 'function' and 'str' arises because of some issue in your data", "> TypeError: '<' not supported between instances of 'function' and 'str' arises because of some issue in your data\r\n\r\nI don't think so ,cause the model is successful in making predictions and the test set is kinda from the same distribution from the train", "> Check if you have any missing values in your data and remove them if any.\r\n\r\nThe test set seems fine and even checked for any null values, still didn't solve the problem.", "@amahendrakar..... Hopefully the link you mentioned is gonna help me, I'll update you soon", "> Although I can upload the training portion of my notebook, if that's possible, Thanks\r\n\r\nWithout a reproducible code it would be difficult for us to pinpoint the issue. Hence, could you please share a minimal code snippet with dummy dataset to reproduce the error? Thanks!", "Okay, Sure\r\n**Here you go!**\r\n\r\nI basically used a custom loss function\r\n```\r\ndef dice_coef(y_true,y_pred,epsilon=0.001):\r\n  y_true_sum  = K.sum(y_true)\r\n  y_pred_sum  = K.sum(y_pred)\r\n  intersection = K.sum(y_true * y_pred)\r\n  return ((2 * intersection + epsilon)/(y_true_sum + y_pred_sum + epsilon))\r\n\r\ndef dice_coef_loss(y_true,y_pred):\r\n    return  1 - dice_coef(y_true,y_pred)\r\n```\r\nThis is the training portion\r\n```\r\nmodel_path = os.getcwd() + \"/drive/MyDrive/Train Data/Models/text_block_model_morph_1024*1024.h5\"\r\n\r\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,\r\n                             monitor=\"val_loss\",\r\n                             mode=\"min\",\r\n                             save_best_only = True,\r\n                             verbose=1)\r\n\r\ndecay_rate = learning_rate / epochs\r\nopt = tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\r\n\r\nmodel.compile(optimizer=opt,loss=dice_coef_loss,metrics=[dice_coef,'accuracy'])\r\nresults = model.fit(X_train, Y_train, validation_split=0.1,batch_size=batch_size, epochs=epochs,callbacks=[checkpoint])\r\n```\r\nHope that's sufficient information, Thank you for responding!!", "I used a **standard U-Net** as my model architecture \r\n\r\n_Link to my test set_ - [https://drive.google.com/drive/folders/1tsRtFiXTSDgbtQl6yabfkBmoZ_Yfo5gl?usp=sharing](https://drive.google.com/drive/folders/1tsRtFiXTSDgbtQl6yabfkBmoZ_Yfo5gl?usp=sharing)\r\n", "What happens if you use built in loss functions `(binary_crossentropy/categorical_crossentropy)` here? Are you able to execute code after losing custom objects from your code and using built in functions?", "> What happens if you use built in loss functions `(binary_crossentropy/categorical_crossentropy)` here? Are you able to execute code after losing custom objects from your code and using built in functions?\n\nDidn't try that yet, that's a great suggestion I'll try that out and let you know! ", "@ymodak ..... The model was able to evaluate on binary_crossentropy, so I guess there's an issue with the custom loss function", "@amahendrakar .......It finally worked!!, I only saved the model weights, rather than saving the whole model and then evaluated using the latest weights on a new model instance, Thank you for your help!\r\n\r\n**This helped me**\r\n[https://www.tensorflow.org/tutorials/keras/save_and_load](https://www.tensorflow.org/tutorials/keras/save_and_load)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46484\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46484\">No</a>\n", "@ymodak  is there a fix for someone who doesn't want to save the weights separately (because for example they want to also save the optimizer's state)?\r\n\r\nEDIT\r\n----\r\nThere is always the possibility to manually save the weights of course (like laid out in [this SO answer](https://stackoverflow.com/a/49504376/4332585)), but it's a bit tedious and I guess error-prone.", "See this: https://github.com/tensorflow/tensorflow/issues/45903#issuecomment-804973541", "> @amahendrakar .......It finally worked!!, I only saved the model weights, rather than saving the whole model and then evaluated using the latest weights on a new model instance, Thank you for your help!\r\n> \r\n> **This helped me**\r\n> https://www.tensorflow.org/tutorials/keras/save_and_load\r\n\r\nhey man, this saved my ass, thank you ,  I had the same issue and cannot figure out until SAW your post"]}, {"number": 46483, "title": "InvalidArgumentError: 5 nodes in a cycle ", "body": "Hello,\r\nI am trying to read image data from TFRecords,parse it , decode the images and the batch it. The moment I use @tf.function for decoding the images using a dataset.map() method, I get the following error. \r\nHere is the code and output:\r\n\r\nThank in advance! \r\n\r\n```\r\nclass DataParser(Hyperparameters):\r\n\r\n   \r\n    def __init__(self): \r\n        self.TFRECORDS_FORMAT=Hyperparameters.TFRECORDS_FORMAT\r\n        self.BATCH_SIZE=Hyperparameters.BATCH_SIZE\r\n        self.HEIGHT=Hyperparameters.HEIGHT\r\n        self.WIDTH=Hyperparameters.WIDTH\r\n        \r\n    def readTFRecs(self,dir_name): \r\n        \r\n        TFRecFiles=tf.constant(tf.io.gfile.listdir(dir_name))\r\n        TFRecFiles=tf.map_fn(lambda name:dir_name+'/'+name,TFRecFiles)\r\n        TFRecDataset=tf.data.TFRecordDataset(TFRecFiles)#.batch(self.BATCH_SIZE).prefetch(1)\r\n        self.dataset_len=tf.data.experimental.cardinality(TFRecDataset).numpy()\r\n        Dataset = TFRecDataset.map(lambda example:tf.io.parse_example(example,self.TFRECORDS_FORMAT))\r\n        return Dataset\r\n    \r\n    @tf.function\r\n    def decode_image(self,entry):\r\n       return tf.image.decode_image(entry['image'],channels=3) #[batch_size,h,w,3]\r\n    \r\n    \r\n    @tf.function\r\n    def makeDataset(self,TFRecDataset):\r\n        Dataset = TFRecDataset.map(lambda entry: self.decode_image(entry))\r\n        return iter(Dataset)\r\n\r\n\r\n\r\ndp=DataParser()  \r\nTFRecDataset=dp.readTFRecs('../input/cassava-tfrecords-512x512')  \r\nIter=dp.makeDataset(TFRecDataset)  \r\nnext(Iter)  \r\n```\r\n\r\n\r\nGives the error:\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-29-f39735f1a1fd> in <module>\r\n      1 dp=DataParser()\r\n      2 TFRecDataset=dp.readTFRecs('../input/cassava-tfrecords-512x512')\r\n----> 3 Iter=dp.makeDataset(TFRecDataset)\r\n      4 next(Iter)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    844               *args, **kwds)\r\n    845       # If we did not create any variables the trace we have is good enough.\r\n--> 846       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    847 \r\n    848     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1846                            resource_variable_ops.BaseResourceVariable))],\r\n   1847         captured_inputs=self.captured_inputs,\r\n-> 1848         cancellation_manager=cancellation_manager)\r\n   1849 \r\n   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1922       # No tape is watching; skip to running the function.\r\n   1923       return self._build_call_outputs(self._inference_function.call(\r\n-> 1924           ctx, args, cancellation_manager=cancellation_manager))\r\n   1925     forward_backward = self._select_forward_and_backward_functions(\r\n   1926         args,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    548               inputs=args,\r\n    549               attrs=attrs,\r\n--> 550               ctx=ctx)\r\n    551         else:\r\n    552           outputs = execute.execute_with_cancellation(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 5 nodes in a cycle [Op:__inference_makeDataset_2557]\r\n```", "comments": ["@AdityaKane2001 \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d75d080236815637596eef74ec99d383/untitled502.ipynb). [please share all dependencies for us to replicate the issue faced]", "Hey,\r\n\r\nHyperparameters is just a config class\r\n\r\n```\r\n#Import required libraries\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nimport pandas as pd\r\nimport cv2\r\n\r\nclass Hyperparameters:\r\n    TFRECORDS_FORMAT={'image': tf.io.FixedLenFeature([], tf.string),\r\n                      'image_name': tf.io.FixedLenFeature([], tf.string),\r\n                      'target': tf.io.FixedLenFeature([], tf.int64)}\r\n    BATCH_SIZE=32\r\n    AUTOTUNE=tf.data.experimental.AUTOTUNE\r\n    HEIGHT=224\r\n    WIDTH=224\r\n    WIDTH_FACTOR=0.2\r\n    HEIGHT_FACTOR=0.2\r\n    FILL_MODE='reflect'\r\n    TRAINING=True\r\n\r\n```", "@AdityaKane2001 \r\nI ran the code shared and face a different issue, please find [gist here](https://colab.research.google.com/gist/Saduf2019/211df62123c9ff4818baab3aa43378f4/untitled504.ipynb). Unless you share all dependencies we cannot replicate the issue to analyse it.", "Hi,\nActually this is the notebook that I am using for a kaggle competition.\nPlease check out the notebook at : kaggle notebook\n<https://www.kaggle.com/adityakane/tfdata-efficientnet>. Before you run the\nnotebook, please uncomment the @tf.function decorator in DataParser class .\nFollowing is the change you need to do:\n\n    #@tf.function\n    def makeDataset(self,TFRecDataset):\n        Dataset = TFRecDataset.map(lambda entry: self.decode_image(entry))\n        #Dataset = Dataset.map(lambda\nentry:(entry['image'],tf.one_hot(entry['target'],5)))\n        Dataset = Dataset.shuffle(4000)\n        #Dataset=Dataset.zip(TFRecDataset.map(lambda entry:entry['target']))\n        Dataset = Dataset.batch(self.BATCH_SIZE).prefetch(1)\n        return Dataset\n\n\n                                                   ||\n\n                                                   ||\n\n                                                   ||\n\n                                                   ||\n\n                                                  \\\\//\n\n\n      @tf.function\n    def makeDataset(self,TFRecDataset):\n        Dataset = TFRecDataset.map(lambda entry: self.decode_image(entry))\n        #Dataset = Dataset.map(lambda\nentry:(entry['image'],tf.one_hot(entry['target'],5)))\n        Dataset = Dataset.shuffle(4000)\n        #Dataset=Dataset.zip(TFRecDataset.map(lambda entry:entry['target']))\n        Dataset = Dataset.batch(self.BATCH_SIZE).prefetch(1)\n        return Dataset\n\n\n\n\nThanks and Regards,\nAditya Kane\n\n\nOn Tue, Jan 19, 2021 at 5:04 PM Saduf2019 <notifications@github.com> wrote:\n\n> @AdityaKane2001 <https://github.com/AdityaKane2001>\n> I ran the code shared and face a different issue, please find gist here\n> <https://colab.research.google.com/gist/Saduf2019/211df62123c9ff4818baab3aa43378f4/untitled504.ipynb>.\n> Unless you share all dependencies we cannot replicate the issue to analyse\n> it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/46483#issuecomment-762785236>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APLNNKSC6YN57DWOSH3OJCDS2VU57ANCNFSM4WFE3PVA>\n> .\n>\n", "After making the change in the last mail, please run the following code in\na new cell:\n\n\ndp=DataParser()\nTFRecDataset=dp.readTFRecs('../input/cassava-tfrecords-512x512')\nDataset=dp.makeDataset(TFRecDataset)\n#Dataset = Dataset.shuffle(600)\nvalDataset=Dataset.take(50).prefetch(dp.AUTOTUNE)\ntrainDataset=Dataset.skip(50).prefetch(dp.AUTOTUNE)\n\n\nThanks and Regards,\nAditya Kane\n\n\nOn Tue, Jan 19, 2021 at 9:08 PM Aditya Kane <adityakane1@gmail.com> wrote:\n\n> Hi,\n> Actually this is the notebook that I am using for a kaggle competition.\n> Please check out the notebook at : kaggle notebook\n> <https://www.kaggle.com/adityakane/tfdata-efficientnet>. Before you run\n> the notebook, please uncomment the @tf.function decorator in DataParser\n> class . Following is the change you need to do:\n>\n>     #@tf.function\n>     def makeDataset(self,TFRecDataset):\n>         Dataset = TFRecDataset.map(lambda entry: self.decode_image(entry))\n>         #Dataset = Dataset.map(lambda\n> entry:(entry['image'],tf.one_hot(entry['target'],5)))\n>         Dataset = Dataset.shuffle(4000)\n>         #Dataset=Dataset.zip(TFRecDataset.map(lambda\n> entry:entry['target']))\n>         Dataset = Dataset.batch(self.BATCH_SIZE).prefetch(1)\n>         return Dataset\n>\n>\n>                                                      ||\n>\n>                                                      ||\n>\n>                                                      ||\n>\n>                                                      ||\n>\n>                                                     \\\\//\n>\n>\n>       @tf.function\n>     def makeDataset(self,TFRecDataset):\n>         Dataset = TFRecDataset.map(lambda entry: self.decode_image(entry))\n>         #Dataset = Dataset.map(lambda\n> entry:(entry['image'],tf.one_hot(entry['target'],5)))\n>         Dataset = Dataset.shuffle(4000)\n>         #Dataset=Dataset.zip(TFRecDataset.map(lambda\n> entry:entry['target']))\n>         Dataset = Dataset.batch(self.BATCH_SIZE).prefetch(1)\n>         return Dataset\n>\n>\n>\n>\n> Thanks and Regards,\n> Aditya Kane\n>\n>\n> On Tue, Jan 19, 2021 at 5:04 PM Saduf2019 <notifications@github.com>\n> wrote:\n>\n>> @AdityaKane2001 <https://github.com/AdityaKane2001>\n>> I ran the code shared and face a different issue, please find gist here\n>> <https://colab.research.google.com/gist/Saduf2019/211df62123c9ff4818baab3aa43378f4/untitled504.ipynb>.\n>> Unless you share all dependencies we cannot replicate the issue to analyse\n>> it.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/46483#issuecomment-762785236>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/APLNNKSC6YN57DWOSH3OJCDS2VU57ANCNFSM4WFE3PVA>\n>> .\n>>\n>\n", "@AdityaKane2001 \r\nI ran the code shared as directed and do not see any errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/cd9ab8cae10af1e3797098c26273d6c3/untitled508.ipynb).", "Hello,\r\nPlease view the notebook [gist](https://colab.research.google.com/gist/AdityaKane2001/2904c6a0864ad78f226eb6824a13ec62/issue_46483.ipynb) attached herewith. The issue is replicated there.\r\n\r\nI have found a workaround to this issue ( [gist](https://colab.research.google.com/gist/AdityaKane2001/ddd1f024d835d7f770adbea4df06be84/issue_46423_workaround.ipynb) here ). Still, it would be really great if you tell what is it caused by and what does it mean.\r\n\r\nThanks and Regards,\r\nAditya Kane\r\n\r\n\r\n", "Closing this issue since you have found workaround. Feel free to reopen if facing further problems with latest tf version.\r\nThanks!", "Ok\r\n"]}, {"number": 46481, "title": "Update 20-documentation-issue.md", "body": "Minor update to the template to have better clarity over the content.", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac"]}, {"number": 46480, "title": "get_variable  missing reuse parameter", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n     Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    MacOSX 10.14.5 Mojave\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: pip\r\n-   **TensorFlow version (use command below)**:2.4.0\r\n-   **Python version**: \r\npython --version\r\nPython 3.6.6 :: Anaconda, Inc.\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\nv2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\ntg.get_variable cannot reuse  existing variable.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\ntf.get_variable cannot pass in the reuse parameter, see  logs below:\r\n\r\n(TensorFlow-LiveLessons) bash-3.2$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n\r\n\r\n(TensorFlow-LiveLessons) bash-3.2$ python\r\nPython 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:07:29) \r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow.compat.v1 as tf\r\n>>> tf.disable_v2_behavior()\r\nWARNING:tensorflow:From /Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n>>> n_input = 784\r\n>>> n_dense = 128\r\n>>> W = tf.Variable(tf.random_normal([n_input, n_dense]))\r\n>>> tf.get_variable('W', [n_input, n_dense], \\\r\n...                       initializer=tf.keras.initializers.VarianceScaling(\r\n...                       scale=1.0, mode=\"fan_avg\",\r\n...                       distribution=\"truncated_normal\"\r\n...                         ))\r\n<tf.Variable 'W:0' shape=(784, 128) dtype=float32_ref>\r\n>>> tf.get_variable('W', [n_input, n_dense], \\\r\n...                       initializer=tf.keras.initializers.VarianceScaling(\r\n...                       scale=1.0, mode=\"fan_avg\",\r\n...                       distribution=\"truncated_normal\"\r\n...                         ))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1593, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1336, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 591, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 543, in _true_getter\r\n    aggregation=aggregation)\r\n  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 893, in _get_single_variable\r\n    (err_msg, \"\".join(traceback.format_list(tb))))\r\nValueError: Variable W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\r\n\r\n  File \"<stdin>\", line 4, in <module>\r\n\r\n>>> tf.get_variable('W', [n_input, n_dense], \\\r\n...                       initializer=tf.keras.initializers.VarianceScaling(\r\n...                       scale=1.0, mode=\"fan_avg\",\r\n...                       distribution=\"truncated_normal\",\r\n...                               reuse=tf.AUTO_REUSE))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 5, in <module>\r\n  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 538, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 605, in new_func\r\n    return func(*args, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'reuse'\r\n>>> exit()\r\n\r\n\r\nDigging into the  source code tensorflow/python/ops/variable_scope.py, missing parameter  reuse\r\n\r\n@tf_export(v1=[\"get_variable\"])\r\ndef get_variable(name,\r\n                 shape=None,\r\n                 dtype=None,\r\n                 initializer=None,\r\n                 regularizer=None,\r\n                 trainable=None,\r\n                 collections=None,\r\n                 caching_device=None,\r\n                 partitioner=None,\r\n                 validate_shape=True,\r\n                 use_resource=None,\r\n                 custom_getter=None,\r\n                 constraint=None,\r\n                 synchronization=VariableSynchronization.AUTO,\r\n                 aggregation=VariableAggregation.NONE):\r\n  return get_variable_scope().get_variable(\r\n      _get_default_variable_store(),\r\n      name,\r\n      shape=shape,\r\n      dtype=dtype,\r\n      initializer=initializer,\r\n      regularizer=regularizer,\r\n      trainable=trainable,\r\n      collections=collections,\r\n      caching_device=caching_device,\r\n      partitioner=partitioner,\r\n      validate_shape=validate_shape,\r\n      use_resource=use_resource,\r\n      custom_getter=custom_getter,\r\n      constraint=constraint,\r\n      synchronization=synchronization,\r\n      aggregation=aggregation)\r\n\r\nif we can add  this  parameter to  the parameter list the above problem could be fixed.\r\nThanks\r\n\r\n# The argument list for get_variable must match arguments to get_local_variable.\r\n# So, if you are updating the arguments, also update arguments to\r\n# get_local_variable below.\r\n@tf_export(v1=[\"get_variable\"])\r\ndef get_variable(name,\r\n                 shape=None,\r\n                 dtype=None,\r\n                 initializer=None,\r\n                 regularizer=None,\r\n                 trainable=None,\r\n                 collections=None,\r\n                 caching_device=None,\r\n                 partitioner=None,\r\n                 validate_shape=True,\r\n                 use_resource=None,\r\n                 custom_getter=None,\r\n                 constraint=None,\r\n                 synchronization=VariableSynchronization.AUTO,\r\n                 aggregation=VariableAggregation.NONE,\r\n                 reuse=None):\r\n  return get_variable_scope().get_variable(\r\n      _get_default_variable_store(),\r\n      name,\r\n      shape=shape,\r\n      dtype=dtype,\r\n      initializer=initializer,\r\n      regularizer=regularizer,\r\n      trainable=trainable,\r\n      collections=collections,\r\n      caching_device=caching_device,\r\n      partitioner=partitioner,\r\n      validate_shape=validate_shape,\r\n      use_resource=use_resource,\r\n      custom_getter=custom_getter,\r\n      constraint=constraint,\r\n      synchronization=synchronization,\r\n      aggregation=aggregation,\r\n      reuse=reuse)\r\n\r\nXiquan Ren", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/cedf5194053023d7d0ef9aed7ac828e3/46480.ipynb#scrollTo=sBIYson64ew0). Thanks!", "Please take a look at https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable. You can create a variable_scope with reuse=True in order to achieve the intended behavior", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46480\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46480\">No</a>\n", "it will not recognize the parameter, please check.\r\n\r\ntf.get_variable('W', [n_input, n_dense],\r\n... initializer=tf.keras.initializers.VarianceScaling(\r\n... scale=1.0, mode=\"fan_avg\",\r\n... distribution=\"truncated_normal\",\r\n... reuse=tf.AUTO_REUSE))\r\nTraceback (most recent call last):\r\nFile \"\", line 5, in\r\nFile \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 538, in new_func\r\nreturn func(*args, **kwargs)\r\nFile \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 605, in new_func\r\nreturn func(*args, **kwargs)\r\nTypeError: init() got an unexpected keyword argument 'reuse'\r\nexit()", "thanks,fixed..."]}, {"number": 46479, "title": "Tensorflow issues: Not creating XLA devices, tf_xla_enable_xla_devices not set, This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.", "body": "Hi, I've just started to install TensorFlow, and I'm facing some issues. \r\n\r\nI'm trying to use PyCharm, so I installed anaconda and created a virtual environment with the versions being:\r\nPython 3.8.5\r\npip 20.3.3\r\ntensorflow 2.4.0.\r\n\r\nBut, I see errors when trying to run a simple tf.constant(\"hello\").\r\nThe two errors are:\r\n\r\n1. I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2. I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n\r\nI've been struggling for a long time...Thanks in advance.\r\n", "comments": [" Not creating XLA devices, tf_xla_enable_xla_devices not set message is an information log which you can ignore.", "@98hychoo \r\nThe Not creating XLA devices, tf_xla_enable_xla_devices not set message is an information log which you can safely ignore.\r\nTo verify that TensorFlow has detected the GPU on you machine, please run the below code and check the number of GPUs available [Please refer to these resolve dissues for reference: #44683, #44938, #\r\n\r\n```import tensorflow as tf```\r\n```print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))```\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@chazuttu @Saduf2019 this does not seem correct information, it is reported in many places these flags are needed for faster training.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46479\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46479\">No</a>\n", "\r\nPlease check this link [tensorflow: Not creating XLA devices, tf_xla_enable_xla_devices not set](https://stackoverflow.com/a/66028149/7194271)\r\n\r\nPlease add the following at the very beginning of your script:\r\n\r\n`os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'`\r\n\r\n", "I installed Cuda 11.2 on my windows 10 PC  ( GPU Nvidia Quadera 2200). The tensorflow sees GPU, but not using it even after  I added the following to my python script.\r\n\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'.\r\n\r\nAny suggestion or guidance is appreciated. \r\n\r\nHere is the log:\r\n\r\n\r\n021-04-16 12:42:09.192167: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-16 12:42:09.195988: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-04-16 12:42:09.231916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: Quadro P2200 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 10 deviceMemorySize: 5.00GiB deviceMemoryBandwidth: 186.45GiB/s\r\n2021-04-16 12:42:09.232100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-16 12:42:09.238778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-16 12:42:09.238893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-16 12:42:09.242343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-16 12:42:09.243752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-16 12:42:09.248336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-04-16 12:42:09.251569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-16 12:42:09.252382: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-16 12:42:09.252524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n\r\n021-04-16 12:42:09.777252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-16 12:42:09.777363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-04-16 12:42:09.777417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-04-16 12:42:09.777585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 3825 MB memory) -> physical GPU (device: 0, name: Quadro P2200, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2021-04-16 12:42:09.778969: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nUsing CPU. Note: This module is much faster with a GPU.\r\n"]}, {"number": 46478, "title": "Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 448 but received input with shape (None, 14944)", "body": "**Sytem Information:**\r\n\r\n- Platform : google colab\r\n- language -python\r\n- library:tensorflow\r\n\r\n**Implementation**\r\n\r\n- CNN\r\n- sentiment analysis\r\n- text classification\r\n\r\n**Code**\r\n```python\r\nfrom keras.utils import to_categorical\r\nX_train, X_test, Y_train, y_test = train_test_split(X,y, test_size = 0.15, random_state = 42)\r\nY_train = to_categorical(Y_train.astype(int))\r\ny_test = to_categorical(y_test.astype(int))\r\nmodel.fit(X_train, Y_train, epochs=10, batch_size=32,verbose = 1,callbacks = callbacks_list,validation_data=(X_test,y_test))\r\n\r\n**Error information**\r\nEpoch 1/10\r\nWARNING:tensorflow:Model was constructed with shape (None, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 934).\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-19-3aeda4411868> in <module>()\r\n      6 Y_train = to_categorical(Y_train.astype(int))\r\n      7 y_test = to_categorical(y_test.astype(int))\r\n----> 8 model.fit(X_train, Y_train, epochs=10, batch_size=32,verbose = 1,callbacks = callbacks_list,validation_data=(X_test,y_test))\r\n\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    975           except Exception as e:  # pylint:disable=broad-except\r\n    976             if hasattr(e, \"ag_error_metadata\"):\r\n--> 977               raise e.ag_error_metadata.to_exception(e)\r\n    978             else:\r\n    979               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\r\n        y_pred = self(x, training=True)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:375 call\r\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:425 call\r\n        inputs, training=training, mask=mask)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph\r\n        outputs = node.layer(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\r\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\r\n        ' but received input with shape ' + display_shape(x.shape))\r\n\r\n    ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 448 but received input with shape (None, 14944)\r\n```", "comments": ["@ravikyram please help me . i cant solve this problem.", "@Ayesha-Julekha \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Try to pass the required dimension as the input. Your Model expects the input in the shape of [None,448] but you are passing (None, 14944). Try to do some reshaping or pass correct input the model.\r\n", "@ravikyram ", "@Ayesha-Julekha \r\n\r\nCan you try with @PrasannaVpk suggestion and see if it works. Thanks!", "\r\n\r\n> Try to pass the required dimension as the input. Your Model expects the input in the shape of [None,448] but you are passing (None, 14944). Try to do some reshaping or pass correct input the model.\r\n\r\ncan you tell me how could i do that", "@PrasannaVpk \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46478\">No</a>\n"]}, {"number": 46477, "title": "bazel coverage only generates empty coverage report", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.9\r\n- Installed using : source\r\n- Bazel version (if compiling from source): 0.29.0\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- CUDA/cuDNN version: N\r\n- GPU model and memory: RTX 2080 super\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe following command is used to test and get test coverage.\r\n```\r\nbazel coverage -s --instrument_test_targets --coverage_report_generator=@bazel_tools//tools/test:coverage_report_generator --coverage_support=@bazel_tools//tools/test:coverage_support --collect_code_coverage --jobs 5 //tensorflow\r\n```\r\n\r\nBut I only get `~/tensorflow-2.1.0/bazel-testlogs/tensorflow/tensorflow/baseline_coverage.dat` and the data is only filenames and online `end_of_record`.\r\n\r\nI am not sure if it is caused by the wrong command.\r\n\r\n", "comments": ["@roguedream,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "> @roguedream,\r\n> Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!\r\n\r\nI tried v2.4 with bazel 3.1.0 using command \r\n```\r\nsudo bazel coverage -s --instrument_test_targets --experimental_cc_coverage --coverage_report_generator=@bazel_tools//tools/test/CoverageOutputGenerator/java/com/google/devtools/coverageoutputgenerator:Main --combined_report=lcov --jobs 4 //tensorflow/core:all\r\n```\r\nI got `_coverage` folder in the `bazel-out` folder. But there is only a `lcov_files.tmp` this folder and it points to many coverage.dat files. But those coverage.dat files are still empty.\r\nIs the command wrong? ", "@roguedream,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46477\">No</a>\n", "Hello, I have the same problem on the latest stable version of tensorflow. I wonder if there is any solution for this? Thank you."]}, {"number": 46476, "title": "[INTEL_MKL] Add `is_weight_const` Attr for QMatMulDequantize Op", "body": "To add the missing `is_weight_const` attr in MklQuantizedMatMulAndDequantize registration.\r\n", "comments": ["Thanks for the review.  Comments changed."]}, {"number": 46473, "title": "Switch cmsis-nn and ethos-u to using OPTIMIZED_KERNEL_DIR.", "body": "This change started as a switch cmsis-nn and ethos-u to using OPTIMIZED_KERNEL_DIR. However, supporting both OPTIMIZED_KERNEL_DIR=cmsis-nn and TAGS=cmsis-nn required a lot of workarounds.\r\n\r\nAs a result, it was decided to also remove the deprecated TAGS functionality.\r\n\r\nThis change also adds a new CO_PROCESSOR command line parameter to the TFLM makefile.\r\n\r\nSpecifying `CO_PROCESSOR=<c>` and `OPTIMIZED_KERNEL_DIR=<o>` will result in the following:\r\n * `tools/make/ext_libs/<o>.inc` and `tools/make/ext_libs/<c>.inc` will be included from the Makefile\r\n * kernel sources will be specialized from both `kernels/<o>.inc` and `kernels/<c>.inc` (in that order).\r\n * the specialization order is important because it means that if the same kernel in implemented in both optimized_kernel_dir and the co_processor, then the co_processor implementation will be used.\r\n\r\nTested the following commands locally:\r\n\r\nBuild for cortex-m55 + reference kernels + ethos-u kernel:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 CO_PROCESSOR=ethos-u microlite\r\n```\r\n\r\nBuild for cortex-m55 + cmsis-nn kernels (as available) + ethos-u kernel:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 OPTIMIZED_KERNEL_DIR=cmsis-nn CO_PROCESSOR=ethos-u microlite\r\n```\r\n\r\nSee the discussion on https://github.com/tensorflow/tensorflow/pull/46352 for some more context.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@mansnils @njeffrie: I wasn't planning on merging this PR yet but because it had all the approvals it did get merged.\r\n\r\nWe can roll back or fix forward as needed.\r\n\r\nI will make a PR that changes the directory names to underscores first thing tomorrow.\r\n\r\nAnd I certainly wanted to chat with @njeffrie about my last change where I decided that removing support for TAGS was the way to go.\r\n\r\nNote that I rebased and amended do that there was only a single commit from this PR to make it easier for anyone to locally roll back if needed since I am breaking backwards compatibility.", "@advaitjain Ok, let's get the last PR merged tomorrow.", "rename complete with https://github.com/tensorflow/tensorflow/pull/46584", "> This change started as a switch cmsis-nn and ethos-u to using OPTIMIZED_KERNEL_DIR. However, supporting both OPTIMIZED_KERNEL_DIR=cmsis-nn and TAGS=cmsis-nn required a lot of workarounds.\r\n> \r\n> As a result, it was decided to also remove the deprecated TAGS functionality.\r\n> \r\n\r\n@dzakhar @JaccovG: With this PR we are no longer supporting the use of TAGS as part of the top-level TFLM Makefile. The optimized kernel implementation can be selected with OPTIMIZED_KERNEL_DIR=<your dir> and for anything else the specific target / optimized kernel makefile can add additional command line options.\r\n\r\nThe diff for the Makefile in this PR will give you some more info on what has changed: https://github.com/tensorflow/tensorflow/pull/46473/files?file-filters%5B%5D=No+extension#diff-84c8d7e242143d674d6e26ab56443970ed8e1f3db0fcb1d0e8e127eaebbe1a1a\r\n\r\nI'd be happy to help you figure out a path forward to getting the ARC platform support up and running."]}, {"number": 46472, "title": "Fixing eigen tp crash", "body": "", "comments": []}, {"number": 46471, "title": "Use TensorFlow with GPU RTX 3090 support on Windows (not WSL2) ", "body": "Is there a set of instructions for using a RTX 3090 with CUDA and tensorflow python API on windows ? I dont mind what versions I need, or what I have to build etc, I would just like a definitive set of instructions on how to get this working. \r\n\r\nCUDA version\r\nCUDNN version\r\nTensorflow version\r\nPython version\r\n\r\nThanks. ", "comments": ["Actually I got it working. Using tf nightly, CUDA 11.1, CUDNN 8, python 3.8.5, TF GPU 2.4.0. Please see attached conda list and pip freeze - I used a combination of both - conda for the base env (python 3.8.5)\r\n\r\nI did have to copy a dll file in my cuda bin, super easy just copy the file cusolver64_11.dll to cusolver64_10.dll - before you do this the stderr from running the gpu device finder will tell you that dll is missing.\r\n\r\nMarcus\r\n\r\n[T2.4_CUDA11.1_CUDNN8_Win10_pipfreeze.txt](https://github.com/tensorflow/tensorflow/files/5823550/T2.4_CUDA11.1_CUDNN8_Win10_pipfreeze.txt)\r\n[T2.4_CUDA11.1_CUDNN8_Win10_condalist.txt](https://github.com/tensorflow/tensorflow/files/5823551/T2.4_CUDA11.1_CUDNN8_Win10_condalist.txt)\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46471\">No</a>\n"]}, {"number": 46470, "title": "Update the TFLM CI docs based on the new way to build a docker image.", "body": "The instructions need to be updated after the change from https://github.com/tensorflow/tensorflow/commit/4491d6ee3a13956f3e455f9506459c82de9ed268", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46469, "title": "Update arm_gcc_download.sh", "body": "This fixes #46393, tested on macOS Catalina", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46468, "title": "[CherryPick:r2.4] Don't use `$HOME`, use `~`.", "body": "Should fix `gpu_on_cpu` Docker build.\n\nPiperOrigin-RevId: 352055331\nChange-Id: I964f2cbca39d126b5043bd621b38fa288dddc416", "comments": []}, {"number": 46467, "title": "Enable clang+bazel as part of the TFLM github CI.", "body": "We patch some of the TF workspace and BUILD files to reduce the number of downloads as part of a bazel build.\r\n    \r\nWe can revisit this decision if the maintenance overhead from this patching becomes too high.\r\n    \r\nAlso, there may be an opportunity to further reduce the downloads, but this change has made a decision to leave the TfLite build files unchanged and incur the cost of the additional downloads that are needed for TfLite but not for TfLite Micro.\r\n\r\nFixes #46465\r\nFixes http://b/177672856\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "[`0a66fd0` (#46467)](https://github.com/tensorflow/tensorflow/pull/46467/commits/0a66fd0cc7b4bdf0ed1b6b33f9d0e21899ec9afa) is the only commit that is part of the current PR.\r\n\r\nThe first commit: [`4dadd12` (#46438)](https://github.com/tensorflow/tensorflow/pull/46438/commits/4dadd1262353df4671b43f8275f3d03129bbb973) is part of #46438.\r\n\r\nAnd the third commit [`335cf39` (#46467)](https://github.com/tensorflow/tensorflow/pull/46467/commits/335cf3943a9db6e20129490af1a9beccfa571b7b) is only needed until an internal change is reviewed.", "Let's discuss this more on Tuesday.\r\n\r\nThe change isn't the cleanest and certainly a compromise.\r\n\r\nI have stripped down the .bzl files in the top-level tensorflow directory to avoid downloading dependencies that are not needed for TFLM.\r\n\r\nI'm making the trade-off that the benefits of a faster CI (and clearer understanding of what is needed for TFLM) are worth the potential maintenance overhead from having a different version of these files for the purposes of the TFLM github CI."]}, {"number": 46466, "title": "Add cuda 11.2 driver and runtime inc files", "body": "CC @sanjoy ", "comments": ["The test failure looks unrelated to the changed made in the PR.", "TF/xla with CUDA 11.2 gives the following warning:\r\n\r\n``warning: Linking two modules of different target triples: '/usr/local/cuda-11.2/nvvm/libdevice/libdevice.10.bc' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'``\r\n\r\ncc @jpienaar @timshen91 ", "@byronyi We have not yet fully tested TF with 11.2 so some failures are expected.  Having said that, that particular issue is probably a false positive, we should not print this warning.  We can either remove the diagnostic printer while linking (it is created and added [here](https://github.com/tensorflow/tensorflow/blob/b444969f2a8101a7add09c71f917196714bf1629/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc#L806)), or change the printer to filter out just the [linker diagnostics](https://github.com/llvm/llvm-project/blob/d75b3719828f3e0c9736476e50a08e5083f90c0b/llvm/include/llvm/IR/DiagnosticInfo.h#L59).", "@sanjoy Except this particular warning, the CUDA 11.2 build seems pretty fine for the model garden RN50."]}, {"number": 46465, "title": "Switch TFLM CI to use clang+bazel", "body": "@tensorflow/micro\r\n\r\nUsing clang instead of gcc with bazel will mirror the internal checks and reduce the likelihood of internal changes breaking the github TFLM CI (e.g. https://github.com/tensorflow/tensorflow/issues/46415).\r\n\r\nIt will also allow showing the results of the sanitizer builds externally as well.\r\n", "comments": ["Corresponding internal issue: http://b/177672856"]}]