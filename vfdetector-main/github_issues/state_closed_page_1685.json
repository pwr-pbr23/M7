[{"number": 2352, "title": "Enable GPU for L2Loss float64", "body": "Enabled GPU registration for L2Loss operations of type double. This partially addresses #1140.\n\nTested locally. I'm assuming that there won't be any change in the tests because of [this commit](https://github.com/tensorflow/tensorflow/commit/a1bc10f9e28cd3d33e91bfaedd8199e4590f2893).\n", "comments": ["Can one of the admins verify this patch?\n", "I think we do want to add a test for double -- that commit you linked was removing test cases, I think.\n", "Actually I'm not sure what's going on now -- if nn_test is passing right now without the GPU double kernel registered, then that kernel isn't being tested.\n", "@vrv Is there something I need to do?\n", "What I'm asking is for a test of the L2Loss op -- you've added the registration but we have no idea if it works for double!  So you should be able to write a test that fails without the registration, and passes after it.\n", "@vrv I found something strange. Even if I change [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_test.py#L129) to `with self.test_session(use_gpu=True)`, the test succeeds; without the registration of the float64 GPU kernel. How is this possible?\n", "I'm not sure, can you try passing 'log_device_placement=True' to the session options passed to test_session to see what's going on?\n", "@vrv Gives me the following output:\n`Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0\nConst: /job:localhost/replica:0/task:0/gpu:0\nIdentity: /job:localhost/replica:0/task:0/gpu:0\nx: /job:localhost/replica:0/task:0/gpu:0\ngradients/L2Loss_grad/mul: /job:localhost/replica:0/task:0/gpu:0\nL2Loss: /job:localhost/replica:0/task:0/cpu:0`\n\nL2Loss on the CPU, this is with `use_gpu=True`. Any hints as to where I should dig for this?\n", "@vrv I don't think that we should add any more tests here, just to be consistent. For example, consider the changes in my commit [here](https://github.com/tensorflow/tensorflow/pull/2506/commits/00d402d350302764595ce5cd6aec03e33471baae).  Even if I remove the registration from `matmul_op.cc`, the test still succeeds as it is able to find the kernel for the CPU (same is true for all the other tests in TF). The only way we will get the behavior you want is by using `force_gpu=True`, which has not been used in other tests. Does this make sense?\n", "You should still add a test for float64 though, otherwise it's just completely untested.\n\nYou can then use log_device_placement temporarily to verify it's running on the GPU.\n", "@vrv There is a test for float64 [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_test.py#L128). I verified using `force_gpu=True`\n", "Nice, let's also add one https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_test.py#L119 as well.  something like\n\n```\nfor dtype in [tf.float32, tf.float64]:\n  with self.test_session():\n    x = tf.constant(..., dtype=dtype)\n    ... same as before ...\n```\n", "@vrv Modified the test.\n", "@tensorflow-jenkins test this please\n", "@vrv The failure was due to a timeout, seems unrelated to the change.\n"]}, {"number": 2351, "title": "No gradient for log_softmax", "body": "It appears that the log_softmax doesn't have a gradient:\n\n```\n>>> import tensorflow as tf\n>>> var = tf.Variable([[1.0, 2.0]])\n>>> softmax = tf.nn.softmax(var)\n>>> log_softmax = tf.nn.log_softmax(var)\n>>> entropy = tf.reduce_sum(softmax * log_softmax)\n>>> trainer = tf.train.GradientDescentOptimizer(0.1).minimize(-entropy)\nLookupError: gradient registry has no entry for: LogSoftmax\n```\n", "comments": ["This has been fixed internally and will be available by the next update.\n"]}, {"number": 2350, "title": "fix shuffle_batch zero_division bug", "body": "fix #1853 \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Only organization members are allowed to run the tests.  I'll fire them off once you make the suggested simplification.\n", "@girving added new commit, if `min_after_dequeue` equals `capacity`,  will get\n`tensorflow.python.framework.errors.InvalidArgumentError: min_after_dequeue 4 must be < capacity 4` instead of `inf`\n", "Hmm, sorry I wasn't reading this carefully enough.  I think we should just throw a `ValueError` with a reasonable message if those two numbers are equal, since it is a useless queue.  @yaroslavvb: Since you filed the original bug, does that sound right to you? \n", "ping for @girving and @yaroslavvb \n", "@girving Correct, it's a useless queue. With my \"convert_to_tensor\" suggestion it will fail with \"tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: min_after_dequeue 4 must be < capacity 4\" which is more informative than original division by zero in Summary construction. Throwing explicit ArgumentError sounds good to me too\n", "Closing due to inactivity, comment and we'll reopen and retest, etc.\n"]}, {"number": 2349, "title": "A couple of bugfixes ", "body": "fixed wrong indexing into dict; every picture is read into 3 channels even if input is grayscale; loaded jpeg is read as rb buffer.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "One fix of this PR seems to aid the following issue: #2304 .\n", "Can one of the admins verify this patch?\n", "Seems fine but are there any tests that should have caused this to fail?  cc @petewarden \n", "Good catch! It would have been good to catch this with a test, but the training apparently still produced good results even with black and white images, so it wasn't obvious.\n\nJenkins, test this please.\n"]}, {"number": 2348, "title": "Build failure ArchLinux with nvcc + gcc 6.1.1", "body": "Just downloaded and built on Archlinux, and it failed with:\n\n```\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/MathFunctions.h(828): error: more than one instance of overloaded function \"fmin\" matches the argument list:\n            function \"std::fmin(float, float)\"\n            function \"fmin(float, float)\"\n            argument types are: (const float, const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/MathFunctions.h(840): error: more than one instance of overloaded function \"fmax\" matches the argument list:\n            function \"std::fmax(float, float)\"\n            function \"fmax(float, float)\"\n            argument types are: (const float, const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(126): error: more than one instance of overloaded function \"erf\" matches the argument list:\n            function \"std::erf(float)\"\n            function \"erf(float)\"\n            argument types are: (const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(126): error: more than one instance of overloaded function \"erf\" matches the argument list:\n            function \"std::erf(float)\"\n            function \"erf(float)\"\n            argument types are: (const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(126): error: more than one instance of overloaded function \"erf\" matches the argument list:\n            function \"std::erf(float)\"\n            function \"erf(float)\"\n            argument types are: (const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(126): error: more than one instance of overloaded function \"erf\" matches the argument list:\n            function \"std::erf(float)\"\n            function \"erf(float)\"\n            argument types are: (const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(138): error: more than one instance of overloaded function \"erfc\" matches the argument list:\n            function \"std::erfc(float)\"\n            function \"erfc(float)\"\n            argument types are: (const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(138): error: more than one instance of overloaded function \"erfc\" matches the argument list:\n            function \"std::erfc(float)\"\n            function \"erfc(float)\"\n            argument types are: (const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(138): error: more than one instance of overloaded function \"erfc\" matches the argument list:\n            function \"std::erfc(float)\"\n            function \"erfc(float)\"\n            argument types are: (const float)\n\nexternal/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/MathFunctions.h(138): error: more than one instance of overloaded function \"erfc\" matches the argument list:\n            function \"std::erfc(float)\"\n            function \"erfc(float)\"\n            argument types are: (const float)\n\n10 errors detected in the compilation of \"/tmp/tmpxft_000017f0_00000000-10_batchtospace_op_gpu.cu.compute_52.cpp1.ii\".\nERROR: /.../tensorflow/tensorflow/core/kernels/BUILD:1320:1: output 'tensorflow/core/kernels/_objs/batchtospace_op_gpu/tensorflow/core/kernels/batchtospace_op_gpu.cu.o' was not created.\nERROR: /.../tensorflow/tensorflow/core/kernels/BUILD:1320:1: not all outputs were created.\n```\n\ngcc version 6.1.1 20160501 (GCC) \nCuda compilation tools, release 7.5, V7.5.17\ncudnn version 5\necd5b7255e05634cad6ea5e0bc680f7a9d981ba8\n", "comments": ["I don't think cuda supports host gcc of >= 5 yet, so you'll have to install gcc 4.8 or 4.9 and use that for nvcc's host compiler\n", "I've built gpu-tensorflow with gcc 5.3.\n", "@vladfi1 \n\n> I've built gpu-tensorflow with gcc 5.3.\n\nHow? The last time I tried to do that, the compilation failed due to explicit version checks in the CUDA headers.\nCould you post instructions how you did it? I'm very interested to compile TensorFlow with newer compilers.\n", "There's [bug 49272](https://bugs.archlinux.org/task/49272) that tracks this.\n\nYou can use [this PKGBUILD](https://gist.github.com/slokhorst/029aa3c86bdb897cb47acc3f137aa11c) to build gcc49. Then follow the instructions [here](https://wiki.archlinux.org/index.php/GPGPU) under `Using CUDA with an older GCC`.\n", "I think that I have fixed the problem in Eigen with these 2 commits: https://bitbucket.org/eigen/eigen/commits/1ba80c847ac312333e13f2274c6e69d4654966d9 and https://bitbucket.org/eigen/eigen/commits/0e3f9602b2aa787d851c6aa85930fa1dcac7f693\n\nThe problem will be fixed in tensorflow as soon as we update the build rules to pull a version of Eigen which contains the fixes. \n", "I updated TensorFlow to pull the latest version of Eigen that contains the fixes for the compilation errors, so I'm closing this issue. Let me know if the fix doesn't work and I'll reopen this bug report.\n"]}, {"number": 2347, "title": "Set -headerpad_max_install_names on Darwin for targets that need binary header padding to rename paths", "body": "Fixes #2332\n", "comments": ["Can one of the admins verify this patch?\n", "why this test only?\n", "also this doesn't select based on platform\n", "Oops. Accidentally did a force push on a different machine without pulling. The select should be there now.\n\nI only noticed it on this test the first time through but I'll run everything again to see if there are others that need this too.\n", "Do you understand why it fixes anything, and what the implications are for setting this globally?  Cause I don't :)\n", "All of the targets that have this problem should be covered now.\n\nIt seems that the problem is occurring when [Bazel's cc_wrapper script](https://github.com/bazelbuild/bazel/blob/master/tools/cpp/osx_cc_wrapper.sh) renames all the paths in the binary with the paths relative to the binary (since relpath in OS X doesn't really work). For these targets, the load commands in the binary end up being [larger after the renaming](http://stackoverflow.com/questions/28324785/install-name-tool-cant-use-change-because-larger-updated-load-commands-do-not), which would require relinking. The `-headerpad_max_install_names` flag would be needed to ensure there is enough room in the binary for the larger load statements.\n\nOn that note, @damienmg, do you think it would be a good idea to set `-headerpad` or `-headerpad_max_install_names` by default in `osx_cc_wrapper.sh` or should we just set the linkopt for targets that need them?\n", "@davidzchen I think this make more sense to add it to the CROSSTOOL file for darwin in Bazel.\n", "Jenkins, test this please.\n", "We probably want to accept this anyway, and remove it once the fix has made its way into a bazel release.\n", "Is this patch ready to be merged?\n", "Rebased on master.\n\nDo we still want to merge this PR?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2346, "title": "Basic implementation of immediate execution mode for TensorFlow", "body": "This version implements Immediate execution for native TensorFlow ops with graph caching:\n\nIE, you can do\n\n```\n    tf = immediate.Env()\n    val = np.ones(())\n    tensor1 = immediate.Tensor.numpy_to_tensor(env, val)\n    tensor2 = immediate.Tensor.numpy_to_tensor(env, val)\n    tensor3 = tf.add(tensor1, tensor2)   # executes operation immediately\n    tensor3 = tf.add(tensor3, tensor2)   # executes operation without modifying graph\n    print tensor3\n\n```\n\nWhat's not implemented yet (rough implementation detail is in the design doc):\n- support for \"tf.nn\" namespace\n- support for manually created Python ops\n- caching for get_session_tensor and get_session_handle ops\n\n@yuanbyu @keveman \n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I signed it!\n", "If I click on the link it gives me, it shows that I signed\n![screen shot 2016-05-12 at 6 01 40 pm](https://cloud.githubusercontent.com/assets/23068/15234921/c1cb14a2-186b-11e6-8e6f-674305907bff.png)\n", "rats, I did all my commits as \"yaroslavvb@Yaroslavs-MacBook-Pro.local\"\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@adarob using `tf` or some other name is up to the user, the framework itself doesn't change anything in the original `tf` or `tensorflow` namespace. You could use `tfi` instead of `tf` for instance. I've been using `tf` in order to reuse existing tests, but run them in immediate mode, but when you want to mix things, I would use `tfi`. Also, Python's symbol resolution happens during module loading time, so when SyntaxNet is loaded it remembers what `tf` was during that time, so if you redefine it later, it won't be affected.\n", "You should do a rebase, I think something went wrong in the one you did. Try making a clean branch from tensorflow/tensorflow, cherry-picking your changes there, and then force-pushing it onto your master.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Something is wrong with your fork. I at this point, I would recommend to keep your changes someplace safe, delete the fork, and re-fork. Then add the new fork as a remote to your existing local copy, and do something like\n\n```\ngit remote add new-fork git@github.com/yaroslavvb/tensorflow\ngit checkout new-fork/master\ngit cherry-pick all-your-actual-changes\ngit push new-fork\n```\n\nOr similar. In the end, you should have only a few commits on top of master when you make the PR.\n", "Thanks for the tip, I'll give that a try\n\nOn Tue, May 31, 2016 at 9:46 AM, Martin Wicke notifications@github.com\nwrote:\n\n> Something is wrong with your fork. I at this point, I would recommend to\n> keep your changes someplace safe, delete the fork, and re-fork. Then add\n> the new fork as a remote to your existing local copy, and do something like\n> \n> git remote add new-fork git@github.com/yaroslavvb/tensorflow\n> git checkout new-fork/master\n> git cherry-pick all-your-actual-changes\n> git push new-fork\n> \n> Or similar. In the end, you should have only a few commits on top of\n> master when you make the PR.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2346#issuecomment-222748552,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHKJG2KnVZP1mKPgRabCoKHj_3frLks5qHGYDgaJpZM4IdnId\n> .\n", "I accidentally committed debug changes to files outside of contrib/ which makes this pull request hard to merge, closing this request to replace it with a fresh one\n"]}, {"number": 2345, "title": "Numpy Layer", "body": "Is there no way for me to create a new layer in python as a function with inputs and outputs as cpu numpy arrays. This would be especially useful for certain custom operations (say an opencv call in the middle of the network, or a special logging layer). But as far as I can tell, it is not available, and one has to go through the hassle of writing a c++ layer and recompiling to be able to get direct access to any data below the symbolic level. Am I missing something?\n\nThe fact I can't do this creates massive workarounds. In some cases, I call sess.run() once, getting the result as a numpy array, do my operations, and then call sess.run() again. So there are 2 sess.run() calls for one iteration of training.\n", "comments": ["https://www.tensorflow.org/versions/r0.7/api_docs/python/script_ops.html#py_func\n", "Great, thanks!\n"]}, {"number": 2344, "title": "Enable GPU for MatMul float64", "body": "Enabled GPU registration for MatMul operations of type double. This partially addresses #1140.\n\nI realize that this is a very small change, I wanted to become familiar with the pull request process before making moderately big changes.\n", "comments": ["Can one of the admins verify this patch?\n", "Did you test this out locally, by any chance?\n", "Yes, tested it using the following:\n\n`bazel test -c opt --test_verbose_timeout_warnings --config=cuda //tensorflow/python:matmul_op_test`\n", "Cool, thanks!  This used to not work because we compiled an Eigen version of contraction which didn't support double, but cublas supports it, so we're all good.\n\n@tensorflow-jenkins test this please\n", "I'm going to work on making float64 work on the GPU for other ops as well. Would you prefer pull requests for each op separately, or multiple ops at once?\n\nI'm sorry if this is the not the right forum for me to be asking this. In case it isn't, is there some place someone new like me could ask questions?\n", "Keeping it manageable is always good!  Doing classes of ops at a time is fine -- you can probably do a bunch of related ops at once so they're easy to test and verify together, but too many at once and it'll be hard to review.    This is a fine forum to ask about this, since it's about development / new code features.\n", "This is weird. The test passes on my machine, don't know what's wrong. I tested it again.\n", "Going to try one more time, the test that failed wasn't the 'double' one...\n\ntest this please, @tensorflow-jenkins \n", "Sorry, I was on vacation, can you rebase?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@vrv I think I messed up the rebase, it is giving me all the commits that weren't there in my branch. How do I fix it?\n", "I'm not sure, I generally do (on the dev branch);\n\n```\ngit fetch upstream\ngit rebase upstream/master\ngit push -u origin $branch\n```\n\nYou might want to start again in a new PR if it's too messy to undo for you\n", "Right, I'm closing this. Will send out a new PR.\n"]}, {"number": 2343, "title": "Distributed Runtime protos aren't sandbox compatible", "body": "The master/worker protos don't include their dependent source files in the rules that constitute them in a way that enables tensorflow to be used from a skylark rule. This is annoying.\n", "comments": ["I met this , is  a same issue\uff1f\n\n/root/tensorflow/tensorflow/core/BUILD:135:1: null failed: protoc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/google/protobuf/protoc '--cpp_out=bazel-out/local_linux-opt/genfiles/' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_cpp_plugin' '--grpc_out=bazel-out/local_linux-opt/genfiles/' -I. -Igoogle/protobuf/src tensorflow/core/protobuf/master_service.proto): protoc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/google/protobuf/protoc '--cpp_out=bazel-out/local_linux-opt/genfiles/' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_cpp_plugin' '--grpc_out=bazel-out/local_linux-opt/genfiles/' -I. -Igoogle/protobuf/src tensorflow/core/protobuf/master_service.proto).\ntensorflow/core/protobuf/master.proto: File not found.\n", "Pretty much, yup. I 'fixed' it by removing the distributed rules from a forked version of Tensorflow, cause I don't care about them right now\n", "Automatically closing due to lack of recent activity. Verify that this is still a problem on the latest version of TensorFlow and reopen if it does. "]}, {"number": 2342, "title": "\"_too_large_attrs\" for Graph visualization on Tensorboard. ", "body": "I am not sure if this issue is beyond the Github scope. If it is, I apologize.\n\nI am trying to visualize the computation graph for the project I am working. I can see all the histograms as well as the events. However, computation graph visualization displays following message.\n\n![](https://raw.githubusercontent.com/agupta83/shared_data/master/tf-failed_adding_edges.png)\nPlease find the [tfevent-file](https://drive.google.com/folderview?id=0BzP3qMqdjzOwb1pGN2diWElyNWs&usp=sharing) shared on google drive\n### Environment info\n\nOperating System: Ubuntu 14.04. \n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root  60M Apr 10 17:49 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n-rw-r--r-- 1 root root  59M Apr 10 17:49 /usr/local/cuda-7.5/lib64/libcudnn.so.7.0.64\n-rw-r--r-- 1 root root  59M Apr 10 17:49 /usr/local/cuda-7.5/lib64/libcudnn.so.7.0\n-rwxr-xr-x 1 root root  59M Apr 10 17:49 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root  59M Apr 10 17:49 /usr/local/cuda-7.5/lib64/libcudnn.so.4\n-rw-r--r-- 1 root root  59M Apr 10 17:49 /usr/local/cuda-7.5/lib64/libcudnn.so\n-rw-r--r-- 1 root root 316K Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\n-rw-r--r-- 1 root root 704K Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 375K Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nVirtualenv with `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl`\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nAttributeError: 'module' object has no attribute 'version'\n```\n\nIf installed from sources, provide the commit hash:\n- Build from source with commit 2296dd8060ce77c71fc820c77442835f050399dd\n### Steps to reproduce\n1. Download the tf [tfevent-file](https://drive.google.com/folderview?id=0BzP3qMqdjzOwb1pGN2diWElyNWs&usp=sharing).\n2. `tensorboard --logdir=path/to/downloaded/folder/`\n### What have you tried?\n1. Modified `prepare_graph_for_ui` [process_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/backend/process_graph.py) from `limit_attr_size=1024` to `limit_attr_size=None`\n2. Modified `LIMIT_ATTR_SIZE = 1024` in [`graph.ts` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/tf-graph-common/lib/graph.ts) to large value (5 \\* 1024 \\* 1024).\n3. Explicitly built tensorboard using `bazel build tensorflow/tensorboard:tensorboard`\n### Logs or other output that would be helpful\n\n```\n...\n...\n...\n127.0.0.1 - - [12/May/2016 10:43:58] \"GET /data/runs HTTP/1.1\" 200 -\n127.0.0.1 - - [12/May/2016 10:43:58] \"GET /data/runs HTTP/1.1\" 200 -\n127.0.0.1 - - [12/May/2016 10:44:02] \"GET /data/graph?run=.&limit_attr_size=1024&large_attrs_key=_too_large_attrs HTTP/1.1\" 200 -\n```\n\n![](https://raw.githubusercontent.com/agupta83/shared_data/master/tf-failed_adding_edges.png)\n", "comments": ["I tried couple of stuff and it finally worked. I made my variables name shorter than before and it fixed the issue.\n", "I'm getting this same issue. What stuff did you try? I don't think that my variable names are too long.\n", "I am not really sure what is causing this issue. It was resolved once I changed scope names and variables names to shorter string. You can check the `tfevent` file attached with this issue to see the how the scope and variable was named before. TF adds `op_name` on top of that. Now I am using something shorter (shown below) and the graph rendered. I am still unsure about how the error shown in the console is related to this but it worked out for me.\n\n``` python\nwith tf.variable_scope('pre-trained-emb') as scope:\n        embedding = _variable_on_cpu(name='embd',\n                                        initializer=PRETRAINED_EMBD)\n        # Lookup\n        embd_input = tf.nn.embedding_lookup(embedding, titles)\n        expand_embd_input = tf.expand_dims(embd_input, -1, name=scope.name)\n```\n", "Can you post a tfevents file that is broken? I tried the original one attached to this thread and it renders just fine.\n", "You should be able to get it here. https://drive.google.com/file/d/0B7nTycTqHbhMZ21uMDMxOUJHNWc/view?usp=sharing\n", "Closing due to inactivity.", "Please elaborate what do you mean by making the variable name shorter. I couldn't understand.  my events file is 11 MB long and it gets stuck at \"Namespace Hierarchy: finding similar subgraphs\" \r\n\r\nand after a while the page becomes unresponsive. \r\n\r\n \r\n\r\n> /data/plugin/graphs/graph?run=.&limit_attr_size=1024&large_attrs_key=_too_large_attrs HTTP/1.1\u001b[0m\" 200 -\r\n"]}, {"number": 2341, "title": "Upstream changes from internal", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "wow, 1 hour to test GPU .... at least it passed.\n"]}, {"number": 2340, "title": "Tensor.eval() Performance Decay", "body": "I used Tensorflow 0.6 (I pip installed TensorFlow: `pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl`) with Python 3 to train Neural Network on GPU (Nvidia GeForce GTX Titan X with Cuda 7.0 and Cudnn 6.5), and I noticed an interesting problem: The time consumption for running `tensor.eval(session=..., feed_dict=...)` at each iteration was continuously growing as iteration goes by, which indicated that performance of tensor.eval() decayed. Specifically, the code looks like this:\n\n```\nimport tensorflow as tf\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\nsession = tf.Session(config=tensorflow.ConfigProto(gpu_options=gpu_options))\n\n\"\"\"Then build the neural network tensor graph, For example,\"\"\"\n\"\"\"we define an neural network to compute output layer tensor a.\"\"\"\n\n# input tensor is an numpy array with dimension 32 * 336 * 84.\ndict = {input_tensor: input_tensor}\nfor _ in 1000000:\n     a.eval(session=session, feed_dict=dict)\n```\n\nIn this code, the running time for each `a.eval()` gradually got larger and larger as iteration time goes by. So I used Profiling tooI (CProfile) to dig into the code to see at which part of code performance going wrong, and it indicated that time consumption for executing `built-in method TF_Run` for running `a.eval()` was very long and kept growing. I am pretty sure the `input_tensor` size keeps exactly same for each iteration, I wonder if anyone can help me figure out the problem, and how I can improve my current performance. Thanks!!!\n", "comments": ["Does the problem persist in TensorFlow 0.8? Can you share a complete program that exhibits the problem? In particular, how are `a` and `input_tensor` defined?\n", "@mrry Thanks for your answer :) Yes, for TF 0.8, we used the same profiling tool to test it, and still got linear performance decay as running time goes by. Actually, I'm really eager to show all my code here, but the complete code has more than 2000 lines... But I can show you the data shape that I defined: it's `[array([[\u2026], \u2026, [\u2026]]), array([[\u2026], \u2026, [\u2026]]), \u2026, array([[\u2026], \u2026, [\u2026]])]` - list with dimensions 32 \\* 336 \\* 84.\n", "If you simply fetch a constant containing that data using `Tensor.eval()` in a loop, does the time increase for each call?\n\nI expect that the problem is somewhere in the other part of the 2000 lines, and it's just being accounted to `TF_Run()` since that's the main entry-point to the native runtime. It would be helpful to make a minimal example that still exhibits the problem. For example, are there queues, loops, or other more exotic features in your program?\n", "I just tried to reproduce this with a simple program, and while the distribution of step times is noisy, it doesn't seem to show an increasing trend:\n\n![figure_1](https://cloud.githubusercontent.com/assets/192142/15228283/41845590-1840-11e6-9c1e-f8ad6a217909.png)\n\n``` python\nimport time\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nc = tf.constant(5)\ntimes = []\nwith tf.Session():\n  c.eval()  # Ignore first, slow step.\n  for _ in range(100000):\n    start = time.time()\n    c.eval()\n    times.append(time.time() - start)\n\nplt.plot(times)\nplt.ylim(0, 0.0010)\nplt.show()\n```\n", "@mrry I think you are right. I also tested `tensor.eval()` in a single loop by myself, and seems time for running this function is not growing. So there might be some places in my code getting the time consumption problem... By the way, I wonder whether or not `tensorflow.concat()` creates new nodes in tensor graph. Also, I wonder if there are tensorflow methods that creates new nodes. Thanks!!!\n", "When we've seen this problem in the past, it's usually because something in the training loop is creating new graph nodes, and the TF runtime has to do more work each time you call `eval()`, because it believes it's running a new graph (even if it's structurally identical to the old ones). If you're calling `tf.concat()` in the training loop, that would be adding new nodes to the graph, and would lead to bad performance. One quick way to test this is to call `tf.get_default_graph().finalize()` before your training loop, and you will get an exception if anything is added to the graph.\n\nYou can usually avoid these performance issues by creating `tf.placeholder()` nodes, defining the (e.g.) `tf.concat()` in terms of them outside the loop, and feeding in the appropriate values.\n\nI'm going to close this issue for now, but let us know if you have other questions!\n"]}, {"number": 2339, "title": "python 3.5 compatibility and remove unused imports", "body": "", "comments": ["Can one of the admins verify this patch?\n", "LGTM, thanks!\n"]}, {"number": 2338, "title": "python 3.5 compatibility and better display effect", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2337, "title": "Update seq2seq", "body": "This PR brings updates to the seq2seq examples, removing all of the `with ops.device(\"/cpu:0\"):`. \n\nThere are many ops implemented with cuda kernels recently, so maybe it's time to remove some cpu device specifications from the translate.py examples.\n\nI don't have profile numbers for the translate.py example, but according to a similar experiment, this will boost the speed 5~6 times on a NVIDIA Titan GPU, especially when there are more than one independent runs going together.\n\nSide note: when training with multiple cards (independent runs), the frequent data transfer between gpu and cpu is particularly harmful in my case.\n", "comments": ["Can one of the admins verify this patch?\n", "ping ...\n\nAny feedbacks are appreciated.\n", "@tensorflow-jenkins test this please\n", "@vrv Thanks for your attention. Updated. Do you know why the test failed on the matmul?\n", "no idea, let me try again.\n\n@tensorflow-jenkins test this please\n"]}, {"number": 2336, "title": "Set workspace name", "body": "Adds the workspace name to runfiles paths, removes a hack introduced in\nhttps://github.com/tensorflow/tensorflow/commit/8cda66e19c0bdc6d8fc373fdcefcc644ab5ecfa3,\nand fixes #2040.\n", "comments": ["Can one of the admins verify this patch?\n", "Does this require a specific version of bazel to be installed, btw?\n", "Yes, this will only work with 0.2.3+ (see https://groups.google.com/d/msg/bazel-discuss/_Xo14sf7fHw/g7wdatTGBgAJ for more info about the change).  For this to work with older versions, I can make it check both paths and use whichever exists.\n", "Given that 0.2.3 isn't officially released yet, I think we'd prefer to try to make it work with both versions, at least for a little transition period.\n", "Okay, finally updated this, at least according to the test suite this should now work with old and new versions of Bazel.\n", "@tensorflow-jenkins test this please\n", "Whoops, I didn't realize your CI was still on 0.2.1, should be fixed it for that version (and previous versions), too.\n", "We should probably upgrade our CI to use a newer version of bazel, but supporting older is good too, particularly since check_version requires 0.2.1 right now\n\n@tensorflow-jenkins test this please\n"]}, {"number": 2335, "title": "\"bazel test\" fails on MacOS with setuptools-21.0", "body": "I just went through official MacOS instructions to setup bazel/swig and tried the following\n\n```\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow\ncd tensorflow\n./configure\nbazel test -c opt //tensorflow/python/...\n```\n\nThey all fail with stack trace like below, any suggestions?\n\n```\nTraceback (most recent call last):\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/python/zero_division_test.runfiles/tensorflow/python/kernel_tests/zero_division_test.py\", line 22, in <module>\n    import tensorflow as tf\n  File \"/Users/yaroslavvb/g/src/hacking/tensorflow/_python_build/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/yaroslavvb/g/src/hacking/tensorflow/_python_build/tensorflow/python/__init__.py\", line 52, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Users/yaroslavvb/g/src/hacking/tensorflow/_python_build/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\n  File \"/Library/Python/2.7/site-packages/protobuf-3.0.0b2-py2.7.egg/google/__init__.py\", line 1, in <module>\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2927, in <module>\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2913, in _call_aside\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2952, in _initialize_master_working_set\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 956, in subscribe\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2952, in <lambda>\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2515, in activate\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2097, in declare_namespace\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2047, in _handle_ns\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2066, in _rebuild_mod_path\n\n  File \"/Library/Python/2.7/site-packages/setuptools-21.0.0-py2.7.egg/pkg_resources/__init__.py\", line 2064, in position_in_sys_path\n\nValueError: '/private/var/tmp/_bazel_yaroslavvb/4a326dbae226c596bf5de825a172dee9/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles' is not in list\n```\n", "comments": ["Googling suggested problem with new setuptools. Downgrading setuptools to version 19 fixes it\n\n```\nsudo rm -R /Library/Python/2.7/site-packages/setuptools*\nsudo pip install setuptools==19.2 \nbazel test -c opt //tensorflow/python:graph_util_test --test_output=streamed\n```\n", "Actually maybe this should be open because setuptools-21.0 is the default for new users and this issue was tricky to diagnose, maybe the installing for MacOS page could be updated to say that setuptools-21.0 is not supported? https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#installation-for-mac-os-x\n", "Can we encode this requirement in the dependencies for the tensorflow pip package? Can tensorflow require setuptools=19.0?\n", "@yaroslavvb: What was the problem with the new setuptools?  Is it something we could work around, and is it possible to upgrade rather than downgrade to fix the problem? \n", "I searched for \"is not in list\" and \"setuptools\" and found other people having this problem in other packages. Here's on description: https://lab.nexedi.cn/nexedi/wendelin.core/commit/2ce96a76421f942e8416ceeebf5088352b5fa1c7\n\nDo you guys have a Mac CB setup yet? \n", "@yaroslavvb: What does \"Mac CB\" mean?\n", "Mac continuous build/test\n\nOn Tue, Jun 7, 2016 at 11:03 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @yaroslavvb https://github.com/yaroslavvb: What does \"Mac CB\" mean?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2335#issuecomment-224363732,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHJTolmXt8xXGVMtX3vO6wJvBlQFTks5qJbJXgaJpZM4Ictt_\n> .\n", "@yaroslavvb: Yes we do, but it doesn't cover all versions of setuptools.   @martinwicke: Yaroslav's linked fix seems like something we could do too.\n", "So basically people hack around a problem in setuptools? This cannot be intended behavior?\n", "FYI our Mac CI uses setuptools 18.3.1.\n", "@yaroslavvb has this issue been resolved?\r\nMac builds seem to be pretty stable so far, but I have not checked the setuptools version on our mac machines.", "This has a high chance of being fixed and/or not longer relevant hence closing"]}, {"number": 2334, "title": "Immediate-mode execution in TensorFlow", "body": "Mirror TensorFlow API to provide \"numpy/Torch\"-like mode for executing TensorFlow programs. \n\nBriefly, provide a mode where you can do the following: \n\n```\na = tf.constant([[1,1],[1,1]])   # constant is created\nb = tf.constant([[2,2],[2,2]])\nc = tf.matmul(a, b)  # executes matrix multiplication\nprint c                      # transfers contents of c from TensorFlow runtime to Python runtime and prints it\n\n```\n\nGoals\n1. No separation between graph-construction and evaluation - each op is executed in TensorFlow runtime the moment the corresponding wrapper is evaluated by Python\n2. Reuse of TensorFlow Graph for efficiency. For instance, after float32 addition was performed once, any additional float32 addition operations should be done by reusing previously created OpDef\n3. Immediate-style Tensor object implements functions in [3.4.8. Emulating numeric types](https://docs.python.org/2/reference/datamodel.html#emulating-numeric-types) and translates them to appropriate TensorFlow operations \n4. Immediate-style Tensor object keep contents of underlying tensors in TensorFlow runtime and only transfers them to Python when necessary, such when needed for flow control and printing.\n\nDesign doc: https://docs.google.com/document/d/1CRfhT2_-EaboFbN1U7gKXlGIVAZ21bWpcl1fOwDZLjw/edit#heading=h.yhcm0hj5bq25\n\nCLA signed (for design doc). I'll start submitting code for this as soon as I figure out github/bazel workflow\n\n@keveman @yuanbyu \n", "comments": ["This is worked on in request #2346 \natm, Tensorflow-native ops work in immediate-mode, but missing infrastructure to support Python-only ops, which happen to be the most popular ops\nEstimating another week to get Python-only ops working\n", "The new pull request is https://github.com/tensorflow/tensorflow/pull/2595.  I've pinged @yuanbyu for comment.\n", "New pull request is https://github.com/tensorflow/tensorflow/pull/2747\n", "@yaroslavvb Can this be closed, then?\n", "@keveman this is the main tracking issue though, those other links are pull requests\n", "All the linked PRs seem closed, therefore closing this issue.\nPlease reopen if this is still being worked on.\n"]}, {"number": 2333, "title": "Building from source fail, error in tensorflow/core/lib/core/threadpool.cc", "body": "Hi, \nI try to build tensorflow from source, but I get the ERROR as following:\n\n```\nERROR: /WORK/sysu_sc_ll/tf-rh/tensorflow-/tensorflow/core/BUILD:766:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command /HOME/sysu_sc_ll/WORKSPACE/tf-rh/bazel-/hackbin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/HOME/sysu_sc_ll/WORKSPACE/tf-rh/bazel-/hackbin -B/usr/bin ... (remaining 78 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/core/lib/core/threadpool.cc:82:49: error: expected template-name before '<' token\n struct ThreadPool::Impl : Eigen::ThreadPoolTempl<EigenEnvironment> {\n                                                 ^\ntensorflow/core/lib/core/threadpool.cc:82:49: error: expected '{' before '<' token\ntensorflow/core/lib/core/threadpool.cc:82:49: error: expected unqualified-id before '<' token\ntensorflow/core/lib/core/threadpool.cc:220:1: error: expected '}' at end of input\n }  // namespace tensorflow\n ^\ntensorflow/core/lib/core/threadpool.cc:220:1: error: expected '}' at end of input\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 12.663s, Critical Path: 10.84s\n\n```\n\nOS:  redhat 6.7\ngcc: 4.9.2\nbuilding with cuda\n\nrun command:\nexport EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nI did not change any source file,\nAnd previously, the is another error in the source tensorflow/core/lib/io/record_reader.cc\nwhich claim that the SIZE_MAX is not defined, I solve it by define it as size_t -1. And now this error show up.\n\nCould anyone give me some help here? Thanks.\n", "comments": ["I can't reproduce your compilation error so it'll be hard for me to diagnose the root cause. However, you should be able to work around the problem by editing the tensorflow.bzl file and deleting the  \"-DTENSORFLOW_USE_EIGEN_THREADPOOL\" string in the tf_copts() function.\n", "I had a similar issue (ref https://github.com/tensorflow/tensorflow/issues/2324) ,  \n\nI tried your recommendation.   Now when I run: bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\n\nI get the following errors:\n\nERROR: /home/julialintern/tensorflow/tensorflow/cc/BUILD:14:1: Traceback (most recent call last):\n    File \"/home/julialintern/tensorflow/tensorflow/cc/BUILD\", line 14\n        cc_library(name = \"cc_op_gen_main\", srcs = [\"...\"], <3 more arguments>)\n    File \"/home/julialintern/tensorflow/tensorflow/cc/BUILD\", line 21, in cc_library\n        tf_copts()\n    File \"/home/julialintern/tensorflow/tensorflow/tensorflow.bzl\", line 98, in tf_copts\n        \"-DEIGEN_AVOID_STL_ARRAY\" + if_cuda([\"-DGOOGLE_CUDA=1\"])\n'+' operator applied to incompatible types (string, select of list).\nERROR: /home/julialintern/tensorflow/tensorflow/cc/BUILD:28:1: Traceback (most recent call last):\n    File \"/home/julialintern/tensorflow/tensorflow/cc/BUILD\", line 28\n        tf_gen_op_wrappers_cc(name = \"cc_ops\", op_lib_names = [\"...\", <17 more arguments>], <3 more arguments>)\n    File \"/home/julialintern/tensorflow/tensorflow/tensorflow.bzl\", line 174, in tf_gen_op_wrappers_cc\n        tf_gen_op_wrapper_cc(n, \"ops/\" + n, pkg = pkg)\n    File \"/home/julialintern/tensorflow/tensorflow/tensorflow.bzl\", line 127, in tf_gen_op_wrapper_cc\n        native.cc_binary(name = tool, copts = tf_copts(), lin...\"], <2 more arguments>)\n    File \"/home/julialintern/tensorflow/tensorflow/tensorflow.bzl\", line 129, in native.cc_binary\n        tf_copts()\n    File \"/home/julialintern/tensorflow/tensorflow/tensorflow.bzl\", line 98, in tf_copts\n        \"-DEIGEN_AVOID_STL_ARRAY\" + if_cuda([\"-DGOOGLE_CUDA=1\"])\n'+' operator applied to incompatible types (string, select of list).\nERROR: /home/julialintern/tensorflow/tensorflow/cc/BUILD:61:1: Traceback (most recent call last):\n    File \"/home/julialintern/tensorflow/tensorflow/cc/BUILD\", line 61\n        cc_binary(name = \"tutorials_example_traine...\", <4 more arguments>)\n    File \"/home/julialintern/tensorflow/tensorflow/cc/BUILD\", line 64, in cc_binary\n        tf_copts()\n    File \"/home/julialintern/tensorflow/tensorflow/tensorflow.bzl\", line 98, in tf_copts\n        \"-DEIGEN_AVOID_STL_ARRAY\" + if_cuda([\"-DGOOGLE_CUDA=1\"])\n'+' operator applied to incompatible types (string, select of list).\nERROR: no such target '//tensorflow/cc:tutorials_example_trainer': target 'tutorials_example_trainer' not declared in package 'tensorflow/cc' defined by /home/julialintern/tensorflow/tensorflow/cc/BUILD.\n", "I tried to build on Raspberry Pi and got the same error as @GekTo \n", "Automatically closing due to lack of recent activity. Please re-open when more information becomes available.\n", "I haven't been able to reproduce this compilation error. \n"]}, {"number": 2332, "title": "install_name_tool error when running reduction_ops_test_gpu on OS X", "body": "I ran into this while testing my patch tensorflow/tensorflow#1289 on OS X. At first, I thought that it was caused by my patch but then I found that it also reproduces at HEAD.\n\nI am not entirely sure whether this is a TensorFlow issue (the error seems to indicate that we need to add a `ld` flag) or a Bazel issue in the gcc wrapper.\n\n+cc @lberki @damienmg\n### Environment info\n\nOperating System: Mac OS X 10.11.4\n\nInstalled version of CUDA and cuDNN: Building without GPU support\n\nIf installed from sources, provide the commit hash: d3d1a63a8b8ad99347d95756e88ae0589ed1a9b0\n### Steps to reproduce\n1. Run `bazel test //tensorflow/core/kernels:reduction_ops_test_gpu` on OS X\n### What have you tried?\n1. This does not reproduce on Linux\n### Logs or other output that would be helpful\n\n```\n\u276f\u276f\u276f bazel test //tensorflow/core/kernels:reduction_ops_test_gpu --test_output=errors\nWARNING: /private/var/tmp/_bazel_dzc/50d95479571c8b29601dcbccdd342764/external/re2/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_dzc/50d95479571c8b29601dcbccdd342764/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /Users/dzc/Projects/tensorflow/tensorflow/google/protobuf/BUILD:59:16: in includes attribute of cc_library rule //google/protobuf:protobuf_lite: 'src/' resolves to 'google/protobuf/src' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/dzc/Projects/tensorflow/tensorflow/google/protobuf/BUILD:124:16: in includes attribute of cc_library rule //google/protobuf:protobuf: 'src/' resolves to 'google/protobuf/src' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/dzc/Projects/tensorflow/tensorflow/google/protobuf/BUILD:266:16: in includes attribute of cc_library rule //google/protobuf:protoc_lib: 'src/' resolves to 'google/protobuf/src' not in 'third_party'. This will be an error in the future.\nINFO: Found 1 test target...\nERROR: /Users/dzc/Projects/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1044:1: Linking of rule '//tensorflow/core/kernels:reduction_ops_test_gpu' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -o bazel-out/local-fastbuild/bin/tensorflow/core/kernels/reduction_ops_test_gpu '-Wl,-rpath,$ORIGIN/../../../_solib_darwin/' ... (remaining 53 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nclang: warning: argument unused during compilation: '-pthread'\nerror: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/install_name_tool: changing install names or rpaths can't be redone for: bazel-out/local-fastbuild/bin/tensorflow/core/kernels/reduction_ops_test_gpu (for architecture x86_64) because larger updated load commands do not fit (the program must be relinked, and you may need to use -headerpad or -headerpad_max_install_names)\nTarget //tensorflow/core/kernels:reduction_ops_test_gpu failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 0.623s, Critical Path: 0.49s\n\nExecuted 0 out of 1 test: 1 fails to build.\n```\n", "comments": ["Try \n\n```\nbazel test --linkopt=-headerpad_max_install_names\n```\n\nI have no idea why but this solved the problem for me in the past.\n", "That worked for me. Perhaps we should add a `select()` so that this linkopt gets added when building on OS X?\n\nI have opened PR #2347 to add this.\n"]}, {"number": 2331, "title": "No ability to verify a jpeg before decode_jpeg-ing", "body": "Say we are writing images to a dir and feeding them to a TF queue.  If TF goes to read a JPEG that isn't there for some reason or hasn't finished writing, `decode_jpeg` will fail.  Using something like `os.stat()` will not work on the tensor that `decode_csv` returns.\n\nIt would be great if we could verify a jpeg is our target image size before decoding so TF doesn't shutdown completely.\n", "comments": ["Can you be more specific about the feature you're looking for?  Do you want an op that checks whether a file exists?\n", "Automatically closing due to lack of recent activity. Please reopen if it is still an issue.\n"]}, {"number": 2330, "title": "softmax_classifier input argument not right?", "body": "The softmax_classifier in losses_ops.py file seems not right to me. I haven't done any test yet, but just look at the the input argument definition, the tensor_in and weight are not compatible for matrix multiplication. \n====== begin ============\ntensor_in: Input tensor, [batch_size, feature_size], features.\n    labels: Tensor, [batch_size, n_classes], labels of the output classes.\n    weights: Tensor, [batch_size, feature_size], linear transformation\n      matrix.\n    biases: Tensor, [batch_size], biases.\n======== end ==============\nTheir inner dimensions do not match for matmul in xw_plus_b function. \n\nAlso, since it is done in a batch, isn't it we should use batch_matmul?\n", "comments": ["It would be great to see a test on this one if you're able to put something in code. Thanks!\n", "I'll close for now since this isn't enough information to reproduce without some example code or perhaps the details of the error.  Please comment and we can reopen if there is more information available.\n"]}, {"number": 2329, "title": "Additional instructions on how to fix missing submodules", "body": "This adds a better hint on how to fix a repository that was cloned without `--recurse-submodules`.\n\nCurrently, it recommends to run `git clone` again, which may be a somewhat involved operation if the user has made modifications, pulled changesets from other branches etc. before their first attempt to compile.\n#### Output:\n##### Old:\n\n`ERROR: It appears that the required submodule google/protobuf is not available in this TensorFlow git clone.\nPlease be sure to use the --recurse-submodules flag when performing git clone of TensorFlow.`\n##### New:\n\n`ERROR: It appears that the required submodule google/protobuf is not available in this TensorFlow git clone.\nPlease re-run your git clone of Tensorflow with the --recurse-submodules flag or try git pull && git submodule init && git submodule update && git submodule status`\n", "comments": ["Can one of the admins verify this patch?\n", "would \"git submodule update --init\" be sufficient ?\n", "Tried \"git submodule update --init --recursive\" and it works. \n", "Okay I think git submodule update --init --recursive might be simpler then.  Feel free to update the PR with that\n", "Now that we don't use submodules for protobuf, we can actually close this, but thanks for the doc suggestion!\n"]}, {"number": 2328, "title": "Can't mix tensors and python objects", "body": "I'd like to do things such as the following:\n\n``` python\n>>> tf.convert_to_tensor([tf.constant(1), 2])\n>>> tf.slice(..., [0, 1, some_tensor], ...)\n```\n\nCurrently both of these raise exceptions. The alternative is to use a combination of `tf.expand_dims` and `tf.concat`, which is a bit unwieldy.\n", "comments": ["Hi there,\n\nSeveral people have requested this recently, and I think it's time we implemented this. I'll take on the feature request.\n\nDerek.\n", "Just pulling in the conversation from #2318, since it's really more appropriate here.\n\nThe downside of this feature is type errors can become harder to hunt down. For instance, having a dimension typo can become a logical error, especially if it's compounded with broadcasting.\n\nWhile manually converting heterogeneous structures may seem tedious, it's also clearer what's going on. It might seem that `tf.convert_to_tensor([tf.constant(1), 2])` is already pretty clear, but if you add a couple layers of variables in between and end up with `tf.convert_to_tensor([x,y])`, a type error is pretty easy to lose.\n\nMy feeling is \"explicit is better than implicit\" here. Perhaps there's some compromise where we don't do recursive auto-coercion for all things that can take tensors? Maybe just allow a single explicit method to handle this behavior? `convert_to_tensor` _would_ seem like a decent candidate, so the above code would become:\n\n```\ntf.convert_to_tensor([tf.constant(1), 2]) # works, this is the explicit coercion method\ntf.slice(..., tf.convert_to_tensor([0, 1, some_tensor]), ...)\n```\n", "@rdadolf I don't see why type errors become logical errors - they'll just point to the nested object instead of the root. In any case, the current behavior allows both python objects and tensors, just not mixed - I assume this is done with an implicit call to `convert_to_tensor`, so any improvements to `convert_to_tensor` should carry over to all its use-sites.\n", "I am personally not a fan of mixed typing like this. I see tensorflow as a programming language with a python wrapper, rather than python module that can interact nicely with other python modules. \n", "> I don't see why type errors become logical errors - they'll just point to the nested object instead of the root.\n\n@vladfi1 The type errors I'm talking about are dimension mismatch due to extra or insufficient nesting. Consider the following exceedingly contrived example:\n\n```\nx = tf.constant([1,2])\ny = tf.constant([1,2])\nx = tf.constant(x)\ny = tf.constant([y]) # oops\nw = tf.constant([[1,2,],[3,4]])\nprint tf.Session().run( tf.add(w, tf.reduce_max(x,0)) )\nprint tf.Session().run( tf.add(w, tf.reduce_max(y,0)) )\n```\n\nCurrently, TF gives you a type error; in the proposed extension, this silently gives you the wrong answer (i.e., it becomes a logical error). This is obviously not real code, but imagine having an n-dim tensor accidentally becoming an n+1 dim tensor whilst threading through a larger body of code, and it can easily end up costing you a day of hunting down the bug.\n\nI'm not suggesting that we put every TF programmer into a padded room to protect them from themselves, but I feel that libraries which try to make everything convertible to everything else end up a mess to debug. This kind of error protection is the reason behind type systems. We should take advantage of it!\n\n>  In any case, the current behavior allows both python objects and tensors, just not mixed - I assume this is done with an implicit call to convert_to_tensor, so any improvements to convert_to_tensor should carry over to all its use-sites.\n\nThis isn't actually the case. The error stems from [here](https://github.com/tensorflow/tensorflow/blob/17dcc5a176d152caec570452d28fb94920cceb8c/tensorflow/python/framework/tensor_util.py#L273), which is used by [this](https://github.com/tensorflow/tensorflow/blob/17dcc5a176d152caec570452d28fb94920cceb8c/tensorflow/python/framework/tensor_util.py#L284), which is used directly by ops (e.g., [here](https://github.com/tensorflow/tensorflow/blob/17dcc5a176d152caec570452d28fb94920cceb8c/tensorflow/python/ops/constant_op.py#L162)). Then there is a whole separate set of conversion routines in the [tensor conversion registry](https://github.com/tensorflow/tensorflow/blob/82ff4cd8b0d541ede107d34d8eecc769c91dda11/tensorflow/python/framework/ops.py#L520), which is [also used](https://github.com/tensorflow/tensorflow/blob/82ff4cd8b0d541ede107d34d8eecc769c91dda11/tensorflow/python/framework/ops.py#L569) by `convert_to_tensor`. And as mentioned in the other thread, this is also separate from `tf.Session().run()`'s feed dict conversion registry.\n"]}, {"number": 2327, "title": "Numerical Problem in tf.nn.softmax_cross_entropy_with_logits ", "body": "The function `tf.nn.softmax_cross_entropy_with_logits(logits, labels)` is numerical unstable when used in weak labelling scenarios (i.e. there are no labels for some rows of the labels). \n\nThe instability does not occure, when using `tf.nn.softmax` followed by a simple cross_entropy implementation, i.e.:\n\n```\nepsilon = tf.constant(value=0.00001, shape=shape)\nlogits = logits + epsilon\nsoftmax = tf.nn.softmax(logits)\ncross_entropy = -tf.reduce_sum(labels * tf.log(softmax),\n                                                 reduction_indices=[1])\n```\n\nI therefore conclude, that this is a bug occurring in the softmax layer. I can not give a a minimal example, as it only occurs in bigger models. Also, I did only observe the Bug when using `weakly labeled data`, that is, when cases without labels occur. This case however is explicitly mentioned in the documentation. \n\n`labels: Each row labels[i] must be a valid probability distribution or all zeros. If all zeros, the corresponding loss will be 0, regardless of the contents of logits[i].`\n\nIf the unstability occurs, tf.nn.softmax_cross_entropy_with_logits() produces high gradients, causing the training process to diverge. This usually happens after about 450 iterations. Changing the learning rate will not avoid this issue.\n### Environment info\n\nOperating System: Linux\n\nInstalled version of CUDA and cuDNN: CUDA 7.5; CUDNN 7.0\n\nInstalled from Pip: \n1. Which pip package you installed. `https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl`\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\". 0.8.0\n### Steps to reproduce\n1. Use loss function provided below\n2. Use data containing about 10% unlabeled entries.\n### What have you tried?\n1. Implementing tf.nn.softmax_cross_entropy_with_logits() myselfe. It works fine.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nThe error occurs, when estimating the loss like below.\n\n```\ndef loss(hypes, logits, labels):\n    \"\"\"Calculates the loss from the logits and the labels.\n    Args:\n      logits: Logits tensor, float - [batch_size, 2].\n      labels: Labels tensor, int32 - [batch_size, 2].\n    Returns:\n      loss: Loss tensor of type float.\n    \"\"\"\n\n    with tf.name_scope('loss'):\n        logits = tf.reshape(logits, (-1, 2))\n        labels = tf.to_float(tf.reshape(labels, (-1, 2)))\n        shape = [logits.get_shape()[0], 2]\n        epsilon = tf.constant(value=hypes['solver']['epsilon'], shape=shape)\n        logits = logits + epsilon\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n            logits, labels, name='xentropy')\n\n        cross_entropy_mean = tf.reduce_mean(\n            cross_entropy, name='xentropy_mean')\n        tf.add_to_collection('losses', cross_entropy_mean)\n\n        loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n    return loss\n```\n\nWhen the error occurs my training output looks like this:\n\n```\n2016-05-10 20:20:56,256 root INFO Step 0/10000: loss = 11.98 ( 0.013 sec (per Batch); 74.5 examples/sec)\n2016-05-10 20:21:01,804 root INFO Step 10/10000: loss = 11.47 ( 0.028 sec (per Batch); 35.1 examples/sec)\n2016-05-10 20:21:07,565 root INFO Step 20/10000: loss = 10.82 ( 0.029 sec (per Batch); 34.7 examples/sec)\n2016-05-10 20:21:12,554 root INFO Step 30/10000: loss = 10.32 ( 0.028 sec (per Batch); 35.5 examples/sec)\n2016-05-10 20:21:17,317 root INFO Step 40/10000: loss = 9.90 ( 0.028 sec (per Batch); 35.5 examples/sec)\n2016-05-10 20:21:22,387 root INFO Step 50/10000: loss = 9.50 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:21:27,209 root INFO Step 60/10000: loss = 9.27 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:21:32,134 root INFO Step 70/10000: loss = 8.75 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:21:37,198 root INFO Step 80/10000: loss = 8.34 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:21:42,074 root INFO Step 90/10000: loss = 8.10 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:21:49,483 root INFO Doing Evaluate with Training Data.\n2016-05-10 20:22:09,883 root INFO Data: train  Num examples:  200  Num correct:  168.3478 Precision @ 1:  0.8417 \n2016-05-10 20:22:09,883 root INFO Doing Evaluation with Testing Data.\n2016-05-10 20:22:15,032 root INFO Data: val  Num examples:  50  Num correct:  41.3079 Precision @ 1:  0.8262 \n2016-05-10 20:22:15,323 root INFO Step 100/10000: loss = 8.03 ( 0.003 sec (per Batch); 344.2 examples/sec)\n2016-05-10 20:22:20,180 root INFO Step 110/10000: loss = 7.56 ( 0.028 sec (per Batch); 35.2 examples/sec)\n2016-05-10 20:22:25,208 root INFO Step 120/10000: loss = 7.13 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:22:30,053 root INFO Step 130/10000: loss = 6.93 ( 0.028 sec (per Batch); 35.5 examples/sec)\n2016-05-10 20:22:34,883 root INFO Step 140/10000: loss = 6.65 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:22:39,765 root INFO Step 150/10000: loss = 6.34 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:22:44,741 root INFO Step 160/10000: loss = 6.23 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:22:49,867 root INFO Step 170/10000: loss = 5.88 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:22:54,890 root INFO Step 180/10000: loss = 5.70 ( 0.028 sec (per Batch); 35.2 examples/sec)\n2016-05-10 20:22:59,822 root INFO Step 190/10000: loss = 5.61 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:23:07,190 root INFO Doing Evaluate with Training Data.\n2016-05-10 20:23:27,775 root INFO Data: train  Num examples:  200  Num correct:  167.6011 Precision @ 1:  0.8380 \n2016-05-10 20:23:27,776 root INFO Doing Evaluation with Testing Data.\n2016-05-10 20:23:32,933 root INFO Data: val  Num examples:  50  Num correct:  42.5874 Precision @ 1:  0.8517 \n2016-05-10 20:23:33,231 root INFO Step 200/10000: loss = 5.41 ( 0.003 sec (per Batch); 336.0 examples/sec)\n2016-05-10 20:23:38,108 root INFO Step 210/10000: loss = 5.36 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:23:43,053 root INFO Step 220/10000: loss = 5.05 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:23:48,021 root INFO Step 230/10000: loss = 4.96 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:23:52,982 root INFO Step 240/10000: loss = 4.70 ( 0.028 sec (per Batch); 35.2 examples/sec)\n2016-05-10 20:23:58,049 root INFO Step 250/10000: loss = 4.55 ( 0.028 sec (per Batch); 35.5 examples/sec)\n2016-05-10 20:24:03,004 root INFO Step 260/10000: loss = 4.43 ( 0.028 sec (per Batch); 35.1 examples/sec)\n2016-05-10 20:24:07,973 root INFO Step 270/10000: loss = 4.32 ( 0.029 sec (per Batch); 35.0 examples/sec)\n2016-05-10 20:24:12,886 root INFO Step 280/10000: loss = 4.14 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:24:17,765 root INFO Step 290/10000: loss = 4.80 ( 0.028 sec (per Batch); 35.3 examples/sec)\n2016-05-10 20:24:25,209 root INFO Doing Evaluate with Training Data.\n2016-05-10 20:24:45,899 root INFO Data: train  Num examples:  200  Num correct:  156.8744 Precision @ 1:  0.7844 \n2016-05-10 20:24:45,900 root INFO Doing Evaluation with Testing Data.\n2016-05-10 20:24:51,048 root INFO Data: val  Num examples:  50  Num correct:  40.3362 Precision @ 1:  0.8067 \n2016-05-10 20:24:51,349 root INFO Step 300/10000: loss = 6.08 ( 0.003 sec (per Batch); 333.3 examples/sec)\n2016-05-10 20:24:56,176 root INFO Step 310/10000: loss = 136.59 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:25:01,130 root INFO Step 320/10000: loss = 2029.89 ( 0.028 sec (per Batch); 35.6 examples/sec)\n2016-05-10 20:25:05,964 root INFO Step 330/10000: loss = 75585.02 ( 0.028 sec (per Batch); 35.6 examples/sec)\n2016-05-10 20:25:10,901 root INFO Step 340/10000: loss = 7492798.00 ( 0.028 sec (per Batch); 35.5 examples/sec)\n2016-05-10 20:25:15,863 root INFO Step 350/10000: loss = 50291024.00 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:25:20,790 root INFO Step 360/10000: loss = 1344814592.00 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:25:25,716 root INFO Step 370/10000: loss = 8610205696.00 ( 0.028 sec (per Batch); 35.4 examples/sec)\n2016-05-10 20:25:30,749 root INFO Step 380/10000: loss = 344546377728.00 ( 0.028 sec (per Batch); 35.5 examples/sec)\n2016-05-10 20:25:35,588 root INFO Step 390/10000: loss = 2766554529792.00 ( 0.028 sec (per Batch); 35.5 examples/sec)\n2016-05-10 20:25:43,335 root INFO Doing Evaluate with Training Data.\n```\n\nWhen training longer, the loss will eventually become `Nan`. The error does not occure, when using the following `Loss` Code:\n\n```\ndef loss(hypes, logits, labels):\n    \"\"\"Calculates the loss from the logits and the labels.\n\n    Args:\n      logits: Logits tensor, float - [batch_size, 2].\n      labels: Labels tensor, int32 - [batch_size, 2].\n\n    Returns:\n      loss: Loss tensor of type float.\n    \"\"\"\n    with tf.name_scope('loss'):\n        logits = tf.reshape(logits, (-1, 2))\n        shape = [logits.get_shape()[0], 2]\n        epsilon = tf.constant(value=hypes['solver']['epsilon'], shape=shape)\n        logits = logits + epsilon\n        labels = tf.to_float(tf.reshape(labels, (-1, 2)))\n\n        softmax = tf.nn.softmax(logits)\n        cross_entropy = -tf.reduce_sum(labels * tf.log(softmax),\n                                       reduction_indices=[1])\n\n        cross_entropy_mean = tf.reduce_mean(cross_entropy,\n                                            name='xentropy_mean')\n        tf.add_to_collection('losses', cross_entropy_mean)\n\n        loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n    return loss\n```\n", "comments": ["As stated in the documentation, we require a valid probability distribution.  Unfortunately, for speed reasons, we don't check this precondition.  Therefore, this is expected behavior.  Implementing your own loss in terms of other ops is the right fix.\n", "@girving no, the documentation reads: `labels: Each row labels[i] must be a valid probability distribution or all zeros.` My data does forfill this condition. (i.e. in cases where a valid probability distribution is not given, all are zero). However, changing the documentation to `labels: Each row labels[i] must be a valid probability distribution.` might be a solution.\n\nEdit: Sry, looked at old documentation. The documentation already is changed in current master.\n"]}, {"number": 2326, "title": "tf.QueueBase.close() docs", "body": "I think the docs should specify that a dequeue might raises a OutOfRange error.\nRight?\n", "comments": ["Looking at the `QueueBase` docs, they leave a lot to be desired. I've sent out a change internally, and it should appear soon....\n"]}, {"number": 2325, "title": "Random numbers and tf.less_equal", "body": "### Environment info\n\nOperating System: Ubuntu 16.04 LTS 64-Bit\nInstalled version of CUDA and cuDNN: none\nTensorflow Version: 0.8.0\n### Steps to reproduce\n1. When comparing a random variable with a tensor using the tf.less_equal operation, the output result is wrong. I can reproduce the error with the minimal example code shown below. When I use a fixed number rather than a random variable, the output result is correct.\n\n`\nimport tensorflow as tf\nimport numpy as np\n\nx=[0,0.3333,0.6666,1] # matrix that I want to compare against another number\nx = tf.convert_to_tensor(np.reshape(x,[2,2]), dtype=tf.float32) # turn into tensor\ny = tf.random_uniform([1,1], minval=0, maxval=1, dtype=tf.float32, seed=12) # random number\n# y = tf.convert_to_tensor(0.63615251, dtype=tf.float32) **# uncomment to use fixed number instead**\n\n**z = tf.less_equal(x,y)**\n\ninit_op = tf.initialize_all_variables()  \nwith tf.Session() as sess:\n  sess.run(init_op)\n  xe = x.eval()\n  ye = y.eval()\n  ze = z.eval()\n\n  print('2-by-2 matrix x:')\n  print(xe)\n  print(' ')\n  print('random value y:')\n  print(ye)\n  print(' ')\n  print('output result of x <= y:')\n  print(ze)\n`\n### I tried:\n1. I tried using tf.tile to make \"y\" the same size as \"x\"\n2. I checked with fixed numbers rather than random numbers; then tf.less_equal works just fine\n### Console output of above minimal example\n\n2-by-2 matrix x:\n[[ 0.          0.33329999]\n [ 0.66659999  1.        ]]\n\nrandom value y:\n[[ 0.63615251]]\n\noutput result of x <= y:\n[[ True False]\n [False False]]\n### comment to above example:\n\n0.333... <= 0.636... should be True, not False\n", "comments": ["When you evaluate z then it will re-evaluate y at that time and produce new random numbers. To evaluate y and z simultaneously then you should do`ye, ze = session.run([y, z])`\n", "@ajaech is absolutely right. This is intended behavior: `tf.random_uniform()` and the other random ops produce new random numbers each time they execute, and each call to `{y,z}.eval()` will execute the random op another time.\n"]}, {"number": 2324, "title": "Issue with tensorflow/core/lib/core/threadpool.cc", "body": "Operating System:  Ubuntu 14.04.4\nCUDA version: 7.5\ncuNN: 4.0.7\n\nInstalled from sources:\ncommit 5681406\n\nrunning : bazel build -c opt //tensorflow/cc:tutorials_example_trainer --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone\n\nNote : that i have configured my WORKSPACE file as per : https://github.com/bazelbuild/bazel/issues/623#issuecomment-158151936: \n\n(made the following corrections:) \ndownload jpeg.BUILD.txt, png.BUILD.txt, and WORKSPACE.txt from @srsaharoy 's message #623 (comment) . (the 2nd post with attachments) and place these files in the tensorflow lib without the .txt extension instead of the existing files (in my case ~/git/tensorflow/tensorflow).\ncreate folder with external source files: ~/git/tensorflow/fix/files/re2 ~/git/tensorflow/fix/files/jpeg-9a/jpeg-9a ~/git/tensorflow/fix/files/gemmlowp ~/git/tensorflow/fix/files/libpng-1.2.53/libpng-1.2.53 ~/git/tensorflow/fix/files/six-1.10.0 Note the dir-in-dir for jpeg-9a and libpng-1.2.53. This is necessary.\nchange paths in WORKSPACE file to match the location of the aux source files\n .... \n\nNow I am just stuck with the following error: \n\nERROR: /home/julialintern/tensorflow/tensorflow/core/BUILD:756:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command \n  (cd /home/julialintern/.cache/bazel/_bazel_julialintern/1a7b4f00b1b7d4c4a3ca618f554c7ad8/tensorflow && \\\n  exec env - \\\n    PATH=/home/julialintern/torch/install/bin:/home/julialintern/bin:/usr/local/cuda/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/julialintern/bin \\\n    TMPDIR=/tmp/user/1001 \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++0x' -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/host/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/host/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/host/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/host/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/host/genfiles/external/png_archive/libpng-1.2.53 -isystem third_party/eigen3 -isystem bazel-out/host/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-50812b426b7c -isystem bazel-out/host/genfiles/external/eigen_archive/eigen-eigen-50812b426b7c -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -DTENSORFLOW_USE_EIGEN_THREADPOOL -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/core/threadpool.o' -MD -MF bazel-out/host/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/core/threadpool.d -c tensorflow/core/lib/core/threadpool.cc -o bazel-out/host/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/core/threadpool.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/core/lib/core/threadpool.cc:83:49: error: expected template-name before '<' token\n struct ThreadPool::Impl : Eigen::ThreadPoolTempl<EigenEnvironment> {\n                                                 ^\ntensorflow/core/lib/core/threadpool.cc:83:49: error: expected '{' before '<' token\ntensorflow/core/lib/core/threadpool.cc:83:49: error: expected unqualified-id before '<' token\ntensorflow/core/lib/core/threadpool.cc:221:1: error: expected '}' at end of input\n }  // namespace tensorflow\n ^\ntensorflow/core/lib/core/threadpool.cc:221:1: error: expected '}' at end of input\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n", "comments": ["This looks like a duplicate of https://github.com/tensorflow/tensorflow/issues/2333\n"]}, {"number": 2323, "title": "the process wil be so slow when reading batch data from csv files in tensorflow", "body": "def read_data(filename):\n\n```\n    filename_queue = tf.train.string_input_producer([filename])\n    reader = tf.TextLineReader()\n    key, value = reader.read(filename_queue)\n    record_defaults = [[1.0 for col in range(1)] for row in range(280)]\n    record_defaults[279][0] = 1\n    a = tf.decode_csv(value, record_defaults=record_defaults)\n    data = tf.pack(a[0:278])\n    label = a[-1]\n    min_after_dequeue = 10000\n    capacity = min_after_dequeue + 3 * batch_size\n    data_batch, label_batch = tf.train.shuffle_batch([data, label], batch_size=batch_size, capacity=capacity,min_after_dequeue=min_after_dequeue)\n    return data_batch, label_batch\n```\n\ndef main(argv=None):  \n\n```\n    data,label = read_data(FLAGS.train_file)\n\n    tf.initialize_all_variables()\n    with tf.Session() as sess:\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord)\n            example, label = sess.run([data, label])\n            print (example)\n```\n\nHere is my code above ,why it take 10 minutes to print example data?How can I optimise my code? I have only set batchsize is 2\n", "comments": ["I have assign min_after_dequeue  to 100,and now it can print the result immediately.\nNow I increase batchsize to 20000,and it will take so longer.\nIn caffe,it only take 198ms to get the data.\nIs it other issue that i haven't noticed?\n I0512 11:17:32.966964   787 caffe.cpp:280] ctr_feature_data     forward: 198.561 ms.\n", "It is because tf.train.shuffle_batch blocks dequeue operation until the queue size reaches to min_after_dequeue. This is intended for better mixing of elements in queue to shuffle data. \n", "And why when I increase batchsize to 20000,and it will take much more to print .Because of larger data?Can it possible to improve the performance when modify the parameters?\n@beopst \n", "Yes, it is because of batch_size. In this case, the queue should still block until it has a sufficient amount of elements for the first single batch. \n\ntf.train.shuffle_batch needs some time only for the first dequeue operation. Once it has elements more than min_after_queue, it consumes much little time since tf.train.shuffle_batch runs in a separate thread. You can see if you call `example, label = sess.run([data, label])` in a loop.\n", "If you want training to start sooner, you should reduce the value of `min_after_dequeue`. This parameter is here to ensure good mixing, so you may find that the loss of randomness leads to poorer accuracy, but this depends on your model.\n", "yes,i have called example, label = sess.run([data, label]) in a loop .But it still take much time when batchsize is set 20000\u3002\nSo reduce batchsize is only one way? it seems not best ways. @beopst \n", "yes,i have called example, label = sess.run([data, label]) in a loop .But it still take much time when batchsize is set 20000\u3002\nSo reduce batchsize is only one way? it seems not best ways. \n@mrry \n"]}]