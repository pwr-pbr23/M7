[{"number": 23456, "title": "Tensorflow softmax is not updating, docu doesn't help?", "body": "Please be so kind to delete this issue for me, thank you in advance.", "comments": ["This should be a stackoverflow question.", "Please be so kind to delete this issue for me, thank you in advance."]}, {"number": 23455, "title": "Excessive MKL memory consumption with variable sized tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Both source and binary\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): gcc 5.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nDuring inference of variable sized examples with Tensorflow MKL system memory consumption gradually increases.\r\nIf a server application is running long enough to go through a lot of examples of same rank but different shapes it would eventually run out of system memory.\r\nAfter further investigation setting the [TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/util/mkl_util.h#L2041) to either 0 or 1 does not help.\r\nMost MKL primitive factories ignore TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE and even some that know about it may choose to ignore it based on other heuristics like [batches smaller than 32](https://github.com/tensorflow/tensorflow/commit/86d9ce130c5691cdba16024f7cc7987082acd294#diff-192dfeafbdd684934bdb0dfa8983674a) for example (I am not sure this is intended behavior).\r\nThese [\"do_not_cache\" heuristics](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/mkl_conv_ops.cc#L902\r\n) on the other hand don't seem to follow the described logic in the comments above replacing \"or\" for \"and\".\r\n\r\n**Describe the expected behavior**\r\nThere should be a way to disable MKL primitive memory reuse globally with an environment variable be it TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE or better yet something else.\r\nPrimitive memory reuse does not play nice with unknown tensor sizes as most of the time there are no cache hits and allocated memory is simply piled up in the cache.\r\n\r\n**Code to reproduce the issue**\r\nI've attached a small example.\r\n[min-leak-example.txt](https://github.com/tensorflow/tensorflow/files/2542973/min-leak-example.txt)\r\n\r\n**Other info / logs**\r\nI've attached Valgrind results of the example running on TF version 1.11.0 with and without MKL.\r\n[min-leak-example-mkl.txt](https://github.com/tensorflow/tensorflow/files/2542977/min-leak-example-mkl.txt)\r\n[min-leak-example-no-mkl.txt](https://github.com/tensorflow/tensorflow/files/2542978/min-leak-example-no-mkl.txt)\r\n\r\nI've been working on a solution I can propose.\r\nShould you find it reasonable I've extended the solution to full refactoring of the MKL primitive factories.", "comments": ["Just for trouble-shooting purpose, please experiment by hard-coding \r\n do_not_cache = true;     (comment out existing logic). \r\n\r\nMy initial feeling is that more and more MKL primitive caching due to new op input dimensions (thus new keys), so that the memory usage is going higher and higher (that is, the feeling of leakage). \r\n\r\nOf cause, this is just a guess. To confirm it, do the hard-coding experiment on all do_not_cache \r\noccurrences (in files of mkl*.cc in folder tensorflow/core/kernels). \r\n\r\nThanks!", "Your intuition is correct! I can confirm that setting do_not_cache = true and manually deleting the returned MklPrimitive pointer at the end of the scope works perfectly, in fact that's what I did for TF 1.9.\r\n\r\nOtherwise the problem indeed looks like a memory leak but in fact a pointer to each allocated primitive is stored in static thread local hash maps. Unfortunately these pointers never get deleted.\r\n\r\nThank you for the suggestion and the quick response!", "> Your intuition is correct! I can confirm that setting do_not_cache = true and manually deleting the returned MklPrimitive pointer at the end of the scope works perfectly, in fact that's what I did for TF 1.9.\r\n> \r\n> Otherwise the problem indeed looks like a memory leak but in fact a pointer to each allocated primitive is stored in static thread local hash maps. Unfortunately these pointers never get deleted.\r\n> \r\n> Thank you for the suggestion and the quick response!\r\n\r\nHi, \r\n     i've tried ur idea of making do_not_cache=true, but memory leak still exists for me but decreased by 40%, can u plz give me ur tf-mkl wheel or ur repo", "The fork I am working on: [https://github.com/hyperscience/tensorflow](https://github.com/hyperscience/tensorflow)\r\nYou can start with this branch: [https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix](https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix)\r\nThis is further refactoring on the above branch: [https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix-refactor](https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix-refactor)\r\nWe have a patched r1.9 that works for us. The long term fixes are cherry picked in the downstream r1.11 and r1.12 branches.\r\nDo not hesitate to contact me should you need any clarification.", "The problem is that there are a lot of primitive reuse cases and not all factories even have an option to turn caching off. In TF 1.9 turning the cache off for the convolution should be enough but in later versions the caching is becoming widely adopted in other operations as well and very few can be turned off. My proposed solution introduces a new environment variable with a default value that preserves current behaviour for backwards compatibility. It changes all factories so that they take this new variable into consideration when deciding to cache.", "> The problem is that there are a lot of primitive reuse cases and not all factories even have an option to turn caching off. In TF 1.9 turning the cache off for the convolution should be enough but in later versions the caching is becoming widely adopted in other operations as well and very few can be turned off. My proposed solution introduces a new environment variable with a default value that preserves current behaviour for backwards compatibility. It changes all factories so that they take this new variable into consideration when deciding to cache.\r\n\r\nThat's a nice idea because for training caching will help but for deploying caching will make the system's memory bloated which we currently experience.Thank You very much for ur response.", "> The problem is that there are a lot of primitive reuse cases and not all factories even have an option to turn caching off. In TF 1.9 turning the cache off for the convolution should be enough but in later versions the caching is becoming widely adopted in other operations as well and very few can be turned off. My proposed solution introduces a new environment variable with a default value that preserves current behaviour for backwards compatibility. It changes all factories so that they take this new variable into consideration when deciding to cache.\r\n\r\nWe tried ur branch r1.9 still we have the memory leak but it is reduced to **1/4th** previously we had and https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix-refactor branch leaks more memory as normal tensorflow-mkl  ", "My r1.9 branch does not use the proposed solution. It is a quick and dirty patch that solved our problems back then. I have explicitly checked for other possible leaks of the same kind and could not locate such at Tensorflow version 1.9. \r\n\r\nThe proposed solution (with refactoring) resides in the branch you've pointed out but I do not recommend building from it as it is branched at an arbitrary point from master. The solution is cherry picked in r1.11 and r1.12 that are supposed to be stable release branches.\r\n\r\nAnd yes more leaks are happening at an increasing number of locations where the MklPrimitiveFactory is introduced in later Tensorflow versions. My branch is not the reason for those leaks and certainly cannot cause more leaking at the said locations compared to the same Tensorflow version.\r\n\r\nAnd as I said the fix is backwards compatible so by default it does nothing unless you set TF_MKL_REUSE_PRIMITIVE_MEMORY to 0.\r\n\r\nI hope someone looks into the problem soon. The refactored solution is already hard to rebase onto master.", "Okay that's why we settled with r1.9 since you said that's the branch where they started caching.Thank you ", "> My r1.9 branch does not use the proposed solution. It is a quick and dirty patch that solved our problems back then. I have explicitly checked for other possible leaks of the same kind and could not locate such at Tensorflow version 1.9.\r\n> \r\n> The proposed solution (with refactoring) resides in the branch you've pointed out but I do not recommend building from it as it is branched at an arbitrary point from master. The solution is cherry picked in r1.11 and r1.12 that are supposed to be stable release branches.\r\n> \r\n> And yes more leaks are happening at an increasing number of locations where the MklPrimitiveFactory is introduced in later Tensorflow versions. My branch is not the reason for those leaks and certainly cannot cause more leaking at the said locations compared to the same Tensorflow version.\r\n> \r\n> And as I said the fix is backwards compatible so by default it does nothing unless you set TF_MKL_REUSE_PRIMITIVE_MEMORY to 0.\r\n> \r\n> I hope someone looks into the problem soon. The refactored solution is already hard to rebase onto master.\r\n\r\nHi mavrov, I am currently having the same problem with variable sized inputs and I would like to try out your solution. If I understood correctly, one possible solution is build from branch https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix-refactor. However, you recommend not building from it. Then, do you recommend merging the aforementioned branch on r1.12? Do I need to set TF_MKL_REUSE_PRIMITIVE_MEMORY to 0 or this environment variable is not yet implemented? Thank you in advance.\r\n\r\nI hope this is fixed too, since it is quite troublesome for object localization tasks, where input images do not have a fixed size.", "You can use the fix branch:\r\n[https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix](https://github.com/hyperscience/tensorflow/tree/ivan/mkl-variable-input-size-fix) and merge with r1.12, yes.\r\nThe fix+refactor branch causes too much conflicts.\r\nYes, you need to set TF_MKL_REUSE_PRIMITIVE_MEMORY to 0 for the fix to kick in. No worries - it is implemented in the fix branch.", "Hi,\r\n\r\nPublic PR has been created here\r\nhttps://github.com/tensorflow/tensorflow/pull/24482\r\n\r\nHopefully it will be approved and merged soon. ", "we are still monitoring this PR. not approved yet", "@TensorFlow-MKL \r\nI am facing a similar issue. Have detailed the issue at: https://stackoverflow.com/questions/56639787/how-to-fix-memory-leak-with-variable-tensors-in-tensorflow-mkl-library\r\n\r\nAny further updates regarding the same?", "I found out that tensorflow 1.14.1 has code for changing the LRU_cache size in tensorflow/core/util/mkl_util.h.\r\nI tried to build tf using that, but I wanted to know if the same change can be made in the 1.13.1 Some computations in 1.14 are making my program slower and I would like to use the 1.13 version", "@shahsahilj  Can you retry the TF 1.14 that was released yesterday and measure perf again?  Some commits made yesterday will address performance degradation. \r\nIt should be possible to back port the fix for LRU cache to 1.13.1 if  you need to do that.", "@agramesh1 Okay, I will recompile tf 1.14. How do I check which instruction sets I should compile it for?", "@shahsahilj we have a description of building from source in our install guide: https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide\r\n", "Thank you for that. I am going to try and compile using those configs", "Sorry for the delayed response but the latest build of TF did fix those issues. Thanks!", "Please close this issue if fixed", "Closing this issue since its resolved. Feel free to reopen if the problem persists. Thanks!", "I encounter this issue when running a DeepLab V3+ inference job with variable input size such as 192 x 192 and small batch size of 8 etc. I tried setting the environ variable TF_MKL_REUSE_PRIMITIVE_MEMORY = 0 as well as setting MKL_DISABLE_FAST_MM = 1 as per the guide over [here](https://software.intel.com/content/www/us/en/develop/documentation/mkl-linux-developer-guide/top/managing-performance-and-memory/using-memory-functions/avoiding-memory-leaks-in-intel-mkl.html]) but it does not seems to have any effect of solving the memory leak issue. When I load a model of input size 512 x 512 and batch size of 8, the inference goes smoothly with no error. Any suggestion?\r\n\r\nSystem information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.6.10\r\n\r\n", "@alankongfq We will check it.\r\nQuestion: \r\n1. Could you provide the cmd to install the Tensorflow 1.15.0?\r\n2. How do you check the memory leak?\r\n\r\nThank you!", "@NeoZhangJianyu  I used to write a script to run Conv2D op with different input sizes (2500 of them, say 32x32, 128x128, etc), and then repeatedly run it. I used \"htop\" to monitor memory usage. This script helped to fix one customer issue which was related to MKL DNN primitive caching. ", "@alankongfq Could you provide more detail to reproduce this issue?\r\n", "@NeoZhangJianyu \r\n1. I had installed Tensorflow 1.15.0 using pip install\r\n2. I used htop or system monitor to observe the memory usage.\r\nI had managed to partially solve my problem as I check with my colleague who trained the model and he had mentioned that he trained in Tensorflow 2.0 so I did a switch and upgrade to the Tensorflow version and memory issue seems to not be there anymore. \r\n\r\nHowever, I had encounter another issue where the prediction was ok in GPU but not in CPU with MKL DNN libraries. I had since switch to running my inference on GPU as a workaround for the time being. In this case, it might be a model related issue and would be hard to pinpoint if its due to the MKL DNN libraries or the type of operation used in the model architecture.\r\n\r\nI would not be able to provide any codes as its work related. ", "@alankongfq Thank you!\r\nI have reproduced the memory leak issue in TF 1.15.\r\nI have feedback to dev team.\r\n\r\nI will try it on TF 2.x\r\n\r\nFor the prediction issue, could you try it on TF-eigen release? It don't use MKLDNN lib.\r\nInstall it: \r\n\r\n`conda install tensorflow=1.15.0=eigen_py36hd3854b5_0`"]}, {"number": 23454, "title": "checkpoint version", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23453, "title": "Update protobuf for Python 3.7 compat", "body": "Updating [`setup.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py) as requested in https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-433987600 already uses 3.6.1, see commit 76c4853b50f201b4a809ac66746c798e049b294c\r\n\r\nRegards tensorflow/tensorflow#20517", "comments": ["still have same error with latest master when build from source ( python 3.7.0 )"]}, {"number": 23452, "title": "Eager: dtype not checked", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Ubuntu 16.04.4 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v1.11.0-0-gc19e29306c 1.11.0`\r\n- Python version: 3.6.5 Anaconda\r\n- CUDA/cuDNN version: Cuda 9.0\r\n- GPU model and memory: does not apply - I used CPU\r\n\r\n**Describe the current behavior**\r\n\r\nThe following code\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nprint(1.2*tf.constant(2))\r\n```\r\ndoes not crash and outputs `Tensor(2, dtype=int32)`. \r\n\r\n**Describe the expected behavior**\r\n\r\nEither there should be an error indicating dtype not matched, or there should be an implicit cast.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee above. ", "comments": ["@meta-inf With the latest tf-nightly the error is thrown correctly. I think this issue is resolved?\r\n\r\n\r\n```\r\nroot@ubuntu:/v# python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.VERSION)\r\n1.13.0-dev20181103\r\n>>> tf.enable_eager_execution()\r\n>>> print(1.2*tf.constant(2))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 893, in r_binary_op_wrapper\r\n    x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1059, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1155, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 229, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 179, in constant\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 110, in convert_to_eager_tensor\r\n    t = ops.EagerTensor(value, context=handle, device=device, dtype=dtype)\r\nTypeError: Cannot convert value 1.2 to EagerTensor with requested dtype: 3\r\n>>> \r\n```", "The only issue seems to be that the TypeError has `dtype: 3` which is not very descriptive. I have added a PR #23479 to fix it so that the output would be:\r\n```\r\nTypeError: Cannot convert value 1.2 to EagerTensor with requested dtype: int32\r\n```"]}, {"number": 23451, "title": "Current implementation does not yet support strides in the batch and depth dimensions.", "body": "Hello,\r\n\r\nI try to design autoencoder in tensorflow that was previously made with tensorflow contrib tf.contrib.layer.\r\nHere is the code : \r\n```\r\n# An undercomplete autoencoder on MNIST dataset\r\nfrom __future__ import division, print_function, absolute_import\r\nimport tensorflow.contrib.layers as lays\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom skimage import transform\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nbatch_size = 500  # Number of samples in each batch\r\nepoch_num = 5     # Number of epochs to train the network\r\nlr = 0.001        # Learning rate\r\n\r\ndef resize_batch(imgs):\r\n    # A function to resize a batch of MNIST images to (32, 32)\r\n    # Args:\r\n    #   imgs: a numpy array of size [batch_size, 28 X 28].\r\n    # Returns:\r\n    #   a numpy array of size [batch_size, 32, 32].\r\n    imgs = imgs.reshape((-1, 28, 28, 1))\r\n    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))\r\n    for i in range(imgs.shape[0]):\r\n        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))\r\n    return resized_imgs\r\n\r\n\r\ndef autoencoder(inputs):\r\n    # encoder\r\n    # 32 x 32 x 1   ->  16 x 16 x 32\r\n    # 16 x 16 x 32  ->  8 x 8 x 16\r\n    # 8 x 8 x 16    ->  2 x 2 x 8\r\n    # print('inputs {0}'.format(inputs))\r\n\r\n    # net = lays.conv2d(inputs, 32, [5, 5], stride=2, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d(net, 16, [5, 5], stride=2, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d(net, 8, [5, 5], stride=4, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # # decoder\r\n    # # 2 x 2 x 8    ->  8 x 8 x 16\r\n    # # 8 x 8 x 16   ->  16 x 16 x 32\r\n    # # 16 x 16 x 32  ->  32 x 32 x 1\r\n    # net = lays.conv2d_transpose(net, 16, [5, 5], stride=4, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d_transpose(net, 32, [5, 5], stride=2, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d_transpose(net, 1, [5, 5], stride=2, padding='SAME', activation_fn=tf.nn.tanh)\r\n\r\n    # print('net {0}'.format(net))\r\n    # return net\r\n\r\n    xi = tf.nn.conv2d(ae_inputs,\r\n                 filter=tf.Variable(tf.random_normal([5,5,1,32])),\r\n                 strides=[1,2,2,1],\r\n                 padding='SAME')\r\n    xi = tf.nn.relu(xi)\r\n    print(\"xi {0}\".format(xi))\r\n\r\n    xi = tf.nn.conv2d(xi,\r\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\r\n                     strides=[1,2,2,32],\r\n                     padding='SAME')\r\n    xi = tf.nn.relu(xi)\r\n    print(\"xi {0}\".format(xi))\r\n\r\n    xi = tf.nn.conv2d(xi,\r\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\r\n                     strides=[1,4,4,16],\r\n                     padding='SAME')\r\n    xi = tf.nn.relu(xi)\r\n\r\n    print(\"xi {0}\".format(xi))\r\n\r\n\r\n\r\n\r\n\r\n\r\n    xo = tf.nn.conv2d_transpose(xi,\r\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\r\n                     output_shape=[tf.shape(xi)[0], 8, 8, 16],\r\n                     strides=[1,4,4,1],\r\n                     padding='SAME')\r\n    xo = tf.nn.relu(xo)\r\n\r\n    print(\"xo {0}\".format(xo))\r\n\r\n    xo = tf.nn.conv2d_transpose(xo,\r\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\r\n                     output_shape=[tf.shape(xo)[0], 16, 16, 32],\r\n                     strides=[1,2,2,1],\r\n                     padding='SAME')\r\n    xo = tf.nn.relu(xo)\r\n\r\n    print(\"xo {0}\".format(xo))\r\n\r\n    xo = tf.nn.conv2d_transpose(xo,\r\n                     filter=tf.Variable(tf.random_normal([5,5,1,32])),\r\n                     output_shape=[tf.shape(xo)[0], 32, 32, 1],\r\n                     strides=[1,2,2,1],\r\n                     padding='SAME')\r\n    xo = tf.nn.tanh(xo)\r\n\r\n\r\n    print(\"xo {0}\".format(xo))\r\n    return xo\r\n# read MNIST dataset\r\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\r\n\r\n# calculate the number of batches per epoch\r\nbatch_per_ep = mnist.train.num_examples // batch_size\r\n\r\nae_inputs = tf.placeholder(tf.float32, (None, 32, 32, 1))  # input to the network (MNIST images)\r\nae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\r\n\r\n# calculate the loss and optimize the network\r\nloss = tf.reduce_mean(tf.square(ae_outputs - ae_inputs))  # claculate the mean square error loss\r\nwith tf.name_scope('train'):\r\n    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\r\n\r\n\r\n\r\n# initialize the network\r\ninit = tf.global_variables_initializer()\r\nsaver = tf.train.Saver()\r\nstddev = tf.sqrt(tf.reduce_mean(tf.square(ae_inputs - ae_outputs)))\r\n\r\nwith tf.name_scope('stddev'):\r\n        tf.summary.scalar('accuracy', stddev)\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    train_writer = tf.summary.FileWriter('/tmp/autoencoder/', sess.graph)\r\n    index = 0\r\n    for ep in range(epoch_num):  # epochs loop\r\n        for batch_n in range(batch_per_ep):  # batches loop\r\n            batch_img, batch_label = mnist.train.next_batch(batch_size)  # read a batch\r\n            batch_img = batch_img.reshape((-1, 28, 28, 1))               # reshape each sample to an (28, 28) image\r\n            batch_img = resize_batch(batch_img)                          # reshape the images to (32, 32)\r\n            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n            run_metadata = tf.RunMetadata()\r\n            accuracy, c = sess.run([train_op, loss], feed_dict={ae_inputs: batch_img},\r\n                              options=run_options,\r\n                              run_metadata=run_metadata)\r\n            print('Epoch: {} - cost= {:.5f}'.format((ep + 1), c))\r\n            if batch_n % 10 == 0:\r\n\r\n              summ=tf.Summary()\r\n              summ.value.add(tag='Cost', simple_value = c)\r\n              summ.value.add(tag='Accuracy', simple_value = accuracy)\r\n              #train_writer.add_run_metadata(run_metadata, 'step%03d' % batch_n)\r\n              train_writer.add_summary(summ,index)\r\n              index+=1\r\n              train_writer.flush()\r\n              save_path = saver.save(sess, \"/tmp/autoencoder/model.ckpt\")\r\n\r\n              #file_writer = tf.summary.FileWriter(logdir='/tmp/autoencoder/')\r\n              print(\"Model saved in path: %s\" % save_path)\r\n\r\n    train_writer.close()\r\n\r\n    # test the trained network\r\n    batch_img, batch_label = mnist.test.next_batch(50)\r\n    batch_img = resize_batch(batch_img)\r\n    recon_img = sess.run([ae_outputs], feed_dict={ae_inputs: batch_img})[0]\r\n\r\n    # plot the reconstructed images and their ground truths (inputs)\r\n    plt.figure(1)\r\n    plt.title('Reconstructed Images')\r\n    for i in range(50):\r\n        plt.subplot(5, 10, i+1)\r\n        plt.imshow(recon_img[i, ..., 0], cmap='gray')\r\n    plt.figure(2)\r\n    plt.title('Input Images')\r\n    for i in range(50):\r\n        plt.subplot(5, 10, i+1)\r\n        plt.imshow(batch_img[i, ..., 0], cmap='gray')\r\n    plt.show()\r\n\r\n\r\n```\r\n\r\nWhere I run the code I get that error:\r\n\r\n```\r\n`WARNING:tensorflow:From ./simple_autoencoder.py:115: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nxi Tensor(\"Relu:0\", shape=(?, 16, 16, 32), dtype=float32)\r\nxi Tensor(\"Relu_1:0\", shape=(?, 8, 8, 16), dtype=float32)\r\nxi Tensor(\"Relu_2:0\", shape=(?, 2, 2, 8), dtype=float32)\r\nxo Tensor(\"Relu_3:0\", shape=(?, 8, 8, 16), dtype=float32)\r\nxo Tensor(\"Relu_4:0\", shape=(?, 16, 16, 32), dtype=float32)\r\nxo Tensor(\"Tanh:0\", shape=(?, 32, 32, 1), dtype=float32)\r\n2018-11-02 09:21:05.882917: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\r\n  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\r\n2018-11-02 09:21:06.345232: E tensorflow/core/common_runtime/executor.cc:630] Executor failed to create kernel. Invalid argument: Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./simple_autoencoder.py\", line 150, in <module>\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\n\r\nCaused by op 'Conv2D_1', defined at:\r\n  File \"./simple_autoencoder.py\", line 121, in <module>\r\n    ae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\r\n  File \"./simple_autoencoder.py\", line 69, in autoencoder\r\n    padding='SAME')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\n`\r\n\r\n```\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 1.10\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\n\r\n", "comments": ["Hey, so the equivalent operation of \r\n```\r\nnet = tf.placeholder(tf.float32, [10, 255, 255, 3])\r\nnet = lays.conv2d(net, 16, [5, 5], stride=2, padding='SAME')\r\n```\r\nin `tf.nn` is\r\n```\r\nfilters = tf.get_variable('Conv/weights', [5, 5, 3, 16], tf.float32) \r\nbiases =  tf.get_variable('Conv/bias', [16], tf.float32)\r\nnet = tf.nn.conv2d(net, filters, [1, 2, 2, 1], padding='SAME') + biases\r\n``` \r\nsince the first and last dimension of the net is batch_size and channel, when we do convolution, we don't put stride on them. ", "@xav12358  - Any update on this ?", "In \"awaiting response\" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks !"]}, {"number": 23450, "title": "[tf.nn.ctc_loss] Why we need input *sequence_length*, when we already inputted *inputs*?", "body": "In tf.nn.ctc_loss:\r\n\r\ntf.nn.ctc_loss(\r\n    labels,\r\n    inputs,\r\n    sequence_length,\r\n    preprocess_collapse_repeated=False,\r\n    ctc_merge_repeated=True,\r\n    ignore_longer_outputs_than_inputs=False,\r\n    time_major=True\r\n)\r\n\r\n_**inputs**_ are sequence,\r\n_**sequence_length**_ are length of sequence.\r\n\r\nQ1:\r\nWhy we need to setting **_sequence_length_** when sequence are given?\r\nQ2:\r\nCan we setting **_sequence_length_** shorter then len(inputs)?", "comments": ["**inputs:** 3-D float Tensor. If time_major == False, this will be a Tensor shaped: [batch_size, max_time, num_classes]. If time_major == True (default), this will be a Tensor shaped: [max_time, batch_size, num_classes]. The logits.\r\n**sequence_length:** 1-D int32 vector, size [batch_size]. The sequence lengths.\r\n\r\nCheck if #14659 helps.\r\n\r\nAlso please note that this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23449, "title": "Bug in tensorflow lite java wrapper", "body": "In file \r\n\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java\" line 286:\r\n  ****/** Gets the number of output tensors. */\r\n  int getOutputTensorCount() {\r\n    return inputTensors.length;\r\n  }****\r\n\r\nIn this function, output tensors.length should be used instead of inputTensors.length", "comments": ["Wow, good catch, that's a nasty bug. Will have a fix up shortly, thanks for the feedback."]}, {"number": 23448, "title": "Not getting position of detected objects", "body": "Hi \r\nNot able to get the position of detected objects.\r\nThanks ", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23446, "title": "1.12.0-final cherry-pick request: Fix a bug in tpu.py and xla.py that while creating an identity node\u2026", "body": "\u2026for control input edges under rewrite context, the parent control flow context is lost.\r\n\r\nPiperOrigin-RevId: 219724472", "comments": ["I'm optimistically cherry-picking this in, so that we can get a round of release testing in tonight."]}, {"number": 23445, "title": "Make tf.case work in eager mode with dict inputs", "body": "`tf.case` does not work in eager mode if the caller passes an unordered dictionary of predicate function pairs. In particular, the example code in the API documentation for `tf.case` doesn't run in eager mode:\r\n\r\n```\r\nIn [1]: import tensorflow as tf \r\n   ...: tf.enable_eager_execution() \r\n   ...: (x,y,z) = (1,2,3) \r\n   ...: def f1(): return tf.constant(17) \r\n   ...: def f2(): return tf.constant(23) \r\n   ...: def f3(): return tf.constant(-1) \r\n   ...: tf.case({tf.less(x, y): f1, tf.greater(x, z): f2}, \r\n   ...:         default=f3, exclusive=True)                                     \r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n[stack trace]\r\n~/temp/testenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in name(self)\r\n    950   def name(self):\r\n    951     raise AttributeError(\r\n--> 952         \"Tensor.name is meaningless when eager execution is enabled.\")\r\n    953 \r\n    954   @property\r\n\r\nAttributeError: Tensor.name is meaningless when eager execution is enabled.\r\n```\r\n\r\nThe reason for this problem is that `tf.case` uses the names of the predicate ops to define the sort order of the predicates. This sort order is needed to make the op have deterministic behavior when `exclusive=False`.\r\n\r\nThis PR modifies `tf.case` so that, if the user passes an unordered dictionary for the `pred_fn_pairs`, and eager mode is enabled, and the `exclusive` argument is set to `True`; then `tf.case` will evaluate all the predicates in dictionary traversal order.\r\n\r\nIf `pred_fn_pairs` is an unordered dictionary, and eager mode is enabled, and the `exclusive` argument is `False`; the new code throws an exception. The traversal order of unordered dictionaries is not deterministic on versions of Python before 3.6.\r\n\r\nI also updated the API docs and fixed a few typos in the docs.", "comments": ["Pushed some updates to address review comments."]}, {"number": 23444, "title": "Update version to 1.12.0 final", "body": "", "comments": ["FYI the plan is that the 1.12.0 branch will be promoted to final.  If we change our minds tomorrow, we can still make the corresponding changes; this is just to make it simpler to run all the tests."]}, {"number": 23443, "title": "Python client link is broken", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: unrelated, I'm using github ui viewing the code\r\n- Doc Link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/client_lib.py#L18\r\n\r\n\r\n**Describe the documentation issue**\r\nThis link https://tensorflow.org/api_guides/python/client gives 404\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@MarkDaoust  -  PTAL", "Hi @xiaofei06, \r\n\r\nThanks for the report.\r\n\r\nNormally I'd just ask you to send a PR, but this is a little bigger than it looks.\r\n\r\nIt affects ~20 files. A fix is on it's way."]}, {"number": 23442, "title": "Can't install tensorflow with Python 3.7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacoS Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): PIP\r\n- TensorFlow version: 1.10.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nThere's no wheel file provided for Python 3.7 hence I cannot install tensorflow while using Python 3.7\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["#20517 \r\n\r\nPlease keep track of the above issue. We are maintaining a single issue related to Python 3.7 compatibility. Feel free to add your comment there. Thanks !", "when I will be abled to install tensorflow with python3.7 ? @harshini-gadige "]}, {"number": 23441, "title": "Fix the compute_output_shape issue in tf.keras.layers.Bidirectional", "body": "This PR fixes #23299, where `tf.keras.layers.Bidirectional` does not work in some cases under eager execution mode because the output shape from the forward layer may not be as expected.", "comments": ["@alextp Could you please help review this PR when you have time?", "@alextp Thanks for your review. `\\` has been removed, and add the break after the parenthesis. Could you review the [change](https://github.com/tensorflow/tensorflow/pull/23441/commits/b11c33a5d1792ba7e4ff0a687d43c0322b3458d0) ?", "@alextp @fchollet This PR has been `ready to pull` for a while. Is there anything else I need to do for merging?", "@akshaym I think you need to merge this internally?", "This PR is no longer needed. See https://github.com/tensorflow/tensorflow/issues/23299 for more details."]}, {"number": 23440, "title": "Tensorflow micro build fail: fatal error: 'NEON_2_SSE.h' file not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest git\r\n\r\n**Describe the problem**\r\n\r\nFollowing the instructions here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro#getting-started\r\n\r\n```\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\ntensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/micro_error_reporter.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/micro_interpreter.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/micro_mutable_op_resolver.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/simple_tensor_allocator.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/kernels/depthwise_conv.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/kernels/fully_connected.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/kernels/softmax.test_target\r\ng++ -O3 -DNDEBUG --std=c++11 -g -DTF_LITE_STATIC_MEMORY -I. -Itensorflow/lite/experimental/micro/tools/make/../../../../../ -Itensorflow/lite/experimental/micro/tools/make/../../../../../../ -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc -o tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o\r\nIn file included from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:16:\r\n./tensorflow/lite/c/builtin_op_data.h:144:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/lite/c/builtin_op_data.h:147:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/lite/c/builtin_op_data.h:217:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/lite/c/builtin_op_data.h:220:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/lite/c/builtin_op_data.h:259:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\nIn file included from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:18:\r\n./tensorflow/lite/kernels/internal/common.h:42:10: fatal error: 'NEON_2_SSE.h' file not found\r\n#include \"NEON_2_SSE.h\"\r\n         ^~~~~~~~~~~~~~\r\n5 warnings and 1 error generated.\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o] Error 1\r\n```\r\n\r\nBefore that there is another error about BZL_FILE_PATH but I've used the fix in https://github.com/tensorflow/tensorflow/pull/23424 to get past that.\r\n\r\nCould this NEON_2_SSE.h problem also be due the the move out of contrib?", "comments": ["Sorry about that! It looks like it did get broken in the move out of contrib, I'll get a fix to you shortly, hopefully.", "Looking into it more deeply, this actually looks like a problem with particular x86 architectures that existed before the move. I wasn't able to reproduce it on my machine, but I've checked in this change which I hope should fix it:\r\nhttps://github.com/tensorflow/tensorflow/commit/4e81d6cc877ba4a56196bbdd8d4e09ab7f7b8412\r\n\r\nCan you give it a try and let me know? Thanks!", "That didn't fix it for me - I'm using a Mac. But it did give me enough trying copying your linux one to a new ```osx_x86_64_makefile.inc``` containing: \r\n```\r\n# Settings for x86 on Mac\r\nifeq ($(TARGET), osx)\r\n  ifeq ($(TARGET_ARCH), x86_64)\r\n    PLATFORM_FLAGS = \\\r\n      -DTF_LITE_DISABLE_X86_NEON\r\n    CXXFLAGS += $(PLATFORM_FLAGS)\r\n    CCFLAGS += $(PLATFORM_FLAGS)\r\n  endif\r\nendif\r\n```\r\nand that does get the build working.", "Thanks for the report, and sorry for the slow response! I believe this is now fixed in the head of TF, so closing, but please reopen if it's still an issue.", "It's still an issue in the projects provided as a set of pre-generated projects at https://drive.google.com/open?id=1cawEQAkqquK_SO4crReDYqf_v7yAwOY8 linked from the README at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro"]}, {"number": 23439, "title": "1.12.0-final cherry-pick request: AsyncCheckpoints: Add missing 'self' arg to write_graph_fn.", "body": "PiperOrigin-RevId: 219365527", "comments": []}, {"number": 23438, "title": "LSTM Cell Training Error", "body": "<em>I have a problem with multi layer lstm training. The accuracy and output model slowly changing when calling \"sess.run(logits, feed_dict={X:...})\" during train process. The problem doesn't happens when I decreased number of layers or neurons or dataset size.\r\n\r\n**System information**\r\n- Google colab selecting gpu\r\n- Python version 3\r\n\r\n\r\n\r\n\r\n```\r\nn_epochs = 6\r\nbatch_size = 150\r\n\r\nn_layers = 3\r\nn_neurons = 300\r\nn_outputs = 2\r\nlearning_rate = 0.001\r\n\r\n\r\nreset_graph()\r\n\r\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\r\ny = tf.placeholder(tf.int32, [None])\r\ndevices = [\"/gpu:0\"] # replace with [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"]\r\nlstm_cells = [DeviceCellWrapper(dev,tf.contrib.rnn.LSTMCell(num_units=n_neurons))\r\n       for layer in range(n_layers)\r\n       for dev in devices]\r\nmulti_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)      \r\noutputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\r\ntop_layer_h_state = states[-1][1]\r\nlogits = tf.layers.dense(top_layer_h_state, n_outputs, name=\"softmax\")\r\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\r\nloss = tf.reduce_mean(xentropy, name=\"loss\")\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\ntraining_op = optimizer.minimize(loss)\r\ncorrect = tf.nn.in_top_k(logits, y, 1)\r\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n\r\nsaver = tf.train.Saver()\r\n\r\n\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    maxValidationPoint = float('-inf')\r\n\r\n    for epoch in range(n_epochs):\r\n        trainDataIndexInPtr = 0\r\n        while(True):\r\n            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\r\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\r\n            if(trainDataIndexInPtr == 0):\r\n                break\r\n\r\n        acc_train = accuracy.eval(feed_dict={X: X_train.reshape((len(X_train), n_steps, n_inputs)), y: y_train.reshape((len(y_train)))})\r\n        acc_validation = accuracy.eval(feed_dict={X: X_validation, y: y_validation})\r\n\r\n        y_pred = sess.run(logits, feed_dict={X: X_validation})\r\n        y_pred2 = sess.run(logits, feed_dict={X: X_test})\r\n\r\n        print(\"Train accuracy =\", \"%.7f\" % acc_train, \"Validation accuracy =\", \"%.7f\" % acc_validation)\r\n\r\n        if(acc_validation > maxAcc_validation ):\r\n            maxAcc_validation = acc_validation\r\n            saver.save(sess, \"./Model\")\r\n```\r\n\r\n### Output:\r\n\r\nTrain accuracy = 0.6400015 Validation accuracy = 0.6296040\r\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070\r\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685\r\nTrain accuracy = 0.6643262 Validation accuracy = 0.6265539\r\nTrain accuracy = 0.6804933 Validation accuracy = 0.6232543\r\nTrain accuracy = 0.7023602 Validation accuracy = 0.6231584\r\n\r\n### if I throw away \"y_pred = sess.run(logits, feed_dict={X: X_validation})\" and \"y_pred2 = sess.run(logits, feed_dict={X: X_test})\", Output:\r\n\r\nTrain accuracy = 0.6400015 Validation accuracy = 0.6296040\r\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070\r\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685\r\nTrain accuracy = 0.6643139 Validation accuracy = 0.6265347\r\n**Train accuracy = 0.6794249 Validation accuracy = 0.6231584\r\nTrain accuracy = 0.7062445 Validation accuracy = 0.6132405**\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23437, "title": "Deploying TF-TRT model on DLA on NVIDIA Xavier platform", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.8\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the TF-TRT integration only allows to deploy on dGPU or iGPU by changing DEFAULT_CUDA_VISIBLE_DEVICE. However, to deploy on DLA you need to go through native TRT support, which nullifies the USP of TF-TRT(falling back on TF for unsupported ops instead of writing custom ops). Please see https://github.com/nvdla/sw for details.\r\n\r\n**Will this change the current api? How?** no\r\n\r\n**Who will benefit with this feature?** Everyone deploying TF models on NVIDIA based embedded H/W including Jetson, DRIVE AGX\r\n\r\n**Any Other info.**\r\n", "comments": ["@dhingratul,\r\n\r\nCan you clarify what you ask?", "@samikama Take a look here,\r\nhttps://devtalk.nvidia.com/default/topic/1043020/jetson-agx-xavier/running-tensorflow-program-on-igpu-dla-solved-/post/5290874/#5290874\r\n\r\nI would like to know if there any WIP to use TF-TRT integration instead of native TRT to use the DLA on DRIVE/Jetson AGX?", "Nagging Assignee @samikama: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@dhingratul, yes there is some work on that front. I am closing this since this is not really an issue and we are already working on it though it is not the highest priority."]}, {"number": 23436, "title": "Tensorflow crashes on source building!!!", "body": "**System information**\r\n- Linux Ubuntu 18.04.1):\r\n- No mobile problem\r\n- https://github.com/tensorflow/tensorflow.git\r\n- TensorFlow version: 1.11.0\r\n- Python version: 2.7.15rc1\r\n- Haven't done any pip installation\r\n- Bazel version: Build label: 0.19.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Not relevant\r\n\r\n\r\n\r\n**I am trying to install Tensorflow, but I keep getting the same problem when I use this command to build it:\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package //tensorflow:libtensorflow_framework.so`**\r\n\r\n**Command sequence:**\r\n````\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package //tensorflow:libtensorflow_framework.so\r\n````\r\n\r\n**The output I get:**\r\nERROR: /root/tensorflow/tensorflow/python/BUILD:371:1: C++ compilation of rule '//tensorflow/python:ndarray_tensor' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 338 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from ./tensorflow/python/lib/core/ndarray_tensor.h:20:0,\r\n                 from tensorflow/python/lib/core/ndarray_tensor.cc:16:\r\n./tensorflow/python/lib/core/numpy.h:35:10: fatal error: Python.h: No such file or directory\r\n #include <Python.h>\r\n          ^~~~~~~~~~\r\ncompilation terminated.\r\n", "comments": ["@PanderBoy18  -  Using Bazel 0.15 and GCC 4.8 should not cause this issue. Try with the below combination. \r\nAlso try restarting the system.\r\n![image](https://user-images.githubusercontent.com/42781361/47881240-cd35c280-dde2-11e8-9ccb-9493953ea490.png)\r\n", "I almost got it working, I am using this code at the end of my script:\r\n```\r\nfilepath = \"RNN_Final-{epoch:02d }-{val_acc:.3f}\"\r\n\r\n#Saves only the best ones\r\ncheckpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\r\n\r\nhistory = model.fit(\r\n    train_x, train_y,\r\n    batch_size=BATCH_SIZE,\r\n    epochs=EPOCHS,\r\n    validation_data=(validation_x, validation_y),\r\n    callbacks=[tensorboard, checkpoint])\r\n```\r\n\r\nAnd this is the error I get:\r\n```\r\nFile \"/var/www/test.nl/test.py\", line 162, in <module>\r\n    callbacks=[tensorboard, checkpoint])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 1605, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 238, in fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.py\", line 214, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.py\", line 568, in on_epoch_end\r\n    filepath = self.filepath.format(epoch=epoch + 1, **logs)\r\nValueError: Invalid conversion specification\r\n```\r\nThe script crashes on this line:\r\n\r\n`callbacks=[tensorboard, checkpoint])`\r\nWhat can I do, or what am I doing wrong?", "The build seems to have succeeded, but this is an issue with using keras callbacks.", "> The build seems to have succeeded, but this is an issue with using keras callbacks.\r\n\r\n@fchollet  Any inputs on this please ?", "Any help on how to fix the Python.h problem? I've fed \"-isystem/usr/include/python2.7\" to both `--cxxopts` for Bazel, or as extra flags to use when configuring Tensorflow, it's just missing in the compile command (as shown with `--verbose_failures`). I don't know enough about the tool chains here to understand where do I need to wrench it in...\r\nOr if somebody could tell me which process is supposed to be used to make sure that the build picks up Python include directory, I can try figuring out why isn't it working in some cases...", "For anybody else who gets stuck on the Python thing - yes, the python development package needs to be installed, but if it's done after configure was ran, you'd need to blow stuff away. I don't know how much stuff needed to be (blown away), I removed Bazel cache directory, and cloned Tensorflow again, that did it.", "Did you do ./configure again?", "@byronyi as far as I can remember - yes.", "@PanderBoy18 I know this is a stale issue. If this is still an issue, can you please share a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23436\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23436\">No</a>\n"]}, {"number": 23435, "title": "Tensorflow crashes on source building!!!", "body": "I am trying to install Tensorflow, but I keep getting the same problem when I use this command to build it:\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package //tensorflow:libtensorflow_framework.so\r\n`\r\nThe error I get is:\r\n\r\n/root/tensorflow/tensorflow/python/BUILD:371:1: C++ compilation of rule '//tensorflow/python:ndarray_tensor' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 338 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from ./tensorflow/python/lib/core/ndarray_tensor.h:20:0,\r\n                 from tensorflow/python/lib/core/ndarray_tensor.cc:16:\r\n./tensorflow/python/lib/core/numpy.h:35:10: fatal error: Python.h: No such file or directory\r\n #include <Python.h>\r\n          ^~~~~~~~~~\r\ncompilation terminated.\r\n\r\nServer: DELL9020 Linux Ubuntu server\r\nPython version: 2.7.15rc1\r\n\r\nHere are the commands I used:\r\n`git clone https://github.com/tensorflow/tensorflow.git`\r\n`cd tensorflow`\r\n`./configure`\r\n\r\nAnd this is the command where it crashes:\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nDoes someone know what to do?\r\n\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. Thank you for your cooperation.\r\n", "https://www.tensorflow.org/install/source\r\n\r\nMake sure you have python-dev package installed. That includes Python.h", "Still same problem after installing python-dev package", "This issue is being triaged here #23436"]}, {"number": 23434, "title": "Windows cluster: could not start gRPC server", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.8.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory:  Quadro K22000 with 3.34GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI have a cluster running on Windows 10 and wanted to test tensorflow on a distributed environment. As a test, I want to detect the video card installed on 3 machines. When I ran the code, I get the following error messages:\r\n\r\nE1101 16:04:32.821000000 17460 server_chttp2.cc:40] {\"created\":\"@1541106272.820000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"Failed to add port to server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of each socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\":10048}]}]}\r\nProcess Process-4:\r\nTraceback (most recent call last):\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\nEE11101 16:04:321.0976000000 17868 server_chttp2.1 16:04:32c.c976000000 16224 server_chttp2.cc:40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307:,40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolv\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"description\":\"Failed to add port to server\",\"filed\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_beu\"ild\\grpc\\src\\grpc\\src\\core\\ext\\transpo:rt\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"descript\"Ti:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.9760000on\":\"Failed to add p0ort to0 server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_bui\"l,d\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"crea\"ted\":\"@1541106272.976000000\",\"descriptdion\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmaeke_builds\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_secrirver_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage option\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of eachf  seoach socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\"c:\"bind\",\"wsa_error\":10048}]}]}\r\nket addressProcess Process-2:\r\n (protocol/network addreTraceback (most recent call last):\r\nss  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\"  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n:10048}]}]}\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in __init__\r\n    self._server_def.SerializeToString(), status)\r\nProcess Process-3:\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\nTraceback (most recent call last):\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\n\r\n\r\n**Describe the expected behavior**\r\nI would like to be able to detect the video card and report it\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nMain.py file:\r\n\r\nfrom multiprocessing import Process, Pipe\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training import HParams\r\nfrom GPUs import gpus_configuration\r\n\r\nN_workers= 3\r\nSPEC_Second= {\"ps\": [\"192.168.3.18:2222\"], \"worker\": [\"192.168.3.18:2222\", \"192.168.3.14:2222\", \"192.168.3.16:2222\"]}\r\n\r\ndef find_amount_gpus(task, connection):\r\n    spec= tf.train.ClusterSpec(SPEC_Second)\r\n    \r\n    params= HParams(cluster= spec, job_name= task[0], task_index= task[1])\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n    \r\n    if (params.job_name == \"ps\"):\r\n        server.join()\r\n    elif (params.job_name == \"worker\"):\r\n        with tf.device(tf.train.replica_device_setter(worker_device= \"/job:worker/task:%d\" % params.task_index, cluster= spec)):\r\n            gpus_configuration(connection)\r\n\r\nif __name__ == '__main__':\r\n    devices= [['ps', 0], ['worker', 0], ['worker', 1], ['worker', 2]]\r\n\r\njobs= []\r\n    pipe_list= []\r\n    \r\n    for i in devices: #for i in range(0, 1):#N_workers):\r\n        print(\"In loop to find GPUs {}\\n\".format(i))\r\n        parent_conn, child_conn= Pipe()\r\n        p= Process(target= find_amount_gpus, args=(i, child_conn))\r\n        jobs.append(p)\r\n        pipe_list.append(parent_conn)\r\n        p.start()\r\n    \r\n    total_results= [out.recv() for out in pipe_list]\r\n    \r\n    Worker_1_gpus= total_results[0]['GPUs_info']\r\n    Worker_1_ip= total_results[0]['IPaddr']\r\n    print(\"Worker 1 {}\\n\".format(Worker_1_gpus))\r\n    print(\"Worker 1 IP {}\\n\".format(Worker_1_ip))\r\n    Worker_2_gpus= total_results[1]['GPUs_info']\r\n    Worker_2_ip= total_results[1]['IPaddr']\r\n    print(\"Worker 2 {}\\n\".format(Worker_2_gpus))\r\n    print(\"Worker 2 IP {}\\n\".format(Worker_2_ip))\r\n    Worker_3_gpus= total_results[2]['GPUs_info']\r\n    Worker_3_ip= total_results[2]['IPaddr']\r\n    print(\"Worker 3 {}\\n\".format(Worker_3_gpus))\r\n    print(\"Worker 3 IP {}\\n\".format(Worker_3_ip))\r\n\r\nGPUs.py file\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\n    \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\" # Turn off Tensorflow messages\r\nos.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\r\n\r\n\"\"\" Determine presence of GPUs or CPUs\r\n\"\"\"\r\n\r\ndef gpus_configuration(child_conn):\r\n    import socket\r\n    \r\n    hostname= socket.gethostname()\r\n    IPaddr= socket.gethostbyname(hostname)\r\n\r\n    config= tf.ConfigProto()\r\n    config.gpu_options.allow_growth= True            \r\n    \r\n    with tf.Session(config= config) as sess:\r\n        local_device_protos= device_lib.list_local_devices()\r\n        sess.close()\r\n\r\n    gpus= len([x.name for x in local_device_protos if x.device_type == 'GPU'])\r\n    \r\n    total_results= {\"GPUs_info\": gpus,\r\n                    \"IPaddr\": IPaddr}\r\n    \r\n    child_conn.send(total_results)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ivan-marroquin  -  PTAL at [this](https://stackoverflow.com/questions/45269790/tensorflow-multoprocessing-unknownerror-could-not-start-grpc-server) and [Distributed computing with TF\r\n](https://learningtensorflow.com/lesson11/)\r\nAlso this should not be an issue with the latest tensorflow version. Please try. Thanks !", "Hi @harshini-gadige ,\r\n\r\nThanks for the suggestion to check the ticket. I will do so. About the suggestion of using the latest version of tensorflow. Are you referring to 1.11? Is this version compatible with cuda 9?\r\n\r\nIvan", "@ivan-marroquin  - Yes. Try with the below combination.\r\n\r\n![image](https://user-images.githubusercontent.com/42781361/47926427-c3ad6880-de7d-11e8-9fac-a422138755e1.png)\r\n", "Hi @harshini-gadige , I will give a try next Monday and let you know. By the way, does gRPC use a TCP/IP connection?", "Hi @harshini-gadige , I updated tensorflow to version 1.11 as suggested. Unfortunately, I still have the same issue. For some reason the gRPC can't be started. I went to examine code examples on tensorflow distributed - as per your suggestion, and I rewrote the main Python script as follows:\r\n\r\ndef find_amount_gpus(device, connection):\r\n    spec= tf.train.ClusterSpec(SPEC_Second)\r\n    server= tf.train.Server(spec, job_name= device[0], task_index= device[1])\r\n   \r\n    if (device[0] == \"ps\"):\r\n        server.join()\r\n\r\n    if (device[0] == \"worker\"):\r\n        with tf.device(tf.train.replica_device_setter(worker_device= \"/job:worker/task:%d\" % device[1], cluster= spec)):\r\n            gpus_configuration(connection)\r\n\r\nif __name__ == '__main__':\r\n    devices= [[\"ps\", 0], [\"worker\", 0], [\"worker\", 1]]\r\n    jobs= []\r\n    pipe_list= []\r\n    \r\n    for i in devices: #for i in range(0, 1):#N_workers):\r\n        parent_conn, child_conn= Pipe()\r\n        p= Process(target= find_amount_gpus, args=(i, child_conn), daemon= True)\r\n        jobs.append(p)\r\n        pipe_list.append(parent_conn)\r\n        p.start()\r\n    \r\n    total_results= [out.recv() for out in pipe_list]\r\n\r\n\r\nBelow you will find the entire error log:\r\n\r\n```\r\nE1105 15:50:49.242000000 17544 external/grpc/src/core/ext/transport/chttp2/server/insecure/server_chttp2.cc:40] {\"created\":\"@1541454649.242000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541454649.242000000\",\"description\":\"Failed to add port to server\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_windows.cc\",\"file_line\":509,\"referenced_errors\":[{\"created\":\"@1541454649.242000000\",\"description\":\"OS Error\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_windows.cc\",\"file_line\":201,\"os_error\":\"Only one usage of each socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\":10048}]}]}\r\nProcess Process-3:\r\nTraceback (most recent call last):\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 35, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= device[0], task_index= device[1])\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 148, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 526, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\nE1105 15:50:49.319000000 14416 external/grpc/src/core/ext/transport/chttp2/server/insecure/server_chttp2.cc:40] {\"created\":\"@1541454649.319000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541454649.319000000\",\"description\":\"Failed to add port to server\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_windows.cc\",\"file_line\":509,\"referenced_errors\":[{\"created\":\"@1541454649.319000000\",\"description\":\"OS Error\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_windows.cc\",\"file_line\":201,\"os_error\":\"Only one usage of each socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\":10048}]}]}\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 35, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= device[0], task_index= device[1])\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 148, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 526, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\n```\r\n", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I do not really have experience with grpc server. @mrry any ideas?\r\nI think this line is the actual error message (reformatting for readability):\r\n```\r\nE1105 15:50:49.319000000 14416 external/grpc/src/core/ext/transport/chttp2/server/insecure/server_chttp2.cc:40] \r\n{\"created\":\"@1541454649.319000000\",\r\n  \"description\":\"No address added out of total 1 resolved\",\r\n  \"file\":\"external/grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":307,\r\n  \"referenced_errors\":[{\"created\":\"@1541454649.319000000\",\r\n  \"description\":\"Failed to add port to server\",\r\n  \"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_windows.cc\",\"file_line\":509,\r\n  \"referenced_errors\":[{\"created\":\"@1541454649.319000000\",\r\n  \"description\":\"OS Error\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_windows.cc\",\r\n  \"file_line\":201,\r\n   \"os_error\":\"Only one usage of each socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\":10048}]}]}\r\n```\r\n\r\nMaybe a socket is being reused somehow?", "Is it possible that the grpc server only works on Linux based systems?", "The gRPC code works on Windows. As @gunan suggested, that error message indicates that you have something else on your system using the same port as you requested for the server.", "Hi @mrry \r\n\r\nThanks for the clarification. I will talk to IT folks to find out what other processes are running on the same port.\r\n\r\nThanks to all for your help. Let's close the incident as it seems to be more related to a network issue.\r\n\r\nIvan "]}, {"number": 23433, "title": "[nGraph] Updated to nGraph version v0.9.1 and ngraph-tf version v0.7.0", "body": "This pull request upgrades nGraph to the latest stable releases. ", "comments": []}, {"number": 23432, "title": "Incorrect image resizing output in pix2pix_eager.ipynb", "body": "The image resizing in the pix2pix sample code provided in this repository outputs images with out-of-bound values for RGB channels. [Link to pix2pix_eager.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb)\r\n\r\nIn `load_image()` function, while loading test dataset, in the line\r\n\r\n```\r\ninput_image = tf.image.resize_images(input_image, size=[IMG_HEIGHT, IMG_WIDTH], align_corners=True, method=2)\r\n```\r\n\r\nvalues greater than 255 and less than 0 are generated.\r\n\r\nThis issue does not occur while loading train dataset because `method=tf.image.ResizeMethod.NEAREST_NEIGHBOR` is used for resizing. Using the same for test dataset resolves the issue.", "comments": ["Hey, thanks. I have a fix out for it: https://github.com/tensorflow/docs/pull/164", "Nagging Assignee @ymodak: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The PR has been merged.", "Closing this issue since the PR has been merged. Feel free to reopen if the problem still persists. Thanks!"]}, {"number": 23431, "title": "tf keras model.save_weights not working with MirroredStrategy in 1.12", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, duplicated the minimal code from documentation snippet from [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\"18.04.1 LTS (Bionic Beaver)\"\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:None\r\n- TensorFlow installed from (source or binary): source and binary\r\n- TensorFlow version (use command below): v1.12.0-rc2-0-g748435b8ef\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): 0.15.2\r\n- GCC/Compiler version (if compiling from source): 6.4.0\r\n- CUDA/cuDNN version: 9.0 / 7.3\r\n- GPU model and memory: 2 x 1080-ti\r\n\r\n\r\n**Describe the current behavior**\r\nCannot save a tf.keras model if trained with MirroredStrategy either by calling save_weight or by a tf.keras.callbacks.ModelCheckpoint, but does work if MirroredStrategy is not used.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\ninputs = tf.keras.layers.Input(shape=(1,))\r\npredictions = tf.keras.layers.Dense(1)(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=predictions)\r\n\r\nfeatures = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\nlabels = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\ntrain_dataset = tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.2),\r\n              distribute=distribution)\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n```\r\nAnd adding:\r\n```\r\nmodel.save_weights('my_weight')\r\n```\r\nError looks like:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 72, in get\r\n    return self._index[device]\r\nKeyError: '/replica:0/task:0/device:CPU:0'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"minimal_example.py\", line 17, in <module>\r\n    model.save_weights('my_weight')\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1449, in save_weights\r\n    session = backend.get_session()\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 469, in get_session\r\n    _initialize_variables(session)\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 722, in _initialize_variables\r\n    variables = _get_variables(ops.get_default_graph())\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 716, in _get_variables\r\n    variables.update(opt.optimizer.variables())\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 787, in variables\r\n    optimizer_variables = [v for v in self._non_slot_variables()\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 788, in <listcomp>\r\n    if _from_current_graph(v)]\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 781, in _from_current_graph\r\n    return variable.op.graph is current_graph\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 308, in op\r\n    return self.get().op\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 76, in get\r\n    (device, self._index.keys(), device_util.current())), e)\r\n  File \"<string>\", line 3, in raise_from\r\nValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1']) (current device )\r\n```\r\nChanging to:\r\n```\r\ncheckpoint_path = \"my_weight\"\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\r\n                                                 save_weights_only=True,\r\n                                                 verbose=1,\r\n                                                 period=1)\r\nmodel.fit(train_dataset,\r\n          epochs=5,\r\n          steps_per_epoch=10,\r\n          callbacks=[cp_callback])\r\n```\r\nAlso does not work an end up with:\r\n```\r\nEpoch 1/5\r\n 1/10 [==>...........................] - ETA: 4s - loss: 1.1921e-07\r\nEpoch 00001: saving model to model_dir/my_weight\r\nWARNING:tensorflow:You are accessing attribute _replicated_modelof the DistributedCallbackModel that may not have been set correctly.\r\nTraceback (most recent call last):\r\n  File \"minimal_example.py\", line 35, in <module>\r\n    callbacks=[cp_callback])\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1624, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 198, in fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 214, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 599, in on_epoch_end\r\n    self.model.save_weights(filepath, overwrite=True)\r\n  File \"/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2336, in save_weights\r\n    self._replicated_model.save_weights(filepath, overwrite=overwrite,\r\nAttributeError: 'NoneType' object has no attribute 'save_weights'\r\n```\r\n", "comments": ["You need to save the weights of the model as a HDF5 file. \r\nTry doing **model.save_weights('my_weight.h5')** instead.", "@ymodak \r\n**model.save_weights('my_weight.h5')**\r\n**model.save_weights('my_model.h5', save_format='h5')**\r\nand\r\n**model.save_weights('my_weight')** witch should save in the TensorFlow checkpoint file format\r\ndoes not work when MirroredStrategy is use. All of them work if MirroredStrategyis not used. ", "sorry about that", "You need to save inside `distribution.scope()`, so this should work:\r\n\r\n```python\r\nwith distribution.scope():\r\n    model.save_weights('my_weight.h5')\r\n```\r\n\r\nAlso if you're then trying to load weights under the `MirroredStrategy`, I think it will only load onto the first tower (although maybe this is fixed?). Anyway you can look [here](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/keras/engine/distributed_training_utils.py#L31-L52) for an example of how to do it.", "> You need to save inside `distribution.scope()`, so this should work:\r\n> \r\n> ```python\r\n> with distribution.scope():\r\n>     model.save_weights('my_weight.h5')\r\n> ```\r\n> \r\n> Also if you're then trying to load weights under the `MirroredStrategy`, I think it will only load onto the first tower (although maybe this is fixed?). Anyway you can look [here](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/keras/engine/distributed_training_utils.py#L31-L52) for an example of how to do it.\r\n\r\nDid you get a chance to try this?", "I'm at version 1.12.0, and `model.save_weights('my_weight.h5')` works fine for a training model with MirroredStrategy. \r\n\r\nI did ran into the callback issue as well. The following seems to work for me\r\n```\r\nModelCheckpoint(filepath='...', save_weights_only=False)\r\n# which is internally doing self.model.save(filepath, overwrite=True)\r\n````\r\n\r\nHowever this doesn't work and raises `AttributeError: 'NoneType' object has no attribute 'save_weights'` like OP's issue.\r\n```\r\nModelCheckpoint(filepath='...', save_weights_only=True)\r\n# which is internally doing self.model.save_weights(filepath, overwrite=True)\r\n````", "This should be working at master, as we have unittests for save and load weights now:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/keras_test.py#L1158\r\n\r\nIt might be working with 1.13 rc as well, but not 100% sure. please try it out and let us know if it is still broken. "]}, {"number": 23430, "title": "Copy just variable's value to another variable (both are trainable variables)", "body": "Hi!\r\n\r\nI have investigated all resources and documentation for finding the way how to copy values of trainable variable to another and couldn't find how to do it. \r\n\r\nAll existing solutions do deep copy (tf.assign(), tf.identity() etc.), so another variable contains the reference to the original variable: when I change one variable, another one is changing too. I don't need this, there shouldn't be any connection between variables.\r\n\r\nHow I can do it?\r\n\r\nThanks.\r\n", "comments": ["This question really should be asked in Stack Overfull and not here.\r\n\r\nDoes this link answer your question? https://stackoverflow.com/questions/33717772/how-can-i-copy-a-variable-in-tensorflow", "Hi @wdirons!\r\n\r\nThanks for this link, but I saw it and it doesn't do what I need.\r\nI found many issues in Stackoverflow for coping variable, but they all talk about coping a reference to another variable.\r\n\r\nSo that's my last point where I guess can find the answer.\r\n\r\nThanks!", "@wdirons  -  Yes. Please open a new issue in Stack Overflow. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23429, "title": "how to get the timestamp of the specified operation?", "body": "Hi,\r\n\r\n  I want to get the timestamp of some operations in each iterations when training DNN. I know timeline can do this, but after some experiments I found timeline caused performance loss. And I only need to get information about a few operations, so I wonder whether it possible to get the timestamp of the specified operation?\r\n\r\nThanks\r\n", "comments": ["@wangshuaizs  - Hi, please note that these type of questions are better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. We encourage you to post it there.\r\n\r\nPlease refer below links for your question. \r\n1) [Timestamp for TF](https://www.tensorflow.org/api_docs/python/tf/timestamp)\r\n2) [More info](https://github.com/tensorflow/tensorflow/issues/5682)\r\n"]}, {"number": 23428, "title": "Fix the C2678 MSVC compilation error", "body": "MSVC 14.15 returns the C2678 error during a compilation of the\r\n'tensorflow/core/kernels/data/scan_dataset_op.cc' unit:\r\n\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\include\\xmemory(217):\r\nerror C2678: binary '*': no operator found which takes a left-hand\r\noperand of type 'const _Iter' (or there is no acceptable conversion)\r\nwith\r\n[\r\n    _Iter=tensorflow::OpInputList::Iterator\r\n]\r\n\r\n.\\tensorflow/core/framework/op_kernel.h(405):\r\nnote: could be 'const tensorflow::Tensor &tensorflow::OpArgIterator<tensorflow::OpInputList,\r\n    const tensorflow::Tensor>::operator *(void)'\r\n\r\nTwo const operator functions: 'operator*() const' and 'operator->()\r\nconst', returning a const pointer or const reference respectively,\r\nhave been added to the 'OpArgIterator' template class.\r\n\r\nSigned-off-by: Pavel Samolysov <samolisov@gmail.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@jsimsa   Please take a look and approve/suggest any changes.", "Nagging Reviewer @jsimsa: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "@jsimsa @harshini-gadige what does Internal CI build failed mean? I've checked the log and see no dependencies on my changes what can brake the build, please let me know If I have to change something in my code.", "AFAICT, the failing tests are unrelated to this change"]}, {"number": 23427, "title": "Build failing with ` ERROR: Config value cuda is not defined in any .rc file`", "body": "I build TF on a biweekly basis, the last successful build was done 3 days ago (29th Oct), but the build job is failing today, while there has been no changes in the installation script.\r\n\r\nThe build happens on an amazon linux machine with cuda 9.0 and cudnn 7.3, using the following command, \r\n\r\n```\r\nbazel build --config=opt --config=mk1 --config=cuda --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nSome of the ralevent flags are set as follows,\r\n```\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=\"$(nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\"\r\n    export TF_CUDNN_VERSION=\"$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)\"\r\n```\r\nThe job failes with the following error message,\r\n```\r\n+ bazel build --config=opt --config=cuda --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/ec2-user/workspace/build-dl-package/src/deep-learning/tensorflow/tensorflow/tools/bazel.rc\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/ec2-user/workspace/build-dl-package/src/deep-learning/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/lib/a-4.2.9-py-2.7.13-dl-gpu//bin/python --action_env PYTHON_LIB_PATH=/usr/lib/a-4.2.9-py-2.7.13-dl-gpu/lib/python2.7/site-packages --python_path=/usr/lib/a-4.2.9-py-2.7.13-dl-gpu//bin/python --define with_jemalloc=true --define with_hdfs_support=true --define with_aws_support=true --define with_kafka_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_VERSION=9.0 --action_env CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 --action_env TF_CUDNN_VERSION=7 --action_env NCCL_INSTALL_PATH=/usr/local/nccl --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2 --action_env LD_LIBRARY_PATH=/usr/java/jdk1.7.0_67/jre/lib/amd64/server:/usr/java/jdk1.7.0_67/jre/lib/amd64:/usr/java/jdk1.7.0_67/jre/../lib/amd64:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/java/jdk1.8.0_121//jre/lib/amd64/server:/usr/lib/torch/install/lib:/usr/local/mpi/lib: --action_env TF_CUDA_CLANG=0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda --define with_mpi_support=true --define grpc_no_ares=true\r\nERROR: Config value cuda is not defined in any .rc file\r\n````\r\nThe build is run against `r1.11` along with commit `89979f42e827d9eb5c349259a5aa2ec32d38c86a` cherry-picked.\r\n\r\nShould I change something?\r\nThank you.", "comments": ["This is an issue with bazel 0.19.0, use bazel 0.18.0 or see: https://github.com/tensorflow/tensorflow/issues/23401", "Thank you. Sorry for creating a duplicate."]}, {"number": 23426, "title": "problems about tf.nn.softmax", "body": "I need a softmax function like this:\r\n\r\ndef softmax(a):\r\n    c = np.max(a)\r\n    exp_a = np.exp(a - c)\r\n    sum_exp_a = np.sum(exp_a)\r\n    y = exp_a / sum_exp_a\r\n    return y \r\n\r\nplease help\r\nThanks!", "comments": ["This is better asked on stackoverflow in future.\r\n\r\n```\r\ndef softmax(a):\r\n  c = tf.reduce_max(a)\r\n  exp_a = tf.exp(a - c)\r\n  sum_exp_a = tf.reduce_sum(exp_a)\r\n  y = exp_a / sum_exp_a\r\n  return y\r\n```\r\n\r\nNow please close this.", "@DewenXu  -  Hi, if this is a feature request, please use [this template ](https://github.com/tensorflow/tensorflow/issues/new?template=30-feature-request.md)and open a new issue with all the information requested in the template. If it's a question, please post to [Stack Overflow](https://stackoverflow.com/questions/tagged//tensorflow)."]}]