[{"number": 6724, "title": "Feature request: 1D deconvolution", "body": "Pretty much topic tells the whole story.\r\n\r\nFor auto-encoder based on 1D convolutions, opposite operation is required to complete decoding part.\r\nThis is planned to be used for sound processing.", "comments": ["There's a kind of deconvolution described in [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf) which is implemented as `nn.conv2d_transpose` are you looking for a 1d version of this?", "interested in this too! ", "Me, too! Don't know if there is a way to do a 1d deconvolution with `nn.conv2d_transpose.`", "This is a very vague feature request. There's already a kind of deconvolution implemented in TensorFlow. Also 1D vector can be turned into as 1xn 2D image using reshape so that 2D functions can be applied.", "BTW, there's 1D FFT+inverse FFT [implemented](https://www.tensorflow.org/versions/master/api_docs/python/math_ops/fourier_transform_functions) in TensorFlow, the other kind of deconvolution can be done by combination of FFT and pointwise divide, would that work for you?", "I will try it out and response here, thanks!", "It worked easily reshaping the data and using the 2D functions, thank you!", "OK, since it works for @Labalex21 I'll close this for now. If this doesn't cover, please open new bug request with detailed specifications of what is needed, possibly with links to existing implementation and/or examples of how it'll be used in TensorFlow", "I recommend, especially if it is easy, that we make a function to optimize it and automatically do the transformation for the user. This is simply to conserve the pattern in Keras that every compatible 2d function has a 1d equivalent (and maybe even a 3d).", "Something like this:\r\n\r\n``` python\r\nfrom keras.engine.topology import Layer\r\nimport keras.backend as K\r\nclass Conv1DTranspose(Layer):\r\n    def __init__(self, filters, kernel_size, strides=1, *args, **kwargs):\r\n        self._filters = filters\r\n        self._kernel_size = (1, kernel_size)\r\n        self._strides = (1, strides)\r\n        self._args, self._kwargs = args, kwargs\r\n        super(Conv1DTranspose, self).__init__()\r\n\r\n    def build(self, input_shape):\r\n        print(\"build\", input_shape)\r\n        self._model = Sequential()\r\n        self._model.add(Lambda(lambda x: K.expand_dims(x,axis=1), batch_input_shape=input_shape))\r\n        self._model.add(Conv2DTranspose(self._filters,\r\n                                        kernel_size=self._kernel_size,\r\n                                        strides=self._strides,\r\n                                        *self._args, **self._kwargs))\r\n        self._model.add(Lambda(lambda x: x[:,0]))\r\n        self._model.summary()\r\n        super(Conv1DTranspose, self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        return self._model(x)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return self._model.compute_output_shape(input_shape)\r\n```", "Hi yaroslavvb, Hi Labalex21, \r\n\r\nWould you like to elaborate the way to re-shape the One D data to 2D matrix? Thanks.", "In [this](https://distill.pub/2016/deconv-checkerboard/) post it's said that transpose_2d is not very good idea for deconvolution and it's better to use image resize functionality. Can anyone check it ?", "It seems that this is already supported in tensorflow or is this a different thing?\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/nn/conv1d_transpose\r\n\r\nIt would be nice to see this implemented in Keras without the Conv2DTranspose hack\r\n\r\n"]}, {"number": 6723, "title": "Fix build errors on AVX2+ hosts with -march=native.", "body": "third_party/eigen3/unsupported/Eigen/CXX/FixedPoint and third_party/eigen3/unsupported/Eigen/CXX/Tensor use different paths to reach into unsupported/Eigen/CXX/src/Tensor. The former's paths cause the build errors documented in #6558. This commit converges on the path in third_party/eigen3/unsupported/Eigen/CXX/Tensor, which fixes the build error.\r\n\r\nI tested the PR with the following command, which fails without the patch.\r\n\r\n```bash\r\nbazel build -c opt --copt=-march=native //tensorflow:libtensorflow.so\r\n```", "comments": ["Can one of the admins verify this patch?", "Could you add a comment immediately preceding the change explaining the problem succintly?", "@drpngx I hope I have addressed your feedback. PTAL?\r\n\r\nI also submitted #6759, which may be a better fix. I don't have access to all the tensorflow buildbots though, so I'm not sure it doesn't break anything. Either that PR or this one should take care of #6558.\r\n\r\nThank you very much for your review!", "Jenkins, test this please.\r\n\r\nPlease ignore the estimator_test error.", "If the ThreadPool include is genuinely used in that file, then this is a better fix. \r\n\r\nIt sounds like we should try to push upstream fixes to eigen as @fnl suggested, then undo this change once propagated.", "@drpngx Are you referring to [this comment](https://github.com/tensorflow/tensorflow/issues/6558#issuecomment-269643996) or another comment from @fnl?\r\n\r\nFWIW, I was able to get a full build using `bazel build -c opt --copt=-march=native //...` on an AVX2 machine, so I'm somewhat skeptical that the Tensor include is needed in the FixedNum file. I'm working on getting a Skylake build going (I'm on OSX, and Apple's clang crashes on `--copt=-march=skylake`).", "Yes, it sounds like it's something with Eigen that should be fixed there, ultimately.\r\n", "My understanding is that @fnl is saying that if TensorContractionThreadPool.h is copied from eigen3 into third_party/eigen3/ then the build succeeds. To me, this seems an indication that the path to reach it is incorrect.\r\n\r\nOr, are you saying that we shouldn't have to ship a modified FixedPoint header at all?", "OK, it looks like #6759 is the better fix. Could you put a comment in that file to explain that we can't add the include in this place?", "@drpngx since you've triggered one of our remote sensors with the word \"Jenkins\" + \"please\", your message arrived to our space time. Please stand by in your coordinates, we will beam you up to our star system soon. Thank you. "]}, {"number": 6722, "title": "tensorflow conv2d unexpected convolution result", "body": "I try to migrate a `Caffe` network and model(weights) to `tensorflow`. The original first layer is a stride one convolution on 1x128x128 gray image with kernel size 5x5, output channel 96, Caffe two padding, tf 'SAME' padding.\r\nI checked the input image and kernel weights are the same for both versions.\r\nHowever, the two's upper left corner of conv1's first feature map are **different**.\r\nOnly the leftmost column is consistent. I manually calculated and checked the results, the first and the second value of Caffe's (-0.71238005 -0.74042225) are correct according to the definition of convolution, the second value in tensorflow's (-0.71238005 -0.31195271) is **incorrect**. Taking into account the padding, the first value is from the 3x3 block of the image, the second should be the 3x4 block.\r\nIs there any subtle difference in computation model between the two version's implementation of convolution? \r\nI am not sure whether it is a bug of `tf` or an usage omission. So I posted the detailed version of the problem on [StackOverflow](http://stackoverflow.com/q/41529997/3863647).", "comments": ["Sorry, I know you closed this, but I've ran into this problem myself today and have been trying to understand what's wrong for 13 hours with no fix. I wonder if you managed to figure out what was wrong? It seems to me that it's a bug with TF.\r\n\r\nI imported some weights from a Caffe network into TF, and for verification I used MATLAB. I also have two OpenCL implementations using FFT convolution and standard convolution. All results excluding TF are consistent, and no matter how much I play around with the padding in TF, it just won't produce the correct results. Plotting the outputs show significant differences, especially in the second layer.  In my case, these errors accumulate and grow through the second convolutional layer and reduces the network's accuracy from 97% to 60%. Of course, it's possible to train the network again in TF to get weights that work, but I don't understand why this bug is not fixed considering how bad it is. "]}, {"number": 6721, "title": "Setting losses_test to medium since it times out on Jenkins at times.", "body": "Change: 143740377", "comments": []}, {"number": 6720, "title": "tf.image.resize_images() - weird padding behaviour? ", "body": "The tf.image.resize_images() seems to use a strange padding option, which one is not clear to me at the moment. I tried to replicate the bilinear interpolation with various padding options in for example skimage, but cant replicate the behaviour.\r\n\r\nIt would be nice to be able to set the padding option used in tf.images.resize_images(), or document what is used at least.\r\n\r\nExample code for comparing the results of  tf.images.resize_images() and skimage transform:\r\nLooks like  tf.images.resize_images() does some weird unsymmetrical padding!?\r\nUsing tensorflow 0.12.1:\r\n```\r\nimport tensorflow as tf\r\nimport tensorlayer as tl\r\nimport numpy as np\r\nimport skimage\r\nfrom scipy.misc import imread, imresize, imsave\r\n\r\nsess = tf.InteractiveSession()\r\n\r\n#create simple test image\r\nimsize = 3\r\nxa, ya = np.ogrid[:imsize, :imsize]\r\nimg = np.repeat((xa + ya)[..., np.newaxis], 3, 2) / float(imsize + imsize)\r\n\r\nx = tf.placeholder(tf.float32, [1, imsize, imsize, 3])\r\ny = tf.image.resize_images(x,(imsize*3, imsize*3))\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\nupsampled_tf_result = sess.run(y, feed_dict={x: [img]})\r\nupsampled_skimage_result = skimage.transform.rescale(img,\r\n                                     3,\r\n                                     mode='symmetric',\r\n                                     cval=0,\r\n                                     order=1,\r\n                                     preserve_range=False)\r\n\r\nprint(np.allclose(upsampled_tf_result, upsampled_skimage_result))\r\n\r\nimsave('upsampled_tf_result.png', np.squeeze(upsampled_tf_result))\r\nimsave('upsampled_skimage_result.png', upsampled_skimage_result)\r\n```", "comments": ["@xmbrst could this be documented better?", "I think we'd need to understand better what's going wrong before knowing how to fix this.  In what way is it weird?", "The corner alignment mechanism is different between tf.image.resize* and skimage.transform.rescale, no matter align_corners=True or False.", "Yes, but how are they different?", "In skimage, the \"area\" of a pixel is taken into account. In tf.image it feels like a pixel is considered as a \"point\" without area. This leads to a difference in alignment. \r\n\r\nE.g. when up-scaling a 2x2 image to a 4x4 image, the alignment is:\r\nskimage: (0, 0) -> (0.5, 0.5), (1, 1) -> (2.5, 2.5)\r\ntf.image.resize_\\*(align_corners=True): (0, 0) -> (0, 0), (1, 1) -> (3, 3)\r\ntf.image.resize_\\*(align_corners=False): (0, 0) -> (0, 0), (1, 1) -> (2, 2)\r\n\r\nLove to hear comments about which alignment is better for NN training.", "Have you considered using `method=ResizeMethod.AREA` if you want area based resizing?", "The AREA method seems to only affect interpolation but not alignment. At least I get strange results with it:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom skimage.transform import rescale\r\n\r\narr = np.array(\r\n    [[1,2,3,4],\r\n    [5,6,7,8],\r\n    [9,10,11,12]], dtype='float32')\r\n\r\ninput = tf.constant(arr)\r\ninput4D = tf.reshape(input, [1, 3, 4, 1])\r\nresize = tf.image.resize_area(input4D, [6, 8], align_corners=True)[0,:,:,0]\r\nsess = tf.Session()\r\nr1 = sess.run(resize)\r\nr2 = rescale(arr/100.0, 2, mode='edge') * 100\r\n```\r\ngives\r\n```\r\nr1=array([[  1.        ,   1.        ,   1.66666651,   2.        ,   2.33333278,   3.        ,   3.        ,   4.00000048],\r\n       [  1.        ,   1.        ,   1.66666651,   2.        ,   2.33333278,   3.        ,   3.        ,   4.00000048],\r\n       [  3.00000024,   3.00000024,   3.66666651,   4.00000048,   4.33333302,   5.00000048,   5.00000095,   6.00000048],\r\n       [  5.        ,   5.        ,   5.66666603,   6.        ,   6.33333206,   7.        ,   7.00000048,   8.00000095],\r\n       [  4.99999952,   4.99999952,   5.66666555,   5.99999952,   6.33333111,   6.99999905,   7.        ,   8.        ],\r\n       [  9.00000191,   9.00000191,   9.66666698,  10.00000191,  10.33333397,  11.00000191,  11.00000191,  12.00000286]], dtype=float32)\r\nr2=array([[  0.99999998,   1.24999997,   1.74999996,   2.24999995,   2.74999994,   3.24999993,   3.74999992,   3.99999991],\r\n       [  2.        ,   2.24999998,   2.74999995,   3.24999994,   3.74999995,   4.24999994,   4.74999991,   4.99999989],\r\n       [  4.00000005,   4.25000001,   4.74999993,   5.24999992,   5.74999998,   6.24999997,   6.74999988,   6.99999984],\r\n       [  6.00000015,   6.25000009,   6.74999999,   7.24999995,   7.74999999,   8.24999996,   8.74999985,   8.9999998 ],\r\n       [  8.00000029,   8.25000023,   8.75000013,   9.25000005,   9.74999999,  10.24999991,  10.74999981,  10.99999975],\r\n       [  9.00000036,   9.25000031,   9.7500002 ,  10.2500001 ,  10.74999999,  11.24999989,  11.74999978,  11.99999973]])\r\n```", "What does that produce?  We have a lot of bugs to triage, so it's helpful if people include output along with code.", "Ug, you're right, that's pretty weird.  @martinwicke Our `tf.image.resize_area` function isn't even reflection equivariant.  It would be lovely to fix this, but I'd be worried about breaking old models.", "There are two separate issues here:\r\n(1) Alignment of the tensor values at the input and output of the resize function.\r\n(2) Interpolation method.\r\n\r\nFor (1), @ppwwyyxx comment is exactly right:\r\nWhen using align_corners=True, we consider the image value as a point sample of a continuous function at the pixel center. When using align_corners=False, we consider the image value as the average of a continuous function over a 1x1 pixel square centered at the pixel center.\r\n\r\nUnfortunately, there is a bug in the implementation of nearest neighbor and area interpolation methods when align_corners=True. For nearest neighbor interpolation this has already been fixed internally and will be pushed to github in the next couple of days. We will fix a similar bug for area interpolation very soon.", "@ppwwyyxx, Regarding your question on \"which alignment is better for NN training\", multiple approaches are possible as long as you are consistent. Here is my own favorite set of rules that we have followed in our [DeepLab](https://arxiv.org/abs/1606.00915) semantic image segmentation system:\r\n\r\n\"DeepLab's Four Alignment Rules\":\r\n(1) Use of odd-sized kernels in all convolution and pooling ops.\r\n(2) Use of SAME boundary conditions in all convolution and pooling ops.\r\n(3) Use align_corners=True when upsampling feature maps with bilinear interpolation.\r\n(4) Use of inputs with height/width equal to a multiple of the output_stride, plus one (for example, when the CNN output stride is 8, use height or width equal to `8 * n + 1`, for some `n`, e.g., image HxW set to 321x513).", "Thanks @gpapan ! For (4), is it to ensure equal paddings on both side of image?\r\n\r\nUPDATE: this is to make sure that stride-2 convolution is always applied on odd-size images so there will be equal padding on both side.", "There are several issues with resize_images. It would be good to have a known-good implementation of this, even if we have to hide it behind a flag (`correct=False`).", "Preferably, the new implementation should follow what is used elsewhere (OpenCV, SciPy, Matlab, ...), which is to align the very corners of top-left (-0.5, -0.5) and bottom-right pixels (height - 0.5, width - 0.5) and resample using corresponding pixel centers.", "Many thanks for TF developers/contributors support. I believe this is critical part of image processing pipelines - it would be great to have this fixed soon please.\r\nDetailed writeup by Oleksandr Savsunenko:\r\nhttps://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35", "Since this doesn't seem to be getting fixed anytime soon, you may want to try DALI from Nvidia:\r\n\r\nhttps://github.com/NVIDIA/DALI", "Are there any plans for fixing this issue? This seems to prevent a raw-image statistics pipeline I'm building from learning correctly.", "It seems like a warning log message would be helpful here to avoid people falling into the same trap as Oleksandr.", "This deserves more attention than it has now.", "please fix this!", "@martinwicke do you know who to assign this to?", "@johnpjf is working on this (maybe not this exactly) but he can comment on what he's doing in this regard.", "Yes, I am working on new anti-aliased image resize which won't have this non- symmetric behavior and will support a larger variety of sampling kernels.\r\n\r\n", "This resizing inconsistency needs addressing or at least mitigation via a log warning .", "Any new news about this ? , results from tensorflow.resize_bilinear is different from cv2.resize bilinear ,, what should i do to make them like each others as the network output different results in each case", "> Any new news about this ? , results from tensorflow.resize_bilinear is different from cv2.resize bilinear ,, what should i do to make them like each others as the network output different results in each case\r\n\r\nIf you don't need to compute its gradient or compute on GPU: use `cv2.resize` via `tf.py_func` wrapper. Otherwise, unless you want to implement a C++ op along with its gradient and submit a PR, I'm afraid we're SOL. ", "Working on it, we'll have something soon!", "@johnpjf Sorry to bother you as well, but did you already merge something here? \r\nI just posted another issue of which I wonder if it is related to changes that were made w.r.t. this issue here.\r\nSee https://github.com/tensorflow/tensorflow/issues/25591", "We're almost there.", "Is it possible that this bug impacts UpSampling2D layers in some way ? I'm trying to replace transposed 2d convolution with Upsampling2d + Conv but suddenly it's completely impossible to get the training to converge appropriately.", "@Overdrivr I'm not familiar with UpSampling layer but it's unlikely that this bug would cause something as extreme as failing to converge.", "Thanks for this fix! I tried some of my cases and the new version seems to work as expected now.\r\n\r\nThere are in fact some similar issues with `tf.image.crop_and_resize`. I have been using my own workaround for it, but if the team is interested I can open an issue describing it in more details.", "@ppwwyyxx I'd be interested in a detailed report. ", "> \r\n> \r\n> @ppwwyyxx, Regarding your question on \"which alignment is better for NN training\", multiple approaches are possible as long as you are consistent. Here is my own favorite set of rules that we have followed in our [DeepLab](https://arxiv.org/abs/1606.00915) semantic image segmentation system:\r\n> \r\n> \"DeepLab's Four Alignment Rules\":\r\n> (1) Use of odd-sized kernels in all convolution and pooling ops.\r\n> (2) Use of SAME boundary conditions in all convolution and pooling ops.\r\n> (3) Use align_corners=True when upsampling feature maps with bilinear interpolation.\r\n> (4) Use of inputs with height/width equal to a multiple of the output_stride, plus one (for example, when the CNN output stride is 8, use height or width equal to `8 * n + 1`, for some `n`, e.g., image HxW set to 321x513).\r\n\r\n@gpapan Why 8n+1 and not 8n?", "I wanted to experiment with the new resizing and discovered to my surprise that it is not (anymore?) differentiable. Is this a bug I should report or is this by design?", "Yeah, that is a bug that deserves reporting. Thank you!\n", "@DeepBlender There certainly should be gradients, thanks for the catch! Looks like I had forgotten to expose gradients for the new kernels (Lanczos etc.).  Fix is in review.\r\nNote that, as in TF 1.0, there is no gradient for the area kernel.", "@johnpjf that sounds great, thanks a lot for your work!\r\nI just started to prepare the bug report, but you were obviously faster.", "Any ideas why we ported align_corners over to TFJS? It doesn't have the legacy issues.", "> Thanks for this fix! I tried some of my cases and the new version seems to work as expected now.\r\n> \r\n> There are in fact some similar issues with `tf.image.crop_and_resize`. I have been using my own workaround for it, but if the team is interested I can open an issue describing it in more details.\r\n\r\n@ppwwyyxx I cant find anti alias option as it is implemented in the fix above. Can you please explain how to use it ? can't find the new option in the documentation [link below].(https://www.tensorflow.org/api_docs/python/tf/image/resize_images)", "@mohapatras antialias is only available in the 2.0 versions, which you can use by\r\n\r\nimport tensorflow.compat.v2 as tf_v2\r\n...\r\ntf_v2.image.resize(..., antialias=True, ...)\r\n", "Can you provide examples of inputs and outputs?", "> Can you provide examples of inputs and outputs?\r\n\r\nHi John, sorry as I didn't check my code carefully. I've deleted the comment since there was a bug in my code. My results show that the result between tf.image.resize and cv2.resize are the same when using bi-linear interpolation. However, the speed difference is obvious. This might come from the GPU-performance-sake of the tensorflow platform. Thank you for your time.", "@johnpjf @martinwicke \r\nThank you for the fix, however, I still found the new implementation is not aligned with OpenCV under bilinear interpolation, is that desired behavior?\r\n\r\nHere is my test code:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.compat.v2 as tf_v2\r\nimport numpy as np\r\nimport cv2\r\nnp.set_printoptions(precision=3)\r\nnp.set_printoptions(suppress=True)\r\nresize_shape = (10, 10)\r\n\r\na = np.ones((1, 2, 2, 1), dtype=np.float32)\r\na[0, 0, 0, 0] = 5.0\r\na[0, 1, 1, 0] = 5.0\r\n\r\nb = tf.constant(a, dtype=tf.float32)\r\n# c = tf.image.resize_bilinear(b, resize_shape)\r\nc = tf_v2.image.resize(b, resize_shape,\r\n                       method='bilinear',\r\n                       antialias=True)\r\nd = tf_v2.image.resize(c, (5, 5),\r\n                       method='bilinear',\r\n                       antialias=True)\r\n\r\nwith tf.Session() as sess:\r\n    np_c = sess.run(c)\r\n    np_d = sess.run(d)\r\n\r\ntemp = cv2.resize(a[0], resize_shape, interpolation=cv2.INTER_LINEAR)\r\ntemp2 = cv2.resize(np_c[0, :, :, 0], (5,5), interpolation=cv2.INTER_LINEAR)\r\n\r\nprint (\"Tensorflow:\")\r\nprint (np_c[0, :, :, 0])\r\nprint (\"OpenCV:\")\r\nprint (temp)\r\nprint (\"Tensorflow:\")\r\nprint (np_d[0, :, :, 0])\r\nprint (\"OpenCV:\")\r\nprint (temp2)\r\nprint (\"Tensorflow:\")\r\nprint (np_c[0, :, :, 0] - temp)\r\nprint (\"OpenCV:\")\r\nprint (np_d[0, :, :, 0] - temp2)\r\n```\r\n\r\nHere is my output with tensorflow 1.14.0:\r\nThe upsample looks correct, but there seems to be some issue with downsample.\r\n```\r\nTensorflow:\r\n[[5.   5.   5.   4.2  3.4  2.6  1.8  1.   1.   1.  ]\r\n [5.   5.   5.   4.2  3.4  2.6  1.8  1.   1.   1.  ]\r\n [5.   5.   5.   4.2  3.4  2.6  1.8  1.   1.   1.  ]\r\n [4.2  4.2  4.2  3.72 3.24 2.76 2.28 1.8  1.8  1.8 ]\r\n [3.4  3.4  3.4  3.24 3.08 2.92 2.76 2.6  2.6  2.6 ]\r\n [2.6  2.6  2.6  2.76 2.92 3.08 3.24 3.4  3.4  3.4 ]\r\n [1.8  1.8  1.8  2.28 2.76 3.24 3.72 4.2  4.2  4.2 ]\r\n [1.   1.   1.   1.8  2.6  3.4  4.2  5.   5.   5.  ]\r\n [1.   1.   1.   1.8  2.6  3.4  4.2  5.   5.   5.  ]\r\n [1.   1.   1.   1.8  2.6  3.4  4.2  5.   5.   5.  ]]\r\nOpenCV:\r\n[[5.   5.   5.   4.2  3.4  2.6  1.8  1.   1.   1.  ]\r\n [5.   5.   5.   4.2  3.4  2.6  1.8  1.   1.   1.  ]\r\n [5.   5.   5.   4.2  3.4  2.6  1.8  1.   1.   1.  ]\r\n [4.2  4.2  4.2  3.72 3.24 2.76 2.28 1.8  1.8  1.8 ]\r\n [3.4  3.4  3.4  3.24 3.08 2.92 2.76 2.6  2.6  2.6 ]\r\n [2.6  2.6  2.6  2.76 2.92 3.08 3.24 3.4  3.4  3.4 ]\r\n [1.8  1.8  1.8  2.28 2.76 3.24 3.72 4.2  4.2  4.2 ]\r\n [1.   1.   1.   1.8  2.6  3.4  4.2  5.   5.   5.  ]\r\n [1.   1.   1.   1.8  2.6  3.4  4.2  5.   5.   5.  ]\r\n [1.   1.   1.   1.8  2.6  3.4  4.2  5.   5.   5.  ]]\r\nTensorflow:\r\n[[5.    4.5   3.    1.5   1.   ]\r\n [4.5   4.125 3.    1.875 1.5  ]\r\n [3.    3.    3.    3.    3.   ]\r\n [1.5   1.875 3.    4.125 4.5  ]\r\n [1.    1.5   3.    4.5   5.   ]]\r\nOpenCV:\r\n[[5.   4.6  3.   1.4  1.  ]\r\n [4.6  4.28 3.   1.72 1.4 ]\r\n [3.   3.   3.   3.   3.  ]\r\n [1.4  1.72 3.   4.28 4.6 ]\r\n [1.   1.4  3.   4.6  5.  ]]\r\nTensorflow:\r\n[[ 0.  0.  0.  0. -0.  0. -0.  0.  0.  0.]\r\n [ 0.  0.  0.  0. -0.  0. -0.  0.  0.  0.]\r\n [ 0.  0.  0.  0. -0.  0. -0.  0.  0.  0.]\r\n [ 0.  0.  0.  0. -0. -0. -0.  0.  0.  0.]\r\n [-0. -0. -0.  0. -0.  0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n [-0. -0. -0.  0.  0.  0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\r\nOpenCV:\r\n[[ 0.    -0.1    0.     0.1    0.   ]\r\n [-0.1   -0.155 -0.     0.155  0.1  ]\r\n [ 0.     0.     0.     0.     0.   ]\r\n [ 0.1    0.155  0.    -0.155 -0.1  ]\r\n [ 0.     0.1    0.    -0.1    0.   ]]\r\n\r\n```\r\n\r\nIf I change bilinear to bicubic, there are even more inconsistent values in outputs:\r\n\r\n```\r\nTensorflow:\r\n[[ 6.03   5.833  5.462  4.696  3.598  2.402  1.304  0.538  0.167 -0.03 ]\r\n [ 5.833  5.649  5.302  4.586  3.559  2.441  1.414  0.698  0.351  0.167]\r\n [ 5.462  5.302  5.     4.378  3.486  2.514  1.622  1.     0.698  0.538]\r\n [ 4.696  4.586  4.378  3.949  3.335  2.665  2.051  1.622  1.414  1.304]\r\n [ 3.598  3.559  3.486  3.335  3.118  2.882  2.665  2.514  2.441  2.402]\r\n [ 2.402  2.441  2.514  2.665  2.882  3.118  3.335  3.486  3.559  3.598]\r\n [ 1.304  1.414  1.622  2.051  2.665  3.335  3.949  4.378  4.586  4.696]\r\n [ 0.538  0.698  1.     1.622  2.514  3.486  4.378  5.     5.302  5.462]\r\n [ 0.167  0.351  0.698  1.414  2.441  3.559  4.586  5.302  5.649  5.833]\r\n [-0.03   0.167  0.538  1.304  2.402  3.598  4.696  5.462  5.833  6.03 ]]\r\nOpenCV:\r\n[[5.957 5.899 5.432 4.576 3.545 2.455 1.424 0.568 0.101 0.043]\r\n [5.899 5.842 5.384 4.545 3.534 2.466 1.455 0.616 0.158 0.101]\r\n [5.432 5.384 5.    4.296 3.448 2.552 1.704 1.    0.616 0.568]\r\n [4.576 4.545 4.296 3.84  3.29  2.71  2.16  1.704 1.455 1.424]\r\n [3.545 3.534 3.448 3.29  3.1   2.9   2.71  2.552 2.466 2.455]\r\n [2.455 2.466 2.552 2.71  2.9   3.1   3.29  3.448 3.534 3.545]\r\n [1.424 1.455 1.704 2.16  2.71  3.29  3.84  4.296 4.545 4.576]\r\n [0.568 0.616 1.    1.704 2.552 3.448 4.296 5.    5.384 5.432]\r\n [0.101 0.158 0.616 1.455 2.466 3.534 4.545 5.384 5.842 5.899]\r\n [0.043 0.101 0.568 1.424 2.455 3.545 4.576 5.432 5.899 5.957]]\r\nTensorflow:\r\n[[5.873 5.046 3.    0.954 0.127]\r\n [5.046 4.457 3.    1.543 0.954]\r\n [3.    3.    3.    3.    3.   ]\r\n [0.954 1.543 3.    4.457 5.046]\r\n [0.127 0.954 3.    5.046 5.873]]\r\nOpenCV:\r\n[[5.904 5.102 3.    0.898 0.096]\r\n [5.102 4.521 3.    1.479 0.898]\r\n [3.    3.    3.    3.    3.   ]\r\n [0.898 1.479 3.    4.521 5.102]\r\n [0.096 0.898 3.    5.102 5.904]]\r\nTensorflow:\r\n[[ 0.072 -0.066  0.03   0.12   0.053 -0.053 -0.12  -0.03   0.066 -0.072]\r\n [-0.066 -0.192 -0.082  0.041  0.025 -0.025 -0.041  0.082  0.192  0.066]\r\n [ 0.03  -0.082  0.     0.082  0.038 -0.038 -0.082  0.     0.082 -0.03 ]\r\n [ 0.12   0.041  0.082  0.109  0.044 -0.044 -0.109 -0.082 -0.041 -0.12 ]\r\n [ 0.053  0.025  0.038  0.044  0.018 -0.018 -0.044 -0.038 -0.025 -0.053]\r\n [-0.053 -0.025 -0.038 -0.044 -0.018  0.018  0.044  0.038  0.025  0.053]\r\n [-0.12  -0.041 -0.082 -0.109 -0.044  0.044  0.109  0.082  0.041  0.12 ]\r\n [-0.03   0.082  0.    -0.082 -0.038  0.038  0.082  0.    -0.082  0.03 ]\r\n [ 0.066  0.192  0.082 -0.041 -0.025  0.025  0.041 -0.082 -0.192 -0.066]\r\n [-0.072  0.066 -0.03  -0.12  -0.053  0.053  0.12   0.03  -0.066  0.072]]\r\nOpenCV:\r\n[[-0.031 -0.056  0.     0.056  0.031]\r\n [-0.056 -0.064  0.     0.064  0.056]\r\n [ 0.     0.     0.     0.     0.   ]\r\n [ 0.056  0.064 -0.    -0.064 -0.056]\r\n [ 0.031  0.056  0.    -0.056 -0.031]]\r\n\r\n```\r\n\r\nAm I missing something?", "For bilinear you are using antialias=True for tensorflow, which enlarges the kernel when downsampling to antialias, this is why your downsampling version is different in TF.\r\nFor bicubic you also have antialias=True, but the upsampling version is also different. This looks like it's from a different choice of the parameter in bicubic kernel https://en.wikipedia.org/wiki/Bicubic_interpolation#Bicubic_convolution_algorithm\r\nTF uses a=0.5, while opencv uses a=0.75.", "Seems bilinear upsampling also can be done with transposed convolution:\r\nhttp://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/\r\n\r\nAlso here is some great explanation of resize in tensorflow:\r\nhttps://jricheimer.github.io/tensorflow/2019/02/11/resize-confusion/\r\n\r\nSome more related coreml:\r\nhttps://machinethink.net/blog/coreml-upsampling/", "Hello,\r\n\r\nCould anybody please tell me if it's totally safe to use `tf.image.resize` in TF 2.0 now?\r\n\r\nThank you very much in advance!", "Yes, tf_v2.image.resize works correctly.", "@johnpjf Perfect. Thanks a lot!", "Hi,\r\nCould anyone please tell me if it is safe to use tf.image.resize_bilinear in tf 1.12?", "@MingxiLi \r\nIn tf 1.12 I believe you would need to use the tf 2.0 version, so something like this would match resize_bilinear:\r\n```\r\ntf.compat.v2.image.resize(\r\n        image, new_shape,\r\n    )\r\n```\r\nsince the defaults for resize are:\r\n\r\n`method=tf.compat.v2.image.ResizeMethod.BILINEAR, antialias=False,`", "Also, UpSampling2D in tf.keras sets align_corner=False as default. Even worse, it doesn't allow you to modify it from the high level api. ", "@brucechou1983 \r\nWhy do you want to set align_corner=True in UpSampling2D?", "@brucechou1983 you can use tensorflow resize with align_corner=True and wrap it inside a lambda layer in keras or tf.keras", "I know I am in the minority here, but I have an application where I actually want the resizing to align the corners like it does in tf.compat.v1.image.resize_bilinear(..., align_corners=True).  I know I can access the old code by using tf.compat.v1.image.resize_bilinear( ), but I am concerned that someday that API compatibility will eventually go away, and I cannot figure out how to duplicate the old functionality using tf.image.resize( ) in version 2.0 or higher.\r\n\r\nCan someone help me figure out how to use the new tf.image.resize( ) function and duplicate the processing in the old tf.compat.v1.image.resize_bilinear( ) function with align_corners=True?\r\n\r\nHere is my code:\r\n```python\r\n!conda list tensor*\r\nimport tensorflow as tf\r\nimport tensorflow.compat.v1 as tf_v1\r\nimport numpy as np\r\nimport cv2\r\n\r\nnp.set_printoptions(precision=3)\r\nnp.set_printoptions(suppress=True)\r\n\r\nresize_shape = (9,9)\r\n\r\na = np.ones((1, 2, 2, 1), dtype=np.float32)\r\na[0, 0, 0, 0] = 5.0\r\na[0, 1, 1, 0] = 5.0\r\n\r\nb = tf.constant(a, dtype=tf.float32)\r\n\r\nc1 = tf_v1.image.resize_bilinear(b, resize_shape, align_corners=True)\r\nc2 = tf.image.resize(b, resize_shape, method='bilinear', antialias=False)\r\n\r\nc1 = c1.numpy()\r\nc2 = c2.numpy()\r\n\r\nprint (\"\\nInput\")\r\nprint (a[0, :, :, 0])\r\nprint (\"Tensorflow 1:\")\r\nprint (c1[0, :, :, 0])\r\nprint (\"Tensorflow 2:\")\r\nprint (c2[0, :, :, 0])\r\n```\r\nAnd here is the output.  I want to get the output from \"Tensorflow 1\" using tf.image.resize( ) in tf 2.0:\r\n\r\n```\r\n# packages in environment at /home/dcouwenh/.conda/envs/hornet2:\r\n#\r\n# Name                    Version                   Build  Channel\r\ntensorboard               2.0.0              pyhb230dea_0  \r\ntensorflow                2.0.0           gpu_py36h6b29c10_0  \r\ntensorflow-base           2.0.0           gpu_py36h0ec5d1f_0  \r\ntensorflow-estimator      2.0.0              pyh2649769_0  \r\ntensorflow-gpu            2.0.0                h0d30ee6_0  \r\n\r\nInput\r\n[[5. 1.]\r\n [1. 5.]]\r\nTensorflow 1:\r\n[[5.    4.5   4.    3.5   3.    2.5   2.    1.5   1.   ]\r\n [4.5   4.125 3.75  3.375 3.    2.625 2.25  1.875 1.5  ]\r\n [4.    3.75  3.5   3.25  3.    2.75  2.5   2.25  2.   ]\r\n [3.5   3.375 3.25  3.125 3.    2.875 2.75  2.625 2.5  ]\r\n [3.    3.    3.    3.    3.    3.    3.    3.    3.   ]\r\n [2.5   2.625 2.75  2.875 3.    3.125 3.25  3.375 3.5  ]\r\n [2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.   ]\r\n [1.5   1.875 2.25  2.625 3.    3.375 3.75  4.125 4.5  ]\r\n [1.    1.5   2.    2.5   3.    3.5   4.    4.5   5.   ]]\r\nTensorflow 2:\r\n[[5.    5.    4.778 3.889 3.    2.111 1.222 1.    1.   ]\r\n [5.    5.    4.778 3.889 3.    2.111 1.222 1.    1.   ]\r\n [4.778 4.778 4.58  3.79  3.    2.21  1.42  1.222 1.222]\r\n [3.889 3.889 3.79  3.395 3.    2.605 2.21  2.111 2.111]\r\n [3.    3.    3.    3.    3.    3.    3.    3.    3.   ]\r\n [2.111 2.111 2.21  2.605 3.    3.395 3.79  3.889 3.889]\r\n [1.222 1.222 1.42  2.21  3.    3.79  4.58  4.778 4.778]\r\n [1.    1.    1.222 2.111 3.    3.889 4.778 5.    5.   ]\r\n [1.    1.    1.222 2.111 3.    3.889 4.778 5.    5.   ]]\r\n```\r\n", "@dcouwenh There are valid use cases that align the center of the corner pixels. I have a change that exposes a \"ScaleAndTranslateOp\" that underlies the new resize, so you can use that when we merge it in (soon).\r\n\r\nUntil then you can use the v1 version, or you can access the low level scale and translate op as a workaround:\r\nhttps://codeclimate.com/github/tensorflow/tensorflow/tensorflow/python/ops/image_ops_impl.py/source\r\nat line 1510.", "That op should also be available in `tf.raw_ops`.\n", "@johnpjf Thanks for your reply.  I edited my code to call the scale_and_translate op directly as you suggested, but I am not able to reproduce the results from the v1 resize_bilinear op.  I am not trying to align the centers of the corner pixels, I am trying to align the corners of the corner pixels (like the v1 code did with align_corners=True).\r\n\r\nI tried to play with the scale and translate parameters, but no matter what I tried I get blocks of pixels in the corners that have a constant value.  In my application, I have a vector field that I want to smoothly vary over the entire area of the output image (like the Tensorflow 1 output shows), and not have the constant value areas in the corners shown in the Tensorflow 2 and scale_and_translate outputs.\r\n\r\nHere is my edited code:\r\n```python\r\n!conda list tensor*\r\nimport tensorflow as tf\r\nimport tensorflow.compat.v1 as tf_v1\r\nimport numpy as np\r\nfrom tensorflow.python.ops import gen_image_ops\r\n\r\nnp.set_printoptions(precision=3)\r\nnp.set_printoptions(suppress=True)\r\n\r\nresize_shape = (9,9)\r\n\r\na = np.ones((1, 2, 2, 1), dtype=np.float32)\r\na[0, 0, 0, 0] = 5.0\r\na[0, 1, 1, 0] = 5.0\r\n\r\nb = tf.constant(a, dtype=tf.float32)\r\n\r\nc1 = tf_v1.image.resize_bilinear(b, resize_shape, align_corners=True)\r\nc2 = tf.image.resize(b, resize_shape, method='bilinear', antialias=False)\r\n\r\nscale = [9/2,9/2]\r\ntranslate = [0,0]\r\nc3 = gen_image_ops.scale_and_translate(\r\n          b,\r\n          resize_shape,\r\n          scale,\r\n          translate,\r\n          kernel_type='triangle',\r\n          antialias=False)\r\n\r\nc1 = c1.numpy()\r\nc2 = c2.numpy()\r\nc3 = c3.numpy()\r\n\r\nprint (\"\\nInput\")\r\nprint (a[0, :, :, 0])\r\nprint (\"Tensorflow 1:\")\r\nprint (c1[0, :, :, 0])\r\nprint (\"Tensorflow 2:\")\r\nprint (c2[0, :, :, 0])\r\nprint (\"scale_and_translate:\")\r\nprint (c3[0, :, :, 0])\r\n```\r\nAnd here is the updated output.  The direct call to scale_and_translate still has the duplicate values in the corners; I am trying to get the Tensorflow 1 output which has linearly varying values coming away from the corners.\r\n\r\n```\r\n# packages in environment at /home/dcouwenh/.conda/envs/hornet2:\r\n#\r\n# Name                    Version                   Build  Channel\r\ntensorboard               2.2.1              pyh532a8cf_0  \r\ntensorboard-plugin-wit    1.6.0                      py_0  \r\ntensorflow                2.1.0           gpu_py37h7a4bb67_0  \r\ntensorflow-base           2.1.0           gpu_py37h6c5654b_0  \r\ntensorflow-estimator      2.1.0              pyhd54b08b_0  \r\ntensorflow-gpu            2.1.0                h0d30ee6_0  \r\n\r\nInput\r\n[[5. 1.]\r\n [1. 5.]]\r\nTensorflow 1:\r\n[[5.    4.5   4.    3.5   3.    2.5   2.    1.5   1.   ]\r\n [4.5   4.125 3.75  3.375 3.    2.625 2.25  1.875 1.5  ]\r\n [4.    3.75  3.5   3.25  3.    2.75  2.5   2.25  2.   ]\r\n [3.5   3.375 3.25  3.125 3.    2.875 2.75  2.625 2.5  ]\r\n [3.    3.    3.    3.    3.    3.    3.    3.    3.   ]\r\n [2.5   2.625 2.75  2.875 3.    3.125 3.25  3.375 3.5  ]\r\n [2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.   ]\r\n [1.5   1.875 2.25  2.625 3.    3.375 3.75  4.125 4.5  ]\r\n [1.    1.5   2.    2.5   3.    3.5   4.    4.5   5.   ]]\r\nTensorflow 2:\r\n[[5.    5.    4.778 3.889 3.    2.111 1.222 1.    1.   ]\r\n [5.    5.    4.778 3.889 3.    2.111 1.222 1.    1.   ]\r\n [4.778 4.778 4.58  3.79  3.    2.21  1.42  1.222 1.222]\r\n [3.889 3.889 3.79  3.395 3.    2.605 2.21  2.111 2.111]\r\n [3.    3.    3.    3.    3.    3.    3.    3.    3.   ]\r\n [2.111 2.111 2.21  2.605 3.    3.395 3.79  3.889 3.889]\r\n [1.222 1.222 1.42  2.21  3.    3.79  4.58  4.778 4.778]\r\n [1.    1.    1.222 2.111 3.    3.889 4.778 5.    5.   ]\r\n [1.    1.    1.222 2.111 3.    3.889 4.778 5.    5.   ]]\r\nscale_and_translate:\r\n[[5.    5.    4.778 3.889 3.    2.111 1.222 1.    1.   ]\r\n [5.    5.    4.778 3.889 3.    2.111 1.222 1.    1.   ]\r\n [4.778 4.778 4.58  3.79  3.    2.21  1.42  1.222 1.222]\r\n [3.889 3.889 3.79  3.395 3.    2.605 2.21  2.111 2.111]\r\n [3.    3.    3.    3.    3.    3.    3.    3.    3.   ]\r\n [2.111 2.111 2.21  2.605 3.    3.395 3.79  3.889 3.889]\r\n [1.222 1.222 1.42  2.21  3.    3.79  4.58  4.778 4.778]\r\n [1.    1.    1.222 2.111 3.    3.889 4.778 5.    5.   ]\r\n [1.    1.    1.222 2.111 3.    3.889 4.778 5.    5.   ]]", "You can figure out what  the params should be writing the  equations out, so for your example you want the pixel at position 0.5 to stay at 0.5, and the  one at 1.5  to transform to 8.5,\r\nso:\r\n```\r\n0.5 = s * 0.5  + t\r\n8.5 = s * 1.5 + t\r\n```\r\nSolving gives:\r\n```\r\ns = 8, t = -3.5\r\n```\r\nso:\r\n```\r\nscale = [8, 8]\r\ntranslate = [-3.5, -3.5]\r\nc3 = tf.raw_ops.ScaleAndTranslate(\r\n          images=b,\r\n          size=resize_shape,\r\n          scale=scale,\r\n          translation=translate,\r\n          kernel_type='triangle',\r\n          antialias=False)\r\n```\r\nMatches the tf1  behavior.\r\n", "@johnpjf Excellent -- thank you!  This is exactly what I was looking for.\r\n\r\nFor completeness, I have edited my code to include your solution for arbitrary input and output shapes.\r\n\r\n```python\r\n!conda list tensor*\r\nimport tensorflow as tf\r\nimport tensorflow.compat.v1 as tf_v1\r\nimport numpy as np\r\nfrom tensorflow.python.ops import gen_image_ops\r\n\r\nnp.set_printoptions(precision=3)\r\nnp.set_printoptions(suppress=True)\r\n\r\nHi,Wi = (3,3)\r\nHo,Wo = (10,9)\r\n\r\na = np.arange(Hi*Wi).reshape(1, Hi, Wi, 1)\r\n\r\nb = tf.constant(a, dtype=tf.float32)\r\n\r\nc1 = tf_v1.image.resize_bilinear(b, (Ho,Wo), align_corners=True)\r\nc2 = tf.image.resize(b, (Ho,Wo), method='bilinear', antialias=False)\r\n\r\nsy = (Ho-1)/(Hi-1)\r\nsx = (Wo-1)/(Wi-1)\r\nscale = [sy,sx]\r\ntx = (1-sx)/2\r\nty = (1-sy)/2\r\ntranslate = [ty,tx]\r\nc3 = gen_image_ops.scale_and_translate(\r\n          b,\r\n          (Ho,Wo),\r\n          (sy,sx),\r\n          (ty,tx),\r\n          kernel_type='triangle',\r\n          antialias=False)\r\n\r\nc1 = c1.numpy()\r\nc2 = c2.numpy()\r\nc3 = c3.numpy()\r\n\r\nprint (\"\\nInput\")\r\nprint (a[0, :, :, 0])\r\nprint (\"Tensorflow 1:\")\r\nprint (c1[0, :, :, 0])\r\nprint (\"Tensorflow 2:\")\r\nprint (c2[0, :, :, 0])\r\nprint (\"scale_and_translate:\")\r\nprint (c3[0, :, :, 0])\r\n```\r\n\r\nAnd here is the output:\r\n```\r\n# packages in environment at /home/dcouwenh/.conda/envs/hornet2:\r\n#\r\n# Name                    Version                   Build  Channel\r\ntensorboard               2.2.1              pyh532a8cf_0  \r\ntensorboard-plugin-wit    1.6.0                      py_0  \r\ntensorflow                2.1.0           gpu_py37h7a4bb67_0  \r\ntensorflow-base           2.1.0           gpu_py37h6c5654b_0  \r\ntensorflow-estimator      2.1.0              pyhd54b08b_0  \r\ntensorflow-gpu            2.1.0                h0d30ee6_0  \r\n\r\nInput\r\n[[0 1 2]\r\n [3 4 5]\r\n [6 7 8]]\r\nTensorflow 1:\r\n[[0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.   ]\r\n [0.667 0.917 1.167 1.417 1.667 1.917 2.167 2.417 2.667]\r\n [1.333 1.583 1.833 2.083 2.333 2.583 2.833 3.083 3.333]\r\n [2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.   ]\r\n [2.667 2.917 3.167 3.417 3.667 3.917 4.167 4.417 4.667]\r\n [3.333 3.583 3.833 4.083 4.333 4.583 4.833 5.083 5.333]\r\n [4.    4.25  4.5   4.75  5.    5.25  5.5   5.75  6.   ]\r\n [4.667 4.917 5.167 5.417 5.667 5.917 6.167 6.417 6.667]\r\n [5.333 5.583 5.833 6.083 6.333 6.583 6.833 7.083 7.333]\r\n [6.    6.25  6.5   6.75  7.    7.25  7.5   7.75  8.   ]]\r\nTensorflow 2:\r\n[[0.    0.    0.333 0.667 1.    1.333 1.667 2.    2.   ]\r\n [0.    0.    0.333 0.667 1.    1.333 1.667 2.    2.   ]\r\n [0.75  0.75  1.083 1.417 1.75  2.083 2.417 2.75  2.75 ]\r\n [1.65  1.65  1.983 2.317 2.65  2.983 3.317 3.65  3.65 ]\r\n [2.55  2.55  2.883 3.217 3.55  3.883 4.217 4.55  4.55 ]\r\n [3.45  3.45  3.783 4.117 4.45  4.783 5.117 5.45  5.45 ]\r\n [4.35  4.35  4.683 5.017 5.35  5.683 6.017 6.35  6.35 ]\r\n [5.25  5.25  5.583 5.917 6.25  6.583 6.917 7.25  7.25 ]\r\n [6.    6.    6.333 6.667 7.    7.333 7.667 8.    8.   ]\r\n [6.    6.    6.333 6.667 7.    7.333 7.667 8.    8.   ]]\r\nscale_and_translate:\r\n[[0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.   ]\r\n [0.667 0.917 1.167 1.417 1.667 1.917 2.167 2.417 2.667]\r\n [1.333 1.583 1.833 2.083 2.333 2.583 2.833 3.083 3.333]\r\n [2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.   ]\r\n [2.667 2.917 3.167 3.417 3.667 3.917 4.167 4.417 4.667]\r\n [3.333 3.583 3.833 4.083 4.333 4.583 4.833 5.083 5.333]\r\n [4.    4.25  4.5   4.75  5.    5.25  5.5   5.75  6.   ]\r\n [4.667 4.917 5.167 5.417 5.667 5.917 6.167 6.417 6.667]\r\n [5.333 5.583 5.833 6.083 6.333 6.583 6.833 7.083 7.333]\r\n [6.    6.25  6.5   6.75  7.    7.25  7.5   7.75  8.   ]]\r\n```"]}, {"number": 6719, "title": "Branch 143877115", "body": "", "comments": []}, {"number": 6718, "title": "Typos in docs for expand_dims", "body": "`dim` has been replaced by `axis` in the documentation.\r\n\r\n(https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#expand_dims)", "comments": ["Perhaps you have an older version of TensorFlow, it's been renamed to axis in 0.12\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'0.12.1'\r\n>>> tf.expand_dims([1], axis=1)\r\n<tf.Tensor 'ExpandDims:0' shape=(1, 1) dtype=int32>\r\n\r\n```", "The typo is:\r\n```\r\nshape(expand_axiss(t, 0)) ==> [1, 2]\r\n```\r\nIt appears several times.", "I see. It seems to have been fixed in master:\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/array_ops/shapes_and_shaping#expand_dims"]}, {"number": 6717, "title": "Incorrect gradient for categorical distribution entropy", "body": "The **Categorical** distribution class provides an awesome **entropy** operator but apparently the **gradient** calculation w.r.t. the input operators **doesn't work**.\r\n\r\n```python\r\nlogits = tf.Variable(initial_value=[[1., 2., 3.], [2., 5., 1.]])\r\n\r\nprobabilities = tf.nn.softmax(logits)\r\nlog_probabilities = tf.nn.log_softmax(logits)\r\nentropy = - tf.reduce_sum(probabilities * log_probabilities, axis=-1)\r\n\r\n# using the actual distribution would be nicer but gradients seem buggy\r\ncategorical_distribution = tf.contrib.distributions.Categorical(p=probabilities)\r\ncategorical_distribution_entropy = categorical_distribution.entropy()\r\n\r\n# initialize\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# works\r\nprint(sess.run(entropy))\r\nprint(sess.run(tf.gradients(entropy, [logits])))\r\n\r\n# apparently loses gradient information\r\nprint(sess.run(categorical_distribution_entropy))\r\nprint(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))\r\n```\r\n\r\nIn the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.\r\n```\r\n[ 0.83239555  0.27431309]\r\n[array([[ 0.14181709,  0.14077036, -0.28258738],\r\n       [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]\r\n[ 0.83239555  0.27431309]\r\n[array([[ 0.,  0.,  0.],\r\n       [ 0.,  0.,  0.]], dtype=float32)]\r\n```", "comments": ["I suspect this a numerical stability issue. [Looking](https://gist.github.com/yaroslavvb/97504b8221a8529e7a51a50915206d68) at your \"working\" graph, it uses `LogSoftmaxGrad`. Whereas in the categorical distribution, it computes `SoftmaxGrad` and combines it with `LogGrad` @ebrevdo \r\n\r\n<img width=\"389\" alt=\"screenshot 2017-01-08 18 31 59\" src=\"https://cloud.githubusercontent.com/assets/23068/21756041/dd734ea4-d5d0-11e6-82d0-001d9fab5bea.png\">\r\n", "Taking a look.", "For numerical stability, we use tf.nn.softmax_cross_entropy_with_logits to calculate the entropy.  I just found out that this operation does not perform backprop with respect to the \"labels\" argument (the exponentiated probabilities term of the entropy).  Thus the zeros.\r\n\r\nProbably the best way to fix this is for us to implement the numerically stable equivalent of your reduce_sum.", "Fix going into master hopefully today / tomorrow.", "thank you for the fix !\r\ni guess we can close this then"]}, {"number": 6716, "title": "Feature request: easier access to tensor de-allocation information", "body": "TLDR; to debug TensorFlow out-of-memory situations one needs to see tensor de-allocation info.\r\n\r\nYou can see allocation stats in timeline, but without de-allocation info you can't calculate peak memory. Currently getting peak memory is possible by:\r\n\r\n1. Hacking TensorFlow to print deallocation messages with timestamps as [here](https://github.com/yaroslavvb/tensorflow/commit/5d4cd97c0a73e91ee37c025cd7a62fb46aae76a0)\r\n\r\n2. A bunch of regular expression to parse `__LOG_MEMORY__` messages as in [here](https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb)\r\n\r\nSince this needs building your own version of tensorflow, this is not accessible to most people. Perhaps this can be remedied by adding deallocation events to session run timeline? @michaelisard \r\n\r\nSome recent places this issue came up:\r\nhttp://stackoverflow.com/questions/41517145/outer-product-based-conv-filters-consume-disproportionately-high-memory\r\nhttp://stackoverflow.com/questions/41496251/fitting-large-matrix-calculations-into-memory-when-using-tensorflow\r\nhttp://stackoverflow.com/questions/41451273/tensorflow-specifying-storage-of-layer-activations\r\nhttp://stackoverflow.com/questions/40190510/tensorflow-how-to-log-gpu-memory-vram-utilization/40197094#comment70145571_40197094\r\nhttps://github.com/tensorflow/tensorflow/issues/6019#issuecomment-268037463", "comments": ["I will take a look but it will take me a few days to get to it. The reason it isn't trivial to put it in the timelines is that deallocation is done when the refcount falls to zero, not by an explicit call at a site that has a reference to the timeline datastructure.", "This would also help with benchmarks which last week [were modified](https://github.com/tensorflow/tensorflow/commit/99d4517a) to store maximum memory usage . As far as I can tell, the memory parsing from timeline just adds up all allocations, so doesn't actually represent peak memory @ebrevdo ", "The [timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) code does try to figure out the deallocations and compute peak usage, based on essentially reconstituting the graph from the logs. It would be better to do it properly, but I don't think it's as bad as you fear. @prb12 can correct me if I'm wrong.", "As @michaelisard  says, the Timeline class _does_ have some code in to try and track the refcounts of Tensors using information in the trace, but it is very unreliable since the only information it has access to is the string names of inputs to each operation (from the ASCII timeline label).  These aren't robust to graph partitioning, rewriting and constant folding etc., and this is why the memory histograms are disabled by default.\r\n\r\nNow that it is possible to retrieve the rewritten graphdefs, tf.Timeline could probably do a much better job of reference count tracking  - but as Michael says, it would be better to actually track the deallocations.  Unfortunately these can happen asynchronously at arbitrary times, so the protobuf format would need extending.   There is also the subtlety of GPU allocations and deallocations happening long before the CUDA kernels actually execute, which potentially makes the UI a little confusing. (not an issue for simply recording peak memory usage)", "More visibility into memory could help solving the mysterious issue of TF running out of memory on step xyz, even though there are no changes to graph between steps. I've ran across it, and also [here](http://stackoverflow.com/questions/41540073/why-the-gpu-memory-is-not-the-same)", "There are numerous sources of non-determinism in memory allocation (and op dispatch), but more importantly, many people forget that there are often multiple steps running at the same time (e.g. if your model involves queues), does checkpointing, etc.", "It seems hard to get accurate deallocation information even from VLOG messages.\r\n\r\nThe problem is that you can have multiple allocations with the same allocation id, but different step ids in the same `.run` call. The deallocation message does not include step_id, so it's ambiguous.\r\n\r\nHere's an example \"leak\" I spent this morning debugging\r\n\r\n```\r\n2017-01-26 10:13:30: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -6 kernel_name: \"Unknown (from Proto)\" tensor { dtype: DT_FLOAT shape { } allocation_description { requested_bytes: 4 allocated_bytes: 4 allocator_name: \"cpu\" allocation_id: 6 has_single_reference: true ptr: 5045586496 } } }\r\n\r\n2017-01-26 10:13:30: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 6 allocator_name: \"cpu\" }\r\n\r\n2017-01-26 10:13:30: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 2 kernel_name: \"a1\" tensor { dtype: DT_FLOAT shape { dim { size: 250000 } } allocation_description { requested_bytes: 1000000 allocated_bytes: 1000192 allocator_name: \"gpu_bfc\" allocation_id: 6 has_single_reference: true ptr: 30078652160 } } }\r\n\r\n2017-01-26 10:13:30: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 6 allocator_name: \"gpu_bfc\" }\r\n\r\n```", "I added some heuristics to deal with cases like above and packaged it up as standalone script at https://github.com/yaroslavvb/memory_util", "I have given this a little thought but not yet looked into the details of the implementation. It looks as if it should be ok to add the step_id to the tracked metadata, so that you won't have to use heuristics to guess it.\r\n\r\nAs for putting deallocation into the timeline, I have one idea which is a bit half-baked and has not been subjected to code review. Before I type it in, I'd like feedback on whether it would solve your problem or not.\r\n\r\nI could add a thread-local callback to the tracking allocator, such that deallocations trigger the callback. Then, when memory tracking is enabled, each Op would install a callback before it began executing, and remove it after it completed. Any tensor that was freed on that thread while the Op was executing would be added to the timeline of the Op. This would obviously fail to work if deallocations happened on an intra-op threadpool, but hopefully that is rare or nonexistent.\r\n\r\nWhat do you think?", "So the problem I'm trying to solve is that we have models that use too much memory, and in order to have confidence that our optimizations are working, we want to be able to track down \"MemoryExceeded\" error to a particular sequence of allocations and deallocations.\r\n\r\nFor your proposal, my question would be how robust it is. What do you mean mean by \"hopefully it's rare\"? If it's pretty rare, then I think it would simplify/robustify things if I could build the memory usage timeline using step-metadata instead of parsing stderr.\r\n\r\nBTW, another thing I came across with LOG_MEMORY messages is that sometimes I get deallocation messages even when there's no mention of this allocation_id in this run call. Can things be de-allocated in a run call if they weren't allocated in the same session.run call?\r\n\r\nIE, message like this\r\n\r\n`     41:2017-01-26 15:06:29: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 29330 allocator_name: \"cuda_host_bfc\" }\r\n`", "The issue would arise if temporaries got allocated by threads on the Eigen threadpool. So it shouldn't happen for GPU allocations. For CPU I haven't looked at all the ops: someone could do it, but it seems unlikely to be performance-friendly.\r\n\r\nAnything held in the resource manager may persist across run calls, and therefore be allocated and deallocated by different calls. E.g., a tensor in a queue, or a variable that is being replaced by one of a different shape. Off-hand I don't think other allocations should cross run calls, but I am not 100% sure.", "BTW, here's an example of things we are using it for, looking at timelines like attached (this is memory usage timeline for regular tf.gradients vs. sublinear memory gradients from https://arxiv.org/abs/1604.06174 for 100 node chain)\r\n\r\n<img width=\"413\" alt=\"default\" src=\"https://cloud.githubusercontent.com/assets/23068/22355705/1f1ddd88-e3e1-11e6-9888-5a5d5214af3c.png\">\r\n<img width=\"413\" alt=\"screenshot 2017-01-26 16 00 56\" src=\"https://cloud.githubusercontent.com/assets/23068/22355707/22164b74-e3e1-11e6-9cf6-148de2ebe981.png\">\r\n", "I was seeing those \"orphan\" deallocations in https://github.com/tensorflow/models/tree/master/resnet which uses queues...although `cuda_host_bfc` seems to rule that out, queues should be using CPU \r\n\r\nTo summarize, I think any change that would make memory timelines easier to build would be welcome. My current vlog memory parsing logic seems to mostly work, but there are occasionally missing bits (ie, because of missing step_id in deallocation), and it breaks when vlog output gets too large (ie, vlog=2 doesn't work because of gazillion GPU polling messages), or when protobuf text output changes, and it relies on order of stderr to be correct (we can only get second timestamps in logs, no microsecond timestamps)", "For the orphan allocation, I think the most likely is that someone is calling AllocateRaw and forgetting to call LogMemory::RecordRawAllocation at the same time. The easiest way to debug it is probably to add a log statement in TrackingAllocator::AllocateRaw. If there's an orphan deallocation of size Q, grep through for all the allocations of size Q and look for the one with no matching __LOG_MEMORY__. Then report the bug :)", "Hi @yuefengz , I see from commit messages you've been doing memory work, do you know what's the status of memory tracking here?\r\n\r\nIn particular I'm wondering the best way to get timeline of memory allocation. We've needed this functionality to use memory-efficient-gradients package that we are planning to release.\r\n\r\nI've been maintaining [memory_util](https://github.com/yaroslavvb/memory_util) for this purpose which gives a timeline of Tensor Allocations from log messages from [log_memory.cc](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/framework/log_memory.cc#L35) , but TF 1.0.1 -> TF 1.1 update seems to have broken it. Now this module prints deallocation messages with missing allocation id. ", "Hi @yaroslavvb , the memory tracking that I've implemented is mostly for recording the memory usages, include temp memory, output memory and persistent memory. This is useful for building a cost model but it does't account for tracking memory allocation and deallocation.", "relevant commit: github.com/tensorflow/tensorflow/commit/86238e8d", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 6715, "title": "Fixed wide_n_deep_tutorial.py", "body": "Fixed issue [#6648](https://github.com/tensorflow/tensorflow/issues/6648#issuecomment-270806057)\r\n\r\nChanged  ```dense_shape=[df[k].size, 1])```  to  ```shape=[df[k].size, 1])``` on line 158  of   [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6714, "title": "TFSlim: evaluate multiple validation batches using `evaluation_loop`", "body": "I'm trying to use placeholders for training with TF-Slim, as my training + evaluation data comes from outside of TF World. I've now got training working by using the workaround described in #6604 and keeping track of the last training batch to supply in the feed dict when calling `sess.run(summary_op)`.\r\n\r\nHowever, evaluation using `evaluation_loop` over multiple batches seems to not be possible. There are `summary_op_feed_dict` and `eval_op_feed_dict` arguments to the `evaluation_loop` function, but if I understand correctly, if I set `num_evals` > 1, then the same feed dict (+ data) will be used for every evaluation.\r\n\r\nI'm happy add this functionality and issue a PR \u2013 is it true that right now the ability to evaluate over multiple validation batches using placeholders is not implemented? I'm using TF 12.1.", "comments": ["I would recommend you to use [evaluate_once](https://github.com/tensorflow/tensorflow/blob/78b12941db2e1b9baee785dce1edf1e557377acc/tensorflow/contrib/training/python/training/evaluation.py#L337). \r\nTake a look at [evaluate_repeatedly](https://github.com/tensorflow/tensorflow/blob/78b12941db2e1b9baee785dce1edf1e557377acc/tensorflow/contrib/training/python/training/evaluation.py#L434) to see how to use evaluate_once", "Thanks for the help @sguada! However, I'm still not sure whether that solves the underlying issue of the `feed_dict` being a constant parameter to `evaluate_once` as far as I can see. In the function, there are the following lines:\r\n\r\n```\r\nwhile not session.should_stop():\r\n        session.run(eval_ops, feed_dict)\r\n```\r\n\r\nAnd so until the `should_stop` flag is set, the same `feed_dict` will be used for every evaluation?\r\n\r\nIn the docstring of `evaluate_repeatedly` there is a comment that reads:\r\n\r\n> _`evaluate_once` creates a local variable used to track the number of evaluations run via `tf.contrib.training.get_or_create_eval_step`_\r\n\r\nPerhaps this could be used in some way? Again, happy to contribute if it turns out there is something is missing, though perhaps I'm misunderstanding?\r\n\r\nThanks so much for your time!", "If you want to change feed_dict in every run, you can use a hook. Example is as follows:\r\n```\r\nclass MyFeedHook(session_run_hook.SessionRunHook):\r\n  def before_run(self, run_context): \r\n    return session_run_hook.SessionRunArgs(\r\n        fetches=None, feed_dict=my-feed-dict-for-this-step)\r\n```", "@kencoken, given the inactivity here It sounds like that solved your problem."]}, {"number": 6713, "title": "Queues don't support enqueuing SparseTensor?", "body": "It seems that right now RandomShuffleQueue doesn't support SparseTensor. When I run this minimal example\r\n```\r\nimport tensorflow as tf\r\nvalues = tf.constant([0, 1, 2], dtype=tf.int32)\r\nindices = tf.constant([[0, 0], [0, 1], [0, 2]], dtype=tf.int64)\r\nshape = tf.constant([1, 3], dtype=tf.int64)\r\n\r\nsparse = tf.SparseTensor(values=values, indices=indices, shape=shape)\r\nqueue = tf.RandomShuffleQueue(2, 1, [tf.int32])\r\nqueue.enqueue([sparse])\r\n```\r\nI get the following error:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-a8e6053e53cf> in <module>()\r\n----> 1 queue.enqueue([sparse])\r\n\r\n/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py in enqueue(self, vals, name)\r\n    320     with ops.name_scope(name, \"%s_enqueue\" % self._name,\r\n    321                         self._scope_vals(vals)) as scope:\r\n--> 322       vals = self._check_enqueue_dtypes(vals)\r\n    323 \r\n    324       # NOTE(mrry): Not using a shape function because we need access to\r\n\r\n/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py in _check_enqueue_dtypes(self, vals)\r\n    275     for i, (val, dtype) in enumerate(zip(vals, self._dtypes)):\r\n    276       tensors.append(ops.convert_to_tensor(val, dtype=dtype,\r\n--> 277                                            name=\"component_%d\" % i))\r\n    278 \r\n    279     return tensors\r\n\r\n/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    667 \r\n    668         if ret is None:\r\n--> 669           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    670 \r\n    671         if ret is NotImplemented:\r\n\r\n/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    174                                          as_ref=False):\r\n    175   _ = as_ref\r\n--> 176   return constant(v, dtype=dtype, name=name)\r\n    177 \r\n    178 \r\n\r\n/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    163   tensor_value = attr_value_pb2.AttrValue()\r\n    164   tensor_value.tensor.CopyFrom(\r\n--> 165       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    166   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    167   const_tensor = g.create_op(\r\n\r\n/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    365       nparray = np.empty(shape, dtype=np_dt)\r\n    366     else:\r\n--> 367       _AssertCompatible(values, dtype)\r\n    368       nparray = np.array(values, dtype=np_dt)\r\n    369       # check to them.\r\n\r\n/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)\r\n    300     else:\r\n    301       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\r\n--> 302                       (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n    303 \r\n    304 \r\n\r\nTypeError: Expected int32, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x105fedef0> of type 'SparseTensor' instead.\r\n```", "comments": ["The main reason I needed this is because shuffle_batch doesn't support dynamic_pad for now ([#5147](https://github.com/tensorflow/tensorflow/issues/5147)). So I used the following [piece of code](https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-271086206). But RandomShuffleQueue doesn't support enqueuing SparseTensors directly. I worked this around by enqueuing _values_, _indices_ and _shape_ instead of the whole SparseTensor and reconstructing SparseTensor after dequeue op. ", "tf.train.shuffle_batch supports SparseTensor, you can use that. It goes the work for you.", "@ebrevdo what about if we want to create queue with different buckets? I'm following the example here: [http://programtalk.com/vs2/python/13142/deep_recommend_system/java_predict_client/src/main/proto/tensorflow/contrib/training/python/training/bucket_ops_test.py/](url)\r\n\r\nIt all looks good, except that my data is a sparse array, e.g.\r\nsources = [[1], [2, 0, 3], [3,2]]\r\ntargets = [[2. 3 4], [1,2], [2,3,4,5]]\r\n\r\nFirst I tried using `convert_to_tensor` but I assume this would not work well with sparse arrays so I started exploring using the SparseTensor, however looks like transforming the data into a SparseTensor won't help much since it cannot be fead to the RandomShuffleQueue. Would the `tf.train.shuffle_batch` be a good substitute? What's the best approach to add `slice_input_producer` in that case? I noticed that it doesn't work with SparseTensor\r\n`        # load train corpus\r\n         sources, targets, lengths, validation_sources, validation_targets, validation_lengths = self._load_corpus()\r\n \r\n         print(\"Finished corpus loading\")\r\n \r\n         # to a constant tensor\r\n         source = tf.convert_to_tensor(sources)\r\n         target = tf.convert_to_tensor(targets)\r\n         length = tf.convert_to_tensor(lengths)\r\n \r\n         input_queue = tf.RandomShuffleQueue(batch_size*64, # capacity\r\n                                             batch_size*32, # min_after_dequeue\r\n                                             tf.int32, name=name)\r\n         input_queue = input_queue.enqueue((length, source, targets))\r\n \r\n         lengths_t, sources_t, targets_t = input_queue.dequeue()\r\n \r\n \r\n         source_batch, target_batch = self.shuffle_bucket_batch(\r\n              length_t, [sources_t, targets_t],\r\n              batch_size=batch_size,\r\n              bucket_boundaries=self.BUCKET_BOUNDARIES,\r\n              # this will bad the source_batch and target_batch independently\r\n              dynamic_pad=True,\r\n              capacity=batch_size*64,\r\n              num_threads=32,\r\n              allow_smaller_final_batch=False, name=name\r\n          )\r\n`", "I have a tf.train.shuffle_batch from which I retrieve batches (sparse tensors) and would like to enqueue them into a FIFO queue. However, this issue still doesn't seem to have been fixed. I get the same error. "]}, {"number": 6712, "title": "Installed succesfully, but went wrong and outputed  \u2018undefined symbol: zgelsd_\u2019 when testing", "body": "### Environment info\r\nOperating System: Ubuntu14.04, python3.4.3\r\n\r\n**I installed tensorflow using pip3 succesfully** from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34-cp34m-linux_x86_64.whl\r\n\r\n**When I tried to test it using offical test code, it went wrong as follows:**\r\n/usr/bin/python3.4 /home/daisy/PycharmProjects/miachaelLiang/test_tensorflow.py\r\nTraceback (most recent call last):\r\n  File \"/home/daisy/PycharmProjects/miachaelLiang/test_tensorflow.py\", line 9, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 44, in <module>\r\n    import numpy as np\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/__init__.py\", line 142, in <module>\r\n    from . import add_newdocs\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/add_newdocs.py\", line 13, in <module>\r\n    from numpy.lib import add_newdoc\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/lib/__init__.py\", line 18, in <module>\r\n    from .polynomial import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/lib/polynomial.py\", line 20, in <module>\r\n    from numpy.linalg import eigvals, lstsq, inv\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/linalg/__init__.py\", line 51, in <module>\r\n    from .linalg import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/linalg/linalg.py\", line 29, in <module>\r\n    from numpy.linalg import lapack_lite, _umath_linalg\r\n### ImportError:\r\n **/usr/local/lib/python3.4/dist-packages/numpy/linalg/lapack_lite.cpython-34m.so: undefined symbol: zgelsd_**\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["This looks to me like your numpy installation is broken. Numpy appears installed by linked incorrectly to lapack. Not much we can do about this in TensorFlow."]}, {"number": 6711, "title": "Android demo accuracy", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI searched #1269 #504 \r\n\r\n### Environment info\r\nMac OS for build and Android version 5 to run .apk demo.   \r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI followed the steps mentioned in #1269 and could able to run the example successfully, but the accuracy of the result is very low and often wrong. I have trained my systems on 25 different daily used products like soap, soup, noodles, etc. \r\nWhere as when i run the same example using following script it give me very high accuracy (approx. 90-95%)\r\n\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\n// change this as you see fit\r\nimage_path = sys.argv[1]\r\n\r\n// Read in the image_data\r\nimage_data = tf.gfile.FastGFile(image_path, 'rb').read()\r\n\r\n// Loads label file, strips off carriage return\r\nlabel_lines = [line.rstrip() for line \r\n                   in tf.gfile.GFile(\"/tf_files/retrained_labels.txt\")]\r\n\r\n// Unpersists graph from file\r\nwith tf.gfile.FastGFile(\"/tf_files/retrained_graph.pb\", 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n\r\nwith tf.Session() as sess:\r\n    // Feed the image_data as input to the graph and get first prediction\r\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\r\n    \r\n    predictions = sess.run(softmax_tensor, \\\r\n             {'DecodeJpeg/contents:0': image_data})\r\n    \r\n    // Sort to show labels of first prediction in order of confidence\r\n    top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\r\n    \r\n    for node_id in top_k:\r\n        human_string = label_lines[node_id]\r\n        score = predictions[0][node_id]\r\n        print('%s (score = %.5f)' % (human_string, score))\r\n```\r\n\r\nThe only difference i see here is that the model file used in the android demo is stripped because it does not support DecodeJpeg, whereas in the above code its the actually generated unstripped model. Is there any specific reason or somewhere i am wrong here? \r\n\r\n### What other attempted solutions have you tried?\r\nYes, i the above script and it gives me quite high accuracy result. \r\n", "comments": ["This type of usage question is better asked on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow).", "@milinddeore It's worth checking that you're not running into https://github.com/tensorflow/tensorflow/issues/4911, which can affect some Android 5.0 devices. You can use the volume keys to turn on debug mode and view the image that's actually being sent to the classifier."]}, {"number": 6710, "title": "Update jpeg.BUILD", "body": "Fixes #6706", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "It looks like the issue is caused by the change here:\r\n```\r\nERROR: /workspace/tensorflow/core/platform/default/build_config/BUILD:108:1: error loading package '@jpeg//': Encountered error while reading extension file 'third_party/common.bzl': no such package '@org_tensorflow//third_party': error loading package 'external': The repository named 'org_tensorflow' could not be resolved and referenced by '//tensorflow/core/platform/default/build_config:jpeg'.\r\n```\r\n@jart @damienmg Could you take a look at this change?", "Even though this change is correct, it's not possible to make due to https://github.com/bazelbuild/bazel/issues/1248.", "@jart should I remove the `load` statement, and simply redefine the routine in jpeg.BUILD and llvm.BUILD for now? I can add a TODO to fix this later, and we will unblock @yaroslavvb ", "Yes that's probably the wisest course of action right now, per discussion in related issue.", "Uh oh, it seems to failing the build.", "I have contacted bazel team to see how we can work around the problem.\r\nIn the meantime, I think we should close this PR as we will not be able to accept it."]}, {"number": 6709, "title": "can't find zlib 1.2.8", "body": "native.new_http_archive(\r\n    name = \"zlib_archive\",\r\n    url = \"http://zlib.net/zlib-1.2.8.tar.gz\",\r\n    sha256 = \"36658cb768a54c1d4dec43c3116c27ed893e88b02ecfcb44f2166f9c0b7f2a0d\",\r\n    strip_prefix = \"zlib-1.2.8\",\r\n    build_file = str(Label(\"//:zlib.BUILD\")),\r\n)\r\n\r\n1.2.10 is released", "comments": ["Fixed in https://github.com/tensorflow/tensorflow/commit/1e317b1f7dc5ccf04fc51ac96d97f5bdaefa9af9"]}, {"number": 6708, "title": "Test fix cherrypicks into release branch.", "body": "", "comments": ["Failure related to an infra flake.\r\nJenkins, test this please."]}, {"number": 6707, "title": "Fix: Check bazel version before trying to setup the workspace.", "body": "I could not prevent all the other errors from showing up, but this way we at least get the bazel version error message at the end:\r\n\r\n```\r\n...\r\nERROR: /tensorflow/tensorflow/workspace.bzl:351:3: //external:zlib_archive: no such attribute 'urls' in 'new_http_archive' rule.\r\nERROR: /tensorflow/tensorflow/workspace.bzl:351:3: //external:zlib_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.\r\nERROR: /tensorflow/WORKSPACE:6:1: Traceback (most recent call last):\r\n\tFile \"/tensorflow/WORKSPACE\", line 6\r\n\t\tcheck_version(\"0.4.2\")\r\n\tFile \"/tensorflow/tensorflow/workspace.bzl\", line 32, in check_version\r\n\t\tfail(\"\r\nCurrent Bazel version is {}, e...))\r\n\r\nCurrent Bazel version is 0.3.2, expected at least 0.4.2\r\n.\r\nERROR: Error evaluating WORKSPACE file.\r\nERROR: error loading package 'external': Package 'external' contains errors.\r\nERROR: missing fetch expression. Type 'bazel help fetch' for syntax and help.\r\n```", "comments": ["Nice, do you know why the order matters?", "I think before we were blocked by tf_workspace() call below.\r\nWhen loading the functions, I think they are non-fatal error messages, but just like what check_version does, tf_workspace did break up all evaluation. The errors stem from addition of mirrors to all the dependency rules we have.", "Jenkins, test this please.", "I was copying this for rules_nodejs and rules_typescript and noticed a bug:\r\n\r\n`Current Bazel version is 1.12.3, expected at least 1.2.1`\r\n\r\n/cc @mprobst", "I also think that the check_version should be inside the `tf_workspace` function. Is too easy to update tensorflow without updating the check_version constraints.", "FYI, I added unit tests in my clone of this feature:\r\nhttps://github.com/bazelbuild/rules_nodejs/pull/19/commits/f401ccb6fa79b4b46efe9cfd8cb6a5ae07d6aaa9"]}, {"number": 6706, "title": "Bazel problem when using Tensorflow as an external dependency.", "body": "When using the HEAD version of Tensorflow as an external Bazel dependency (like tensorflow_serving does), we run into an issue in the line:\r\n\r\n`load(\"@//third_party:common.bzl\", \"template_rule\")`\r\n\r\ninside\r\n\r\n`tensorflow/third_party/jpeg/jpeg.BUILD`\r\n\r\nThat line assumes that Tensorflow is the main repository. But when Tensorflow is included in another project the main repository is the main project so that common.bzl file is not found. I think that line needs to read:\r\n\r\n`load(\"@org_tensorflow//third_party:common.bzl\", \"template_rule\")`", "comments": ["So a straightforward [fix](https://github.com/tensorflow/tensorflow/pull/6710) [fails](https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/3179/console) with\r\n\r\n```\r\nINFO: Loading package: @png_archive//\r\nERROR: /workspace/tensorflow/core/platform/default/build_config/BUILD:108:1: error loading package '@jpeg//': Encountered error while reading extension file 'third_party/common.bzl': no such package '@org_tensorflow//third_party': error loading package 'external': The repository named 'org_tensorflow' could not be resolved and referenced by '//tensorflow/core/platform/default/build_config:jpeg'.\r\n\r\n```", "It's not possible to use TensorFlow as an external dependency yet due to https://github.com/bazelbuild/bazel/issues/1248", "Well, tensorflow_serving certainly uses it right now, but synced to a slightly older version that didn't have this issue. We also do it for an internal project at Snapchat.\r\n\r\nI think there should be workaround in Tensorflow while Bazel doesn't fix that issue. The problem was introduced recently in Tensorflow.", "Do you know what change caused the issue?\n", "Your problem most likely originated in https://github.com/tensorflow/tensorflow/pull/6462. I wasn't aware that other Bazel projects were referencing TensorFlow as an external dependency. So it seems fixing this should be much higher on our list of priorities.\r\n\r\nAs a workaround, what we can do is add an `omit_jpeg` parameter to [tf_workspace](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L39) similar to what Closure Rules [does](https://github.com/bazelbuild/rules_closure/blob/master/closure/repositories.bzl). Then you could copy jpeg.BUILD into your codebase, replace `@//` with `@org_tensorflow//` and then declare the new_http_archive for jpeg repository yourself. That's the best workaround that I can think of at the moment that doesn't involve patching Bazel.\r\n\r\nIf you're willing to use a patched Bazel, I can take a peak into the codebase and see if I can fix the issue referenced above in the next few days.", "We can also do what @gunan is proposing (https://github.com/tensorflow/tensorflow/pull/6710#issuecomment-271129858) and rollback the change.", "Can you please rollback the breaking change until Bazel is fixed? Today the nasm library URL broke and there is no commit that both has all the working urls and a working BUILD rule for jpeg.", "@jart let's roll this back so we stop breaking tensorflow/serving in particular. @damienmg told me the fix for this should be in bazel 0.4.4, so hopefully we can re-enable this soon.", "https://github.com/tensorflow/tensorflow/pull/6958\r\nThis PR pushed out our workarounds we implemented internally with @damienmg's help.\r\n@kirilg confirmed that the workarounds fixed servo's problems.\r\n\r\n@raraujosc could you try syncing to head and retry?", "I've verified that using the HEAD version of tensorflow and passing extra arguments to tf_workspace now allows me to add tensorflow as an external dependency. Thanks!", "Then i think we can close this issue for now.", "Are we sure this is fixed? As far as I can tell, running `./configure` is still mandatory, so I'm not sure how TensorFlow could be used as an external dependency.", "What this issue refers to should be fixed.\r\nBut you are right, I am not sure how the options set by `configure` are handled.\r\n\r\n@kirilg @raraujosc @nealwu how do you handle `configure` script when depending on tensorflow as an external dependency?\r\nDo you simply default to GPU? Or create your own configure script?", "Yes, this issue is fixed but is not in 1.0 release. Can you please cherrypick it into 1.0? Thanks.", "TensorFlow serving requires you to run ./configure from within the TensorFlow directory. It would be great if we could somehow depend on TF as a workspace target and configure it with some defaults, but I agree it's a different issue and this one is resolved (though may need to be cherrypicked into 1.0 as Robson mentioned).", "FWIW a bit update on Bazel side:\r\n\r\nWe released 0.4.4 that should remove the need for the @%s// trick (to be verified).\r\n\r\nThe change for removing need from ./configure as a separate step are still in-flight: see https://cr.bazel.build/8218 and its parent, should be in next Bazel release.\r\n\r\nOnce those change are in the Bazel release, we can update TF configure script to be only an user interface that set the good environment variable and just call `bazel fetch //...` at the end.", "I believe this is fixed now. @girving, you have a project depending on TF using workspace... does everything work now?", "Yes, everything should work now (the repo is https://github.com/tensorflow/deepmath).  I haven't synced in the past little while, so it's possible other bugs have surfaced since then (there are no continuous tests for this stuff).", "Thanks. I'll close this issue then. An integration test would indeed be nice.", "For posterity, this was the commit that fixed my issue: https://github.com/tensorflow/tensorflow/commit/c699e5fa03b1f01913636de620aae5e550e58d40.", "Is there a bug to track the being able to include tensorflow in your project without having to run ./configure?", "I don't think we have an issue for that, but we're working on it. You can\nsee that we've moved some code in configure to just modifying --action_env\nalready. Once that's all it does, you don't have to run it any more. Still\nWIP.\n", "@martinwicke Any update on this? I would like to integrate Tensorflow as a library in a C++ project that also uses Bazel", "At the moment, you can depend on TF the same way tensorflow/serving github project is depending.\r\nWe are working on making things better, but it is taking time.", "Thanks. Regarding this I have a few questions:\r\n\r\n1. If you have a new_http_archive which can be Tensorflow git .zip, rather than using a local_archive how do you run ./configure ? Should I maybe write a gen_rule?\r\n\r\n2. What is io_bazel_rules_closure within the top WORKSPACE file in serving?\r\n\r\nI also tried pre-compiling separately tensorflow_cc and creating a include and lib folder, but I some issues including Eigen, which has a BUILD file which calls some other Tensorflow stuff...", "I haven't tried, but I think the easiest way to use tensorflow without ./configure right now would be:\r\n1 - fork it\r\n2 - run ./configure\r\n3 - add the 3 generated files to your repo\r\n4 - depend on the forked repo", "@martinwicke Didn't you say configure wasn't necessary, or was that rerunning configure?", "@gunan, it shouldn't be, is it still?", "Unfortunately it is, still.\r\n@case540 has volunteered and working towards making configure script completely optional.\r\nHe can report on his progress."]}, {"number": 6705, "title": "Fix typo ci_parameterized_build.sh", "body": "", "comments": []}, {"number": 6704, "title": "Fix flakiness of estimator_test on GPU.", "body": "Fix for estimator_test created with help from @ispirmustafa \r\nWe should merge this instead of #6697", "comments": []}, {"number": 6703, "title": "Fix basic_session_run_hooks_test flakiness.", "body": "CC @caisq ", "comments": []}, {"number": 6702, "title": "Deadlock when decoding TFRecords", "body": "I am storing my training examples as variable-length TFRecords using the following function for encoding:\r\n\r\n```\r\ndef convert_to_tf_example(const_exonic_seq, const_intronic_seq,\r\n                          alt_exonic_seq, alt_intronic_seq,\r\n                          psi_distribution, psi_std,\r\n                          alt_ss_position, alt_ss_type,\r\n                          const_site_id, const_site_position,\r\n                          n_alt_ss, event_type):\r\n    \"\"\"Encode a COSSMO example as a TFRecord\"\"\"\r\n\r\n    assert len(alt_exonic_seq) == n_alt_ss\r\n    assert len(alt_intronic_seq) == n_alt_ss\r\n    # assert len(psi_distribution) == n_alt_ss\r\n    # assert len(psi_std) == n_alt_ss\r\n    assert event_type in ['acceptor', 'donor']\r\n    assert len(alt_ss_type) == n_alt_ss\r\n    assert all([t in ('annotated', 'gtex', 'maxent', 'hard_negative')\r\n                for t in alt_ss_type])\r\n\r\n    example = tf.train.SequenceExample(\r\n        context=tf.train.Features(feature={\r\n            'n_alt_ss': tf.train.Feature(\r\n                int64_list=tf.train.Int64List(value=[n_alt_ss])\r\n            ),\r\n            'event_type': tf.train.Feature(\r\n                bytes_list=tf.train.BytesList(value=[event_type])\r\n            ),\r\n            'const_seq': tf.train.Feature(\r\n                bytes_list=tf.train.BytesList(\r\n                    value=[const_exonic_seq, const_intronic_seq])\r\n            ),\r\n            'const_site_id': tf.train.Feature(\r\n                bytes_list=tf.train.BytesList(\r\n                    value=[const_site_id])\r\n            ),\r\n            'const_site_position': tf.train.Feature(\r\n                int64_list=tf.train.Int64List(\r\n                    value=[const_site_position])\r\n            )\r\n        }),\r\n        feature_lists=tf.train.FeatureLists(feature_list={\r\n            'alt_seq': tf.train.FeatureList(\r\n                feature=[tf.train.Feature(\r\n                    bytes_list=tf.train.BytesList(value=[aes, ais])\r\n                ) for aes, ais in\r\n                         zip(alt_exonic_seq, alt_intronic_seq)]\r\n            ),\r\n            'psi': tf.train.FeatureList(\r\n                feature=[tf.train.Feature(\r\n                    float_list=tf.train.FloatList(value=psi))\r\n                         for psi in psi_distribution]),\r\n            'psi_std': tf.train.FeatureList(\r\n                feature=[tf.train.Feature(\r\n                    float_list=tf.train.FloatList(value=psi_sd))\r\n                         for psi_sd in psi_std]),\r\n            'alt_ss_position': tf.train.FeatureList(\r\n                feature=[tf.train.Feature(\r\n                    int64_list=tf.train.Int64List(value=[pos]))\r\n                         for pos in alt_ss_position]),\r\n            'alt_ss_type': tf.train.FeatureList(\r\n                feature=[tf.train.Feature(\r\n                    bytes_list=tf.train.BytesList(value=[t]))\r\n                        for t in alt_ss_type])\r\n        })\r\n    )\r\n    return example\r\n```\r\n\r\nThe data stored here is genomic, but the details shouldn't matter.\r\n\r\nWhen I'm training, I use the following function to decode the TFRecords:\r\n```\r\ndef read_single_cossmo_example(serialized_example, n_tissues):\r\n    \"\"\"Decode a single COSSMO example\"\"\"\r\n\r\n    decoded_features = tf.parse_single_sequence_example(\r\n        serialized_example,\r\n        context_features={\r\n            'n_alt_ss': tf.FixedLenFeature([], tf.int64),\r\n            'event_type': tf.FixedLenFeature([], tf.string),\r\n            'const_seq': tf.FixedLenFeature([2], tf.string),\r\n            'const_site_id': tf.FixedLenFeature([], tf.string),\r\n            'const_site_position': tf.FixedLenFeature([], tf.int64)\r\n        },\r\n        sequence_features={\r\n            'alt_seq': tf.FixedLenSequenceFeature([2], tf.string),\r\n            'psi': tf.FixedLenSequenceFeature([n_tissues], tf.float32),\r\n            'psi_std': tf.FixedLenSequenceFeature([n_tissues], tf.float32),\r\n            'alt_ss_position': tf.FixedLenSequenceFeature([], tf.int64),\r\n            'alt_ss_type': tf.FixedLenSequenceFeature([], tf.string)\r\n        }\r\n    )\r\n    return decoded_features\r\n```\r\n\r\nDuring training, I've been getting deadlocks with triple-digit CPU loads during training (running on a six-core i7) and I've isolated the problem to the above decoding function. A simple Tensorflow program like the following will reproduce the deadlock:\r\n\r\n```\r\n with tf.device('/cpu:0'):\r\n    filename_queue = tf.train.string_input_producer(\r\n        train_files, num_epochs=num_epochs, shuffle=shuffle)\r\n    file_reader = tf.TFRecordReader()\r\n    tf_record_key, serialized_example = file_reader.read(filename_queue)\r\n    decoded_example = read_single_cossmo_example(serialized_example,\r\n                                                  n_tissues)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(\r\n        tf.variables_initializer(\r\n            tf.global_variables() + tf.local_variables()\r\n        )\r\n    )\r\n\r\n    # Start queue runners\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n    try:\r\n        while not coord.should_stop():\r\n            fetch_vals = sess.run(decoded_example)\r\n    except tf.errors.OutOfRangeError:\r\n        pass\r\n    except KeyboardInterrupt:\r\n        print \"Training stopped by Ctrl+C.\"\r\n    finally:\r\n        coord.request_stop()\r\n    coord.join(threads)\r\n```\r\n\r\nNow, setting `intra_op_threads = 1` and `inter_op_threads = 1` will prevent the small script from deadlocking. However, even when restricting the thread pool I have run into deadlocks when using these TFRecords in long running training sessions, so I suspect there is a deeper issue.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA: 8.0\r\ncuDNN: 5.1.5\r\nIf installed from binary pip package, provide:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Can you give a more complete reproducible example? I tried running your code on some synthetic data, and couldn't reproduce.\r\n\r\nIn particular, the function read_single_cossmo_example executes to completion. Do you mean that the problem happens on session.run on tensors created in that function?\r\n\r\nAlso, deadlock scenarios are typically low CPU usage -- you end up with every thread waiting on a mutex, and those don't take any CPU.\r\n\r\nSome possibilities:\r\n\r\n1. Parse example is inefficient and if you wait long enough, it'll produce the answer. One way to tease out inefficiencies is to use google profiling tools to get CPU profile, like is done in https://github.com/tensorflow/tensorflow/issues/6116\r\n\r\n2. There's some bug in parse example which causes things to go into an infinite loop. In this case it would be useful to get the example which causes this condition. Also, perhaps you can `gdb attach -p <pid>` to the hanging process and do `bt` to see which function it is hanging in. If you compile with -c dbg, it'll give you line numbers as well, although it can make everything 10x slower and harder to reproduce", "Thanks @yaroslavvb,\r\n\r\nthe behaviour I'm seeing is kind of subtle, so hopefully I can describe it a little better.\r\n\r\nThe graph creation works fine, the \"deadlock\" happens in `session.run`, when running the graph. By deadlock I mean that CPU load gets extremely high, such that the machine becomes unresponsive.\r\n\r\nThis is quite certainly an issue with multi-threading. If I don't supply a ConfigProto to session.run, i.e. using the default parallelism settings, then I run into the deadlock after very iterations of the loop. \r\n\r\nIf I set \r\n```\r\ntf.Session(config=tf.ConfigProto(\r\n        intra_op_parallelism_threads=1,\r\n        inter_op_parallelism_threads=1,\r\n        log_device_placement=True))\r\n```\r\nthe script above should run forever without a problem.\r\n\r\n_However,_ my problem is that even when I restricted parallelism, executing a long-running training script would still encounter the CPU load explosion situation which would crash my machine. I'm not sure how I could create a standalone reproducible example of this issue, so I'm trying to reduce the issue to the root cause and it seems to me that the fact that decoding the TFRecords fails with default parallelism settings is the original problem here.\r\n\r\nI'm aware that `tf.parse_single_sequence_example` is inefficient, but there is no equivalent to `tf.parse_example` for sequence examples.\r\n\r\nI will use the Google profiling tools to generate profiles in both the multi-threaded and single-threaded case. Hopefully, this will give some additional clues.", "Could it be that there's a bad tfrecords example which causes infinite loop? If you use 1 thread, it takes longer to get to that example, so that explains the extra delay. You can fix seeds to help with determinism and print out example keys to see which example could be the problem", "I set a seed and printed every tf record key, but the it always hangs at a different example.\r\n\r\nI did manage to run the CPU profiler though and created two profiles for the singled threaded configuration and the hanging multi threaded configuration.\r\n\r\nSingle-threaded: https://drive.google.com/open?id=0B_AiZCOgaorxSXRaaUl3SGJCYUk\r\nMulti-threaded (hanging): https://drive.google.com/open?id=0B_AiZCOgaorxWlVxaUFnS1YzWWs\r\n\r\nYou can see that the hanging program spends all its time in `__sched_yield` in the Eigen thread pool.", "Hm, from the profile it seems to get stuck in NonBlockingThreadPoolTempl::WorkerLoop -> sched_yield .... not really familiar with that part of the code but this vaguely seems like a bug in `parse_single_sequence_example` \r\n\r\nblame [says](https://github.com/tensorflow/tensorflow/blame/a4b9b3ad4d602621f2a81fe86d11a531aaa4701d/tensorflow/core/ops/parsing_ops.cc) vrv put that code in, but it maybe misattribution", "Per last comment, I'm assigning to @vrv to take this out of triage queue.", "I don't really know what parse_single_sequence_example is, I think the original author was @ebrevdo, so assigning to him.", "That op does not use multiple threads.  Could you try running with\n--config=asan to see if there are any associated memory issues?\n\nIf you want me to try to debug, you'll have to send me enough input data,\nso i can reproduce, by email.  You can send it to my username@, the domain\nis either gmail.com or google.com.\n\nOn Tue, Jan 10, 2017 at 4:58 PM, Vijay Vasudevan <notifications@github.com>\nwrote:\n\n> I don't really know what parse_single_sequence_example is, I think the\n> original author was @ebrevdo <https://github.com/ebrevdo>, so assigning\n> to him.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6702#issuecomment-271746489>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimweFQ3p65c426AabmTtJ0Dg8oRGGks5rRCkrgaJpZM4LdLzp>\n> .\n>\n", "@ebrevdo Emailed you with some scripts that should hopefully reproduce the problem", "@ebrevdo Have you been able to gain any insights?", "@ebrevdo @yaroslavvb After additional investigation I believe that this problem is OS-specific. I had a suspicion that the way the kernel queues tasks was related to the problem after I observed that the machine load would get extremely high (e.g. >1000 on a six core machine), but CPU, memory, and disk I/O would all remain low.\r\nSo I tried both my test script as well as my regular training task on Ubuntu 16.04 (from 14.04 before) and I haven't encountered the issue since.", "nice investigation! Can you share more details for posterity? IE, what commands did you to use to query machine load (how is that different from CPU?)", "I mean load average as reported by e.g. `top` or `uptime` and many other tools. For example, here is an article that explains load and how it is different from CPU utilization: http://www.linuxjournal.com/article/9001. You can find people reporting similar problems by googling \"high load average low cpu\".", "Closing since this seems fixed by upgrading the OS."]}, {"number": 6701, "title": "Possible bug: tf.reverse() does not accept bool type 'dim' argument in r0.12", "body": "I think tf.reverse() got messed up in r0.12. Though the [documentation](https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#reverse) says the argument 'dim' has to be 'bool', while running it complains -- \"Value passed to parameter 'axis' has DataType bool not in list of allowed values: int32, int64\". I can see that this comes from a new op -- tf.reverse_v2(). I saw that in r0.11 this runs with no complains. I have prepared a mwe for this (link provided below).\r\nEither the documentation needs to be updated (for r0.12) or this needs to be fixed so that dims can accept boolean. I'd, rather, opt for a fix as this might be a reason for backward incompatibility in some legacy code.\r\n\r\n### Environment info\r\nOperating System: Centos 7\r\nInstalled version of CUDA and cuDNN: cuda 8.0, cuDNN 5.1\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): e4dde23d58a10c9d0c14005d20d1ecdd599539ac\r\n2. The output of `bazel version` - 4.2\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI have prepared a mwe for this (Just 26 lines of code). This tries to reproduce one of the examples given in the documentation. Attached\r\n[here](https://github.com/tensorflow/tensorflow/files/690954/mwe.txt)\r\n\r\n### Logs or other output that would be helpful\r\nTraceback (most recent call last):\r\n  File \"mwe.py\", line 16, in <module>\r\n    t, dims, t_rev = flip()\r\n  File \"mwe.py\", line 10, in flip\r\n    t_rev = tf.reverse(t, dims)\r\n  File \"/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2679, in reverse\r\n    return gen_array_ops.reverse_v2(tensor, axis, name)\r\n  File \"/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2963, in reverse_v2\r\n    name=name)\r\n  File \"/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 585, in apply_op\r\n    param_name=input_name)\r\n  File \"/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 61, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'axis' has DataType bool not in list of allowed values: int32, int64\r\n", "comments": ["There's been some backward incompatible changes: https://github.com/tensorflow/tensorflow/blob/c72d891418c326794a913cc13382c104255fe1f2/RELEASE.md#breaking-changes-to-the-api\r\n\r\n`tf.reverse() now takes indices of axes to be reversed. E.g. tf.reverse(a, [True, False, True]) must now be written as tf.reverse(a, [0, 2]). tf.reverse_v2() will remain until 1.0 final.`\r\n\r\ndoes that help?", "@yaroslavvb, I see. Thats quite a list. Yes, the workaround works. I just thought, as it is not conforming to the documentation, it might be an unintentional bug. Please close the issue if its not unintentional.", "I suspect you were looking at the wrong documentation -- your commit hash is 21 hours old so you have a version newer than 0.12 and should be looking at documentation from head. Navigate to \"VERSIONS\" and click HEAD, the link should be:\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/array_ops/slicing_and_joining#reverse\r\n", "@yaroslavvb, got it. I never knew I can get the latest documentation in such way. Thanks for the pointer. Closing the issue."]}, {"number": 6700, "title": "tensorflow docs have sigmoid_cross_entropy_with_logits args flipped", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.nn.sigmoid_cross_entropy_with_logits.md \r\n\r\nlabels comes before logits, please excuse if i'm way off base, not sure where these docs are generated from", "comments": ["Arg order change was introduced in https://github.com/tensorflow/tensorflow/commit/333dc32ff79af21484695157f3d141dc776f7c02 by @martinwicke ... I guess docs need to be regenerated?", "wow, isn't this going to break upgraded code in very hard to debug ways?", "This is one of a number of \"hard to debug breaking changes\" coming up in preparation to 1.0 release, documented [here](https://github.com/tensorflow/tensorflow/blob/c72d891418c326794a913cc13382c104255fe1f2/RELEASE.md#breaking-changes-to-the-api) . The recommendation for backward compatibility is to not use any positional arguments but specify keywords explicitly", "And in fact, the new interface disallows positional arguments for that\nexact reason.\n", "Some other function, e.g. sparse_softmax_cross_entropy_with_logits also have this breaking change. These need to be documented in RELEASE.md.\r\n\r\nBut in one function: [tf.nn.weighted_cross_entropy_with_logits](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.weighted_cross_entropy_with_logits.md) , positional arguments are not disallowed. This may lead to silent failure.", "That's a good point. We should enforce names args for that function as\nwell.\n", "argument order was addressed in 1.0 with scripts for compatibility to add named fields (for google searches)"]}, {"number": 6699, "title": "Branch 143784180", "body": "", "comments": ["The failure in PR tests is the same as the failure in internal continuous tests.\r\nWe should fix that failure first and create a new push.", "Closing in favor of #6719 "]}, {"number": 6698, "title": "Crash: Could not create cuDNN handle when convnets are used", "body": "Tensorflow (GPU) was imported successfully, but when running a session that involves a convolutional neural network (CNN), Python crashes with the following message:\r\n\r\n    E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n    E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n    F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\nThe problem persists on any combination of CUDA toolkit 7.5/8.0 and Tensorflow installed from pip/source. Test sessions that do not use CNNs are run successfully.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThe issue is similar to https://github.com/tensorflow/tensorflow/issues/6586, where I first commented. But since I experience the problem on a Mac, I was suggested to open a separate issue.\r\n\r\n### Environment info\r\nOperating System: macOS Sierra 10.12.2\r\nXcode version 8.2 (8C38) (When I later tried CUDA 7.5, I installed Command Line Tools version 7.3.1 because CUDA 7.5 lacked support of the more recent compilers.)\r\nPython 3.5.2 (anaconda)\r\n\r\nInstalled version of CUDA: tried both 8.0 (initially) and 7.5 (reported here, toolkit only -- the driver is still 8.0)\r\nInstalled version of cuDNN: 5.1 (different installations according to CUDA versions)\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n    lrwxr-xr-x  1 root   wheel        33  5 Jan 20:33 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n    -rwxr-xr-x@ 1 root   wheel      8280 13 Apr  2016 /usr/local/cuda/lib/libcuda.dylib\r\n    lrwxr-xr-x@ 1 root   wheel        45 13 Apr  2016 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\r\n    lrwxr-xr-x@ 1 root   wheel        50 13 Apr  2016 /usr/local/cuda/lib/libcudart.7.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\r\n    lrwxr-xr-x@ 1 root   wheel        46 13 Apr  2016 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\r\n    lrwxr-xr-x@ 1 root   wheel        49 13 Apr  2016 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\r\n    lrwxr-xr-x  1 root   wheel        16  5 Jan 17:14 /usr/local/cuda/lib/libcudnn.5 -> libcudnn.5.dylib\r\n    -rwxr-xr-x@ 1 ymfa   staff  58975112 10 Jun  2016 /usr/local/cuda/lib/libcudnn.5.dylib\r\n    lrwxr-xr-x@ 1 ymfa   staff        16 10 Jun  2016 /usr/local/cuda/lib/libcudnn.dylib -> libcudnn.5.dylib\r\n    lrwxr-xr-x  1 root   wheel        16  5 Jan 17:14 /usr/local/cuda/lib/libcudnn5.dylib -> libcudnn.5.dylib\r\n    -rw-r--r--@ 1 ymfa   staff  56392320 10 Jun  2016 /usr/local/cuda/lib/libcudnn_static.a\r\n\r\nI tried both installing from pip and source. I first installed from binary pip package:\r\n\r\n1. A link to the pip package you installed:\r\n`tensorflow-gpu`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n`0.12.head`\r\n\r\nLater I installed from source (the pip package was uninstalled):\r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n`d67c09d98a576e1fbf2f3609ddb842e53890f31c`\r\n2. The output of `bazel version`\r\n\r\n    Build label: 0.4.3-homebrew\r\n    Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n    Build time: Thu Dec 22 15:20:15 2016 (1482420015)\r\n    Build timestamp: 1482420015\r\n    Build timestamp as int: 1482420015\r\n\r\n### If possible, provide a minimal reproducible example\r\n\r\nI made a minimal example by simplifying the network and reducing the training data to only twenty images and two classes for classification. [issue.zip](https://github.com/tensorflow/tensorflow/files/691561/issue.zip) contains the Python code and the data. I wrote two convolutional layers because I found the network with only one convolutional layer runs without problem.\r\n\r\n### Complete log using CUDA 7.5 and Tensorflow compiled from source\r\n\r\n    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.7.5.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.5.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.7.5.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.1.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.7.5.dylib locally\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n    I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:874] OS X does not support NUMA - returning NUMA node zero\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\n    name: GeForce GT 650M\r\n    major: 3 minor: 0 memoryClockRate (GHz) 0.9\r\n    pciBusID 0000:01:00.0\r\n    Total memory: 1023.69MiB\r\n    Free memory: 740.18MiB\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\r\n    E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n    E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n    F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\n### Complete log using CUDA 8.0 and Tensorflow installed from pip\r\n\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\n    I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\n    name: GeForce GT 650M\r\n    major: 3 minor: 0 memoryClockRate (GHz) 0.9\r\n    pciBusID 0000:01:00.0\r\n    Total memory: 1023.69MiB\r\n    Free memory: 590.00MiB\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0: Y \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\r\n    E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n    E tensorflow/stream_executor/cuda/cuda_dnn.cc:392] error retrieving driver version: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"\"\r\n    E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n    F tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n", "comments": ["```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:02:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 3.76GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\nI met exactly the same problem as you do with CUDA8 and TF r0.12.1.\r\n", "@EncodeTS I just added a minimal reproducible example to my first post. Could you check if it reproduces the problem on your machine? On my machine, one convolutional layer works but not two convolutional layers, which led me to think that the problem might be caused by some resource limitations.", "I can confirm that @ymfa minimal example fails on MacOS NVidia 750, but also same example works on Linux/Titan X", "The minimal example works on my Ubuntu. It looks like the issue I had encountered has a very low occurrence probability on my computer.", "I'm encountering the same problem. The graph will run fine when forced to the cpu, but crashed on the gpu.\r\n\r\n### Environment\r\nOS: macOS 10.12.2\r\nGPU: GeForce GT 750M\r\nTF: 0.12.1 (pip install)\r\nPython: 3.6.0\r\nCUDA: 8.0\r\ncuDNN: 5.1\r\n\r\n(output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nlrwxr-xr-x  1 root  wheel     33 Dec 14 14:25 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n-rwxr-xr-x  1 root  wheel  13504 Dec  2 16:48 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Nov  3 11:40 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x  1 root  wheel     50 Nov  3 11:40 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x  1 root  wheel     46 Nov  3 11:40 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x  1 root  wheel     49 Nov  3 11:40 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  wheel     47 Dec 14 10:21 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Dec 14 10:21 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  wheel     48 Dec 14 10:21 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a\r\n```\r\n\r\n### Example\r\nThe minimal example provided by @ymfa both fails and succeeds on my setup. The following are three outputs that have been produced.\r\n**fail(1)**\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.76GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nTraining...\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAbort trap: 6\r\n```\r\n**fail(2)**\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.53GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nTraining...\r\nE tensorflow/stream_executor/cuda/cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nW tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(20, 400), b.shape=(400, 2), m=20, n=2, k=400\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Flatten/Reshape, Variable_4/read)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"issue.py\", line 52, in <module>\r\n    sess.run(training_operation, feed_dict={x: X, y: Y})\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(20, 400), b.shape=(400, 2), m=20, n=2, k=400\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Flatten/Reshape, Variable_4/read)]]\r\n\r\nCaused by op 'MatMul', defined at:\r\n  File \"issue.py\", line 43, in <module>\r\n    logits = SimpleNet(x)\r\n  File \"issue.py\", line 34, in SimpleNet\r\n    logits = tf.matmul(fc1, fc1_W) + fc1_b\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1729, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1442, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(20, 400), b.shape=(400, 2), m=20, n=2, k=400\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Flatten/Reshape, Variable_4/read)]]\r\n```\r\n**pass**\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.71GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nTraining...\r\nTraining complete!\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Not so fast, I see this crash too. Macbook pro, geforce 650. TF v1. Running via jupyter kernels, which I have to frequently restart. Maybe this graphics card is just too weak? Seeing as how the op uses the same card: likely.\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\n...\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GT 650M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9\r\npciBusID 0000:01:00.0\r\nTotal memory: 1023.69MiB\r\nFree memory: 870.46MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n```\r\n", "I have the same problem with GTX 960m, cudnn5.1.5 and cuda-8.0.44.", "Have the same problem with centOS, titan X", "Have the same problem with ubuntu(14.04) and GRID K520 (aws g2.2)", "Have the same problem windows 10 cudnn 5.1 cuda 8 gtx 1060.  Program works on cpu version of tensor flow but get these same errors with the gpu version.", "I had the same issue with gtx1060, win8.1, cuda8.0.60, cudnn5.0. Upgraded to the latest stable tensorflow-gpu nightly build (currently http://ci.tensorflow.org/job/nightly-win/133/) and cudnn5.1. Problem solved. ", "Same issue here. \r\n\r\nI was having this issue with the software versions listed below, except TF was version 1.0.0. I then upgraded to TF 1.0.1. I ran the same program once and **it worked**. I then ran it again and it **didn't work** -- it produced the same error as before.\r\n\r\nTensorflow-gpu 1.0.1\r\nMac OS X 10.12.3\r\nCuda 8.0.61\r\nCuDNN 5.1\r\nGeForce GT 750M", "having the same problem with gtx650, ubuntu 16.04, CUDA Version 8.0.61, TF version 1.0.0\r\nit was working just now, but giving some low memory warnings. However, it was running\r\nNow it doesn't run at all, giving me same Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) error", "Having the same issue with gtx 1080 ti, windows 10, CUDA Version 8.0.61, TF version 1.0.1, 5.1 Cudann, cuda 8.0.61", "I was able to get a program to work by limiting the gpu usage.  In my case with a 3gb gtx 1060 on ubuntu 16.04, if I set gpu option per_process_gpu_memory_fraction to .7 it works.  Anything higher, I get these errors \r\n\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\nIt could be a case of bad error reporting by tensorflow.  Seems completely unrelated.  Maybe it is a clue to getting this resolved in a better manner?", "@zheng-xq is there an obvious setup issue?", "Same issue too. I'm on Windows 10, GTX1070, CUDA 8.0, cuDNN 5.1.\r\n\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n> F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:659] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n", "If it helps anyone, seems there are sometimes zombie processes left which prevent from tf to start again properly and gave me this error. killing them work around the issue. ", "Here is a bit more info on how I temporarily resolved it.  I believe these issues are all related to GPU memory allocation and have nothing to do with the errors being reported.  There were other errors before this indicating some sort of memory allocation problem but the program continued to progress, eventually giving the cudnn errors that everyone is getting.  The reason I believe it works sometimes is that if you use the gpu for other things besides tensorflow such as your primary display, the available memory fluctuates.  Sometimes you can allocate what you need and other times it can't.\r\n\r\nFrom the API \r\nhttps://www.tensorflow.org/versions/r0.12/how_tos/using_gpu/\r\n\"By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation.\"\r\n\r\nI think this default allocation is broken in some way that causes this erratic behavior and certain situations to work and others to fail.\r\n\r\nI have resolved this issue by changing the default behavior of TF to allocate a minimum amount of memory and grow as needed as detailed in the webpage.\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config, ...)\r\n\r\nI have also tried the alternate way and was able to get it to work and fail with experimentally choosing a percentage that worked.  In my case it ended up being about .7.\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\r\nsession = tf.Session(config=config, ...)\r\n\r\nStill no word from anyone on the TF team confirming this but it is worth a shot to see if others can confirm similar behavior.", "I am also getting the `CUDNN_STATUS_NOT_INITIALIZED` error. Here is the full error log:\r\n```\r\n2017-04-26 00:08:57.526234: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\n2017-04-26 00:09:01.111706: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2017-04-26 00:09:01.111805: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2017-04-26 00:09:01.114040: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-04-26 00:09:01.114232: F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:659] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\n\r\nI am on Windows 10, CUDA 8.0, cuDNN 5.1 . Can anything be done to avoid these? I was able to run earlier some other tensorflow tests and it worked fine (including conv op), but now it doesn't work on this new test...\r\n\r\n@serans1 What zombie processes are you referring to?\r\n\r\nPlease let me know if there is a workaround for this. Thank you!\r\n\r\n**EDIT** This might have been a newbie mistake, but I will just mention it here, in case someone else runs in the same issue:\r\nMy problem was that I already had running an instance of a Jupyter Python Notebook (whose cells were all ran already, hence loaded in the memory), and also some other process that was taking up GPU memory (minimized video game). Therefore, when I checked the memory usage on my GPU, it was already at around 4+GB (50+%). I closed the Jupyter Notebook and the other application, and re-ran my tensorflow test. Now everything ran smoothly :) Also, while running I noticed that at peak it uses up to 90% of my GPU memory, and thus it makes sense why it couldn't initialize CUDNN when it had less than 50% available in my initial situation.\r\n\r\nSorry again for my mistake! I'm just at the beginning of playing around with this :)", "**The same problem,is there any solution to it ?**\r\n\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 960M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\r\npciBusID 0000:01:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.35GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0 \r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y \r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:586] Could not identify NUMA node of /job:localhost/replica:0/task:0/gpu:0, defaulting to 0.  Your kernel may not have been built with NUMA support.\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:392] error retrieving driver version: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) ", "I have exactly same issue. \r\nBut I can run my codes with root access(with sudo).\r\nCurrently I'm working on Ubuntu 16.04 with GTX 960.\r\nMy CUDA version is 8.0 and I'm using tensorflow 1.01", "Windows 10 / Tensorflow 1.01\r\nI was using it perfectly but now accidentally the same error happen to me\r\n\r\nname: GeForce GTX 1070\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7715\r\npciBusID 0000:03:00.0\r\nTotal memory: 8.00GiB\r\nFree memory: 6.68GiB\r\n2017-05-08 21:12:16.103654: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:908] DMA: 0\r\n2017-05-08 21:12:16.105184: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:918] 0:   Y\r\n2017-05-08 21:12:16.106710: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:03:00.0)\r\n2017-05-08 21:12:24.395060: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2017-05-08 21:12:24.395177: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2017-05-08 21:12:24.396636: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-05-08 21:12:24.396846: F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:659] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)", "@strickon's method worked for me. Seems like tensorflow is trying to hog way too many resources at once and can't which crashes the operation. I specifically used:\r\n\r\nconfig.gpu_options.allow_growth = True", "Confirming @strickon 's suggestion works for me. \r\n\r\nAm running https://github.com/awjuliani/DeepRL-Agents/blob/master/Double-Dueling-DQN.ipynb and was getting the failures mentioned in this thread on the first call to sess.run within the update block ( The line:  ` Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])}) `. \r\n\r\nAdding the allow_growth flag (as per below) got me past this bump - the code is currently running in the background, we'll see how far it goes.\r\n\r\n```python\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```\r\n\r\nStack: \r\n* MacBook Pro, running Sierra  10.12.4, with NVIDIA GeForce GT 750M 2048 MB. Typically only have 1.7GB free. \r\n* [TensorFlow  1.1](https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl) Using Anaconda install instructions.\r\n* Python 3.6, not virtual (Anaconda)\r\n* CUDA 8 / cuDNN 5\r\n\r\nI'd be fine with dumping more stats on request.\r\n", "I was working with two terminals at the same time and had same issue. It was solved by closing one terminal.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "After implementing the changes suggested by @strickon, I began to see a new set of info logs show up: \r\n```\r\n2017-06-23 04:45:57.156787: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\pool_allocator.cc:247] PoolAllocator: After 3205 get requests, put_count=2333 evicted_count=1000 eviction_rate=0.428633 and unsatisfied allocation rate=0.615289\r\n2017-06-23 04:45:57.156880: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\r\nstep 0 - loss = 5.632, (19.351 sec/step)\r\n```\r\nUnsure if related.", "Same error here.\r\n\r\nWindows 10 x86_64, GeForce GTX 970, drivers 376.53, Cuda 8.0, cuDNN 5.1., tensorflow-gpu 1.2.0 from pip, python 3.6\r\n\r\nI am trying to run the default example from the tutorials section of the website: \r\n\r\nhttps://www.tensorflow.org/tutorials/image_recognition\r\n\r\n`python classify_image.py`\r\n\r\nI have the same error: \r\n\r\n`\r\n```\r\n(C:\\ProgramData\\Anaconda3) C:\\Users\\Locky\\Google \u0414\u0438\u0441\u043a\\MachineLearning\\Tensorflow-Tutorials\\Repo\\models\\tutorials\\image\\imagenet>python classify_image.py\r\n2017-06-25 18:36:32.318287: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.318514: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.323556: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.323719: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.323834: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.323930: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.324205: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.324351: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 18:36:32.707933: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:940] Found device 0 with properties:\r\nname: GeForce GTX 970\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\r\npciBusID 0000:01:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.31GiB\r\n2017-06-25 18:36:32.708332: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:961] DMA: 0\r\n2017-06-25 18:36:32.713764: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   Y\r\n2017-06-25 18:36:32.713991: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\r\n2017-06-25 18:36:34.854555: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\framework\\op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\n2017-06-25 18:36:35.836895: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2017-06-25 18:36:35.837068: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2017-06-25 18:36:35.841593: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-06-25 18:36:35.841690: F c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\36\\tensorflow\\core\\kernels\\conv_ops.cc:671] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n\r\n(C:\\ProgramData\\Anaconda3) C:\\Users\\Locky\\Google \u0414\u0438\u0441\u043a\\MachineLearning\\Tensorflow-Tutorials\\Repo\\models\\tutorials\\image\\imagenet>\r\n\r\n````", "In my case, this happened because other tensorflow instances were holding the GPU. (Other scripts running.)\r\n\r\nCould I propose a better error messages? Say, \"Error: other tensorflow instances running, while only a single one is supported.\"\r\n", "I have the same issue. Running macOS 10.12.5 GT 750M 2GB\r\n\r\n```\r\npython neural_style.py --content /Users/qinyuhang/Pictures/0.jpeg  --styles IMG_1105.JPG --output 1.out.jpg --iterations 500\r\n2017-07-05 22:16:54.531699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:857] OS X does not support NUMA - returning NUMA node zero\r\n2017-07-05 22:16:54.532257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.54GiB\r\n2017-07-05 22:16:54.532435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-07-05 22:16:54.532461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-07-05 22:16:54.532471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\n2017-07-05 22:17:07.284016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\n2017-07-05 22:17:44.973549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nOptimization started...\r\nIteration    1/ 500\r\n2017-07-05 22:17:47.485948: E tensorflow/stream_executor/cuda/cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-07-05 22:17:47.485977: E tensorflow/stream_executor/cuda/cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-07-05 22:17:47.485983: F tensorflow/core/kernels/conv_ops.cc:671] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n[1]    66448 abort      python neural_style.py --content /Users/qinyuhang/Pictures/0.jpeg --styles   \r\n\r\n```", "Solved it (at least for me). The error message does not lead you to the right problem. I had this error from 2 different sources:\r\n\r\nFirst (like  @lockywolf said):\r\nI use jupyter notebook and sometimes the TF kernel wont free the GPU memory and you have to **restart the jupyter to get it to work again**. This happens generally after run-time errors or improper kernel restarting...\r\n\r\nSecond:\r\nSometimes you get greedy with the GPU memory and try things like this:\r\n```\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\r\nsess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```\r\nThis was fatal to my configuration and started to get this error. The solution was to **use the default way to start the interactive session**:\r\n`sess = tf.InteractiveSession()`\r\n\r\n\r\nSystem:\r\n\r\nUbuntu 14.04\r\nGeForce GTX 780\r\nCUDA Driver Version = 8.0\r\nCUDNN Version = 5.1\r\nTensorFlow Version = 1.2.1\r\n", "I've the same issue running my own scripts now.\r\nI think it is the same reason like @lockywolf described:\r\n\r\n> In my case, this happened because other tensorflow instances were holding the GPU. (Other scripts running.)\r\n\r\nI had this error quite often but irregular, then i followed @RawthiL 's lead and added a session to my script. However, i executed the script successfully restarted the kernel and got the same error message again. Is there any solution to open the session, claim the GPU and close it after the calculation is done?\r\n\r\ncheers! \r\n\r\n**Edit:**\r\nBeside @RawthiL 's solution i followed the [Keras TF introduction](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html) where they say:\r\n\r\n> We should start by creating a TensorFlow session and registering it with Keras. This means that Keras will use the session we registered to initialize all variables that it creates internally.\r\n> \r\n> import tensorflow as tf\r\n> sess = tf.Session()\r\n> \r\n> from keras import backend as K\r\n> K.set_session(sess)", "Same problem. Been fighting uphill to get this working all day.\r\n\r\n```\r\n$ ~/neural-style$ python neural_style.py --content ~/Documents/8UhFDcjT.jpg --styles ~/Documents/9odz6-jbngd.png --output ./Documents/Scott.png\r\n2017-07-26 20:57:08.373361: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-26 20:57:08.373397: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-26 20:57:08.373413: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-26 20:57:08.373417: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-26 20:57:08.373421: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-26 20:57:08.431319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-07-26 20:57:08.431630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 870M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.967\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.95GiB\r\nFree memory: 2.53GiB\r\n2017-07-26 20:57:08.431664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-07-26 20:57:08.431674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-07-26 20:57:08.431690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 870M, pci bus id: 0000:01:00.0)\r\n2017-07-26 20:57:11.692616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 870M, pci bus id: 0000:01:00.0)\r\n2017-07-26 20:57:19.800938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 870M, pci bus id: 0000:01:00.0)\r\nOptimization started...\r\nIteration    1/1000\r\n2017-07-26 20:57:20.535515: E tensorflow/stream_executor/cuda/cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-07-26 20:57:20.535573: E tensorflow/stream_executor/cuda/cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-07-26 20:57:20.535588: F tensorflow/core/kernels/conv_ops.cc:671] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n```", "I found that in some cases resetting the jupyter kernel wont work. Actually it happened to me while using jupyterhub.\r\nI restarted the kernel, deactivated my virtualenv and the GPU memory was still being held by some process. The` nvidia-smi` command said that there was no process using the GPU and when I  tried to reset it with `sudo nvidia-smi --gpu-reset -i 0` (for the 0 gpu core) it said the following:\r\n\r\n> Unable to reset this GPU because it's being used by some other process (e.g. CUDA application, graphics application like X server, monitoring application like other instance of nvidia-smi). Please first kill all processes using this GPU and all compute applications running in the system (even when they are running on other GPUs) and then try to reset the GPU again.\r\n> Terminating early due to previous errors.\r\n\r\nSo there was some process holding the GPU, and I looked for them using `sudo fuser -v /dev/nvidia*` which said that there was actually something holding the GPU... python itself... killing it and re-launching virtualenv and jupyter did the trick.\r\nI might not be the best way to solve this, but is better than resetting the computer when all other options fail.", "Have the same issue. GPU is GTX 1070 and CUDA 8.0 and CUDNN 5.1 for CUDA 8.0.\r\n\r\nIssue does not depend on user code, it depends on hardware or Nvidia or Google software state. This error can start rising at any time and reboot can fix it with the same user code.\r\n\r\n", "Same issue with Windows 10, GTX770, CUDA 8.0, CUDNN 5.1, TF-GPU 1.1.0, not sure where to get the device driver version but Windows Device Manager reports 21.21.13.7651 for the display driver.\r\n\r\n```\r\nconnect  84557d348c06492e80ff0304d516367b\r\n2017-08-11 15:51:41.974028: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2017-08-11 15:51:41.974536: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2017-08-11 15:51:41.974923: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-08-11 15:51:41.975194: F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:659] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\n", "Same issue with Windows 10, GTX770, CUDA 8.0, CUDNN 5.1, TF-GPU 1.1.0, not sure where to get the device driver version but Windows Device Manager reports 21.21.13.7651 for the display driver.\r\n```\r\nconnect  84557d348c06492e80ff0304d516367b\r\n2017-08-11 15:51:41.974028: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2017-08-11 15:51:41.974536: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2017-08-11 15:51:41.974923: E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-08-11 15:51:41.975194: F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:659] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\n@ggranum's fix worked for me:\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```\r\n\r\n", "In my case the same issue was resolved by updating the NVIDIA gpu driver.", "Has this issue been completely resolved. I am running TF 1.3.0 on Ubuntu 16.04 with CUDA 8.0 and cuDNN 5.1. I used Anaconda to install my packages. Randomly 4 days ago, I too experienced this error\r\n\r\n`name: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:05:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 10.30GiB\r\n2017-09-05 07:47:05.397839: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x30028e0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-09-05 07:47:05.401343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:06:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 10.75GiB\r\n2017-09-05 07:47:05.658932: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x2ffe910 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-09-05 07:47:05.659690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:09:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 10.75GiB\r\n2017-09-05 07:47:05.898536: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x2ffa940 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-09-05 07:47:05.899294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:0a:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 10.75GiB\r\n2017-09-05 07:47:05.903197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3 \r\n2017-09-05 07:47:05.903209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y \r\n2017-09-05 07:47:05.903215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y \r\n2017-09-05 07:47:05.903218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y \r\n2017-09-05 07:47:05.903223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y \r\n2017-09-05 07:47:05.903236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)\r\n2017-09-05 07:47:05.903242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)\r\n2017-09-05 07:47:05.903248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0)\r\n2017-09-05 07:47:05.903252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0)\r\n2017-09-05 07:47:20.297138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)\r\n2017-09-05 07:47:20.297190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)\r\n2017-09-05 07:47:20.297206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0)\r\n2017-09-05 07:47:20.297220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0)\r\n2017-09-05 07:47:24.845499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)\r\n2017-09-05 07:47:24.845534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)\r\n2017-09-05 07:47:24.845542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0)\r\n2017-09-05 07:47:24.845548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0)\r\n2017-09-05 07:47:34.884524: E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-09-05 07:47:34.884597: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-09-05 07:47:34.884616: F tensorflow/core/kernels/conv_ops.cc:672] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)`\r\n\r\nI have 4 1080ti GPUs. During the running of my model I monitored nvidia-smi and got \r\n\r\n-----------------------------------------------------------------------------+\r\n| Processes:                                                                               GPU Memory |\r\n|  GPU       PID  Type  Process name                                               Usage      |\r\n|=============================================================================|\r\n|    0      1422    G   /usr/lib/xorg/Xorg                                                 279MiB |\r\n|    0      3530    G   compiz                                                                195MiB |\r\n|    0     11249    C   /home/simon/anaconda3/bin/python             10157MiB |\r\n|    1     11249    C   /home/simon/anaconda3/bin/python             10611MiB |\r\n|    2     11249    C   /home/simon/anaconda3/bin/python             10611MiB |\r\n|    3     11249    C   /home/simon/anaconda3/bin/python             10611MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nSo for some reason Python is hogging memory. Of course if I kill this, it kills my jupyter notebook. I have no zombie processes running. I have tried.\r\n\r\n`gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))`\r\n\r\nwhich does reduce the GPU usage but I still get the same cuDDN handle error. I've reinstalled TF. CUDA, cuDNN, Anaconda with no impact on the problem. \r\n\r\nWhy does this error occur randomly and how can this be solved. ", "TensorFlow 1.3 is built against cuDNN 6.\r\nPlease upgrade your cuDNN installation.", "Thanks, Gunan - that makes no difference, unfortunately. Even with cuDNN 6, I am still getting the cuDNN cannot create handle error. Even setting the GPUptions directly doesn't prevent the error, although it does reduce the amount of GPU memory used. The GPU memory is taken up by Python, so if I shut this down, it closes my Jupyter notebook. I have been stuck on this for nearly 4 days now and seem to have exhausted all the suggestions I have seen online. Could this be a TF 1.3 issue?", "Just for those who are driven mad by this:\r\n\r\nI occasionally got a CUBLAS error as well. So I did this:\r\n\r\n`cd /usr/local/cuda/samples/7_CUDALibraries/simpleCUBLAS`\r\n`make`\r\n`./simpleCUBLAS`\r\n\r\nand discovered that I could not initialise CUBLAS\r\n\r\nSo next I did this (based on advice)\r\n\r\n`sudo rm -f ~/.nv`\r\n\r\nAnd it worked. Cheers.....thats 4 days wasted. Hope this saves someone else", "@SimonWalsh1000 That worked!! thanks", "check your .theanorc in your home path(if Ubuntu), and set the cnmem smaller....maybe cnmem=0.8, and it worked for me now", "I got it working perfectly under Windows 10 with GTX 1070. \r\nI was using cudnn 7.0.2 \r\nDowngrading to vs 6.0 solved me problems:\r\n\r\n```\r\ncuda_8.0.61_win10.exe\r\ncudnn-8.0-windows10-x64-v6.0.zip\r\npython-3.6.2-amd64.exe\r\n```\r\n\r\nPosted the whole installation process here:\r\n[http://klaatuveratanecto.com/installing-tensorflow-gpu-windows-10-running-image_retraining/](http://klaatuveratanecto.com/installing-tensorflow-gpu-windows-10-running-image_retraining/)", "Hi, I got the same question. However, I found the reason is that I used tensorflow twice at the same time. \r\n\r\nFor example, I usually used the Jupyter notebook for the simple script and used the PyCharm for the project. If I didn't shut down the jupyter notebook , I could meet this error in the Pycharm. \r\n\r\nWish this could help. \r\n\r\n------------------\r\nWIndows10 64, \r\nNVIDIA TitanX , \r\nDriver 385.41, \r\nCuda 8.0.60\r\nCudnn 6.0\r\nPython 3.5.2\r\nTensorflow 1.3", "I agree with @strickon : it seems to be an memory allocation issue. \r\nI had a notebook with tensorflow program running and I tried to run a python + tensorflow in another Windows terminal and got the error. Then I restarted my notebook (release GPU memory) and tried to run the python on Windows terminal again and it worked! I think that tensorflow should provide a better error message to advise the user with a more detailed explanation.", "I am on windows 10 , cuda 8 and cudnn 6   with : \r\n\r\nname: Quadro K620\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.66GiB\r\n\r\nPretty much same steps worked out for me too, I have little understanding how that worked. I just close all the windows, closed python terminal opened on pycharm --including those windows opened by the earlier execution of the same program to plot progress in the training and reopen and run  -  it works with no error.  The earlier errors reported seems to give no direct clue --- \r\n", "Hello,\r\nI had the same problem, running python with sudo solved my problem.\r\n", "@SimonWalsh1000  You are my hero !!   It works for me as well !", "@hesamaraghi Running with `sudo` also helped us. We were able to run as non-root by adding our non-root user to `nvidia-persistenced` group. See my original comment: https://github.com/tensorflow/tensorflow/issues/14048#issuecomment-340898847", "I had the same problem in Ubuntu 16.04 and cuda-8.0 (with GTX1080Ti). I'd just like to inform any of you with the same problem that the solution given by @SimonWalsh1000 worked for me perfectly (i.e., the CUBLAS initialisation problem was solved by `sudo rm -rf ~/.nv/`). So, many thanks @SimonWalsh1000, it did cost me some hours...", "@SimonWalsh1000 It really works. Thanks so much!", "@SimonWalsh1000 it works like a charm, thank you !!!!", "I had the same problem in on Windows 10, CUDA 8.0, cuDNN 6.1 with GTX1070Ti.\r\nI find the reason: i have runned tensorflow code in annconda spyder IDE, after that I run another tensorflow code in annconda prompt.\r\nsolve it by closing spyder IDE\r\n@lockywolf is right\r\n", "I had the same problem. I try the @strickon 's method, and I don't know about \"nvidia-smi\" maybe it is a command on Linux. I solved this problem through update the **cuDNN 6.0 for CUDA8.0**  to **cuDNN 7.0 for CUDA8.0**\r\n\r\n**system at begin:**\r\n- Windows10\r\n- CUDA8.0\r\n- cuDNN6.0\r\n- Anaconda3.5(python3.5)\r\n- GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\n- 2.00GiB freeMemory: 1.66GiB\r\n\r\n**system after solved:**\r\n- Windows10\r\n- CUDA8.0\r\n- **cuDNN7.0**\r\n- Anaconda3.5(python3.5)\r\n- GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\n- 2.00GiB freeMemory: 1.66GiB\r\n\r\nI think this problem may be caused by the mismatch of the version of library and hardware. @chleibig also solve this by update GPU driver. Hope this can be helpful.", "For me putting: **config.gpu_options.allow_growth = True** in the tensorflow session fixed the problem.\r\nCuda 8, tf 1.4, cudnn 6", "run this fix the issue.\r\n\r\nsudo rm -rf ~/.nv", "same question. Is there any solution to solve the problem?\r\nMy situation is:\r\nname: GeForce GTX 1080\r\ntotalMemory: 7.92GiB  freeMemory: 2.50GiB\r\ntensorflow: gpu-1.4.0\r\n\r\nI'm testing one gpu but running three tensorflow instance.\r\nin my code like this: \r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n\r\nthe other two tensorflow instances running fine, but only the last one run error like this:\r\n\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR \r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:672] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)\r\n\r\nwhy? Is gpu config too small: gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\r\nI'm not sure. want some suggestion. I'll try.\r\n\r\n", "Check out my solution....\n\nOn 19 December 2017 at 08:20, tbchj <notifications@github.com> wrote:\n\n> same question. Is there any solution to solve the problem?\n> My situation is:\n> name: GeForce GTX 1080\n> totalMemory: 7.92GiB freeMemory: 2.50GiB\n> tensorflow: gpu-1.4.0\n>\n> I'm testing one gpu but running three tensorflow instance.\n> in my code like this:\n> gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n> sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n>\n> the other two tensorflow instances running fine, but only the last one run\n> error like this:\n>\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn\n> handle: CUDNN_STATUS_INTERNAL_ERROR\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy\n> cudnn handle: CUDNN_STATUS_BAD_PARAM\n> F tensorflow/core/kernels/conv_ops.cc:672] Check failed:\n> stream->parent()->GetConvolveAlgorithms( conv_parameters.\n> ShouldIncludeWinogradNonfusedAlgo(), &algorithms)\n>\n> why? Is gpu config too small: gpu_options = tf.GPUOptions(per_process_gpu_\n> memory_fraction=0.3)\n> I'm not sure. want some suggestion. I'll try.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6698#issuecomment-352670885>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJq-HpINYs1Uae6ghIW3qKCD56SUDhFeks5tB3HZgaJpZM4Lc7S1>\n> .\n>\n\n\n\n-- \nBest\nSimon\n\nSLFWalsh MD MRCP FFRRCSI\nslfwalsh@gmail.com\n", "In my case, I was running torch on a background and have the same problem.\r\nI think... CUDNN_STATUS_INTERNAL_ERROR can happen when other program using cudnn", "In my case, I can run the cudnn in ipython environment, however, I got the same error messages when I tried to run the code in jupyter notebook ", "Hi, I'm having the same problem and none of the suggestions so far has helped me to solve it.\r\nI'm using an Asus Zenbook Pro laptop with Windows 10 with the following specs:\r\n\r\n![imagen](https://user-images.githubusercontent.com/6187545/35390462-571504bc-01ba-11e8-9fb9-647b218f9ae9.png)\r\n\r\nMy GPU specs are the following:\r\n\r\n![imagen](https://user-images.githubusercontent.com/6187545/35390360-e6a49abc-01b9-11e8-8daa-4458a8e492b6.png)\r\n\r\nI'm following this tutorial: https://www.tensorflow.org/get_started/mnist/pros, in which you have to implement and train 1) a softmax regression and 2) a multilayer CNN with the MNIST dataset.\r\n\r\nThese are my codes: [MNIST_Tutorial.zip](https://github.com/tensorflow/tensorflow/files/1664160/MNIST_Tutorial.zip). The zip has 2 files: MNIST_softmax_regression.py and MNIST_multilayer_CNN.py.\r\n\r\n1) When I run MNIST_softmax_regression.py, it works fine:\r\n![imagen](https://user-images.githubusercontent.com/6187545/35392138-ab475648-01bf-11e8-930b-65119023b363.png)\r\nAs you can see, the GPU is getting used and the final accuracy is about 92% as expected according to the tutorial.\r\n\r\n2) However, when I run MNIST_multilayer_CNN.py, python crashes:\r\n![imagen](https://user-images.githubusercontent.com/6187545/35392589-2a6bad7e-01c1-11e8-8428-5872d0015023.png)\r\n\r\nI tried 2 workarounds based on previous suggestions:\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n```\r\nand\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.8\r\nwith tf.Session(config=config) as sess:\r\n```\r\nNone of them worked, although the second one produces the following output:\r\n\r\n![imagen](https://user-images.githubusercontent.com/6187545/35392215-f4289412-01bf-11e8-8c08-bae791a6f334.png)\r\n\r\nas you can see, tensorflow first tries to allocate memory multiple times (CUBLAS_STATUS_ALLOC_FAILED) until it apparently succeeds but then the CUDNN_STATUS_NOT_INITIALIZED error appears and everything fails again.\r\n\r\nBtw, I installed tensorflow according to the alternative approach at the end of these instructions: http://www.python36.com/install-tensorflow-gpu-windows/\r\n![imagen](https://user-images.githubusercontent.com/6187545/35392828-d6b849fc-01c1-11e8-9918-c67bb0ad726f.png)\r\n\r\nI used this CUDA installer: \r\n![imagen](https://user-images.githubusercontent.com/6187545/35392911-24d21014-01c2-11e8-95fe-472639e57d6b.png)\r\n![imagen](https://user-images.githubusercontent.com/6187545/35393565-102aafde-01c4-11e8-86ce-5551c496d233.png)\r\n\r\nAnd used this .whl file to install tensorflow:\r\n![imagen](https://user-images.githubusercontent.com/6187545/35393014-7f212b54-01c2-11e8-81ed-7070c2346d75.png)\r\n\r\nHere some more info about python, pip and conda:\r\n![imagen](https://user-images.githubusercontent.com/6187545/35394028-6997dbf4-01c5-11e8-9be2-9a9bc4989733.png)\r\n\r\nAny help will be deeply appreciated.\r\nThanks in advance.", "Hello,\r\nI'm facing the same issue on two different machines:\r\n\r\nSetup 1:\r\nWindows 10 Pro 64bit\r\n[GPU Info](http://gpuz.techpowerup.com/18/01/26/mm7.png)\r\nCuda 8.0\r\ncudnn 6.0\r\nTensorflow 1.4\r\nPython 3.6.4\r\n\r\nSetup2:\r\nWindows 10 Pro 64bit\r\n[GPU Info](http://gpuz.techpowerup.com/18/01/26/das.png)\r\nCUDA 8.0\r\ncudnn 6.0\r\nTensorflow 1.4\r\nPython 3.6.2\r\n\r\nAny updates?", "Have very similar set up to above, running on:\r\n\r\nwindows 10\r\nGPU\r\ntensorflow 1.5\r\nCUDA 9.0.176\r\ncudnn 7\r\npython 3.6.4, anaconda\r\n\r\nI tried the config changes and I'm still getting the \"CUDNN_STATUS_NOT_INITIALIZED\" set of errors.\r\n\r\nI'm not sure where the equivalent of the .nv folder resides on windows, so I wasn't able to run the @SimonWalsh1000 solution.\r\n\r\n@HeinzBenjamin, any success?\r\n\r\nEDIT: Still stumped, could it be because I'm on tensorflow 1.5 & CUDA 9?", "I've met the same issue.\r\nHowever, I found that after I installed CUDA 9.0, my driver will not be the latest version.\r\nSO, try to update your Nvdia driver to the latest version and restart your PC. It works for me!", "yesterday my code was working just fine, there was an update to ubuntu this morning and now my code produces this. nothing else has changed.\r\n\r\n2018-02-11 07:54:57.097712: E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018-02-11 07:54:57.097756: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018-02-11 07:54:57.097767: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms)\r\n\r\nI have rebooted the system a dozen times.\r\nafter a few reboots, the error changed to\r\n\r\n2018-02-11 07:19:33.487404: I tensorflow/stream_executor/cuda/cuda_dnn.cc:393] possibly insufficient driver version: 384.111.0 2018-02-11 07:19:33.487423: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM 2018-02-11 07:19:33.487439: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms)\r\n\r\nbut after upgrading to 390.25 it now produces the first error again.\r\n\r\nmy other tensorflow code works just fine.\r\n\r\ni also tried removing the nv directory but that had no effect \r\n\r\nubuntu 17.10, gtx 1060 6gb", "I got this error on Windows 10 with CUDA 9.0 and a GT 750M I solved it by limiting the GPU usage to 0.7 with: config.gpu_options.per_process_gpu_memory_fraction = 0.7\r\n\r\nAs someone else posted, anything higher than 0.7 crashes Python.\r\n\r\n", "After also receiving the trinity of errors:\r\n\r\n```\r\nCUDNN_STATUS_BAD_PARAM\r\nCUDNN_STATUS_NOT_INITIALIZED\r\nconv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms)\r\n```\r\n\r\nTried @zzhang68 's[ solution](https://github.com/tensorflow/tensorflow/issues/6698#issuecomment-364702793)...Updated drivers after 9.0 installed older drivers.\r\n **_And it worked!_**\r\n\r\nWindows 10 | GTX 980 Ti\r\n[CUDA 9.0](https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal) (which came with outdated drivers!!!!)\r\n[\\cudnn-9.0-windows10-x64-v7\\cuda\\bin  (cudann64_7.dll) in PATH](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.5/prod/9.0_20171129/cudnn-9.0-windows10-x64-v7)\r\n\r\npython 3.6 miniconda\r\ntensorflow-gpu 1.5.0 \r\n\r\n", "face same problem. tf1.5 py2.7 titan x cuda8. \r\n`config.gpu_options.allow_growth = True `\r\nnot work", "I got this error on windows 10 with CUDA 9.0 and GTX 1060.\r\npython 3.5\r\ntensorflow-gpu 1.5.0\r\nI find a easy way to solve it : update my NVIDIA Display Driver to the newest version,reboot PC \r\nthen it worked!", "@SimonWalsh1000 , it really works for me, thanks a lot!", "The solution from @strickon and @ggranum plus a driver update resolved this for me. My guess is that some people have customized power configurations that deflate some functionality until it's needed.", "updating my gpu driver solved this issue for me. my gpu driver was december 2017 and the latest was 26 feb 2018. \r\n\r\nyou need to have the correct tensorflow, CUDA version, cuDNN version and gpu driver in order to avoid this issue\r\n\r\nmy spec:\r\ntensorflow 1.6\r\ncuDNN v7.0.4 (Nov 13, 2017), for CUDA 9.0 (i had to use this version for my TF to work)", "Here's how I fixed it. I had both CUDA 9.1 and CUDA 9.0 installed. Like others, I had to upgrade my GPU drivers again after installing CUDA (via the Geforce Experience program). Keras' backend TensorFlow is using CUDA 9.0 as of today's date, so make sure you have that installed. Then, download cuDNN 7.0.5 (not the latest 7.1 version) from https://developer.nvidia.com/rdp/cudnn-download and then extract it and copy the bin, include, etc folders over to your C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 folder. Now it should work.", "Thanks for all this help and after I try degradating my cuCNN from cnDNN-9.1 into cnDNN-9.0 and it works.\r\nMy enviroment is Centos7 + CUDA 9.0 + Tensorflow 1.6", "Same error on Python3.5, ubuntu 16.04, tf1.5\r\nUpdating the gpu driver to version of 390.42 solved this issue for me.\r\n", "Hi Guys,\r\n\r\n\r\nI have just got the same problem \r\n    **\"  E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \"**\r\n\r\n\r\nand solved by:\r\n 1- Updating the NVIDIA Geforce920M's driver\r\n 2- Setting properly the tf session as follows:\r\n          config = tf.ConfigProto()\r\n          config.gpu_options.allow_growth = True\r\n          sess = tf.Session(config=config)\r\n3-    Restarting the Pc\r\n\r\nAfter that I got a more precised error message:\r\n**\"cuDNN7.1 found, but cuDNN7.0 expected. Upgrade\"**\r\n\r\nAnd solved by:\r\ninstead of upgrading the rest(tf,cuda,..) to meet cuDNN, I rather downgraded cuDNN7.0 to meet the rest.\r\n (downgrading cuDNN from 7.1 to 7.0.4 ) and it worked good.", "I also encountered this error when I was running [The Cnn_Mnist.py](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/examples/tutorials/layers/cnn_mnist.py)\r\n\r\n\r\n## environment INFO\uff1a\r\n\r\n* Window10 + tensorflow_gpuV1.6 + cudav9.0\uff0c cudnnv7.0  + Python3.5\uff08Anaconda\uff09+ GeForce 920MX\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 385.54                 Driver Version: 385.54                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce 920MX      WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   37C    P0    N/A /  N/A |     84MiB /  2048MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     11988      C   ...naconda3\\envs\\tensorflow_GPU\\python.exe N/A      |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n## Error INFO:\r\n\r\n```\r\n2018-03-20 13:38:27.439071: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018-03-20 13:38:27.443473: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018-03-20 13:38:27.449591: F C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)\r\n```\r\n## Sincerely hope to get everyone's help  :D\r\n", "In my case (Windows 10), this problem was caused by using the wrong version of cuDNN. Although I followed TensorFlow's official instructions closely, I accidentally had downloaded version 7.0.5 for CUDA 9.1, while TF calls explicitly for CUDA 9.0.\r\n\r\nAs soon as I corrected the cuDNN mistake, my convnets started working \ud83d\udcaf \ud83d\udc4d \ud83e\udd47 :)", "Same issue tf 1.2, cuda 8.0, cudnn 5.1\r\nNvidia updated drivers", "Well, I managed to  update the nvidia driver to the last version according to cuda, and it works. So, you can try this method. \r\n\r\nWell, Well. It can't work well. The problem occurs again", "Using: cudnn-9.0-windows10-x64-v7 and tensorflow-gpu==1.7.0\r\n\r\ntutorials\\image\\imagenet>python classify_image.py\r\nfails with error: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\nAdding the three lines of code from ggranum above solves the problem", "For me the problem was using wrong cudnn lib\r\nI used cudnn for cuda 9.1 when I had cuda 9.0. So i reinstalled cudnn for cuda 9.0 and everything worked.", "Got the same problem with Win10/Anaconda3/tf-1.3/keras-2.1.3\r\nadd the following code to the very beginning of the .py file, which solves my problem.\r\n\r\n```\r\nfrom __future__ import print_function, division\r\nimport tensorflow as tf\r\nfrom keras.backend.tensorflow_backend import set_session  \r\nconfig = tf.ConfigProto()  \r\nconfig.gpu_options.allow_growth = True  \r\nset_session(tf.Session(config=config)) \r\n```\r\n", "@serans1 \r\nThis works for me :)", "Thank you @zzhang68 . Your solution worked for me.", "Adding this in the begining of the file worked for me:\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)", "GTX 1070. Was getting this issue. My driver was last updated at 2017. Updated it to the latest driver (May 2018), reset my computer and stopped getting the problem. Hope this helps", "works for me too with @zzhang68 solution.\r\nUbuntu16.04, tensorflow1.7, nvidia1080, cuda9.0, cudnn7.05.\r\nAfter updating driver to 390.59, the problem disappeared.", "Another option for win10 using tensorflow cpu...try \r\n\r\ndef run_inference_for_single_image(image, graph):\r\n  with graph.as_default():\r\n    config = tf.ConfigProto(\r\n        device_count = {'GPU': 0}\r\n    )\r\n    with tf.Session(config=config) as sess:\r\n", "@lwd1132438569  May I ask which \"latest version\" do you mean? I also encounter this problem with my Ubuntu, and I have python 3.5.2, CUDA 9.0,  tensorflow-gpu 1.9.0, the Driver is 390.48 right now.\r\nI wanna try, but I am afraid tensorflow won't support the 'latest' version now....\r\nThanks1 ", "@vburca thank you so much. I did not realize that having another jupyter noteboook would use up GPU memory. Thanks a lot!!!", "I faced the same problem. I my case I downgraded the tensorflow's version and it worked for my application.", "I found the same problem. In my case, that reason was system memory shortage. When I finished other app running, that problem had gone.", "```\r\n2018-09-03 22:50:26.576765: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2018-09-03 22:50:26.576831: E tensorflow/stream_executor/cuda/cuda_dnn.cc:360] Possibly insufficient driver version: 390.77.0\r\n[1]    8515 segmentation fault (core dumped)  python3 training.py\r\n\r\n```\r\n\r\nGTX1070\r\nCUDA9.0\r\nCUDNN7.1 for CUDA9.0\r\nTensorFlow 1.10.1\r\nRuning a simple tensorflow like hello world without problem.\r\nNowhere to know why this happen.................", "definitely cuda related memory problem, kill all other cuda related process and train/test ur model, that should solve the problem ", "@drproy2k solution seems effective for me as well. The problem was that I was running another jupyter notebook instance with keras, and I was trying to run keras training in Pycharm. So simply closing jupyter notebook and killing this process solved this problem.", "[Solved] In my case, I had installed CUDA v9.2 and the corresponding cuDNN, but had not correctly installed cuDNN specific to CUDA v9.0 which tensorflow requires. \r\n\r\nEnsure that you download the correct version of cuDNN from here: https://developer.nvidia.com/rdp/cudnn-archive\r\n\r\nand **NOT** the one from here: https://developer.nvidia.com/cudnn", "The golden trick, restart everything, worked for me.", "**Restart** did the trick for me too \ud83d\udc4d \r\n(But an explanation why this happens would be really nice)", "> cuDNN\r\n\r\nI was facing the same problem. Models with convolution layers would not work. \r\nI downloaded cuDNN version 7.0 for CUDA 9.0 .  After replacing the file cudnn64_7.dll , I can use convnets without any hassles.\r\n\r\nVersion of the DLL causing problems=> **6.14.11.9020**\r\nVersion of the DLL which solved the problem=> **6.14.11.9000**\r\nTensorflow GPU version=> **1.11.00**\r\nCUDA version=> **9.0**\r\nPython version=>**3.5**\r\nOS=>**Windows 10**\r\nOther steps=> Create a BAT file to append to the PATH variable and then launch CMD.EXE with /k option\r\nthanks all.\r\n", "> I was able to get a program to work by limiting the gpu usage. In my case with a 3gb gtx 1060 on ubuntu 16.04, if I set gpu option per_process_gpu_memory_fraction to .7 it works. Anything higher, I get these errors\r\n> \r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n> F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n> \r\n> It could be a case of bad error reporting by tensorflow. Seems completely unrelated. Maybe it is a clue to getting this resolved in a better manner?\r\n\r\nGreat,when i decrease the gpu_memory_fraction from 0.8 to 0.7,it  start working!", "I faced this issue after **accidentally upgrading tensorflow-gpu** from version 1.6.0 to 1.18.0. This caused instability due to the versions both of CUDA and cuDNN. The solution was rolling back to tensorflow-gpu 1.6.0.\r\n\r\nThis was the solution to my problems:\r\n\r\nhttps://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible\r\n\r\nWhenever you start facing facing this kind of issues, before you upgrade your NVIDIA dependencies, **ALWAYS** try to solve the problem by uninstalling the versions of tensorflow and installing a version compatible with your CUDA dependencies first. \r\n\r\n**Step 1:** Check your tensorflow packages versions. If you have GPU, I recommend uninstalling the cpu-version of tensorflow in order to avoid conflicts.\r\n\r\n`pip list | grep tensorflow`\r\n\r\n**Step 2:** Uninstalling tensorflow-gpu. \r\n\r\n`pip uninstall tensorflow`\r\n\r\n**Step 3:** Check your CUDA and cuDNN versions. You may need to adjust these paths.\r\n\r\n-- CUDA\r\n`cat /usr/local/cuda/version.txt`\r\nIn case this fails, find your cuda version text file using:\r\n`sudo find / -name version.txt `\r\n\r\n-- cuDNN\r\n`cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2`\r\nIn case this fails, find your cuda version text file using:\r\n`sudo find / -name cudnn.h `\r\n\r\n**Step 4:** Check if your tensorflow-gpu, cuda and cudnn versions match this table.\r\n![image](https://user-images.githubusercontent.com/22535074/47266659-9c77a280-d531-11e8-9fe3-d1c154ecde71.png)\r\n\r\nIn my case, I needed **tensorflow-gpu 1.6.0** in order to match the other requirements. \r\n\r\nSo I installed this version using:\r\n`\r\npip install tensorflow-gpu==1.6.0\r\n`\r\nthese are the specifications that worked!\r\n\r\n**OS:** Ubuntu 16.04 \r\n**CUDA Version:** 9.0, V9.0.176\r\n**cuDNN Version:** 7.0\r\n**Tensorflow-gpu Version:** 1.6.0\r\n**Python Version:** 3.5.0\r\n\r\nGood luck!\r\n", "In my case, I forgot to close jupyter notebook when I started to run another piece of code in VS code, Close jupyter notebook fixed the problem.", "I faced this same problem.\r\nIn my case i was running Jupyter notebook while training my network.\r\nClosing Jupyter notebook fixed my problem.\r\n\r\n(I think it might have to do something with too high demands of my GPU)\r\n\r\nHope this helped!\r\n", "hi,guys,i faced the same issues.i using win10 tensorflow-gpu1.8.0 cuda 9.0 NVIDA gtx1050Ti ,when i change the version of cudann from 7.0 to 7.1,the problem solved", "I faced the same problem today (gtx1080, cuda 9.2, tfversion = 1.12.0). So in my case, I was running Jupyter notebook , and then I tried running my other script, that's when the error was thrown. What Solved is, like @RoytenBerge said, shutting down the jupyter kernal.  \r\n\r\n\r\n", "it worked for me when adding these lines of code to the begining of  script    @Codersadis\r\n\r\nadd the following code to the very beginning of the .py file, which solves my problem.\r\n\r\nfrom __future__ import print_function, division\r\nimport tensorflow as tf\r\nfrom keras.backend.tensorflow_backend import set_session  \r\nconfig = tf.ConfigProto()  \r\nconfig.gpu_options.allow_growth = True  \r\nset_session(tf.Session(config=config)) ", "and Thanks@Codersadis", "@drproy2k thank you, it worked for me too. i was running anaconda prompt while spyder is running. after i shut down spyder, it worked perfectly!", "This error is due a RAM memory issue. Suggest you increase to 32GB or 64GB of DDR3 or DDR4 RAM.\r\nAlso reduce the quantity/size of data that is being inferenced.\r\n\r\nIts not the GPU. I have 2 X 1080Ti cards in SLI.\r\n", "I followed version installation guide to resolve this-\r\nhttps://www.tensorflow.org/install/source#tested_source_configurations. The compatible configuration:-\r\nTF 1.12\r\nTF-gpu 1.9 \r\nCUDA 8\r\n\r\n", "same issue with GeForce GTX 970, CUDNN 7.4.1, CUDA 9.0.176, TF-gpu 1.12.0", "I was facing the same problem when using the community supported version of tensorflow inside a conda environment (i.e. using > conda install tensorflow-gpu )\r\n\r\nTurns out this version is not actually good in all situations (even though I've been using it on other machines). The best version to use is the pip installable version https://www.tensorflow.org/install/pip inside a conda environment. When I did this everything worked.", "I didn't realize that I had the Cuda 10.0 version of the CUDNN lib installed alongside the CUDA 9.0 that I had installed presently. Once I downloaded and replace the V10 CUDNN with the V9.0 CUDNN everything worked just fine!\r\nThis was an overlook from failing to install things correctly, and looking back I can see why... If you've made it this far and are tired of experimenting, I've written a blog post at https://aaronjencks.blogspot.com/2019/03/the-ultimate-guide-to-installing.html that will walk you through the entire process of getting tensorflow and all of its dependencies to work from start to finish", "@kheffah  having same problem within conda. Already using pip for installing TF and Keras.\r\nGPU GT 840M, compute compatible 5.0, CUDA 9, cuDNN 7.4.2, TF 1.12.0. Windows 8 x64\r\n\r\ntesting code run just fine\r\n```\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\n```\r\n\r\nthis is the error in spyder. already try the memory 0.7 and growth trick. no luck\r\n```\r\nclassifier.fit_generator(training_set,\r\n                    steps_per_epoch=32,\r\n                    epochs=25,\r\n                    verbose=1,\r\n                    validation_data=test_set,\r\n                    validation_steps=6.25)\r\nEpoch 1/25\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-4-6d704090deaf>\", line 11, in <module>\r\n    validation_steps=6.25)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1418, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training_generator.py\", line 217, in fit_generator\r\n    class_weight=class_weight)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1217, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2715, in __call__\r\n    return self._call(inputs)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2675, in _call\r\n    fetched = self._callable_fn(*array_vals)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n\r\n  File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]\r\n\t [[{{node loss/mul/_91}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_609_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "Switch to tensorflow 1.7\n\nOn Thu., 3 Jan. 2019, 19:29 maxi.wu <notifications@github.com wrote:\n\n> @kheffah <https://github.com/kheffah> having same problem within conda.\n> Already using pip for installing TF and Keras.\n> GPU GT 840M, compute compatible 5.0, CUDA 9, cuDNN 7.4.2, TF 1.12.0.\n> Windows 8 x64\n>\n> testing code run just fine\n>\n> a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n> b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n> c = tf.matmul(a, b)\n>\n> this is the error in spyder. already try the memory 0.7 and growth trick.\n> no luck\n>\n> classifier.fit_generator(training_set,\n>                     steps_per_epoch=32,\n>                     epochs=25,\n>                     verbose=1,\n>                     validation_data=test_set,\n>                     validation_steps=6.25)\n> Epoch 1/25\n> Traceback (most recent call last):\n>\n>   File \"<ipython-input-4-6d704090deaf>\", line 11, in <module>\n>     validation_steps=6.25)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n>     return func(*args, **kwargs)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1418, in fit_generator\n>     initial_epoch=initial_epoch)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training_generator.py\", line 217, in fit_generator\n>     class_weight=class_weight)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1217, in train_on_batch\n>     outputs = self.train_function(ins)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2715, in __call__\n>     return self._call(inputs)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2675, in _call\n>     fetched = self._callable_fn(*array_vals)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1439, in __call__\n>     run_metadata_ptr)\n>\n>   File \"c:\\Users\\maxi.wu\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n>     c_api.TF_GetCode(self.status.status))\n>\n> UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n> \t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]\n> \t [[{{node loss/mul/_91}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_609_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6698#issuecomment-451079405>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABE215xY0OhbFjA_GhVtEIDl_IB4qQGmks5u_b9NgaJpZM4Lc7S1>\n> .\n>\n", "i had the same problem on win10 system. but it is found to be memory problem. kill the other running app which consumes huge memory resources and have a try. ", "I had a similar problem on windows 10 NVIDIA GEFORCE GTX 1050 and as soon as I closed all other running tasks, and retried as suggested by @xhm1014 above, my code just started running like that. I  think this must be a memory related issue.", "Definitely memory related. You should upgrade your RAM up to 64GB.\n\nOn Fri, Jan 18, 2019 at 5:30 PM Samuel Nde <notifications@github.com> wrote:\n\n> I had a similar problem on windows 10 NVIDIA GEFORCE GTX 1050 and as soon\n> as I closed all other running tasks, and retried as suggested by @xhm1014\n> <https://github.com/xhm1014> above, my code just started running like\n> that. I think this must be a memory related issue.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6698#issuecomment-455441208>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABE217cdDKUuRtdD9jJ_eh2tJWrm2fjeks5vEWnwgaJpZM4Lc7S1>\n> .\n>\n", "I had the error and I 'fixed' it by closing my multiple instances of Jupyter and closing other applications. I'm new to working with tensorflow in general so it's likely this only fixed my problem.", "E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\nI had this issue with 10.1 Cuda+cuDNN7.5 and TF 1.11 compiled from source with cuda. The script I was trying to use needed these lines inserted somewhere:\r\n`config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True`\r\n\r\nand then later:\r\n`sess = tf.Session(graph=detection_graph,config=config)`\r\n\r\nThis  done, a lot of \"GPU out of memory errors\" - but detection goes on very quickly as I suppose it should when we're using GPU. Thanks for sharing!", "I faced the same issues.and use below line fixed it.  check [here](https://www.tensorflow.org/install/gpu) get detail.\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 \r\n", "> @EncodeTS I just added a minimal reproducible example to my first post. Could you check if it reproduces the problem on your machine? On my machine, one convolutional layer works but not two convolutional layers, which led me to think that the problem might be caused by some resource limitations.\r\n\r\nActually, I'm working on Ubuntu 18.04, not macOS, but this looks to make sense that it might be caused by some resource limitations. Me either faced the same issue on GTX 1050 ti (4 GB)  but the issue has gone away when I run the same architecture on GTX 1080 ti (11 GB). Though all the environments are not the same between the two systems, I tried my best by utilizing the docker container.", "This problem is generally related to the version of cuda and GPU memory, if former, the easiest way is to change your cuda version by Anaconda\uff01if later, you can find some ways to solve in other answers.\r\n\u8fd9\u4e2a\u95ee\u9898\u4e00\u822c\u4e0e\u663e\u5b58\u548ccuda\u7248\u672c\u6709\u5173\uff0c\u5982\u679c\u5c1d\u8bd5\u4e86\u4e0a\u9762\u7684\u66f4\u6539GPU memory\u7684\u65b9\u6cd5\u65e0\u6548\uff0c\u8003\u8651\u66f4\u6539cuda\u7248\u672c\uff0c\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u4e0d\u7528\u53bb\u7ba1\u7cfb\u7edf\u88c5\u4e86\u4ec0\u4e48cuda\u7248\u672c\uff0c\u76f4\u63a5\u5728Anaconda\u4e2d\u7684\u9879\u76ee\u73af\u5883\u4e0b\u4fee\u6539cuda\u7248\u672c\u5373\u53ef\uff0c\u4eb2\u6d4b\u6709\u6548\u3002", "if you are still getting this issue, try the following. it worked for me\r\n`\r\ntf.config.gpu.set_per_process_memory_growth(True);\r\ntf.config.gpu.set_per_process_memory_fraction(0.4);`\r\n\r\ntensorflow 2 alpha\r\ncuda 10.0\r\nGTX 1650", "I have similar issue: CUDNN_STATUS_ALLOC_FAILED. \r\nI broke my head for 3-4 hours. Finally fixed.\r\nthis indeed works, as mentioned above by many :\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)\r\n\r\n**But the key is to write it immediately below \"import tensorflow as tf\"** which I wasn't doing. I had written it after all the imports. \r\n", "May be tensorflow-gpu version has problems, you should check your own versions try again and again, uninstall and install..... tensorflow-gpu\u627e\u5230\u5bf9\u5e94\u7684\u7248\u672c\u53f7\u7136\u540e\u5378\u8f7d\u518d\u91cd\u88c5", "> it worked for me when adding these lines of code to the begining of script @Codersadis\r\n> \r\n> add the following code to the very beginning of the .py file, which solves my problem.\r\n> \r\n> from **future** import print_function, division\r\n> import tensorflow as tf\r\n> from keras.backend.tensorflow_backend import set_session\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> set_session(tf.Session(config=config))\r\n\r\nI am getting the same error with `tensorflow-gpu == 1.8.0`, `cudnn version = 7.0.5` and `cuda 9.1.85` \r\n, `ubuntu 16.04` even after I add the above suggested solution.\r\nFollowing is the stack-trace:\r\n```\r\nINFO - Waveunet Training - Running command 'run'\r\nINFO - Waveunet Training - Started\r\nSCRIPT START\r\nEPOCH: 0\r\nDataset ready!\r\nTraining...\r\nSep_Vars: 10265550\r\nNum of variables65\r\n2019-07-25 05:10:09.872823: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-25 05:10:10.286584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-25 05:10:10.286914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48\r\npciBusID: 0000:00:05.0\r\ntotalMemory: 7.92GiB freeMemory: 7.83GiB\r\n2019-07-25 05:10:10.286964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2019-07-25 05:10:10.640890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-25 05:10:10.640952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2019-07-25 05:10:10.640968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2019-07-25 05:10:10.641194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7566 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1)\r\n2019-07-25 05:10:27.643833: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:94] Filling up shuffle buffer (this may take a while): 2054 of 4000\r\n2019-07-25 05:10:35.917445: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:129] Shuffle buffer filled.\r\n2019-07-25 05:10:36.175698: E tensorflow/stream_executor/cuda/cuda_dnn.cc:455] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2019-07-25 05:10:36.175820: E tensorflow/stream_executor/cuda/cuda_dnn.cc:463] possibly insufficient driver version: 384.183.0\r\n2019-07-25 05:10:36.175842: E tensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2019-07-25 05:10:36.175859: F tensorflow/core/kernels/conv_ops.cc:713] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted (core dumped)\r\n```\r\nPlease help", "> \r\n> \r\n> I have similar issue: CUDNN_STATUS_ALLOC_FAILED.\r\n> I broke my head for 3-4 hours. Finally fixed.\r\n> this indeed works, as mentioned above by many :\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = tf.Session(config=config)\r\n> \r\n> **But the key is to write it immediately below \"import tensorflow as tf\"** which I wasn't doing. I had written it after all the imports.\r\n\r\ngreat reply, worked for me !!", "> > it worked for me when adding these lines of code to the begining of script @Codersadis\r\n> > add the following code to the very beginning of the .py file, which solves my problem.\r\n> > from **future** import print_function, division\r\n> > import tensorflow as tf\r\n> > from keras.backend.tensorflow_backend import set_session\r\n> > config = tf.ConfigProto()\r\n> > config.gpu_options.allow_growth = True\r\n> > set_session(tf.Session(config=config))\r\n> \r\n> I am getting the same error with `tensorflow-gpu == 1.8.0`, `cudnn version = 7.0.5` and `cuda 9.1.85`\r\n> , `ubuntu 16.04` even after I add the above suggested solution.\r\n> Following is the stack-trace:\r\n> \r\n> ```\r\n> INFO - Waveunet Training - Running command 'run'\r\n> INFO - Waveunet Training - Started\r\n> SCRIPT START\r\n> EPOCH: 0\r\n> Dataset ready!\r\n> Training...\r\n> Sep_Vars: 10265550\r\n> Num of variables65\r\n> 2019-07-25 05:10:09.872823: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2019-07-25 05:10:10.286584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-25 05:10:10.286914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\n> name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48\r\n> pciBusID: 0000:00:05.0\r\n> totalMemory: 7.92GiB freeMemory: 7.83GiB\r\n> 2019-07-25 05:10:10.286964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n> 2019-07-25 05:10:10.640890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-07-25 05:10:10.640952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n> 2019-07-25 05:10:10.640968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n> 2019-07-25 05:10:10.641194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7566 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1)\r\n> 2019-07-25 05:10:27.643833: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:94] Filling up shuffle buffer (this may take a while): 2054 of 4000\r\n> 2019-07-25 05:10:35.917445: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:129] Shuffle buffer filled.\r\n> 2019-07-25 05:10:36.175698: E tensorflow/stream_executor/cuda/cuda_dnn.cc:455] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n> 2019-07-25 05:10:36.175820: E tensorflow/stream_executor/cuda/cuda_dnn.cc:463] possibly insufficient driver version: 384.183.0\r\n> 2019-07-25 05:10:36.175842: E tensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n> 2019-07-25 05:10:36.175859: F tensorflow/core/kernels/conv_ops.cc:713] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\n> Aborted (core dumped)\r\n> ```\r\n> \r\n> Please help\r\n\r\nChanging the Nvidia driver to 396+ solved the issue for me.", "It has to do with the memory fraction available to load GPU resources to create cudnn handle, also known as `per_process_gpu_memory_fraction`.\r\nReducing this memory fraction by yourself will solve the error.\r\n\r\n\r\n    > sess_config = tf.ConfigProto(gpu_options =\r\n    > tf.GPUOptions(per_process_gpu_memory_fraction=0.7),\r\n    > allow_soft_placement = True)\r\n    > \r\n    > with tf.Session(config=sess_config) as sess:\r\n    >      sess.run([whatever])\r\n\r\nUse as small fraction as could fit in your memory. (In the code, I use 0.7, you can start with 0.3 or even smaller, then increase until you get the same error, that's your limit.)\r\nPass it to your `tf.Session()` or `tf.train.MonitoredTrainingSession()` or Supervisor's `sv.managed_session()` as config.\r\n\r\nThis should allow your GPU create a cudnn handle for your TensorFlow code.", "I was getting the following error with tensorflow 2.0 in my conda environment.\r\n\r\n```2019-12-03 23:48:29.888625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-12-03 23:49:06.381259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-03 23:49:07.220066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:01:00.0\r\n2019-12-03 23:49:07.236411: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-03 23:49:07.247476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-03 23:49:07.256881: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-03 23:49:07.269536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:01:00.0\r\n2019-12-03 23:49:07.281954: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-03 23:49:07.295302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-03 23:49:08.589865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-03 23:49:08.599121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-12-03 23:49:08.610543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-12-03 23:49:08.616005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4627 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-12-03 23:49:58.521484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-12-03 23:49:59.604517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-03 23:50:04.209110: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2019-12-03 23:50:04.216670: E tensorflow/stream_executor/cuda/cuda_dnn.cc:333] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2019-12-03 23:50:04.226172: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2019-12-03 23:50:04.234741: E tensorflow/stream_executor/cuda/cuda_dnn.cc:333] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2019-12-03 23:50:04.244958: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node sequential/conv2d/Conv2D}}]]\r\n```\r\n\r\nso i added the following code to my CNN\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n```\r\n\r\nMy output is now\r\n\r\n```\r\n2019-12-04 00:10:07.708573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-12-04 00:10:11.643304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-04 00:10:12.753615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:01:00.0\r\n2019-12-04 00:10:12.769498: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-04 00:10:12.783900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-04 00:10:54.941468: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-04 00:10:55.372516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:01:00.0\r\n2019-12-04 00:10:55.383385: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-04 00:10:55.406053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-04 00:10:56.741665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-04 00:10:56.747255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-12-04 00:10:56.752302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-12-04 00:10:56.756861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4627 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-12-04 00:11:08.281356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-12-04 00:11:08.934804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-04 00:11:11.870237: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n```\r\n\r\nAs everyone suggested it is due to tensorflow using all of the GPU/GPUs. My CNN trains without error now.", "> I was facing the same problem when using the community supported version of tensorflow inside a conda environment (i.e. using > conda install tensorflow-gpu )\r\n> \r\n> Turns out this version is not actually good in all situations (even though I've been using it on other machines). The best version to use is the pip installable version https://www.tensorflow.org/install/pip inside a conda environment. When I did this everything worked.\r\n\r\nThat solved for me, thanks!", "This also resolved the issue for me. \r\n\r\nGeForce GTX 1050, CUDA 10.0\r\n\r\nNote: this is the only thing I can find that works in TF 2.0 for now. Thanks!\r\n\r\n\r\n\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(gpus[0], True)\r\n", "> This also resolved the issue for me.\r\n> \r\n> GeForce GTX 1050, CUDA 10.0\r\n> \r\n> Note: this is the only thing I can find that works in TF 2.0 for now. Thanks!\r\n> \r\n> > gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> > tf.config.experimental.set_memory_growth(gpus[0], True)\r\n\r\nThis didn't make any difference for me... TF 2.0, RTX 2060, CUDA 10.1, CuDNN 7.6\r\n\r\nThis is with 16 GB RAM, 6 GB video memory, and a basic MNIST toy model with one conv layer. No memory problems, just a stack trace.\r\n\r\nNo GPU problems at all with Pytorch, as usual", "In my case, I have two machines, both with RTX 2080Ti, TF 2.1, CUDA 10.1, CuDNN 7.6. One works, the other one raises the aforementioned error. Both machines have the same amount of RAM, 16GB. There are hardware differentes, though, like the CPU. But the problem is only occurring when using the GPU.\r\n", "> In my case, I have two machines, both with RTX 2080Ti, TF 2.1, CUDA 10.1, CuDNN 7.6. One works, the other one raises the aforementioned error. Both machines have the same amount of RAM, 16GB. There are hardware differentes, though, like the CPU. But the problem is only occurring when using the GPU.\r\n\r\nSame platform, same problem", "If you are using the latest tensorflow and keras. Try this from [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth), it worked for me:\r\n\r\n```python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n```\r\n", "This one works for me.\r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)", "> This one works for me.\r\n> physical_devices = tf.config.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nThis worked for me. Thanks ", "@Samaritan1011001 your solution works for me thanks a lot.\r\n", "@Samaritan1011001 your solution works for me, too! thanks xD !", "> This problem is generally related to the version of cuda and GPU memory, if former, the easiest way is to change your cuda version by Anaconda\uff01if later, you can find some ways to solve in other answers.\r\n> \u8fd9\u4e2a\u95ee\u9898\u4e00\u822c\u4e0e\u663e\u5b58\u548ccuda\u7248\u672c\u6709\u5173\uff0c\u5982\u679c\u5c1d\u8bd5\u4e86\u4e0a\u9762\u7684\u66f4\u6539GPU memory\u7684\u65b9\u6cd5\u65e0\u6548\uff0c\u8003\u8651\u66f4\u6539cuda\u7248\u672c\uff0c\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u4e0d\u7528\u53bb\u7ba1\u7cfb\u7edf\u88c5\u4e86\u4ec0\u4e48cuda\u7248\u672c\uff0c\u76f4\u63a5\u5728Anaconda\u4e2d\u7684\u9879\u76ee\u73af\u5883\u4e0b\u4fee\u6539cuda\u7248\u672c\u5373\u53ef\uff0c\u4eb2\u6d4b\u6709\u6548\u3002\r\n\r\n\u4e0d\u597d\u610f\u601d\u8bf7\u95ee\u4e00\u4e0b\u8981\u600e\u4e48\u5728anaconda\u91cc\u9762\u76f4\u63a5\u4fee\u6539cuda\u7684\u7248\u672c\u5462\uff1f\u611f\u6fc0\u4e0d\u5c3d\r\nMay I know how to change cuda's version in anaconda prompt? thanks a lot"]}, {"number": 6697, "title": "Temporary fix to failing test method in estimator_test", "body": "", "comments": ["CC @ispirmustafa \r\nWe also need to cherrypick this into our release branch, release tests are failing with this.", "hmm, looks like CPU failure is a very similar test.\r\n`tensorflow/python/training/basic_session_run_hooks_test.py`\r\n\r\nCould you also edit the test above?", "If the basic_session_run_hooks_test test is also failing, too, the issue might be related to the StopAfterNEvalsHook and/or MonitoredSession.\r\n\r\nSee \r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/3161/console", "```\r\nFAIL: test_save_secs_saves_periodically (__main__.CheckpointSaverHookTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/workspace/pip_test/tests/basic_session_run_hooks_test.py\", line 349, in test_save_secs_saves_periodically\r\n    self.global_step.name))\r\nAssertionError: 3 != 4\r\n\r\n----------------------------------------------------------------------\r\nRan 47 tests in 30.707s\r\n\r\nFAILED (failures=1)\r\n```\r\n\r\nAny thoughts on why the hook got called one extra time, @ispirmustafa ?", "closing in favor of #6704 "]}, {"number": 6696, "title": "conv2d / Kernel Size > Input does not raise ValueError anymore", "body": "I noticed that tensorflow 0.10.0 raises a ValueError when I pass a kernel size that is larger than the first input dimensions. The error occurred when I tried to pass data in NCHW format, while the default data_format is NHWC.\r\n\r\nI used this code for replication of the problem:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# NHWC\r\ninput_nhwc = tf.placeholder(tf.float32, [None, 84, 84, 4])\r\noutput_nhwc = tf.contrib.layers.conv2d(input_nhwc, 32, [8, 8], 4)\r\n\r\n# NCHW\r\ninput_nchw = tf.placeholder(tf.float32, [None, 4, 84, 84])\r\noutput_nchw = tf.contrib.layers.conv2d(input_nchw, 32, [8, 8], 4)\r\n```\r\n\r\ntensorflow 0.12.1 does not raise any error, whereas tensorflow 0.10.0 raises the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"conv2d_error.py\", line 5, in <module>\r\n    output_nhwc = tf.contrib.layers.conv2d(input_nhwc, 32, [8, 8], 4)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 171, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 411, in convolution2d\r\n    padding=padding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 394, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2319, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1711, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 246, in conv2d_shape\r\n    padding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 184, in get2d_conv_output_size\r\n    (row_stride, col_stride), padding_type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 149, in get_conv_output_size\r\n    \"Filter: %r Input: %r\" % (filter_size, input_size))\r\nValueError: Filter must not be larger than the input: Filter: (8, 8) Input: (4, 84)\r\n```\r\n\r\nI tried to track if there's any kind of automatic detection of data formats (there is not), and further why the issue persists. All I could find were the filter and input shapes in nn_ops.py, which seem to pass compability tests in tensorflow 0.12.0.\r\n\r\n```\r\nNHWC\r\nFilter shape: (8, 8, 4, 32) | input shape: (?, 84, 84, 4)\r\n----------------\r\nNCHW\r\nFilter shape: (8, 8, 84, 32) | input shape: (?, 4, 84, 84)\r\n```\r\n\r\nI'm not sure if this is expected behavior, or if an exception should have been raised in tensorflow 0.12.1 as well.\r\n", "comments": ["@sherrym do you know why this went away?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing this since it seems stale. Please reopen if it needs attention."]}, {"number": 6695, "title": "Cherry Pick bundle_shim to 0.12.x branch", "body": "bundle_shim.py was added to [\"allows a system to load both legacy session bundle and SavedModel bundle.\"](https://github.com/tensorflow/tensorflow/commit/de1f21e98a6dd14a5e4c1d2d9c14b430f57493b3#diff-340f927179763894226603c8e38e0ec4) however this is only on master. This leads to issues saving and then loading back models, for example see: https://github.com/tensorflow/tensorflow/issues/6336.\r\n\r\nI suggest cherry picking it to current release branch.", "comments": ["We don't plan to cherry-pick this into 0.12, particularly because it's in contrib.\r\n\r\nIt'll be in the next major release, though."]}]