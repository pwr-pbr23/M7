[{"number": 38897, "title": "Qualify TensorFlow Docker Hub image as Docker Certified", "body": "The TensorFlow Docker Hub image should apply for Docker Certified qualification, just like most other popular Docker Hub images. This will not change the current API.\r\n\r\nAccording to the official Docker documentation:\r\n\r\n> The Docker Certification program for Containers and Plugins is designed for both technology partners and enterprise customers to recognize high-quality Containers and Plugins, provide collaborative support, and ensure compatibility with the Docker Enterprise platform. Docker Certified products give enterprises a trusted way to run more technology in containers with support from both Docker and the publisher. The Docker Technology Partner guide explains the Technology Partner program, inclusive of process and requirements to Certify Containers and Plugins.\r\n\r\nMore information:\r\n- https://docs.docker.com/docker-hub/publish/publisher_faq/#what-is-the-certification-program-for-containers-and-plugins-and-what-are-some-benefits\r\n- https://docs.docker.com/docker-hub/publish/certify-images/", "comments": []}, {"number": 38866, "title": "Fit an Use the Keras scikit-learn wrapper with Generator dataset", "body": "**System information**\r\n- TensorFlow version (you are using) : tf  2.2.0-rc3\r\n\r\n- Are you willing to contribute it (Yes/No) : Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWith the Keras scikit-learn wrapper, I want that the fit using generator dataset and generator validation dataset will be available. Actually its requires X,y like-arrays.\r\n\r\n**Will this change the current api? How?**\r\nIt will extend the  Keras scikit-learn wrapper functionalities by permiting to use data augmentation using ImageGenarator.\r\n\r\n**Who will benefit with this feature?**\r\nEvery Keras users that use the wrapper for Computer Vision.\r\n\r\n**Any Other info.**\r\nIt will be useful for memory efficiency usage also.", "comments": []}, {"number": 38828, "title": "AutoGraph and tf.function are not working for TPU", "body": "It seems that TPU only supports keras fit() function, but unable to use functions from tf.function and autographs.\r\n\r\nTensorflow version 2.1\r\npython version 3.6\r\n\r\nIssue can be reproduced in colab.", "comments": ["In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nYou can also share the link of the Colab gist you are using. Thanks!", "Here's the link to colab file\r\nhttps://colab.research.google.com/drive/1hJUOZ5VfqqxIp36jpfVX1tS-r5AUexj-\r\n\r\nError suggests that gradients and optimizer have different scope which is quite reasonable as GradientTape introduces it's own scope.", "> https://colab.research.google.com/drive/1hJUOZ5VfqqxIp36jpfVX1tS-r5AUexj-\r\n\r\n@saahiluppal,\r\nI am unable access this colab link. Please select 'File' -> 'Save a copy as Github Gist', and share the link of the new window. Thanks!\r\n", "Here's the link to gist.\r\nhttps://gist.github.com/saahiluppal/0bf79c27a15eaf6a72b25322eae6b6aa\r\n\r\nThanks.", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/baa48b9fc7e2662d776cdb9dc65e9943/38828-2-1.ipynb) and [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/8ebb5491f3cc5a0bf0f6cd4ffe6e88b5/38828-2-2.ipynb). Please find the attached gist. Thanks!", "Sorry @jvishnuvardhan , I am no longer working on TPUs. I'm going to re-assign it back to you to re-triage. Thanks!", "I face a different error on tf-nigtly [2.4.0-dev20201021], please find the [gist here](https://colab.research.google.com/gist/Saduf2019/996eb8311c72a0aaeb9f2258f600219e/untitled450.ipynb).", "I faced a different error on TF 2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/2bea00ac27ed38c70b7e62f8666141b8/untitled450.ipynb#scrollTo=0t1dEzszdUWS) ..Thanks!", "The error message indicates your client and TPU worker are not the same version. Can you make sure they are both in TF nightly or TF 2.5?", "@rxsang thanks for noticing that ,I have updated the gist in TF v2.5 . "]}, {"number": 38781, "title": "[TFlite dynamic range quantization]How can I disable dynamic quantization of activations? in inference?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n-  Ubuntu 16.04:\r\n- pip installed:\r\n- TensorFlow version 2.1.0:\r\n\r\n\r\n\r\n\r\n\r\nThis document [https://tensorflow.google.cn/lite/performance/post_training_quantization](url)\r\nsaid that we can use \"Dynamic range quantization\",which statically quantizes only the weights from floating point to 8-bits of precision.And \"dynamic-range\" operators will dynamically quantize activations based on their range to 8-bits and perform computations with 8-bit weights and activations.\r\n\r\nBut I want to quantize the weights to int8/uint8, and make sure all the operators are computed using only floating-point kernels at inference,which means no activations will be quantized.\r\n\r\nSo,how can I realize this?How can I avoid quantization of these \"dynamic-range\" operators?\r\n", "comments": []}, {"number": 38778, "title": "Considerations about micro_speech example", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- Tensorflow version: master branch\r\n- Target platform: mbed, STM32F746\r\n\r\n**Describe the problem**\r\nI decided to open this issue because there are some others issues and pull requests related to the `micro_speech` example (like https://github.com/tensorflow/tensorflow/issues/35889),  and so I would like to take stock of the situation, describe the issues I found and how to solve them (see below).\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nTo build the `micro_speech` example I used the steps provided by the documentation related to the example in [README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/README.md):\r\n\r\n- `git clone https://github.com/tensorflow/tensorflow.git`\r\n- `cd tensorflow`\r\n- `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project`\r\n\r\nHowever, I obtained the following error:\r\n`make: *** No rule to make target \"tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/tensorflow/lite/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c\", needed by \"generate_micro_speech_mbed_project\".  Stop.`\r\n\r\nSo, as suggested in https://github.com/tensorflow/tensorflow/pull/36444, I removed `arm_cmplx_mag_squared_q10p6.c/h` from `tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc`.\r\nBut when I launched again the build I obtained this other issue:\r\n`make: *** No rule to make target \"tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/third_party/CMSIS_ext/README.md\", needed by \"generate_micro_speech_mbed_project\".  Stop.`\r\nI managed to solve the new issue by removing `third_party/CMSIS_ext/README.md` line from `tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc` since it is not a header file needed for the build.\r\n\r\nAt this point the project is created in `tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed`, but when I launched the command `mbed compile -m DISCO_F746NG -t GCC_ARM` I got this other error:\r\n`Compile [ 95.6%]: arm_mult_q15.c\r\n[Error] arm_mult_q15.c@101,6: conflicting types for 'arm_mult_q15'\r\n[ERROR] ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:101:6: error: conflicting types for 'arm_mult_q15'\r\n void arm_mult_q15(\r\n      ^~~~~~~~~~~~\r\nIn file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:29:0:\r\n./mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h:1924:8: note: previous declaration of 'arm_mult_q15' was here\r\n   void arm_mult_q15(\r\n        ^~~~~~~~~~~~`\r\nBut again I managed to fix it by simply upgrading Mbed to 6.0.0-alpha-3 version as suggested in https://github.com/tensorflow/tensorflow/pull/37930.\r\n\r\n**I added all the 3 changes in the [micro_speech_fix branch](https://github.com/biagiom/tensorflow/tree/micro_speech_fix) of my tensorflow repo.**\r\n\r\n**Moreover, another useful way to solve the build issues is to use the `cmsis-nn` TAG instead of `CMSIS` (see also https://github.com/tensorflow/tensorflow/issues/35889), that is:**\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"cmsis-nn disco_f746ng\" generate_micro_speech_mbed_project`.\r\nThis also has some \"advantages\":\r\n\r\n- when adding `CMSIS` to `TAGS=,` the [CMSIS-NN optimized kernels](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels/cmsis-nn) are not added to the example during the build instead of the `cmsis-nn` tag. Moreover, since the _tiny_conv_ model is based on a conv2D layer, we can benefit for the optimized implementation of the convolution.\r\n- the source code in [`tensorflow/lite/micro/examples/micro_speech/CMSIS`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/CMSIS) that will be  added to the build using the `CMSIS` TAG is not directly related to the `micro_speech` example but (as far as I know) it is mostly used for [testing purposes (simple_features_generator_test)](https://github.com/tensorflow/tensorflow/blob/2ced38fde44235e3684c91280572fe55707df1a1/tensorflow/lite/micro/examples/micro_speech/Makefile.inc#L249) and for the apollo3 board but not for mbed/stm32f7. In particular, I also noticed that also when building the `simple_features_generator_test` with the `CMSIS` tag, it generates an error which says that the `arm_cmplx_mag_squared_q10p6.c` is not found. This is because make tries to find that file in the CMSIS_ext directory in `tensorflow/lite/micro/tools/make/downloads` but this doesn't exists. Moreover there is no way to download the custom CMSIS (CMSIS_ext) because its url is not defined in [third_party_downloads.inc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/tools/make/third_party_downloads.inc). I also noticed that [mbed_makefile.inc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/tools/make/targets/mbed_makefile.inc) includes [this line](https://github.com/tensorflow/tensorflow/blob/b9edec000c94761fc52e2ce38efa6385fff45f42/tensorflow/lite/micro/tools/make/targets/mbed_makefile.inc#L5), but `CUST_CMSIS_URL` and `CUST_CMSIS_MD5` are never defined.\r\n\r\nIn conclusion, I would like to suggest the following changes:\r\n\r\n1. Update the [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/README.md) related to the `micro_speech` example in order to use the `cmsis-nn` TAG instead of `CMSIS` (or use both if needed).\r\n2. Remove `arm_cmplx_mag_squared_q10p6.c/h` from [`tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc). Otherwise if the files are useful, I suggest to add them directly into [the `tensorflow/lite/micro/examples/micro_speech/CMSIS/` folder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/CMSIS) and remove the dependency from CMSIS_ext. Also we need to update the [mbed_makefile.inc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/tools/make/targets/mbed_makefile.inc) file.\r\n3. Update Mbed to 6.0.0-alpha-3 version (see https://github.com/tensorflow/tensorflow/pull/37930).\r\n4. Remove [`third_party/CMSIS_ext/README.md` line](https://github.com/tensorflow/tensorflow/blob/b9f8df3930b34de2f461bb99813ade4b890e82c6/tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc#L20) from `tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc` since it is not a header file needed for the build.\r\n\r\nI can contribute with some pull requests based on the above ideas if needed and I'm open to other suggestions/ideas in order to improve the `micro_speech` example and solve its issues.\r\n\r\nBest regards,\r\nBiagio.\r\n", "comments": []}, {"number": 38772, "title": "No default summary writer available when using tf.py_function with autograph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 3.6.8\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nUsing `tf.summary` returns `False` inside a `tf.py_function` when using autograph.\r\n\r\n**Describe the expected behavior**\r\n`tf.summary` should return `True`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\ndef eager_pyfunc():\r\n    def inner_func():\r\n        bool_out = tf.summary.scalar('myscalar', tf.constant(32.9))\r\n        tf.print(bool_out, name='log-myscalar-success')\r\n\r\n    tf.py_function(inner_func, [], [], name='log-myscalar')\r\n\r\n    bool_out2 = tf.summary.scalar('myscalar2', tf.constant(52.3))\r\n    tf.print(bool_out2, name='log-myscalar2-success')\r\n\r\n\r\nwith tf.summary.create_file_writer('./logs').as_default():\r\n    tf.summary.experimental.set_step(0)\r\n    eager_pyfunc()\r\n```\r\noutputs:\r\n```\r\n0\r\n1\r\n```\r\nRemoving the `tf.function` outputs:\r\n```\r\n1\r\n1\r\n```\r\n\r\n**Other info / logs** \r\nCould be related to https://github.com/tensorflow/tensorflow/issues/26409.", "comments": ["I have tried on colab with TF version 2.1.0, 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/0e983c11cd21b6e87bbf32a31a5b8196/untitled802.ipynb).Thanks!", "Was able to reproduce the issue in TF 2.6.0-dev20210530,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/605268f00c4c0c9c11e391010ee9d2cb/untitled126.ipynb)..Thanks !"]}, {"number": 38770, "title": ".NET Language Bindings", "body": ".NET is one of the top languages and yet there are no Tensorflow language bindings.  This makes it hard to use in an Enterprise setting and is limiting the adoption of Tensorflow.  This is a huge lever as well to drive adoption deeper into traditional companies.\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently there are Java and Python language bindings for Tensorflow, but no official .NET bindings.  There is a third party unofficial project but it only supports old versions of Tensorflow and is not officially maintained\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThe entire .NET ecosystem, which is about the same size or larger than the Java ecosystem.  Visual Studio is the #1 IDE in the world.\r\n\r\n**Any Other info.**\r\n", "comments": ["Unfortunately we don't have anyone internally that can work on this at the moment. However, PRs are welcome, as well as community maintained repositories."]}, {"number": 38766, "title": "Gradient checkpointing for TF keras models", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI have implemented a version of gradient check pointing for TF keras sequential models (with future plans to extend it for Keras functional API and custom models). The PR can be found here - https://github.com/tensorflow/addons/pull/1600. I initially envisioned it as a package in TF addons repo but reviewers felt it was not right place and could potentially go in as a fix for the existing recompute_grad functionality in TF core - https://github.com/tensorflow/tensorflow/blob/64f4a59d5e39b60d67047d5e0b82de0cbcc6c2df/tensorflow/python/ops/custom_gradient.py#L458\r\n\r\nHere are the issues with the existing implementation of recompute_grad in TF core and my solutions to those\r\n1. Issue - Not usable. Most people have no idea how to use it. No docs or tutorials that explains how to use it.\r\nSolution - My PR provides a notebook tutorial to demonstrate how to use the implemented functionality.\r\n2. For people who did figure out how to use it, no memory savings was observed.\r\nSolution - My PR provides links to results with observed memory savings. Caveat - only CPU profiled results available. GPU and TPU results need to be done.\r\n3. There is probably an expectation that the user explicitly has to partition the model and decorate each partition. This is not user friendly and can make tasks such as transfer learning difficult.\r\nSolution - My PR expects no explicit partitioning of the model. The user just needs to add a single decorator to the model. That is it.\r\n4. No checkpointing functionality is implemented.\r\nSolution - My PR implements the checkpointing functionality that allows the user to balance the tradeoff between memory and compute time.\r\n\r\nDoes it make sense to port the PR to TF core?\r\n\r\n**Will this change the current api? How?**\r\nThe existing implementation can potentially be shoe horned into the existing API for recompute_grad if desirable.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to train models in resource constrained environments.\r\n\r\n**Any Other info.**\r\n", "comments": ["Does it make sense to port the above mentioned PR as a PR to TF core? Also tagging @seanpmorgan  from TF addons to keep in loop. Thanks!", "This functionality would be very helpful for training memory-intensive language models!", "By the way I am thinking of addressing this with separate PRs as follows\r\n1. Provide a fix (which I already have) and tutorial for the existing tf recompute_grad function. From what I understand, using this requires the user to manually partition the model which might be suboptimal in many cases.\r\n2. Provide a new decorator function '@recompute_sequential' that enables gradient checkpointing on Keras sequential models (like in the PR I have linked above). The big advantage over 1 is that the user does not have to manually partition the model.\r\n3. Provide another decorator function '@recompute_functional' that enables gradient checkpointing on Keras functions models and potentially sub-classed models. \r\n\r\nDoes this approach make sense? Thanks!"]}, {"number": 38762, "title": "TPU PyFunction results in UnavailableError: failed to connect to all addresses", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Modified [Colab MNIST guide](https://www.tensorflow.org/guide/tpu)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): `2.2-rc3`\r\n\r\n**Describe the current behavior**\r\nWhen processing pipeline for `tf.data.Dataset` contains usage of `tf.py_function` the `UnavailableError: failed to connect to all addresses` is thrown on TPU environment.\r\n\r\n**Describe the expected behavior**\r\n`tf.py_function` is working on TPU environments. \r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab notebook](https://colab.research.google.com/drive/1D7qU4f1FZqieYHdyUezUEFPWoVZZJSxi) with simplified example. In my original code the preprocessing function is more complicated. \r\n\r\n**Other info / logs**\r\nRelated issue: [34346](https://github.com/tensorflow/tensorflow/issues/34346).\r\nStacktrace:\r\n```\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   1985       ctx.executor = executor_new\r\n-> 1986       yield\r\n   1987     finally:\r\n\r\n14 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    660       except AttributeError:\r\n--> 661         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    662 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in from_compatible_tensor_list(element_spec, tensor_list)\r\n    229       lambda spec, value: spec._from_compatible_tensor_list(value),\r\n--> 230       element_spec, tensor_list)\r\n    231 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in _from_tensor_list_helper(decode_fn, element_spec, tensor_list)\r\n    204     value = tensor_list[i:i + num_flat_values]\r\n--> 205     flat_ret.append(decode_fn(component_spec, value))\r\n    206     i += num_flat_values\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in <lambda>(spec, value)\r\n    228   return _from_tensor_list_helper(\r\n--> 229       lambda spec, value: spec._from_compatible_tensor_list(value),\r\n    230       element_spec, tensor_list)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_spec.py in _from_compatible_tensor_list(self, tensor_list)\r\n    176     assert len(tensor_list) == 1\r\n--> 177     tensor_list[0].set_shape(self._shape)\r\n    178     return tensor_list[0]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)\r\n   1103   def set_shape(self, shape):\r\n-> 1104     if not self.shape.is_compatible_with(shape):\r\n   1105       raise ValueError(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in shape(self)\r\n   1066       except core._NotOkStatusException as e:\r\n-> 1067         six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n   1068 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1587494349.376555159\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3959,\"referenced_errors\":[{\"created\":\"@1587494349.376552078\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-8-f9a6a321af70> in <module>()\r\n      1 train_dataset, test_dataset = get_dataset()\r\n----> 2 list(train_dataset.take(1))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)\r\n    629 \r\n    630   def __next__(self):  # For Python 3 compatibility\r\n--> 631     return self.next()\r\n    632 \r\n    633   def _next_internal(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)\r\n    668     \"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\r\n    669     try:\r\n--> 670       return self._next_internal()\r\n    671     except errors.OutOfRangeError:\r\n    672       raise StopIteration\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    659         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    660       except AttributeError:\r\n--> 661         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    662 \r\n    663   @property\r\n\r\n/usr/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)\r\n     97                 value = type()\r\n     98             try:\r\n---> 99                 self.gen.throw(type, value, traceback)\r\n    100             except StopIteration as exc:\r\n    101                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   1987     finally:\r\n   1988       ctx.executor = executor_old\r\n-> 1989       executor_new.wait()\r\n   1990 \r\n   1991 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1587494349.376555159\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3959,\"referenced_errors\":[{\"created\":\"@1587494349.376552078\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n```", "comments": ["Was able to reproduce the issue. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/8dbbdad52583e167dfa9d9b7ac69a7f3/38762.ipynb). Thanks!", "I'm having the exact same issue, when using a generator for training in TPUs in Colab.\r\nWhen using TensorFlow 2.2 (Stable), I get [this other issue](https://github.com/tensorflow/tensorflow/issues/34346).\r\n\r\nHowever, when trying with a nightly version, I'm getting the error from this issue:\r\n```\r\nUnavailableError: {{function_node __inference_train_function_5896}} failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1589320590.316232748\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1589320590.316230089\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n```", "I am having this issue as well. This issue should be higher priority because it makes it impossible to run Huggingface tokenization on TPUs with TensorFlow", "@oja I'm having the exact same problem so I thought I'd share what I found:\r\nhttps://github.com/huggingface/transformers/pull/1424/files#diff-5843fc9f06d46f05183ab24e6d139575R39\r\n\r\nIn this code they're doing tokenization in advance and building a tf.data.Dataset from that. Of course this won't work if your dataset won't fit in memory, and it would still be great to have py_function working with TPUs. :)", "Hi @noahtren, unfortunately my dataset is too large to do any significant work outside of the tf.data pipeline. I have opened a bug https://github.com/huggingface/transformers/issues/5066", "I have the same issue with tf.numpy_function.", "Same issue, need to be solved otherwise the usage of Tensorflow Dataset will be so limited.", "Has this issue been resolved? Facing this issue in Tensorflow 2.3.0 TPU v2-8 Cloud TPU while using tf.py_function for making tensorflow dataset", "Facing Same issue while trying to use Bert encoder by warping tf.py_function in data pipeline. I am using TensorFlow 2.2 TPU v3-8. Has this issue been solved?  ", "Same Issue", "Same problem.", "Same Issue. Please resolve it quickly as it is limiting the usage of tf.data API drastically for large datasets", "Facing similar issue ", "Similar issue trying to fit a model with a DirectoryIterator object", "@oja and any others who were planning on doing tokenization with HuggingFace tokenizers:\r\n\r\nThe Wordpiece tokenizer from TF.Text can do tokenization that is compatible with graph mode (doesn't require `py_function`.) Since HuggingFace is popular, I have a script that copies the vocab over from a pretrained HuggingFace tokenizer and uses it with a `tensorflow_text.WordpieceTokenizer`: https://gist.github.com/noahtren/6f9f6ecf2f81d0975c4f54afaeb95318", "@noahtren This looks great, thanks!", "Facing same issue when iterating over a dataset created wih tf.data.Dataset.from_generator", "same here, issue persists with generators", "facing the same problem when using tf.py_function to define a customized layer, please fix it.", "Same issue.", "Here are some suggestions:\r\n\r\n* `tf.data.Dataset.from_generator` is known to be incompatible with TPU hardware ([reference](https://github.com/tensorflow/tensorflow/issues/39099#issuecomment-749878735)). \r\n* It's better to apply `tf.py_function` related functionalities to your dataset ahead of time and serialize them as TFRecords inside a GCS Bucket which you would use anyway when using TPUs for training on large datasets. This effectively eliminates the need to use `tf.py_function` during the data loading phase for training. It might even add a bit of speed up in the overall training pipeline since now we are eliminating a non-graph operation from the pipeline. ", "same issue here too", "same issue", "same issue here ", "@rxsang Could you take over the issue?", "sayakpaul@ summarized the problem well in https://github.com/tensorflow/tensorflow/issues/38762#issuecomment-787600368, I would suggest finding alternatives of pyfunctions to run on TPUs in general.", "Yeah same issue here. Cannot use Image generator and custom generator together to train the model using TPUs. The code works fine with CPU/GPU, but doesn't work with TPUs. Please provide solution to this!", "Was able to replicate the issue with TF 2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ae26be7d9ca781dc8741914c2e472311/untitled294.ipynb) ..Thanks!", "Has anybody a solve?", "Seems to still be an issue using `py_function` with a TPU."]}, {"number": 38756, "title": "tf.name_scope has no effect when used with tf.cond and autograph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nUsing `tf.summary.scalar` in a method that is called in `tf.cond` logs the scalar without the `name_scope`. The results are different than when eager execution is used.\r\n\r\n**Describe the expected behavior**\r\n`tf.name_scope` should be used.\r\n\r\n**Standalone code to reproduce the issue**\r\n### Eager Execution\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef test_summary():\r\n    with tf.name_scope('MyScope') as scope:\r\n        mynum = tf.convert_to_tensor(43.9, name=scope)\r\n\r\n        def log_mynum():\r\n            tf.summary.scalar('mynum', data=mynum)\r\n        tf.cond(tf.math.equal(mynum, 43.9), true_fn=log_mynum,\r\n                false_fn=lambda: None, name='tb-mynum')\r\n\r\n        log_mynum()\r\n\r\nwith tf.summary.create_file_writer('./logs').as_default():\r\n    tf.summary.experimental.set_step(0)\r\n    test_summary()\r\n```\r\n![eager](https://user-images.githubusercontent.com/31281983/79886925-e2802180-83c7-11ea-8579-f7a5fddb5fa6.png)\r\n\r\n### Autograph\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\ndef test_summary():\r\n    with tf.name_scope('MyScope') as scope:\r\n        mynum = tf.convert_to_tensor(43.9, name=scope)\r\n\r\n        def log_mynum():\r\n            tf.summary.scalar('mynum', data=mynum)\r\n        tf.cond(tf.math.equal(mynum, 43.9), true_fn=log_mynum,\r\n                false_fn=lambda: None, name='tb-mynum')\r\n\r\n        log_mynum()\r\n\r\nwith tf.summary.create_file_writer('./logs').as_default():\r\n    tf.summary.experimental.set_step(0)\r\n    test_summary()\r\n```\r\n![autograph](https://user-images.githubusercontent.com/31281983/79887247-689c6800-83c8-11ea-8d3d-e4fb7465ad04.png)\r\n\r\n\r\n**Other info / logs**\r\nn/a\r\n", "comments": ["Could you try this in nightly? 8c1ead8 partially fixes this by propagating the parent `name_scope`. Although, I think there might still be a difference between graph and eager mode because graph mode adds an additional name_scope \"cond\". We should reconcile those.", "I just ran on `tf-nightly` and I got the following warning:\r\n```\r\n2020-04-23 15:07:04.707011: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: MyScope/tb-mynum/branch_executed/_7\r\n```\r\nHowever, the TensorBoard output for autograph was the same as the eager execution output. \r\n\r\nAs for the additional name_scope, I don't see it in the TensorBoard output, but on 2.1.0 I do remember seeing that a \"cond\" op is added to the graph (which makes sense).\r\n\r\n**EDIT**: the autograph and eager TensorBoard outputs actually aren't the same. With eager, I see only one scalar chart, with the scope `MyScope/mynum`. With autograph, I see two scalar charts, with the scopes `MyScope/mynum` and `MyScope/tb-mynum/mynum`.", "Thanks for trying that. I have a fix that should make the eager tensorboard consistent with the graph one. I am testing it out.", "@sumanthratna Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "> @sumanthratna Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!\r\n\r\nI just tried on tf 2.5.0 and the results have not changed since https://github.com/tensorflow/tensorflow/commit/8c1ead8d919c7536c3d8f52fbab44e1dfcd9fd79 was merged. It appears to be the same issue that @saxenasaurabh described in https://github.com/tensorflow/tensorflow/issues/38756#issuecomment-618571584. \r\n\r\n# Eager\r\n![Screen Shot 2021-07-13 at 17 17 37](https://user-images.githubusercontent.com/31281983/125526527-b58bbc6f-aad5-42a3-922b-5e9defabc7c5.png)\r\n\r\n# Autograph\r\nwhen running, I see this in stdout: \r\n```\r\n2021-07-13 17:19:49.308004: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-07-13 17:19:49.342490: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: MyScope/tb-mynum/branch_executed/_7\r\n```\r\n\r\n![Screen Shot 2021-07-13 at 17 20 59](https://user-images.githubusercontent.com/31281983/125526794-3d4d764c-974c-461c-8869-1ce4bd05d859.png)"]}, {"number": 38754, "title": "tf.name_scope with spaces does not raise ValueError in eager execution ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):  n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nIn autograph, `tf.name_scope` does not allow spaces in the string argument. In eager execution, `tf.name_scope` *does* allow spaces in the string argument. \r\n\r\n**Describe the expected behavior**\r\nThe constraints on `tf.name_scope` should be the same across eager execution and autograph. \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n# eager execution since there's no @tf.function decorator\r\ndef graphcry():\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n\r\n    with tf.name_scope('scalaragain scope'):  # doesn't work\r\n        tf.summary.scalar('scalaragain', data=myscalar)\r\n\r\n    with tf.name_scope('nospace_scope'):  # works\r\n        tf.summary.scalar('nospace', data=myscalar)\r\n\r\ngraphcry()\r\n```\r\n\r\n**Other info / logs**\r\nCC @jvishnuvardhan, also see #38661 \r\n", "comments": ["@sumanthratna Looks like this was resolved. I checked in `TF2.3` and `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5ed9cf685953a339a32f0b465f5ec0f8/untitled.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Hi @jvishnuvardhan! It looks my reproducibility script didn't indicate the difference between JIT and eager.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n# eager execution since there's no @tf.function decorator\r\ndef graphcry():\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n\r\n    with tf.name_scope('scalaragain scope'):\r\n      print('Scope1: Working')\r\n      tf.summary.scalar('scalaragain', data=myscalar)# works\r\n\r\n    with tf.name_scope('nospace_scope'):  # works\r\n      print('Scope2: Working')\r\n      tf.summary.scalar('nospace', data=myscalar)\r\n\r\ngraphcry()\r\n\r\n@tf.function\r\ndef graphcry2():\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n\r\n    with tf.name_scope('jit_scalaragain scope'):\r\n      print('Scope1 JIT: Working')\r\n      tf.summary.scalar('scalaragain', data=myscalar)# doesn't work\r\n\r\n    with tf.name_scope('jit_nospace_scope'):  # works\r\n      print('Scope2 JIT: Working')\r\n      tf.summary.scalar('nospace', data=myscalar)\r\n\r\ngraphcry2()\r\n```\r\n\r\nresults in:\r\n```\r\nScope1: Working\r\nScope2: Working\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-182149e1e839> in <module>()\r\n     28       tf.summary.scalar('nospace', data=myscalar)\r\n     29 \r\n---> 30 graphcry2()\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-2-182149e1e839>:22 graphcry2  *\r\n        with tf.name_scope('jit_scalaragain scope'):\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:6631 __enter__\r\n        scope_name = scope.__enter__()\r\n    /usr/lib/python3.6/contextlib.py:81 __enter__\r\n        return next(self.gen)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:4190 name_scope\r\n        raise ValueError(\"'%s' is not a valid scope name\" % name)\r\n\r\n    ValueError: 'jit_scalaragain scope' is not a valid scope name\r\n```\r\nnote how no errors occur with eager, but ValueError occurs with JIT. IMO the behavior should be the same for both eager and JIT.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210526, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ae03cc43eb93b9aca7986cbb0bdfbe54/38754.ipynb). Thanks!", "It is still replicating in[ 2.8 version.](https://colab.sandbox.google.com/gist/mohantym/2b5cf3c827b98f07f02b2bf620d2444f/38754.ipynb) "]}, {"number": 38746, "title": "tflite gpu delegate create and load model use v2 api is very slow compare with v1 api(10x) why ?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.2 rc2\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\ni compile tflite.a (2.2 rc2) from source and use ndk c++ api to run tflite model as follow:\r\n`\r\n#ifdef V2\r\n    TfLiteGpuDelegateOptionsV2 tOptions = TfLiteGpuDelegateOptionsV2Default();\r\n    if (m_bGPUAllowFP16)\r\n    {\r\n        tOptions.is_precision_loss_allowed = 1;\r\n    }\r\n    tOptions.inference_preference = 1;\r\n\r\n    m_pGPUDelegate = TfLiteGpuDelegateV2Create(&tOptions);\r\n#else\r\n    TfLiteGpuDelegateOptions tOptions = {.metadata = nullptr, .compile_options = {.precision_loss_allowed = 0, .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST, .dynamic_batch_enabled = 0,},};\r\n    if (m_bGPUAllowFP16)\r\n    {\r\n        tOptions.compile_options.precision_loss_allowed = 1;\r\n    }\r\n\r\n    m_pGPUDelegate = TfLiteGpuDelegateCreate(&tOptions);\r\n#endif\r\n\r\n    auto iRetCode = m_pInterp->ModifyGraphWithDelegate(m_pGPUDelegate);\r\n    if (iRetCode != kTfLiteOk)\r\n    {\r\n        return -1;\r\n    }\r\n`\r\n\r\nbut the time cost is very different, v1 load time cost is only 10% of v2 load time.  the model has Conv2DTranspose op, if use v1 api the inference time is 4x of v2 api, so why has this performance different?", "comments": ["Could you share logcat output for v2 API?", "i use time count around the init op, logcat has no other tflite output log, i test the follow set, the load time is fast but still low than v1 load time: \r\ntOptions.inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER;\r\n170     tOptions.inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;\r\n171     tOptions.inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;\r\n172     tOptions.inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO;", "Can you share your model?", "can use the official model mobilenetv2, or tensorflow object detection api trained ssd mobilenetv2 model.   my model only retrained these model, no other change", "Our benchmark tool is using v2 API and I don't see any issue with it.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/delegates/gpu_delegate_provider.cc#L113\r\nYou'd better check it to figure out if your delegate creation code is correct or not.", "this is all the create code (as you link code, maybe v1 the backend is opengl, v2 is opencl)\r\n![image](https://user-images.githubusercontent.com/6283983/81525278-bfbda900-9386-11ea-8cfd-15a8ec3facd9.png)\r\n", "The backend type might have the difference. FYI, you can explicitly select backend with this.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h#L70\r\n\r\nBTW, I don't have this issue. You mentioned that you have the issue with Pixel 2 and Samsung Galaxy. Could you share which SW version you're using for those devices?\r\n\r\nCould you confirm that if you see the same issue with the original mobilenetv2 here?\r\nhttps://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet\r\nIf you do, could you also tell me which specific mobilenetv2 you used?", "i download the mobilenet_v2_1.0_224 and load tflite model on two devices:\r\none on MTK pad **<Android 9, Kernel: 4.14.87>**: \r\n     V2 api:\r\n               load model: \"input: 224/224, type: 1, output type: 1, time cost: 1592 ms\" \r\n               model forward avg time: 60ms\r\n    V1 api : \r\n               load model: \"input: 224/224, type: 1, output type: 1, time cost: 1184 ms\"  \r\n               model forward avg time: 100ms\r\none on Samsung Galaxy Note8 **<Android 9, Kernel: 4.4.153-17975032>**: \r\n     V2 api: \r\n            load model: \"input: 224/224, type: 1, output type: 1, **time cost: 1424 ms\"**\r\n            model forward avg time: 10ms\r\n     V1 api:\r\n           load model: \"input: 224/224, type: 1, output type: 1, **time cost: 333 ms\"**\r\n           model forward avg time: 50 ms\r\ntwo device performance is different a lot, but V2 api load model time all most the same, and more slow than V1 api, but forward time is much faster than V1 api.  \r\n\r\nused tflite2.2 version to compile static lib and use jni & ndk cross compile", "@terryheo any guide for me to process this problem?", "Passing to @impjdi , any idea on this?", "OpenCL compilation takes much longer than OpenGL even if you turn off tuning (`TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER`); this is not what we can change.  If this slow init is not desirable, I would suggest to use OpenGL instead.", "Is there a way to precompile and cache the OpenCL kernels? Maybe as part of the tflite export? Or maybe the GPU delegate can cache the kernels locally on the device for subsequent runs?", "> Is there a way to precompile and cache the OpenCL kernels? Maybe as part of the tflite export? Or maybe the GPU delegate can cache the kernels locally on the device for subsequent runs?\r\n\r\n+1", "While this functionality is present (was just added about a month ago), it doesn't follow the paths of the GPU delegate, and there is a lot more involved in getting the plumbing done.  You can take a look at `tensorflow/lite/delegates/gpu/cl/serialization.h` and make your experiment with it (sorry, no official documentation or support).  Note that the generated cache binary is not universal, i.e. it differs by mobile vendor, device, OS version, and GPU driver.  So for each new model, you would have to run & generate it once on that particular user device and store it.  You also need to know when you're going to flush it with a new OS version, GPU driver, and your ML model, so you'll need quite a lot of logic around this.", "> While this functionality is present (was just added about a month ago), it doesn't follow the paths of the GPU delegate, and there is a lot more involved in getting the plumbing done. You can take a look at `tensorflow/lite/delegates/gpu/cl/serialization.h` and make your experiment with it (sorry, no official documentation or support). Note that the generated cache binary is not universal, i.e. it differs by mobile vendor, device, OS version, and GPU driver. So for each new model, you would have to run & generate it once on that particular user device and store it. You also need to know when you're going to flush it with a new OS version, GPU driver, and your ML model, so you'll need quite a lot of logic around this.\r\n\r\ni use mnn(https://github.com/alibaba/MNN) framework which support gpu cache, only the first time load will cost lot of time", "> While this functionality is present (was just added about a month ago), it doesn't follow the paths of the GPU delegate, and there is a lot more involved in getting the plumbing done. You can take a look at `tensorflow/lite/delegates/gpu/cl/serialization.h` and make your experiment with it (sorry, no official documentation or support). Note that the generated cache binary is not universal, i.e. it differs by mobile vendor, device, OS version, and GPU driver. So for each new model, you would have to run & generate it once on that particular user device and store it. You also need to know when you're going to flush it with a new OS version, GPU driver, and your ML model, so you'll need quite a lot of logic around this.\r\n\r\n@impjdi   hi impjdi,   could you give a example?  it's very useful to save init time!! \r\nmycode as follows, but i don't how to use Encode API in  serialization.h.   \r\nThanks!\r\n\r\n```\r\n    // Load model\r\n    std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(model_file);\r\n\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder builder(*model, resolver);\r\n    builder(&_interpreter);\r\n\r\n    TfLiteGpuDelegateOptionsV2 gpu_opts = TfLiteGpuDelegateOptionsV2Default();\r\n    auto delegate = CreateGPUDelegate(&gpu_opts);\r\n\r\n    _interpreter->ModifyGraphWithDelegate(delegate.get());\r\n\r\n```\r\n\r\n", "I tried to initialize multiple tflite models with GPU delegate v2 api at the same time through multi-threading, but found that it takes about the same time for multi-threading to initialize multiple models at the same time and single-threading to initialize multiple models sequentially\uff0cCan someone explain why~", "@menchunlei Sorry for the late response, Gray.  I missed your message.  I don't think we have a good public documentation or code snippet that can be shared as of right now, but serialization will be supported across all delegates in near future.\r\n\r\n@linshizuowei During the OpenCL initialization, we do tuning on the GPU.  So if your GPU is busy, multithreading is not going to help much.", "@impjdi : I have been testing with delegate serialization in tflite 2.7. It seems to be working well, but I have a question about your comment above \"Note that the generated cache binary is not universal, i.e. it differs by mobile vendor, device, OS version, and GPU driver. So for each new model, you would have to run & generate it once on that particular user device and store it. You also need to know when you're going to flush it with a new OS version, GPU driver, and your ML model\". With 2.7's serialization, does this still apply? If so, how does the cache get cleaned for OS version or GPU driver version? We are very interested in shipping this feature, but need to understand how to manage the cache. I don't see any real guidance in the 2.7 docs. Thanks!", "@lauriebyrum If you are using Android's getCacheDir as the serialization directory, then it will get cleaned automatically for OS version update. @impjdi does the GPU delegate know if the GPU driver version has been updated? In the general case, user may have to append the GPU driver version to serialization_dir to avoid incorrect cache hits.", "If we need to append the driver version, sample code would be very much appreciated...", "(how bad is it if the cache doesn't get cleaned across these version updates? by any chance, does tflite ignore incompatible caches or are we going to get failures?)", "> by any chance, does tflite ignore incompatible caches or are we going to get failures?\r\n> how bad is it if the cache doesn't get cleaned across these version updates?\r\n\r\nThe TFLite layer will pass on the cached representation if it finds one; @impjdi will the GPU delegate crash (or just be suboptimal) if the GPU driver version has changed underneath?\r\n\r\n\r\n", "Chances are high that it'll crash with driver version update.  I unfortunately don't have a code snippet that checks the driver version.  So far, we used this caching only in a setting where the app is updated with the OS update only, and there are no individual graphics driver update from Play Store, and we never ran into this issue ourselves =/", "Thanks, @impjdi . To make sure I understand your comment: do third party apps need to somehow watch out for gpu driver updates or not? If they only occur during OS updates, we don't, but if there's some way they can happen outside of that, we do and I don't understand how to do that. Thanks!\r\n", "A colleague did some research and found https://www.gizmochina.com/2020/03/25/gpu-driver-update-tool-play-store/ and https://www.androidauthority.com/gpu-driver-updates-phones-1096423/ . So it sounds like we app developers really do need to worry about gpu driver updates separate from OS updates. Does Google have guidance on handling this? We are hoping to ship with this in January, so we don't have much time.", "Hey @lauriebyrum , sorry for the delay. I confirmed with our internal GPU team, and looks like the serialization data will automatically get re-initialized in case of a GPU driver update. This happens within the internal code that does serialization in `tensorflow/lite/delegates/gpu/cl/serialization.cc`, where the `GetPlatformVersion()` check fails if there is an update. Additionally, if you use CacheDir() from Android, your serialization folder will get reset for app or OS updates. Does that sound good?", "That's fantastic news! Thanks @srjoglekar246 ", "@srjoglekar246 : one more question. We are storing our serialized models in a subdir under getCacheDir so we can just throw away the subdir if things seem to be unstable for a user. I happened to have a pending android security update for one of my phones, so I went ahead and updated and the serialized bits did not get deleted during the update. That said, the serialization still seemed to work, so maybe the update was smart enough to realize that deleting the bits wasn't needed? Does this match your expectation? I was expecting that android updates would just toss out the caches generally, but it seems I'm wrong? (Maybe a \"security update\" isn't an \"OS update\" so what I did didn't trigger the cache cleanup?)", "@lauriebyrum My bad, can you try using [getCodeCacheDir](https://developer.android.com/reference/android/content/Context#getCodeCacheDir())?", "tflite gpu delegate supported Conv2DTranspose????"]}, {"number": 38740, "title": "Generate  C++ code from TensorFlow Lite metadata", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  Master branch.\r\n- Are you willing to contribute it (Yes/No):  Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI was so excited to hear that we can generate Android Java code from TensorFlow Lite meta-data with some codegen tools in TFLite. However, in our cases, most of the AI implementations are preferred to be wrapped into C++ interfaces, which reduce lots of work to maintain different  AI codes on different platforms. So,  I think C++ wrappers code generator of TensorFlow Lite metadata would be more favorable on business sight, which always has apps on lots of platform e.g. Android, IOS, other OS on devices and so on, and without develop AI implementations redundantly.  \r\n\r\n**Will this change the current api? How?**\r\nMaybe not, just support some new tools to generate a class I guessed.\r\n\r\n**Who will benefit with this feature?**\r\nI think lots of CPU manufacturers, solutions developers, smart devices manufacturers would be happy to hear good news from C++ code generator, they always need to develop some AI SDKs for their customers and they hardly know which platforms will run their SDKs, so C/C++ interfaces is the best choices for them.\r\n\r\n**Any Other info.**\r\n", "comments": ["@lu-wang-g ", "Thanks for the feedback, Sun! I'm glad that you like the idea of metadata + codegen. You're right that C++ is super attractive to many users, we find this trend in our product analysis as well, especially those who are looking for cross-platform solutions. C++ is definitely on our top priority list, and we are actively working on it, starting from building some C++ utils and blocks. Please stay tuned for more updates about TFLite Support.", "Over to @lu-wang-g for follow-up."]}, {"number": 38724, "title": "mixed precision for non-Keras TensorFlow scripts", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): No (it would take too long for me to learn the low-level functionality of mixed-precision)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, https://www.tensorflow.org/guide/keras/mixed_precision only works with Keras. \r\n\r\n**Will this change the current API? How?**\r\nThe mixed_precision module will either be relocated so that it is not exclusively for Keras, or a new submodule will be created.\r\n\r\n**Who will benefit with this feature?**\r\nUsers who want to use mixed-precision but aren't using Keras.\r\n\r\n**Any Other info.**\r\nI'm not sure if this is even possible.", "comments": ["I think you should be able to use: https://www.tensorflow.org/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite", "`tf.train.experimental.enable_mixed_precision_graph_rewrite` returns an instance of `tf.keras.optimizers.Optimizer`, so I'm not sure if this is what I want. If it *does* work without Keras, this should definitely be added to the documentation.", "We have considered having a non-Keras mixed precision API but it probably won't happen. Having an easy-to-use non-Keras API would require automatically inserting casts before each op, which is fairly complicated and would negate any user casts. Still, the final decision has not been made and there will be a mixed precision RFC. Once the RFC is available, you and others can voice you concerns with it, including the fact it is Keras only.\r\n\r\nFor `enable_mixed_precision_graph_rewrite`, you are required to use an optimizer, and in TF 2, only Keras optimizers are available. Other than Keras optimizers, you do not need Keras. Technically, you can also pass a dummy optimizer and not use it, as long as you are careful to implement loss scaling. But we plan on deprecating `enable_mixed_precision_graph_rewrite` soon, so it's not a good option in general.\r\n\r\nIt is possible to use mixed precision outside Keras by manually inserting casts in the appropriate places. Variables must be created in float32 then casted to float16, and the model inputs must be casted to float16 as well. Loss scaling must also be done.\r\n\r\nI'll keep this issue open until the RFC has been up for review for a few weeks and approved.", "Thanks for your detailed reply, @reedwm! Can you explain what happens if I do the following in a script that doesn't use Keras? \r\n```python\r\ntf.config.optimizer.set_experimental_options({\r\n    'auto_mixed_precision': True\r\n})\r\n```\r\nWill it simply not do anything?", "That `tf.config.optimizer.set_experimental_options` code is equivalent to calling `enable_mixed_precision_graph_rewrite`, except it doesn't wrap an optimizer with a [LossScaleOptimizer](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer). It will affect non-Keras code since it rewrites the graph to use mixed precision and doesn't care about whether Keras is used or not.\r\n\r\nHowever, that code is dangerous since it doesn't do any loss scaling. You will either have to do loss scaling by hand, or, if you use a Keras optimizer, wrap the optimizer with a LossScaleOptimizer. In the latter case, you should use `enable_mixed_precision_graph_rewrite` instead of `tf.config.optimizer.set_experimental_options` to automatically have the optimizer wrapped with a LossScaleOptimizer. "]}, {"number": 42780, "title": "[ko][zh-cn] How to get chinese/korean fonts to work in matplotlib + Colab?", "body": "The default Matplotlib setup in Colab doesn't include Chinese or Korean fonts, so these characters don't render.\r\n\r\nI believe this is one of the reasons we have not been translating figure text.\r\n\r\nI can get the browser to render this text by outputting svg-text:\r\n\r\n```\r\nfrom IPython.display import set_matplotlib_formats\r\nset_matplotlib_formats('svg')\r\nmatplotlib.rcParams['svg.fonttype'] = 'none'\r\n```\r\n\r\nBut that messes up a lot of the formatting.\r\n\r\nSome searching shows that it might just be a matter of installing the right fonts and adding them to the `matplotlib.rc` configuration, but I haven't found a end-to-end setup that works yet. \r\n\r\nDoes anyone have experience setting this up?\r\n\r\n", "comments": ["@rickiepark \r\n\r\n Haesun are you familiar with the required setup for Korean?", "Yes, here is my font setup for Korean.\r\nhttps://colab.research.google.com/drive/16LVK_RhuxdQSIFYFwgo2dKVQwXtOctAy\r\nBut it's not good experience because it needs to restart runtime. :(", "I share the notebook, please try again. :)", "@MarkDaoust Do you know what the status of this is?\r\nI see TF Hub referenced this issue a few month ago", "Status: still broken.\r\n \r\nThe reference from TFHub is that they have one multi-lingual tutorial that breaks if they push it to the site because the fonts are not available.\r\n\r\nIf this is high enough priority, we could upstream Haesun's font setup (+fonts for Chinese) into Colab and the site generator. Worth it? or should we just close this?"]}, {"number": 38693, "title": "Does `tf.constant()` waste memory? What is the alternative?", "body": "This is about [this tutorial](https://www.tensorflow.org/guide/data) on input pipelines, and in particular the following note under \"Reading input data\" > \"Consuming NumPy arrays\":\r\n\r\n> Note: The above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations. This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.\r\n\r\nCould we have a slightly more detailed justification for this note, namely as to why `tf.data.Dataset.from_tensor_slices()` is suboptimal in this case. In particular:\r\n\r\n1. In which way are the contents of the array copied multiple times?\r\n2. What is the alternative to that code if we don't want to run into the 2GB limit? What is the best practice in general?", "comments": ["This is a question better suited for StackOverflow, since it is not about an implementation bug/feature request.", "This issue wasn't raised as a bug/feature request but rather as pertaining to the documentation, hence the `type:docs` label. It refers to a tutorial published by TensorFlow which fails to explain why `tf.constant()` wastes memory in the particular context showcased therein.", "Apologies. Reopening", "Hi! Any news on this?"]}, {"number": 38675, "title": "[2.2] XLA requires 2x GPU memory with sparse_categorical_crossentropy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0, 2.2.0rc3 and tf-nightly\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6.5.32\r\n- GPU model and memory: 4 x NVIDIA V100 on GCP\r\n\r\n**Describe the current behavior**\r\n\r\nI am currently trying to upgrade from TensorFlow 2.0.0 to 2.2.0rc3 and noticed a regression related to model training using XLA in a multi-GPU environment (GCP VM with 4 NVIDIA V100s).\r\n\r\nThe training code linked below runs comfortably with a huge batch size of 3072 on 4 GPUs using the normal TensorFlow runtime. However when enabling XLA with `TF_XLA_FLAGS=--tf_xla_auto_jit=2` the same code runs out of GPU memory on the first batch of data. With XLA I can only use a maximum batch size of 1536 (50%) to prevent the code from running out of memory which doesn't seem right.\r\n\r\n**In which cases are the memory requirements of XLA and the default runtime similar?**\r\nTo narrow down the possible causes for this I found a few cases where the maximum batch size for XLA and the normal runtime are the same:\r\n\r\n1. TensorFlow 2.0.0 doesn't seem to show this issue.\r\n\r\n2. Removing `.prefetch(1)` from the datapipline fixes the issue.\r\n\r\n3. Changing the training to one-hot encoded labels seems to fix the increase XLA memory requirements as well. To test this I changed the preprocessing and loss to:\r\n   ```python\r\n   def preprocessing(data):\r\n    return (\r\n        tf.cast(_decode_and_center_crop(data[\"image\"]), tf.float32),\r\n        tf.cast(tf.one_hot(data[\"label\"], 1000), tf.float32),\r\n    )\r\n   ```\r\n   and\r\n   ```python\r\n       model.compile(\r\n        optimizer=\"adam\",\r\n        loss=\"categorical_crossentropy\",\r\n        metrics=[\"accuracy\", \"top_k_categorical_accuracy\"],\r\n    )\r\n   ```\r\n\r\nThe above conditions suggest that the prefetched `int32` labels and sparse categorical cross entropy might cause the regression with XLA, though I might miss something here. Any help would be very appreciated.\r\n\r\n**Describe the expected behavior**\r\n\r\nGPU memory requirements (messured here by maximum usable batch size) should be similar between XLA and the default runtime.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport larq_zoo as lqz  # !pip install larq_zoo==1.0b4\r\n\r\nbatch_size = 3072\r\n\r\n\r\ndef _decode_and_center_crop(image_bytes):\r\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\r\n    shape = tf.image.extract_jpeg_shape(image_bytes)\r\n    image_height = shape[0]\r\n    image_width = shape[1]\r\n    image_size = 224\r\n\r\n    padded_center_crop_size = tf.cast(\r\n        (\r\n            (image_size / (image_size + 32))\r\n            * tf.cast(tf.minimum(image_height, image_width), tf.float32)\r\n        ),\r\n        tf.int32,\r\n    )\r\n\r\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\r\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\r\n    crop_window = tf.stack(\r\n        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]\r\n    )\r\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\r\n    return tf.image.resize(image, [image_size, image_size], method=\"bicubic\")\r\n\r\n\r\ndef preprocessing(data):\r\n    return (\r\n        tf.cast(_decode_and_center_crop(data[\"image\"]), tf.float32),\r\n        data[\"label\"],\r\n    )\r\n\r\n\r\ndataset = tfds.load(\r\n    \"imagenet2012:5.0.0\",\r\n    decoders={\"image\": tfds.decode.SkipDecoding()},\r\n    split=\"train\",\r\n    data_dir=\"gs://my-data-bucket\",\r\n)\r\n\r\ndataset = (\r\n    dataset.map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nwith tf.distribute.MirroredStrategy().scope():\r\n    model = lqz.sota.QuickNet(weights=None)\r\n\r\n    model.compile(\r\n        optimizer=\"adam\",\r\n        loss=\"sparse_categorical_crossentropy\",\r\n        metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\r\n    )\r\n\r\nmodel.fit(dataset, epochs=5)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI attached the [XLA dumps](https://www.tensorflow.org/xla#reproducible_bug_reports) below:\r\n[xla_dumps.tar.gz](https://github.com/tensorflow/tensorflow/files/4497906/xla_dumps.tar.gz)", "comments": ["@lgeiger Thanks for looking into this!\r\nXLA in general makes the memory fragmentation worse, but you are right it should not increase the memory consumption by that much.\r\n\r\nWe'll track this bug, but in general we have found that it is very difficult to deal with such problems when using autoclustering: so we try to use explicit compilation scopes with `tf.function(experimental_compile=True)` [instead](https://www.tensorflow.org/xla#explicit_compilation_with_tffunction). If you could change the test case to use that, it would be very helpful (but I understand if it's not possible, e.g. here the code inside the `lqz` model would probably need to be annotated). Investing time into annotation could also make the possible performance impact more apparent though (by identifying a chunk in the profiler which is too slow and should be optimized, and adding an explicit annotation around that).", "@cheshire Thanks for taking a look, appreciate you help!\r\nUnfortunately explicitely setting compilation scopes is currently not an option for us, since I am mostly dealing with research code where flexibility and readability is currently more important that using XLA to get the best possible performance.\r\n\r\nI tried to narrow the problem a bit down though, and unfortunately after more testing is seems that even when removing `prefetch(1)` or using one hot encoded labels I see out of memory errors. So identical code might or might not run out of memory for seemingly arbitrarily reason. Does XLA autoclustering generate a deterministic graph, or can results differ from compilation to compilation?\r\n\r\nI'll try to investigate further, but it's tricky to narrow it down.", "For now it looks like the only reliable way for me to get this working is to either disable XLA autoclustering or to not use distributed training."]}, {"number": 38673, "title": "Unable To Find Relevant Documentation For Quantization from python/tf/dtype/DType ", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/dtypes/DType\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe dtypes, such as, `tf.qint8`, `tf.quint8`, `tf.qint16`, `tf.quint16`, `tf.qint32`, are a bit unclear. What is `Quantized` suppose to mean? Where should a reader go to learn more about it? Clicking on the hyperlink of any of the dtypes of the above leads to [tf](https://www.tensorflow.org/api_docs/python/tf) which is just text. It does not give info about variable itself ( what is `quantization`? How and why is it an `int` ? )\r\n\r\n### Clear description\r\n\r\nNo clear description about what `quantization` really means. \r\nGoogling for `quantized tensorflow` leads us to,\r\n1): [Post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)\r\n2): [TensorFlow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec)\r\n3): [Converting Quantized Models](https://www.tensorflow.org/lite/convert/quantization)\r\n4): [tf.quantization.quantize](https://www.tensorflow.org/api_docs/python/tf/quantization/quantize)\r\n5): [Post-training dynamic range quantization](https://www.tensorflow.org/lite/performance/post_training_quant)\r\n\r\nNone of which give a quick definition into what `quantization` is and what it is in Tensorflow.\r\n\r\n### Correct links\r\n\r\nThe link is correct, it is this https://www.tensorflow.org/api_docs/python/tf/dtypes/DType#tf.qint32 .\r\n\r\n### Parameters defined\r\n\r\nNot related to code.\r\n\r\n### Returns defined\r\n\r\nNot based on code.\r\n\r\n### Raises listed and defined\r\n\r\nNot related to code.\r\n\r\n### Usage example\r\n\r\nNot related to code.\r\n\r\n### Request visuals, if applicable\r\n\r\nNot really.\r\n\r\n### Submit a pull request?\r\n\r\nI do not plan to. Don't really have the time. \r\n\r\nThis is similar to issue #15 , closed, at [here](https://github.com/tensorflow/tensorflow/issues/15) and #494, [here](https://github.com/tensorflow/tensorflow/issues/494).\r\n\r\nThank you! And have a nice day. \r\n", "comments": ["Please have a look at [model_optimization](https://www.tensorflow.org/lite/performance/model_optimization)\r\n\r\nQuantization refers to converting the floating-point weights into integer weights. \r\nUsually while training our weights are in float64 or float32. \r\nQuantization reduces them to integer data type in either `tf.qint8, tf.quint8, tf.qint16, tf.quint16, tf.qint32` data type.\r\nThis saves up space, latency and other factors as appropriately discussed in the above link.\r\n\r\n", "Hi @oke-aditya !\r\nThanks! That was useful.\r\nThe [model_optimization](https://www.tensorflow.org/lite/performance/model_optimization) page contains a useful heading for `Quantization`. Can we add a link to the before mentioned page on the [dtype page](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) ? It could help to quickly look up what `Quantization` actually means and for what purpose it exists.", "> \r\n> \r\n> Hi @oke-aditya !\r\n> Thanks! That was useful.\r\n> The [model_optimization](https://www.tensorflow.org/lite/performance/model_optimization) page contains a useful heading for `Quantization`. Can we add a link to the before mentioned page on the [dtype page](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) ? It could help to quickly look up what `Quantization` actually means and for what purpose it exists.\r\n\r\nI am unsure of this also I am unsure where to update in source code as well. ", "@ymodak  can you help us out here? Thank you."]}, {"number": 38670, "title": "Make it simpler to write custom metrics! ", "body": "Why is it so complicated to write a simple custom metric? I do not want to deal with tensors, just numpy arrays. Please make it simpler and ability to use only numpy arrays. This request is for Keras.\r\n\r\n\r\n<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@nectario If you have a simple metric that is Mean based then you can write a function that accepts labels, predictions and returns the metric value as shown below. \r\n\r\n```\r\ndef mae(y_true, y_pred):\r\n    return tf.keras.backend.mean(tf.math.abs(y_pred - y_true), axis=-1)\r\n```\r\n\r\nFor anything else you can subclass the metric class, please see `BinaryTruePositives` example here:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric?version=nightly", "I need to write a metric that outputs the average number of positive results.", "This is what I have so far to calculate the direction accuracy:\r\n\r\n```\r\nfrom tensorflow.keras.metrics import Metric\r\nfrom tensorflow.python.ops import init_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import array_ops\r\n\r\nimport tensorflow\r\n\r\nclass DirectionAccuracy(Metric):\r\n\r\n    def __init__(self, name=\"direction_accuracy\", dtype=None, **kwargs):\r\n        super(Metric, self).__init__(name=name, dtype=dtype, **kwargs)\r\n        self.total_count = self.add_weight(\"total_count\", initializer=init_ops.zeros_initializer)\r\n        self.match_count = self.add_weight(\"match_count\", initializer=init_ops.zeros_initializer)\r\n        self.direction_matches = self.add_weight(\"direction_matches\", initializer=init_ops.zeros_initializer)\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        self.direction_matches = math_ops.multiply(y_true, y_pred)\r\n        count_result = math_ops.count_nonzero(tensorflow.greater_equal(self.direction_matches, 0.), dtype=\"float32\")\r\n\r\n        self.match_count.assign_add(count_result)\r\n        self.total_count.assign_add(len(y_true))\r\n        \r\n        \r\n    def result(self):\r\n      return math_ops.div_no_nan(self.match_count, self.total_count)\r\n```", "Even when I am observing the source code for other custom metrics it's impossible to understand what is going on.\r\n", "Hello @nectario, sorry this has been difficult for you. For avg number of positive results you should just have to write a simple function and pass that to the compile API. Like the categorical accuracy function here: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b220dae89d64be05276018e171fcfefa95c1bc5/tensorflow/python/keras/metrics.py#L3250\r\n\r\nYou do not have to subclass metric. Subclassing is required only if it not mean based.", "Agree with topic Author, I find the entry level barrier for basic extensions to be much higher than other alternatives. Maybe because I am a DS and not a Software Dev.\r\n\r\nI would like to write F1Score by building on Precision and Recall classes that are already there with support of Sparse y_true. Creating their new instances with a set of thresholds via a functional approach every time F1Score is called seems inefficient."]}, {"number": 38658, "title": "clip_by_value handles python float and numpy float differently", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04 & macos10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ubuntu 16.04: v2.1.0-rc2-17-ge5bf8de & macos: v2.1.0-rc2-17-ge5bf8de410\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`clip_by_value(t, clip_value_min, clip_value_max)` treats python float and numpy float32/float64 differently for parameters `clip_value_min` and `clip_value_max`.\r\n\r\nIt throws exception when `t` is `int`, `clip_value_min` and `clip_value_max` are `float`, but it **doesn't** throw exception when `clip_value_min` and `clip_value_max` are numpy float. See the example below.\r\n\r\n**Describe the expected behavior**\r\nI expect the behavior should be consistent whether the passed in `clip_value_min` and `clip_value_max` are python `float` or numpy `float`. \r\n\r\nIt shouldn't silently cast numpy `float` to `int` and report unexpected result.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# ok case\r\ntf.clip_by_value([1,2,3], clip_value_min=0, clip_value_max=1) \r\n\r\n# exception when using python float\r\ntf.clip_by_value([1,2,3], clip_value_min=0., clip_value_max=1.)\r\n\r\n# no exception when using numpy float16/32/64\r\ntf.clip_by_value([1,2,3], clip_value_min=np.float32(0.), clip_value_max=np.float32(1.))\r\n\r\n# no exception; same output as above;\r\n# 0.5 is silenlty casted to 0, and 1.2 is silently casted to 1\r\ntf.clip_by_value([1,2,3], clip_value_min=np.float32(0.5), clip_value_max=np.float32(1.2))\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/38f5f93247775bd0ad7e1892a45c130c/38658-2-1.ipynb), [TFv2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/636509f9af81761c8a974c23fa36a906/38658-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/a4999bb2386d34dffed2909e6309fd63/38658-tf-nightly.ipynb). Please find the attached gist. Thanks!", "You have to cast your input `t` to float explicitly if using `clip_value` as float.\r\nhttps://github.com/tensorflow/tensorflow/blob/6728f85d82d73c34ce9cb7cf03e311c9965f13f6/tensorflow/python/ops/clip_ops.py#L78\r\n```python\r\n# tf.cast the input to float first\r\ntf.clip_by_value(tf.cast([1,2,3],dtype= 'float32'), clip_value_min=0., clip_value_max=1.)\r\n```", "@ymodak Yes, I understand that usage.\r\n\r\nHowever, when I don't cast `t`,  it doesn't explain the inconsistent handling of numpy float and python float, where `clip_by_value` doesn't throw an exception when I use `clip_value_min=np.float(0.)`  and `clip_value_max=np.float(1.)`, but throws `TypeError` if they are python float.\r\n\r\nAlso, it's obvious there's silent casting in the fourth example code I provided. I believe that's inadequate handling of the input without warning the user.", "Was able to reproduce your issue in Tf Nightly 2.8.0-dev20211207, please find the gist [here](https://colab.sandbox.google.com/gist/sanatmpa1/7f9ffb31565e7d2f67914f82ac653dcf/38658.ipynb). Thanks!", "Was able to reproduce the issue in tf v2.7.Please find the gist [here](https://colab.sandbox.google.com/gist/tilakrayal/0971cc14430b1f9ca5acf52833fe29ba/untitled155.ipynb)."]}, {"number": 38643, "title": "Resize Second Derivatives", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0.dev20200416\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf.image.resize` can calculate derivatives but there exists no gradient for the gradient of it.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo (the API will not change, but the higher-order derivative will be calculable)\r\n\r\n**Who will benefit with this feature?**\r\n\r\nI ran into this issue when implemented ProGAN with a regularized JS-GAN, as it requires the second derivative of the discriminator, which involves a resize operation. If a second-order derivative is added, this will be possible.\r\n\r\n**Any Other info.**\r\n\r\nN/A", "comments": ["You can get around this by using `tf.nn.avg_pool2d`, which has a second derivative, though it means you can't use other resize methods, and only allows downsampling.", "For upsampling by 2x, this code works (for `NCHW`):\r\n\r\n```python\r\n_upsample_matrix: tf.Tensor = tf.ones([2, 2, 1, 1])\r\ndef upsample(x: tf.Tensor) -> tf.Tensor:\r\n    batch, channels, height, width = x.shape\r\n    x = tf.reshape(x, [-1, 1, height, width])\r\n    x = tf.nn.conv2d_transpose(\r\n        x, _upsample_matrix, (height * 2, width * 2), 2, data_format='NCHW'\r\n    )\r\n    x = tf.reshape(x, [-1, channels, height * 2, width * 2])\r\n    return x\r\n```\r\n\r\nImplementing this using `NHWC` needs `tf.transpose`, which is less efficient:\r\n\r\n```python\r\n_upsample_matrix: tf.Tensor = tf.ones([2, 2, 1, 1])\r\ndef upsample(x: tf.Tensor) -> tf.Tensor:\r\n    x = tf.transpose(x, perm=[0, 3, 1, 2])\r\n    batch, channels, height, width = x.shape\r\n    x = tf.reshape(x, [-1, 1, height, width])\r\n    x = tf.nn.conv2d_transpose(\r\n        x, _upsample_matrix, (height * 2, width * 2), 2, data_format='NCHW'\r\n    )\r\n    x = tf.reshape(x, [-1, channels, height * 2, width * 2])\r\n    x = tf.transpose(x, perm=[0, 2, 3, 1])\r\n    return x\r\n```\r\n\r\nHowever both these workarounds are still limited to nearest neighbour(ish) upsampling."]}, {"number": 38642, "title": "Roadmap for publishing TFLite runtime on PyPi?", "body": "First off, congrats to the team for all the great progress made towards publishing tflite runtime 2.x binaries for a matrix of architectures and python versions.\r\n\r\nI know the topic of publishing tflite-runtime on pypi has been discussed, but I cannot find an issue or thread to follow. Could you please point me to it if there is one?\r\n\r\nThank you!", "comments": ["Just to be clear, this would be to avoid pulling in the full TF module (which includes the TFLite runtime)?", "> Just to be clear, this would be to avoid pulling in the full TF module (which includes the TFLite runtime)?\r\n\r\nYes, exactly. Operating Edge / IoT applications focused on inference could benefit from a light ~10MB TFLite binary as opposed to a full 1GB+ TF download.", "@ivelin Is this what you're looking for?\r\nhttps://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter", "@terryheo I am aware of the page you shared and admit that it gets us pretty close to the end goal. Lots of great progress in the past year. We no longer need to build tflite from source.\r\n\r\nI am hoping to eventually see something a bit more streamlined like the full tensorflow distribution: https://pypi.org/project/tensorflow/\r\n", "I am looking for a version of pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl for the Raspberry Zero containing the ARM 6 processor. I can not find an example to build and install just the runtime. Any help appreciate! \r\n\r\n", "I think the main advantage of releasing tflite-runtime on PyPI is that it would allow third-party Python libraries to declare it as a requirement. That isn't currently possible, as there's no way of declaring a requirement on a set of URLs.  So those libraries have to require the full tensorflow package instead, which, on Linux, increases the size from 2 MB to 420 MB.", "Also why is the interpreter on https://www.tensorflow.org/lite/guide/python still Tensorflow 2.1. \r\n2.2 has been out for months now, Python 3.8 even longer and there is still no support. ", "@gilescoope: https://www.tensorflow.org/lite/guide/python now has Python 3.8 packages for Linux, but not for Windows or Mac. The TensorFlow version is still 2.1.0.", "Some TensorFlow 2.3.0 models are not compatible with the TFLite binary provided by: https://www.tensorflow.org/lite/guide/python\r\nThis is because new operations from 2.3.0 are not available in the 2.1.0 version.\r\nCan we get an updated TFLite interpreter in the short term?", "+1\r\n\r\nBuilding from source is a huge inconvenience", "https://www.tensorflow.org/lite/guide/python has now been updated to version 2.5.0. Which is strange, because the main TensorFlow project hasn't even released version 2.4.0 yet.", "Awesome!\r\n\r\nIs there an update on a plan to publish the wheels on pypi so that \r\n`pip3 install tflite` \r\njust works and there is no need to write conditional logic in install scripts that map OS and architecture to the correct tflite wheel URL?", "@mhsmith we update the version on master as soon as the branch is cut.", "i can't find the whl version from armv6l https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp37-cp37m-linux_armv6l.whl ..??? \r\n", "FYI, this effort is under experimental.\r\nAs the result, tflite_runtime 2.7 is published at Pypi.\r\nhttps://pypi.org/project/tflite-runtime\r\nSo you can use `pip install tflite-runtime` on some platform.\r\n\r\nWe can't host armv6l on Pypi since it only allows manylinux compatible wheels and `armv6l` isn't compatible with  manylinux2014. https://www.python.org/dev/peps/pep-0599/#the-manylinux2014-policy\r\nBut you can download it separately.\r\n\r\n- https://storage.googleapis.com/tensorflow-release-public/prod/tensorflow/lite/release/rpi/release/5/20211118-210013/tflite_runtime-2.7.0/tflite_runtime-2.7.0-cp37-cp37m-linux_armv6l.whl\r\n- https://storage.googleapis.com/tensorflow-release-public/prod/tensorflow/lite/release/rpi/release/5/20211118-210013/tflite_runtime-2.7.0/tflite_runtime-2.7.0-cp38-cp38-linux_armv6l.whl\r\n- https://storage.googleapis.com/tensorflow-release-public/prod/tensorflow/lite/release/rpi/release/5/20211118-210013/tflite_runtime-2.7.0/tflite_runtime-2.7.0-cp39-cp39-linux_armv6l.whl", "> FYI, this effort is under experimental. As the result, tflite_runtime 2.7 is published at Pypi. https://pypi.org/project/tflite-runtime So you can use `pip install tflite-runtime` on some platform.\r\n> \r\n> We can't host armv6l on Pypi since it only allows manylinux compatible wheels and `armv6l` isn't compatible with manylinux2014. https://www.python.org/dev/peps/pep-0599/#the-manylinux2014-policy But you can download it separately.\r\n> \r\n> * https://storage.googleapis.com/tensorflow-release-public/prod/tensorflow/lite/release/rpi/release/5/20211118-210013/tflite_runtime-2.7.0/tflite_runtime-2.7.0-cp37-cp37m-linux_armv6l.whl\r\n> * https://storage.googleapis.com/tensorflow-release-public/prod/tensorflow/lite/release/rpi/release/5/20211118-210013/tflite_runtime-2.7.0/tflite_runtime-2.7.0-cp38-cp38-linux_armv6l.whl\r\n> * https://storage.googleapis.com/tensorflow-release-public/prod/tensorflow/lite/release/rpi/release/5/20211118-210013/tflite_runtime-2.7.0/tflite_runtime-2.7.0-cp39-cp39-linux_armv6l.whl\r\n\r\nVery nice. Thank you for the update. Will give it a try.\r\n\r\n", "great to hear that tflite-runtime is on pypi! Is there a reason that builds are only provided for linux and not mac/windows? I'm still hoping for a simple cross-platform install process for tflite-runtime where you don't have to do platform detection to install requirements"]}, {"number": 38572, "title": "[TFLITE] Modelmaker and code generator for boundingRect/Cuboid + keypoints", "body": "**System information**\r\n- TensorFlow version (you are using):\r\nTensorflow-nightly\r\n- Are you willing to contribute it (Yes/No):\r\nNo\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn Tensorflow Hub at https://github.com/tensorflow/hub/issues/424 I've proposed to introduce one of the many single stage anchor-free models that adapt quite easily to keypoints tasks (face detection+ladmarks, people detection + pose, 3d cuboid + object keypoints etc.). \r\nIt could be very useful if the new ModelMaker and the code generator could support generic object occupancy (2d/3d) + arbitrary target keypoints.\r\n\r\n**Will this change the current api? How?**\r\nYes.\r\n**Who will benefit with this feature?**\r\nMany end2end task like face detection with landmakrs, people detection + pose, 3d object detection + specific object keypoints, hand detection + hand landmarks etc..)\r\n**Any Other info.**\r\n", "comments": []}, {"number": 38539, "title": "tf.function decorated functions fail in graph mode if any of the branches of conditionals would be invalid at runtime", "body": "**System information** \r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1.0 \r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nAs far as I followed the development of TF's @tf.function-decoration/autotracing, the aim is to mostly write ideomatic Python and have TF take care of building a corresponding TF op graph, with this feature being a highlight of TF2.\r\n\r\nThe simple function below, where one branch can only be properly executed when the conditional is met, albeit ideomatic Python, fails in graph mode, albeit it works in eager mode.\r\n\r\n(I am aware that I can work around the situation; but I guess it not working as-is is a bug?)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe function working identically in both eager or graph mode.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\ndef test_function(tensor):\r\n    if tf.size(tensor) == 2:\r\n        return tensor[1]\r\n    else:\r\n        return tensor[0]\r\n\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\n\r\nprint(test_function(tf.constant([1])))\r\nprint(test_function(tf.constant([1, 2])))\r\nprint(test_function(tf.constant([1, 2, 3])))\r\n\r\ntf.config.experimental_run_functions_eagerly(False)\r\n\r\nprint(test_function(tf.constant([1])))\r\nprint(test_function(tf.constant([1, 2])))\r\nprint(test_function(tf.constant([1, 2, 3])))\r\n```\r\n```\r\n> TF_CPP_MIN_LOG_LEVEL=2 python test.py\r\ntf.Tensor(1, shape=(), dtype=int32)\r\ntf.Tensor(2, shape=(), dtype=int32)\r\ntf.Tensor(1, shape=(), dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 20, in <module>\r\n    print(test_function(tf.constant([1])))\r\n  File \"\u2026/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"\u2026/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"\u2026/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"\u2026/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"\u2026/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"\u2026/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"\u2026/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"\u2026/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"\u2026/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    test.py:7 test_function  *\r\n        return tensor[1]\r\n    \u2026/tensorflow_core/python/ops/array_ops.py:898 _slice_helper\r\n        name=name)\r\n    \u2026/tensorflow_core/python/ops/array_ops.py:1064 strided_slice\r\n        shrink_axis_mask=shrink_axis_mask)\r\n    \u2026/tensorflow_core/python/ops/gen_array_ops.py:9535 strided_slice\r\n        shrink_axis_mask=shrink_axis_mask, name=name)\r\n    \u2026/tensorflow_core/python/framework/op_def_library.py:742 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    \u2026/tensorflow_core/python/framework/func_graph.py:595 _create_op_internal\r\n        compute_device)\r\n    \u2026/tensorflow_core/python/framework/ops.py:3322 _create_op_internal\r\n        op_def=op_def)\r\n    \u2026/tensorflow_core/python/framework/ops.py:1786 __init__\r\n        control_input_ops)\r\n    \u2026/tensorflow_core/python/framework/ops.py:1622 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: slice index 1 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\r\n```", "comments": ["Could able to replicate the issue with Tf2.1.\r\nPlease find the [gist here](https://colab.sandbox.google.com/gist/gadagashwini/f0af4a0d27a4f1ebe8e9b37f20a13d51/untitled508.ipynb). Thanks", "This is a known inconsistency that we hope to remedy, so I'll leave the issue open to track.\r\n\r\nThe main issue is that functions like `tf.size`, `tf.shape` and `tf.rank` always return a Tensor, forcing AutoGraph to create a tf.cond which triggers shape checks in both tensor[1] and tensor[0]. If the shape of tensor is unknown, the code once more works correctly.\r\n\r\nIncluding a few workarounds below.\r\n\r\nIf the tensor is a vector, `len` will work consistently:\r\n\r\n```\r\n    if len(tensor) == 2:\r\n        return tensor[1]\r\n    else:\r\n        return tensor[0]\r\n```\r\n\r\nIf the tensor is higher-dimensional and you indeed need the entire size (rather than just the size of dimension zero), please use `.shape.num_elements()` instead of `tf.size` until this is resolved:\r\n\r\n```\r\n    if tensor.shape.num_elements() == 2:\r\n        return tensor[1]\r\n    else:\r\n        return tensor[0]\r\n```\r\n\r\nIn case `tensor.shape` may be unknown, the following code is more complete:\r\n\r\n```\r\ndef proper_size(tensor):\r\n  if tensor.shape.is_fully_defined():\r\n    return tensor.shape.num_elements()\r\n  return tf.shape(tensor)\r\n\r\n...\r\n\r\nif proper_size(tensor):\r\n  return tensor[1]\r\nelse:\r\n  return tensor[0]\r\n```", "@mdanatg thank you for the in-depth response, good to hear it's being addressed already.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210525, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ea14b161312f8dc20de763ac491bedee/38539.ipynb). Thanks!", "Was able to reproduce your issue in tf v2.7, please find the gist [here](https://colab.sandbox.google.com/gist/tilakrayal/849a93f687cf8c98d9e624b345f5bfb1/untitled156.ipynb). Thanks!"]}, {"number": 38536, "title": "C API for TensorFlow Lite for Microcontrollers (micro)?", "body": "Hi,\r\n\r\nI'm porting TF micro for my custom micro-controller. I also saw TF distribute C API (not C++) for general TF (not micro). Is it possible to add the C API to the micro package?", "comments": ["@nkreeger who is working on this.", "@jdduke @jvishnuvardhan @amahendrakar @galah92\r\nHey! I'm also working on an embedded project and would very much benefit if I had a C API for the micro package. Do you guys have any updates on the issue or maybe even past experience? Thanks.", "@nkreeger is working on this.", "@petewarden should we move this request to the new repo?"]}, {"number": 38513, "title": "Tflite GPU Delegate for Jetson TK1 Platform 32bit K1 GPU", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n\r\nv2.1.0\r\n\r\n- Are you willing to contribute it (Yes/No):\r\n\r\nyes, I'm barely looking into it, and would appreciate help, as the issues ( inevitably ) occur. \r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nJust getting started, and completely un-optimistic ( but worth a shot right? )\r\n\r\n**Will this change the current api? How?**\r\n\r\nHardly,\r\nWill merely require build instructions on how to get openGL ES 3.1 compiled with jetson tk1\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who has a aging 32bit TK1 dev board or Nyan-Big Chromebook\r\n\r\n**Any Other info.**\r\n\r\nOpenGL ES support since Jetpack v1.2\r\nhttps://developer.nvidia.com/jetson-tk1-development-pack-1_2", "comments": ["@DOCgould \r\n\r\n Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "Hi @ravikyram,\n\nThere are a few devices which use the tk1 processor,\nPulling from the elinux website:\n\nJetson TK1 development board,[86]\u00a0Nvidia\u00a0Shield Tablet,[87]\u00a0Acer Chromebook 13,[88]\u00a0HP Chromebook 14 G3,[89]\u00a0Lenovo ThinkVision 28, Xiaomi MiPad,[90]\u00a0Snail Games\nOBox, UTStarcom MC8718, Google\u00a0Project Tango\u00a0tablet,[91]\u00a0Apalis TK1 System on Module,[92]\u00a0Fuze Tomahawk F1,[93]\u00a0JXD Singularity S192[94]\n\nThe idea being utilizing the power of the tk1 GPU ( Cuda 6.5 ) alongside the quantized tensorflow application (tflite). \n\nExcuse me a few hours, I'm going to sleep. But I'll initiate a pull request sometime tomorrow for sure. ", "Hey Guys, \r\n\r\nafter doing some poking into the repo\r\n\r\nHere is my approach\r\n\r\n1. Make a Cuda Compiled Interpreter of the ops supported by OpenGL ES 3.1\r\n2. Create a GPU Delegate Following the API\r\n3. Deploy\r\n\r\nI think this is going to be really hard, because as of now, the android NDK/SDK appears to be a major component of this. \r\n\r\nNothing to pull yet( and probably not for a little while either), but Ill keep you posted on my progress!! "]}, {"number": 38469, "title": "tflite buffer from GPU", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n\r\ntemplate <typename T>\r\nStatus GlBuffer::MappedRead(\r\n    const std::function<Status(absl::Span<const T> d)>& reader) const {\r\n  if (bytes_size_ % sizeof(T) != 0) {\r\n    return InvalidArgumentError(\"Buffer is not aligned\");\r\n  }\r\n  gl_buffer_internal::BufferBinder binder(target_, id_);\r\n  gl_buffer_internal::BufferMapper mapper(target_, offset_, bytes_size_,\r\n                                          GL_MAP_READ_BIT);\r\n  if (!mapper.data()) {\r\n    return GetOpenGlErrors();\r\n  }\r\n  float *data = (float *)mapper.data();\r\n  for (int i = 0; i < 30; i++) {\r\n    __android_log_print(ANDROID_LOG_WARN, \"jni-tflite\", \"value=%f\", data[i]);\r\n  }\r\n  return reader(absl::MakeSpan(reinterpret_cast<const T*>(mapper.data()),\r\n                               bytes_size_ / sizeof(T)));\r\n}\r\n\r\nAbove code is in gl_buffer.h for read data from gpu to cpu, printed value is all zero for input node as i want to inpect the result is as expected. but output node has non zero values. why ?\r\n\r\nStatus Invoke(TfLiteContext* context) {\r\n    const EGLContext egl_context_at_delegate_init = env_->context().context();\r\n    const EGLContext egl_context_at_delegate_invoke = eglGetCurrentContext();\r\n    if (egl_context_at_delegate_init != egl_context_at_delegate_invoke) {\r\n      return FailedPreconditionError(\r\n          \"Delegate should run on the same thread where it was initialized.\");\r\n    }\r\n\r\n    // Push input data from a tensor to GPU.\r\n    for (ValueId id : inputs_) {\r\n      const ValueRef& ref = tensors_[id];\r\n      auto external_object = bhwc_objects_.FindBuffer(ref.tensor_index);\r\n      if (external_object) {\r\n        // Use input from GPU.\r\n        // Conversion is needed only when external object is not phwc4.\r\n        if (!IsPHWC4(tensors_[id].shape)) {\r\n          RETURN_IF_ERROR(bhwc_to_phwc4_.Convert(\r\n              ref.shape, *external_object, command_queue_.get(), external_object   \"Note:read data to cpu all is zero!!!?\"\r\n              phwc4_objects_.FindBuffer(id)));\r\n        }\r\n      } else {\r\n        // Copy from CPU to GPU\r\n        TfLiteTensor& tensor = context->tensors[ref.tensor_index];\r\n        RETURN_IF_ERROR(CopyToBufferHandle(id, &tensor));\r\n      }\r\n    }\r\n\r\n    // Run inference.\r\n    RETURN_IF_ERROR(inference_context_->Reset());\r\n    RETURN_IF_ERROR(inference_context_->Execute());\r\n\r\n    // Push output data from GPU to a tensor.\r\n    bool finished_gpu_processing = false;\r\n    for (ValueId id : outputs_) {\r\n      const ValueRef& ref = tensors_[id];\r\n      auto external_object = bhwc_objects_.FindBuffer(ref.tensor_index);\r\n      if (external_object) {\r\n        // Convert data from PHWC4 to BHWC and leave it in GPU object.\r\n        // Conversion is needed only when external object is not phwc4.\r\n        if (!IsPHWC4(tensors_[id].shape)) {\r\n          RETURN_IF_ERROR(\r\n              phwc4_to_bhwc_.Convert(ref.shape, *phwc4_objects_.FindBuffer(id),\r\n                                     command_queue_.get(), external_object));\r\n        }\r\n      } else {\r\n        // Wait until all GPU command are completed. This call leads to a lower\r\n        // processing latency because a buffer reading below will not stall if\r\n        // data is not yet ready.\r\n        if (!finished_gpu_processing) {\r\n          RETURN_IF_ERROR(command_queue_->WaitForCompletion());\r\n          finished_gpu_processing = true;\r\n        }\r\n        // Copy from GPU to CPU.\r\n        TfLiteTensor& tensor = context->tensors[ref.tensor_index];\r\n        RETURN_IF_ERROR(CopyFromBufferHandle(id, &tensor));          \"#Note:this is output node to print value has non zero value\" \r\n      }\r\n    }\r\n    return OkStatus();\r\n  }\r\n\r\n\r\n", "comments": []}, {"number": 38462, "title": "More clarity on TFLite + GPU", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/performance/gpu_advanced\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAfter a couple of days digging through documentation and source code, I'm still very confused about the current state of GPU support in tensorflow/lite.\r\n\r\n1. https://www.tensorflow.org/lite/performance/gpu_advanced#android_cc : talks about C/C++, which gives the illusion that one might use the lite/c API. But as far as I can see, the `ModifyGraphWithDelegate` function is not present in lite/c (why? It would be very helpful), even though it has the concept of delegates;\r\n2. https://www.tensorflow.org/lite/performance/gpu_advanced#android_cc : suggests a build command that generates a 60MB shared library... I don't see any benefit in giving such suggestion, since other commands listed in other pages will generate properly optimized binaries;\r\n3. https://www.tensorflow.org/lite/performance/gpu_advanced#android_cc : building on (2.), I'm also under the impression that building the delegate as a separate shared lib would not be the best option for minimizing the overall size - in this case, a target for building the delegate + libtensorflowlite together would be highly appreciated, at least as a documentation snippet (not to mention prebuilt binaries, which are referred by the team as \"coming soon\" in several not-so-recent issue comments);\r\n3. https://www.tensorflow.org/lite/performance/gpu_advanced#inputoutput_buffers : suggests the use of `GpuDelegate` which, as far as I understand comes from `lite/delegates/gpu/gl_delegate.h` and as such is deprecated. A big notice in the source code warns to migrate to the new implementation before the end of 2019, so it probably shouldn't be in documentation;\r\n4. https://www.tensorflow.org/lite/performance/gpu_advanced#inputoutput_buffers : \r\nWhile a replacement exists (`lite/delegates/gpu/delegate.h`), it does not have any `bindGlBufferToTensor()` function, and it is not clear how to achieve the same thing with the new delegate. There are several unanswered questions on SO about this;\r\n5. https://www.tensorflow.org/lite/performance/gpu_advanced#inputoutput_buffers : the example uses a SSBO, however the delegate seems to support [textures as well?](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl_delegate.h#L53-L57) If this can be a different way to send initial input, it would be nice to have it documented;\r\n\r\nIt is hard for us to plan the adoption of TFLite without a clear view over what you have, or at least where you're heading. For example, I'm especially interested in using GL buffers as input (sounds like a game changer), but I have no clue about what's the state of this in TFLite. Same with using delegates in lite/c, the abstraction is there but `ModifyGraphWithDelegate` is not. So doc fixes apart, could we have a very brief description of where TFLite + GPU/delegates is headed and what's coming in the next couple of months, so that people can decide if it meets their needs and plan accordingly? \r\n\r\nI understand that some of these APIs are marked as experimental and I really appreciate your work. Thanks!\r\n", "comments": []}, {"number": 38453, "title": "Gradient not registered in autograph mode with tf.data.Dataset loop", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): True\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or\r\nbinary): PIP\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n\r\n\r\n**Describe the current behavior**\r\nIn autograph mode, gradient is not registered properly (propagates None) if function value depends on dataset values\r\n\r\n**Describe the expected behavior**\r\nGradient in autograph should match gradient in eager mode\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([2., 3.])\r\n\r\ndef fun(x):\r\n    for data in dataset:\r\n        x = x * data\r\n    return x\r\n\r\ndecorated_fun = tf.function(fun)\r\n\r\na = tf.constant(1.)\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(a)\r\n    b = fun(a)\r\n    c = decorated_fun(a)\r\nprint(tape.gradient(b, a)) # prints 6. as expected\r\nprint(tape.gradient(c, a)) # prints None\r\n\r\n```\r\n\r\n", "comments": ["Also note that the dataset example in the [official performance doc](https://www.tensorflow.org/tutorials/customization/performance) wouldn't propagate the gradient either.\r\n\r\n```python\r\n@tf.function\r\ndef train(a, dataset):\r\n    loss = tf.constant(0.)\r\n    for x, y in dataset:\r\n        loss += a * tf.abs(y - x) # Some dummy computation.\r\n    return loss\r\n\r\ndata = [(1., 1.)] * 10\r\n\r\ndataset = tf.data.Dataset.from_generator(lambda: data, (tf.float32, tf.float32))\r\n\r\na_tensor = tf.constant(1.)\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(a_tensor)\r\n    list_val = train(a_tensor, data)\r\n    dataset_val = train(a_tensor, dataset)\r\nprint(tape.gradient(list_val, a_tensor)) # prints 0.\r\nprint(tape.gradient(dataset_val, a_tensor)) # prints None\r\n```", "i am able to replicate the code shared, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/3a8f938d5bb54c5e2ce7422ab0d4c88b/38453.ipynb)", "In the meantime I have found a workaround for the above by going a bit lower level and implementing the while_loop myself. I am posting it here in case it helps tf maintainers to figure out what the bug is related to.\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\nthis_dataset = tf.data.Dataset.from_tensor_slices([2., 3.])\r\n\r\ndef fun(x, dataset):\r\n    for data in dataset:\r\n        x = x * data\r\n    return x\r\n\r\ndef low_level_fun(x, dataset, n):\r\n    iterator = iter(dataset)\r\n    i0 = tf.constant(0)\r\n    \r\n    def body(val, i):\r\n        next_data = iterator.get_next()\r\n        return val * next_data, i+1\r\n    \r\n    def cond(_val, i):\r\n        return i < n\r\n    \r\n    res, _ = tf.while_loop(cond, body, [x, i0], back_prop=True)\r\n    \r\n    return res\r\n\r\ndecorated_fun = tf.function(fun)\r\ndecorated_low_level_fun = tf.function(low_level_fun)\r\n\r\na = tf.constant(1.)\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(a)\r\n    b = fun(a, this_dataset)\r\n    c = decorated_fun(a, this_dataset)\r\n    d = low_level_fun(a, this_dataset, 2)\r\n    e = decorated_low_level_fun(a, this_dataset, 2)\r\n    \r\nprint(tape.gradient(b, a)) # prints 6. as expected\r\nprint(tape.gradient(c, a)) # prints None\r\nprint(tape.gradient(d, a)) # prints 6. as expected\r\nprint(tape.gradient(e, a)) # prints 6. as expected\r\n\r\n```", "tf.data does not support gradients.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38453\">No</a>\n", "> tf.data does not support gradients.\r\n\r\nCan you explain what the intended use of dataset is then? My understanding (backed by the \"loss\" example in [here](https://www.tensorflow.org/tutorials/customization/performance)) is that it can (should?) be used to construct a loss function that you should be able to differentiate with respect to the \"model\" parameters then.\r\n\r\nWhat is not legit in what I am saying?\r\n", "The intended use for tf.data is to perform the (non-trainable) [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) for data used by your ML workload.", "OK so what is the recommended interface for building a loss function depending on data (basically something similar to the official doc for performance)?", "Think of tf.data as a tool for data loading into tensorflow, not for data processing as part of TF.\r\n\r\nSo we recommend you don't use tf.data to transform a piece of data you already have loaded in memory, as it's not really designed for that. Instead use your own TF loops (which should be relatively straightforward to write with autograph) to do your mapping, filtering, batching (which is just tf.stack), etc, all of which are differentiable operations.", "Nevermind, I think there's an actual bug here where autograph is generating loops which break autodiff sometimes.", "@AdrienCorenflos `for data in iter(dataset):` will generate gradients as expected. With `iter`, autograph will generate code very similar to what you wrote. Without, it uses `reduce`, `scan` and `take_while` which don't support gradients. We'll look into making them behave consistently.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210525, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/919a01ca7bab6405ddb543a0fb8ce484/38453.ipynb). Thanks!", "Was able to reproduce your issue in Tf v2.7 please find the gist [here](https://colab.sandbox.google.com/gist/tilakrayal/92a721fc3289d9b9aa6c704c593cb717/untitled157.ipynb). Thanks!"]}, {"number": 38429, "title": "Broadcasting sparse indices", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes (partially, what can be done on the python side, and maybe CPU kernels)\r\n\r\n**Will this change the current api? How?**\r\nAdd an operation to `tf.sparse.broadcast_indices` (feel free to suggest a beter name), doing the following:\r\n```\r\ntf.sparse.broadcast_indices(a, b, default_value): Given two sparse tensors of same dense shape, inserts `default_value` at positions such that a.indices == b.indices.\r\n```\r\nThis would provide a basic building block for arbitrary pointwise-binary operations on SparseTensors. Therefore I would also suggest adding a function like\r\n```\r\ntf.sparse.map_binary_op(a, b, op, def_value=0):\r\n   a, b = broadcast_indices(a, b, def_value)\r\n   return SparseTensor(a.indices, op(a.values, b.values), a.shape) \r\n```\r\n\r\nThis would then allow for more efficient implementations e.g. of binary metrics when both labels and predictions are sparse, e.g. when calculating P@k statistics.\r\n\r\nI think I can provide an initial, sub-optimal implementation that builds this on top of `tf.sparse.add` for default values of zero. Would you be interested in a PR like that (maybe putting it into experimental namespace at first)? \r\n", "comments": []}]