[{"number": 4611, "title": "Documentation for Allocator", "body": "Feature request: Documentation for Allocator on the web site (referenced by https://www.tensorflow.org/versions/r0.10/api_docs/cc/ClassTensor.html).\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nThere is no documentation for Allocator or references to it except a few SO threads about interpreting PoolAllocator log messages. Information about it does not seem to exist outside of the source itself.\n", "comments": ["I doubt we'll ever get to this -- Allocator is really an internal implementation detail, and subject to changes.  For now, sources -- and associated unit tests -- are your best bet.  \n"]}, {"number": 4610, "title": "Updating estimators to return the directory of the export.", "body": "Cherypick request to r0.11\n", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @philstahlfeld and @theweiho to be potential reviewers\n", "Feel free to merge without tests since the release build will run them again anyway.\n", "Good point. Already passed many tests anyway.\n"]}, {"number": 4609, "title": "User ops example \"Fact\" can't work.", "body": "I build tensorflow from sources(master). When i try to use the user_ops example \"Fact\", I got the follow errors. I try to change to branch r0.10, the error does't exists. It may be a bug in master branch?\n\n```\nIn [11]: tf.user_ops.my_fact()\n\nAttributeError                            Traceback (most recent call last)\n<ipython-input-11-d6bdebf36140> in <module>()\n----> 1 tf.user_ops.my_fact()\n\n/path/to/tensorflow/_python_build/tensorflow/python/user_ops/user_ops.py in my_fact()\n     26 def my_fact():\n     27   \"\"\"Example of overriding the generated code for an Op.\"\"\"\n---> 28   return gen_user_ops._fact()\n\nAttributeError: 'module' object has no attribute '_fact'\n```\n", "comments": ["a redundant `BUILD` in core/user_ops, removed it and it works.\n"]}, {"number": 4608, "title": "optional arguments should be optional", "body": "the input_op_fn appears to be optional (it has a default value), but the default value is trash and causes the function to crash with a less than useful argument.\n\nI don't have the skill to fix the rnn.py function at the moment, but it seems to me that the null_input_op_fn should make some guess at how the data should be unwrapped rather than just return something that will cause a crash.\n", "comments": ["Can one of the admins verify this patch?\n", "@pfaucon, thanks for your PR! By analyzing the annotation information on this pull request, we identified @benoitsteiner and @tensorflower-gardener to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the change! This behavior is intentional. You can only avoid passing input_op_fn if your input is already in the right format (sequence) for the RNN.\n", "Could you clarify what you mean \"if your input is already in the right format\"?  I would be very happy to write a passing test case that does not use any of the optional methods if you show me the way.\n\nI've tried making a new np.array containing a list (but not list of lists) and that fails.\nI've tried passing a list and that fails (no dtype).\n", "Ah, you're right that there's no way to get it to work without input_op_fn. That's because RNN's expect an unpacked (python) list of tensors, but the fit function takes in an np.array.\n\nA minimal example would be something like:\n`classifier = tf.contrib.learn.TensorFlowRNNClassifier(rnn_size=2, cell_type=\"lstm\", n_classes=2, steps=100, input_op_fn=lambda x: tf.unpack(x, axis=1))\nclassifier.fit(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8 , 9], [10, 11, 12]]], dtype=np.float32), np.array([0, 1], dtype=np.float32))`\n\nKeep in mind that we can't merge in failing tests because our test suite ensures that every commit passes all tests before merging pull requests. So any pull request with a previously failing unit test would also require a fix. If you simply have an example that fails without the fix, please file an issue instead.\n", "I'm new to Tensorflow, so I'm not sure what the best approach would be to go from here but in your opinion would it be reasonable to modify the set input_op_fn as an unpack?  Maybe read in the input tensor and guess the direction of the unpack/slice? Or just remove the default and force people to provide an input function?\n"]}, {"number": 4607, "title": "dtype argument for foldl/foldr", "body": "Why doesn't foldl/foldr have a dtype argument as in map_fn, so that the output dtype can be different from the input dtype?\n\nUse case: \ninput is a list of indices (int32),  output is sum of embeddings of the indices (float32).\n\nThanks!\n", "comments": ["@hhexiy I'm not positive, but I believe that you can call `foldl` and `foldr` with an appropriate `initializer`, and the output type and shape can be anything that you want.\n", "Automatically closing due to lack of recent response. Please reopen when additional information becomes available. Thanks!\n"]}, {"number": 4606, "title": "Branch 134473452", "body": "@gunan fyi\n", "comments": ["@jhseu, thanks for your PR! By analyzing the annotation information on this pull request, we identified @charlesnicholson, @tensorflower-gardener and @danmane to be potential reviewers\n", "Jenkins, test this please\n"]}, {"number": 4605, "title": "Branch 134471468", "body": "", "comments": ["@jhseu, thanks for your PR! By analyzing the annotation information on this pull request, we identified @charlesnicholson, @tensorflower-gardener and @danmane to be potential reviewers\n", "Jenkins, test this please\n", "@gunan fyi for 0.11\n", "Hmm, somehow missing the change we wanted for the cherrypick. Abandoning.\n"]}, {"number": 4604, "title": "Using negative index slice on tf.scan accesses uninitialized memory", "body": "Accessing last element of array produced by scan by using array[-1] gives random results. IE\n\n```\nimport tensorflow as tf\nout = tf.scan(lambda p, c: p * c, tf.fill([2], 2), initializer=tf.constant(1)) \nsess = tf.Session()\nprint sess.run(out)\nprint sess.run(out[1])\nprint sess.run(out[-1])\n\n```\n\nThis generates\n\n```\n[2 4]\n4\n181075993\n```\n\nThe last number is different on reruns, `tf.__version__` is 0.10\n", "comments": ["I reproduced this on docker image tensorflow/tensorflow:0.10.0 \nbut it works fine for me on  tensorflow/tensorflow:nightly\n", "Closing this.  Feel free to re-open if it still is an issue at HEAD.\n"]}, {"number": 4603, "title": "import_meta_graph: ValueError: At least two variables have the same name: Variable_1", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nNone\n### Environment info\n\nOperating System: Ubuntu 16.04.1 LTS\nInstalled version of CUDA and cuDNN: none\npip package: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n$ python -c \"import tensorflow; print(tensorflow.**version**)\"\n0.10.0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nUsing commands from https://www.tensorflow.org/versions/r0.9/get_started/index.html to create a model:\n\n```\nimport tensorflow as tf\nimport numpy as np\nx_data = np.random.rand(100).astype(np.float32)\ny_data = x_data * 0.1 + 0.3\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.zeros([1]))\ny = W * x_data + b\nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\nsess.run(train)\n```\n\nThen this works:\n\n```\nsaver = tf.train.Saver()\nsaver.save(sess, 'my-model')\nsess = tf.Session()\nnew_saver = tf.train.import_meta_graph('my-model.meta')\n```\n\nBut this doesn't:\n\n```\ntf.train.export_meta_graph(filename='my-model.meta')\nnew_saver = tf.train.import_meta_graph('my-model.meta')\n```\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1458, in import_meta_graph\n    return _import_meta_graph_def(read_meta_graph_file(meta_graph_or_file))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1386, in _import_meta_graph_def\n    return Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 861, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 502, in build\n    vars_to_save = self._ValidateAndSliceInputs(names_to_variables)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 399, in _ValidateAndSliceInputs\n    names_to_variables = self._VarListToDict(names_to_variables)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 377, in _VarListToDict\n    name)\nValueError: At least two variables have the same name: Variable_1\n\nWhat I actually want to use is this, which also does not work (same error):\n\n```\nmeta_graph_def = tf.train.export_meta_graph()\nsaver = tf.train.import_meta_graph(meta_graph_def)\n```\n\nI need a solution that does not rely on the file system.\nAm I doing something wrong?\n### What other attempted solutions have you tried?\n\nI tried to look at the files to see where these duplicated names occur but did not find any.\n", "comments": ["You should call `tf.train.import_meta_graph()` with a cleared `Graph`.  Example:\n\n``` python\n\n    # Create a clean graph and import the MetaGraphDef nodes.\n    new_graph = tf.Graph()\n    with tf.Session(graph=new_graph) as sess:\n      # Import the previously export meta graph.\n      saver = tf.train.import_meta_graph(meta_graph_def)\n```\n\nPlease feel free to reopen if this doesn't solve the issue.\n", "Thanks! This information was missing from https://www.tensorflow.org/versions/r0.9/how_tos/meta_graph/index.html\n"]}, {"number": 4602, "title": "Make TF_OperationGetAttrMetadata extern", "body": "`TF_OperationGetAttrMetadata` should probably be `extern` as the rest of the functions.\n\nRegards,\nIvan\n", "comments": ["Can one of the admins verify this patch?\n", "@IvanUkhov, thanks for your PR! By analyzing the annotation information on this pull request, we identified @asimshankar, @tensorflower-gardener and @keveman to be potential reviewers\n", "Jenkins, test this please\n", "The latest internal build made a rename. Mind resolving the merge conflict?\n", "@jhseu, done!\n", "Jenkins, test this please\n", "(attempt 2) Jenkins, test this please\n"]}, {"number": 4601, "title": "convert Inception v1 model .pb file into 8bit precision fail", "body": "I use the guide below to download a v3 model and convert it to 8 bit precision without no issue.\n\nhttps://www.tensorflow.org/versions/master/how_tos/quantization/index.html\n\ncurl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz\ntar xzf /tmp/inceptionv3.tgz -C /tmp/\nbazel build tensorflow/contrib/quantization/tools:quantize_graph\n**bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \\\n--input=/tmp/classify_image_graph_def.pb \\\n--output_node_names=\"softmax\" --output=/tmp/quantized_graph.pb \\\n--mode=eightbit**\n\nHowever, i download a v1 model from below and try to convert to 8 bit precision fail.\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/\n\nhttps://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\n\nthe model file is: \"tensorflow_inception_graph.pb\" \n## I get error:\n\nFile \"/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py\", line 319, in rewrite\n    for output_node_name in output_node_names]\n## KeyError: 'softmax'\n\nit says key error: softmax. is this caused by v1 model use something other than softmax or it has different name? What should i input for --output_node_names parameter when I convert v1? Or how can i get the output node name? If i try to convert other model .pb file how to input this parameter?\n\nThanks\n", "comments": ["i get the answer, the value for v1 model for this --output_node_names parameter is \"output\" but not \"softmax\".\n"]}, {"number": 4600, "title": "fold/scan gradient high memory usage", "body": "It seems that the gradients for higher order functions (foldl/foldr/scan) require a very large amount of memory.\n\nThe following code block will easily compute `res` (even with `swap_memory` disabled) but will fail when computing `grad`:\n\n``` python\nv = tf.Variable([1.0])\ndef foo(aggregate, arr):\n    arr = v * tf.tile(arr, [10000])\n    return aggregate + arr[0, 0]\n\narr = np.empty([10000, 1000], dtype=np.float32)\nres = tf.foldl(foo, arr, 0.0, 1, swap_memory=True)\nopt = tf.train.GradientDescentOptimizer(0.001)\ngrad = opt.compute_gradients(res)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nsess.run(res) #This will run\nsess.run(grad) #This will cause an OOM error\n```\n\nThis was tested on a machine with a GTX1070 and 24GB RAM. \n\nI'm guessing that what's happening is that tensorflow is only keeping track of data necessary for one iteration of `foldl` at a time when computing `res` (~40MB) but has to keep track of all the memory in all iterations of `foldl` when computing `grad` (~400GB).\n\nIn fact, reducing `arr` to be of size 1000x1000, still causes an OOM error, but reducing the size of `arr` to be 500x1000 ends up with the code block succeeding. This makes sense given that setting `arr` to these two sizes requires ~40GB and ~20GB respectively, the latter of which just fits on my machine's memory.\n\nFrom what I understand of automatic differentiation, tensorflow shouldn't need to simultaneously keep track of the data needed across all iterations of `foldl` when computing gradients. Or am I missing something?\n", "comments": ["I believe TensorFlow doesn't (yet) have optimized scheduling policies that intelligently trade-offs among latency and memory.  Adding @yuanbyu for further comments. \n", "Thanks for the reply @concretevitamin :)\n\nI'm not entirely sure a complex scheduling policy would be needed in this case. Wouldn't it be enough to just compute the gradient from each iteration individually and then incrementally build up the overall gradient of `foldl`? This would be similar to the way I imagine the actual value of `foldl` is currently computed.\n\nAlso do you happen to know of any way I could work around this memory issue given the current implementation of tensorflow?\n", "I don't know if the proposed backward algo is doable with the current implementation.  Maybe Yuan can comment.\n\nIn the mean time, could you try to break up the long `foldl` into a few pieces, and perhaps place control dependencies among them? \n", "No. If the backprop needs a forward tensor, it needs its values for all iterations. This is very similar to a very deep network where each iteration can be viewed as a layer. Of course, this can be optimized by for example recomputing the values as opposed to keeping them in memory."]}, {"number": 4599, "title": "corrected comment in simple_placer concerning  step 2 in placement algorithm", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@larissa95, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @josh11b to be potential reviewers\n", "Thanks!\n"]}, {"number": 4598, "title": "get EIGEN_VERSION from workspace.bzl", "body": "perhaps a better fix for #4575\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please\n", "This pull request is duplicate with #4563 .\n", "Closing this in favor of other other change.\n"]}, {"number": 4597, "title": "Update farmhash commit hash to point to the farmhash's version that a\u2026", "body": "\u2026dds support for ppc64le architecture\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please\n"]}, {"number": 4596, "title": "Tensorboard generating javascript errors: Failed to load resource: net::ERR_CONNECTION_RESET", "body": "I'm running tensorboard on the Windows 10 Bash on Ubuntu shell. After bringing up the tensorboard webserver (localhost:6006) with `tensorboard --logdir=/mnt/d/Dropbox/Andor/Summaries --debug` and connecting to it with my Chrome browser, I don't see anything on the screen. The JS console gives me a couple of the following ERRORS:\n\n```\nFailed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/external/plottable/plottable.min.js\nFailed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/external/polymer/polymer.html\nFailed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/dist/tf-tensorboard.html\nFailed to load resource: net::ERR_CONNECTION_RESET http://localhost:6006/external/iron-icons/iron-icons.html\n```\n\nExcept for http://localhost:6006/dist/tf-tensorboard.html, the other three missing resources are all there in the correct directories (/usr/local/lib/python2.7/dist-packages/external/...). The /usr/local/lib/python2.7/dist-packages/dist/ directory does not exist in my installation.\n\nAfter these errors in the JS console come hundreds of `Uncaught ReferenceError: Polymer is not defined`\n\nI have searched on the web for related issues but only found [this one issue](https://github.com/tensorflow/tensorflow/issues/1421) which I believe is unrelated as I don't get any of the described errors in my case. In fact, there are no errors on the server's console output, no warnings, all ok (I'm running the server in debug mode).\n\nOther parameters of my system:\n- I did a pip install of tensorflow\n\nAlso:\n\n```\npython -c \"import tensorflow; print(tensorflow.__version__)\n0.8.0\n```\n\nA small event file is attached here:\n[events.out.tfevents.1474905790.zip](https://github.com/tensorflow/tensorflow/files/494789/events.out.tfevents.1474905790.zip)\n\nAlso, the server's console output is here (after starting the server AND making a request in the browser http://localhost:6006):\n\n```\nUser@PC_001:/mnt/d/Dropbox/Andor$ tensorboard --logdir=/mnt/d/Dropbox/Andor/Summaries --debug\nINFO:tensorflow:TensorBoard is in debug mode.\nINFO:tensorflow:Starting TensorBoard in directory /mnt/d/Dropbox/Andor\nINFO:tensorflow:TensorBoard path_to_run is: {'/mnt/d/Dropbox/Andor/Summaries': None}\nINFO:tensorflow:Adding events from directory /mnt/d/Dropbox/Andor/Summaries/train\nINFO:tensorflow:Constructing EventAccumulator for /mnt/d/Dropbox/Andor/Summaries/train\nDEBUG:tensorflow:Opening a record reader pointing at /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001\nDEBUG:tensorflow:No more events in /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001\nINFO:tensorflow:No path found after /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001\nDEBUG:tensorflow:No more events in /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001\nINFO:tensorflow:No path found after /mnt/d/Dropbox/Andor/Summaries/train/events.out.tfevents.1474905790.DUCANDUGMBH_001\nINFO:tensorflow:Multiplexer done loading. Load took 0.0 secs\nINFO:tensorflow:TensorBoard is tag: 16\nStarting TensorBoard 16 on port 6006\n(You can navigate to http://0.0.0.0:6006)\n127.0.0.1 - - [27/Sep/2016 09:12:48] \"GET / HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/lodash/lodash.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/plottable/plottable.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/d3/d3.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/plottable/plottable.css HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/graphlib/dist/graphlib.core.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/dagre/dist/dagre.core.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/polymer/polymer.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/iron-ajax/iron-ajax.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/iron-collapse/iron-collapse.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/iron-list/iron-list.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-button/paper-button.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-checkbox/paper-checkbox.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-icon-button/paper-icon-button.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-header-panel/paper-header-panel.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-input/paper-input.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-item/paper-item.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-menu/paper-menu.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:49] \"GET /external/paper-progress/paper-progress.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-radio-button/paper-radio-button.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-radio-group/paper-radio-group.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-slider/paper-slider.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-styles/paper-styles.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-toggle-button/paper-toggle-button.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-toolbar/paper-toolbar.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-tabs/paper-tabs.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/iron-resizable-behavior/iron-resizable-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/iron-ajax/iron-request.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-material/paper-material.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-ripple/paper-ripple.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-behaviors/paper-button-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/iron-flex-layout/iron-flex-layout.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-styles/default-theme.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-behaviors/paper-checked-element-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/paper-menu-button/paper-menu-button.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/iron-a11y-keys-behavior/iron-a11y-keys-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/iron-behaviors/iron-control-state.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:50] \"GET /external/iron-behaviors/iron-button-state.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-icons/iron-icons.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-icon/iron-icon.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-form-element-behavior/iron-form-element-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-validatable-behavior/iron-validatable-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-behaviors/paper-inky-focus-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-input/iron-input.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-input/paper-input-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-input/paper-input-char-counter.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-input/paper-input-container.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-input/paper-input-error.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-item/paper-item-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-item/paper-item-shared-styles.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-menu-behavior/iron-menu-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-styles/color.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/paper-menu/paper-menu-shared-styles.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-range-behavior/iron-range-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-selector/iron-selectable.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:51] \"GET /external/iron-flex-layout/classes/iron-flex-layout.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-styles/shadow.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-styles/typography.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/iron-menu-behavior/iron-menubar-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-tabs/paper-tabs-icons.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-tabs/paper-tab.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/promise-polyfill/promise-polyfill-lite.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-material/paper-material-shared-styles.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-behaviors/paper-ripple-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/iron-checked-element-behavior/iron-checked-element-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/iron-dropdown/iron-dropdown.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/neon-animation/animations/fade-in-animation.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/neon-animation/animations/fade-out-animation.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-menu-button/paper-menu-button-animations.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/iron-meta/iron-meta.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/iron-a11y-announcer/iron-a11y-announcer.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/paper-input/paper-input-addon-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/iron-selector/iron-selection.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:52] \"GET /external/iron-selector/iron-multi-selectable.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/iron-flex-layout/classes/iron-shadow-flex-layout.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/font-roboto/roboto.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/iron-iconset-svg/iron-iconset-svg.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/promise-polyfill/Promise.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/iron-overlay-behavior/iron-overlay-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/neon-animation/neon-animation-runner-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/neon-animation/animations/opaque-animation.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/iron-dropdown/iron-dropdown-scroll-manager.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/neon-animation/neon-animation-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/neon-animation/web-animations.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/iron-fit-behavior/iron-fit-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/iron-overlay-behavior/iron-overlay-backdrop.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/iron-overlay-behavior/iron-overlay-manager.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/neon-animation/neon-animatable-behavior.html HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:53] \"GET /external/web-animations-js/web-animations-next-lite.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [27/Sep/2016 09:12:54] \"GET /external/web-animations-js/web-animations-next-lite.min.js.map HTTP/1.1\" 200 -\n\n```\n", "comments": ["Ok, I found the solution. It's apparently the browser (Chrome), which has an expired certificate.\n\nTensorboard on Windows 10 (bash on ubuntu) works well for me now with the Firefox browser.\n\nOne more unrelated thing, though:\nThe resource: `web-animations-next-lite.min.js.map` in the `/usr/local/lib/python3.4/dist-packages/external/web_animations_js` directory seems to be missing in the Windows python 3.4 pip install of tensorflow (I used the `https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl`). I had to manually generate this one file from GitHub. Only after that, all 404 errors disappeared.\n", "@sven1977 Glad you worked things out!  I'll suggest that you upgrade to the latest version of TensorFlow, to pick up all the bugfixes we've added since 0.8:\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#download-and-setup\n\nI'm closing this out, but feel free to comment or file a new bug if you have other issues.\n"]}, {"number": 4595, "title": "Rename TF_Attr_Type and TF_Attr_Metadata", "body": "The names of the `TF_Attr_Type` enum and `TF_Attr_Metadata` struct seem to be unlike any other names in the C API. Contrast them with `TF_OperationDescription`, `TF_SessionOptions`, `TF_SessionWithGraph`, and `TF_DataType`, which are also composed of multiple words. Since 0.11 hasn\u2019t been released yet, perhaps it\u2019s not too late to unify the naming scheme.\n\nRegards,\nIvan\n", "comments": ["@josh11b \n", "Thanks for pointing this out @IvanUkhov - this was a silly oversight on my part.\nI will have this fixed in HEAD shortly, however we're unlikely to merge that into the release branch for 0.11. \n\nOnce a release branch is cut, we try to avoid merging changes that don't fix a breaking bug in order to prevent release delays. Since this is a naming change of functions barely used yet, it probably isn't worth holding up the release for it.\n"]}, {"number": 4594, "title": "Update optimizers.py", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@gautam1858, thanks for your PR! By analyzing the annotation information on this pull request, we identified @alexgkendall, @ilblackdragon and @tensorflower-gardener to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it !\n\nRegards,\nGautam.R\n\nOn Tue, Sep 27, 2016 at 11:11 AM, googlebot notifications@github.com\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4594#issuecomment-249771100,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEuHEp7DEl13r3aapwGK9GSCRN9fLSXBks5quKyEgaJpZM4KHRoF\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the change! We can't change the default yet because some ops may not be colocateable with the gradient (e.g., if an implementation doesn't exist on the device). Also, changing the default would require some analysis of performance impact.\n"]}, {"number": 4593, "title": "tf.cumsum returns wrong value on reverse", "body": "To reproduce the issue:\n\n``` python\nv = [ 10.52005689  10.9425797   10.60515152   9.59574335   8.4282654\n   9.97478889   9.33102282  10.68171879  10.85262443   9.06804822]\nw = tf.cumsum(tf.reverse(v,[True]))\nq = tf.cumsum(tf.reverse(v,[True]),reverse=True)\nwith tf.Session() as sess:\n    res = sess.run([w,q])\n    print res[0]\n    print res[1]\n```\n\nreturns\n\n``` python\n[   9.06804822   19.92067265   30.60239143   39.93341425   49.90820314\n   58.33646854   67.93221189   78.5373634    89.47994311  100.        ]\n[ 100.           90.93195178   80.07932735   69.39760857   60.06658575\n   50.09179686   41.66353146   32.06778811   21.4626366    10.52005689]\n```\n\nwhich are NOT the reverse of each other.  Note, the value for w is correct.  The q value should be the reverse of w, but is not.  The cumsum values are also incorrect...\n", "comments": ["I think there's a bug in the question. Perhaps you meant to do\n\n``` python\nw = tf.cumsum(tf.reverse(v, [True]))\nq = tf.cumsum(v, reverse=True)\n```\n\nwhich works fine for me. \n", "RIght, the issue is that calling cumsum with the reverse flag set to True on a tensor that is being reversed returns the wrong value.  So to your point:\n\n``` python\nw = tf.cumsum(tf.reverse(v,[True]), reverse=True)\n```\n\nis NOT the same as\n\n``` python\nq = tf.reverse(tf.cumsum(tf.reverse(v[True])))\n```\n", "@jameshensman My question was too obtuse.  The issue is this:\n\n``` python\ntf.cumsum(tf.reverse(v,[True]),reverse=True)\n```\n\nis NOT the same as\n\n``` python\ntf.reverse(tf.cumsum(tf.reverse(v,[True]),[True])\n```\n", "As far as I can tell, this is exactly the semantics of the two ops.\n\n``` python\nv = tf.constant(np.arange(0, 5))\nprint tf.cumsum(tf.reverse(v,[True]),reverse=True).eval()\nprint tf.reverse(tf.cumsum(tf.reverse(v,[True])),[True]).eval()\n```\n\nprints\n\n```\n[10  6  3  1  0]\n[10 10  9  7  4]\n```\n"]}, {"number": 4592, "title": "Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary", "body": "when i  change 'max_step' from 1000 to 10000 in mnist_with_summaries.py ,after 8550 iters occur these error:\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: layer1/biases/summaries/HistogramSummary\n     [[Node: layer1/biases/summaries/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer1/biases/summaries/HistogramSummary/tag, layer1/biases/Variable/read/_21)]]\nTraceback (most recent call last):\n", "comments": ["I did not confirm this myself, but could it be that this model is basically fully trained within 1000 steps?  If this is the case, continue training may or may not lead to NaNs.  Please confirm.\n", "I think mnist_with_summaries prior to r0.11 had bugs.  If you are using mnist_with_summaries.py at HEAD, they might be fixed.  I wouldn't use mnist_with_summaries.py for anything real -- the point was to demonstrate how to attach summaries to a model.\n", "Automatically closing due to lack of recent activity. Please reopen when additional information becomes available. Thanks!\n", "but my tensorflow version is 1.0.1, so I don't know why have a error? the error is \"Invalid argument: Nan in summary histogram for: train/boxes_delta_y\r\n\t [[Node: train/boxes_delta_y = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](train/boxes_delta_y/tag, strided_slice_45/_99)]]\"\r\n\r\nI need help, help, help!!! Please!!!", "InvalidArgumentError: Nan in summary histogram for: dnn/dnn/logits_activation\r\n\t [[Node: dnn/dnn/logits_activation = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](dnn/dnn/logits_activation/tag, dnn/logits/BiasAdd)]]\r\n\r\nI am having the same error too.....have been trying to figure out what's wrong but not being able to do it....someone please help me on this", "@issxjl2015, @sswarnakar, you may have the same error but for different reasons. Please post detailed issues with **reproducible test cases** as new issues if you would like help. Assuming the causes of the error being the same is easy to do, but often not true. Thank you."]}, {"number": 4591, "title": "Branch 134346700", "body": "", "comments": []}, {"number": 4590, "title": "Better shape inference for tf.slice and tf.strided_slice", "body": "Currently, if any of the values in the `size` argument is not a constant, the output shape is completely unknown (but the correct rank):\n\n``` python\n>>> z = tf.zeros((1, 2, 3))\n>>> z.get_shape().as_list()\n[1, 2, 3]\n>>> m = tf.slice(z, [0, 0, 0], [-1, -1, -1])\n>>> m.get_shape().as_list()\n[1, 2, 3]\n>>> m = tf.slice(z, [0, 0, 0], [tf.constant(1) + 0, -1, -1])\n>>> m.get_shape().as_list()\n[None, None, None]\n```\n\nThe desired behaviour would instead treat the second and third dimensions correctly:\n\n``` python\n>>> m = tf.slice(z, [0, 0, 0], [tf.constant(1) + 0, -1, -1])\n>>> m.get_shape().as_list()\n[None, 2, 3]\n```\n\nLooking briefly at the code, this would requite being a bit more clever in terms of how constant values are computed; right now if anything in a `Pack`-ed array is unknown at graph construction time, the entire array is unknown (see `_ConstantValue`'s `Pack` case).\n\nI guess this request ends up being just a request for a better constant propagation system which supports partially-known tensors. Perhaps you already have other use cases for such a feature, in which case view this as just another request that such a feature would enable.\n", "comments": ["Thanks for filing the issue @gibiansky!\n\nDo you have a use-case where the current shape inference behavior is making it hard or impossible to do something?  That will make it easier for us to prioritize this general class of problems.\n\nFYI there is a trade-off between full-blown shape inference including const-propagation, vs simpler code.  So it's not obvious that full-featured shape inference will always be worth it.  Of course this is a philosophical statement; that's why concrete examples are always helpful to determine priorities.\n", "Sure. What motivated filing this was a confusing error message and non-obvious solution with regard to `dynamic_rnn`. I was slicing a part of my input sequence (for example I needed to train on times T_0 through T_end, where T_end is computed as part of the graph) using `tf.slice`. However `dynamic_rnn` requires that the input has the last dimension defined (since that dimension is the dimension of the network input). As a result of the bug above, the output of `tf.slice` is of completely unknown shape, and so `dynamic_rnn` fails. It's possible to fix this with a `tf.reshape` with a constant dimension, but this is definitely a suboptimal situation (and non-obvious). Does that make sense?\n", "@gibiansky yes that makes sense, and thanks for the clear write-up!  :)\n\nFYI the following commit added the current logic:\nhttps://github.com/tensorflow/tensorflow/commit/5989d094ae15011c9a8a92d6fbcd829be71afa12\n\nThere's a technical detail that makes it hard for us to implement better shape inference when the size argument is only partially known.  I'll think about this a bit.\n", "Yeah, I tried to trace through the code to figure out how I might implement better shape inference just for `tf.slice`, but it's tricky. It would require a fair bit more work on the constant propagation \u2013 namely, you'd need to have a way to represent \"partially known\" tensors. The `size` array is computed by doing `tf.pack([tf.constant(1) + 0, -1, -1])`, roughly, but `Pack` can only be constant-propagated if the _entire_ tensor is known. \n\nIf I had to do this, what I would do is change [`_ConstantValue`](https://github.com/tensorflow/tensorflow/blob/c856366b739850a9f4b0bf1469de7f052619042b/tensorflow/python/framework/tensor_util.py#L555) to return a different data type. Right now it returns a `np.ndarray` containing the values that are in the tensor being converted to a constant (or `None` if the tensor is not a constant).\n\nInstead, it could return a tuple containing `(values, mask)`, where of types `(np.ndarray, np.ndarray)`, respectively, where `mask` is an array of booleans which select whether the respective elements are known (and their values are stored in `values`) or whether they are unknown. All cases where `_ConstantValues` currently returns non-None values would have `mask` be an array entirely of `True`; some cases where `None` is currently returned would be able to return a partially-True `mask`.\n", "@gibiansky Indeed, I think it's possible to implement, but it's a bit involved.\n\nI'm marking this as Contributions welcome for now, since using `tf.reshape` always works as a workaround.\n", "This is also an issue for strided_slice. There are some internal CLs that should be submitted soon to handle the case of constant begin and end in strided_slice.", "I created a PR #13561 to improve the shape inference so that as long as the size is not `-1`, the right shape value will be inferred:\r\n```python\r\n>>> import tensorflow as tf\r\n>>> z = tf.zeros((1, 2, 3))\r\n>>> z.get_shape().as_list()\r\n[1, 2, 3]\r\n>>> m = tf.slice(z, [0, 0, 0], [tf.constant(1) + 0, 2, -1])\r\n>>> m.get_shape().as_list()\r\n[None, 2, None]\r\n```\r\n*Note: The above example used to return `[None, None, None]`*.\r\n\r\nThe PR is not a completely fix for this issue. It is an improvement nevertheless.", "@yongtang Thanks for the PR! Closing this for now, stay tuned for future improvement."]}, {"number": 4589, "title": "Support for native half-float computation (float16/fp16)", "body": "https://github.com/tensorflow/tensorflow/issues/1300 added support for fp16 storage, but there is currently no support for native fp16 computation, which is available on some hardware such as Pascal GPUs.\n\nIn particular, the conv2d and matmul ops could take a new parameter along the lines of \"compute_dtype\", which would be plumbed through to CUDNN (convolution descriptor) and CUBLAS (Hgemm) in the backend, with the potential for up to a 2x speedup.\n\nRelated issues:\nhttps://github.com/tensorflow/tensorflow/issues/1300\nhttps://github.com/tensorflow/tensorflow/issues/4314\nhttps://github.com/tensorflow/tensorflow/issues/851#issuecomment-230923665\n", "comments": ["@benoitsteiner - Hello Benoit, since you've been looking at this, could you comment?\n", "Any update on the timeline for resolving this issue? Will it be fixed in the next TensorFlow software release (i.e v11)?\n", "Still no updates on this one? ", "Any update?", "I'd also be interested in this feature! We just bought several P100 GPUs and are training on Imagenet a lot. Using FP16 would really speed things up.", "I'm also very interested in this as I'm currently developing a net optimized for inference speed. Would you accept pull requests for this? I guess it's mainly about what is described in this [ToDo](https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1805) - find out which devices and CUDA versions support native float computation and then set the cudnn descriptor accordingly.\r\n\r\n[Nvidia's blog](https://devblogs.nvidia.com/parallelforall/mixed-precision-programming-cuda-8/) (table 2) states that this is the case for cuda >= 7.5 and cudnn >= 5.1.\r\n\r\nEdit:\r\nAnother example, [BLAS](https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/stream_executor/cuda/cuda_blas.cc#L1757) in this case.", "You need 5.3+ computability to use FP16", "Started working on this ;)", "We support native fp16 in most cases. However, we still compute convolutions and matrix multiplications using fp32 floats to avoid numerical stability issues.\r\n\r\nIn the case of the multiplications of a N by K matrix with a K by N matrix, we could use native fp16 provided that K is small. When K is large the reduced precision of fp16 introduces enough noise in the computation to cause issues such as reduced inference accuracy. Similarly, in the case of convolution, we could also use native fp16 provided that both the convolution window and the input depth are small. \r\n\r\nThe trick here is going to be to figure out what thresholds are acceptable.", "I played a bit with cuda fp16 ops - I'm not sure if its even useful to find such a threshold. The native fp16 op (i.e. hgemm) seems to be faster for larger square matrices only. [Here](https://github.com/hma02/cublasHgemm-P100) exists a benchmark of hgemm on a P100. As this is currently the only card where implementing these ops is useful (the  10xx cards support the fp16 ops but only with a major decrease in performance),  the fp16 ops are probably only interesting if future cuda/cudnn releases offer a speed up for computations on smaller matrices.", "You might also want to take a look at the upcoming [Volta architecture](https://devblogs.nvidia.com/parallelforall/inside-volta/) that will supports mixed fp16/fp32 computation and should solve the problem. ", "Meanwhile caffe2 [adds 16 bit floating point](https://caffe2.ai/blog/2017/05/10/caffe2-adds-FP16-training-support.html) training support  in collaboration with NVIDIA.", "This is being worked on.  Assigning to myself so I can update this as we go.  ", "Please keep us posted <3", "Deffinetly we need this. My test on P100 shows great speed improvment in caffe fp16 for my model. Tensorflow now is still behinds for p100.", "@reedwm Can you give some update and info on FP16? ", "We're currently working on adding float16 ops to more TensorFlow ops, such as `tf.nn.fused_batch_norm`, as well as increasing the performance when using float16 ops. Although float16 ops use float32 internally for computations to avoid numerical precision issues, using float16 ops will result in increased performance over their float32 counterparts when run on P100 GPUs due to the reduced memory traffic.  Additionally, we plan on adding float16 support to [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks). \r\n\r\nI'll keep this bug updated as more progress is made.", "@reedwm Does it mean that TensorFlow does not plan to implement native FP16 (half-precision) FMA Cuda ops (opcode HFMA2) -- [https://devblogs.nvidia.com/parallelforall/mixed-precision-programming-cuda-8/](url) ? For inference there could be some applications which could have a significant speedup (theoretically 2x increase in FLOPS) and it is acceptable for these applications to have reduced precision?", "News say that Intel will also add native  fp16 support for next Intel Xeon Phi processor (Knights Mill is commonly suggested) this year. There are few research that shows that in many cases fp16 is enouph for deep learning. Maybe not in all, but in many. Mb will be beter to allow the user make choice what presision is enouph? For example for my RL task I definitly know that fp16 presision  is  enough. And Cafee training acuracy for fp32 and fp16 is almost the same with much better speed for my task. ", "NVidia reported some promising results in training neural networks with native half-float  \r\nhttp://on-demand.gputechconf.com/gtc/2017/presentation/s7218-boris-gainsberg-training-deep-networks-with-half-precision-float.pdf\r\n\r\nIE, apparently resnet-50 training works out of the box in native half-float ops with no decrease in accuracy", "@agupta74  Currently, float16 matmuls do compute in float16 if the environmental variable [TF_FP16_MATMUL_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/matmul_autotune.cc#L44) is set to \"False\". We plan on adding a similar environmental variable to convolutions. \r\n\r\nWe are currently discussing whether to expose this as an argument, e.g. adding a `gpu_compute_type` argument to `tf.matmul`. Adding such an parameter forces us to keep it for a very long time due to our [backwards comparability guarantees](https://www.tensorflow.org/programmers_guide/version_semantics). This parameter would only apply to certain devices (GPUs with fp16 support) and will be unnecessary with Volta GPUs ([tensor cores](https://devblogs.nvidia.com/parallelforall/inside-volta/) will always be used on Volta devices) so we are still discussing whether the option is worth exposing without environmental variables.\r\n\r\n@yaroslavvb Once we add float16 support to tf_cnn_benchmarks, we plan on reporting our training results.", "I am trying to build a seq2seq model using fp16.\r\nUsing the same network architecture, fp32 model trains fine, but the model using fp16 suffers from CUDA_ERROR_OUT_OF_MEMORY.\r\nWhy is lower precision holds up larger memory? Is there any solution for this problem?\r\n\r\nI am currently using python3, tf 1.0, cuda 8.0, cudnn 5.1", "@Sanghoon94  Please open a different GitHub issue.  Without code you are asking for a guess that will not end up being very useful.  No guarantee someone will look at it but sharing the code allows for a better conversation.  You might also want to upgrade to TF 1.2.1 or TF 1.3 just to be more current as if anyone on the team tests the example you provide, they are going to use a much newer binary.  TF 1.0 is essentially 6 months old.  ", "Hello @reedwm. Just wanted to know what the current status as of 1.3.1 ", "I don't think much has changed in 1.3.1. But on the master branch, the status is the following.\r\n\r\n[tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) has support for fp16 by using the flag fp16. It currently does an unnecessary costly cast [here](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/convnet_builder.py#L445), which I will remove once I verify doing so does not affect convergence. The cast is unnecessary since `tf.nn.fused_batch_norm` now supports fp16 on the GPU (and soon, on the CPU once #13388 is submitted).\r\n\r\nBy default, fp16 convolutions still do internal computations in fp32 (on pre-Volta hardware), but they can do fp16 internal computations by setting the newly added environmental variable [TF_FP16_CONV_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2136) to 0. As I stated before, fp16 matmuls also do compute in fp32, unless [TF_FP16_MATMUL_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/matmul_autotune.cc#L44) is set to 0.", "@reedwm what about INT8? Any plans for its support?", "@petewarden can you comment on int8 support? \r\n\r\nAlso, int8 support should probably be a separate issue.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "+1, any word on INT8?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@reedwm @tfboyd can we close this issue if fp16 support has been added? Thanks!\r\nFeel free to open a separate issue for INT8. ", "Closing since fp16 support has been added to many ops. If a particular op does not support fp16 but should, another issue can be filed. We also plan making mixed-precision training easier in the future by improving the API.", "I converted all operations in graph to DT_HALF, run Google Cloud with Nvidia Tesla T4 and model with DT_HALF is slower in about 8-9 times than DT_FLOAT.\r\n\r\nIs this normal behaviour? "]}, {"number": 4588, "title": "dynamic_rnn broken on master?", "body": "The following code works on TF 0.10 but fails on `master`:\n\n```\nimport tensorflow as tf\ni = tf.placeholder(tf.float32, shape=[1, None, 20])\ncell = tf.nn.rnn_cell.GRUCell(30)\no = tf.nn.dynamic_rnn(cell, i, dtype=tf.float32)\n```\n\nThe autogenerated documentation for `master` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md) does not mention anything which could explain this.\n\nThe failure is the following exception:\n\n```\nValueError: Time steps is not the same for all the elements in the input in a batch.\n```\n\nraised on line `915` of `rnn.py`\n\nI could not find any issues in the issue tracker that address this.\n", "comments": ["This bug was introduced in [this commit](https://github.com/tensorflow/tensorflow/commit/bdb6e00b08b5023825eb9acd0e4e4fd1cbc6d01e).\n\nI think it can be patched by changing the lines:\n\n``` python\ngot_time_steps = shape[0]\ngot_batch_size = shape[1]\n```\n\nto \n\n``` python\ngot_time_steps = shape[0].value\ngot_batch_size = shape[1].value\n```\n", "Also, usability note: `dynamic_rnn` should check in advance that the provided `initial_state` has a shape. If it does not then it crashes deep down inside `dynamic_rnn` and it's really hard to understand why. The error it gives ends up being something like `as_list() is undefined on unknown TensorShape`, which gives no indication as to _which_ TensorShape is unknown.\n", "I can't replicate your error.  Are you using the most recent version of master?  Have you tried downloading a nightly?\n\nThese exact lines do not replicate the error for me:\n\n```\nimport tensorflow as tf\ni = tf.placeholder(tf.float32, shape=[1, None, 20])\ncell = tf.nn.rnn_cell.GRUCell(30)\no = tf.nn.dynamic_rnn(cell, i, dtype=tf.float32)\n```\n", "I was using the most recent version of master at the time. I am building the current `master` from a clean slate and will see if I can still reproduce the error.\n", "I have tested with current `master` and have confirmed the issue still exists.\n\n`tf.__version__` returns `0.10.0`; `tf.__git_version__` returns `v0.10.0rc0-2191-gf38886b`; running `git log` in the TF repo I just built and installed gives the the latest commit of `f38886ba307314cc0ae5b15279f5693f39c92f21` (which confirms that the `tf.__git_version__` is correct).\n", "Please let me know what else I can do to be helpful. This does not seem like it should be related to the platform because `rnn.py` is platform-independent; in addition, looking at the code the way it is on `master` confirms that this issue _should_ happen (see my first comment for the code responsible).\n", "I could reproduce the issue as well. However, this is a sneaky one, as the behaviour differs between Python versions.\n\nApparently, when `const_{time|batch}` is `None`(, and the `Dimension`-typed `got_{time|batch}` is not),\n1. Python 3.4.2 calls `Dimension.__eq__`, and negates the result\n2. Python 3.5.2 calls `Dimension.__ne__`\n\nBoth return `None`, but\n1. in the first case, the `if` then becomes `if not None`, so `if True`\n2. in the second, it becomes `if None`, i.e. `if False`\n\nI propose fixing the issue either as @gibiansky suggested, or just call `as_list()` on `shape` so that we have an `int`--`int` comparison.\n", "@gunan this is a hole in our jenkins testing infrastructure.\n", "@DavidNemeskey thanks for getting to the core of the issue.  If either you or @gibiansky wants to submit a PR I'd happily review.  if you dont have bandwidth i can push a change within the day.\n", "@mrry is this a directionality bug the TensorShape comparison code?\n", "@ebrevdo `TensorShape.__eq__()` and `TensorShape.__ne__()` probably don't have the behavior you'd expect when some of the dimensions are unknown. (In particular, `(None, x) == (None, x)` and `(None, x) != (None, x)` both return `None` because the value is unknown.)\n\nSo this library code probably shouldn't be using `==` or `!=` to compare shapes. Use `merge_with()`, `assert_compatible_with()` etc. instead.\n", "Do we have unit tests for this?\nI can investigate the reason why the tests did not catch this.\n", "I think we test with Python 3.5 only?\n\nOn Sep 30, 2016 9:58 AM, \"gunan\" notifications@github.com wrote:\n\n> Do we have unit tests for this?\n> I can investigate the reason why the tests did not catch this.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4588#issuecomment-250796933,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimwfNtnH9S3V1GRAl-3XcR6otbwrKks5qvT_JgaJpZM4KG5Ob\n> .\n", "Which test would this be?\n\nOn Fri, Sep 30, 2016 at 1:33 PM, ebrevdo notifications@github.com wrote:\n\n> I think we test with Python 3.5 only?\n> \n> On Sep 30, 2016 9:58 AM, \"gunan\" notifications@github.com wrote:\n> \n> > Do we have unit tests for this?\n> > I can investigate the reason why the tests did not catch this.\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> > 4588#issuecomment-250796933>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/ABtimwfNtnH9S3V1GRAl-\n> > 3XcR6otbwrKks5qvT_JgaJpZM4KG5Ob>\n> > \n> > .\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4588#issuecomment-250845206,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOSXR7-dTtwbj7A2-9dGblB-rYd8sks5qvXH_gaJpZM4KG5Ob\n> .\n", "Any progress on this? It's marked as 'awaiting response' but I'm not sure who this is waiting on.\n\nI see this issue on Python 3.4 I believe.\n", "I think the we had multiple conversations going on here.\n1- Looks like we are open for contributions here, if you would like to create a PR to fix the issue, @ebrevdo agreed to review the fix.\n\n2- It looks like our testing does not catch this issue. I was trying to see what we have for rnns as unit tests. We obviously need to expand them to catch this problem, and fix the issue. The fix can be packaged with an extension to the unit test for RNN.\n", "I'll see if I can get a fix + unit test in.\n\nOn Wed, Oct 5, 2016 at 1:57 PM, gunan notifications@github.com wrote:\n\n> I think the we had multiple conversations going on here.\n> 1- Looks like we are open for contributions here, if you would like to\n> create a PR to fix the issue, @ebrevdo https://github.com/ebrevdo\n> agreed to review the fix.\n> \n> 2- It looks like our testing does not catch this issue. I was trying to\n> see what we have for rnns as unit tests. We obviously need to expand them\n> to catch this problem, and fix the issue. The fix can be packaged with an\n> extension to the unit test for RNN.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4588#issuecomment-251797143,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim7pkBn0YsnDJPEzEap545fe9qPpyks5qxA83gaJpZM4KG5Ob\n> .\n", "hello, please i run this code : \r\n\r\nfrom sparkdl import DeepImagePredictor\r\nimage_df = ImageSchema.readImages(\"flower_photos/sample/\")\r\n\r\npredictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\", decodePredictions=True, topK=10)\r\npredictions_df = predictor.transform(image_df)\r\n\r\n\r\nand i get this error : \r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-d7bde844670d> in <module>()\r\n      4 \r\n      5 predictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\", decodePredictions=True, topK=10)\r\n----> 6 predictions_df = predictor.transform(image_df)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py in transform(self, dataset, params)\r\n    171                 return self.copy(params)._transform(dataset)\r\n    172             else:\r\n--> 173                 return self._transform(dataset)\r\n    174         else:\r\n    175             raise ValueError(\"Params must be a param map but got %s.\" % type(params))\r\n\r\n/tmp/spark-ad4dff5c-2eda-49cc-b5a8-b81ede3fac4b/userFiles-d4fb97fa-84ed-4aaf-8036-d6d38363201e/databricks_spark-deep-learning-1.0.0-spark2.3-s_2.11.jar/sparkdl/transformers/named_image.py in _transform(self, dataset)\r\n     94             modelName=self.getModelName(),\r\n     95             featurize=False)\r\n---> 96         transformed = transformer.transform(dataset)\r\n     97         if self.getOrDefault(self.decodePredictions):\r\n     98             return self._decodeOutputAsPredictions(transformed)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py in transform(self, dataset, params)\r\n    171                 return self.copy(params)._transform(dataset)\r\n    172             else:\r\n--> 173                 return self._transform(dataset)\r\n    174         else:\r\n    175             raise ValueError(\"Params must be a param map but got %s.\" % type(params))\r\n\r\n/tmp/spark-ad4dff5c-2eda-49cc-b5a8-b81ede3fac4b/userFiles-d4fb97fa-84ed-4aaf-8036-d6d38363201e/databricks_spark-deep-learning-1.0.0-spark2.3-s_2.11.jar/sparkdl/transformers/named_image.py in _transform(self, dataset)\r\n    327             outputMode=modelGraphSpec[\"outputMode\"])\r\n    328         resizeUdf = createResizeImageUDF(modelGraphSpec[\"inputTensorSize\"])\r\n--> 329         result = tfTransformer.transform(dataset.withColumn(resizedCol, resizeUdf(inputCol)))\r\n    330         return result.drop(resizedCol)\r\n    331 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py in transform(self, dataset, params)\r\n    171                 return self.copy(params)._transform(dataset)\r\n    172             else:\r\n--> 173                 return self._transform(dataset)\r\n    174         else:\r\n    175             raise ValueError(\"Params must be a param map but got %s.\" % type(params))\r\n\r\n/tmp/spark-ad4dff5c-2eda-49cc-b5a8-b81ede3fac4b/userFiles-d4fb97fa-84ed-4aaf-8036-d6d38363201e/databricks_spark-deep-learning-1.0.0-spark2.3-s_2.11.jar/sparkdl/transformers/tf_image.py in _transform(self, dataset)\r\n    145                                  \"width\": \"__sdl_image_width\",\r\n    146                                  \"num_channels\": \"__sdl_image_nchannels\",\r\n--> 147                                  \"image_buffer\": \"__sdl_image_data\"})\r\n    148                 .drop(\"__sdl_image_height\", \"__sdl_image_width\", \"__sdl_image_nchannels\",\r\n    149                       \"__sdl_image_data\")\r\n\r\n/tmp/spark-ad4dff5c-2eda-49cc-b5a8-b81ede3fac4b/userFiles-d4fb97fa-84ed-4aaf-8036-d6d38363201e/databricks_tensorframes-0.3.0-s_2.11.jar/tensorframes/core.py in map_rows(fetches, dframe, feed_dict, initial_variables)\r\n    262     if isinstance(dframe, pd.DataFrame):\r\n    263         return _map_pd(fetches, dframe, feed_dict, block=False, trim=None, initial_variables=initial_variables)\r\n--> 264     return _map(fetches, dframe, feed_dict, block=False, trim=None, initial_variables=initial_variables)\r\n    265 \r\n    266 def map_blocks(fetches, dframe, feed_dict=None, trim=False, initial_variables=_initial_variables_default):\r\n\r\n/tmp/spark-ad4dff5c-2eda-49cc-b5a8-b81ede3fac4b/userFiles-d4fb97fa-84ed-4aaf-8036-d6d38363201e/databricks_tensorframes-0.3.0-s_2.11.jar/tensorframes/core.py in _map(fetches, dframe, feed_dict, block, trim, initial_variables)\r\n    150         builder = _java_api().map_rows(dframe._jdf)\r\n    151     _add_graph(graph, builder)\r\n--> 152     ph_names = _add_shapes(graph, builder, fetches)\r\n    153     _add_inputs(builder, feed_dict, ph_names)\r\n    154     jdf = builder.buildDF()\r\n\r\n/tmp/spark-ad4dff5c-2eda-49cc-b5a8-b81ede3fac4b/userFiles-d4fb97fa-84ed-4aaf-8036-d6d38363201e/databricks_tensorframes-0.3.0-s_2.11.jar/tensorframes/core.py in _add_shapes(graph, builder, fetches)\r\n     83             t = graph.get_tensor_by_name(op_name + \":0\")\r\n     84             ph_names.append(t.name)\r\n---> 85             ph_shapes.append(_get_shape(t))\r\n     86     logger.info(\"fetches: %s %s\", str(names), str(shapes))\r\n     87     logger.info(\"inputs: %s %s\", str(ph_names), str(ph_shapes))\r\n\r\n/tmp/spark-ad4dff5c-2eda-49cc-b5a8-b81ede3fac4b/userFiles-d4fb97fa-84ed-4aaf-8036-d6d38363201e/databricks_tensorframes-0.3.0-s_2.11.jar/tensorframes/core.py in _get_shape(node)\r\n     36 \r\n     37 def _get_shape(node):\r\n---> 38     l = node.get_shape().as_list()\r\n     39     return [-1 if x is None else x for x in l]\r\n     40 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py in as_list(self)\r\n    898     \"\"\"\r\n    899     if self._dims is None:\r\n--> 900       raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n    901     return [dim.value for dim in self._dims]\r\n    902 \r\n\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\n\r\nAny help please ! Thanks ", "@ikrambennasrbennasr did you find a solution? im getting the same error :(", "Hi @dimagoldin  in fact I did not understand the main origin of this error. But what I do is create another environment and isntaller tensorflow with python3.6. This error did not appear to me."]}, {"number": 4587, "title": "Branch 134301156", "body": "", "comments": ["@jhseu, thanks for your PR! By analyzing the annotation information on this pull request, we identified @Stibbons, @tensorflower-gardener and @dsmilkov to be potential reviewers\n", "@tensorflow-jenkins test this please\n", "tensorflow/python/kernel_tests:io_ops_test seems to be flaky lately. If that is our only breakage, we can go ahead and merge this in.\n", "I have a fix for that test internally. I'll abandon this merge and create a new pull request.\n"]}, {"number": 4586, "title": "tensorflow/examples/android", "body": "hi all\n\nThis example seems like a new version and i got some errors when i want to build the APK on android studio.\nDose any have same issue?\n\nthanks for your time\n\n<img width=\"1280\" alt=\"2016-09-27 1 16 48\" src=\"https://cloud.githubusercontent.com/assets/12976847/18844583/6ab64f50-8450-11e6-8099-b7af5ca8ef20.png\">\n", "comments": ["The Java side of the Android interface with TensorFlow was moved to `tensorflow/contrib/android/java`. If you add that as an additional srcs directory it should work for you.\n\n[build.gradle](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/build.gradle) should probably be updated to include this automatically.\n", "It works, thanks for your helping.\n", "https://github.com/tensorflow/tensorflow/commit/7f648692df34679d299093391bac45ed789d09ba adds contrib/android/java to the src dir list, so this should now work automatically.\n"]}, {"number": 4585, "title": "Doc: update README iOS instructions [skip ci]", "body": "Automake seems to be required to build on MacOS 10.11.\nAdd instruction to the dedicated README to install this dependency.\n", "comments": ["Can one of the admins verify this patch?\n", "@arnlen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @martinwicke and @petewarden to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Pete, mind taking a quick look?\n", "Jenkins, test this please.\n", "Jenkins, retest this please\n"]}, {"number": 4584, "title": "A strange problem about gfile", "body": "<img width=\"848\" alt=\"2016-09-26 20 46 30\" src=\"https://cloud.githubusercontent.com/assets/11813885/18834822/9ee71bea-842a-11e6-95d3-2af68449343d.png\">\n\nI use Tensorflow of gpu's version and Python3 on Linux.As you can see, I read a file by Gfile with mode \"r\",but it seems to return a byte object rather than a string.Can anyone explain this issue?\n", "comments": ["gfile currently returns a byte stream. If you want to convert it into a unicode (native) python string you need to know its encoding. If it is utf-8 you can do `str(foo.read(), 'utf-8')`.\n", "@aselle Thank you,I understand the issue now.\n"]}, {"number": 4583, "title": "Bazel can't fetch numeric-1.2.6.min.js because numericjs.com has expired", "body": "INFO: Downloading from http://www.numericjs.com/lib/numeric-1.2.6.min.js: 0B\n\n---\n\nWhen i opened it on my broswer, it prompted that numericjs.com has expired\n", "comments": ["same problem here\n", "Hey @dsmilkov where can we download this from GitHub? I found https://github.com/sloisel/numeric/blob/master/src/numeric.js but it doesn't appear to be the same code that we have checked in internally, even though the versions are the same. \n\nOr better yet, is there another library we could use?\n\ncc: @danmane  \n", "https://github.com/sloisel/numeric/blob/master/src/numeric.js is just a portion of the library (uncompiled), and the author doesn't store the compiled/bundled one on github.\n\nWe can link to:\nhttps://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js\n\nCloudfare CDN should be pretty stable. Let me know your thoughts\n", "Submitted an internal change where numericjs gets pulled from cloudflare cdn. The internal change should show up in github within a day.\n\nThanks @sharkdtu and @natlachaman  for bringing this issue to our attention!\n"]}, {"number": 4582, "title": "Tensorflow model as executable file", "body": "I want to share my model as an executable file (without sharing my source code ) . I am using Tensorflow on Ubutun 16.04.1 LTS .I tried pyinstaller and cx_freeze but it give me alot of errors\n\n**pyinstaller error** \n![jvssz](https://cloud.githubusercontent.com/assets/15180702/18830940/609bb61c-83e3-11e6-939d-3d7b8a7cf7a0.png)\n\n**cx_freeze error** \n![ikzr5](https://cloud.githubusercontent.com/assets/15180702/18830975/8c4a345a-83e3-11e6-854e-649b1a269d94.png)\n\nI installed pyqt4 and I got this error \n` ` `\nError loading Python lib '/root/build/test/libpython2.7.so.1.0': /root/build/test/libpython2.7.so.1.0: cannot open shared object file: No such file or directory \n` ` ` \n", "comments": ["Nuitka?\n", "NO , I use pyinstaller to compile the code and convert it to executable \n", "That sounds like a problem in Anaconda/pyfreeze -- the error in your stack\ntrace is in QT framework which is not part of tensorflow\n\nOn Tue, Sep 27, 2016 at 6:32 AM, moh3th1 notifications@github.com wrote:\n\n> NO , I use pyinstaller to compile the code and convert it to executable\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4582#issuecomment-249865541,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHAMghMYpUBmMbUYT020pToqxu69Rks5quRrXgaJpZM4KGXFE\n> .\n", "Agreed, this doesn't seem like a TensorFlow-specific issue, so I'm going to close it.\n", "@moh3th1 Have you solved the problem about the pyinstaller with tensorflow"]}]