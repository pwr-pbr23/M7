[{"number": 46359, "title": "tflite custom Bazel build for wasm target ", "body": "I am using tflite in  c++ cross platform library. I plan to build  them with emscripten for wasm. I wish to use the shared libraries (emscripten).  I am new to bazel but i didnt find any custom build for emscripten target in  the repository. \r\nI can see that xnnpack helps us with building  optimized kernels for  all targets(including wasm). \r\nMedia pipe also claims to use wasm and it supports both tensorflow and tflite but i dont find any emscripten build in their repository as well.\r\n\r\nI need flatbuffer &  inference portion of tflite in wasm\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@dsmilkov\r\nI was  going through your old ported repo on https://github.com/dsmilkov/tflite-wasm.\r\nI was curious to know if its possible to use emscripten with bazel for current tflite/tensorflow c++ source code to  convert them into emscripten static library(.a).   \r\n\r\n", "@jdduke might have some idea on this. Could you take a look?", "@jdduke\r\nThe CmakeLists  for tflite doesn't  support emscripten(for xnnpack. I saw the xnnpack and they dont seem to have wasm_Simd files in their build system). I am building the tflite with emscripten. I am able to build all the required static libraries of libtensorflowlite. I need a delegate to launch the interpreter. But emscripten build also have source code dependencies on xnnpack_delegates.h/cc .\r\nSo can you please guide me on how to build xnnpack and xnnpack_delegate.cc so that i could register the xnnpack as delegate using low level delegate api(or any other means) listed in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack\r\n\r\n\r\nI want to build  these files \r\n```\r\nif(TFLITE_ENABLE_XNNPACK)\r\n  find_package(xnnpack REQUIRED)\r\n  populate_tflite_source_vars(\"delegates/xnnpack\"\r\n    TFLITE_DELEGATES_XNNPACK_SRCS\r\n    FILTER \".*(_test|_tester)\\\\.(cc|h)\"\r\n  )\r\n  list(APPEND TFLITE_TARGET_DEPENDENCIES\r\n    XNNPACK\r\n  )\r\n  list(APPEND TFLITE_TARGET_PUBLIC_OPTIONS \"-DTFLITE_BUILD_WITH_XNNPACK_DELEGATE\")\r\nendif()\r\n```\r\nIf I use  bazel for xxnpack then should I be able to get libxnnpack.a(I dont want js/wasm)\r\nThen I would need to link xnnpack_delegate.o with libxnnpack.a . ", "@annxingyuan may be able to offer some guidance. ", "I m forcing the xnnpack_delegate.cc  commenting on find_package(xnnpack)\r\nits failing on \"fp16.h' file not found\". I see that this is half float library. Is this supported  like numpy(not at cpu/compiler level)  \r\nIf I am able to build xnnpack_delegate.o then I can use emcc to link it  with libxnnpack.a(from bazel) ?? \r\nBuilding libxnnpack.a for emscripten is not easy as well. \r\nI dont understand bazel so How I can build the bazel for webassembly or WebAssembly  simd. The config is not defined in .rc file. \r\n`bazel build -c opt --config emscripten_wasm : wasm_ukernels`\r\n`bazel build -c opt --config emscripten_wasmsimd: wasm_ukernels`\r\nAdded  this to rc file \r\nbuild:emscripten_wasm --crosstool_top=//toolchain:emscripten --cpu=wasm\r\n\r\ncould you please point to the tool chain http archive & rule files. It seems tfjs does similar for wasm backend.\r\n\r\n@annxingyuan\r\nThe kernels built for emscripten will surely not work as those cpu kernels have support for multi threaded blas,eigen and other neon optimization?\r\nI wanted to use tfjs-wasm as  a static library  where  I can load Tflite files with help of flatbuffer(model stored in .data file)and run my inference without writing any javascript code\r\n\r\n"]}, {"number": 46356, "title": "Duplicate classes (com.google.flatbuffers) when flatbuffers is already in project", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android/macOS 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: TF Lite \r\n- Python version: 2.7.3\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI tried to include ARCore and Sceneform in the same project to show a 3D model on detected objects. But I can't compile, because Sceneform already includes flatbuffers.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- Download TF Lite example from https://github.com/tensorflow/examples\r\n- Add Sceneform to project\r\n- Duplicate class errors related to com.google.flatbuffers.\r\n\r\n\r\n**Any other info / logs**\r\nI found this possibly related issue: https://github.com/tensorflow/tensorflow/issues/33472\r\n\r\nIs it now possible on Android to use an external dependency on flatbuffers, or another project like Sceneform that already includes flatbuffers?\r\n", "comments": ["@alex-imperfect \r\nCould you please specify the tf version.", "Sorry, how do I get the tensorflow version from the Android Studio project?\r\n\r\nI got the example from here. I don't see a tf version specified in the readme or in the code.\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android", "The tflite version should be somewhere in your code, maybe in build.gradlle or other build system's config file that you use.\r\n\r\nIf you use a new enough version of tflite, I think this problem should not happen. Since most of the symbols are hidden by https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/tflite_version_script.lds", "Hi Sir @thaink @Saduf2019 , i already use 'org.tensorflow:tensorflow-lite:2.5.0' but still get duplicate error as below:\r\n\"\r\nDuplicate class com.google.flatbuffers.Utf8Safe found in modules jetified-flatbuffers-java-1.12.0 (com.google.flatbuffers:flatbuffers-java:1.12.0) and jetified-sceneform-base-1.17.1-runtime (com.google.ar.sceneform:sceneform-base:1.17.1)\r\n\"", "@RyoKusnadi I don't see how is it related to TFLite in your error message?", "\r\nHi sir @thaink  ,I'm using some dependencies from tensorflow in my project but when i used these two dependencies 'org.tensorflow:tensorflow-lite-metadata:0.1.0' and 'com.google.ar.sceneform.ux:sceneform-ux:$1.17.1', it shows the conflict of duplicate class. The tensorflow-lite-metadata dependency generated the com.google.flatbuffers:flatbuffers-java:1.12.0 class in the gradle and that makes conflict with the sceneform dependency which also generated the flatbuffers class. I already tried to exclude the flatbuffers class in tensorflow-lite-metadata but it just make the model not working at all.", "@lu-wang-g Could you take a look at this issue?", "Hello Team. \r\n@lu-wang-g @thaink @ymodak \r\n\r\nThis issues is still valid, please help me so to proceeded. \r\n\r\nTensorflow version --> 2.6.1\r\n\r\nDescription - \r\nAndroid framework from tflite. \r\nLink - https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\nClone the repo. \r\nIn the app level gradle file (https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/build.gradle)\r\n\r\nIntroduce -->  implementation \"com.google.ar.sceneform.ux:sceneform-ux:1.6.0\"\r\n\r\nWe get an error as below - \r\n`Duplicate class com.google.flatbuffers.Constants found in modules jetified-flatbuffers-java-1.12.0.jar (com.google.flatbuffers:flatbuffers-java:1.12.0) and jetified-sceneform-base-1.6.0-runtime.jar (com.google.ar.sceneform:sceneform-base:1.6.0)\r\nDuplicate class com.google.flatbuffers.FlatBufferBuilder found in modules jetified-flatbuffers-java-1.12.0.jar (com.google.flatbuffers:flatbuffers-java:1.12.0) and jetified-sceneform-base-1.6.0-runtime.jar (com.google.ar.sceneform:sceneform-base:1.6.0)\r\nDuplicate class com.google.flatbuffers.FlatBufferBuilder$ByteBufferFactory found in modules jetified-flatbuffers-java-1.12.0.jar (com.google.flatbuffers:flatbuffers-java:1.12.0) and jetified-sceneform-base-1.6.0-runtime.jar (com.google.ar.sceneform:sceneform-base:1.6.0)\r\nDuplicate class com.google.flatbuffers.FlatBufferBuilder$HeapByteBufferFactory found in modules jetified-flatbuffers-java-1.12.0.jar (com.google.flatbuffers:flatbuffers-java:1.12.0) and jetified-sceneform-base-1.6.0-runtime.jar (com.google.ar.sceneform:sceneform-base:1.6.0)\r\nDuplicate class com.google.flatbuffers.Struct found in modules jetified-flatbuffers-java-1.12.0.jar (com.google.flatbuffers:flatbuffers-java:1.12.0) and jetified-sceneform-base-1.6.0-runtime.jar (com.google.ar.sceneform:sceneform-base:1.6.0)\r\nDuplicate class com.google.flatbuffers.Table found in modules jetified-flatbuffers-java-1.12.0.jar (com.google.flatbuffers:flatbuffers-java:1.12.0) and jetified-sceneform-base-1.6.0-runtime.jar (com.google.ar.sceneform:sceneform-base:1.6.0)\r\n\r\nGo to the documentation to learn how to Fix dependency resolution errors.`\r\n\r\n\r\n\r\nAlso please look at this gradle file -\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/lib_interpreter/build.gradle\r\n\r\nProbably, flatbuffers is used by (implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT')\r\n\r\n\r\nSo overall how Sceneform and tensorflowlite can coexist?\r\n\r\nNeed your help to proceed further. \r\n\r\n-----------------------------------------------------------------------------------------------------\r\nOne of things I explored wrongly(NOT related to this bug, rather just might work as hint)-\r\n\r\nPlease visit this link. \r\nhttps://github.com/objectbox/objectbox-java/issues/894 \r\n\r\n\r\n\r\n\r\n----------------------------------------------------------\r\nAlso I have attached the dependencies tree for gradle where you can see the compiletime classpath. \r\n[dependencies tree.txt](https://github.com/tensorflow/tensorflow/files/7537932/dependencies.tree.txt)\r\n", "I have the same issue when include tensor flow lite with arcore in same project so any help", "any updates regarding this issue", "I'm having this issue too", "Sorry that I just noticed this issue. Seems like it's due to that Sceneform repackaged FlatBuffers, which conflicts with the official Flatbuffer Java time. I'll loop in the Sceneform team to take a look at this issue."]}, {"number": 46342, "title": "Minor Bug in estimator/tensorflow_estimator/python/estimator/canned/dnn.py", "body": "**System information**\r\n- Model Example : https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier\r\n- OS Platform and Distribution : MacOS 11.1\r\n- TensorFlow installed from : Binary (pip)\r\n- TensorFlow version : 2.4.0\r\n- Python version: 3.8\r\n\r\n**Current Behavior : Error**\r\n\r\nI was training a DNN Classifier recently and customizing the model as per the example mentioned in the link above.\r\n\r\nHere is the code snippet of my model :\r\n```\r\n# Build a DNN with 2 hidden layers with 15 and 10 hidden nodes each.\r\nclassifier = tf.estimator.DNNClassifier(\r\n    \r\n    feature_columns=feature_columns, # Specify the list of feature columns\r\n    \r\n    hidden_units=[15, 10], # Specify the list of hidden units\r\n    \r\n    n_classes=2, # Specify the number of classes\r\n    \r\n    # Specify the optimizer\r\n    optimizer = tf.compat.v1.train.ProximalAdagradOptimizer(\r\n      learning_rate=0.02,\r\n      l2_regularization_strength=0.005\r\n    ),\r\n    \r\n    # Specify the dropout\r\n    dropout = 0.2\r\n)\r\n```\r\n\r\nHowever, while training the defined model, I get the following error : \r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-17-dcc1423bd4e7> in <module>\r\n      1 # Train the Model.\r\n----> 2 classifier.train( input_fn=train_input_fn )\r\n\r\n/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    347 \r\n    348       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 349       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    350       logging.info('Loss for final step: %s.', loss)\r\n    351       return self\r\n\r\n/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1173       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1174     else:\r\n-> 1175       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1176 \r\n   1177   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1201           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\r\n   1202       worker_hooks.extend(input_hooks)\r\n-> 1203       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\r\n   1204                                            self.config)\r\n   1205       global_step_tensor = tf.compat.v1.train.get_global_step(g)\r\n\r\n/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1161 \r\n   1162     logging.info('Calling model_fn.')\r\n-> 1163     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1164     logging.info('Done calling model_fn.')\r\n   1165 \r\n\r\n/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py in _model_fn(features, labels, mode, config)\r\n    748     def _model_fn(features, labels, mode, config):\r\n    749       \"\"\"Call the defined shared dnn_model_fn_v2.\"\"\"\r\n--> 750       return dnn_model_fn_v2(\r\n    751           features=features,\r\n    752           labels=labels,\r\n\r\n/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py in dnn_model_fn_v2(***failed resolving arguments***)\r\n    572   # relies on global step as step counter.\r\n    573   if mode == ModeKeys.TRAIN:\r\n--> 574     optimizer = optimizers.get_optimizer_instance_v2(optimizer)\r\n    575     optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()\r\n    576 \r\n\r\n/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/optimizers.py in get_optimizer_instance_v2(opt, learning_rate)\r\n    141     opt = opt()\r\n    142   if not isinstance(opt, optimizer_v2.OptimizerV2):\r\n--> 143     raise ValueError(\r\n    144         'The given object is not a tf.keras.optimizers.Optimizer instance.'\r\n    145         ' Given: {}'.format(opt))\r\n\r\nValueError: The given object is not a tf.keras.optimizers.Optimizer instance. Given: <tensorflow.python.training.proximal_adagrad.ProximalAdagradOptimizer object at 0x13b9bea90>\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe model is demanding the optimizer to be an instance of the Optimizer of the Keras API. This should not be the case, this is perfectly fine code.\r\n\r\nSo I did some digging inside the code in **dnn.py** and found a minor issue here :\r\n\r\nhttps://github.com/tensorflow/estimator/blob/49efa0187fcfefa1660f0aae745a4c3c0793f752/tensorflow_estimator/python/estimator/canned/dnn.py#L574\r\n\r\nCode There (Line 574) :\r\n```\r\nif mode == ModeKeys.TRAIN:\r\n    optimizer = optimizers.get_optimizer_instance_v2(optimizer)\r\n    optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()\r\n```\r\n\r\nHere we can clearly see, that we are calling the `get_optimizer_instance_v2` without checking the type of the optimizer. If our optimizer is of type **tf.compat.v1.train.Optimizer**, this call will fail, which is precisely what is happening.\r\n\r\n**Method to reproduce**\r\n\r\nCreate a DNN Model with the `tf.estimator` api and specify optimizer of the type : `tf.compat.v1.train.ProximalAdagradOptimizer`\r\n\r\nPlease verify this issue on your end.\r\n\r\nIf verified, I will generate a pull request fixing this issue.", "comments": ["@Anmol-Sharma,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. \r\n\r\n> If verified, I will generate a pull request fixing this issue.\r\n\r\nAlso, please feel free to submit a PR regarding this. Thanks!", "@amahendrakar You can use the code below as an example to trouble-shoot. It is similar to what I'm doing. I've verified the issue with this script as well.\r\n\r\nJust use the Titanic dataset from kaggle and try to run this on this script.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n# For Tensorflow\r\nimport tensorflow as tf\r\nfrom tensorflow.estimator import LinearRegressor, DNNClassifier\r\n\r\n# For Splitting Training / Testing Data\r\nfrom sklearn.model_selection import train_test_split\r\nimport time\r\n\r\n\r\n# Titanic Dataset from Kaggle\r\ndf = pd.read_csv(\"../../datasets/Titanic.csv\")\r\n\r\n\r\n# Drop the non-useful columns :- PassengerId, Name, Ticket Nr\r\ndf.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace = True)\r\n\r\n\r\n# Check of null-values\r\ndf.isna().sum()\r\n\r\n# Cabin has a lot of null values, so we discard that column\r\ndf.drop(columns = [\"Cabin\"], inplace = True)\r\n\r\n# Group Age by Sex and Age and Set the mean age by sex for the missing values.\r\nres = df.groupby(\"Sex\")[\"Age\"].mean()\r\nidx = res.index\r\nmapping = dict(zip(idx, res))\r\n\r\n# Add data for missing Age\r\ndf.loc[df[\"Age\"].isnull() == True, \"Age\"] = df.loc[df[\"Age\"].isnull() == True, \"Sex\"].map(lambda x : mapping[x])\r\n\r\n# Add data for missing Embarked : Set the Embarked to the most common value\r\ndf.loc[df[\"Embarked\"].isnull() == True, \"Embarked\"] = df[\"Embarked\"].value_counts()[0]\r\n\r\ndf.drop(df[df.Embarked == 644].index.values, inplace = True)\r\n\r\n\r\noutput = df.Survived\r\ndf.drop(columns = [\"Survived\"], inplace = True)\r\n\r\ntrain_x, test_x, train_y, test_y = train_test_split(df, output, test_size=0.1, random_state=int(time.time()))\r\n\r\n\r\nCATEGORICAL_COLUMNS = [\"Pclass\", \"Sex\", \"Embarked\"]\r\nNUMERIC_COLUMNS = ['Age', 'Fare']\r\n\r\n# Create the list of feature columns\r\nfeature_columns = []\r\nfor feature_name in CATEGORICAL_COLUMNS:\r\n    vocabulary = train_x[feature_name].unique()\r\n    # For Categorical column we are specify categorical column with vocab list.\r\n    # There are other types for categorical data as well. Refer to TF documentation.\r\n    col = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\r\n    feature_columns.append(tf.feature_column.indicator_column(col))\r\n\r\n\r\n# For Non categorical data, we define the column type to be numeric_column\r\nfor feature_name in NUMERIC_COLUMNS:\r\n    feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\r\n\r\n# Define an inut function, which will generate the dataset for our model\r\ndef make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\r\n    \r\n    # A helper function to be returned\r\n    def input_function():\r\n        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\r\n        if shuffle:\r\n            ds = ds.shuffle(1000)\r\n        ds = ds.batch(batch_size).repeat(num_epochs)\r\n        return ds\r\n    \r\n    return input_function\r\n\r\ntrain_input_fn = make_input_fn(train_x, train_y, batch_size = 2)\r\neval_input_fn = make_input_fn(test_x, test_y, num_epochs=1, shuffle=False)\r\n\r\n# Build a DNN with 2 hidden layers with 15 and 10 hidden nodes each.\r\nclassifier = tf.estimator.DNNClassifier(\r\n    \r\n    feature_columns=feature_columns, # Specify the list of feature columns\r\n    \r\n    hidden_units=[15, 10], # Specify the list of hidden units\r\n    \r\n    n_classes=2, # Specify the number of classes\r\n    \r\n    # Specify the dropout\r\n    dropout = 0.2\r\n)\r\n\r\n# Train the Model.\r\nclassifier.train( input_fn=train_input_fn )\r\n\r\neval_result = classifier.evaluate(input_fn=eval_input_fn)\r\n\r\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\r\n```\r\n\r\nAlso, I think the PR will have to be issued for the same in the [tensorflow/estimator](https://github.com/tensorflow/estimator) repository, as the core code in dnn.py is present there.\r\nI will submit a PR there for this fix and link it here to this issue.", "@Anmol-Sharma,\r\nI did not face any errors while running the code with TF v2.4. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c9e179b16e30b3ca6d7d1696c3f489f6/46342.ipynb). \r\n\r\nCould you please try running the code in a new virtual environment and check if you are facing the same error. Thanks!", "@amahendrakar \r\n\r\nThat is because in your code, one extra line of optimizer specification is missing. This \ud83d\udc47 \r\n```\r\n# Specify the optimizer\r\n    optimizer = tf.compat.v1.train.ProximalAdagradOptimizer(\r\n      learning_rate=0.02,\r\n      l2_regularization_strength=0.005\r\n    ),\r\n```\r\n\r\nAdd this to your model declaration and then try to train, you will get the error.", "@Anmol-Sharma,\r\nThank you for the update. \r\n\r\n@jvishnuvardhan,\r\nI was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4fe23c2686e381e2858d607749047d23/46342.ipynb). Thanks!", "@amahendrakar \ud83d\udc4d \r\n\r\nI've issued a PR, fixing the issue in the tensorflow/estimator repo. Have a look when you can.", "I found similar issue when use `tf.compat.v1.train.AdagradOptimizer` and multi_head in estimator in TF estimator 2.4\r\n\r\nThe PR https://github.com/tensorflow/estimator/pull/59 not solve my issue.\r\n\r\n\r\n```\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/head/multi_head.py\", line 491, in create_estimator_spec\r\n    train_op = base_head.create_estimator_spec_train_op(\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/head/base_head.py\", line 904, in create_estimator_spec_train_op\r\n    validate_v2_optimizer(optimizer)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/head/base_head.py\", line 667, in validate_v2_optimizer\r\n    raise ValueError(\r\nValueError: The given optimizer is not a tf.keras.optimizers.Optimizer instance. Given: <tensorflow.python.training.adagrad.AdagradOptimizer object at 0x7f4f8d11d070>\r\n```\r\n\r\nSo I follow code in https://github.com/tensorflow/estimator/blob/r1.14/tensorflow_estimator/python/estimator/head/multi_head.py#L427\r\n\r\nChange code in function `create_estimator_spec_train_op` like below:\r\n\r\n```\r\n          if isinstance(optimizer, optimizer_v2.OptimizerV2):\r\n            validate_v2_optimizer(optimizer)\r\n            validate_trainable_variables(trainable_variables)\r\n            # optimizer_v2.get_updates always returns a list, and the first\r\n            # element is the train_op.\r\n            train_op = optimizer.get_updates(\r\n                loss, trainable_variables)[0]\r\n          else:\r\n            train_op = optimizer.minimize(\r\n                regularized_training_loss,\r\n                global_step=training_util.get_global_step())\r\n```", "@Anmol-Sharma Is this still an issue ?\r\nEstimators are not recommended for new code. Estimators  can behave unexpectedly, especially when combined with TF 2 code. Could you please refer to this [link](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) for more details  and let us know if it helps?\r\nThanks!"]}, {"number": 46340, "title": "Issue with generating examples for loaded estimator when running with tf.function", "body": "**System information**\r\n- I've written a code to generate examples for a loaded estimator, based on https://www.tensorflow.org/tutorials/load_data/tfrecord \r\n- OS Platform and Distribution: Linux Ubuntu 18.04.5\r\n- Python version: 3.7.4\r\n- Tensorflow : 2.4.0\r\n\r\n**Describe the current behavior**\r\nI am loading a saved boosted trees estimator using tf.saved_model.load(model_path)\r\nThen, to predict on a new example, I'm using the following function which generate the examples properly to the model: \r\n\r\n      @tf.function(input_signature=[tf.TensorSpec(shape=[84], dtype=tf.float32)])\r\n          def predict(self, feature_vector):\r\n              example = tf.train.Example()\r\n              for i in range(len(self.col_names)):\r\n                  colName = self.col_names[i]\r\n                  value = feature_vector[i]\r\n                  example.features.feature[colName].float_list.value.extend([value])\r\n              predictions = importedModel.signatures[\"predict\"](examples=tf.constant([example.SerializeToString()]))['predictions']\r\n              return predictions\r\n\r\nHowever, when running with the @tf.function - I get the error for the extend function:\r\n     example.features.feature[colName].float_list.value.extend([value])\r\n\r\n    TypeError: <tf.Tensor 'strided_slice:0' shape=() dtype=float32> has type Tensor, but expected one of: int, long, float\r\n\r\n**If I'm running without the tf.function - everything works fine.** \r\n\r\nWhen running with the tf.function()- the \"value\" is a tensor - how can I convert it to a float? Or how can I create the \"examples\" dict to be able to predict ? (I tried also this function specifically): \r\ndef _float_feature(value):\r\n  \"\"\"Returns a float_list from a float / double.\"\"\"\r\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\nfrom https://www.tensorflow.org/tutorials/load_data/tfrecord  \r\n\r\n(I'm using the tf.function to be able to save the class and convert it to tflite..) \r\n\r\n** To reproduce **\r\nThis is a \"light\" example: \r\n\r\n    import tensorflow as tf\r\n    \r\n    class EXAMPLE(tf.Module):\r\n\r\n    # def __init__(self):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\r\n    def _float_feature(self, value):\r\n        \"\"\"Returns a float_list from a float / double.\"\"\"\r\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    regressor = EXAMPLE()\r\n    output = regressor._float_feature(5)\r\n\r\nWhen removing the tf.function line - it works great and with it outputs the error.\r\nAll the online examples use the \"extend\" option to create the examples, but using tf.function won't allow it. Is there an alternative? \r\nThanks you", "comments": ["@AiaHaruv \r\nPlease share indented code for us to replicate the issue faced or if possible share a colab gist with the issue reported.", "This is a code which reproduce the issue: \r\n\r\n    import tensorflow as tf\r\n    \r\n    class EXAMPLE(tf.Module):\r\n    \r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\r\n    def _float_feature(self, value):\r\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n    if name == \"main\":\r\n    \r\n    regressor = EXAMPLE()\r\n    output = regressor._float_feature(5)", "@Saduf2019 \r\nOr alternatively, here is a colab link with the issue: \r\n\r\nhttps://colab.research.google.com/gist/AiaHaruv/fda86e56513d11cc729d5475730b5344/regressor_bug.ipynb\r\n\r\n", "@AiaHaruv \r\nCan you also share the saved model, as i am facing \"OSError: SavedModel file does not exist at: drive/MyDrive/example_estimator//{saved_model.pbtxt|saved_model.pb}\" error while i try to replicate the issue.", "@Saduf2019 \r\nThere is a link to the model : \r\nhttps://drive.google.com/drive/folders/1XXluMC__1cg9VNGPtd5XtLlPacpytwcv?usp=sharing\r\n\r\nUsing this snippet reproduce the same problem in case it still doesn't work: \r\n\r\n    import tensorflow as tf\r\n    \r\n    class EXAMPLE(tf.Module):\r\n    \r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\r\n    def _float_feature(self, value):\r\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n    if name == \"main\":\r\n    \r\n    regressor = EXAMPLE()\r\n    output = regressor._float_feature(5)\r\n", "I ran the code shared with the saved model shared above, but face [this error](https://colab.research.google.com/gist/Saduf2019/4e806e80c1a3de7782da7238c459e125/untitled499.ipynb)", "@Saduf2019 - Thank you for your response. \r\nI edited the notebook so it won't load the model:\r\nhttps://colab.research.google.com/gist/AiaHaruv/fda86e56513d11cc729d5475730b5344/regressor_bug.ipynb\r\n\r\nSince the error is in the example generation and not the prediction itself or in loading the model -it cab be reproduced without the model itself (like the snippet I shared with the _float_feature function.. )\r\n\r\n(Not sure why it didn't loaded the model, maybe drive wasn't mounted ? Or is there some other way for me to share the model?)\r\n\r\nThank you", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/da70426796516a3bc869e50903e6b2ea/untitled503.ipynb)", "Was able to reproduce the issue  in TF-Nightly 2.6. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/a5616c89c42edb42976f76c4a1638417/nighly.ipynb).Thanks!"]}, {"number": 46336, "title": "3D SparseTensor matrix multiplication with 2D Tensor :InvalidArgumentError: Tensor 'a_shape' must have 2 elements [Op:SparseTensorDenseMatMul]", "body": "**System information**\r\n- TensorFlow version 2.4:\r\n\r\nTrying to do a 3D SparseTensor matrix multiplication with 2D Tensor. Here is a toy example:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = np.array([[[1., 0., 2., 0.],\r\n              [3., 0., 0., 4.]]])\r\nb = (np.array([1., 2.])[:,np.newaxis]).T\r\n\r\na_t = tf.constant(a)\r\nb_t = tf.constant(b)\r\n\r\na_s = tf.sparse.from_dense(a_t)\r\n\r\ntf.sparse.sparse_dense_matmul(b_t,a_s)\r\n```\r\nexpected result(1, 1, 4):\r\n`[[[7., 0., 2., 8.]]]`\r\n\r\nbut output some errors actually :\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-30-2ee379f1d3e8> in <module>\r\n----> 1 tf.sparse.sparse_dense_matmul(b_t,a_s)\r\n\r\n/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in sparse_tensor_dense_matmul(sp_a, b, adjoint_a, adjoint_b, name)\r\n   2564       return array_ops.transpose(\r\n   2565           sparse_tensor_dense_matmul(\r\n-> 2566               b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))\r\n   2567 \r\n   2568   else:\r\n\r\n/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in sparse_tensor_dense_matmul(sp_a, b, adjoint_a, adjoint_b, name)\r\n   2577           b=b,\r\n   2578           adjoint_a=adjoint_a,\r\n-> 2579           adjoint_b=adjoint_b)\r\n   2580 \r\n   2581 \r\n\r\n/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_sparse_ops.py in sparse_tensor_dense_mat_mul(a_indices, a_values, a_shape, b, adjoint_a, adjoint_b, name)\r\n   3049       return _result\r\n   3050     except _core._NotOkStatusException as e:\r\n-> 3051       _ops.raise_from_not_ok_status(e, name)\r\n   3052     except _core._FallbackException:\r\n   3053       pass\r\n\r\n/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6860   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6861   # pylint: disable=protected-access\r\n-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6863   # pylint: enable=protected-access\r\n   6864 \r\n\r\n/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Tensor 'a_shape' must have 2 elements [Op:SparseTensorDenseMatMul]\r\n```\r\nCan you please add the possiblity to do 3D SparseTensor matrix multiplication with 2D Tensor? Similar to 3D Tensor multiplication with 2D Tensor\r\n\r\n", "comments": ["I have the same problem. Did you find the answer?", "Nope not yet..."]}, {"number": 46303, "title": "Add TFLite metadata support to TensorFlowLiteSwift", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTFLite offers the ability to embed metadata into its models. For example, we can ship an image classifier with its labels included. https://www.tensorflow.org/lite/convert/metadata\r\n\r\nCurrently, the metadata is not accessible from the TensorFlowLiteSwift package. Available platforms can be found here: https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata\r\n\r\nI am using a custom TFLite image classifier in my Swift-based iOS app, and I would like to be able to ship the labels within the model itself because we use Firebase Machine Learning to send model updates (sometimes with different labels) over-the-air.\r\n\r\nWith the APIs available today, we are able to embed this information into the model, but have no way of reading it in our Swift client.\r\n\r\n**Will this change the current api? How?**\r\nIn theory, this should be an additive API change similar to that in the Java and C++ packages. Existing dependents should not be affected, but will be able to opt-in to this new behavior.\r\n\r\n**Who will benefit with this feature?**\r\nDependents of the TensorFlowLiteSwift package that want to be able to read metadata from their TFLite models, such as labels for image classifiers, etc...\r\n\r\n**Any Other info.**\r\nMy guess is that the TensorFlow team already intends to do this eventually. Because I couldn't find any indication of this, I wanted to explicitly track such availability with this issue.\r\n", "comments": ["@willbattel Thanks for filing the issue! You're right, the swift metadata API is in our roadmap. We've been prioritizing TFLite high level API (Support library / Task library / Metadata) for Android (Java) and native users, and only covers very little for iOS (Task library NL cases). Let's use this issue to track any future updates we may have for the swift metadata API. "]}, {"number": 46296, "title": "Unable to build debug version", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.19042\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): Git branch master, commit 277e22a01540b76151f5a664c8fd08854663fe27 (08.01.2021 12:05)\r\n- Python version: Python 3.8.7\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2019 (MSVC 19.28.29335)\r\n- CUDA/cuDNN version: 11.1/8\r\n- GPU model and memory: GeForce RTX 2070 SUPER (4 GB)\r\n\r\n**Describe the problem**\r\nI try to build CPU-Debug, CPU-Release, GPU-Debug and GPU-Release with the following commands:\r\n```\r\nbazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=dbg tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c\r\nbazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=opt tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c\r\nbazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=dbg --config=cuda tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c\r\nbazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=opt --config=cuda tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c\r\n```\r\nBuilding the release versions works for both CPU and GPU. When trying to build the debug versions I get the following error messages (for CGU and GPU):\r\n```\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,bool>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,signed char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,__int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,unsigned char>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,Eigen::half>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,unsigned short>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,float>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,unsigned __int64>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,double>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,Eigen::bfloat16>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,std::complex<float> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,std::complex<double> >\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,unsigned int>\": must return a value\r\nC:\\users\\transporter\\_bazel_transporter\\7ekuv3jw\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: \"xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,std::complex<double> >\": must return a value\r\n```\r\nAre warnings perhaps treated as errors in the debug variant?", "comments": ["I have the same issue. Did you have any success yet in solving it? \r\nBest regards\r\nMarc", "No, nothing has happened so far. I tried again with commit c925da2cbf53df537aff5184691e80421cab3c4e, but that gave the same result.", "You can try using the following command line option:\r\n--copt=/wd4716\r\n\r\nAfter that, this error went away but I faced some others... Been following the instructions in this merge request and its comments\r\nhttps://github.com/tensorflow/tensorflow/issues/41118 \r\nA build is currently running, don't know yet whether I'll face other problems now.\r\nHope this helps you"]}, {"number": 46295, "title": "tensorflow/c/eager/c_api_test fails to find GPU implementation of MatMulOp", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1/7.6.4.38\r\n- GPU model and memory: GTX1080TI\r\n\r\n**Describe the current behavior**\r\n\r\nThe 4 tests `CAPI.TensorHandleSilentCopy*` from `tensorflow/c/eager/c_api_test` fail each with \r\n\r\n```\r\ntensorflow/c/eager/c_api_test.cc:442: Failure\r\nExpected equality of these values:\r\n  TF_GetCode(status.get())\r\n    Which is: 3\r\n  TF_OK\r\n    Which is: 0\r\nCould not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:GPU:\r\n0, /job:localhost/replica:0/task:0/device:CPU:0].\r\n```\r\n\r\nReason seems to be that `MatMulOp` is not found for GPU which is odd as seemingly everything else works.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n`/tmp/ebbuild/TensorFlow/2.4.0/fosscuda-2019b-Python-3.7.4/tmp004t3B-bazel-tf/7277201245461b79db55d0e3e6d95f77/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/c_api_test_gpu --gtest_filter=*TensorHandleSilentCopy*`\r\n\r\n**Other info / logs** \r\n\r\nFor some reason this test works on our POWER9 nodes which have otherwise the exact same environment (same software versions etc), but use V100s", "comments": []}, {"number": 46281, "title": "why ExtractVolumePatches uses different implements for ThreadPoolDevice and GPUDevice.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):2.5\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):3.7.1\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:7.0GB\r\n\r\n\r\n**Describe the current behavior**\r\nin tensorflow/core/kernels/eigen_volume_patch.h\r\n```c++\r\n650 OVERRIDE_EVALUATOR(Eigen::ThreadPoolDevice);\r\n651 OVERRIDE_EVALUATOR(Eigen::DefaultDevice);\r\n```\r\nOverrides the TensorExecutor to modify the PADDING_SAME to\r\n```c++\r\n  131         case PADDING_SAME: {                                                                                                                                                                                                                     \r\n  132           m_outputPlanes = Eigen::divup(m_input_planes_eff, m_plane_strides);\r\n  133           m_outputRows = Eigen::divup(m_input_rows_eff, m_row_strides);\r\n  134           m_outputCols = Eigen::divup(m_input_cols_eff, m_col_strides);\r\n  135           const Index dz = numext::maxi<DenseIndex>(\r\n  136               0, (m_outputPlanes - 1) * m_plane_strides + m_patch_planes_eff -\r\n  137                      m_input_planes_eff);\r\n  138           const Index dy = numext::maxi<DenseIndex>(\r\n  139               0, (m_outputRows - 1) * m_row_strides + m_patch_rows_eff -\r\n  140                      m_input_rows_eff);\r\n  141           const Index dx = numext::maxi<DenseIndex>(\r\n  142               0, (m_outputCols - 1) * m_col_strides + m_patch_cols_eff -\r\n  143                      m_input_cols_eff);\r\n  144           m_planePaddingTop = dz / 2;\r\n  145           m_rowPaddingTop = dy / 2;\r\n  146           m_colPaddingLeft = dx / 2;\r\n  147           break;\r\n  148         }\r\n```\r\n\r\nbut for GPUDevice, we don't do this override. We use default eigen implement:\r\n```c++\r\n  257         case PADDING_SAME: {                                                                                                                                                                                                                     \r\n  258           m_outputPlanes = numext::ceil(m_input_planes_eff / static_cast<float>(m_plane_strides));\r\n  259           m_outputRows = numext::ceil(m_input_rows_eff / static_cast<float>(m_row_strides));\r\n  260           m_outputCols = numext::ceil(m_input_cols_eff / static_cast<float>(m_col_strides));\r\n  261           const Index dz = m_outputPlanes * m_plane_strides + m_patch_planes_eff - 1 - m_input_planes_eff;\r\n  262           const Index dy = m_outputRows * m_row_strides + m_patch_rows_eff - 1 - m_input_rows_eff;\r\n  263           const Index dx = m_outputCols * m_col_strides + m_patch_cols_eff - 1 - m_input_cols_eff;\r\n  264           m_planePaddingTop = dz - dz / 2;\r\n  265           m_rowPaddingTop = dy - dy / 2;\r\n  266           m_colPaddingLeft = dx - dx / 2;\r\n  267           break;\r\n  268         }\r\n```\r\n\r\nThe question is: Why do we use different implements for CPU/GPU devices?\r\n\r\n**Describe the expected behavior**\r\nDifferent devices should produce same results.\r\n\r\n", "comments": ["@quintinwang5, just so I understand are you saying that TF produces substantially different results on CPU vs GPU?  If yes, can you demonstrate this using a Python snippet?", "@sanjoy The code is here. If we use `padding=\"SAME\"`. The result will be different. And `padding=\"VALID\"` works well.\r\n``` python\r\n#!/usr/bin/env python\r\n# coding=utf-8\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.compat.v1.disable_eager_execution() # need to disable eager in TF2.x\r\nwith tf.device(\"/CPU:0\"):\r\n    a = tf.random.normal(shape=[6, 9, 7, 8, 4], seed=1)\r\n    b = tf.random.normal(shape=[6, 9, 7, 8, 4], seed=1)\r\n    c = tf.extract_volume_patches(a, ksizes=[1, 1, 1, 1, 1], strides=[1, 2, 3, 4, 1], padding=\"SAME\")\r\nwith tf.device(\"/GPU:0\"):\r\n    d = tf.extract_volume_patches(b, ksizes=[1, 1, 1, 1, 1], strides=[1, 2, 3, 4, 1], padding=\"SAME\")\r\n\r\n\r\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=False, log_device_placement=True))\r\n\r\nprint(sess.run(tf.reduce_all(tf.equal(c, d))))\r\n\r\n```", "I don't have enough context here, but it looks like @mjanusz added the override.  Michal any ideas?  Does the GPU kernel need some changes?"]}, {"number": 46277, "title": "ESP32 example creation fails for person_detection example", "body": "**System information**\r\n- OS Platform and Distribution: ESP32\r\n- TensorFlow version: master, commit 78012e5d6c45ffd3673b8970b199a67980df1963\r\n\r\n**Describe the problem**\r\n\r\nExample creation fails with following error:\r\n\r\n```\r\n% make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\nmake: *** No rule to make target 'generate_person_detection_esp_project'.  Stop.\r\n```\r\n", "comments": ["Still no solution yet? Please!", "MR is ongoing... https://github.com/tensorflow/tensorflow/pull/47063\r\n\r\nFeel free to cherry-pick changes for now."]}, {"number": 46266, "title": "Normalisation layer save standard deviation instead of variance", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe Normalisation layer in tensorflow/python/keras/layers/preprocessing/normalization.py seems to save the variance instead of the standard deviation. And calculates the std on runtime std = sqrt(var).\r\nThis seems a bit wastfull to me as instead of saving the std once.\r\nIt would be a very small change with not much impact, but easy to implement.\r\n\r\n**Will this change the current api? How?**\r\nWhen using adapt() to initialize it doesn't matter which metric is saved -> no change in api\r\nWhen directly initializing it, it could still take the variance (or std as an option) and save std in the background\r\n\r\n**Any Other info.**\r\nCould be changed at the next big change e.g: together with the behaviour I stated in [keras #14356](https://github.com/keras-team/keras/issues/14356#issue-781559372)", "comments": ["We should not change the weight format (in particular for backwards compatibility and because we want the weights of multiple normalizable layers to be easily mergeable), but what we could do is compute the `sqrt` in the constructor of the layer (or upon weight loading) and store it on the layer."]}, {"number": 46255, "title": "small discrepancies between images loaded with `tf.keras.preprocessing.image_dataset_from_directory()` and `tf.keras.preprocessing.image.load_img()`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0\r\n- Python version: 3.6.9 (Colab)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nThe two keras functions\r\n `tf.keras.preprocessing.image_dataset_from_directory()`\r\nand\r\n`tf.keras.preprocessing.image.load_img()`\r\nproduce slightly different results when loading the same image and applying resizing.\r\nI think the culprit might be the different resize operations used under the hood (tf's image_ops for the former vs PIL for the latter).\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n`image_dataset_from_directory()` is commonly used with model.fit() while `load_img()` with model.predict().\r\nIntuitively, I expected the exact same tensor as output for the two loading/preprocessing pipelines most often used in keras.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nColab Gist reproducing the problem:\r\nhttps://colab.research.google.com/gist/fabiocarrara/0d8a34e5ef805e9d91e82d9949365b1e/untitled0.ipynb\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nPIL Version: 7.0.0\r\n", "comments": ["Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/7bd48cebf0996373f354fcb22b5182b6/46255.ipynb) and TF-nightly. \r\n\r\nWhereas with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/7d4808542c10d8c7de9b5ca60831b530/46255-2-3.ipynb), I'm facing an error stating `AttributeError: 'BatchDataset' object has no attribute 'file_paths'`.  Please check the linked gist for reference. Thanks!", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/3cddd5d3abc7665b022ee6955cad32ab/untitled190.ipynb)..Thanks !", "This could be due to the different dtype in both the cases, `float32` vs `uint8`", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> This could be due to the different dtype in both the cases, `float32` vs `uint8`\r\n\r\nIt could be, but it seems there is no way to choose the dtype when using `image_dataset_from_directory()` or `load_img()`. Could the two outputs be aligned somehow?", "Both `load_img` and `image_dataset_from_directory` are moved under `tf.keras.utils`. \r\n[Here](https://colab.sandbox.google.com/gist/sachinprasadhs/977c25487fee1ca95f587364047f0be5/untitled0.ipynb) is the updated gist for reference.\r\n\r\nPlease consider creating this bug in keras repo. \r\nSince development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 46254, "title": "Cannot Convert SentencePieceTokenizer to TensorRT", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0/7.6\r\n- GPU model and memory: RTX 2080\r\n\r\n\r\n**Describe the current behavior**\r\nWhenever I try to convert a model containing the tokenizer as subgraph, I get an error.\r\n\r\n**Describe the expected behavior**\r\nIt should just convert it.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1_S37VihkTZ1B0HgjW8D7DMZcI8nwz2Bk\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-7-e780e9ed6737> in <module>\r\n      4 )\r\n      5 \r\n----> 6 converter.convert()\r\n\r\n~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py in convert(self, calibration_input_fn)\r\n   1094                                   self._input_saved_model_tags)\r\n   1095     func = self._saved_model.signatures[self._input_saved_model_signature_key]\r\n-> 1096     frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)\r\n   1097     grappler_meta_graph_def = saver.export_meta_graph(\r\n   1098         graph_def=frozen_func.graph.as_graph_def(), graph=frozen_func.graph)\r\n\r\n~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func, lower_control_flow, aggressive_inlining)\r\n   1069       func=func,\r\n   1070       lower_control_flow=lower_control_flow,\r\n-> 1071       aggressive_inlining=aggressive_inlining)\r\n   1072 \r\n   1073   output_graph_def, converted_input_indices = _replace_variables_by_constants(\r\n\r\n~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in __init__(self, func, lower_control_flow, aggressive_inlining, variable_names_allowlist, variable_names_denylist)\r\n    804         variable_names_allowlist=variable_names_allowlist,\r\n    805         variable_names_denylist=variable_names_denylist)\r\n--> 806     self._build_tensor_data()\r\n    807 \r\n    808   def _build_tensor_data(self):\r\n\r\n~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in _build_tensor_data(self)\r\n    823         data = map_index_to_variable[idx].numpy()\r\n    824       else:\r\n--> 825         data = val_tensor.numpy()\r\n    826       self._tensor_data[tensor_name] = _TensorData(\r\n    827           numpy=data,\r\n\r\n~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1069     \"\"\"\r\n   1070     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1071     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1072     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1073 \r\n\r\n~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1037       return self._numpy_internal()\r\n   1038     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1039       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1040 \r\n   1041   @property\r\n\r\n~/anaconda3/envs/ds/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```\r\n", "comments": ["@dshahrokhian \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ca4dff5ddcc5b906fad0d2c75aee57d7/untitled.ipynb) and share all dependencies for us to replicate the issue faced and analyse it.", "@Saduf2019 Was there an issue colab instance I have shared? It had all the dependencies already.\r\nI have added the file to your gist.\r\n\r\nThanks.", "@dshahrokhian\r\nwhat you add to your gist is local to you i cannot access it, please send it through attachment.", "@Saduf2019 \r\nMy apologies, here it is: https://drive.google.com/file/d/1l1kdHuM_oJf_sJwR1p8g1Pn4P_DRtCWc/view?usp=sharing", "@ymodak \r\nI ran the code shared on tf 2.4 and nightly, please find the gist for [tf 2.4](https://colab.research.google.com/gist/Saduf2019/65e9cb7c2b26eae2e5eb8a21fb7b02db/untitled492.ipynb) and cloab crashes on [nightly](https://colab.research.google.com/gist/Saduf2019/263fe39332add8ab3eebe9d0a030afdc/untitled.ipynb).", "Perhaps [tensorflow/text](https://github.com/tensorflow/text) repo can be a right place fo this issue. Can you please raise it on that repo?\r\nThanks!", "@ymodak correct me if I am wrong, but this has more to do with core tensorflow not being able to handle lookup tables during TF-TRT conversion. \r\n\r\nI had already opened an issue on `tensorflow/text`, as well as `tensorflow/tensorrt`. In [this](https://github.com/tensorflow/tensorrt/issues/233) issue, one of your core contributors also thinks the same.\r\n\r\nThank you for the support.", "hi all! I am facing the very same issue, so did not open a new one. Any update on this? Thanks for the support @bixia1 ", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/41027e5a650bace7b265bede0db6e443/untitled492.ipynb#scrollTo=AKpLR7ODG89j)..Thanks !"]}, {"number": 46247, "title": "Unexpected Events CUDA_ERROR_ILLEGAL_ADDRESS and CUDA_ERROR_LAUNCH_FAILED", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: I have followed this tutorial using my own data. Tutorial: https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/\r\nMy data: https://gist.github.com/Urkchar/e01a667c1656e874f918ff92db5b998f\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Microsoft Windows 10 Home version 10.0.19041 Build 19041\r\n-   **TensorFlow installed from (source or binary)**: I installed TensorFlow with `pip install tensorflow`\r\n-   **TensorFlow version**: 2.4.0\r\n-   **Python version**: 3.8.6\r\n-   **CUDA/cuDNN version**: CUDA version 11.2. cuDNN version 8.0.5\r\n-   **GPU model and memory**: Nvidia GTX 1070 with 8 GB memory\r\n-   **Exact command to reproduce**:\r\n`model.fit(x, y, epochs=8, batch_size=256, callbacks=desired_callbacks)`\r\n\r\nI receive unexpected errors when trying to fit a model. Examples include `CUDA_ERROR_ILLEGAL_ADDRESS` and `CUDA_ERROR_LAUNCH_FAILED`. Full logs and error messages below. \r\nStuff like `F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1` Makes me think that this is a bug because these are unexpected errors or failures. \r\n```py\r\nimport sys\r\nimport numpy\r\nfrom nltk.tokenize import RegexpTokenizer\r\nfrom nltk.corpus import stopwords\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, LSTM\r\nfrom keras.utils import np_utils\r\nfrom keras.callbacks import ModelCheckpoint\r\n\r\nprint(sys.version_info)\r\n\r\n\r\ndef tokenize_words(text_input):\r\n    # Lowercase everything to standardize it\r\n    text_input = text_input.lower()\r\n\r\n    # Instantiate the tokenizer\r\n    tokenizer = RegexpTokenizer(r\"\\w+\")\r\n    tokens = tokenizer.tokenize(text_input)\r\n\r\n    # If the create token isn't in the stop words, make it part of \"filtered\"\r\n    filtered = filter(lambda token: token not in stopwords.words(\"english\"), tokens)\r\n    return \" \".join(filtered)\r\n\r\n\r\ninput_file = open(\"yejibro_data.txt\").read()\r\n# input_file = open(\"84-0.txt\", \"r\", encoding=\"utf-8\").read()\r\n\r\n# Preprocces the input data, make tokens\r\nprocessed_inputs = tokenize_words(input_file)\r\n\r\nchars = sorted(set(processed_inputs))\r\nchar_to_num = dict((c, i) for i, c in enumerate(chars))\r\n\r\ninput_len = len(processed_inputs)\r\nvocab_len = len(chars)\r\nprint(f\"Total number of characters: {input_len}\")\r\nprint(f\"Total vocab: {vocab_len}\")\r\n\r\nseq_length = 100\r\nx_data = []\r\ny_data = []\r\n\r\n# Loop through inputs, start at the beginning and go until we hit the final character we can create\r\n# a sequence out of\r\nfor i in range(0, input_len - seq_length, 1):\r\n    # Define input and output sequences\r\n    # Input is the current character plus desired sequence length\r\n    in_seq = processed_inputs[i:i + seq_length]\r\n\r\n    # Out sequence is the initial character plus total sequence length\r\n    out_seq = processed_inputs[i + seq_length]\r\n\r\n    # We now convert list of characters to integers based on previous mappings and add the values to\r\n    # our lists\r\n    x_data.append([char_to_num[char] for char in in_seq])\r\n    y_data.append(char_to_num[out_seq])\r\n\r\nn_patterns = len(x_data)\r\nprint(f\"Total patterns: {n_patterns}\")\r\n\r\nx = numpy.reshape(x_data, (n_patterns, seq_length, 1))\r\nx = x/float(vocab_len)\r\n\r\ny = np_utils.to_categorical(y_data)\r\n\r\nmodel = Sequential()\r\nmodel.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(256, return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(y.shape[1], activation=\"softmax\"))\r\n\r\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\r\n\r\n# print(model.summary())\r\n\r\nfilepath = \"model_weights_saved.hdf5\"\r\ncheckpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\r\ndesired_callbacks = [checkpoint]\r\n\r\nmodel.fit(x, y, epochs=8, batch_size=256, callbacks=desired_callbacks)\r\n```\r\nHere are some error messages and logs that occurred at seemingly random points during the code. \r\nhttps://paste.pythondiscord.com/yomuzokuto.apache\r\nhttps://paste.pythondiscord.com/orusujidaq.yaml\r\nhttps://paste.pythondiscord.com/igexetayen.swift\r\nhttps://paste.pythondiscord.com/ovawipegap.yaml\r\nhttps://paste.pythondiscord.com/yopitiweri.less\r\n\r\nFinal note: I would be very happy to include more information when needed. I may have neglected to include some information that is necessary to figure out what's going on so please let me know. \r\nFinal final note: This is my first bug report on GitHub so I tired to follow the template to the best of my ability. \r\n", "comments": ["@Urkchar \r\nCan you please refer to this [issue](https://stackoverflow.com/questions/46241403/tensorflow-throw-cuda-error-launch-failed-after-a-long-run) and let us know if it helps.\r\nplease ensure you are using the [following](\r\nhttps://www.tensorflow.org/install/source#gpu) configs.", "@Urkchar Please roll back to cuda 11.0 (currently you are on 11.2) for TF 2.4", "> @Urkchar\r\n> Can you please refer to this [issue](https://stackoverflow.com/questions/46241403/tensorflow-throw-cuda-error-launch-failed-after-a-long-run) and let us know if it helps.\r\n> please ensure you are using the [following](https://www.tensorflow.org/install/source#gpu) configs.\r\n\r\nI will be happy to try out one of those configs. \r\nHow can I check which build tools and which compiler I am using? I didn't install these manually so perhaps they came with something else?", "> @Urkchar Please roll back to cuda 11.0 (currently you are on 11.2) for TF 2.4\r\n\r\nI will try this. Thank you. So TensorFlow currently doesn't support CUDA 11.2?", "> @Urkchar Please roll back to cuda 11.0 (currently you are on 11.2) for TF 2.4\r\n\r\nShould I get the update or the base version of 11.0? Does it matter?\r\n![image](https://user-images.githubusercontent.com/49205496/103973105-43a57c00-5134-11eb-93aa-80bda562727f.png)\r\n", "I changed from CUDA 11.2 to CUDA 11.0 (base) and was training fine up until epoch 8. It crashed with `CUDNN_STATUS_INTERNAL_ERROR` and `CUDA_ERROR_LAUNCH_FAILED`. Full output: https://paste.pythondiscord.com/egadilaqex.yaml\r\nI was monitoring the temperature of my GPU like was suggested in https://stackoverflow.com/questions/46241403/tensorflow-throw-cuda-error-launch-failed-after-a-long-run however there were no values out of the ordinary. ", "Now I'm getting this error `CUDA_ERROR_ILLEGAL_ADDRESS`. Full output: https://paste.pythondiscord.com/yusegemuga.apache\r\nIt appears to happen at random points during `model.fit(...)`. I ran the same exact code again and I got this error https://paste.pythondiscord.com/olovaguvun.apache\r\nI ran the exact same code a third time and got this error: https://paste.pythondiscord.com/elufotohog.apache\r\nFourth time gave this different error: https://paste.pythondiscord.com/damicanuli.sql\r\nRunning a fifth tame gave what appears to be the same error as Run 3 of this post however there was also the number `20` as the last line of output. Full output: https://paste.pythondiscord.com/ladifezepu.apache\r\nIf you need more examples of what's going on, I'd be happy to run this code some more. ", "@Urkchar \r\nCan you please refer to these solved issues for the error reported above and let us know.\r\n[link](https://www.mathworks.com/matlabcentral/answers/303228-cuda_error_illegal_address), [link1](https://github.com/tensorflow/tensorflow/issues/6509)", "I tried the fix in the first link, setting the environment variables and I still get the error. Full output: https://paste.pythondiscord.com/ujexuzaqih.apache\r\nI also tried running a second time after the fix and got this different unexpected error: https://paste.pythondiscord.com/ekowigurex.apache\r\nI'm unsure what I'm supposed to try for link1. From what I can tell, the fix was temporarily disabling some \"kernels\". This however was merged into the full release I think so I believe that I already have this fix. https://github.com/tensorflow/tensorflow/pull/6822\r\nIf I'm missing something from either of the links, please let me know. ", "Are you able to run the program under [CUDA memcheck](https://docs.nvidia.com/cuda/cuda-memcheck/index.html)?  If yes, can you please attach the logs?  That could give us a strong hint on what's going on.\r\n\r\nPlease also attach `\"yejibro_data.txt\"`, with it I will be able to check if it reproduces on my end.", "> Are you able to run the program under [CUDA memcheck](https://docs.nvidia.com/cuda/cuda-memcheck/index.html)? If yes, can you please attach the logs? That could give us a strong hint on what's going on.\r\n> \r\n> Please also attach `\"yejibro_data.txt\"`, with it I will be able to check if it reproduces on my end.\r\n\r\nData\r\n[yejibro_data.txt](https://github.com/tensorflow/tensorflow/files/5806483/yejibro_data.txt)\r\nThe data changed slightly since my first post. There were some lines with odd characters (\u00e1\u00e4\u00e6\u00e9\u00f1\u00f3\u00f4\u00f6) so I wanted to see if removing those helped. It didn't. The new data is attached of course.\r\nLogs\r\nhttps://gist.github.com/Urkchar/ad6faf20f52732fb3eab2cd04684f01d\r\nEdit:\r\nI tried running it again in the same way and I got this output. I figured it was unreasonable for me to wait 938 hours for this to finish so I terminated it. \r\nLogs\r\nhttps://gist.github.com/Urkchar/7da8f54f87a52ffb2e79ac5b0db4845e", "Thanks, unfortunately the reproducer runs fine on my Titan-V.\r\n\r\nThis is a bit of a shot in the dark but does setting the `TF_FORCE_GPU_ALLOW_GROWTH` env var to `1` workaround the issue?", "I can try that. For my knowledge, what would that do?", "> Thanks, unfortunately the reproducer runs fine on my Titan-V.\r\n> \r\n> This is a bit of a shot in the dark but does setting the `TF_FORCE_GPU_ALLOW_GROWTH` env var to `1` workaround the issue?\r\n\r\nSomething is not quite right with your idea.\r\n```\r\n2021-01-13 00:03:47.680127: E tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] The TF_FORCE_GPU_ALLOW_GROWTH environment variable is set but could not be parsed: \"1\". Valid values are \"true\" or \"false\". Using original config value of 0.\r\n```", "Sorry, please set `TF_FORCE_GPU_ALLOW_GROWTH` to `true`.  This prevents TensorFlow from allocating all of the GPU memory on startup.  My (purely speculative) idea is that allocating all of the GPU memory on startup is preventing cuDNN from initializing.", "After applying that workaround, I tried running it how I normally would in Visual Studio Code. I got this error\r\nhttps://gist.github.com/Urkchar/d95c204fade15be20dfc92d1ec207867\r\nAs you can see, there's also another problem with the ETA. It used to be between 4 and 7 minutes per epoch and now it's about 2 hours. I am running it with cuda-memcheck now but I fear it could take a long time before an error is encountered. 2 hours per epoch for 8 epochs is 16 hours. \r\nEdit\r\nI was right about it taking a long time. I just terminated it.\r\nLogs\r\nhttps://gist.github.com/Urkchar/365035a226c6118318fb85208a383fa5", "Have you been able to solve this? I am facing a similar issue and it might be related (see https://github.com/tensorflow/tensorflow/issues/50326).", "> Have you been able to solve this? I am facing a similar issue and it might be related (see #50326).\r\n\r\nI think I was able to solve this. I think it was because I didn't install cuDNN correctly. "]}, {"number": 46236, "title": "Implement a simple way to load a TF dataset from a Spark dataset and a simple way to convert a TF dataset to a Spark dataset", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.2\r\n- Are you willing to contribute it (Yes/No): Hmm, yes though it may take a while for me to figure out how to do this. May be easier if an experienced contributor were to implement this (?)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, from what I can tell, if you want to load data from Spark into a TF dataset, you have to first persist the data, as e.g. Parquet files or tfrecord files, then load data into a TF dataset from those files. This is inefficient as it requires intermediary processing and storage. Ideally, this new feature would allow a developer to load/transform data in Spark, then convert it to a TF dataset, build a model, etc.  Conversely, if you need an easy conversion from a TF dataset to Spark, the complementary converter would handle that. Perhaps a .toSparkDataset() method on TF dataset, or a separate static converter method somewhere.\r\n\r\n**Will this change the current api? How?**\r\nYes. This will require new classes/methods -- up to the implementor.\r\n\r\n**Who will benefit with this feature?**\r\nI suspect many developers will benefit. Spark offers rich functionality for loading, transforming, joining data. Many systems may already have ETL type of code in Spark and want to leverage TensorFlow. Having an easy bridge from Spark to TF and back without the overhead of intermediary files (or a reduced such overhead) would be really useful.\r\n\r\nLinkedIn's Spark-TFRecord library and the Spark Tensorflow Connector both require intermediary tfrecord files. Uber's Petastorm seems to offer a Spark-TF converter but if I understand correctly it uses intermediary local files for conversion; also I'm not seeing the reverse TF-to-Spark converter there (?).\r\n\r\n**Any Other info.**\r\n- I'm using Apache Spark 3.0.", "comments": ["Generally, TF appears to drive implementors toward doing data prep and data post-processing in Spark, which means storing input data in intermediary storage, then loading it into TF datasets. That's rather inconvenient.\r\n\r\nThere is the [Big DL library](https://github.com/intel-analytics/BigDL) which presumably allows one to train TF models in a distributed fashion on Spark.", "@dgoldenberg-audiomack   I like how you used the word presumably. I have been struggling to make it work actually.", "@Adamage LOL I was exercising a bit of caution b/c I've not evaluated Big DL library yet :)"]}, {"number": 46234, "title": "`sample_weight` does not work for multi-output (multi-task) models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.12\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Tesla T4 15109MiB\r\n\r\n**Describe the current behavior**\r\nIn a multi-task setting, where the model (say, a regression model) has multiple outputs, `sample_weight` does not allow for multidimensional arrays - specifically, a `ValueError` is thrown during `.fit`. More surprisingly, if a list of weight arrays is provided instead or if the weights array has an extra dimension (via `.expand_dims`, for example), this fails silently - the weights do not appear to be used and no `ValueError` is thrown.\r\n\r\n**Describe the expected behavior**\r\nIdeally, the multi-task setting should support 2D `sample_weight` - my specific use case is with missing labels. Samples are not guaranteed to have labels for all tasks and one way to handle this is to set weights for missing labels to 0. Adding this is perhaps more of a feature request; the bug is the unexpected behavior.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nn_data_points = 100\r\nn_feats = 15\r\nn_outputs = 5\r\n\r\nX = np.random.uniform(0, 1, (n_data_points, n_feats))\r\ny = np.random.uniform(0, 1, (n_data_points, n_outputs))\r\n# set the last task to be some larger value\r\ny[:, -1] = 100\r\n\r\nweights = np.ones_like(y)\r\n# set the weights for the last task to be 0\r\nweights[:, -1] = 0.\r\n\r\ndef get_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(32, input_shape=(n_feats,)))\r\n    model.add(tf.keras.layers.Dense(32))\r\n    model.add(tf.keras.layers.Dense(n_outputs))\r\n    model.compile(optimizer='sgd', loss='mse'))\r\n    return model\r\n\r\n# Produces some large loss value\r\nmodel = get_model()\r\nmodel.fit(X, y, epochs=1)\r\n\r\n# Produces approximately the same large loss value (should not!)\r\nmodel = get_model()\r\nmodel.fit(X, y, sample_weight=np.expand_dims(weights, axis=-1), epochs=1)\r\n\r\n# Throws a ValueError\r\nmodel = get_model()\r\nmodel.fit(X, y, sample_weight=weights, epochs=1)\r\n```\r\n\r\n`ValueError` thrown:\r\n```\r\nValueError: Can not squeeze dim[1], expected a dimension of 1, got 5 for '{{node mean_squared_error/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [?,5].\r\n```\r\n\r\n**Other info / logs**\r\n<details><summary>full traceback</summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-66-3ba28a4faca1> in <module>\r\n----> 1 model.fit(X, y, sample_weight=weights, epochs=1)\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    625       # This is the first call of __call__, so we have to initialize.\r\n    626       initializers = []\r\n--> 627       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    628     finally:\r\n    629       # At this point we know that the initialization is complete (or less\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    504     self._concrete_stateful_fn = (\r\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 506             *args, **kwds))\r\n    507 \r\n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2444       args, kwargs = None, None\r\n   2445     with self._lock:\r\n-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2447     return graph_function\r\n   2448 \r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2775 \r\n   2776       self._function_cache.missed.add(call_context_key)\r\n-> 2777       graph_function = self._create_graph_function(args, kwargs)\r\n   2778       self._function_cache.primary[cache_key] = graph_function\r\n   2779       return graph_function, args, kwargs\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665             arg_names=arg_names,\r\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2667             capture_by_value=self._capture_by_value),\r\n   2668         self._function_attributes,\r\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    440         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    443 \r\n\r\n~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in user code:\r\n\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\r\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:145 __call__\r\n        losses, sample_weight, reduction=self._get_reduction())\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:107 compute_weighted_loss\r\n        losses, sample_weight)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/losses/util.py:144 scale_losses_by_sample_weight\r\n        losses, None, sample_weight)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/losses/util.py:96 squeeze_or_expand_dimensions\r\n        sample_weight = array_ops.squeeze(sample_weight, [-1])\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:180 wrapper\r\n        return target(*args, **kwargs)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507 new_func\r\n        return func(*args, **kwargs)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:4145 squeeze\r\n        return gen_array_ops.squeeze(input, axis, name)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py:9875 squeeze\r\n        \"Squeeze\", input=input, squeeze_dims=axis, name=name)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal\r\n        compute_device)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal\r\n        op_def=op_def)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1817 __init__\r\n        control_input_ops, op_def)\r\n    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 5 for '{{node mean_squared_error/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [?,5].\r\n```</details>", "comments": ["> `ValueError: Can not squeeze dim[1], expected a dimension of 1, got 5`\r\n\r\nAs per the error log, changing the shape of the `weights` variable from `(x, 5)` to `(x, 1)`, I was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dab2aa671b0c8a04517afc203916a062/46234.ipynb#scrollTo=cZJ-g6wToRxZ&line=4&uniqifier=1). Thanks!", "@amahendrakar thanks for the response!\r\n\r\nI don't think this addresses the issue I'm referring to, though. My problem is that I would like to apply weights differently across tasks for each sample (so you shouldn't reshape to `(500, 1)` and take the first 100). 1D weights (applying only one weight per sample) works as expected. 2D `(n_samples, n_tasks)` weights do not work, and this is also not documented to work, so it's not entirely surprising, though undesirable (potential feature request).\r\n\r\nThe issue is when you have a 2D matrix, and add an empty dimension - that suppresses the `ValueError` but it's unclear how the weights are being applied.", "I was able to  replicate in  TF 2.7 . Attaching [Gist](https://colab.research.google.com/gist/mohantym/1bda213515d9c671663c6c4fe592179a/46234.ipynb#scrollTo=MhHafWmto8nM) for reference."]}, {"number": 46205, "title": "ButchNormalization fails when nvidia GPU is used and the training size and the batch size is \"well set\"", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary installed by pip3\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.0/8.0\r\n- GPU model and memory: Nvidia 1080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nWhen the size of training set and the batch size is set so that the last batch contains only one element (i.e. len(training_set)%batch_size == 1 ), one of the weights of the batch normalization (the variance) is set to nan. The problem disappears when using CPU.\r\n\r\n**Describe the expected behavior**\r\nHave valid weights computed regardless of the size of the training set.\r\n\r\n**Standalone code to reproduce the issue**\r\nfrom tensorflow import keras                                                                                                                                                                                       \r\nimport numpy as np                                                                                                                                                                                                 \r\n                                                                                                                                                                                                                   \r\ninp = keras.layers.Input((1,1,1))                                                                                                                                                                                  \r\nmid = keras.layers.Conv2D(1, (1,1))(inp)                                                                                                                                                                           \r\nout = keras.layers.BatchNormalization()(mid)                                                                                                                                                                       \r\n                                                                                                                                                                                                                   \r\nmod = keras.models.Model(inputs=inp, outputs=out)                                                                                                                                                                  \r\nmod.compile(optimizer='adam', loss='mse')                                                                                                                                                                          \r\n                                                                                                                                                                                                                   \r\ndata = np.reshape(np.arange(9), (9,1,1,1))                                                                                                                                                                         \r\n                                                                                                                                                                                                                   \r\nmod.fit(data,data,batch_size=8)                                                                                                                                                                                    \r\nprint(mod.layers[2].get_weights()[3])             #This is the last layer's last weight; this is NAN with GPU             \r\n", "comments": ["I have tried in colab with TF GPU version 2.4 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/949502c0c9083531adec9df661b04f01/untitled599.ipynb).Colab is using Cuda 10.1.Thanks!", "Strange, I just rerun the same box and got this (I mean the same block in the notebook you linked in):\r\n\r\n2/2 [==============================] - 0s 5ms/step - loss: 19.9350\r\n[nan]", "@enyecz I ran @ravikyram gist and saw a number (instead of nan as you mentioned). Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/062e6c781ea27e283eab3636e423fc31/untitled599.ipynb)\r\n\r\nDid you ran the code with (i) colab, (ii) local with CUDA 11.0, and (iii) both colab & local? Thanks!", "It seems to me that collab don't like pip. If you run pip first to install tensorflow-gpu, you don't get a GPU later, and running by CPU, there is no bug. In that case, you need to wait for the timeout, and your GPU-s come back with TF 2.4.0 (that's why I saw nan at the very same notebook).\r\nPlease take a look at the video I did, there I show you the whole case. There I use tf.test.is_gpu_available() to show that the GPU is lost.\r\n\r\nhttps://user-images.githubusercontent.com/14057706/103832769-7477a980-507f-11eb-9da7-f7d143dc5109.mp4\r\n\r\n\r\n\r\n", "@enyecz Agree with you. I just tried with `TF2.3-gpu`, which results in `nan`.  Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/0a952a327962dd10f7d07ec03c1e0704/untitled599.ipynb)\r\n\r\nThere is some issue in `TF2.4-gpu` when pip installed in a colab. We will look into the issue. Thanks! ", "Was able to reproduce the issue with TF v2.5-gpu,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4a89ff3d35fac190ffd204cbe6c1f015/untitled599.ipynb#scrollTo=mdDdU-pI-J-P) ..Thanks!", "It is still replicating in TF 2.7. Attaching [Gist](https://colab.research.google.com/gist/mohantym/acebe3e0712983a629e878d4e5ef1b9f/untitled599.ipynb#scrollTo=Ld1i7Dou-umj) for reference. Thanks!", "It says:\r\n\r\n2/2 [==============================] - 13s 26ms/step - loss: 18.6861\r\n[nan]\r\n\r\nSo yes. This is still a bug."]}, {"number": 46191, "title": "Add support for padding and cropping to tf.keras.layers.experimental.preprocessing.Resizing", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): No, unfortunately I don't have the time to do so.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe new layer `tf.keras.layers.experimental.preprocessing.Resizing` allows for models to be made more portable by handling image resizing within the model itself, as described in the docs [here](https://www.tensorflow.org/guide/keras/preprocessing_layers#benefits_of_doing_preprocessing_inside_the_model_at_inference_time). This layer provides options for interpolation, but cannot be configured to crop or pad the image to maintain aspect-ratio.\r\n\r\nIn my case, I use the `tf.image.resize_with_pad` in my `tf.data` pipeline in order to maintain aspect ratio and letter-box the shorter sides. This cannot be done inside of the model without a custom or Lambda layer.\r\n\r\nThe layer should ideally also include the functionality from `tf.image.resize_with_crop_or_pad`.\r\n\r\n**Will this change the current api? How?**\r\nYes, but purely additive. I suggest adding an additional parameter `resize_type` to the layer and simply have the default value perform the current behavior. This would not affect current users, but offer the feature to those who chose to enable it.\r\n\r\n**Who will benefit with this feature?**\r\nThis would be handy for developers/researchers who want a simple, built-in mechanism for resizing images inside of a model but don't want the current stretching behavior. In particular, this would benefit the portability of models, where we can do more within the model itself.\r\n\r\n**Any Other info.**\r\nN/A", "comments": ["@willbattel , we would like to work on this feature improvement and give the updated code", "@willbattel,\r\nLatest Tensorflow v2.8, [tf.keras.layers.experimental.preprocessing.Resizing](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing) has `crop_to_aspect_ratio`\r\n\r\n```\r\ntf.keras.layers.Resizing(\r\n    height,\r\n    width,\r\n    interpolation='bilinear',\r\n    crop_to_aspect_ratio=False,\r\n    **kwargs\r\n)\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@gadagashwini thank you for letting me know. That is great to see.\r\n\r\nThe other thing we are still interested is a `pad_to_aspect_ratio` option so that we can include the whole image, without cropping, in its original aspect ratio by adding padding on the short sides- the same way `tf.image.resize_with_pad` does."]}, {"number": 46168, "title": "Test TensorFloat32 with conv2d", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.config.experimental.enable_tensor_float_32_execution(False)\r\nx_in = np.array([[\r\n  [[2], [1], [2], [0], [1]],\r\n  [[1], [3], [2], [2], [3]],\r\n  [[1], [1], [3], [3], [0]],\r\n  [[2], [2], [0], [1], [1]],\r\n  [[0], [0], [3], [1], [2]], ]])\r\nkernel_in = np.array([\r\n [ [[2, 0.1]], [[3, 0.2]] ],\r\n [ [[0, 0.3]],[[1, 0.4]] ], ])\r\nx = tf.constant(x_in, dtype=tf.float32)\r\nkernel = tf.constant(kernel_in, dtype=tf.float32)\r\nout = tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='VALID')\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4.1708 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): pip3 install tensorflow-gpu\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda11.1/cudnn8.0.5\r\n- GPU model and memory: GeForce RTX 3090 \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI ran this on an RTX 3090 with Nsight system. Compared with  tf.config.experimental.enable_tensor_float_32_execution(False), the  conv2d kernels don`t have higher performance with tf.config.experimental.enable_tensor_float_32_execution(True).\r\n\r\n**Describe the expected behavior**\r\nWith tf.config.experimental.enable_tensor_float_32_execution(True), the conv2d kernels should have higher performance.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@WangTuoxyty \r\n\r\nI have tried in colab with TF -gpu version 2.4 and i did not notice any major performance issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d45331077ba164318e8b11e2f05e21d1/untitled595.ipynb).\r\nPlease, elaborate the issue with reproducible code if i miss something. It helps us in debugging faster. Thanks!", "> @WangTuoxyty\r\n> \r\n> I have tried in colab with TF -gpu version 2.4 and i did not notice any major performance issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d45331077ba164318e8b11e2f05e21d1/untitled595.ipynb).\r\n> Please, elaborate the issue with reproducible code if i miss something. It helps us in debugging faster. Thanks!\r\n\r\nI saved the code to a file \"test_conv.py\", and execute command \"nsys nvprof python3 test_conv.py\" in a terminal, and this is a part of the output:\r\nGenerating CUDA Kernel Statistics...\r\nCUDA Kernel Statistics (nanoseconds)\r\n\r\nTime(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \r\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \r\n   96.7         1927958          28         68855.6           67072           71072  redzone_checker                                                                                                                                                                                                                                                                                                                              \r\n    0.9           18078           7          2582.6            2560            2592  void cudnn::cnn::conv2d_grouped_direct_kernel<float, float, float, float, float, float, true, false, 0, 1, 0>(cudnnTensorStruct, float const*, cudnnFilterStruct, float const*, cudnnConvolutionStruct, cudnnTensorStruct, float*, float, float, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divis\r\n    0.5            9536           4          2384.0            2304            2432  void fft2d_r2c_16x16<float>(float2*, float const*, int, int, int, int, int, int, int, int)                                                                                                                                                                                                                                                   \r\n    0.4            7200           2          3600.0            3584            3616  void fft2d_r2c_32x32<float, false, 1u, true>(float2*, float const*, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool, int2, int, int)                                                                                                                                                                               \r\n    0.3            6400           2          3200.0            3200            3200  void fft2d_c2r_32x32<float, false, false, 1u, false, false>(float*, float2 const*, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, bool, float*, float*, int2, int, int)                                                                                                                                  \r\n    0.3            5759           2          2879.5            2848            2911  void fft2d_r2c_32x32<float, false, 5u, false>(float2*, float const*, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool, int2, int, int)                                                                                                                                                                              \r\n    0.2            4895           2          2447.5            2431            2464  void gemmk1_kernel<float2, 256, 5, false, false, true, false, cublasGemvTensorStridedBatched<float2 const>, cublasGemvTensorStridedBatched<float2>, float2>(cublasGemmk1Params<float2, cublasGemvTensorStridedBatched<float2 const>, cublasGemvTensorStridedBatched<float2>, float2, biasType<cublasGemvTensorStridedBatched<float2>::value_t\r\n    0.2            4704           2          2352.0            2336            2368  void fft2d_c2r_16x16<float, false>(float*, float2*, int, int, int, int, int, int, int, int, int, int, float, float, int, float*, float*)                                                                                                                                                                                                     \r\n    0.2            3135           2          1567.5            1567            1568  void gemmk1_kernel<float2, 256, 5, true, false, false, false, cublasGemvTensorStridedBatched<float2 const>, cublasGemvTensorStridedBatched<float2>, float2>(cublasGemmk1Params<float2, cublasGemvTensorStridedBatched<float2 const>, cublasGemvTensorStridedBatched<float2>, float2, biasType<cublasGemvTensorStridedBatched<float2>::value_t\r\n    0.2            3040           2          1520.0            1504            1536  void flip_filter<float, float>(float*, float const*, int, int, int, int)                                                                                                                                                                                                                                                                     \r\n    0.1            1568           1          1568.0            1568            1568  void tensorflow::functor::ShuffleInTensor3Simple<float, 2, 1, 0, false>(int, float const*, tensorflow::functor::Dimension<3>, float*)                                                                                                                                                                                                        \r\n    0.1            1440           1          1440.0            1440            1440  void tensorflow::functor::ShuffleInTensor3Simple<float, 0, 2, 1, false>(int, float const*, tensorflow::functor::Dimension<3>, float*) ", "Hi @WangTuoxyty,\r\n\r\nThe benchmark you're using is very small so tf32 or not will not make a big difference, do you see the same issue when you try larger convolutions?", "> Hi @WangTuoxyty,\r\n> \r\n> The benchmark you're using is very small so tf32 or not will not make a big difference, do you see the same issue when you try larger convolutions?\r\n\r\nI change the shape of x_in from [1, 5, 5, 1] to [10, 5000, 5000, 1]\uff0cwhile it has the same result."]}, {"number": 46165, "title": "train_on_batch is much slower than custom training loop on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: Colab\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: Colab\r\n- GPU model and memory: Colab\r\n\r\n**Describe the current behavior**\r\n\r\nOn Colab, `train_on_batch` is 3 times slower than custom training loop on GPU\r\n\r\n**Describe the expected behavior**\r\n\r\nShould be comparable.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1s6O4MyfqyCFI8uEM6JlTVNF18BLvYfka?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nI think it's because `train_on_batch` uses `tf.data.Dataset`, which places tensor on CPU, on all kinds of input. The data transfer cost from host to device is not negligible resulting in slow down.", "comments": ["Running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a9a1a86c627a985d7bf219a2ae2289be/46165-2-3.ipynb#scrollTo=5dIEuEZDHA11) throws an error stating `TypeError: minimize() got an unexpected keyword argument 'tape'`.\r\n\r\nWas able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/0fa486f44eb9aa0a385c2813ed3f2537/46165.ipynb).\r\n\r\nWhereas with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/37e884a75bd1538610e9461f7f81e683/46165-2-5.ipynb), the code runs on CPU and takes almost the same time. Please check the linked gist for reference. Thanks!"]}, {"number": 46132, "title": "Android: Instantiating TFLite Interpreter with GPU delegate takes 2.5 seconds.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Catalina v10.15.7**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Pixel 3a**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **v2.4.0**\r\n- Python version: **3.8.0**\r\n- Bazel version (if compiling from source): **-** \r\n- GCC/Compiler version (if compiling from source): **-**\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: **Adreno 615**\r\n\r\n**Describe the current behavior**\r\nThe instantiation time of tiflite interpreter with GPU delegate on Android is extremely high. For some models It takes about 2.5 seconds to instantiate it. This behavior was tested with various segmentation models of different sizes (500Kb - 12Mb) and on different  mobile devices. The instantiation time very slightly dependent on the model size and is faster on flagship devices (such as Samsung Galaxy S20) but overall it is very slow. Initializing an interpreter with the same model without GPU delegate is sub millisecond.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n// interpreterBinary: ByteBuffer direct allocated byte buffer.\r\nval options = Interpreter.Options().addDelegate(GpuDelegate())\r\nval stopwatch = Stopwatch.createStarted() // start trace\r\nval interpreter = Interpreter(interpreterBinary, options)\r\nstopwatch.stop() // end trace\r\n```\r\n\r\n", "comments": ["@alexanderar  Can u download the Android prebuilt benchmark tools from https://www.tensorflow.org/lite/performance/measurement\r\nAnd run it in the device to see if the gpu delegate is fallback to cpu.\r\nI notice  tat in my case the transpose  convolution is never used using GPU and is using CPU fallback.\r\nU might try \r\n./android_aarch64_benchmark_model  --graph=unet.tflite  --use_gpu=true --enable_op_profiling=true\r\nor \r\n./android_aarch64_benchmark_model  --graph=unet.tflite  --use_nnapi=true --enable_op_profiling=true\r\n\r\nNot sure u are using quantitated model or not. But try to use floating model and int8 model to see if there is any changes.\r\nIf u see :\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t     TfLiteNnapiDelegate\t        4\t    19.800\t    38.140%\t    38.140%\t     0.000\t        4\r\n\t                 CONV_2D\t        3\t    17.773\t    34.235%\t    72.375%\t     0.000\t        3\r\n\t          TRANSPOSE_CONV\t        2\t    13.034\t    25.107%\t    97.482%\t     0.000\t        2\r\n\t                     ADD\t        2\t     1.307\t     2.518%\t   100.000%\t     0.000\t        2\r\n\r\nLike above , TfLiteNnapiDelegate contains all layers using nnapi acceleration. \r\nWhich for some reason the CONV_2D, TRANSPOSE_CONV and ADD layers areall fall back to CPU.\r\n", "@alexanderar  If you are targeting for Android phone with qualcomm chipset only. \r\nI would recommend to use qualcomm snpe sdk directly to do the inference.\r\nThere are some many cases that the TFLite fall back to CPU implementation..\r\nhttps://developer.qualcomm.com/docs/snpe/overview.html ", "@ek9852 Actually I am not even talking about running the model. Just the instantiation of the interpreter takes a very long time:\r\n`val interpreter = Interpreter(interpreterBinary, options)` on Pixel3a takes 2.8 seconds for some networks. For other is the quite fast. I thought that it may somehow depend on the model size but in this specific case the model is just  400Kb.", "Yup, that is expected, unfortunately :-(. the GPU delegate needs to generate shaders & constants during initialization, so that takes a bunch of time. It shouldn't be as high as 2.5s for a 400kb model; which is strange. Can you share the model (even if untrained)? We are working on ways to optimize this in the future.\r\nFor now, if your model is quantized, using the NNAPI delegate will probably do a better job for you. For float models, try XNNPack.", "@alexanderar could you share the tflite model you used? To me 2.5s on Pixel 3a looks too much even though the shader compilation time is considered. I want to see what's going on there.", "@terryheo, here is the model. It is untrained but I think that it should be enough to reproduce the issue. \r\nI tested it multiple times and it took me about 3.3 sec on average on Pixel 3a device with this code (simplified):\r\n\r\n```\r\nval options = Interpreter.Options().setUseNNAPI(false)\r\n   .setNumThreads(Runtime.getRuntime().availableProcessors())\r\n   options.addDelegate(GpuDelegate())\r\n        \r\ninterpreter = Interpreter(interpreterBinary, options)\r\n```\r\n\r\n[model.tflite.zip](https://github.com/tensorflow/tensorflow/files/5826041/model.tflite.zip)\r\n"]}, {"number": 46123, "title": "Small last batches for TripletSemiHardLoss", "body": "I am using Tensorflow 2.3\r\n\r\nI am using the following code for tfrecord\r\n\r\n    dataset = tf.data.TFRecordDataset(filenames=filenames)\r\n    dataset = dataset.shuffle(buffer_size=100000)\r\n    # dataset handling API\r\n    dataset = dataset.cache()\r\n    dataset = dataset.map(parse_examples,\r\n                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.batch(50)\r\n\r\nSay if I have 101 images, then I will have 3 batches where the last batch contains only 1 element.\r\n\r\nThis will give error if I use TripletSemiHardLoss as it requires both positive and negative examples. I would like to request a convenient way to ensure \r\na. all batches have 50 elements or \r\nb. to ensure each batch has both positive and negative examples\r\n\r\n", "comments": ["@tianhuat,\r\n Can you please share the complete code and the Error Message when you are using `TripletSemiHardLoss`. Thanks!", "@tianhuat,\r\nCan you please respond to the above comment? Thanks! ", "For example, you can see this:\r\nhttps://stackoverflow.com/questions/59857152/nan-loss-in-keras-with-triplet-loss", "You can guarantee that all batches are of size 50 by passing `drop_remainder=True` to batch:\r\n\r\n```python\r\ndataset = dataset.batch(50, drop_remainder=True)\r\n```"]}, {"number": 46106, "title": " Request: offer image-enlarging library that uses AI (TensorFlow)", "body": "**System information**\r\n- TensorFlow version (you are using):\r\ntensorflow-lite-2.3.0\r\n- Are you willing to contribute it (Yes/No):\r\nNot sure how. \r\n\r\n**Describe the feature and the current behavior/state.**\r\nhttps://issuetracker.google.com/issues/176311044\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wishes to enlarge/enhance images.\r\n\r\n**Any Other info.**\r\nI know we already have a sample of it, but it's very restricted (only allows from 50x50 input images) and isn't comfortable at all to use on new projects.", "comments": ["> isn't comfortable at all to use on new projects.\r\n\r\nCan you be a bit more specific? Is it that you'd like to see a higher-level task-like API for image super-sampling?", "@jdduke Specific about what? You mean which sample? If so, this:\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/super_resolution\r\n\r\nCheck the link. I've explained there more.\r\nI think it could benefit plenty of apps to have a nice library that helps with resizing/enhancing images.\r\n", "By reading https://issuetracker.google.com/issues/176311044, I think this user request can be mapped to our existing projects by:\r\n1. Adding a new Model Maker API that customizes the super resolution training script and allows configuring input/output image size. A stretched goal, as mentioned in b/176311044, will be wrapping the script into some UI tool, such that users can simply update training images and get the custom model. \r\n2. Adding a new Task library API that can consume the generated super resolution models. \r\n\r\n@AndroidDeveloperLB what do you think? Is it something you're thinking about?", "@lu-wang-g \r\nThere are various online solutions for enhancing images, but they cost money and most of them indeed run on the server instead of locally :\r\nhttps://www.youtube.com/results?search_query=image+enhance\r\nhttps://www.google.com/search?q=enhance+image&oq=enhance\r\n\r\nI want a free solution, that will run directly on the device, no server needed.\r\nNot sure what you suggest. I offered multiple ways to have such a thing.\r\n\r\nIn the end, for most developers, a simple usage could be available, as adding just a dependency:\r\n\r\n```\r\n    implementation 'com.google.tensor-flow:super-resolution:1.0.0'\r\n```\r\n\r\n(or the core, with addition of some small extra steps)\r\n\r\nBut, it could be configured to various purposes, to reduce the size that is added to the app or to focus on specific sizes and contents, based on the needs.\r\n\r\nToday we have nothing. We have to either use the sample we've found of \"super resolution\", and only resize from 50x50 pixels, or we have to train images ourselves, and still probably be restricted to some width&height as the input. Or, as I've mentioned, we need to use some third party solution that is almost always server based or paid for.\r\n\r\nI requested : \r\n1. A more general solution, not restricted to input size of the image (not to force us to use input image of specific resolution), that will fit for most cases that developers would love to have.\r\n2. A solution with minimal effort from developers - won't require knowing how to train an AI. At most, we could just give it the images to train with. \r\n3. Since we might be dealing with mobile devices, where storage is important, think of a way to reduce it. Maybe work with Play-Services that will have a model file that is used for all app? Maybe ask us what are the specifications to create the file, and let us download it? Maybe we could tell it \"OK use up to X MB for Y purposes\" (Y can be \"animals\", \"people\", or \"general\")...\r\n4. Free. The device is doing the work, so no need for costs.\r\n\r\nI think now it's the time to do it. AI is gaining more and more popularity, and devices are quite capable of doing this job without a server side.\r\n\r\nI think there are plenty of ideas you can come up with to help developers use such a functionality. ", "As you described, [Google MLKit](https://developers.google.com/ml-kit) will be a more ideal solution for the use cases. MLKit provides turnkey solutions and users don't need to worry about any underneath ML technologies. TFLite currently focuses more on on-device ML development and customization, so some basic background of ML is required. I agree we should do more than just a sample super resolution app. In fact we are collecting user requests on the most interested tasks they want us to improve more, and super resolution was nominated quite a few times. Thanks for sharing your ideas. We'll gather all the feedback and prioritize on the most wanted cases. Please stay tuned on our future updates. ", "@lu-wang-g I don't know. Depends on the needs. \r\nThe sample is sadly even quite hard to convert to a library.\r\nHow can I vote for image-enhancing , and others? \r\nHow can I stay tuned about this?\r\n", "You can create requests like this to vote for other cases. This issue can be used as a tracking bug to track any future updates. You can also follow our roadmap: https://www.tensorflow.org/lite/guide/roadmap, for the overall TFLite plans.", "@lu-wang-g Where? Where do I vote? Where can I see other requests of others? Where can I subscribe to such requests?", "By \"vote\", I mean you can file request like this and explain the use cases you're interested. And we'll collect all the requests and make plans accordingly. We're also planning to send out a user survey to this email group: https://groups.google.com/a/tensorflow.org/g/tflite. Please subscribe to it and we'll keep you updated there.", "@lu-wang-g It was written \"super resolution was nominated quite a few times.\" , so this means it was already requested, and that I can vote on it. \r\nWhere can I write and find about this?\r\nSearching \"super\" , \"resolution\", \"enhance\" on the website doesn't return me such a thing.\r\nAnyway, I made a new request there too:\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/RLtObidqqpY", "By saying \"super resolution was nominated quite a few times\", I actually mean we've heard several request from Google internal users and ML developers. I don't think you can search from them on the website. Feel free to let us know through github / tensorflow user group (they are identical) if you have other new use cases in mind. ", "@lu-wang-g But I couldn't see even one place that it was nominated. \r\nWhat other things have people requested? Maybe from them I could think of other nice additions.\r\n"]}, {"number": 46089, "title": "Big SparseTensor constant and Datasets either turn into a large graph or a slow execution.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Testing/Sid\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.243/7.6.5\r\n- GPU model and memory: Quadro T2000 \r\n\r\nI am loading a sparse matrix via `numpy.load` with shape `[16777216  5416537]` and 335548396 values (this is the TFIDF matrix of DrQA). I want to use a dataset and use the matrix at some point of my computation. I have tried to different ways, one is simply super slow and the other one hits the 2GB limit (I assume the optimizer is kicking in and unfolding the constant matrix and overpopulation the graph, but I am not sure and my test with the config for Grappler did not work)\r\n\r\nThe slow version (I have 32GB and 16 cores available) is something like this:\r\n```python\r\nindices, values, dense_shape = load_parts_of_sparse_matrix_as_np_arrays()\r\n\r\ndef f(x, indices, values, dense_shape):\r\n  m = tf.SparseTensor(indices, values, dense_shape=dense_shape)\r\n  return do_something(x, m)\r\n\r\nds = load_ds()\r\n\r\nds = tf.data.Dataset.zip((\r\n  ds,\r\n  tf.data.Dataset.from_tensors(indices).repeat(),\r\n  tf.data.Dataset.from_tensors(values).repeat(),\r\n  tf.data.Dataset.from_tensors(dense_shape).repeat(),\r\n))\r\n\r\nds.map(f)\r\n```\r\nI guess memory is being copied at every iteration.\r\n\r\n\r\nThe following modification hits the 2gb limit\r\n```python\r\nindices, values, dense_shape = load_parts_of_sparse_matrix_as_np_arrays()\r\n\r\nm = tf.SparseTensor(indices, values, dense_shape=dense_shape)\r\n\r\ndef f(x):\r\n  return do_something(x, m)\r\n\r\nds = load_ds()\r\n\r\nds.map(f)\r\n```\r\nThis is a bit harder for me to understand. My only guess is that TF wraps the function passed to map by a `tf.function` then it trace the function, detects the constant and expands it overpopulating the graph somehow. I turned constant_folding off without success.\r\n\r\nSo far the only way I get good results is by disabling eager execution and use a placeholder for the parts of the sparse tensor, in any other way I hit the 2GB result. Sadly, I can't control this when using datasets.\r\n\r\nI tried asking for help in Stack Overflow first, but I have received no answer yet.\r\n\r\n", "comments": ["Just to add, I am using `tf.sparse.sparse_dense_matmul` to multiply a dense tensor and a sparse one. I was previously using a `py_function` and the sparse representation of the matrix was kept in the Python side with `scipy.sparse`. This was fast (between 10-20 iterations per second.) I was expecting to have similar results in TF, but moving the constant matrix to the graph results in an almost 80-160x slower performance (between 4 to 8 seconds per iteration). \r\n\r\nI can still use the `py_function` in my application, but I guess there has to be a way to make TF faster than scipy for this task.", "Extra thoughts: I realized that the slow version is extra slow because I run the `ds.map` with the argument `num_parallel_calls=32`. This creates a memory bottleneck. If I reduce from 32 to 4 it runs faster (no memory bottleneck is noticeable), but still extremely slow in comparison.", "Hi @jorgeecardona,\r\n\r\nThe second approach should work performantly as long as the sparse indices/values tensors can fit in the graph. The size of the dense matrix represented by the SparseTensor shouldn't matter, since only the indices/values/dense_shape are serialized into the graph.\r\n\r\nIf the indices/values are too large and exceed the 2GB graph size limit, I think you could work around the issue by providing the SparseTensor through a generator dataset:\r\n\r\n```python\r\nSIZE = 1000000\r\nindices, values, dense_shape = [[i, i] for i in range(SIZE)], list(range(SIZE)), [SIZE, SIZE]\r\nm = tf.SparseTensor(indices, values, dense_shape=dense_shape)\r\n\r\ndef gen():\r\n  yield m\r\n\r\ndef do_something(x, m):\r\n  return m\r\n\r\nds = tf.data.Dataset.range(10).repeat()\r\nds_m = tf.data.Dataset.from_generator(gen, output_signature=tf.SparseTensorSpec.from_value(m)).cache().repeat()\r\n\r\nds = tf.data.Dataset.zip((ds, ds_m))\r\nds = ds.map(do_something)\r\n```", "Hi @aaudiber, thank you for your answer. I updated to TF 2.4 (in 2.3 I can't use the `output_signature` argument of `from_generator`) and I still get low performance from that. The numbers are very similar to the first implementation, and I feel there is a similar memory bottleneck when I set `num_parallel_calls` to 16.\r\n\r\nCould this all be a bad implementation of the sparse multiplication (`sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul`)? At this moment is basically 10 times slower than using `py_function` and `scipy.sparse`.\r\n\r\nBest.\r\n", "Hm, each repeat() of the generator dataset will call the generator function again, which may cause the sparse tensor to be copied. Could you try adding a `.cache()` transformation to the `from_generator` dataset, right before the `.repeat()`. I just edited the example above to insert the `.cache()` in the right place.", "Hi, I don't see an improvement with the cache. This is part of the code in case it helps:\r\n```python\r\ndef closest_docs_2(ds, indices, values, dense_shape, deterministic=True):\r\n\r\n    b = tf.SparseTensor(indices, values, dense_shape=dense_shape)\r\n\r\n    def b_gen():\r\n        yield b\r\n\r\n    def f(x, b):\r\n\r\n        x = tf.sparse.reshape(x, [1, -1])\r\n\r\n        x_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n            x.indices, x.values, x.dense_shape)\r\n\r\n        b_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n            b.indices, b.values, b.dense_shape)\r\n\r\n        c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\r\n            a=x_sm, b=b_sm, type=tf.float32)\r\n\r\n        c = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\r\n            c_sm, tf.float32\r\n        )\r\n\r\n        return tf.gather(\r\n            c.indices[:,1], tf.argsort(c.values, direction='DESCENDING')\r\n        )\r\n\r\n        return tf.argsort(values, direction='DESCENDING')\r\n\r\n    ds = tf.data.Dataset.zip((\r\n        ds,\r\n        tf.data.Dataset.from_generator(\r\n            b_gen, output_signature=tf.SparseTensorSpec.from_value(b)\r\n        ).cache().repeat()\r\n    ))\r\n\r\n    # Find the closests documents.\r\n    return ds.map(f, num_parallel_calls=8, deterministic=deterministic)\r\n```\r\nThe code with scipy is not different in spirit.", "Hi, \r\n\r\nI am still running into this, I was able to multiply some sparse matrics with less values as the one previously tested and I got better performance in TF (both CPU and GPU) that scipy. But ones I try to use a similar matrix (similar in number of values as the one originally tested) I hit the protobuf limit:\r\n\r\n```bash\r\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:504] CHECK failed: (value.size()) <= (kint32max): \r\nSegmentation fault\r\n```\r\nI am now running TF 2.4.1. \r\n\r\nIt works still if I return the sparsetensor with the matrix from a py_function but this is called on each iteration of the dataset map and is very slow. If the  sparsetensor is created before the dataset.map I hit the limit (I assume the map traces the function and injects the eager sparse tensor into the graph.)\r\n"]}, {"number": 46083, "title": "ESP32-EYE person_detection example will not build", "body": "@tensorflow/micro\r\n\r\nUbuntu 20.04.1 LTS\r\nGNU Make 4.2.1\r\nESP IDF v4.2 checked out tag and ran install.sh and source export.sh\r\nCloned esp32-camera into idf components\r\n\r\nI tried several different TF commits. I ran git clean -f -x -d between TF versions.\r\n\r\n**TF master at 49524d68892d13265ec33b0e1cf7422fd71e316c**\r\nMaking the ESP project works but running \"idf.py build\" gives:\r\n```\r\nCMake Error at /home/felix/esp-idf/tools/cmake/component.cmake:468 (add_library):\r\n  Cannot find source file:\r\n\r\n    /home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/tensorflow/lite/micro/examples/person_detection_experimental/detection_responder.cc\r\n```\r\nI edited the file tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/CMakeLists.txt   and added  \"../components/tfmicro/\"  to the front of all the paths.\r\n\r\nI can now run idf.py menuconfig to set (necessary) support for array rtc_gpio_desc.  \r\nNow I run \"idf.py build\" and get:\r\n\r\n```\r\n #error \"No camera module configured, please configure in menuconfig\"\r\n```\r\nThe camera module selection is not showing in the menuconfig system when running tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/idf.py menuconfig. (should I run it from somewhere else?) \r\n\r\nI manually added \"#define CONFIG_CAMERA_MODEL_ESP_EYE 1\" in /tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/components/tfmicro/tensorflow/lite/micro/examples/person_detection/esp/app_camera_esp.h. \r\n\r\nI then run idf.py build again.  This time I get a link error:\r\n```\r\n/home/felix/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld: esp-idf/main/libmain.a(main_functions.cc.obj): in function `loop':\r\n/home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/build/../components/tfmicro/tensorflow/lite/micro/examples/person_detection/main_functions.cc:101: undefined reference to `GetImage(tflite::ErrorReporter*, int, int, int, signed char*)'\r\n```\r\n\r\nIn \"tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/components/tfmicro/tensorflow/lite/micro/examples/person_detection/esp/image_provider.cc\"  Change the definitions of \r\n```\r\nTfLiteStatus PerformCapture(tflite::ErrorReporter* error_reporter, int8_t* image_data) \r\n```\r\nand \r\n```\r\nTfLiteStatus GetImage(tflite::ErrorReporter* error_reporter, int image_width, int image_height, int channels, int8_t* image_data) \r\n```\r\nThe image_data pointer was wrongly typed as uint8_t*, changing to int8_t* fixes the undefined reference issue and the example now builds and runs! Hurray!  That was not easy.\r\n\r\n**TF v2.4.0**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_int8_esp_project\r\n```\r\ntensorflow/lite/micro/tools/make/Makefile:418: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:418: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:378: tensorflow/lite/micro/tools/make/targets/esp_makefile.inc: No such file or directory\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/esp_makefile.inc'.  Stop.\r\n```\r\nI renamed esp32_makefile.inc to esp_makefile.inc and now generation works.\r\n\r\ncd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/\r\n\r\nidf.py build fails with....\r\n```\r\nCMake Error at /home/felix/esp-idf/tools/cmake/component.cmake:468 (add_library):\r\n  Cannot find source file:\r\n\r\n    /home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/tensorflow/lite/micro/examples/person_detection_experimental/detection_responder.cc\r\n```\r\nEdit the file tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/CMakeLists.txt   add ../components/tfmicro/  to the front of all the paths\r\nI can now run idf.py menuconfig to set (necessary) support for array rtc_gpio_desc.  \r\nidf.py build now results in\r\n```\r\nfatal_exception  esp-idf/app_trace/libapp_trace.a  -lgcov  esp-idf/app_trace/libapp_trace.a  -lgcov  -lc  -lm && :\r\n/home/felix/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld: esp-idf/esp32/libesp32.a(cpu_start.c.obj):(.literal.main_task+0x18): undefined reference to `app_main'\r\n```\r\n\r\n**TF v2.3.0**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_int8_esp_project\r\ncd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/\r\nidf.py build fails with....\r\n```\r\nCMake Error at /home/felix/esp-idf/tools/cmake/component.cmake:468 (add_library):\r\n  Cannot find source file:\r\n\r\n    /home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/tensorflow/lite/micro/examples/person_detection_experimental/detection_responder.cc\r\n``` \r\n\r\nCan anyone point to a combination of dependencies and TF that this example will actually build and run for esp32? I've managed to run the hello world but I'd like a more sophisticated example to work from.", "comments": ["Hi @felixcollins this is fixed bug. tag v2.4.0 doesn't contain the fix, hence the error.\r\n\r\nPlease cherry pick following commit: https://github.com/tensorflow/tensorflow/commit/a4373953b95a298913318a9722b5d8347f8cf1e5 or switch to master.", "@vikramdattu  Did you read my description?  For TF 2.4 I said _\"I renamed esp32_makefile.inc to esp_makefile.inc and now generation works.\"_   After that it fails with a link error : \"undefined reference to `app_main'\".   I'll try again with Master and report back, but last time I tried it had similar issues. I changed to trying to get a tag to work so at least it was a stable point to work from.", "@vikramdattu I updated the issue description with my experiences with master branch.", "@vikramdattu  Yay! I got it working.  I just updated the description again. I managed to get it to build and run but as you can see there are a few issues with the project generation system and a couple of small mistakes in the image_provider.cc file for esp. ", "@felixcollins can you please raise an PR for the fix so it can be merged?", "@vikramdattu Sorry, I've been taken off this work now and do not have a fork set up to submit a PR from. The only changes I could submit would be the fix of the type the image data pointer for PerformCapture and GetImage in esp/image_provider.cc. This is simply removing the two 'u' characters from the front of the type. The rest of the build system issues I did not fix, I just hacked around them.", "Hello @felixcollins \r\nI wonder how you fix this problem. \r\n\r\nIn #47063,\r\n\r\n```\r\nmsb@msb-VirtualBox:/usr/my/tensorflow$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\nmake: *** No rule to make target 'generate_person_detection_esp_project'.  Stop.\r\n```\r\n\r\nCan you give me any advice or comments?\r\n\r\nIt's not working the beginning of the project. \r\n", "Sorry @SunBeenMoon  I spent ages hacking around getting a build to work. I recommend you come back in a few years when they have things stabilised. The TFLite Micro seems to be very bleeding edge at the moment.", "> Hello @felixcollins\r\n> I wonder how you fix this problem.\r\n> \r\n> In #47063,\r\n> \r\n> ```\r\n> msb@msb-VirtualBox:/usr/my/tensorflow$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project\r\n> tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\n> tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\n> make: *** No rule to make target 'generate_person_detection_esp_project'.  Stop.\r\n> ```\r\n> \r\n> Can you give me any advice or comments?\r\n> \r\n> It's not working the beginning of the project.\r\n\r\n@SunBeenMoon Instead of generating 'person detection\u2019, I tried generate_person_detection_int8_esp_project and had the project folder generated in \\tools\\make\\gen path, however the main folder was empty and i manually moved the files to the project folder and modified the CMakeLists.txt ", "I ran the person detection model in the example on my ESP-EYE but the performance wasn't as good as I expected, I wondered if it was because the camera needed some time to focus..., or it's just because of something else?", "Hi @yifeif what's the time it is taking per inference for you?\r\n\r\nLast time I checked it is taking 700ms with greyscale model and about 1450ms (~1.5sec) for int8 model.\r\n\r\n", "Hi @vikramdattu, I simply used the stopwatch on my smartphone and I would like to say that's about 2secs/inference (that's longer than yours hh). In fact I am following an O'Relly's book called \"TinyML\" and this example belongs to Chap 9 & 10. The book mentioned that since the person detection model is much larger, it would take longer to run inference, so I think that 2sec delay is acceptable. Plus, I believe on the Sparkfun boards the time would take even longer:  _'When the program is running the blue LED will toggle on and off, once for each inference. Because the vision model we are using for person detection is relatively\r\nlarge, it takes a long time to run inference\u2014around 6 seconds in total.'_"]}, {"number": 46080, "title": "[RNN] only LSTM not working", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): \r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n[kaggle notebook](https://www.kaggle.com/davidweingut/notebook0189756df4)\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\nlong strings are replaced by `[long string]`\r\n<details>\r\n<summary>massive log omitting weights(?)</summary>\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198                                                  debug_info_str,\r\n--> 199                                                  enable_mlir_converter)\r\n    200       return model_str\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at \"sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268\") at \"StatefulPartitionedCall@__inference_signature_wrapper_7157\") at \"StatefulPartitionedCall\")): We cannot duplicate the value since it's not constant.\r\n\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(callsite(unknown at \"sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268\") at \"StatefulPartitionedCall@__inference_signature_wrapper_7157\") at \"StatefulPartitionedCall\")): see current operation: %6 = \"tfl.unidirectional_sequence_lstm\"(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = \"TANH\", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>\r\n<unknown>:0: error: Failed to duplicate values for the stateful op\r\n\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<1x?xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<49x128xf32>} : () -> tensor<49x128xf32>\r\n  %cst_0 = \"std.constant\"() {value = dense<[0.0391180739, -0.035346359, -0.0317100026, -0.0274537932, -2.994480e-02, -0.0300172977, -0.0454840586, -0.0257392451, -0.030700814, -0.0278897285, -0.0355975218, -0.0340279937, -0.0132417707, -0.0220720414, -0.0170032419, 0.0107899494, -0.0109964302, 0.0110128662, -0.00513247494, 0.0186021626, 0.0219315775, 0.0235501211, 0.0213683285, 0.0133540835, 3.695680e-02, 0.0311833415, 0.0377812907, 0.0342479311, 0.0296398606, 0.0385234021, 0.0154196564, 0.0205324832, 0.0297776293, 0.00336526474, 0.0244599823, 0.00573094795, 0.0279730614, 0.0106867487, -0.011602954, 0.0209239982, -0.0380180553, 0.00730686448, -0.0307899173, -0.0395634323, -0.0275964253, -0.0415802076, -5.012860e-02, -0.0420668162, 0.021869421]> : tensor<49xf32>} : () -> tensor<49xf32>\r\n  %cst_1 = \"std.constant\"() {value = dense<49> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_2 = \"std.constant\"() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_3 = \"std.constant\"() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_4 = \"std.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %cst_5 = \"std.constant\"() {value = dense<512> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_6 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_7 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_8 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_9 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_10 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_11 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_12 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_13 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_14 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_15 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_16 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_17 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_18 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<49x512xf32>} : () -> tensor<49x512xf32>\r\n  %cst_19 = \"std.constant\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_20 = \"std.constant\"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_21 = \"std.constant\"() {value} : () -> none\r\n  %0 = \"tfl.cast\"(%arg0) : (tensor<1x?xf32>) -> tensor<1x?xi32>\r\n  %1 = \"tfl.gather\"(%cst, %0) {axis = 0 : i32} : (tensor<49x128xf32>, tensor<1x?xi32>) -> tensor<1x?x128xf32>\r\n  %2 = \"tfl.shape\"(%1) : (tensor<1x?x128xf32>) -> tensor<3xi32>\r\n  %3 = \"tfl.strided_slice\"(%2, %cst_19, %cst_20, %cst_20) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %4 = \"tfl.pack\"(%3, %cst_5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %5 = \"tfl.fill\"(%4, %cst_4) : (tensor<2xi32>, tensor<f32>) -> tensor<?x512xf32>\r\n  %6 = \"tfl.unidirectional_sequence_lstm\"(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = \"TANH\", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>\r\n  %7 = \"tfl.shape\"(%6) : (tensor<1x?x512xf32>) -> tensor<3xi32>\r\n  %8 = \"tfl.gather\"(%7, %cst_3) {axis = 0 : i32} : (tensor<3xi32>, tensor<2xi32>) -> tensor<2xi32>\r\n  %9 = \"tfl.reduce_prod\"(%8, %cst_19) {keep_dims = false} : (tensor<2xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %10 = \"tfl.concatenation\"(%8, %cst_1) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<2xi32>, tensor<1xi32>) -> tensor<3xi32>\r\n  %11 = \"tfl.gather\"(%7, %cst_2) {axis = 0 : i32} : (tensor<3xi32>, tensor<1xi32>) -> tensor<1xi32>\r\n  %12 = \"tfl.reduce_prod\"(%11, %cst_19) {keep_dims = false} : (tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %13 = \"tfl.pack\"(%9, %12) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %14 = \"tfl.reshape\"(%6, %13) : (tensor<1x?x512xf32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %15 = \"tfl.fully_connected\"(%14, %cst_18, %cst_21) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x?xf32>, tensor<49x512xf32>, none) -> tensor<?x49xf32>\r\n  %16 = \"tfl.reshape\"(%15, %10) : (tensor<?x49xf32>, tensor<3xi32>) -> tensor<?x?x?xf32>\r\n  %17 = \"tfl.add\"(%16, %cst_0) {fused_activation_function = \"NONE\"} : (tensor<?x?x?xf32>, tensor<49xf32>) -> tensor<?x?x49xf32>\r\n  \"std.return\"(%17) : (tensor<?x?x49xf32>) -> ()\r\n}) {arg0 = {tf_saved_model.index_path = [\"embedding_1_input\"]}, result0 = {tf_saved_model.index_path = [\"dense_1\"]}, sym_name = \"serving_default\", tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_embedding_1_input:0\", outputs = \"StatefulPartitionedCall:0\"}, tf_saved_model.exported_names = [\"serving_default\"], type = (tensor<1x?xf32>) -> tensor<?x?x49xf32>} : () -> ()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-20-6750f334da79> in <module>\r\n      1 converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\n----> 2 tflite_model = converter.convert()\r\n      3 \r\n      4 with open('chopin.tflite', 'wb') as f:\r\n      5   f.write(tflite_model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    708     return super(TFLiteSavedModelConverterV2,\r\n    709                  self).convert(meta_graph.graph_def, input_tensors,\r\n--> 710                                output_tensors)\r\n    711 \r\n    712 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    631         input_tensors=input_tensors,\r\n    632         output_tensors=output_tensors,\r\n--> 633         **converter_kwargs)\r\n    634 \r\n    635     calibrate_and_quantize, flags = quant_mode.quantizer_flags(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    572       input_data.SerializeToString(),\r\n    573       debug_info_str=debug_info_str,\r\n--> 574       enable_mlir_converter=enable_mlir_converter)\r\n    575   return data\r\n    576 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    200       return model_str\r\n    201     except Exception as e:\r\n--> 202       raise ConverterError(str(e))\r\n    203 \r\n    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at \"sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268\") at \"StatefulPartitionedCall@__inference_signature_wrapper_7157\") at \"StatefulPartitionedCall\")): We cannot duplicate the value since it's not constant.\r\n\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(callsite(unknown at \"sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268\") at \"StatefulPartitionedCall@__inference_signature_wrapper_7157\") at \"StatefulPartitionedCall\")): see current operation: %6 = \"tfl.unidirectional_sequence_lstm\"(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = \"TANH\", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>\r\n<unknown>:0: error: Failed to duplicate values for the stateful op\r\n\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<1x?xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<49x128xf32>} : () -> tensor<49x128xf32>\r\n  %cst_0 = \"std.constant\"() {value = dense<[0.0391180739, -0.035346359, -0.0317100026, -0.0274537932, -2.994480e-02, -0.0300172977, -0.0454840586, -0.0257392451, -0.030700814, -0.0278897285, -0.0355975218, -0.0340279937, -0.0132417707, -0.0220720414, -0.0170032419, 0.0107899494, -0.0109964302, 0.0110128662, -0.00513247494, 0.0186021626, 0.0219315775, 0.0235501211, 0.0213683285, 0.0133540835, 3.695680e-02, 0.0311833415, 0.0377812907, 0.0342479311, 0.0296398606, 0.0385234021, 0.0154196564, 0.0205324832, 0.0297776293, 0.00336526474, 0.0244599823, 0.00573094795, 0.0279730614, 0.0106867487, -0.011602954, 0.0209239982, -0.0380180553, 0.00730686448, -0.0307899173, -0.0395634323, -0.0275964253, -0.0415802076, -5.012860e-02, -0.0420668162, 0.021869421]> : tensor<49xf32>} : () -> tensor<49xf32>\r\n  %cst_1 = \"std.constant\"() {value = dense<49> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_2 = \"std.constant\"() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_3 = \"std.constant\"() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_4 = \"std.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %cst_5 = \"std.constant\"() {value = dense<512> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_6 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_7 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_8 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_9 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x512xf32>} : () -> tensor<512x512xf32>\r\n  %cst_10 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_11 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_12 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_13 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512xf32>} : () -> tensor<512xf32>\r\n  %cst_14 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_15 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_16 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_17 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<512x128xf32>} : () -> tensor<512x128xf32>\r\n  %cst_18 = \"std.constant\"() {value = dense<\"[long string]\"> : tensor<49x512xf32>} : () -> tensor<49x512xf32>\r\n%cst_19 = \"std.constant\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_20 = \"std.constant\"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_21 = \"std.constant\"() {value} : () -> none\r\n  %0 = \"tfl.cast\"(%arg0) : (tensor<1x?xf32>) -> tensor<1x?xi32>\r\n  %1 = \"tfl.gather\"(%cst, %0) {axis = 0 : i32} : (tensor<49x128xf32>, tensor<1x?xi32>) -> tensor<1x?x128xf32>\r\n  %2 = \"tfl.shape\"(%1) : (tensor<1x?x128xf32>) -> tensor<3xi32>\r\n  %3 = \"tfl.strided_slice\"(%2, %cst_19, %cst_20, %cst_20) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %4 = \"tfl.pack\"(%3, %cst_5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %5 = \"tfl.fill\"(%4, %cst_4) : (tensor<2xi32>, tensor<f32>) -> tensor<?x512xf32>\r\n  %6 = \"tfl.unidirectional_sequence_lstm\"(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = \"TANH\", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>\r\n  %7 = \"tfl.shape\"(%6) : (tensor<1x?x512xf32>) -> tensor<3xi32>\r\n  %8 = \"tfl.gather\"(%7, %cst_3) {axis = 0 : i32} : (tensor<3xi32>, tensor<2xi32>) -> tensor<2xi32>\r\n  %9 = \"tfl.reduce_prod\"(%8, %cst_19) {keep_dims = false} : (tensor<2xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %10 = \"tfl.concatenation\"(%8, %cst_1) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<2xi32>, tensor<1xi32>) -> tensor<3xi32>\r\n  %11 = \"tfl.gather\"(%7, %cst_2) {axis = 0 : i32} : (tensor<3xi32>, tensor<1xi32>) -> tensor<1xi32>\r\n  %12 = \"tfl.reduce_prod\"(%11, %cst_19) {keep_dims = false} : (tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %13 = \"tfl.pack\"(%9, %12) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %14 = \"tfl.reshape\"(%6, %13) : (tensor<1x?x512xf32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %15 = \"tfl.fully_connected\"(%14, %cst_18, %cst_21) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x?xf32>, tensor<49x512xf32>, none) -> tensor<?x49xf32>\r\n  %16 = \"tfl.reshape\"(%15, %10) : (tensor<?x49xf32>, tensor<3xi32>) -> tensor<?x?x?xf32>\r\n  %17 = \"tfl.add\"(%16, %cst_0) {fused_activation_function = \"NONE\"} : (tensor<?x?x?xf32>, tensor<49xf32>) -> tensor<?x?x49xf32>\r\n  \"std.return\"(%17) : (tensor<?x?x49xf32>) -> ()\r\n}) {arg0 = {tf_saved_model.index_path = [\"embedding_1_input\"]}, result0 = {tf_saved_model.index_path = [\"dense_1\"]}, sym_name = \"serving_default\", tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_embedding_1_input:0\", outputs = \"StatefulPartitionedCall:0\"}, tf_saved_model.exported_names = [\"serving_default\"], type = (tensor<1x?xf32>) -> tensor<?x?x49xf32>} : () -> ()\r\n```\r\n\r\n</details>\r\n\r\n**Failure details**\r\nConversion of SimpleRNN and GRU works when not stateful, but LSTM does not.\r\n", "comments": ["I think the const-folding does not happen in this case:\r\n\r\n```\r\n%2 = \"tfl.shape\"(%1) : (tensor<1x?x128xf32>) -> tensor<3xi32>\r\n%3 = \"tfl.strided_slice\"(%2, %cst_19, %cst_20, %cst_20) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n%4 = \"tfl.pack\"(%3, %cst_5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n%5 = \"tfl.fill\"(%4, %cst_4) : (tensor<2xi32>, tensor<f32>) -> tensor<?x512xf32>\r\n```\r\n\r\n`%2` should yield an array of [1, None, 128] and the strided_slice should just output 1 in this case.", "Hi Jacques, do you have any idea how to address this one?\r\n\r\nthanks", "You couldn't really fold the shape operation there, what one would need is to make the strided slice folder smarter. Now there is a gradient here from just doing a special case for shape to adding a (and I don't know what the best naming is) \"subvalue query\" interface (see in the shape_inference.cc the todo and query methods). But here I'd also go up a level and look at the origin that produced the shape & slice, it could have been clearer before lowered to TFL (it may also have been identical).", "@dave7895 Maybe fixing the shape of the given input can solve the problem since the necessary constant folding has not conducted due to the partial known shape.", "@abattery that could help, going to try it when I have time. \nWas rather wondering why would work with the other RNNs and not the LSTM, therefore using GRUs now anyways", "I think I have taken the appropiate steps and now the error changed, not sure what happened.\r\n[notebook](https://www.kaggle.com/davidweingut/notebook0189756df4/data?scriptVersionId=53483100)\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198                                                  debug_info_str,\r\n--> 199                                                  enable_mlir_converter)\r\n    200       return model_str\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: <unknown>:0: error: loc(\"lstm_2/Variable\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n<unknown>:0: note: loc(\"lstm_2/Variable\"): see current operation: \"tf_saved_model.global_tensor\"() {is_mutable, sym_name = \"lstm_2/Variable\", type = tensor<1x512xf32>, value = dense<0.000000e+00> : tensor<1x512xf32>} : () -> ()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-24-6750f334da79> in <module>\r\n      1 converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\n----> 2 tflite_model = converter.convert()\r\n      3 \r\n      4 with open('chopin.tflite', 'wb') as f:\r\n      5   f.write(tflite_model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    708     return super(TFLiteSavedModelConverterV2,\r\n    709                  self).convert(meta_graph.graph_def, input_tensors,\r\n--> 710                                output_tensors)\r\n    711 \r\n    712 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    631         input_tensors=input_tensors,\r\n    632         output_tensors=output_tensors,\r\n--> 633         **converter_kwargs)\r\n    634 \r\n    635     calibrate_and_quantize, flags = quant_mode.quantizer_flags(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    572       input_data.SerializeToString(),\r\n    573       debug_info_str=debug_info_str,\r\n--> 574       enable_mlir_converter=enable_mlir_converter)\r\n    575   return data\r\n    576 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    200       return model_str\r\n    201     except Exception as e:\r\n--> 202       raise ConverterError(str(e))\r\n    203 \r\n    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(\"lstm_2/Variable\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n<unknown>:0: note: loc(\"lstm_2/Variable\"): see current operation: \"tf_saved_model.global_tensor\"() {is_mutable, sym_name = \"lstm_2/Variable\", type = tensor<1x512xf32>, value = dense<0.000000e+00> : tensor<1x512xf32>} : () -> ()\r\n```"]}, {"number": 46077, "title": "Assigning values to variables in Tensorflow graphs using the C API", "body": "I am attempting to assign a value to a variable in a Tensorflow graph loaded using the C API. When I export a graph using Tensorflow 2.x it contains the following ops related to a particular variable:\r\n\r\n```\r\nvariable_name type: VarHandleOp device:  number inputs: 0 number outputs: 1\r\nNumber inputs: 0\r\nNumber outputs: 1\r\n dims: 0 []\r\n\r\nvariable_name/Read/ReadVariableOp type: ReadVariableOp device:  number inputs: 1 number outputs: 1\r\nNumber inputs: 1\r\nNumber outputs: 1\r\n dims: 0 []\r\n```\r\nAccording to this video (https://www.youtube.com/watch?v=uaRO0AV6Tto) variables in TF 2.x now use VarHandleOps in order to mediate write access to them. The TF test suite for the C API is the only example I've found so far of using a VarHandleOp to access a variable but it is created from scratch using the Eager API (https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/c/eager/c_api_test.cc#L409).\r\n\r\nI would like to be able to load a graph and assign to a variable already contained within it, is this even possible with the Eager API? When I try using the older graph API I can fetch the VarHandleOp like so:\r\n```\r\nTF_Operation* op = TF_GraphOperationByName(graph, \"variable_name\");\r\n```\r\nBut then I can't figure out how to run this op to return a TFE_TensorHandle (like here https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/c/eager/c_api_test.cc#L418), a type which doesn't even seem to exist in the graph API. TF_SessionRun()'s inputs and outputs deal in TF_Output and TFE_Execute expects a TFE_Op rather than the TF_Operation I have access to.\r\n\r\nI think this may be a possible bug caused by the introduction of VarHandleOps in the Eager API but no corresponding mechanism in the graph API, not sure if this is the correct place to post it - please let me know if there's a better place.\r\n\r\nAny tips or pointers gratefully received!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46077\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46077\">No</a>\n", "Thanks for the response @rohan100jain , I posted it on Stack Overflow a while back (https://stackoverflow.com/questions/65509638/how-to-assign-values-to-variables-in-tensorflow-graphs-using-the-c-api) but I'm now fairly sure it's a bug introduced in TF 2.x as described below in the issue template as requested.\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes, there is no stock example showing how to assign to a variable in an imported graph using the C API\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.15.7\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**: C API libtensorflow-cpu-darwin-x86_64-2.3.0 from https://www.tensorflow.org/install/lang_c\r\n-   **TensorFlow version (use command below)**: 2.3.0\r\n-   **Python version**: 3.8.6\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**: N/A\r\n-   **CUDA/cuDNN version**: N/A, using CPU only\r\n-   **GPU model and memory**: N/A, using CPU only\r\n-   **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nIt is not possible to assign a value to a variable in an imported graph using the C API. This seems to be an oversight caused by the introduction of VarHandleOps in the eager API of TF 2.x, but no equivalent for the graph API of TF 2.x.\r\n\r\nThis is a regression on 1.x behaviour, variables should be able to be varied.\r\n\r\n### Source code / logs\r\nAccording to this video (https://www.youtube.com/watch?v=uaRO0AV6Tto) variables in TF 2.x now use VarHandleOps in order to mediate write access to them. The TF test suite for the C API is the only example I've found so far of using a VarHandleOp to access a variable but it is created from scratch using the Eager API (https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/c/eager/c_api_test.cc#L409).\r\n\r\nI would like to be able to load a graph and assign to a variable already contained within it, is this even possible with the Eager API? When I try using the older graph API I can fetch the VarHandleOp like so:\r\n\r\nTF_Operation* op = TF_GraphOperationByName(graph, \"variable_name\");\r\n\r\nBut then I can't figure out how to run this op to return a TFE_TensorHandle (like here https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/c/eager/c_api_test.cc#L418), a type which doesn't even seem to exist in the graph API. TF_SessionRun()'s inputs and outputs deal in TF_Output and TFE_Execute expects a TFE_Op rather than the TF_Operation I have access to.\r\n\r\nI think this may be a possible bug caused by the introduction of VarHandleOps in the Eager API but no corresponding mechanism in the graph API.", "Hi @saxenasaurabh, please let me know if there's anything I can do to help investigate/solve this :) I'm at a dead end for what I can do by myself as far as I can see", "This looks promising, not sure how to use it yet though:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/experimental/saved_model/core/ops/variable_ops.h"]}, {"number": 46073, "title": "Cannot quantize part of a model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0-dev20201208\r\n- Python version: 3.7\r\n\r\n**Standalone code to reproduce the issue**\r\nI follow the tutorial [here](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide) to see if I can only quantize part of a model. Here is my code:\r\n\r\n```\r\ni = tf.keras.Input(shape=(20,))\r\nx = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))(i)\r\nx = tf.keras.layers.Dense(10)(x)\r\nx = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))(x)\r\no = tf.keras.layers.Flatten()(x)\r\nannotated_model = tf.keras.Model(inputs=i, outputs=o)\r\nquant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nquantized_tflite_model = converter.convert()\r\npathlib.Path('/tmp/tmp.tflite').write_bytes(quantized_tflite_model)\r\n```\r\nBasically I want to quantize the 1st and 3rd dense layers of a model. And here is how the resultant model looks like:\r\n<img width=\"237\" alt=\"Screen Shot 2020-12-30 at 8 02 19 PM\" src=\"https://user-images.githubusercontent.com/23546158/103350164-0741a280-4ada-11eb-9c0e-713bb2ceddca.png\">\r\nApparently, the 2nd quantization goes to the wrong place...", "comments": ["@daverim @teijeong can you help this since you are assigned to this thread? I think it's a very basic function but seems doesn't work...", "Was able to reproduce the issue with TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/edfc37a8c5d517725e198274deed71e6/46073.ipynb). \r\n![tmp tflite](https://user-images.githubusercontent.com/57165142/103513806-5ea49180-4e91-11eb-9e29-2c4f693fb066.png)\r\n\r\nThanks!", "@amahendrakar thanks for confirming. While you are working on this, can you also elaborate which part of the codebase can be the reason? Maybe the toco converter? I'd also want to take a look at this issue.", "Something in the converter, definitely --- not sure why, still investigating... will update soon", "@daverim Thanks! I went through the quantization code (python part), and have a feeling that the [tutorial](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide) about \"quantizing some layers\" is for weights-only quantization, while I'm looking for quantizing the whole layer so it can be delegated to DSP. Is that right?\r\nIn addition, where I can find any document about how toco does graph transformation, e.g., remove fake quant op, adding quant/dequant ops, etc. I feel there's some disconnection between the high-level quant APIs and toco's model conversion logics.", "hi @xumengwei,\r\n\r\nIt turns out that there is kind of an issue in QAT. The input (other than an Input layer) to a quantized annotated layer must also be quantized annotated in order to be fully quantized -- this is because we need to record the min/max ranges of the input to the layer in order to quantize the layer. \r\n\r\nBecause you have the output of a non-quantized dense layer as input to a quantized layer, you end up with two float dense layers. If you add another layer the next layer will be quantized, as the input is quantized\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef model():\r\n  i = tf.keras.Input(shape=(20,))\r\n  quantized = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10, activation='relu'))(i)\r\n  non_quantized = tf.keras.layers.Dense(10, activation='relu')(quantized)\r\n  also_non_quantized = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10, activation='relu'))(non_quantized)\r\n  quantized_again = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10, activation='relu'))(also_non_quantized)\r\n  o = tf.keras.layers.Flatten()(quantized_again)\r\n  model = tf.keras.Model(inputs=i, outputs=o)\r\n  quantize_aware_model = tfmot.quantization.keras.quantize_apply(model)\r\n  return quantize_aware_model\r\n\r\nquantize_aware_model = model()\r\nquantize_aware_model.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quantized)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nquantized_tflite_model = converter.convert()\r\nopen('tmp.tflite', 'wb').write(bytearray(quantized_tflite_model))\r\n```\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/2448933/103981318-f02f3e80-51c4-11eb-884b-4cc409430224.png)\r\n\r\nThis is a bug that I am working on addressing in QAT, and not really a tflite error, as the recording ops (fakequant) is not added to input tensors when expected -- in the meantime, you can use this information to change the structure of your model just be aware that for a layer to be quantized its input must also be quantized. In your case, you would need to add an additional quantized op before the layer you want to have quantized.\r\n\r\nAs for conversion logic, the new logic is basically implemented as tensorflow graphdef -> tensorflow mlir -> tflite mlir -> tflite flatbuffer. For more information you can take a look at https://www.tensorflow.org/mlir and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/README.md.\r\n\r\nHope that helps, will update on this later\r\n\r\n", "@daverim thanks for digging into it! Regarding your revised code, I have a confusion: the layer `also_non_quantized ` is annotated as quantized; although it's not really quantized because its input is not quantized, but won't its weights be quantized anyway? If not, what's the way to do weights-only quantization for certain layers?", "@xumengwei  Try this\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef get_file_size(file_path):\r\n    size = os.path.getsize(file_path)\r\n    return size\r\n    \r\ndef convert_bytes(size, unit=None):\r\n    if unit == \"KB\":\r\n        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\r\n    elif unit == \"MB\":\r\n        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\r\n    else:\r\n        return print('File size: ' + str(size) + ' bytes')\r\n\r\ndef convert_model_to_tflite(model_path = \"model.h5\", filename = f\"model.tflite\", modelname = \"model\"):\r\n  model = model_load(model_name=modelname, weights=model_path)\r\n  fixed_input = Input((230,230,3))\r\n  fixed_model = Model(fixed_input,model(fixed_input))\r\n  converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n  converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n  converter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.\r\n    tf.float16\r\n  ]\r\n  tflite_model = converter.convert()\r\n  open(filename, \"wb\").write(tflite_model)\r\n  print(convert_bytes(get_file_size(f\"model.tflite\"), \"MB\"))\r\n```\r\n\r\n```\r\n>>> File size: 9.698 Megabytes\r\n>>> None\r\n```", "Yes, quantize part of the model is very important to some kind of net like landmark detection ", "Hi @xumengwei !\r\nWe are checking to see if you still need help in this issue , Have you tried @ashishpatel26's suggestions yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46073\">No</a>\n", "It seems to me that the issue of being able to manually select which weights to quantize when using the `TFLiteConverter` is still not resolved. There seems to be a disconnect between the tenforflow_model_optimization team, which says you can [quantize some layers (i.e., pick-and-choose which ones to quantize)](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide#quantize_some_layers), and the tflite team, which doesn't mention this capability anywhere. **Is it possible to only quantize the weights of, e.g., 4 out of 5 layers in a tflite model?**", "It is possible to quantize per-layer using quantized aware training as you mention. You can use the quantize_annotate_layer as mentioned for during training.\r\n\r\nFor post-training quantization, you can use the quantization debugger: https://www.tensorflow.org/lite/performance/quantization_debugger#selective_quantization\r\n\r\nPlease leave feedback if you are interested in this.", "Thanks for your reply @daverim! I'm aware of the per-layer quantization available during training via `quantize_annotate_layer`, but what I'm after is a way to only quantize certain weights of a tflite model. For example, if I have a model with 5 LSTM layers, I'd like to have the first layer run entirely at float32 but the weights for layers 2-5 to be quantized to int8. Is this possible? Whether or not the activations and inputs/outputs are quantized is a separate topic--for now I'm just focused on weight & bias quantization for tflite.", "Reopening this issue .", "Update: It seems like you can perform [selective quantization](https://www.tensorflow.org/lite/performance/quantization_debugger#selective_quantization) with the `QuantizationDebugger`.  However, you have to know the exact node(s) you'd like to leave at float32, which isn't a trivial task because the nodes don't have names that clearly map them back to Keras layers. I'm also finding that the `QuantizationDebugger` is flaky with custom architectures.\r\n\r\nRegardless, this capability isn't built into the `TFLiteConverter`. It seems that one could build a wrapper around `tensorflow.lite.python.convert.mlir_quantize`, but this feels like a missing feature of the TFLite library given all the emphasis on selective quantization both in the debugger and in the QAT fake quant nodes.", "Any update on adding selective quantization of layers to TfLiteConverter, I'd like to see this added too"]}, {"number": 46070, "title": "Make `TextVectorization` support pickling", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version Tensorflow 2.2:\r\n- Are you willing to contribute it No:\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nMake `TextVectorization` support pickling, currently it does not support pickling:\r\n```\r\nimport cloudpickle\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\nptf = cloudpickle.dumps(TextVectorization())\r\n```\r\nGet error like `Cannot convert a Tensor of dtype resource to a NumPy array.`\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nSupport pickling will make it work with some distributed framework such as `Horovod`. Such framework usually require to pickle the user defined training function to remote and executing. So pickling is required.\r\n\r\n**Any Other info.**\r\n\r\nN/A", "comments": []}, {"number": 46065, "title": "Support Relu activation function with CUDA cuDNN gpu accleeration  instead of just tanh", "body": "**System information**\r\n- TensorFlow version (you are using):\r\n   2.4\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe docs say that cuDNN requires tanh for some reason although its not clear why this limitation exists:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#used-in-the-notebooks_1\r\n\r\nTensorflow should support more activation functions like Relu when using cuDNN.  On the Nvidia cuDNN side, there is no limitation with using relu\r\n\r\nhttps://developer.nvidia.com/blog/optimizing-recurrent-neural-networks-cudnn-5/\r\n\r\nRight now I am being forced to use tanh instead of relu simply because of the fact that the performance of cuDNN with tanh is like 5 orders of magnitude better for training in terms of speed compared to the generic kernel. \r\n\r\nIn summary, please remove the validation and requirements check for cuDNN that prevents you from using relu. Thank you.\r\n\r\n**Will this change the current api? How?**\r\nIt should not affect the API at all\r\n\r\n**Who will benefit with this feature?**\r\nHuge performance benefits if able to use cuDNN instead of the generic tensorflow kernel, so anyone with an nvidia GPU would benefit.\r\n**Any Other info.**\r\n", "comments": ["The cuDNN docs say that it supports four types of rnn cell, and the **relu activation single-gated rnn cell** is one of it. However, the relu activation (three-gated) GRU cell is not included in cuDNN. `CUDNN_GRU` (and `CUDNN_LSTM`) descriptor is tied to tanh activation. You can see [cudnnRNNMode](https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNMode_t) for the more detailed fomula of each cell type."]}]