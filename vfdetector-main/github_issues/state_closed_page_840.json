[{"number": 28322, "title": "Do not copy cublas header files again", "body": "...even if the specified path is ifferent. This is a little awkward because we might pick up the header from the wrong (the CUDA toolkit instead of the cuBLAS) directory.\r\n\r\nPiperOrigin-RevId: 245996105", "comments": []}, {"number": 28321, "title": "A fix to RequantizationPerChannel Op", "body": "  - Added registration for output type: quint8", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28321) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28321) for more info**.\n\n<!-- cla_yes -->", "Approving CLA as this is a cherrypick of a commit already in master.", "@penpornk yes, I am ok with the commit being used here."]}, {"number": 28320, "title": "GetElasedTimeUs -> GetElapsedTimeUs", "body": "Fixes tricky typo in function name", "comments": []}, {"number": 28319, "title": "Update documentation template to match recommendations from doc sprints", "body": "Example issue from @dynamicwebpaige this template is based on - https://github.com/tensorflow/tensorflow/issues/25794\r\n\r\nTF API 2.0 Symbol documentation: https://docs.google.com/document/d/1e20k9CuaZ_-hp25-sSd8E8qldxKPKQR-SkwojYr_r-U/edit\r\n\r\nTensorflow doc sprint tasks: https://docs.google.com/spreadsheets/d/1p3vqbocbKmcZQGrlxk9m2jHuleu8yr-XRbfKum0IDD8/edit#gid=0\r\n\r\nFrom Gwitter conversation:\r\nAugustina Ragwitz @missaugustina 12:51\r\nThere is a link to file a new docs issue in the README for the docs repo that uses a template that I'm not sure is current. I can file a bug for this against the docs repo - https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md\r\nor maybe i can't.. .guess it would have to go in the main TF repo\r\n\r\nBilly Lamberta @lamberta 13:17\r\nThe template shown yesterday was a proposal, but I agree it's better than what we have now. @margaretmz or @lc0 (or whoever) , can you make a PR for the new docs issue template on tensorflow/tensorflow? I think that file is here: https://github.com/tensorflow/tensorflow/blob/master/.github/ISSUE_TEMPLATE/20-documentation-issue.md\r\n\r\nAugustina Ragwitz @missaugustina 13:19\r\nI can do that\r\ni just need the new suggested template\r\ni didn't get a link yesterday, was it shared? i don't remember seeing it\r\n\r\nBilly Lamberta @lamberta 13:20\r\nThanks. Yeah, I don't have it, I believe Margaret and Sergii have it\r\n\r\nAugustina Ragwitz @missaugustina 13:22\r\ni'll start a PR\r\ni have the one Paige used in her example update, if anyone wants to suggest the new one I can update or someone else can", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28319) for more info**.\n\n<!-- need_sender_cla -->", "I've signed the CLA", "@missaugustina Thank you for proposing it! I think it should definitely save some time to people attending the sprints.\r\n\r\n@lamberta I just found, that github also supports `labels` for template. Should be try to automatically add docs label? What do you think?", "In the tensorflow repo, not everyone has access to set labels, so I'm not sure how it would work in a template.", "I signed it", "I'll submit the label as a separate PR since we're not 100% certain how permissions would work", "I signed it", "Having some CLA issues and I've started an internal thread to see where the problem is. Sorry about that, will get it figured out", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28319) for more info**.\n\n<!-- ok -->", "Rebased to a single commit & updated with requested changes.", "I checked the errors on the failed builds, are those experimental queues? The errors don't appear (to me) to have anything to do with changing the doc template in the .github dir.", "@missaugustina The CI  builds runs a snapshot of the tensorflow so it may be picking up errors that were not necessarily caused by this PR. I'm following this as it moves through the process, and it should be fine, but PRs to tensorflow/tensorflow can take a while. Sorry!"]}, {"number": 28318, "title": "[ TF 2.0 ] tf.function throws exception", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): pip install tensorflow-gpu==2.0.0-alpha0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10/cuDNN 7.5\r\n- GPU model and memory:  GeForce GTX 980 4G\r\n\r\n**Describe the current behavior**\r\nI ported Deep Deterministic Policy Gradient algorithm to tf-2.0 and it works fine when I run the function with\r\n`with tf.device(\"/gpu:0\"):` but fails when I try to wrap the function with tf.function\r\n\r\n**Code to reproduce the issue**\r\n[Code Repo](https://github.com/Jonathan-Livingston-Seagull/DDPG)\r\n**Other info / logs**\r\n```\r\n/miniconda3/envs/rl/bin/python /media/seagull/use_me/seagull/github/DDPG/ddpg-pendulam/pendulam_train_script.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0501 21:58:48.871961 139821025609536 tf_logging.py:161] Entity <bound method RoboschoolMujocoXmlEnv.reset of <roboschool.gym_pendulums.RoboschoolInvertedPendulum object at 0x7f2a9ccfe780>> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <bound method RoboschoolMujocoXmlEnv.reset of <roboschool.gym_pendulums.RoboschoolInvertedPendulum object at 0x7f2a9ccfe780>>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: node <gast.gast.Attribute object at 0x7f2a2043dcf8> has ctx unset\r\nTraceback (most recent call last):\r\n  File \"/media/seagull/use_me/seagull/github/DDPG/ddpg-pendulam/pendulam_train_script.py\", line 39, in <module>\r\n    scores = ddpg()\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 426, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1580, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1512, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 317, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 686, in wrapper\r\n    ), args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 392, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/tmp0cbzqvh_.py\", line 51, in tf__ddpg\r\n    ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (1, n_episodes + 1), {}), None, loop_body_1, ())\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 81, in for_stmt\r\n    return _py_for_stmt(iter_, extra_test, body, init_state)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 90, in _py_for_stmt\r\n    state = body(target, *state)\r\n  File \"/tmp/tmp0cbzqvh_.py\", line 37, in loop_body_1\r\n    state, score, break__1 = ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (max_t,), {}), extra_test, loop_body, (state, score, break__1))\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 81, in for_stmt\r\n    return _py_for_stmt(iter_, extra_test, body, init_state)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 90, in _py_for_stmt\r\n    state = body(target, *state)\r\n  File \"/tmp/tmp0cbzqvh_.py\", line 21, in loop_body\r\n    action = ag__.converted_call('act', agent, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), ([state_1],), {})[0]\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 392, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/tmpsruok5vd.py\", line 6, in tf__act\r\n    action = ag__.converted_call('actor_local', self, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (tf.convert_to_tensor(state),), {})\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1108, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1186, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 304, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 245, in constant\r\n    allow_broadcast=True)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 283, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 476, in make_tensor_proto\r\n    _GetDenseDimensions(values)))\r\nValueError: Argument must be a dense tensor: [array([0.        , 0.        , 0.99759441, 0.06932093, 0.        ])] - got shape [1, 5], but wanted [1].\r\n```", "comments": ["@Jonathan-Livingston-Seagull, In order to expedite the trouble-shooting process, Can you please provide minimum code snippet to reproduce the issue reported here. Thanks!", "When I tried to make the following minimal example, I have found the reason for the reported issue. However I get another exception after solving that one. Please find the minimal code below. The source of the error seems like calling 'np.isfinite()' method inside the tf function definition. In my real code this method is being called by a third party lib that I use inside my tf function definition. \r\n```\r\nimport random\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nclass Actor(tf.keras.Model):\r\n    def __init__(self, action_size, fc1_units=400, fc2_units=300):\r\n        super(Actor, self).__init__()\r\n        self.fc1 = tf.keras.layers.Dense(units=fc1_units, activation=tf.nn.relu)\r\n        self.fc2 = tf.keras.layers.Dense(units=fc2_units, activation=tf.nn.relu)\r\n        self.fc3 = tf.keras.layers.Dense(action_size, activation=tf.nn.tanh)\r\n\r\n    def call(self, state):\r\n        return self.fc3(self.fc2(self.fc1(state)))\r\n\r\n\r\nclass Agent():\r\n\r\n    def __init__(self, state_size, action_size, random_seed):\r\n        self.state_size = state_size\r\n        self.action_size = action_size\r\n        self.seed = random.seed(random_seed)\r\n\r\n        # Actor Network (w/ Target Network)\r\n        self.actor_local = Actor(action_size)\r\n\r\n    def act(self, state, add_noise=True):\r\n        action = self.actor_local(state)\r\n        return tf.clip_by_value(action, -1, 1)\r\n\r\n\r\nagent = Agent(state_size=5, action_size=1, random_seed=2)\r\n\r\n@tf.function\r\ndef ddpg():\r\n    import numpy as np\r\n    state = np.arange(5).astype(np.float64).reshape((1, -1))\r\n    state = agent.act(state)\r\n    np.isfinite(state)\r\n    return state\r\n\r\nscores = ddpg()\r\n\r\n#with tf.device(\"/gpu:0\"):\r\n#    scores = ddpg()\r\n#    print(\"run is successful\")\r\n```\r\n\r\nException:\r\n\r\n```\r\n\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0503 18:53:51.962915 140631398553408 tf_logging.py:161] Entity <method-wrapper '__call__' of numpy.ufunc object at 0x5594ba8c9580> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\nTraceback (most recent call last):\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 369, in converted_call\r\n    experimental_partial_types=partial_types)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 513, in to_graph\r\n    arg_values, arg_types)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 196, in entity_to_graph\r\n    'Object conversion is not yet supported. If you are '\r\nNotImplementedError: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/media/seagull/use_me/seagull/github/DDPG/ddpg-pendulam/ddpg_agent_tf.py\", line 43, in <module>\r\n    scores = ddpg()\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 426, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1580, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1512, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 317, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 686, in wrapper\r\n    ), args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 392, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/tmp4fw9we40.py\", line 8, in tf__ddpg\r\n    ag__.converted_call('isfinite', np, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (state,), {})\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 390, in converted_call\r\n    return _call_unconverted(f, args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 188, in _call_unconverted\r\n    return f(*args, **kwargs)\r\nTypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''", "@Jonathan-Livingston-Seagull I think you need to use `tf.py_function`. Please check the code below which works without any issue. Thanks!\r\n\r\n```\r\nimport random\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nclass Actor(tf.keras.Model):\r\n    def __init__(self, action_size, fc1_units=400, fc2_units=300):\r\n        super(Actor, self).__init__()\r\n        self.fc1 = tf.keras.layers.Dense(units=fc1_units, activation=tf.nn.relu)\r\n        self.fc2 = tf.keras.layers.Dense(units=fc2_units, activation=tf.nn.relu)\r\n        self.fc3 = tf.keras.layers.Dense(action_size, activation=tf.nn.tanh)\r\n\r\n    def call(self, state):\r\n        return self.fc3(self.fc2(self.fc1(state)))\r\n\r\n\r\nclass Agent():\r\n\r\n    def __init__(self, state_size, action_size, random_seed):\r\n        self.state_size = state_size\r\n        self.action_size = action_size\r\n        self.seed = random.seed(random_seed)\r\n\r\n        # Actor Network (w/ Target Network)\r\n        self.actor_local = Actor(action_size)\r\n\r\n    def act(self, state, add_noise=True):\r\n        action = self.actor_local(state)\r\n        return tf.clip_by_value(action, -1, 1)\r\n\r\n\r\nagent = Agent(state_size=5, action_size=1, random_seed=2)\r\n\r\ndef check_state(state):\r\n  return np.isfinite(state)\r\n\r\n@tf.function\r\ndef ddpg():\r\n    #import numpy as np\r\n    state = np.arange(5).astype(np.float64).reshape((1, -1))\r\n    state = agent.act(state)\r\n    #np.isfinite(state)\r\n    tf.py_function(check_state,inp=[state],Tout=[tf.bool])\r\n    return state\r\n\r\nscores = ddpg()\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    scores = ddpg()\r\n    print(\"run is successful\")\r\n```", "@jvishnuvardhan Thanks . your suggestion solved that issue. However I encountered another issue which I explained in the toy example below.\r\nI wanted to break my loop based on the value returned from py_function as shown below.\r\n\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef dummy_py_function():\r\n    return np.random.random(5), np.random.choice([True, False])\r\n\r\n\r\n@tf.function\r\ndef random_break():\r\n    for i in range(10):\r\n        random_array, done = tf.py_function(dummy_py_function, inp=[], Tout=[tf.float32, tf.bool])\r\n        if done:\r\n            break\r\n        else:\r\n            continue\r\n\r\nrandom_break() \r\n\r\n```\r\nThis throws the following exception\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/seagull/use_me/seagull/github/DDPG/ddpg-pendulam/test.py\", line 18, in <module>\r\n    random_break()\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 426, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1580, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1512, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 317, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 686, in wrapper\r\n    ), args, kwargs)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 392, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/tmpjfr3w2a8.py\", line 25, in tf__random_break\r\n    break_, = ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (10,), {}), extra_test, loop_body, (break_,))\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 81, in for_stmt\r\n    return _py_for_stmt(iter_, extra_test, body, init_state)\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 88, in _py_for_stmt\r\n    if extra_test is not None and not extra_test(*state):\r\n  File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 660, in __bool__\r\n    raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n```\r\n\r\nHowever the following code works fine. I have noticed that the issue appears when the py_function returns more than one value.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef dummy_py_function():\r\n    return np.random.choice([True, False])\r\n\r\n\r\n@tf.function\r\ndef random_break():\r\n    for i in range(10):\r\n        done = tf.py_function(dummy_py_function, inp=[], Tout=[tf.bool])\r\n        if done:\r\n            break\r\n        else:\r\n            continue\r\n\r\nrandom_break()\r\n```", "@Jonathan-Livingston-Seagull You could do this. I am not sure whether we can return two different data types from py.funciton. Below, I have split the py_function into to two py_functions. It runs through without any issues. thanks!\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef dummy_py_function():\r\n    return np.random.random(5)\r\n\r\ndef dummy_py_function2():\r\n    return np.random.choice([True, False])\r\n\r\n@tf.function\r\ndef random_break():\r\n    for i in range(10):\r\n      print(i)\r\n      random_array = tf.py_function(dummy_py_function, inp=[], Tout=[tf.float32])\r\n      done = tf.py_function(dummy_py_function2, inp=[], Tout=[tf.bool])\r\n      print(random_array)\r\n      print(done)\r\n      if done:\r\n          break\r\n      else:\r\n          continue\r\n\r\nrandom_break() \r\n```", "@jvishnuvardhan You are right, as I mentioned in my previous message if the number of returned value from the py_function is one than it run through. \r\n\r\n- I think this is not about the different return datatypes, even if I return the same datatype, it does not work as soon as the py_function returns more than one value. i.e the code below does not work\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef dummy_py_function1():\r\n    return False, False\r\n\r\n@tf.function\r\ndef random_break():\r\n    for i in range(10):\r\n        print(i)\r\n        done1, done2 = tf.py_function(dummy_py_function1, inp=[], Tout=[tf.bool, tf.bool])\r\n        if done1:\r\n            print(\"breaking\")\r\n            break\r\n        else:\r\n            continue\r\n\r\nrandom_break()\r\n```\r\n\r\n- The second point is that the \"if condition\" always satisfied even if you return False from the function in the code you provided above. ", "@Jonathan-Livingston-Seagull Thanks! please close if the issue was resolved. Thanks!", "\r\n\r\n\r\n> When I tried to make the following minimal example, I have found the reason for the reported issue. However I get another exception after solving that one. Please find the minimal code below. The source of the error seems like calling 'np.isfinite()' method inside the tf function definition. In my real code this method is being called by a third party lib that I use inside my tf function definition.\r\n> \r\n> ```\r\n> import random\r\n> import numpy as np\r\n> \r\n> import tensorflow as tf\r\n> \r\n> \r\n> class Actor(tf.keras.Model):\r\n>     def __init__(self, action_size, fc1_units=400, fc2_units=300):\r\n>         super(Actor, self).__init__()\r\n>         self.fc1 = tf.keras.layers.Dense(units=fc1_units, activation=tf.nn.relu)\r\n>         self.fc2 = tf.keras.layers.Dense(units=fc2_units, activation=tf.nn.relu)\r\n>         self.fc3 = tf.keras.layers.Dense(action_size, activation=tf.nn.tanh)\r\n> \r\n>     def call(self, state):\r\n>         return self.fc3(self.fc2(self.fc1(state)))\r\n> \r\n> \r\n> class Agent():\r\n> \r\n>     def __init__(self, state_size, action_size, random_seed):\r\n>         self.state_size = state_size\r\n>         self.action_size = action_size\r\n>         self.seed = random.seed(random_seed)\r\n> \r\n>         # Actor Network (w/ Target Network)\r\n>         self.actor_local = Actor(action_size)\r\n> \r\n>     def act(self, state, add_noise=True):\r\n>         action = self.actor_local(state)\r\n>         return tf.clip_by_value(action, -1, 1)\r\n> \r\n> \r\n> agent = Agent(state_size=5, action_size=1, random_seed=2)\r\n> \r\n> @tf.function\r\n> def ddpg():\r\n>     import numpy as np\r\n>     state = np.arange(5).astype(np.float64).reshape((1, -1))\r\n>     state = agent.act(state)\r\n>     np.isfinite(state)\r\n>     return state\r\n> \r\n> scores = ddpg()\r\n> \r\n> #with tf.device(\"/gpu:0\"):\r\n> #    scores = ddpg()\r\n> #    print(\"run is successful\")\r\n> ```\r\n> \r\n> Exception:\r\n> \r\n> ```\r\n> \r\n> WARNING: Logging before flag parsing goes to stderr.\r\n> W0503 18:53:51.962915 140631398553408 tf_logging.py:161] Entity <method-wrapper '__call__' of numpy.ufunc object at 0x5594ba8c9580> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\n> Traceback (most recent call last):\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 369, in converted_call\r\n>     experimental_partial_types=partial_types)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 513, in to_graph\r\n>     arg_values, arg_types)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 196, in entity_to_graph\r\n>     'Object conversion is not yet supported. If you are '\r\n> NotImplementedError: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/media/seagull/use_me/seagull/github/DDPG/ddpg-pendulam/ddpg_agent_tf.py\", line 43, in <module>\r\n>     scores = ddpg()\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 426, in __call__\r\n>     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 370, in _initialize\r\n>     *args, **kwds))\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n>     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1580, in _maybe_define_function\r\n>     graph_function = self._create_graph_function(args, kwargs)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1512, in _create_graph_function\r\n>     capture_by_value=self._capture_by_value),\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in func_graph_from_py_func\r\n>     func_outputs = python_func(*func_args, **func_kwargs)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 317, in wrapped_fn\r\n>     return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 686, in wrapper\r\n>     ), args, kwargs)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 392, in converted_call\r\n>     result = converted_f(*effective_args, **kwargs)\r\n>   File \"/tmp/tmp4fw9we40.py\", line 8, in tf__ddpg\r\n>     ag__.converted_call('isfinite', np, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (state,), {})\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 390, in converted_call\r\n>     return _call_unconverted(f, args, kwargs)\r\n>   File \"/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 188, in _call_unconverted\r\n>     return f(*args, **kwargs)\r\n> TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\r\n> ```\r\n\r\nCould you share how U solve the W501 issue? Appreciated.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Please open a new ticket if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28318\">No</a>\n"]}, {"number": 28317, "title": "XLA missing [ op: Unique ] for most optimizers", "body": "Unable to train with anything but sgd using xla, due to a missing operation.\r\n\r\nline 76 in https://github.com/tensorflow/tensorflow/blob/677a14872f028e9be51bd469ec2d5cc43ea9155e/tensorflow/python/training/optimizer.py\r\n```\r\ndef _deduplicate_indexed_slices(values, indices):\r\n  \"\"\"Sums `values` associated with any non-unique `indices`.\r\n  Args:\r\n    values: A `Tensor` with rank >= 1.\r\n    indices: A one-dimensional integer `Tensor`, indexing into the first\r\n      dimension of `values` (as in an IndexedSlices object).\r\n  Returns:\r\n    A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\r\n    de-duplicated version of `indices` and `summed_values` contains the sum of\r\n    `values` slices associated with each unique index.\r\n  \"\"\"\r\n  unique_indices, new_index_positions = array_ops.unique(indices) <------- MISSING\r\n  summed_values = math_ops.unsorted_segment_sum(\r\n      values, new_index_positions,\r\n      array_ops.shape(unique_indices)[0])\r\n  return (summed_values, unique_indices)\r\n```\r\n\r\n", "comments": ["Can you please help us in reproducing the issue with a minimal code. Also let us know information such as TensorFlow version, platform etc. You can refer this [link](https://github.com/tensorflow/tensorflow/issues/new/choose) to help us. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n\r\n"]}, {"number": 28316, "title": "autograph throwing \"could not get source code\" on recent nightlies", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux fedora 29\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): e.g. , 2.0.0-dev20190501\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nOn recent nightly builds - v2 as well as v1 - the following fails:\r\n\r\n```\r\n# in p.py\r\n@tf.function\r\ndef add(a,b): return(a+b)\r\n```\r\n\r\n```\r\n# called in shell\r\nIn [2]: import tensorflow as tf                                                                                                                                                                                    \r\n\r\nIn [3]: exec(open(\"p.py\").read()) \r\n   ...: add(1,2)  \r\n```\r\n\r\nError stack:\r\n\r\n```\r\n2019-05-01 20:36:06.363503: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-01 20:36:06.401892: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz\r\n2019-05-01 20:36:06.402171: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560bd15a4920 executing computations on platform Host. Devices:\r\n2019-05-01 20:36:06.402190: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nConverted call: <function add at 0x7f56080b1400> \r\n    args: (1, 2)\r\n    kwargs: {}\r\n\r\nEntity <function add at 0x7f56080b1400> is not cached for key <code object add at 0x7f55ed2e68a0, file \"<string>\", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f55ed284978>, frozenset())\r\nConverting <function add at 0x7f56080b1400>\r\nError transforming <function add at 0x7f56080b1400>\r\nTraceback (most recent call last):\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 53, in parse_entity\r\n    source = inspect_utils.getimmediatesource(entity)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 124, in getimmediatesource\r\n    lines, lnum = inspect.findsource(obj)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/inspect.py\", line 786, in findsource\r\n    raise OSError('could not get source code')\r\nOSError: could not get source code\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 440, in to_graph\r\n    return conversion.convert(entity, program_ctx)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 309, in convert\r\n    entity, program_ctx, free_nonglobal_var_names)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 232, in _convert_with_cache\r\n    entity, program_ctx)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 440, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 603, in convert_func_to_ast\r\n    node, source = parser.parse_entity(f, future_features=future_features)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 61, in parse_entity\r\n    ' @tf.autograph.do_not_convert. Original error: {}'.format(entity, e))\r\nValueError: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nError transforming entity <function add at 0x7f56080b1400>\r\nTraceback (most recent call last):\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 53, in parse_entity\r\n    source = inspect_utils.getimmediatesource(entity)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 124, in getimmediatesource\r\n    lines, lnum = inspect.findsource(obj)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/inspect.py\", line 786, in findsource\r\n    raise OSError('could not get source code')\r\nOSError: could not get source code\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 440, in to_graph\r\n    return conversion.convert(entity, program_ctx)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 309, in convert\r\n    entity, program_ctx, free_nonglobal_var_names)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 232, in _convert_with_cache\r\n    entity, program_ctx)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 440, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 603, in convert_func_to_ast\r\n    node, source = parser.parse_entity(f, future_features=future_features)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 61, in parse_entity\r\n    ' @tf.autograph.do_not_convert. Original error: {}'.format(entity, e))\r\nValueError: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 325, in converted_call\r\n    experimental_optional_features=options.optional_features)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 442, in to_graph\r\n    errors.report_internal_error(entity, e)\r\n  File \"/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/errors.py\", line 99, in report_internal_error\r\n    'report. Caused by: %s' % (entity, exception))\r\ntensorflow.python.autograph.pyct.errors.AutoGraphError: Unexpected error transforming <function add at 0x7f56080b1400>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0501 20:36:06.408588 140007718111040 ag_logging.py:145] Entity <function add at 0x7f56080b1400> could not be transformed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function add at 0x7f56080b1400>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nWARNING: Entity <function add at 0x7f56080b1400> could not be transformed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function add at 0x7f56080b1400>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n\r\n```\r\n\r\nThanks!", "comments": ["When I use\r\n\r\n```\r\nimport(p)\r\n```\r\n\r\nto source the file instead of \r\n\r\n```\r\nexec(open(\"p.py\").read()) \r\n```\r\n\r\nit works...", "@skeydan Can you please let us know if you are happy to close this issue as in previous comment it says that after doing import(p), it worked.", "I can close it, I'd just be wondering why the first variant wouldn't work?", "> I can close it, I'd just be wondering why the first variant wouldn't work?\r\n\r\nHey @skeydan , It actually worked both ways.\r\nI'm using 2.0.0-alpha0", "@skeydan As you confirmed its working, closing this issue"]}, {"number": 28315, "title": "[TL LITE] Add input type bool to Cast operation", "body": "This is to solve issue #28206. ", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` "]}, {"number": 28314, "title": "[TF 2.0 API Docs] tf.image.extract_image_patches", "body": "### Existing URLs containing the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/extract_image_patches\r\n\r\n### Description of issue (what needs changing):\r\n\r\n- #### Clear Description\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Only a single sentence description is provided.\r\n\r\n- #### Usage Example\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No usage example is provided.\r\n\r\n- #### Raises Listed and Defined\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Errors are not defined.\r\n\r\n- #### Visuals, if Applicable\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No visuals are included.\r\n", "comments": ["Thanks for identifying this!\r\n\r\n@dynamicwebpaige Should we be marking API issues using the r2.0 branch or master? I'm not sure it matters for the most part, but just as an example I believe this API will be `tf.image.extract_patches` on TF2 release.\r\nhttps://github.com/tensorflow/tensorflow/commit/52d006a310b8ef6b8bbe48911a183215c4494943#diff-68052049ee5dc7573829a5fdb7a57310", "My pleasure, Sean. Very good question regarding the branch. I totally\nforgot to check the branch onto which I was adding the issue. I'd be happy\nto resubmit to a development branch.\n\nOn Wed, May 1, 2019 at 4:51 PM Sean Morgan <notifications@github.com> wrote:\n\n> Thanks for identifying this!\n>\n> @dynamicwebpaige <https://github.com/dynamicwebpaige> Should we be\n> marking API issues using the r2.0 branch or master? I'm not sure it matters\n> for the most part, but just as an example I believe this API will be\n> tf.image.extract_patches on TF2 release.\n> 52d006a#diff-68052049ee5dc7573829a5fdb7a57310\n> <https://github.com/tensorflow/tensorflow/commit/52d006a310b8ef6b8bbe48911a183215c4494943#diff-68052049ee5dc7573829a5fdb7a57310>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28314#issuecomment-488499833>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AENIODCJKNOTZC4HGCB4LI3PTIUINANCNFSM4HJUVJZQ>\n> .\n>\n", "@hendercine,\r\nIs this still an issue?\r\n\r\nPlease take a look at similar API [tf.image.extract_patches](https://www.tensorflow.org/api_docs/python/tf/image/extract_patches) which has a clear description and usage example. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 28313, "title": "3D CNN with tf.nn.conv3d_transpose on CPU: 50-100 GB RAM free but segfault + \"terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 (three machines) & Windows 10 Pro (two machines)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary; tensorflow-gpu2.0.0alpha0 (Windows machines), tensorflow/tensorflow:2.0.0a0-py3-jupyter docker image (Ubuntu 18.04 machines)**\r\n- TensorFlow version (use command below): **v1.12.0-9492-g2c319fb415 2.0.0-alpha0 (Windows)**\r\n- Python version: **3.6.4 (Windows)**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **10/7.4.1 (Windows); N/A (Ubuntu 18.04)**\r\n- GPU model and memory: **Titan RTX (Windows machine 1); GTX 1080 (Windows machine 2); CPU-only (Ubuntu 18.04 machines 1-3)**\r\n\r\nAll machines specs:\r\n- CPUs: **Dual socket, 8 cores each CPU (Xeon E5-2670, E5-2687w on the machine with the Titan RTX), 32 total logical processors.** \r\n- RAM: **128 GB**  \r\n\r\n**Describe the current behavior**\r\n\r\n3D CNN with deconvolution (tf.nn.conv3d_transpose) to upsample 1 channel volumes x2 in all three dimensions. Training on GPU and using 64^3 image patches works fine. For inference I moved ops to the CPU in order to handle large volumes that wouldn't otherwise fit in GPU memory. But now I have run into the below errors:\r\n\r\n1. Frequent crashes when using more than 50% of available CPU cores. On Ubuntu 18.04 it completely kills the node and I have to physically power cycle the machine. On Windows, I'm lucky if it doesn't freeze indefinitely at a blue screen. \r\n\r\n2. Visual bug for inputs >~130^3 (generally whenever it gives me a warning that I'm allocating >10% of system memory) where the last slices of the volume converge to intensity ~ 0.39 (coincidentally, very close to a constant that the model adds to the output to correct for grayscale intensity). This get progressively worse with larger volumes (~130^3 < dims < ~175^3)\r\n\r\n3. Segfault for inputs > ~175^3. \r\nExample output: \r\n\r\n**On Windows:** \r\n2019-04-26 14:56:19.461143: W tensorflow/core/framework/allocator.cc:124] Allocation of 20320616448 exceeds 10% of system memory.\r\n2019-04-26 14:58:23.532021: W tensorflow/core/framework/allocator.cc:124] Allocation of 20320616448 exceeds 10% of system memory.\r\n2019-04-26 14:58:23.532021: W tensorflow/core/framework/allocator.cc:124] Allocation of 20320616448 exceeds 10% of system memory.\r\nSegmentation Fault\r\n\r\nNote: It once gave me a popup with an error of the form, \"Instruction at 0xDED referenced memory at 0XBEEF. The memory could not be read\".   \r\n\r\n**On Ubuntu 18.04: ** \r\n2019-05-01 02:29:47.571830: W tensorflow/core/framework/allocator.cc:116] Allocation of 26311577600 exceeds 10% of system memory.\r\nterminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc\r\n\r\n**Note:** I generally still have 50-100 GB of free memory when these errors occur so it's not for lack of memory. \r\n**Note:**: I have replicated this phenomenon across all machines using just CPU. On the Windows machines, I get the same results when I have attempted to move all ops to the GPU save for deconvolution (which will not fit on the GPU).\r\n\r\n**Describe the expected behavior**\r\nNo crashing, processing large inputs if available resources allow. \r\n\r\n\r\n**Code to reproduce the issue**\r\nN/A\r\n\r\n**Other info / logs**\r\nN/A\r\n", "comments": ["Still experiencing this issue. Running same tests on an AWS instance (r4.4xlarge).  New error using the docker image (tensorflow/tensorflow:2.0.0a0-py3-jupyter): *** Error in `python': munmap_chunk(): invalid pointer: 0x00007fb8d4794040 *** (see attached: full stack trace log, top resource usage at time of crash\r\n[051519_AWS_docker_crash_stack_trace_TF_issues.txt](https://github.com/tensorflow/tensorflow/files/3184641/051519_AWS_docker_crash_stack_trace_TF_issues.txt)\r\n![051519_AWS_docker_crash_top_TF_issues](https://user-images.githubusercontent.com/13070236/57813253-516aea00-7724-11e9-9d55-85e1c3f32210.png)\r\n\r\n)\r\nNOTE: tf.ConfigProto w/   \r\n\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\n\r\nto reduce the possibility of multithreading bugs (which would freeze the machines on previous tests).", "Update: \r\n\r\nOn **AWS** instance (r4.4xlarge): Tested  pre-compiled binary (tensorflow2.0.0a0), same resource utilization and error as above. \r\n\r\nOn **AWS/Ubuntu 18.04** machines: Tested different versions of building from source (w & w/o XLA JIT support, --config=opt --config=numa, --config=numa, --config=opt) all with the same resource utilization and error as above. \r\n\r\nOn **Windows** machine 1: Tested different versions of building from source (w & w/o XLA JIT support, --config=opt --config=numa --config=cuda, --config=numa --config=cuda, --config=cuda) all with the same resource utilization pattern, though a large spike in committed memory (~30% to ~80-90%.) before a segfault; see below:\r\n\r\n2019-05-15 09:40:39.824670: W tensorflow/core/framework/allocator.cc:116] Allocation of 20744736768 exceeds 10% of system memory.\r\n2019-05-15 10:18:20.418309: W tensorflow/core/framework/allocator.cc:116] Allocation of 20744736768 exceeds 10% of system memory.\r\n2019-05-15 10:18:20.468114: W tensorflow/core/framework/allocator.cc:116] Allocation of 20744736768 exceeds 10% of system memory.\r\nProcess finished with exit code -1073741819 (0xC0000005)  \r\n\r\n", "Update: A semi-solution on the AWS instance involving compiling with the Intel MKL-DNN solves the gray issue as well as the crashing issue (in some cases; see below). \r\n\r\nBUT it requires >5x the amount of RAM and is >2x slower. Additionally, memory is not deallocated immediately, causing crashes when there is a queue of input samples. The huge increase in required RAM usages causes crashes for larger inputs that a normal tensorflow build would theoretically have been able to handle if not for this bug. ", "Update: Now there is another bug where inference results are artifacted (see attached; should be completely black)\r\n\r\n![051719_artifacts_TF_issues](https://user-images.githubusercontent.com/13070236/57960418-79da1c00-78bd-11e9-9987-05fd8b33d95b.png)\r\n\r\nconfigs file for MKL-DNN & TF: \r\n\r\nimport os\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nthreads = 64\r\nsockets = 2\r\nargs =  tf.ConfigProto(\r\n  allow_soft_placement=True,\r\n  log_device_placement=False,\r\n  intra_op_parallelism_threads=threads,# Setting this equal to the number of physical cores is recommended. Setting the value to 0, which is the default, results in the value being set to the number of logical cores - this is an alternate option to try for some architectures. This value and OMP_NUM_THREADS should be equal.\r\n  inter_op_parallelism_threads=sockets, #Setting this equal to the number of sockets is recommended. Setting the value to 0, which is the default, results in the value being set to the number of logical cores.\r\n  gpu_options=(tf.GPUOptions(allow_growth=True)\r\n               ))\r\nos.environ[\"KMP_BLOCKTIME\"] = '0'\r\nos.environ[\"KMP_SETTINGS\"] = '1'\r\nos.environ[\"KMP_AFFINITY\"]=  'granularity=fine,verbose,compact,1,0'\r\nos.environ[\"OMP_NUM_THREADS\"]= str(threads)", "May be related to: #21637 \r\n\r\nReplicated original bug across: \r\n\r\n1. tf-nightly (pip package)\r\n2. tensorflow (pip package)\r\n3. Build from source (master & r2.0)\r\n\r\nReplicated Intel MKL-DNN bug across:\r\n1. intel-tensorflow (pip package)\r\n2. Build from source (master & r2.0)\r\na. Newest MKL-DNN source build (master)\r\nb. v0.19 \r\nc. v0.19-rc\r\nd. v1.0-pc2\r\ne. v1.0-pc", "Update:\r\n\r\nXLA JIT compilation of the CPU deconvolution ops FIXES the problem altogether (no empty slices from the original bug, no checkerboard artifacts from the MKL-DNN bug) BUT is now two orders of magnitude slower (~200 minutes vs. 4 minutes) and still allocates much more memory than a normal tensorflow inference, so this is unfortunately not a feasible solution.\r\n\r\n", "Any ideas on where to go next? ", "@jvishnuvardhan I don't have expertise on this thus unassigning myself.", "Anyone?", "@TeoZosa It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28313\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28313\">No</a>\n"]}, {"number": 28312, "title": "Tests for PaddedBatchDatasetOp", "body": "This PR adds tests for `PaddedBatchDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": []}, {"number": 28311, "title": "tf2.0 get numpy from tensor", "body": "\r\nHow to get the numpy from tensor? it says there is no numpy method. as far as I know, tf2.0 does not encourage users using session anymore, so how to get the acutally value from a tensor?\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\ndataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\r\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\r\n\r\ntrain_dataset = train_dataset.shuffle(100).batch(12).repeat()\r\n\r\nfor img, label in train_dataset.take(1):\r\n    img = img.numpy()\r\n    print(img.shape)\r\n    print(img)\r\n    \r\n\r\n```\r\n\r\nerror:\r\n\r\n```\r\n img = img.numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "comments": ["> How to get the numpy from tensor? it says there is no numpy method. as far as I know, tf2.0 does not encourage users using session anymore, so how to get the acutally value from a tensor?\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> import tensorflow_datasets as tfds\r\n> \r\n> \r\n> dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\r\n> train_dataset, test_dataset = dataset['train'], dataset['test']\r\n> \r\n> train_dataset = train_dataset.shuffle(100).batch(12).repeat()\r\n> \r\n> for img, label in train_dataset.take(1):\r\n>     img = img.numpy()\r\n>     print(img.shape)\r\n>     print(img)\r\n>     \r\n> ```\r\n> \r\n> error:\r\n> \r\n> ```\r\n>  img = img.numpy()\r\n> AttributeError: 'Tensor' object has no attribute 'numpy'\r\n> ```\r\n\r\nPlease include the following details.\r\n1. `print(tf.__version__)`\r\n2. Is it running in Graph Mode or Eager Mode? (You can check that by printing `type(img)` before calling `.numpy()`.\r\n", "@jinfagang As mentioned by @captain-pool , you might be using Graph Mode. If you have installed TF2.0, then running your code produces output as expected.\r\n`print(img.shape)` ==> `(12, 28, 28, 1)`\r\n`type(img)`            ==> `numpy.ndarray`\r\n\r\nHere is GitHub [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/11f4737679237250cff63aba0e6714b0/untitled129.ipynb). You could run it in Google colab and see the results. Thanks!\r\n\r\nIn future, please post this kind of support questions in [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan  Thanks, I will, but since I already in tf2.0 but can not get value instantly which is not same with what tensorflow 2.0 claims.\r\n\r\ntype(img) got something like this:\r\n\r\n```\r\n(416, 416, 3)\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n```", "@jinfagang  Me too. I can not get the numpy value from tensor due to the .numpy() which is different in the former version with eager version", "@jinfagang but you can check whether the function was wrapped by '@tf.function'", "is there any way to use .numpy() within a function decorated by @tf.function?", "Hey Hurley! Nope there isn't .numpy() is a function available to\nEagerTensors However once decorated with @tf.function, the function becomes\na graph Tensor.\n\nOn Thu, Aug 8, 2019 at 2:02 AM Hurley <notifications@github.com> wrote:\n\n> is there any way to use .numpy() within a function decorated by\n> @tf.function?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28311?email_source=notifications&email_token=ADKYRWN25ZGSVIYAA76DU4LQDMWMFA5CNFSM4HJUB2Z2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3ZTKOA#issuecomment-519255352>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADKYRWLPNAODJOLSMNZ5KGLQDMWMFANCNFSM4HJUB2ZQ>\n> .\n>\n", "Thanks for the response @captain-pool !\r\nWhat I really wanted is to be able to get values from a tensor, for example: in\r\n```\r\ntf.random.fixed_unigram_candidate_sampler(\r\n    ...\r\n    unigrams=()\r\n)\r\n```\r\n`unigrams` accept a list of values, and I'd like to provide `unigrams` with values calculated from Tensor calculations, but it's not possible to implement within a @tf.function in tf2.0. \r\n\r\nIf there any way to work this around so I can get values from a Tensor while within a function decorated by @tf.function?\r\n\r\nThanks for your time!\r\nHurley\r\n", "> Thanks for the response @captain-pool !\r\n> What I really wanted is to be able to get values from a tensor, for example: in\r\n> \r\n> ```\r\n> tf.random.fixed_unigram_candidate_sampler(\r\n>     ...\r\n>     unigrams=()\r\n> )\r\n> ```\r\n> \r\n> `unigrams` accept a list of values, and I'd like to provide `unigrams` with values calculated from Tensor calculations, but it's not possible to implement within a @tf.function in tf2.0.\r\n> \r\n> If there any way to work this around so I can get values from a Tensor while within a function decorated by @tf.function?\r\n> \r\n> Thanks for your time!\r\n> Hurley\r\n\r\nHey @hurleyLi , I'm not sure about it, any TensorFlow function should be able to take tensors in value fields.\r\nCC: @jvishnuvardhan ", "Please open a new issue by filling [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) with your details, and provide a simple standalone code to reproduce the issue. Thanks!", "I encounter the same issue. I wrote a custom loss function and I want to see its returned value, \r\n```\r\n\tfor ep in range(epoch):\r\n\t\tprint('Start of epoch %d' % (ep,))\r\n\t\t# Each step crosses one minibatch\r\n\t\tfor step, data in enumerate(dsTrain):\r\n\t\t\tyPred = TUNet(data[0])\r\n\t\t\tloss = SoftDiceLoss(yPred, data[1])\r\n\t\t\tprint(loss)\r\n```\r\nThe TUNet was built by keras' layers, and SoftDiceLoss() is implemented by tf tensors\r\n```\r\n\t\tdef SoftDiceLoss(yPred, yTrue):\r\n\t\t\t'''\r\n\t\t\tSoftDiceLoss calculates multi-class soft dice loss\r\n\t\t\tloss = avg_batch(1-(sum(W_k*sum(yPred.*yTrue)))/(sum(W_ksum(yPred^2+yTrue^2))))\r\n\t\t\twhere W_k = 1/(number of voxels in class k)^2\r\n\t\t   Class number of segmented regions includes background\r\n\t\t\tInput:\r\n\t\t\t\tyPred/yTrue: prediced and desired outputs shaped as [mbSize, classNum, tensor dimensions]. Also, both must be float-point\r\n\t\t\tReturn:\r\n\t\t\t\tloss: a scalar\r\n\t\t\t'''\r\n\t\t\tepsilon = 1e-16 \r\n\t\t\tyTrue =tf.dtypes.cast(yTrue, dtype=yPred.dtype)\r\n\t\t\t# Dot product yPred and yTrue and sum them up for each datum and class\r\n\t\t\tcrossProd=tf.multiply(yPred, yTrue)\r\n\t\t\t# As a symbolic tensor, dimensions and shapes etc. cannot be extracted from data, nor can it be used in subroutines.\r\n\t\t\tcrossProdSum=tf.math.reduce_sum(crossProd, axis=np.arange(2, 5))\r\n\t\t\t# Calculate weight for each datum and class \r\n\t\t\tweight = tf.math.reduce_sum(yTrue, axis=np.arange(2, 5))\r\n\t\t\tweight = tf.math.divide(1, tf.math.square(weight)+epsilon)\r\n\t\t\t# Weighted sum over classes\r\n\t\t\tnumerator = 2*tf.math.reduce_sum(tf.multiply(crossProdSum, weight), axis=1)\r\n\t\t\t# Saquared summation \r\n\t\t\tyySum = tf.math.reduce_sum(tf.math.square(yPred) + tf.math.square(yTrue), axis=np.arange(2, 5))\r\n\t\t\t# Weighted sum over classes\r\n\t\t\tdenominator = tf.math.reduce_sum(tf.multiply(weight, yySum), axis=1)\r\n\t\t\t# Get individual loss and average over minibatch\r\n\t\t\tloss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+epsilon))\r\n\t\t\t\r\n\t\t\treturn loss\r\n```\r\nSo how can I see the loss value?\r\n", "Maybe `numpy.asarray` will help.", "> \r\n> \r\n> Maybe `numpy.asarray` will help.\r\n\r\nThis method does not work.\r\nThe tensor cannot be converted to an array by asarray", "> > Maybe `numpy.asarray` will help.\r\n> \r\n> This method does not work.\r\n> The tensor cannot be converted to an array by asarray\r\n\r\nI do not know whether I fully understand the condition in which this problem is raised. But based on my experience, it does work. If there is any misunderstanding, I apologize first.\r\n\r\nFor a simple example,\r\n`import numpy as np`\r\n`import tensorflow as tf`\r\n`from tensorflow.keras.layers import Dense`\r\n`from tensorflow.keras.models import Sequential`\r\n\r\n`X = 10 * np.random.rand(10000,1)`\r\n`Y = 2 * X + 1`\r\n\r\n`model = Sequential([`\r\n`    Dense(3),`\r\n`    Dense(1)`\r\n`])`\r\n`model.compile(optimizer='adam',loss='mean_squared_error',metric = 'accuracy')`\r\n`model.fit(X,Y,epochs = 10)`\r\n\r\n`output = model(np.array([[2]]))`\r\n\r\n`print(output)`\r\n`# tf.Tensor([[5.050682]], shape=(1, 1), dtype=float32)`\r\n\r\n`print(np.asarray(output))`\r\n`# [[5.050682]]` `", "> `print(np.asarray(output))`\r\n\r\nNotImplementedError: Cannot convert a symbolic Tensor (module_apply_default/truediv:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n"]}, {"number": 28310, "title": "tf.hessians fails when applied to the output of a RNN", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary: conda install tensorflow\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Python version**: both 3.6.8 and 3.7.3\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: Not used\r\n- **Exact command to reproduce**: none\r\n\r\n### Describe the problem\r\nI need to compute the hessians of a **tensor** output from a **RNN** structure w.r.t to its inputs. However, this leads to an error reported in the joint file. I think it is a bug as just replacing ``tf.hessians`` by ``tf.gradients`` leads to a working code.\r\n\r\nI think it may be due to an low-level implementation of RNN cells with ``tf.while_loop``s, as indicated by the error.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import RNN, GRUCell, Dense\r\n\r\n# Define size of variable. TODO: adapt to data\r\ninp_dim = 2\r\nnum_units = 50\r\nbatch_size = 100\r\ntimesteps = 10\r\n\r\n# Reset the graph, so as to avoid errors\r\ntf.reset_default_graph()\r\n\r\ninputs = tf.ones(shape=(timesteps, batch_size, inp_dim))\r\n\r\n### Building the model\r\ncells = [GRUCell(num_units), GRUCell(num_units)]\r\nrnn = RNN(cells, time_major=True, return_sequences=True)\r\nfinal_layer = Dense(1, input_shape=(num_units,))\r\n\r\n# Apply to inputs\r\nlast_state = rnn(inputs)\r\nf = final_layer(last_state)\r\n\r\n[derivs] = tf.hessians(f, inputs)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    grads = sess.run(derivs)\r\n```\r\n[stacktrace.log](https://github.com/tensorflow/tensorflow/files/3134827/stacktrace.log)\r\n", "comments": ["I write this comment so as to provide an update on this issue. I narrowed down the issue do the simpler MWE I am posting just below. When computing an output **tensor** using a ``tf.map_fn``tools and applying ``tf.hessians`` to it w.r.t its inputs, the graph fails to construct. I don't need to use an eager execution or a ``tf.Session()`` \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# create output tensors\r\ntf.reset_default_graph()\r\nx = tf.constant([1., 2.])\r\nf = tf.map_fn(lambda a: a**2, x)\r\ng = x*x\r\n\r\n# Compute gradients\r\nd_fx = tf.gradients(f, x) # Works\r\nd_gx = tf.gradients(g, x) # Works\r\ndd_fx = tf.hessians(f, x) # Fails with same error as original issue\r\ndd_gx = tf.hessians(g, x) # Works\r\n```\r\n\r\nI do not report the stacktrace, as it is the same as the one in the original post. \r\n\r\nPS: when updating the issue I created, should I edit my original post or create new comments?", "@csejourne Able to reproduce this issue in 1.13 . We will look at it.\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-ffbab77f1668> in <module>()\r\n     10 d_fx = tf.gradients(f, x) # Works\r\n     11 d_gx = tf.gradients(g, x) # Works\r\n---> 12 dd_fx = tf.hessians(f, x) # Fails with same error as original issue\r\n     13 dd_gx = tf.hessians(g, x) # Works\r\n\r\n18 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_util.py in CheckInputFromValidContext(op, input_op)\r\n    312         input_op.name, \"\".join(traceback.format_list(input_op.traceback)))\r\n    313     logging.info(log_msg)\r\n--> 314     raise ValueError(error_msg + \" See info log for more details.\")\r\n\r\nValueError: Cannot use 'while/gradients/f_count_1' as input to 'while/gradients/f_count' because they are in different while loops. See info log for more details.", "I thought of a workaround about this one. Maybe using the argument ``unroll=True`` would allow to bypass this error. However, I tried and got the input shape wrong, so I can't say for now if it would work on now. I\u00b4ll get back to you", "Ok, so I got two different workaround that allow to compute ``tf.hessians`` w.r.t the inputs.\r\n\r\n### First workaround\r\n\r\nUsing ``unroll=True`` prevents Tensorflow from creating the symbolic ``tf.while_loop`` as said in the documentation. Without this symbolic loop, the following code runs smoothly.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import RNN, GRUCell, Dense\r\n\r\n# Define size of variable. TODO: adapt to data\r\ninp_dim = 2\r\nnum_units = 50\r\nbatch_size = 100\r\ntimesteps = 10\r\n\r\n# Reset the graph, so as to avoid errors\r\ntf.reset_default_graph()\r\n\r\ninputs = tf.ones(shape=(timesteps, batch_size, inp_dim))\r\n\r\n### Building the model\r\ncell = GRUCell(num_units)\r\nrnn = RNN(cell, time_major=True, unroll=True)\r\nfinal_layer = Dense(1, input_shape=(num_units,))\r\n\r\n# Apply to inputs\r\nlast_state = rnn(inputs)\r\nf = final_layer(last_state)\r\n\r\n[derivs] = tf.hessians(f, inputs)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    grads = sess.run(derivs)\r\n```\r\n\r\nHowever for my need I needed to discard the time dimension (i.e remove it or set it to 1). It is at the moment not possible to unroll the RNN when ``time_dimension in [None, 1]``. For this reason, I just did the following workaround.\r\n\r\n#### Second workaround\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import RNN, GRUCell, Dense\r\n\r\n# Define size of variable. TODO: adapt to data\r\ninp_dim = 2\r\nnum_units = 50\r\nbatch_size = 100\r\n# Reset the graph, so as to avoid errors\r\ntf.reset_default_graph()\r\n\r\nstate = [tf.Variable(tf.zeros((batch_size, num_units)))]\r\ninputs = tf.ones(shape=(batch_size, inp_dim))\r\n\r\n### Building the model\r\ncells = [GRUCell(num_units), GRUCell(num_units)]\r\n\r\n_, temp = cells[0](inputs, state)\r\nf, _ = cells[1](inputs, temp)\r\n\r\n[derivs] = tf.hessians(f, inputs)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    grads = sess.run(derivs)\r\n```\r\n\r\nThis just consists of chaining a la mano the different cells used. It also works smoothly", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28310\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28310\">No</a>\n"]}, {"number": 28309, "title": "Errors in tensorflow/stream_executor/gpu/gpu_timer.cc while compiling on Windows with XLA enabled", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: checkout from `master` branch\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: conda 4.6.14\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27030.1 for x64, Visual Studio Build Tools 2017\r\n- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the problem**\r\n```\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2660: 'CreateEventA': function does not take 3 arguments\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2660: 'CreateEventA': function does not take 3 arguments\r\n```\r\n\r\nIn tensorflow/stream_executor/gpu/gpu_timer.cc(29) the function is `CreateEvent`:\r\n\r\n```c++\r\n29  port::Status status = GpuDriver::CreateEvent(context, &start_event_,\r\n30                                               GpuDriver::EventFlags::kDefault);\r\n```\r\n\r\nBut compiler looks for `CreateEventA`. Maybe there is a Windows macro conflicting with that symbol.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\n(neural) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ python configure.py\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is D:\\Programs\\Anaconda3\\envs\\neural\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  D:\\Programs\\Anaconda3\\envs\\neural\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\Programs\\Anaconda3\\envs\\neural\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(neural) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package -j 12\r\n\r\n[...]\r\n\r\nERROR: D:/neural/tensorflow/tensorflow/stream_executor/gpu/BUILD:168:1: C++ compilation of rule '//tensorflow/stream_executor/gpu:gpu_timer' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019A26nPRjStR25RPVWFZL5iMn5PgXYVtDTages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJVindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJVx9R2xvWau\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual SXgi164MCGG3u5RKXaBGVBabmxQTVRAAVE5tDTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual SXgi164MCGG3u5RKXaBGVBabmxQTVRAAVE5tDTools\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJVx9R2xvWaun;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJV\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/envs/neural/python.exe\r\n    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/envs/neural/lib/site-packages\r\n    SET TEMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TF_NEED_TENSORRT=0\r\n    SET TMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n  D:/Programs/Anaconda3/envs/neural/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SXgi164MCGG3u5RKXaBGVBabmxQTVRAAVE5tDTGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/stream_executor/gpu/_objs/gpu_timer/gpu_timer.o /c tensorflow/stream_executor/gpu/gpu_timer.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'\r\n.\\tensorflow/stream_executor/gpu/gpu_driver.h(60): note: see declaration of 'stream_executor::gpu::GpuDriver'\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2660: 'CreateEventA': function does not take 3 arguments\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\synchapi.h(469): note: see declaration of 'CreateEventA'\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'\r\n.\\tensorflow/stream_executor/gpu/gpu_driver.h(60): note: see declaration of 'stream_executor::gpu::GpuDriver'\r\ntensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2660: 'CreateEventA': function does not take 3 arguments\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\synchapi.h(469): note: see declaration of 'CreateEventA'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 679.504s, Critical Path: 262.52s\r\nINFO: 1869 processes: 1869 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[FullLog.txt](https://github.com/tensorflow/tensorflow/files/3134823/FullLog.txt)\r\n", "comments": ["I do not know if we support XLA on windows.\r\nPlease redirect to XLA team.", "So far XLA is not supported on Windows; if we want that we'll need to fix all the compile errors and enable it in our Windows CI.", "Could/should we make building XLA on Windows an explicit build #error?\n\nOn Wed, May 29, 2019 at 12:44 PM r4nt <notifications@github.com> wrote:\n\n> So far XLA is not supported on Windows; if we want that we'll need to fix\n> all the compile errors and enable it in our Windows CI.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28309?email_source=notifications&email_token=AABEZB5NG72II5XSXYJACPDPXZNCNA5CNFSM4HJT6UG2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWO5IDQ#issuecomment-496882702>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABEZB43R5WWG5CUKNZ6WNLPXZNCNANCNFSM4HJT6UGQ>\n> .\n>\n", "We can easily do that during configure. I guess we can also have an \"always\nfail\" build rule that is conditionally depended on based on XLA and\noperating system.\nIt can be a simple genrule that runs `echo \"Cannot build XLA on windows\";\nexit 1;`\n\nOn Sun, Jun 2, 2019 at 3:00 AM Justin Lebar <notifications@github.com>\nwrote:\n\n> Could/should we make building XLA on Windows an explicit build #error?\n>\n> On Wed, May 29, 2019 at 12:44 PM r4nt <notifications@github.com> wrote:\n>\n> > So far XLA is not supported on Windows; if we want that we'll need to fix\n> > all the compile errors and enable it in our Windows CI.\n> >\n> > \u2014\n> > You are receiving this because you were assigned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/28309?email_source=notifications&email_token=AABEZB5NG72II5XSXYJACPDPXZNCNA5CNFSM4HJT6UG2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWO5IDQ#issuecomment-496882702\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AABEZB43R5WWG5CUKNZ6WNLPXZNCNANCNFSM4HJT6UGQ\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28309?email_source=notifications&email_token=AB4UEOPEJBW4DG3PCOO24WLPYOK5DA5CNFSM4HJT6UG2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWXSCDY#issuecomment-498016527>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB4UEOKQN2DBBGA4MMGFEY3PYOK5DANCNFSM4HJT6UGQ>\n> .\n>\n", "Making it an error in one of these ways definitely seems like a good idea.\r\n\r\nGenerally we should also work on getting it to compile :)", "@bystepii We are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. you can also refer these similar issues **[link](https://github.com/tensorflow/tensorflow/issues/8310)**, **[link2](https://github.com/tensorflow/tensorflow/issues/15213)** Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28309\">No</a>\n"]}, {"number": 28308, "title": "Installation gives me error", "body": "I am trying to install tensorflow with a pip in a virualvenv. Python version 3.7 running on windows 10. Error: (venv) C:\\Users\\rojto\\PythonCLI>pip install --upgrade tensorflow\r\nCollecting tensorflow\r\n  ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow", "comments": ["@TomasRoj \r\n\r\nPlease provide all of the information requested in the issue template. Thanks!", "I gave you everything you need. Os, platform and error message. No more informations needed,"]}, {"number": 28307, "title": "Tensorflow classification call \"created device\" multiple times using c++", "body": "I was doing image classification using tensorflow c++. My codes are as below,\r\nStatus run_status = session->Run({ { input_layer, resized_tensor },{ input_layer2, b } ,{ input_layer1, a } }, { output_layer }, {}, &outputs);\r\n\tif (!run_status.ok()) {\r\n\t\tLOG(ERROR) << \"Running model failed: \" << run_status;\r\n\t\treturn -1;\r\n\t}\r\nThe first time to call the Run() function, The output is as follows,\r\n2019-05-01 18:02:36.575459: I D:\\projects\\c++\\tensorflow_GPU\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3039 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-05-01 18:02:36.620825: I D:\\projects\\c++\\tensorflow_GPU\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2019-05-01 18:02:36.624719: I D:\\projects\\c++\\tensorflow_GPU\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-01 18:02:36.628097: I D:\\projects\\c++\\tensorflow_GPU\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0\r\n2019-05-01 18:02:36.631864: I D:\\projects\\c++\\tensorflow_GPU\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N\r\nWhen I call the Run function again, It has the same message,\r\nD:\\projects\\c++\\tensorflow_GPU\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3039 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\r\nHow to avoid tensorflow calling \"created Tensorflow device\" one more time after call Run() again? Will it cause speed slower?\r\n\r\nPlease help me. Thank you in advance\r\n\r\n\r\n\r\n", "comments": ["Hello, @davidyec have you solved this?"]}, {"number": 28306, "title": "[tflite] CONV_2D with bias tensor index -1 causes type-mismatch RuntimeError", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have created `.tflite` with single `CONV_2D` op with bias tensor index `-1`(= optional).\r\n\r\n\r\nWhen supplying bias tensor `-1`(for making bias optional), it raises RuntimeError when running it in Python interpreter as shown the below(or seg fault/asan error in C++ interpreter) \r\n\r\nSupplying zero-valued bias tensor(i.e. use valid tensor id for bias) passes `AllocateTensors` and `Invoke` without any problem.\r\n\r\nFYI, we cannot omit bias tensor(i.e, `len(inputs) = 2`) since in such case we'll hit another assertion:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/lite/kernels/conv.cc#L237 \r\n\r\n \r\n**Describe the expected behavior**\r\n\r\nAs described in tflite document:\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_compatibility\r\n\r\n`CONV_2D` should accept optional bias tensor and does not raise RuntimeError.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSimply run python interepreter with attached `.tfite` file.\r\n\r\n```\r\nimport sys\r\n\r\nimport numpy as np\r\n\r\nimport tensorflow\r\nprint(tensorflow.__version__)\r\nfrom tensorflow.lite.python import interpreter as interpreter_wrapper\r\n\r\ninterpreter = interpreter_wrapper.Interpreter(model_path=sys.argv[1])\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nprint(input_details)\r\nprint(output_details)\r\n```\r\n\r\n[conv2d_bias_optional.tflite.zip](https://github.com/tensorflow/tensorflow/files/3134449/conv2d_bias_optional.tflite.zip)\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bora.py\", line 10, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/syoyo/miniconda3/envs/onnx-chainer/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 73, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/syoyo/miniconda3/envs/onnx-chainer/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:247 bias->type != input_type (0 != 1)Node number 0 (CONV_2D) failed to prepare.\r\n```\r\n\r\n", "comments": ["@syoyo Is this still an issue for you? If this is still an issue, Can you please share the model code. I tried loading your tflite model but noticed some error as shown in this [gist](https://colab.research.google.com/gist/jvishnuvardhan/bdba6d1bac86bde04dca0b1893a137fe/untitled971.ipynb).\r\n\r\nThanks!", "We are not using tflite anymore and does not plan to try to reproduce an issue, so you can close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28306\">No</a>\n"]}, {"number": 28305, "title": "tensorflow support for multiple GPUs", "body": "\r\n![Screenshot from 2019-05-01 17-05-24](https://user-images.githubusercontent.com/42462408/57015613-56fc0680-6c33-11e9-98c7-f5cee6d6062b.png)\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: CUSTOM CODE\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: UBUNTU 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: SOURCE\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Python version**: 3.6.7\r\n- **Bazel version (if compiling from source)**: 0.19.2\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: CUDA 10.1, cuDNN 7.5.1\r\n- **GPU model and memory**: 2 RTX 2080\r\n- **Exact command to reproduce**: \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n\r\ntensorflow and keras not able to support multiple GPUs\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nSource  Code\r\n\r\ndef train_model(base_path):\r\n    start = time.time()\r\n    print(\"***start training***\")\r\n    if not os.path.exists(base_path_xcep_mod):\r\n        os.makedirs(base_path_xcep_mod)\r\n        \r\n        \r\n\r\n\r\n    log_file_path = base_path + '_emotion_training.log'\r\n    #csv_logger = CSVLogger(base_path +\"log.csv\", append=False, separator=',')\r\n    early_stop = EarlyStopping('val_loss', patience=patience)\r\n    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\r\n    trained_models_path = base_path + '_mini_XCEPTION'\r\n    model_names = trained_models_path + '.{epoch:02d}.hdf5'\r\n    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\r\n    tbCallBack = TensorBoard(log_dir=base_path +\"log\", histogram_freq=0, \r\n                                             write_graph=True, write_images=True)\r\n    callbacks = [model_checkpoint, early_stop, reduce_lr, tbCallBack]\r\n\r\n    with tf.device(\"/cpu:0\"):\r\n        model = cnn_five_fc_two_two()\r\n\r\n    #model = multi_gpu_model(cnn_five_fc_two_two(), gpus=2)\r\n    model = multi_gpu_model( model, G )\r\n\r\n    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\r\n\r\n    \r\n\r\n\r\n\r\n    model.fit_generator(data_generator.flow(xtrain, ytrain,(batch_size * G)),\r\n\t    steps_per_epoch=len(xtrain) // (batch_size * G),\r\n\t    epochs=num_epochs,\r\n\t    callbacks=callbacks, verbose=2,\r\n            validation_data=(xtest, ytest)\r\n                       )\r\n\r\n\r\n    end = time.time()\r\n    print(\"time taken\", end-start)\r\n\r\nLogs\r\n\r\n\r\nUse tf.cast instead.\r\nEpoch 1/2\r\n2019-05-01 17:05:17.176287: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2019-05-01 17:05:18.149549: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-05-01 17:05:18.747678: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-05-01 17:05:18.748678: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-05-01 17:05:18.753580: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"ED_cnn_five_fc_two_two.py\", line 215, in <module>\r\n    train_model(base_path_xcep_mod)\r\n  File \"ED_cnn_five_fc_two_two.py\", line 203, in train_model\r\n    validation_data=(xtest, ytest)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1418, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 217, in fit_generator\r\n    class_weight=class_weight)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1217, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\r\n    return self._call(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\r\n    fetched = self._callable_fn(*array_vals)\r\n  File \"/home/aditi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"/home/aditi/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node replica_0/sequential_1/conv2d_1/convolution}}]]\r\n\r\n", "comments": ["Your code seems to be using an old-fashioned implementation.Try using Estimator and Distribution Strategy to use multiple GPUs.", "@Aditi0207 : TensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0) . Please have a look on this [link](https://www.tensorflow.org/install/gpu#software_requirements). Also try @rootkitchao's suggestion and let us know if that helps. Thanks!", "> @Aditi0207 : TensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0) . Please have a look on this [link](https://www.tensorflow.org/install/gpu#software_requirements). Also try @rootkitchao's suggestion and let us know if that helps. Thanks!\r\n\r\ni am using Tensorflow 1.13.1. with CUDA 10.1", "@Aditi0207 : Unfortunately, none of the TensorFlow versions officially support CUDA 10.1. As of now TensorFlow version >= 1.13.0 supports CUDA 10.0. Thanks!", "> @Aditi0207 : Unfortunately, none of the TensorFlow versions officially support CUDA 10.1. As of now TensorFlow version >= 1.13.0 supports CUDA 10.0. Thanks!\r\n\r\ni had built tensorflow from source with cuda 10.0\r\ni downgraded to 10.0. still encountering the same issue. not able to use 2 GPUs effectively for multi processing", "@Aditi0207 Please follow @rootkitchao suggestion. There are tutorials on TF website [here](https://www.tensorflow.org/tutorials). Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 28304, "title": "Error .\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131 while building", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Not mobile device**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version: **r1.13**\r\n- Python version: **3.6.8**\r\n- Installed using virtualenv? pip? conda?: **virtualenv**\r\n- Bazel version (if compiling from source):  **0.21.0**\r\n- GCC/Compiler version (if compiling from source): **\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0439 \u043a\u043e\u043c\u043f\u0438\u043b\u044f\u0442\u043e\u0440 Microsoft (R) C/C++ \u0432\u0435\u0440\u0441\u0438\u0438 19.00.24234.1 \u0434\u043b\u044f x64**\r\n- CUDA/cuDNN version: **No CUDA**\r\n- GPU model and memory: **No GPU**\r\n\r\n\r\n\r\n**Describe the problem**\r\nHi!\r\nI've tried to build tensorflow from source. I've installed all necessary tools but clashed with some errors while building tensorflow with XLA JIT support. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nMy config looks like that:\r\n**(.venv) C:\\libs\\tensorflow>python configure.py**\r\n\r\nlocation of python. [Default is C:\\libs\\tensorflow\\.venv\\Scripts\\python.exe]\r\nPython library path to use.  Default is [C:\\libs\\tensorflow\\.venv\\Lib\\site-packages]\r\nXLA JIT support will be enabled for TensorFlow.\r\nNo ROCm support will be enabled for TensorFlow.\r\nNo CUDA support will be enabled for TensorFlow.\r\nDefault is /arch:AVX\r\nEigen strong inline overridden.\r\n\r\nThen I started building process:\r\n**(.venv) C:\\libs\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package**\r\n\r\n**Any other info / logs**\r\nError message looks like that:\r\n...\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131: \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u043e\u0439\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): note: \u0441\u0431\u043e\u0439 \u0431\u044b\u043b \u0432\u044b\u0437\u0432\u0430\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0443\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f, \u043d\u0435 \u043f\u043e\u0434\u0434\u0430\u044e\u0449\u0438\u043c\u0441\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0443\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): error C2131: \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u043e\u0439\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): note: \u0441\u0431\u043e\u0439 \u0431\u044b\u043b \u0432\u044b\u0437\u0432\u0430\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0443\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f, \u043d\u0435 \u043f\u043e\u0434\u0434\u0430\u044e\u0449\u0438\u043c\u0441\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0443\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): error C2131: \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u043e\u0439\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): note: \u0441\u0431\u043e\u0439 \u0431\u044b\u043b \u0432\u044b\u0437\u0432\u0430\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0443\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f, \u043d\u0435 \u043f\u043e\u0434\u0434\u0430\u044e\u0449\u0438\u043c\u0441\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0443\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): error C2131: \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u043e\u0439\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): note: \u0441\u0431\u043e\u0439 \u0431\u044b\u043b \u0432\u044b\u0437\u0432\u0430\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0443\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f, \u043d\u0435 \u043f\u043e\u0434\u0434\u0430\u044e\u0449\u0438\u043c\u0441\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0443\r\n...\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1871,018s, Critical Path: 142,69s\r\nINFO: 664 processes: 664 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@MChaus Current master is TF1.13.1. checkout the master branch (not r1.13) and compile tensorflow from there?checkout the master branch (not r1.13) and compile tensorflow from there? Thanks!", "> @MChaus Current master is TF1.13.1. checkout the master branch (not r1.13) and compile tensorflow from there?checkout the master branch (not r1.13) and compile tensorflow from there? Thanks!\r\n\r\nUnfortunately building TF from master branch didn't help. I have the same error. \r\nAlso I checked git tag. The response of **git describe --tags** was:\r\nv1.12.1-1110-g530d379ac1\r\n\r\nAlso I've updated my bazel. Now I have 0.24.1 version.", "@MChaus The error trace log provided here is in other language, can you please repost in English.Also can you confirm which version of Microsoft Visual studio you have used. As per the build [link](https://tensorflow.google.cn/install/source_windows#cpu) Microsoft Visual studio is 2015.", "@muddham I have Microsoft VS Community 2017 with installed build tools for VC++ 2015.3. \r\nBuild was started in command line VS2015 x64 Native Tools.\r\n\r\nThe translation of the log:\r\n\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): note: the failure was caused by a pointer value that cannot be analyzed\r\n\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): note: the failure was caused by a pointer value that cannot be analyzed\r\n\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): note: the failure was caused by a pointer value that cannot be analyzed\r\n\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): note: the failure was caused by a pointer value that cannot be analyzed\r\n\r\nThis is a literal translation from Russian.", "@MChaus Please refer the [link](https://github.com/tensorflow/tensorflow/issues/24890),install TF 2.0 nightly directly via pip and it works and see if it works", "@muddham I've installed latest stable version of TF last week via pip - tensorflow 1.13.1. It works fine, but I want to try build TF with additional configurations like MKL for further acceleration. \r\nI desided to start from the simpliest default version without any additional dependencies and got these errors.", "@MChaus Could you please try installing TF using Conda [link](https://anaconda.org/anaconda/tensorflow-mkl). Please let us know how it progresses. Thanks!", "@muddham I've installed TF-mkl via Anaconda today and found out that it is 2 times slower than TF installed via pip. Maybe it's because I have AMD processor. Well, it looks like that I can use standard TF from pip without any inconveniences, but I can't build TF from source with custom configurations for today.", "@MChaus, our optimizations through MKL are built to accelerate performance on Intel Xeon processors. We don't support accelerations for other processors as of yet.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Please open a new issue if you notice any bug when you install TF from source. In this issue, you cannot use MKL to accelerate performance as it doesn't support AMD processor. Thanks!", "@jvishnuvardhan \r\nSorry to ping you all over the place, the bug is still a bug. Use conda doesn't resolve the esoteric compilation issues. \r\n\r\nI'm on an 8700k which supports both mkl and AVX. \r\n\r\n```\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): note: failure was caused by unevaluable pointer value\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): note: failure was caused by unevaluable pointer value\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): note: failure was caused by unevaluable pointer value\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): error C2131: expression did not evaluate to a constant\r\n.\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): note: failure was caused by unevaluable pointer value\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nWindows 10, bazel 24.1, cuda 10.0\r\n\r\nedit: https://github.com/tensorflow/tensorflow/issues/28211\r\n\r\nLooks like some people were able to bypass using vscode2019? Checking that out now. Maybe XLA does not work on windows?", "@rlewkowicz No problem. Ping me when you need help.\r\nBased on https://github.com/tensorflow/tensorflow/issues/28211#issuecomment-499219197, it seems XLA support was not built into TF builds related to Windows. It's better if you can open a new issue with details as the original issue was little different than what you have. Thanks!", "Closing XLA support at configure step helps.\r\n\r\n> XLA support was not built into TF builds related to Windows. \r\n\r\n``` \r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: **N**\r\n```\r\n"]}, {"number": 28303, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "Python 3.7.3\r\ntensorflow-gpu                     2.0.0a0\r\nNVIDIA CUDA runtime 10.1 with cudnn-10.1-windows10-x64-v7.5.1.10\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\hanna\\.vscode\\extensions\\ms-python.python-2019.4.12954\\pythonFiles\\ptvsd_launcher.py\", line 43, in <module>\r\n    main(ptvsdArgs)\r\n  File \"c:\\Users\\hanna\\.vscode\\extensions\\ms-python.python-2019.4.12954\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 410, in main\r\n    run()\r\n  File \"c:\\Users\\hanna\\.vscode\\extensions\\ms-python.python-2019.4.12954\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 291, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"c:\\Users\\hanna\\Google Drive\\hannannussbaum\\Data Science\\TensorFlowTest.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\audio\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hanna\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n", "comments": ["Tensorflow requires cuda10.0 instead of 10.1.Please reinstall the correct version of CUDA and cuDNN.", "I installed cuda 10 and cudnn for cuda 10.\r\nI followed instructions on:\r\nhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html\r\nI get this: \r\n\r\npython\r\nPython 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2019-05-01 19:15:16.317636: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-05-01 19:15:16.544680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.92GiB\r\n2019-05-01 19:15:16.551192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-05-01 19:15:17.055182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-01 19:15:17.058195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-05-01 19:15:17.060549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n2019-05-01 19:15:17.062575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4639 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n\r\nDoes this mean that installation is ok and it is using gpu with cuda and cudnn?\r\nI also downgraded tensorflow from ver 2.0 to 1.13 \r\nIs ver 2 supported?", "> I installed cuda 10 and cudnn for cuda 10.\r\n> I followed instructions on:\r\n> https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html\r\n> I get this:\r\n> python\r\n> Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> \r\n> \r\n> \r\n> import tensorflow as tf\r\n> hello = tf.constant('Hello, TensorFlow!')\r\n> sess = tf.Session()\r\n> 2019-05-01 19:15:16.317636: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n> 2019-05-01 19:15:16.544680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\n> name: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\n> pciBusID: 0000:01:00.0\r\n> totalMemory: 6.00GiB freeMemory: 4.92GiB\r\n> 2019-05-01 19:15:16.551192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n> 2019-05-01 19:15:17.055182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-05-01 19:15:17.058195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n> 2019-05-01 19:15:17.060549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n> 2019-05-01 19:15:17.062575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4639 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n> print(sess.run(hello))\r\n> b'Hello, TensorFlow!'\r\n> \r\n> \r\n> \r\n> Does this mean that installation is ok and it is using gpu with cuda and cudnn?\r\n> I also downgraded tensorflow from ver 2.0 to 1.13\r\n> Is ver 2 supported?\r\n \r\nSorry, I don't use Anaconda, and I'm not sure Tensorflow 1.13 supports Python 3.7.", "@hn2 This is duplicate of [26364](https://github.com/tensorflow/tensorflow/issues/26364). Thanks!", " Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28302, "title": "saved_model does not save and restore the state of the random number generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip wheel\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA (Running on CPU only)\r\n\r\n**Describe the current behavior**\r\n\r\nUsing tf.saved_model to save session does not save the state of the random number generator associated with the graph that the session executes.\r\n\r\n**Describe the expected behavior**\r\n\r\nI was hoping to find a way to save and restore the state of the random number generator. \r\n\r\n**Code to reproduce the issue**\r\n\r\nGiven two files, save_rng.py and restore_rng.py as below:\r\n\r\nsave_rng.py\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ntf.set_random_seed(0)\r\n\r\nsess = tf.Session()\r\n\r\nbase_dist = tfp.distributions.MultivariateNormalDiag(\r\n    loc=tf.zeros(4),\r\n    scale_diag=tf.ones(4)\r\n)\r\n\r\nsamples_0 = base_dist.sample(sample_shape=(2))\r\n\r\nbuilder = tf.saved_model.builder.SavedModelBuilder('./sess_save')\r\n\r\nbuilder.add_meta_graph_and_variables(sess,\r\n                                     [tf.saved_model.tag_constants.TRAINING])\r\n\r\nbuilder.save()\r\n\r\nsamples_1 = base_dist.sample(sample_shape=(2))\r\n\r\nout = sess.run([samples_0, samples_1])\r\n\r\nprint(out)\r\n```\r\n\r\nrestore_rng.py\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ntf.set_random_seed(0)\r\n\r\nsess = tf.Session()\r\n\r\nbase_dist = tfp.distributions.MultivariateNormalDiag(\r\n    loc=tf.zeros(4),\r\n    scale_diag=tf.ones(4)\r\n)\r\n\r\ntf.saved_model.loader.load(sess,\r\n                           [tf.saved_model.tag_constants.TRAINING],\r\n                           './sess_save')\r\n\r\nsamples_1 = base_dist.sample(sample_shape=(2))\r\n\r\nout = sess.run(samples_1)\r\n\r\nprint(out)\r\n\r\n```\r\n\r\n\r\nsave_rng.py is executed first, and then restore_rng.py.\r\n\r\n1. If the state of the random number generator is saved and restored, then out[1] in save_rng.py should have the same value as out in restore_rng.py. But they don't contain the same values.\r\n\r\n2. tf.saved_model.loader.load is definitely changing the state of the random number generator because out[0] in save_rng.py is not even equal to out in restore_rng.py, which should be equal otherwise because I'm setting the global level seed to 0.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@wangpengmit do we have stateless random distributions that are usable? And a guide or something to point people to?\r\n\r\nFor background, traditionally TF's random state has not been saveable. It has just lived in the kernels for the random operations. It also depends on the number of ops previously created in the graph, which may be the odd behavior you're seeing on load. We've recently added \"stateless\" versions of these operations which take the state as a Tensor. With that setup you can save the rng's state in a Variable and so in a SavedModel.", "@allenlavoie More precisely, we have a new *stateful* RNG (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/stateful_random_ops.py) that stores its state in tf.Variable and can be checkpointed. The only guide so far is the RFC (https://github.com/tensorflow/community/blob/master/rfcs/20181217-tf2-random-numbers.md) for it. We'll perhaps add a tutorial/guide in the future. There are also *stateless* RNGs (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/stateless_random_ops.py) that have existed for a while and may also suit your need.", "Maybe I'll start calling these \"external-state\" random ops if I remember. The point is that there's no state in the kernel itself, even if it modifies a variable.", "Closing this issue since the behavior is fixed by the new stateful ops and no work will be done to fix the old ones.", "Is there an equivalent way to separate `tf.data.Dataset.shuffle` state?"]}, {"number": 28301, "title": "saving tensorflow models using hdf5 (without using a keras interface)", "body": "how to saving tensorflow models using hdf5 (without using a keras interface) ?", "comments": ["You can try this out \r\nhttps://geekyisawesome.blogspot.com/2018/06/savingloading-tensorflow-model-using.html?m=1\r\n\r\nHope this helps.", "@elham1992  This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@elham1992 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28300, "title": "During cuda config detection, make find_cuda_config strip spaces around versions", "body": "PiperOrigin-RevId: 246040939", "comments": ["according to the presubmit, the issue is still there.\r\nI will keep investigating."]}, {"number": 28299, "title": "Correct out-of-date docs for StaticHashTable (redo of #28199)", "body": "The API documentation for `tf.lookup.StaticHashTable` hasn't been updated to reflect recent code changes. This PR updates the docstring for both the V1 and V2 entry points to that class with code snippets that work on the latest master branch. I've included separate example code for both eager mode and graph mode.", "comments": ["@MarkDaoust Could you PTAL and approve.", "@MarkDaoust can you please review this PR ?  ", "Nagging Reviewer @MarkDaoust, @rohan100jain: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 28298, "title": "auto_mixed_precision slow due to \"Sum\" being on blacklist.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 30d87b5299a393cd0608ec1181af9f4529065749\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: 8 V100s\r\n\r\n**Describe the current behavior**\r\n\r\n[Sum is on the blacklist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/auto_mixed_precision_lists.h#L168) for the auto_mixed_precision grappler pass. Unfortunately, Sum is used in [the gradient of tf.add](https://github.com/tensorflow/tensorflow/blob/63db602e38c0c82bed9dfcf96f7df2b2041ffb20/tensorflow/python/ops/math_grad.py#L1008), if non-static shapes are used. This greatly slows down [resnet50](https://github.com/tensorflow/models/tree/master/official/resnet), from 709 to 551 images/sec. The Sums are done in fp32, and relus and batchnorm gradients are fp32 as well.\r\n\r\nUsing static shapes fixes this issue, but many users use dynamic shapes with tf.add.\r\n\r\nSum was added to the blacklist in 180542ba39fdb4a61cbe7a92d7ef512526383dea. The justification is \"The dice loss function used in U-net uses Sums that overflow in fp16\"\r\n\r\n**Describe the expected behavior**\r\n\r\nSum should not be on the blacklist, and resnet50 should get 709 images/sec. Maybe we can find a way to do the dice loss function Sum in fp32, but other Sums in fp16?\r\n\r\n**Code to reproduce the issue**\r\nN/A\r\n\r\n**Other info / logs**\r\nN/A\r\n\r\n/CC @azaks2 @tfboyd @nluehr @benbarsdell @MattConley \r\n", "comments": ["https://github.com/tensorflow/tensorflow/pull/28057 is supposed to help with this; can you see if it improves performance in this case?", "I'll wait until the PR is merged, then try.", "This issue is fixed by #28057 "]}, {"number": 28297, "title": "Unexpected bahaviour of per_process_gpu_memory_fraction", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.30\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: RTX2080Ti,/11GB\r\n\r\n**Describe the current behavior**\r\nI experimented with the `per_process_gpu_memory_fraction` option (mainly to test unified memory) and found out that the option works only when the model is fairly small.\r\n\r\nTo compare, I have a very simple model (one Matmul and one add) and an AlexNet. \r\nThe experiment code:\r\n```python\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.7)\r\nconfig = tf.ConfigProto(gpu_options=gpu_options)\r\ns = tf.Session(config=config)\r\ns.run(tf.global_variables_initializer())\r\ns.run(train_op)\r\n```\r\nIn which `train_op` is the target op of the simple model and alexnet.\r\n\r\n- With the simple model, I can verify that the allocation always follows the set ratio, for instance in the case above (1.7x more memory), I can see from the verbose log\r\n\r\n```\r\ntensorflow/stream_executor/stream_executor_pimpl.cc:524] Called    StreamExecutor::UnifiedMemoryAllocate(size=19587203072) returns 0x7f03ac000000\r\ntensorflow/core/common_runtime/bfc_allocator.cc:135] Extending allocation by 18.24GiB bytes.\r\n```\r\nAnd it works when the ratio is less than 1.0, it just allocates less memory.\r\n\r\n- With AlexNet, no matter what the number I set, it will always allocate what's available. I ran gdb on the C++ library and stopped at `core/common_runtime/gpu/gpu_devices.cc` in function `CreateDevices()` and printed out the `SessionOption` protobuf. `per_process_gpu_memory_fraction` is always 0.\r\n\r\n- If I define both models, the behavior is the same as that in the AlexNet case. I guess adding many layers changed something in tensorflow python library?\r\n\r\n- Also, I saw somewhere that if `per_process_gpu_memory_fraction` is too small and will not fit the model, tensorflow will ignore the option. I tested with 0.5 and I think 5GB is more than enough to train AlexNet with batch size of 8.\r\n\r\n**Describe the expected behavior**\r\nAs long as the memory specified (`total_mem * per_process_gpu_memory_fraction`) fits the model, the option should be preserved. Especially when unified (really the managed memory) memory is available on GPUs.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nThe simple model:\r\n```\r\na = tf.get_variable('a', [1024, 1024], tf.float32, xavier_initializer())\r\nwith tf.device(\"/device:CPU:0\"):\r\n    b = tf.ones([1024, 1], name='b')\r\n\r\nwith tf.device(\"/device:GPU:0\"):\r\n    axb = tf.matmul(a, b, name='axb')  # 1024x1\r\n    relu_b = tf.nn.relu(b, name='relu_b')  # 1024x1\r\n\r\ntrain_op = axb + relu_b\r\n```\r\nAlexNet is a standard implementation with `tf.nn.conv2d` and `tf.nn.relu`, etc\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThanks a lot for your help!", "comments": ["After some more digging, found that, \r\n\r\nif the session is created before the model is defined, the option works fine.", "@LuyuanChen Is this resolved? If it was resolved, Please close the issue. Thanks!", "I don't understand. It's some undefined behaviour in tf. Although I can get past it, it doesn't mean the bahaviour is correct. Please take a look.", "@LuyuanChen Sorry for the delay in my response. Could you check whether the issue still persists with TF1.14 or tf-nightly (TF1.15). Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28297\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28297\">No</a>\n"]}, {"number": 28296, "title": "5 more cherrypicks into 1.14", "body": "", "comments": []}, {"number": 28295, "title": "crosstool_wrapper_driver_is_not_gcc.tpl: Pass -no-canonical-prefixes through to the host compiler.", "body": "-no-canonical-prefixes is useful for preventing GCC from emitting absolute paths into .d files.", "comments": ["Austin, can you take a look? Looks ok to me, but I have no idea, really.", "This looks like a no-op for anyone who doesn't care about `-no-canonical-prefixes`, so it seems good to me."]}, {"number": 28294, "title": "Fix documentation in `tf.keras.layers.Add` example", "body": "This fixes #27632\r\n(https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add)", "comments": ["Merged in ac6e979", "@mihaimaruseac Thank you."]}, {"number": 28293, "title": "Cherrypicks gpu memory fixes", "body": "Cherry-pick GPU memory changes to ensure that horovod can support 1.14.", "comments": []}]