[{"number": 34544, "title": "Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [585,1024,3], [batch]: [600,799,3]", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary (Used pip to install tensorflow 1.15 and downloaded models from github and used checkout version v1.13.0 and after that followed: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6.1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K80\r\n\r\n\r\nI am trying to train a model, at first I had dataset of 5000 images and training worked fine, Now I have added couple of more images and now my dataset contains 6,423\u202c images. Now when I use:\r\n\r\npython model_main.py --logtostderr --pipeline_config_path=training/faster_rcnn_resnet50_coco.config --model_dir=training\r\n\r\nIt starts settings up for couple of minutes and after these lines:\r\n\r\n```\r\n    INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.                                                                                                                                                                           \r\n    I1123 10:26:21.548237 140482563244160 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.                                                                                                                     \r\n    2019-11-23 10:28:30.801453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0 \r\n```\r\n\r\nI get following erros:\r\n\r\n```\r\n    2019-11-23 10:08:38.843259: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_3_hash_table_2/N10tensorflow6lookup15LookupInterfaceE does not exist.               \r\n    2019-11-23 10:08:38.843323: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_1_hash_table_1/N10tensorflow6lookup15LookupInterfaceE does not exist.               \r\n    2019-11-23 10:08:38.843345: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_2_hash_table/N10tensorflow6lookup15LookupInterfaceE does not exist.                 \r\n    2019-11-23 10:08:38.851405: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_3_hash_table_2/N10tensorflow6lookup15LookupInterfaceE does not exist.               \r\n    2019-11-23 10:08:38.851488: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_1_hash_table_1/N10tensorflow6lookup15LookupInterfaceE does not exist.               \r\n    2019-11-23 10:08:38.851512: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_2_hash_table/N10tensorflow6lookup15LookupInterfaceE does not exist.                 \r\n    2019-11-23 10:08:38.851807: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_1_hash_table_1/N10tensorflow6lookup15LookupInterfaceE does not exist.               \r\n    2019-11-23 10:08:38.851848: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_2_hash_table/N10tensorflow6lookup15LookupInterfaceE does not exist.                 \r\n    2019-11-23 10:08:38.851899: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:788 : Not found: Resource localhost/_3_hash_table_2/N10tensorflow6lookup15LookupInterfaceE does not exist.               \r\n    Traceback (most recent call last):                                                                                                                                                                                                             \r\n    File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call                                                                                                                                 \r\n     return fn(*args)                                                                                                                                                                                                                           \r\n    File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn                                                                                                                                  \r\n     target_list, run_metadata)                                                                                                                                                                                                                 \r\n    File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun                                                                                                                      \r\n     run_metadata)                                                                                                                                                                                                                            \r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.                                                                                                                                                           \r\n    (0) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [585,1024,3], [batch]: [600,799,3]                                                                                                   \r\n    [[{{node IteratorGetNext}}]]                                                                                                                                                                                                                 \r\n    [[ToAbsoluteCoordinates_118/Assert/AssertGuard/Assert/data_0/_5709]]                                                                                                                                                                  \r\n    (1) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [585,1024,3], [batch]: [600,799,3]                                                                                                   \r\n    [[{{node IteratorGetNext}}]]                                                                                                                                                                                                        \r\n    0 successful operations.                                                                                                                                                                                                                     \r\n    0 derived errors ignored. \r\n```\r\n\r\n\r\nand training stops.\r\n\r\nMy train.record and test.record: https://drive.google.com/file/d/1sWn_bNp_9TwGMDxxA52WKgeI9B7GfBnr/view?usp=sharing\r\n\r\nI am using faster_rcnn_resnet50_coco_2018_01_28 and my config file: https://drive.google.com/file/d/1tsKcPgDUoQdERzvFCfQ4eBBraS4DcNnb/view?usp=sharing", "comments": ["Update: Changed the batch_size to 1 in config file and I am not getting that error anymore, instead I get following error now:\r\nInvalid argument: assertion failed: [maximum box coordinate value is larger than 1.100000: ]\r\nbut If I put anything other than 1 in batch_size I get the same error and sometimes shapes are same and sometimes they are different like:\r\n[tensor]: [600,900,3], [batch]: [576,1024,3]\r\n[tensor]: [600,800,3], [batch]: [ 600,900,3] \r\netc", "@D4n1aLLL,  Could you post your complete code for us to analyze the issue better?", "> \r\n> \r\n> @D4n1aLLL, Could you post your complete code for us to analyze the issue better?\r\n\r\nI didnt write any code. I am using this: https://github.com/tensorflow/models/blob/v1.13.0/research/object_detection/model_main.py", "@D4n1aLLL Please use tensorflow 1.13 and let me konw if you are still running into the same error. Thanks!", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments and we can open the issue again. Thanks!", "Had a similar setup (Fine tuning RN50, using `model_main.py`.\r\nChanging the batch size back to 1 fixed the issue.", "Changing batch size to 1 fixed the issue but you are still not able to train with a batch size > 1. \r\nTo be able to do that, you have to set `image_resizer` properties (by fixing image size).\r\nYou should have something like that in your `pipeline.config` : \r\n```\r\nimage_resizer {\r\n  keep_aspect_ratio_resizer {\r\n    min_dimension: 600\r\n    max_dimension: 1024\r\n  }\r\n}\r\n```\r\nChange it to something like : \r\n```\r\nimage_resizer {\r\n  fixed_shape_resizer {\r\n    height: 600\r\n    width: 800\r\n  }\r\n}\r\n```\r\nAnd now you can put the batch size you want !", "Does not work"]}, {"number": 34543, "title": "Hey i need help with this problem in kali linux", "body": "Umm so i was using Kali linux today and I wanted to download HiddenEye but i get this problem here is a screneshot - http://prntscr.com/q0xuqi\r\n\r\nAny way on how i can fix that and i am new to kali linux stuff", "comments": ["@Feelingloved ,\r\nThis is not Build/Installation or Bug/Performance issue. This repo is to report issue related to Tensorflow/core.Please raise the issue in right repo.Thanks!\r\n"]}, {"number": 34542, "title": "\u5411\u4e0b\u517c\u5bb9\u6027\u592a\u5dee\u4e86\uff0c\u4e00\u5347\u7ea7\u5168\u5b8c\u86cb\uff0c\u597d\u6c14\u4eba\u554a\uff01", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["\u6587\u4ef6\u8def\u5f84\u4e0d\u65ad\u5730\u4fee\u6539 \u6362\u6765\u6362\u53bb\uff0c\u6655", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new/choose). Please provide all the information it asks. Thank you.\r\n", "xswl"]}, {"number": 34541, "title": "RNN with cell  get_initial_state  and state_size incompatible", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): TF2.0\r\n- Python version: py3.74\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(None, 100, 1), ndim=3)]); however `cell.state_size` is [100]\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```\r\n# coding:utf-8\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.layers as layers\r\nimport numpy as np\r\nfrom tensorflow.keras.utils import plot_model\r\n\r\nfrom tensorflow.keras.utils import plot_model\r\nfrom tensorflow.keras import layers\r\ntf.keras.backend.set_floatx('float64')\r\n#%%###################################################################\r\n# y(n) = func_g(x)y(n-1) + X\\beta + \\alpha\r\n###############################################################\r\ndef func_g (x):\r\n    v = np.sin(x)\r\n    return v\r\n########################################################\r\n#%% deine the neutron for X\\beta + alpha\r\n############################################\r\nclass Linear(layers.Layer):\r\n\r\n  def __init__(self, CityNum, CityFactorNum):   \r\n    self.CityNum = CityNum    \r\n    self.CityFactorNum = CityFactorNum\r\n    super(Linear, self).__init__()\r\n    \r\n  def build(self, input_shape):\r\n    self.beta  = self.add_weight(shape=(self.CityFactorNum, 1),  initializer='random_normal', trainable=True)\r\n    self.alpha = self.add_weight(shape=(self.CityNum,),        initializer='random_normal', trainable=True)\r\n\r\n  def call(self, X):\r\n    v = tf.matmul( X[:,1:], self.beta)  + self.alpha[ tf.dtypes.cast( X[0,0], tf.int32 ) ]\r\n    return v\r\n#############################################33\r\n#%% define rnn cell node\r\n###############################################\r\nclass MinimalRNNCell(tf.keras.layers.Layer):\r\n    def __init__(self, units, CityNum, CityFactorNum, **kwargs):\r\n        self.units = units\r\n        self.state_size = 100\r\n        self.CityNum = CityNum    \r\n        self.CityFactorNum = CityFactorNum\r\n        super(MinimalRNNCell, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        \r\n        self.beta  = self.add_weight(shape=(self.CityNum,),  initializer='random_normal', trainable=True)\r\n        self.alpha = self.add_weight(shape=(self.CityFactorNum-1,), initializer='random_normal', trainable=True)\r\n        \r\n        self.dense_1  = layers.Dense(32, activation='tanh')\r\n        self.dense_2  = layers.Dense(64, activation='tanh')\r\n        self.dense_3  = layers.Dense(64, activation='tanh')\r\n        self.dense_4  = layers.Dense(64, activation='tanh')\r\n        self.dense_5= layers.Dense(1)\r\n\r\n        \r\n        self.Xbeta_Add_alpha = Linear(self.CityNum, self.CityFactorNum)\r\n                \r\n        self.built = True\r\n    \r\n    def get_initial_state(self, inputs, batch_size=None, dtype=None):\r\n        initial_states = []\r\n        initial_states = [tf.ones(1)]\r\n\r\n        return tuple(initial_states)\r\n        \r\n\r\n    def call(self, inputs, states):\r\n \r\n        X_input = inputs[0]\r\n        U_input = inputs[1]\r\n     \r\n        s1 = states[0] \r\n\r\n        gU = self.dense_1(U_input)\r\n        gU = self.dense_2(gU)\r\n        gU = self.dense_3(gU)\r\n        gU = self.dense_4(gU)\r\n        gU = self.dense_5(gU)\r\n        X = self.Xbeta_Add_alpha(U_input)\r\n\r\n        gUZ  = layers.dot( [gU, s1], axes=1, name = 'dot')\r\n        gUZX = layers.add( [gUZ, X], name = 'add')\r\n    \r\n        output = [gUZ, gUZX]\r\n        new_state = [gUZX]\r\n        \r\n        return output, new_state\r\n\r\n#########################################\r\n#%% define model \r\nsample_num = 2000\r\ntime_step = 100\r\nCityNum, CityFactorNum = 100, 1\r\nUFactorNum = 1\r\n#############################################\r\ndef rnn_model (CityNum, CityFactorNum, time_step):\r\n    \r\n    input_X = tf.keras.Input(shape=[time_step, CityFactorNum])\r\n    input_U = tf.keras.Input(shape=[time_step, UFactorNum])\r\n    \r\n    \r\n    \r\n    cells = MinimalRNNCell(1, CityNum, CityFactorNum)\r\n    \r\n    #rnn = tf.keras.layers.RNN(cells, return_sequences=True)(input)\r\n    #out = tf.keras.layers.Dense(units=1)(rnn)\r\n\r\n    out = tf.keras.layers.RNN(cells, return_sequences=True)([input_X, input_U])\r\n    out = tf.keras.layers.Dense(1)(out)\r\n    \r\n    \r\n    \r\n    model = tf.keras.Model(inputs=[input_X, input_U], outputs=out)\r\n    plot_model(model, to_file='model.png')\r\n    \r\n    model.summary()\r\n    model.compile(optimizer='rmsprop',  loss=['mse'],  metrics=['mse'])\r\n    \r\n    return model \r\n\r\n################################################################\r\n#%% set the test data\r\n##(sample_num,  time_step_num, units)\r\n######################################################################\r\nsample_num = 2000\r\ntime_step = 100\r\nCityNum, CityFactorNum = 100, 1\r\n\r\nX = np.random.uniform(-10,10, size=(sample_num,time_step,1))\r\nY = np.zeros((sample_num,time_step,1))\r\n\r\n#for i1 in range()\r\n\r\n\r\nfor i1 in range(sample_num):\r\n    for i2 in range(1,time_step):\r\n        for i3 in range(1):\r\n            Y[i1, i2, i3] = func_g(X[i1, i2, i3]) +  Y[i1, i2-1, i3]\r\n\r\n#-----------------------------------------------------------------------\r\nmodel = rnn_model (CityNum, CityFactorNum, time_step)\r\nmodel.fit(X, Y, batch_size = 100, epochs = 50)\r\n#----------------------------------------------------------------------\r\nstart = np.random.uniform(-10,10, size=(1,time_step,1))\r\nstart = np.linspace(-10,10,time_step)\r\nstart = np.reshape(start, (1,time_step,1))\r\nnext = model.predict(start)\r\n\r\n\r\nplt.plot(start[0,:,0], next[0,:,0],'rs')\r\n################################################################\r\n#%% true solution \r\n##############################################################\r\nnext = next\r\nfor i1 in range(1):\r\n    for i2 in range(1,time_step):\r\n        for i3 in range(1):\r\n            next[i1, i2, i3] = func_g(start[i1, i2, i3]) + next[i1, i2-1, i3]\r\n\r\nplt.plot(start[0,:,0], next[0,:,0],'bo')\r\n\r\n```", "comments": ["it seems that the error occurs  with the following  (([input_X, input_U]))\r\n    out = tf.keras.layers.RNN(cells, return_sequences=True)([input_X, input_U])\r\n", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191124 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e17b1d46e3672796971d566ba519b171/untitled400.ipynb). Thanks!", "The issue happens because you override the method of get_initial_state(), which returns a state that has a wrong shape. Usually the state tensor should have shape (batch, state_size) with all zeros. If you need to provide custom values (one in this case), you should return tf.ones((batch_size, state_size)), rather than tf.ones(1).\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34541\">No</a>\n", "@qlzh727 hi\uff0ci want train a ctc model, and i use some code like\r\n       `\r\n1. def rnn_layer(self, inputs, states=None):\r\n2.               gru = tf.keras.layers.GRUCell(1024)\r\n3.              if states is None:\r\n4.                  states = gru.get_initial_state(inputs=inputs)\r\n5.                  layers = tf.keras.layers\r\n6.                  inner, last_state = tf.keras.layers.RNN(\r\n7.                       cell=gru,\r\n8.                       return_sequences=True,\r\n9.                       return_state=True\r\n10.                 )(inputs=[inputs], initial_state=[states])\r\n\r\n`\r\n\r\nwhen i run , and i get error\" assert initial_state is None and constants is None\",  hope some advice, thxs", "Please unwrap the [inputs] since rnn layer only expect one input, and it shouldn't be a list.\r\n\r\n"]}, {"number": 34540, "title": "autograph bug", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.15\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nautograph failed when I use autoaugment which provided on [here](https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py) in tf.function\r\n**Describe the expected behavior**\r\nWorks well with autograph\r\n**Code to reproduce the issue**\r\n[colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/zh-cn/tutorials/quickstart/beginner.ipynb#scrollTo=jvl8IpUaM-VL)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[tensorflow.log](https://github.com/tensorflow/tensorflow/files/3882224/tensorflow.log)\r\n\r\n", "comments": ["fixed with separate functions"]}, {"number": 34539, "title": "RNN document is not clear", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nProblem 1:\r\nCould you provides the math model of the rnn?\r\n\r\nProblem 2:   this is not return information about  the method of \r\nget_initial_state(inputs=None, batch_size=None, dtype=None).\r\nand \r\nhow to define own get_initial_state ?\r\nwhat is the connect between   state_size  and get_initial_state\r\nProblem 3, \r\ncould  you write more clear about  arguments  of the methods:  call, build, get_initial_state,\r\nsuch as the   dimension of them.  each dimension is what .\r\n\r\nProblem 4:\r\nis the  cell  defined layer by user ?\r\n\r\nthanks! \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n \r\n\r\n", "comments": ["@ymodak  Can this issue be included for Google Code-in as a task ?", "If no one is working on it, I can take this task-up.", "I getting this issue since long back .. I posted but it seems no one is taking care this issue . TF1.X \r\nversion was pretty clear and return state as well. TF2.0 RNN document is not clear and the return type specially get_initial_state(inputs=None, batch_size=None, dtype=None).\r\nmy issue n0  #34850\r\n@Dexter2389 It would be great if you take care this issue and resolve it.\r\n\r\nHi, I am working on tensorflow 2.0 and getting an error at line 3 while running the model\r\n\r\n1) lstm_cell =tf.keras.layers.LSTMCell(units=128)\r\n2) lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=0.5)\r\n3) self._initial_state = lstm_cell.get_initial_state(128, tf.float32)\r\n\r\nGot ERROR at line 3\r\nValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\nmay I know why I am getting this error?", "The documentation for `tf.keras.layers.RNN` has been significantly improved now.\r\nIt answers most of the ask in the original issue such as arguments of the methods: call, build, get_initial_state and more.\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 34538, "title": "Build from head fails with Bazel 1.1.0 but succeeds with Bazel 0.26.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source (building source issue)\r\n- TensorFlow version: build from head (2.0.0)\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 1.1.0 fails while 0.26.1 succeeds\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: Nvidia RTX 2080 Ti\r\n\r\n**Describe the problem**\r\n\r\nBuilding with Bazel 1.1.0 fails with the following message:\r\nImportError: /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN10tensorflowlsERSoRKNS_6StatusE\r\n\r\nNote: The failure of target //tensorflow/python/keras/api:create_tensorflow.python_api_1_keras_python_api_gen (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.\r\nIf this error started occurring in Bazel 0.27 and later, it may be because the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. See https://github.com/bazelbuild/bazel/issues/7899 for more information.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout -b mybranch\r\n./configure\r\n[tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/3882194/tf_configure.bazelrc.txt)\r\nbazel build --explain=verbose_explanations.txt --verbose_explanations --verbose_failures --subcommands=pretty_print --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package &> log.txt\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nSee ful log.txt (which shows failure with Bazel 1.1.0)\r\nand log2.txt (which shows success with Bazel 0.26.1)\r\nfrom the following link: [Dropbox folder](https://www.dropbox.com/sh/mz1qvrd2yw0fb0a/AAB5ZAIWH0fj7C5okI1By-rna?dl=0)", "comments": ["@dbonner,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34538\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34538\">No</a>\n"]}, {"number": 34537, "title": "[tflite] add int8 input/output to label_image", "body": "More and more models, such as [MobilenetV3's EdgeTPU ones](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet), are using [post-training full integer quantization](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations). With this patch, I can get reasonable results.\r\n\r\n```\r\n./label_image_int8 -m mobilenet_edgetpu_224_1.0_int8.tflite\r\nLoaded model mobilenet_edgetpu_224_1.0_int8.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked\r\naverage time: 15.363 ms\r\n0.867188: 653 military uniform\r\n0.0390625: 835 suit\r\n0.015625: 458 bow tie\r\n0.0078125: 907 Windsor tie\r\n0.00390625: 716 pickelhaube\r\n```", "comments": ["@srjoglekar246 Can you please take a look on this PR? Thanks!", "Here are internal failures , can you please check \r\n`In file included from ./third_party/tensorflow/lite/examples/label_image/get_top_n.h:19:\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:33:16: error: unknown type name 'TfLiteType'\r\n               TfLiteType input_type) {\r\n               ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n.h:27:16: error: unknown type name 'TfLiteType'\r\n               TfLiteType input_type);\r\n               ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n.h:31:69: error: unknown type name 'TfLiteType'\r\n                               std::vector>*, TfLiteType);\r\n                                                                    ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n.h:30:15: error: explicit instantiation of 'get_top_n' does not refer to a function template, variable template, member function, member class, or static data member\r\ntemplate void get_top_n(float*, int, size_t, float,\r\n              ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n.h:34:33: error: unknown type name 'TfLiteType'\r\n                                TfLiteType);\r\n                                ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n.h:32:15: error: explicit instantiation of 'get_top_n' does not refer to a function template, variable template, member function, member class, or static data member\r\ntemplate void get_top_n(int8_t*, int, size_t, float,\r\n              ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n.h:37:34: error: unknown type name 'TfLiteType'\r\n                                 TfLiteType);\r\n                                 ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n.h:35:15: error: explicit instantiation of 'get_top_n' does not refer to a function template, variable template, member function, member class, or static data member\r\ntemplate void get_top_n(uint8_t*, int, size_t, float,\r\n              ^\r\nIn file included from ./third_party/tensorflow/lite/examples/label_image/get_top_n.h:19:\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:44:12: error: use of undeclared identifier 'kTfLiteFloat32'\r\n      case kTfLiteFloat32:\r\n           ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:47:12: error: use of undeclared identifier 'kTfLiteInt8'\r\n      case kTfLiteInt8:\r\n           ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:50:12: error: use of undeclared identifier 'kTfLiteUInt8'\r\n      case kTfLiteUInt8:\r\n           ^\r\n11 errors generated.\r\n\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:33:16: error: unknown type name 'TfLiteType'\r\n               TfLiteType input_type) {\r\n               ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:44:12: error: use of undeclared identifier 'kTfLiteFloat32'\r\n      case kTfLiteFloat32:\r\n           ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:47:12: error: use of undeclared identifier 'kTfLiteInt8'\r\n      case kTfLiteInt8:\r\n           ^\r\n./third_party/tensorflow/lite/examples/label_image/get_top_n_impl.h:50:12: error: use of undeclared identifier 'kTfLiteUInt8'\r\n      case kTfLiteUInt8:`", "I think we need to add a BUILD dependency on `tensorflow/lite/c:common` & import `common.h` for this to BUILD properly using bazel. @freedomtan can you take a look?", "@srjoglekar246 I cannot reproduce what @rthadur said on my Mac (with either Python 2 or 3). I'll add the dependency you suggest later anyway.", "@yifeif May I know the reason of rollback so that I can fix it?", "Thanks for checking @freedomtan! It broke one of the internal address sanitizer test. @srjoglekar246, do you mind working with @freedomtan to get this change in again? Thank you :)"]}, {"number": 34536, "title": "Is TF Lite optimized for nvidia gpu's and Intel CPUs?", "body": "Is TF Lite optimized for nvidia gpu's (as similar to TensorRT) and Intel CPUs. If not is there a list of supported hardware/accelerators out there?\r\n\r\n", "comments": ["I think there is a plan in 2019 to add \"First-class x86 support\". At least that is written in the roadmap https://www.tensorflow.org/lite/guide/roadmap\r\n\r\nSo hopefully we will be able to optimize models on intel cpus.", "re: Nvidia GPU: No.  We targeted major mobile GPUs and they include Adreno, Mali, and PowerVR.\r\n\r\nre: Intel CPU: You got the answer from @Cospel up there :)", "@impjdi   regarding the question if TF Lite is optimized for x86 CPU:  A post dated August 2019 about a 2019 Roadmap does not answer the question if it has been optimized for x86 CPUs. ", "@dlarue \r\n\r\nUnfortunately, I'm not a direct member of TFLite and don't know their plans in detail.  AFAIK there are plans to support XNNPACK and from what I've seen, it woks great on Intel CPU-based desktops quite well.", "are there any updates to the x86 support?", "Maybe with TF 2.3 release.", "@impjdi, hey, it\u2019s been a year \u2014\u00a0is it possible to use CUDA for Lite models? I see the `tensorflow-gpu` package and [a good amount of instructions](https://www.tensorflow.org/install/gpu). However, there are no mentions of `tensorflow.lite.Interpreter`.", "For that, you may want to create a CUDA delegate.  The link you provided is for TF, *not* TFLite.", "Any update?"]}, {"number": 34535, "title": "[Intel MKL] Fixing a bug in Elu Op", "body": "This PR fixes a bug in Elu op when fused with Convolution. It also updates the unit test by using different random generator to generate input with both negative and positive values. Previously the random input was only positive. So the bug was never caught.", "comments": ["Thank you @penpornk. I have fixed the merge conflicts. "]}, {"number": 34534, "title": "[TF2.0] tf.function use within tf.custom_gradient is extremly slow", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I wrote a simple example similar to the provided examples\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not tested\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below): r2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): does not apply\r\n- GCC/Compiler version (if compiling from source): does not apply\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Nvidia GTX 1050 2048MB\r\n\r\n**Describe the current behavior**\r\nWhen calling a `tf.function` from within a `tf.custom_gradient` function a large delay occurs **before** and **after** the `tf.function` call. With more complex functions this delay before and after goes up.\r\n\r\n**Describe the expected behavior**\r\nThere should not be any delay or at least no perceivable.\r\n\r\n**Code to reproduce the issue**\r\nThe following code snippet reproduces this behavior. I execute the code twice to account for `tf.function`s tracing which has to be one once for `_df`.\r\n```python\r\n@tf.function\r\ndef _df(x):\r\n    tf.print('Start inner')\r\n    start = tf.timestamp()\r\n    \r\n    result = x@x\r\n    \r\n    end = tf.timestamp()\r\n    tf.print('End inner; took: ', end-start)\r\n    return result\r\n    \r\n@tf.custom_gradient\r\ndef f(x):\r\n    def df(dy):\r\n        tf.print('Before outer')\r\n        start = tf.timestamp()\r\n        \r\n        grad = _df(x)\r\n        \r\n        end = tf.timestamp()\r\n        tf.print('After outer; took: ', end-start)\r\n        return dy*grad\r\n    return x@x, df\r\n\r\n\r\nfor i in range(2):\r\n    x = tf.random.normal((1000, 1000))\r\n    t = tf.timestamp()\r\n    with tf.GradientTape() as tape:\r\n        tape.watch([x])\r\n        r = f(x)\r\n    tf.print('Forwardpass took', tf.timestamp() - t)\r\n    t = tf.timestamp()\r\n    tape.gradient(r, x)\r\n    tf.print('Backwardpass took', tf.timestamp() - t)\r\n```\r\n\r\nOutput:\r\n```\r\nForwardpass took 0.00038313865661621094\r\nBefore outer\r\nStart inner\r\nEnd inner; took:  2.6941299438476563e-05\r\nAfter outer; took:  0.093798160552978516\r\nBackwardpass took 0.099112987518310547\r\nForwardpass took 0.0002689361572265625\r\nBefore outer\r\nStart inner\r\nEnd inner; took:  2.8848648071289063e-05\r\nAfter outer; took:  0.00881505012512207\r\nBackwardpass took 0.01347804069519043\r\n```", "comments": ["@n-gao \r\nI have tried on colab with TF version 2.0 ,2.1.0dev20191124 .Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/4108b210f3e0fb3f67090211140ab36f/untitled401.ipynb). Is this the expected behavior? Thanks!", "Yes it is but I forgot about asynchronous execution and this is probably the intended behavior, right? The inner loop has a low execution time because all of the commands are queued to the GPU and when the `tf.function` exists tf waits for the complete execution of the function. Or did I miss something?\r\nThank you!", "@n-gao you are right. This is the intended behaviour.\r\n\r\nI am closing this issue as it is not a bug/performance, build/install, feature request related issues. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 34533, "title": "[r2.1:Cherrypick]Override EIGEN strong inline for release builds as well.", "body": "PiperOrigin-RevId: 282049163\nChange-Id: I2fefa6ed9198aee013ce3bdc07d21dfb127833a7", "comments": []}, {"number": 34532, "title": "[ROCm] r1.15 rccl upstream patch", "body": "This patch is a backport of current RCCL support in master for the r1.15 branch.  RCCL support was not complete in the r1.15 branch, and since this is the last V1 release branch, it is important to have this feature here.\r\n\r\nFurther, without this PR, the r1.15 branch will not build for the latest ROCm release due to missing clang 10-based header files.  See #31849 for the same change to master.", "comments": ["Thank you @mihaimaruseac for approving.  I was concerned with the number of failing checks even though the build was successful for ROCm and non-ROCm paths on our test systems.  I tried reproducing the sanity test failure locally but the indicated failures were unrelated to this PR.  I hope you were able to otherwise test this satisfactorily.  ", "We have some issues with the tests on the old release branches. We're working on fixes.", "Hi @mihaimaruseac , could you help update the ETA to have this PR merged to r1.15 release branch?", "I think first week of December?", "@mihaimaruseac gentle ping, thanks!", "@mihaimaruseac gentle ping, thanks.", "Apologies. I didn't yet get a chance to investigate why the CI fails on the release branches. It seems to be picking some configuration from master but didn't yet have time to dig more into this.", "@mihaimaruseac gentle ping, thanks.  This is also blocking the subsequent PR https://github.com/tensorflow/tensorflow/pull/34769.", "Apologies for the delay. I tried now to get https://github.com/tensorflow/tensorflow/pull/33981 merged so that the builds would run against the `r1.15` branch instead of `master`. However, the builds use remote execution and at the moment our remote execution is only configured from `HEAD`.\r\n\r\nI'll try over the holidays and bring @gunan in too and see what we can do.", "So I got the linux builds and one Windows one to pass on the 2.0 equivalent of #33981 (#33982). I'll try to get the others too but by start of the new year I will cherry-pick these changes to #33981 with as many presubmits passing as possible.", "Actually, remote builds do not care about which branch we are running from.\r\nAll remote build parameters, docker containers and the compiler options are baked into the branch, under `third_party/toolchains/preconfig` folder.", "@mihaimaruseac @gunan, gentle ping\r\n\r\nanything we can / need to do on our end to help out?", "#33981 was merged less than an hour ago. This means we can attempt running presubmits on the branch.\r\n\r\nProbably some will fail at this moment as VMs changed but I will run presubmits again later in the night, when the VMs for this branch can be reused.", "Windows Bazel GPU and Ubuntu Sanity failures are expected at this time, trying them again later.", "@jeffdaily @deven-amd can you please check the Linux GPU build? I'm still going to run it on the 1.15 VMs around 8 hours from now, just in case the failure is not related to the PR but to the VMs.\r\n\r\nOnce this is merged, I think next is #34769 and #35230. Both of them are gated on this one, right?", "According to https://source.cloud.google.com/results/invocations/efa3a582-4b1e-472a-af44-1bad8131553e/targets/%2F%2Ftensorflow%2Fcore%2Fkernels:collective_nccl_test_gpu/log (the only failure on the proper VMs), we have some failures introduced by this PR (other PRs on the branch are completely green)\r\n\r\n```\r\n2020-01-16 06:03:24.050562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/device:GPU:0 with 1628 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\r\n2020-01-16 06:03:24.154879: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-01-16 06:03:24.154914: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n*** Received signal 6 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/b/f/w/bazel-out/k8-opt/bin/tensorflow/core/kernels/collective_nccl_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U_S_Stensorflow_Score_Skernels_Ccollective_Unccl_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.1(+0x10462bd)[0x7f0ca2e412bd]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f0ca1bef390]\r\n/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7f0ca0b9c428]\r\n/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7f0ca0b9e02a]\r\n/b/f/w/bazel-out/k8-opt/bin/tensorflow/core/kernels/collective_nccl_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U_S_Stensorflow_Score_Skernels_Ccollective_Unccl_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.1(+0x15df094)[0x7f0ca33da094]\r\n/b/f/w/bazel-out/k8-opt/bin/tensorflow/core/kernels/collective_nccl_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U_S_Stensorflow_Score_Skernels_Ccollective_Unccl_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.1(_ZN10tensorflow8EventMgr10PollEventsEbPN4absl13InlinedVectorINS0_5InUseELm4ESaIS3_EEE+0x207)[0x7f0ca2e1ef27]\r\n/b/f/w/bazel-out/k8-opt/bin/tensorflow/core/kernels/collective_nccl_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U_S_Stensorflow_Score_Skernels_Ccollective_Unccl_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.1(_ZN10tensorflow8EventMgr8PollLoopEv+0x9f)[0x7f0ca2e1f7bf]\r\n/b/f/w/bazel-out/k8-opt/bin/tensorflow/core/kernels/collective_nccl_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U_S_Stensorflow_Score_Skernels_Ccollective_Unccl_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.1(_ZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x281)[0x7f0ca2e26e81]\r\n/b/f/w/bazel-out/k8-opt/bin/tensorflow/core/kernels/collective_nccl_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U_S_Stensorflow_Score_Skernels_Ccollective_Unccl_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.1(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x48)[0x7f0ca2e24578]\r\n/b/f/w/bazel-out/k8-opt/bin/tensorflow/core/kernels/collective_nccl_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U_S_Stensorflow_Score_Skernels_Ccollective_Unccl_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.1(+0x1685a8f)[0x7f0ca3480a8f]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7f0ca1be56ba]\r\n/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f0ca0c6e41d]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace()\r\n\r\n\r\n\tgsignal\r\n\tabort\r\n\r\n\ttensorflow::EventMgr::PollEvents(bool, absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> >*)\r\n\ttensorflow::EventMgr::PollLoop()\r\n\tEigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n\tstd::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n\r\n\r\n\tclone\r\n*** End stack trace ***\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310:    16 Aborted                 (core dumped) \"${TEST_PATH}\" \"$@\" 2>&1\r\n```", "@mihaimaruseac thank you for bringing this test to my attention.  I am trying to reproduce now on our platform.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34532) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34532) for more info**.\n\n<!-- ok -->", "@mihaimaruseac I was able to reproduce the failing test more than once.  After this change, https://github.com/tensorflow/tensorflow/pull/34532/commits/c954406a1eaa6c9f1d28134e4229f8aaad636fac , I was no longer able to reproduce.  Let's watch CI now.  Thanks.", "Thank you"]}, {"number": 34531, "title": "R1.15 rccl upstream patch", "body": "This patch is a backport of current RCCL support in master for the r1.15 branch.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34531) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 34530, "title": "Prediction fails on variable size inputs due to np.concatenate", "body": "I've built an autoencoder that takes input of size `(None, 2)` (batch size must be 1). This works for training, because it deals with each batch individually. I've posted my implementation [here]( https://datascience.stackexchange.com/questions/63571/tensorflow-training-with-batch-size-of-1-none-features-but-model-expects-ex?noredirect=1#comment68767_63571)\r\n\r\nWhen it comes to prediction however, the model first (correctly) predicts on every input, but then fails because it tries to concatenate them, as you can see from the traceback.\r\n\r\n````\r\nself.results = np.concatenate(self.results, axis=0)\r\n  File \"<__array_function__ internals>\", line 6, in concatenate\r\nValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 12 and the array at index 1 has size 51\r\n2019-11-22 16:08:58.238687: W tensorflow/core/kernels/data/generator_dataset_op.cc:102] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]\r\n````\r\n\r\nIs there any way to disable this, in order to keep the performance optimization? Predicting on each sample in a loop instead gives a warning, and is roughly 10 times slower.\r\n````\r\nWARNING:tensorflow:5 out of the last 5 calls to <function _make_execution_function.<locals>.distributed_function at 0x7f63d8635268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n````", "comments": ["Okay, so I found that I need to do `predict_on_batch` on single samples in a loop. I assume the concatenation in `predict()` will not go away anytime soon, because it's what's sensible in 99.9% of cases."]}, {"number": 34529, "title": "Correctly access the vocab_size", "body": "Whenever I run:\r\n\r\n```\r\nimport tensorflow.keras as keras\r\nl = keras.layers.experimental.preprocessing.TextVectorization()\r\nl.set_vocabulary([\"hello\", \"world\"])\r\nl.get_vocabulary()\r\n```\r\n\r\nI get:\r\n\r\n> AttributeError: 'TextVectorization' object has no attribute 'vocab_size'\r\n\r\nThis PR fixes it.\r\n", "comments": ["Hi! Are there plans to merge this? This is currently breaking the `get_vocabulary` method of TextVectorization layers."]}, {"number": 34528, "title": "External loss function raises error on incompatible types", "body": "On TensorFlow 2.0 (OS Ubuntu 16.04), when I've an external loss function, the tf.keras.model fit() function call raises an error of incompatible types, whereas the same code on TensorFlow 1.15 does not raise any error and works fine.  The function fit() fails with:\r\n\r\n```\r\n(...)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in\r\n_apply_op_helper(self, op_type_name, name, **keywords)\r\n    561                   \"%s type %s of argument '%s'.\" %\r\n    562                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\r\n--> 563                    inferred_from[input_arg.type_attr]))\r\n    564 \r\n    565           types = [values.dtype]\r\n\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type uint8 of\r\nargument 'x'.\r\n```\r\n\r\nThe minimal code to reproduce is:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras import backend as K\r\n\r\ndef loss_func(y_true, y_pred):\r\n    y_true_f = K.batch_flatten(y_true)\r\n    y_pred_f = K.batch_flatten(y_pred)\r\n    intersection = K.sum(y_true_f * y_pred_f, axis=1, keepdims=True)\r\n    union = K.sum(y_true_f, axis=1, keepdims=True) \\\r\n            + K.sum(y_pred_f, axis=1, keepdims=True)\r\n    return -(2. * intersection + K.epsilon()) / (union + K.epsilon())\r\n\r\n(Xtr, Ytr), (Xva, Yva) = tf.keras.datasets.cifar10.load_data()\r\nXtr, Ytr, Xva, Yva, nc = Xtr[:1000], Ytr[:1000], Xva[:100], Yva[:100], 10\r\nXtr, Xva = Xtr.astype('float32') / 255, Xva.astype('float32') / 255\r\nYtr, Yva, ins = to_categorical(Ytr, nc), to_categorical(Yva, nc), Xtr.shape[1:]\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Input(ins))\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3)))\r\nmodel.add(tf.keras.layers.Activation('relu'))\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(32, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))\r\nmodel.add(tf.keras.layers.Dense(nc, activation='softmax'))\r\nopt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\r\nmodel.compile(loss=loss_func, optimizer=opt, metrics=[])\r\n\r\nYtr, Yva = Ytr.astype('uint8'), Yva.astype('uint8') # (1)\r\n\r\nmodel.fit(x=Xtr, y=Ytr, batch_size=32, epochs=1, validation_data=(Xva, Yva))\r\n```\r\n\r\nIf I remove line (1) it works on TensorFlow 2.0.", "comments": ["I have tried on colab with TF version 1.15, 2.0, 2.1.0-dev20191124 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/a3a85b9e573c95299b5d248d662f8db7/untitled402.ipynb). Thanks!", "@andmax \r\n\r\nThis was resolved in ``tf-nightly`(!pip install tf-nightly)`. Please check the gist [here](https://colab.sandbox.google.com/gist/ravikyram/cc4d485120973b17d5616b85b81a61f0/untitled715.ipynb).\r\n\r\nYou could use tf-nightly for now and in the next couple of months new stable version will be released.Can you please confirm whether we can close this issue as this was resolved. \r\nThanks!\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34528\">No</a>\n", "Looks like it has been resolved"]}, {"number": 34527, "title": "Xxx", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nhttps://pornhublive.com/landing/mg-mobile/mg-top-cats-v2/&AFNO=1-537210&UHNSMTY=437?stno=2-630-0-5767-0-0-3192-4807", "comments": ["Xxx\r\n", "@mouse36872 The issue tracker is for Tensorflow feature requests or bug reports.  Please refrain from posting inappropriate content to this forum. This is unacceptable to this community."]}, {"number": 34526, "title": "Xxx", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\nhttps://pornhublive.com/landing/mg-mobile/mg-top-cats-v2/&AFNO=1-537210&UHNSMTY=437?stno=2-630-0-5767-0-0-3192-4807\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Xxx", "@mouse36872 The issue tracker is for Tensorflow feature requests or bug reports.  Please refrain from posting inappropriate content to this forum. This is unacceptable to this community."]}, {"number": 34525, "title": "Cannot Use GpuDelegate on simple model with 3D input", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina and Android 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2019-11-22 17:05:40.034915: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-11-22 17:05:40.034972: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-22 17:05:40.046101: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-22 17:05:40.046128: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 39 nodes (34), 63 edges (58), time = 4.433ms.\r\n2019-11-22 17:05:40.046137: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.069ms.\r\n2019-11-22 17:05:40.075988: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-11-22 17:05:40.076066: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-22 17:05:40.089383: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-22 17:05:40.089403: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 34 nodes (-5), 54 edges (-9), time = 9.559ms.\r\n2019-11-22 17:05:40.089408: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 34 nodes (0), 54 edges (0), time = 0.949ms.\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n*The model*\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Input(shape=(40, 300), dtype=tf.dtypes.float32))\r\nmodel.add(tf.keras.layers.Dense(10))\r\n\r\nsaved_model_dir = './saved_model/'\r\nmodel.save(saved_model_dir, save_format='tf')\r\n```\r\n\r\n*TFLite conversion code*\r\n\r\n```\r\ntflite_model_path = './model.tflite'\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\ntflite_model = converter.convert()\r\nwith open(tflite_model_path, 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\n*Android code*\r\n\r\n```\r\nString dst = \"<Absolute path to the .tflite file>\"\r\nInterpreter.Options options = new Interpreter.Options();\r\nGpuDelegate gpuDelegate = new GpuDelegate();\r\noptions.addDelegate(gpuDelegate);\r\nInterpreter interpreter = new Interpreter(new File(dst), options);  // The exception is thrown here\r\n\r\nfloat[][][] input = new float[1][40][300];\r\nfor (int i = 0; i < input.length; i++) {\r\n    for (int j = 0; j < input[i].length; j++) {\r\n        for (int k = 0; k < input[i][j].length; k++) {\r\n            input[i][j][k] = (float) Math.random();\r\n        }\r\n    }\r\n}\r\nfloat[][][] output = new float[1][40][10];\r\ninterpreter.run(input, output);\r\n```\r\n\r\nThis works on Android when running on CPU (not using the GPU delegate), but when using the GPU delegate, the following exception is thrown:\r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.tflitegpu, PID: 24680\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.tflitegpu/com.example.tflitegpu.MainActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input data should match weights width\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 2 (TfLiteGpuDelegateV2) failed to prepare.\r\n```\r\n\r\nThe Full Traceback and the TFLite file are attached.\r\n\r\nThank you for your help!\r\n\r\n[traceback.txt](https://github.com/tensorflow/tensorflow/files/3880354/traceback.txt)\r\n[model.tflite.zip](https://github.com/tensorflow/tensorflow/files/3880355/model.tflite.zip)\r\n", "comments": ["@ghiles-bgp , was your intention making fully connected layer with 40 x 300 input with 1 batch?\r\n\r\nThe error happens at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/model_builder.cc#L1206 since input shape is 40 x 300 and batch size is 1 while width of weights is 300. @impjdi , could you confirm the current GPU delegate handles multi dimensional input? ", "Thanks for looking into this @terryheo \r\n\r\nIIRC I have received an issue related to\r\n\r\n> \"Amount of input data should match weights width\"\r\n\r\n@ekaterinaignasheva Was that assigned to you?", "I checked with @ekaterinaignasheva in person.\r\n\r\n`FULLY_CONNECTED` is essentially matrix matrix multiplication, and we unfortunately don't support it, only matrix vector multiplication.  We're currently short staffed and can't prioritize to implement that.  Would it be possible to rewrite your graph to use 1x1 convolution instead?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Thank you for the 1x1 convolution suggestion, we were not aware of the trick. Since the matrix by matrix multiplication is not supported I think the issue can be closed since this is more a feature request and you seem to be aware of it.\r\nThank's again for your help!"]}, {"number": 34524, "title": "`tfjs.converters.convert_tf_saved_model` fails for NN with embedding (in most cases)", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux x64\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.0.0\r\n- Python version:\r\n3.7.4\r\n- CUDA/cuDNN version:\r\nCUDA Version: 10.0\r\n- GPU model and memory:\r\nMultiple GeForce GTX 1080\r\n\r\n**Describe the current behavior**\r\n\r\n`tfjs.converters.convert_tf_saved_model` fails:\r\n1. When converting module with `tf.nn.embedding_lookup` and single GPU (of few in the system) is used (for conversion).\r\nor\r\n2. If module contains prebuilt `tf.keras.layers.Embedding` layer.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n`tfjs.converters.convert_tf_saved_model` is expected to work fine in both cases.\r\n\r\n**Code to reproduce the issue**\r\n\r\nCase 1:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflowjs as tfjs\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(gpus) > 1, 'Probably multiple GPUs required'\r\ntf.config.experimental.set_visible_devices([gpus[0]], 'GPU')\r\n\r\nclass EmbModule(tf.Module):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n    \r\n    @tf.function(input_signature = [tf.TensorSpec(shape = [1, 2], dtype = tf.int32)])\r\n    def apply(self, inp):\r\n        return tf.nn.embedding_lookup(tf.ones([3, 4]), inp)\r\n\r\nembModule = EmbModule()\r\n\r\ntf.saved_model.save(embModule, 'embModule/1/')\r\n# Next line fails as single GPU (of few in the system) is used:\r\ntfjs.converters.convert_tf_saved_model('embModule/1/', 'embModule/1-js')\r\n```\r\nError:\r\n```\r\nUnknownError: Failed to create session\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\nAssertionError: Identity is not in graph\r\n```\r\n\r\nCase 2:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflowjs as tfjs\r\n\r\nclass EmbModule(tf.Module):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        self.emb = tf.keras.layers.Embedding(3, 4)\r\n        self.emb.build((1, 2))\r\n    \r\n    @tf.function(input_signature = [tf.TensorSpec(shape = [1, 2], dtype = tf.int32)])\r\n    def apply(self, inp):\r\n        return self.emb(inp)\r\n\r\nembModule = EmbModule()\r\n\r\ntf.saved_model.save(embModule, 'embModule/1/')\r\n# Next line fails as 'Embedding' layer was built in constructor:\r\ntfjs.converters.convert_tf_saved_model('embModule/1/', 'embModule/1-js')\r\n```\r\nError:\r\n```\r\nInvalidArgumentError: Input 0 of node StatefulPartitionedCall/embedding/embedding_lookup was passed float from Func/StatefulPartitionedCall/input/_2:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\nValueError: Input 0 of node StatefulPartitionedCall/embedding/embedding_lookup was passed float from Func/StatefulPartitionedCall/input/_2:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\nAssertionError: Identity is not in graph\r\n```\r\n", "comments": ["@denis-sh ,\r\nCan you please raise TF.js related issue in this [repo](https://github.com/tensorflow/tfjs/issues/new)?Thanks! ", "OK, created tensorflow/tfjs#2452"]}, {"number": 34522, "title": "[TFLite] Support Metal GPU Delegate for macOS", "body": "This PR supports Metal GPU delegate on macOS. \r\n\r\n- Removed UIKit dependencies\r\n- Added dylib build for macOS\r\n- Added `extern` to  `TFLGpuDelegateCreate` and `TFLGpuDelegateDelete`\r\n  - This option is required to call API from TFLite UnityPlugin. ref #34450 \r\n\r\n\r\nIn this PR, I don't add `MTLFeatureSet_macOS_GPUFamily1_v4` or lower support since I don't have these macs. I'm not sure if we should support these. \r\nhttps://developer.apple.com/documentation/metal/mtlfeatureset?language=objc", "comments": ["Looks pretty reasonable to me, @impjdi any thoughts?", "@jdduke \r\n\r\nLooks reasonable to me too.  If this works and doesn't break existing stuff, I welcome this change :)", "@jdduke Thanks for the review, added the comment in the BUILD file"]}, {"number": 34521, "title": "keras.Model does not work with keras.Input that was created with `tensor=` kwarg.", "body": "### System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **1.15.0-rc2**\r\n- Python version: **3.7.4**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n### current behavior\r\n`keras.Input` and `keras.InputLayer` give the option to pass in a `tf.placeholder` through the argument `tensor=None`. When using the output of `keras.Input(tensor=some_placeholder)` as inputs to the functional keras.Model and subsequently calling `model.predict(batch)` on the newly created model an error occurs:\r\n\r\n```\r\nValueError: ('Error when checking model input: expected no data, but got:', array([[0., 1.],\r\n       [2., 3.],\r\n       [4., 5.],\r\n       [6., 7.],\r\n       [8., 9.]]))\r\n```\r\n\r\n\r\n### expected behavior\r\nNo error occurs and keras.Model correctly feeds the inputs through the graph.\r\n\r\n### Code to reproduce the issue\r\nThe following code breaks:\r\n**snippet 1**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\nprint(\"TF VERSION: \", tf.__version__)\r\n\r\nph = tf.placeholder(shape=[None,2], dtype=tf.float32)\r\n\r\n# create input layer from a predefined tensorflow placeholder\r\ninputs = keras.Input(tensor=ph)\r\n# perform some operation\r\nA = tf.constant([[0.5, 0],[0, 0.5]], dtype=tf.float32)\r\nx = tf.linalg.matmul(inputs, A)\r\noutputs = tf.reduce_sum(x, axis=1)\r\n\r\n# make a model\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n# try to feed a batch through the model\r\nbatch = np.linspace(0, 9, 10).reshape(5, 2)\r\nout = model.predict(batch)\r\n\r\nprint(out)\r\n```\r\n### Explanation\r\nI already debugged through the keras code and I think I spotted the issue:\r\n_tesorflow/python/keras/engine/network.py, lines 342-352_\r\n**snippet 2**\r\n```\r\nfor i, layer in enumerate(self._input_layers):\r\n      self.input_names.append(layer.name)\r\n      if layer.is_placeholder:\r\n        self._feed_input_names.append(layer.name)\r\n        # Use batch_input_shape here because non-eager composite tensors may not\r\n        # have a shape attribute that's meaningful (sparse, for instance, has\r\n        # a tensor that's non-constant and needs to be fed). This means that\r\n        # input layers that create placeholders will need to have the\r\n        # batch_input_shape attr to allow for input shape validation.\r\n        self._feed_input_shapes.append(layer._batch_input_shape)\r\n        self._feed_inputs.append(layer.input)\r\n```\r\nsnippet 2 is taken from the Network constructor. As can be seen, only tensors coming from layers with `is_placeholder` set to true are added to the `_feed_inputs` array.\r\nIn out case, this array is empty, because our `InputLayer` that was created from a `tf.placeholder` has `is_placeholder` set to false.\r\nLooking at the constructor code of `InputLayer` we can see why this is the case:\r\n_tensorflow/python/keras/engine/input_layer.py, lines 115 - 138_\r\n**snippet 3**\r\n```\r\n    if input_tensor is None:\r\n      if input_shape is not None:\r\n        batch_input_shape = (batch_size,) + tuple(input_shape)\r\n      else:\r\n        batch_input_shape = None\r\n      graph = backend.get_graph()\r\n      with graph.as_default():\r\n        input_tensor = backend.placeholder(\r\n            shape=batch_input_shape,\r\n            dtype=dtype,\r\n            name=self.name,\r\n            sparse=sparse,\r\n            ragged=ragged)\r\n\r\n      self.is_placeholder = True\r\n      self._batch_input_shape = batch_input_shape\r\n    else:\r\n      if not tf_utils.is_symbolic_tensor(input_tensor):\r\n        raise ValueError('You should not pass an EagerTensor to `Input`. '\r\n                         'For example, instead of creating an '\r\n                         'InputLayer, you should instantiate your model and '\r\n                         'directly call it on your input.')\r\n      self.is_placeholder = False\r\n      self._batch_input_shape = tuple(input_tensor.shape.as_list())\r\n```\r\ninput_tensor has the value from the `tensor=` argument. In our case, this holds the placeholder we passed to `Input(tensor=ph)`. Since it is not None `self.is_placeholder = False` is run in the else-case.\r\nThis subsequently results in the layer not being added to the _feed_inputs of the model which then does not expect any inputs to the predict method.\r\n\r\nThe issue can be worked around by manually setting the `is_placeholder` field of our input layer.\r\n**snippet 4**\r\n```\r\ninputs._keras_history[0].is_placeholder = True\r\n```\r\nAdding this line of code just below line 10 of **snippet 1** resolves the error.\r\nFixed code:\r\n**snippet 5**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\nprint(\"TF VERSION: \", tf.__version__)\r\n\r\nph = tf.placeholder(shape=[None,2], dtype=tf.float32)\r\n\r\n# create input layer from a predefined tensorflow placeholder\r\ninputs = keras.Input(tensor=ph)\r\ninputs._keras_history[0].is_placeholder=True\r\n# perform some operation\r\nA = tf.constant([[0.5, 0],[0, 0.5]], dtype=tf.float32)\r\nx = tf.linalg.matmul(inputs, A)\r\noutputs = tf.reduce_sum(x, axis=1)\r\n\r\n# make a model\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n# try to feed a batch through the model\r\nbatch = np.linspace(0, 9, 10).reshape(5, 2)\r\nout = model.predict(batch)\r\n\r\nprint(out)\r\n```\r\nOutput:\r\n```\r\nTF VERSION:  1.15.0-rc2\r\n[0.5 2.5 4.5 6.5 8.5]\r\n```\r\n\r\nSince I can not think of any reason, why an InputLayer created from a placeholder should have `is_placeholder=False` I think this issue can be resolved by removing the respective line of code.\r\n", "comments": ["I have tried on colab with TF version 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/bba32c27b2cba874adbb8d2addcb9285/untitled403.ipynb).Thanks!", "@7Z0nE Thanks for the issue!\r\n\r\nPlaceholders are removed in TF2, instead you can write the code like this:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\nprint(\"TF VERSION: \", tf.__version__)\r\n\r\n\r\n# create input layer from a predefined tensorflow placeholder\r\ninputs = keras.Input(shape=[2,], dtype=tf.float32)\r\n# perform some operation\r\nA = tf.constant([[0.5, 0],[0, 0.5]], dtype=tf.float32)\r\nx = tf.linalg.matmul(inputs, A)\r\noutputs = tf.reduce_sum(x, axis=1)\r\n\r\n# make a model\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n# try to feed a batch through the model\r\nbatch = np.linspace(0, 9, 10).reshape(5, 2)\r\nout = model.predict(batch)\r\n\r\nprint(out)\r\n```\r\n\r\nUnfortunately we are not able to backport fixes like this to earlier version.\r\n\r\nPlease try with the latest tf-nightly `pip install -U tf-nightly`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34521\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34521\">No</a>\n"]}, {"number": 34520, "title": "does not contain a toolchain for cpu 'arm64-v8a' when building tensorflow lite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:r1.15\r\n- Bazel version (if compiling from source):0.24.1\r\n\r\nHi,\r\nI'm trying to build the tensorflow lite for 'arm64-v8a' with linux on an amd64 with linux. the command i us`bazel build -c opt //tensorflow/lite:libtensorflowlite.so --cpu=arm64-v8a`, I got the following error:\r\n\r\n`/home/wang/.cache/bazel/_bazel_wang/c72f4772665ac4cb0690414b07635968/external/local_config_cc/BUILD:45:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'`\r\nI'm new to bazel and cannot find a detailed walkthrough.what should i do?\r\n", "comments": ["i changed the command `bazel build -c opt //tensorflow/lite:libtensorflowlite.so --fat_apk_cpu=arm64-v8a`,and it works well. i want know What is the difference between the --fat_apk_cpu=arm64-v8a and --cpu=arm64-v8a? and is the generated library suitable for arm64-v8a with linux?", "`fat_apk_cpu` is for Android builds, and is likely unsuited for building for Linux. Does `--cpu=aarch64` or `--cpu=arm64` work for you?", "hi, @jdduke\r\nthanks for your suggest ,I've tried the command you mentioned\uff0cbut i got the same error:\r\n\r\n`/home/wang/.cache/bazel/_bazel_wang/c72f4772665ac4cb0690414b07635968/external/local_config_cc/BUILD:45:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'aarch64'`\r\n\r\nDoes this mean I need to rewrite a `BUILD` for aarch64?", "What version of bazel are you using? [This thread](https://github.com/tensorflow/tensorflow/pull/16175) and [this thread](https://github.com/tensorflow/tensorflow/issues/22629) might be useful reference points.", "> What version of bazel are you using? [This thread](https://github.com/tensorflow/tensorflow/pull/16175) and [this thread](https://github.com/tensorflow/tensorflow/issues/22629) might be useful reference points.\r\n\r\nSorry, maybe I didn\u2019t express it clearly, I mean cross compiling the tflite for aarch64 on the x86_64.\r\nand my bazel version is 0.24.1", "Here are some [instructions that might help](https://github.com/xifengcun/tensorflow-aarch64-crossbuild). As noted in other threads, we don't (yet) officially support cross-compilation to arm64 builds on x86 linux, so efforts to do so will likely require some amount of hacking around on your part. This holds similarly for TensorFlow.", "All that said, if you can get this working without too much trouble, we can consider making upstream changes to make this easier. Pull requests welcome :).", "> \u7efc\u4e0a\u6240\u8ff0\uff0c\u5982\u679c\u60a8\u53ef\u4ee5\u8f7b\u677e\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u8003\u8651\u8fdb\u884c\u4e0a\u6e38\u66f4\u6539\u4ee5\u7b80\u5316\u6b64\u8fc7\u7a0b\u3002\u62c9\u8bf7\u6c42\u6b22\u8fce\uff1a\uff09\u3002\r\n\r\nthanks very, the instructuins is helpful!", "Just reconfigure Tensorflow because this step adds ANDROID NDK TOOLCHAIN in path which is used by bazel\r\n\r\non the top most tensorflow root just run\r\n\r\n`./configure` \r\n", "I could get around this issue by [configuring WORKSPACE and .bazelrc](https://www.tensorflow.org/lite/guide/build_android#configure_workspace_and_bazelrc), answering \"Yes\" when the script asks to interactively configure the `./WORKSPACE` for Android builds."]}, {"number": 34519, "title": "tf.range + for x,y in dataset issue", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: win10 1809, pycharm2019.2.3\r\n- TensorFlow installed from: pip install tensorflow-gpu\r\n- TensorFlow version: tensorflow-gpu 2.0.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0/7.6.4\r\n- GPU model and memory: rtx2080 8g\r\n\r\n**Describe the current behavior**\r\ntf.range + for x,y in dataset raise exception.\r\n\r\n**Describe the expected behavior**\r\nrun normally\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [4, 5, 6]))\r\n\r\n    @tf.function\r\n    def f():\r\n        for e in tf.range(3):\r\n            for x, y in dataset:\r\n                tf.print(x, y, e)\r\n    f()\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2019-11-22 18:56:57.100113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-11-22 18:56:58.357108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-22 18:56:58.432503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8\r\npciBusID: 0000:01:00.0\r\n2019-11-22 18:56:58.432679: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-22 18:56:58.433099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-22 18:56:58.433463: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-11-22 18:56:58.435936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8\r\npciBusID: 0000:01:00.0\r\n2019-11-22 18:56:58.436136: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-22 18:56:58.436559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-22 18:56:59.016772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-22 18:56:59.016930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-22 18:56:59.017022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-22 18:56:59.017714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6273 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-11-22 18:56:59.409303: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper\r\n\t [[{{node while_input_4/_12}}]]\r\n\t [[Func/while/body/_1/input/_47/_20]]\r\n2019-11-22 18:56:59.409640: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper\r\n\t [[{{node while_input_4/_12}}]]\r\nTraceback (most recent call last):\r\n  File \"D:/work/python/PycharmProjects/tftest/test.py\", line 83, in <module>\r\n    f()\r\n  File \"D:\\work\\python\\PycharmProjects\\tftest\\venv-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\work\\python\\PycharmProjects\\tftest\\venv-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 526, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"D:\\work\\python\\PycharmProjects\\tftest\\venv-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"D:\\work\\python\\PycharmProjects\\tftest\\venv-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"D:\\work\\python\\PycharmProjects\\tftest\\venv-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"D:\\work\\python\\PycharmProjects\\tftest\\venv-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper\r\n\t [[{{node while_input_4/_12}}]]\r\n\t [[Func/while/body/_1/input/_47/_20]]\r\n  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper\r\n\t [[{{node while_input_4/_12}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_f_61]\r\n\r\nFunction call stack:\r\nf -> f\r\n\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["@SSSxCCC cannot reproduce the issue\r\n![image](https://user-images.githubusercontent.com/24864163/69483691-6b625380-0e50-11ea-8eb7-04affe81ad53.png)\r\n", "@srihari-humbarwadi thank you for your reply! Are you sure your tf is tensorflow-gpu? When I pip install tensorflow, the code works fine, but when I pip install tensorflow-gpu, the code still raise the exception I mentioned.", "@SSSxCCC can reproduce with `tensorflow-gpu==2.0.0 `. Removing `tf.function` seems to fix the issue which is odd, here is the notebook\r\nhttps://colab.research.google.com/drive/1PfiaUAFGD2g-Pbi6Lrd9_hb9dkY0KJcQ", "Issue replicating for TF-gpu-2.0. works fine with CPU, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/982a0c8f3d937897842b63570c587120/copy-of-untitled.ipynb) of colab.Thanks!", "Hi @jsimsa could you help to take a look? Thanks", "As far as I can tell, this is a op placement issue -- the `tf.print` op will be placed on GPU because `tf.range` will be placed on GPU (which is the default policy for ops that have GPU kernel in the presence of GPU), but the tf.data input will reside on CPU. The right thing to do is for the `tf.print` op to be placed on CPU.\r\n\r\n@mhong could you please triage this to someone on the runtime team to investigate. Thank you.", "@jsimsa Thanks for the information. I will look into this issue.", "CC @cheshire ", "After investigating this bug, I found the following that may help explain the root cause.\r\n\r\nAfter the op placement, the graph contains the follow connected line of nodes, where each node is uni-directionally connected to the next node\r\n\r\nnode_1: {name=\"while_input_4\", op=\"_Arg\", device=\"CPU\"}\r\n\r\nnode_2: {name=\"while/enter/_6\", op=\"Enter\", device=\"GPU\"}\r\n\r\nnode_3: {name=\"while/merge/_12\", op=\"Merge\", device=\"GPU\"}\r\n\r\nnode_4: {name=\"while/while_input_4_switch/_19\", op=\"Switch\", device=\"GPU\"}\r\n\r\nnode_5: {name=\"Func/while/body/_1/input/_49\", op=\"Identity\", device=\"GPU\"}\r\n\r\nnode_6: {name=\"while/body/_1/ScanDataset\", op=\"Scandataset\", device=\"CPU\"}.\r\n\r\nnode_6 also contains a sub-graph which has the \"PrintV2Op\". Therefore the first \"_Arg\" op, the \"Scandataset\" op as well as the \"PrintV2Op\" op are all correctly placed on CPU.\r\n\r\nThe issue is that control flow ops such as node_2 are placed on GPU. The edge that connects node_1 and node_2 has dtype=DT_VARIANT and the underlying datatype = DatasetVariantWrapper. DatasetVariantWrapper does not have encoder/decoder necessary for host-to-device transfer. Therefore we get the above errors.\r\n\r\n**How to fix this issue**\r\n\r\nIt seems that in this case, the control flow ops (i.e. node_2, node_3, node_4, node_5) should all be placed on the CPU.\r\n\r\nOne idea is that, if an edge connects from a control flow op to a dataset op, then we should co-locate the control flow op with dataset op.\r\n\r\nIt will be great if people with more expertise in control flow ops and dataset ops can comment on this.\r\n\r\n", "I don't know how much work this will be to retrofit into the current system, but in theory control flow ops like merge and switch should be \"device polymorphic\" on their data input/outputs since they just forward  the input tensors to the output tensors.", "The problem here is in an assumption that the placer algorithm makes -- that any tensor can be copied between different devices -- which is generally not true (e.g. for dataset variant tensors). The proper way to fix this would be to make sure that placer only allows placement that collocate a node with its non-copyable inputs.", "In this specific case, the error happens when:\r\n\r\n- An edge connects two device\r\n- The edge has dtype = DT_VARIANT.\r\n- The underlying data type (e.g. tensor::flat<Variant>().data()[0].TypeName()) is not registered in UnaryVariantOpRegistry.\r\n\r\nThe issue here is that, the underlying data type is known only when we start to run the graph. This makes it hard for the placement algorithm to decide whether this edge is allowed. Is there is any way to get such information in the graph compile time?\r\n\r\nThere seems to be two high level solutions. One solution is to let the underlying nodes expose proper information (e.g. via colocation attributes) to the placer algorithm. Another solution is to let control flow ops be device polymorphic.  Either way, we will need expertise in dataset ops or the control flow ops.\r\n", "@sanjoy I agree; sadly TF doesn't let you specify a kernel which doesn't look at its tensor content (and I don't know how to fold that information in to the current placer algorithm).\r\n\r\n@lindong28 @jsimsa should we add a registry of variant-ops-whose-outputs-cannot-be-copied and seed that with the tf.data ops then?\r\n\r\n@saxenasaurabh maybe we can avoid this placing business altogether if we can delay inlining the control flow functions until after placement?", "@alextp I don't understand your suggestion. IIUC we must place the lowered control flow ops so we will still need update the placement logic?\r\n\r\n@ezhulenev probably understands the placement logic better. Looking at the example here it seems we need to recognize patterns like Input(CPU) -> Enter -> Merge -> Switch -> Identity -> Consumer (CPU) and if so place the entire chain(-s in case of nesting) on CPU?", "The lowered ops are currently placed on the wrong device. If we had device\nassignments for the ops before/after each lowered op when we added the\nlowered op we could add them on a device which would minimize communication.\n\nOn Tue, Feb 11, 2020 at 10:39 AM Saurabh Saxena <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> I don't understand your suggestion.\n> IIUC we must place the lowered control flow ops so we will still need\n> update the placement logic?\n>\n> @ezhulenev <https://github.com/ezhulenev> probably understands the\n> placement logic better. Looking at the example here it seems we need to\n> recognize patterns like Input(CPU) -> Enter -> Merge -> Switch -> Identity\n> -> Consumer (CPU) and if so place the entire chain(-s in case of nesting)\n> on CPU?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34519?email_source=notifications&email_token=AAABHROLCIIYZQXBITFQNRLRCLWF5A5CNFSM4JQPHXJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELNSMCI#issuecomment-584787465>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLRLIXZULXO45B4XMTRCLWF5ANCNFSM4JQPHXJQ>\n> .\n>\n\n\n-- \n - Alex\n", "I see. Yeah that would be simpler than trying to handle this in the placement code I agree.", "@SSSxCCC, Seems to be fixed in TF-gpu==2.2.rc1. Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/9827e15c992a573346829d512197d275/untitled480.ipynb). Thanks", "Hi,\r\nThe notebook linked above ([there](https://colab.research.google.com/gist/gadagashwini/9827e15c992a573346829d512197d275/untitled480.ipynb#scrollTo=tIcffx9fJXAv)) still outputs the same error for me, both with TF 2.2.0-rc1 and rc2 with GPU enabled. So I'm not sure that it has been fixed.", "I met the similar issue when using tf.range loop and tf.dataset on GPU. I have tried with 2.2.0rc1/2 but with no luck (as mentioned by @mgoutay ). ", "@houtoms, Can you share the colab gist. ", "I just changed the tf version to 2.2rc2 in the above colab link and it fails https://colab.research.google.com/drive/102xUthT3hSfyTuoX-WD3NsYASLXRboQc", "Just checked with 2.2rc3 but the issue is still on. The colab link is same with the above.", "Similar problem was fixed in https://github.com/tensorflow/tensorflow/commit/eaa1729a52952f2a541491a1ce2c34af7ab66fc8#diff-e0dcdefdcbc0c9884e4333c1723ef40d, if a placer can prove that DT_VARIANT has host only underlying type, it will place all dependent ops on CPU. However in this case DT_VARIANT tensor passed as an input to the function, and placer can't tell anything about the underlying host type.\r\n\r\n```\r\nGraph Before calling Placer #nodes 100 #edges 163\r\n\r\n(n2:variant@CPU:0) -> () {\r\n   .....\r\n   n22 = Enter[T=variant, frame_name=\"while\", is_constant=false, parallel_iterations=10](n2)\r\n   .....\r\n}\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34519\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34519\">No</a>\n", "Hi,\r\n\r\nThis is probably not monitored actively, but I am facing the same issue and I managed to hack my way through:\r\n\r\n```python\r\nimport tensorflow as tf\r\ndataset = tf.data.Dataset.from_tensor_slices(([1., 2, 3], [4., 5, 6]))\r\n\r\nwith tf.device('/GPU:0'):\r\n    var = tf.ones([500, 50])\r\n\r\n@tf.function\r\ndef f():\r\n  with tf.device('/CPU:0'):\r\n      iterator = iter(dataset)\r\n  for e in tf.range(3):\r\n      for x, y in iterator:\r\n          z = tf.multiply(x, y, name='mul')\r\n          tf.print(tf.reduce_sum(z * var))\r\nf()\r\n```\r\n\r\nEDIT: Not really sure where the reduce_sum op takes place though, it's not logged by log_device_placement.", "Thank you @AdrienCorenflos for adding your solution!", "This issue should be fixed as of TF 2.3. ", "Hey Rachel and everyone, I think the issue still exists in TF 2.3. But I validated that it has been fixed in tf-nightly==2.4.0.dev20200902 and thus it should be fixed in the next TF 2.4 release.\r\n\r\nIt can be validated using this colab example: https://colab.research.google.com/drive/1C4AXS8cyEsA_JmU7QVVk7UXM_uygVf05?authuser=1#scrollTo=jJBsJsRSR6AD\r\n\r\n", "This specific issue (dataset defined outside the @tf.function, and used inside a loop in the tf.function) has been fixed as of TF 2.3. There are still remaining issues with a subtly different setup (dataset defined _inside_ the @tf.function and used inside a loop in the tf.function), see #34112. ", "> This specific issue (dataset defined outside the @tf.function, and used inside a loop in the tf.function) has been fixed as of TF 2.3. There are still remaining issues with a subtly different setup (dataset defined _inside_ the @tf.function and used inside a loop in the tf.function), see #34112.\r\n\r\nIndeed calling an eval dataset loop inside a training loop is still a problem! However using MirroredStrategy for both training and eval makes the problem go away. Any workaround  for now before systematic fix?\r\n\r\n```\r\ndt1 = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\r\ndt2 = tf.data.Dataset.from_tensor_slices([0, 1, 1, 1])\r\n\r\n\r\n@tf.function\r\ndef test():\r\n    for i, x in enumerate(dt1):\r\n        if i == 3:\r\n            for j, y in enumerate(dt2):\r\n                tf.print(y)\r\n        tf.print(x)\r\n\r\ntest()\r\n```\r\n\r\n8\r\n3\r\n0\r\n> InternalError:  2 root error(s) found.\r\n>   (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper\r\n> \t [[{{node cond/cond_input_0/_5/_12}}]]\r\n>   (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper\r\n> \t [[{{node cond/cond_input_0/_5/_12}}]]\r\n> \t [[Identity_1/_30]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored.\r\n> \t [[ReduceDataset]] [Op:__inference_test_539]\r\n> \r\n> Function call stack:\r\n> test -> tf_data_experimental_scan_scan_body -> tf_data_experimental_scan_scan_body"]}, {"number": 34518, "title": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "I researched that issue, but unfortunnately all i was able to find was a compatibility problems. i made sure to install exactly what was specified in the tensorflow gpu installation procedure. The only thing that is different is that i'm running the 430 nvidia driver (which was installed by asking to install the 418)\r\n\r\n**System information**\r\n- https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n- Ubuntu 18.04\r\n- Tensorflow-gpu installed using pip \r\n- Tensorflow-gpu 2.0.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: cuda 10.1 /cuDNN 7.6.5\r\n- GPU model and memory: NVIDIA GeForce GTX 1050 Mobile  2Gb\r\n\r\n**Describe the current behavior**\r\n``` python\r\nfor image_path in TEST_IMAGE_PATHS:\r\n  show_inference(detection_model, image_path)\r\n```\r\n``` python\r\nUnknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n```\r\n**Describe the expected behavior**\r\nExpecting to show the image with rectangle for detected objects\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\nI used the provided code and i'm getting this error while computing the block\r\n ``` python\r\nfor image_path in TEST_IMAGE_PATHS:\r\n  show_inference(detection_model, image_path)\r\n```\r\n\r\n**Other info / logs**\r\n ``` python\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-18-c197149a2864> in <module>\r\n      1 for image_path in TEST_IMAGE_PATHS:\r\n----> 2   show_inference(detection_model, image_path)\r\n\r\n<ipython-input-17-e474e557b383> in show_inference(model, image_path)\r\n      4   image_np = np.array(Image.open(image_path))\r\n      5   # Actual detection.\r\n----> 6   output_dict = run_inference_for_single_image(model, image_np)\r\n      7   # Visualization of the results of a detection.\r\n      8   vis_util.visualize_boxes_and_labels_on_image_array(\r\n\r\n<ipython-input-16-4110867dcb70> in run_inference_for_single_image(model, image)\r\n      7 \r\n      8   # Run inference\r\n----> 9   output_dict = model(input_tensor)\r\n     10 \r\n     11   # All outputs are batches tensors.\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1079       TypeError: For invalid positional/keyword argument combinations.\r\n   1080     \"\"\"\r\n-> 1081     return self._call_impl(args, kwargs)\r\n   1082 \r\n   1083   def _call_impl(self, args, kwargs, cancellation_manager=None):\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n   1119       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n   1120           list(kwargs.keys()), list(self._arg_keywords)))\r\n-> 1121     return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n   1122 \r\n   1123   def _filtered_call(self, args, kwargs):\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~/.local/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1 (defined at /home/niccle27/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n\t [[Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Switch_5/_970]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1 (defined at /home/niccle27/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_16931]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n ``` \r\n", "comments": ["``` python\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\nAdding those lines just after importing the modules fixed the precedent issue. But i still cannot run , on jupyther, the kernel die: \r\n\r\n``` python\r\n2019-11-23 21:33:10.370989: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.40G (1507524608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n```\r\nDoes anybody have a workarround this ? \r\n", "The code is working as expected with CPU version of Tensorflow. Please find the colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/c7ce19638ffb638e5b62374c28faa0e6/untitled270.ipynb).  Thanks!", "I haven't tried yet on the CPU, as i would like to be able use my GPU for training faster. Would it be possible to have a fix for it ? \r\n\r\nThank you in advance", "i have the same problem, with the same error too", "Still having the issue, even so i upgraded to latest cudnn package. Does anybody have a fix ? ", "Hope this helps. I was facing the similar issue with:\r\ntensorlfow 2.0\r\nCUDA 10.0\r\ncuDNN 7.6.5\r\n\r\nI downgraded to cuDNN 7.6.2 and my model is training now. I tried cuDNN 7.5.1 before cuDNN 7.6.2, but that complained that source was built with 7.6 + \r\n\r\nAlso, you are on CUDA 10.1. I started with CUDA 10.2, but eventually decided to downgrade to CUDA 10.0 and it was working fine for smaller models until I hit the above error message with more robust CNNs. ", "well, i tried to downgrade to cudnn 7.6.2 and CUDA 10.0, but the issue persist ... What configuration are you using to get this to work ? ", "I've the same issue. \r\n\r\nLatest tensorflow built from source (HEAD 55119aadc69d394047e5f75d514fb6488cd4adb4), Cuda 10.0, cudnn 7.6.5", "@menatte Can you please downgrade to downgrade to cudnn 7.6.2 and let me know if it works. Thanks!", "I was able to get this to work by downgrading :\r\n\r\ncudnn: 7.6.2.24-1+cuda10.0   \r\nDriver Version: 430.50 \r\nTensorflow-gpu 1.14\r\n\r\nThat unfortunnately works for tensorflow only, the same error appear while using keras with the same backend\r\n", "@niccle27 \r\nThanks!\r\nIt works by using tensorflow-gpu 1.14", "@niccle27  I am closing this issue as it has been resolved. If you have any more concerns, please create an other issue and we can take a look at it. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34518\">No</a>\n", "i need to use two class id's for this is it possible"]}, {"number": 34517, "title": "Tensorflow deadlock when using multi-feed_dicts or multi-queues to different gpus", "body": "\r\n**System information**\r\n- pycharm\r\n- 1.10\r\n- Python version:3.6\r\n- CUDA/cuDNN version:9\r\n- GPU model and memory:GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nI used the data-parallel mode to train my network. For the require of research, I needed feed different category images to different GPU(all The images have same size, such as input dogs' images to GPU 0 and cats' images to GPU 1), so I initiated two tf.placeholder() and feed the different images batch to different GPU and started to train the network. It seems fine at the beginning. But after 4000 iterations or some uncertain steps, it hangs up!\r\n**Describe the expected behavior**\r\n I needed feed different category images to different GPU(all The images have same size, such as input dogs' images to GPU 0 and cats' images to GPU 1) and apply the averaged gradients.\r\n**Code to reproduce the issue**\r\n\r\nimages0, images1 = self.load_batches()\r\n\r\nself.images0 = tf.placeholder(tf.float32, shape=(self.batch_size, self.c_dim, self.output_size, self.output_size))\r\nself.images1 = tf.placeholder(tf.float32, shape=(self.batch_size, self.c_dim, self.output_size, self.output_size))\r\n\r\nfor  i  in range(self.config.num_gpus):\r\n    with tf.device(device_setter):\r\n            if i == 0:\r\n                    self.set_tower_loss('', self.images0, Generator, Discriminator)\r\n            else:\r\n                    self.set_tower_loss('', self.images1, Generator, Discriminator)\r\n   \r\n _, g_grads= self.sess.run([self.d_grads] , feed_dict={self.images0: images0, self.images1: images1} )\r\n\r\n**Other info / logs**\r\nGPU memory is employed but utility is 0 and system doe not updated.\r\n", "comments": ["@ZK-Zhou ,\r\nCan you share a complete standalone code to reproduce the issue? also mention the tensorflow version. Thanks!", "@ZK-Zhou ,\r\nAny update on the issue ?Thanks!", "sorry to reply late! I have arranged a little model to reproduce this issue, please click my github and run it:https://github.com/ZK-Zhou/multi_queue", "@oanush ", "my tensorflow version is 1.6.0", "Apologies for the delay in response. TF 1.6 is obsolete. Can you please test with latest version of tensorflow and check if the issue still persists? Thanks!"]}, {"number": 34516, "title": "Allow evaluator not in cluster_spec, to be consistent with legacy Estimator", "body": "PiperOrigin-RevId: 281833366\r\nChange-Id: Ic580172ba5ec038e246028031ec277b18f31ea56", "comments": []}, {"number": 34515, "title": "Avoid doing reset when position is still in buffer in BufferedInputStream.", "body": "See https://github.com/tensorflow/tensorflow/issues/34510", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34515) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34515) for more info**.\n\n<!-- ok -->", "@googlebot I signed it!", "@gharibian would you mind taking a look? Thanks", "Adding @rohan100jain as I haven't worked on TensorFlow in a few years.  My drive-by comment would be to add a test of the behavior of Tell() that would fail before this PR and would succeed with the changes.", "@burgerkingeater Could you please check reviewer comments and keep us posted. Thanks!", "Yes sorry for the delayed response, this is still being worked on. I will\naddress the comment ASAP.\n\nOn Fri, Feb 21, 2020 at 10:50 AM Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 37 days with no activity and the awaiting response label was\n> assigned. Is this PR still valid? Assigning the stalled label. Please\n> comment to reassure me that this is still being worked on.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34515?email_source=notifications&email_token=AAK4WZ2C6QNF57ECRY5ZFZTREAO7XA5CNFSM4JQM2ATKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMTWPAY#issuecomment-589784963>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAK4WZ2XS2XROZX2LEYPL7LREAO7XANCNFSM4JQM2ATA>\n> .\n>\n", "Yes it\u2019s still being worked on, thanks\n\nOn Mon, Mar 9, 2020 at 6:10 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> It has been 15 days with no activity and the awaiting response label was\n> assigned. Is this PR still valid? Assigning the stalled label. Please\n> comment to reassure me that this is still being worked on.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34515?email_source=notifications&email_token=AAK4WZ22PPVDPMJZXPX6ERLRGWOP3A5CNFSM4JQM2ATKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOJTWZI#issuecomment-596851557>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAK4WZZGVS47XZ7O5FVVEZLRGWOP3ANCNFSM4JQM2ATA>\n> .\n>\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@burgerkingeater Any update on this PR, please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "@rohan100jain  buffered_inputstream_test.cc is inside legacy_lib_io_all_tests filegroup: https://github.com/burgerkingeater/tensorflow/blob/0601b81c88ec390de0997448b4a739d723a86479/tensorflow/core/lib/io/BUILD#L344 and doesn't seem to be invokable by bazel test. do you know how I can manually run this test file? Thanks.", "@gbaned can you reopen this PR? thanks", "@burgerkingeater  looks like your branch is removed, so we cannot reopen the PR.Please create a new one.Thank you\r\n\r\n![image](https://user-images.githubusercontent.com/43972606/96284486-7b1be680-0f92-11eb-9cae-2e6aeb2a36d8.png)\r\n", "@rthadur created a new one: https://github.com/tensorflow/tensorflow/pull/44104"]}, {"number": 34514, "title": "module 'tensorflow' has no attribute 'enable_eager_execution'", "body": "My tensorflow 2.0 has encountered a strange problem, I showed it when creating a new regression model class.\r\nModule 'tensorflow' has no attribute 'enable_eager_execution'\r\nI am confused.I really hope to get your answer. Here is my code.\r\n\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nx=tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\r\ny=tf.constant([[10.0],[20.0]])\r\nclass Linear(tf.keras.Model):\r\n    def __init__self():\r\n        super().__init__()\r\n        self.dense=tf.keras.layers.Dense(units=1,kernel_initializer=tf.zeros_initializer(),\r\n                                        bias_initializer=tf.zeros_initializer())\r\n    def call(self,input):\r\n        output=self.dense(input)\r\n        return output\r\n\r\nmodel=Linear()\r\noptimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\nfor i in range(100):\r\n    with tf.GradientTape() as tape:\r\n        y_pred=model(x)\r\n        loss=tf.reduce_mean(tf.square(y_pred-y))\r\n    grads=tape.gradient(loss,model.variables)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-22-12c6c55a29ae> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.enable_eager_execution()\r\n      3 x=tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\r\n      4 y=tf.constant([[10.0],[20.0]])\r\n      5 class Linear(tf.keras.Model):\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\r\n", "comments": ["> My tensorflow 2.0 has encountered a strange problem, I showed it when creating a new regression model class.\r\n> Module 'tensorflow' has no attribute 'enable_eager_execution'\r\n> I am confused.I really hope to get your answer. Here is my code.\r\n> \r\n> import tensorflow as tf\r\n> tf.enable_eager_execution()\r\n> x=tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\r\n> y=tf.constant([[10.0],[20.0]])\r\n> class Linear(tf.keras.Model):\r\n> def __init__self():\r\n> super().**init**()\r\n> self.dense=tf.keras.layers.Dense(units=1,kernel_initializer=tf.zeros_initializer(),\r\n> bias_initializer=tf.zeros_initializer())\r\n> def call(self,input):\r\n> output=self.dense(input)\r\n> return output\r\n> \r\n> ## model=Linear()\r\n> optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n> for i in range(100):\r\n> with tf.GradientTape() as tape:\r\n> y_pred=model(x)\r\n> loss=tf.reduce_mean(tf.square(y_pred-y))\r\n> grads=tape.gradient(loss,model.variables)\r\n> AttributeError Traceback (most recent call last)\r\n> in\r\n> 1 import tensorflow as tf\r\n> ----> 2 tf.enable_eager_execution()\r\n> 3 x=tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\r\n> 4 y=tf.constant([[10.0],[20.0]])\r\n> 5 class Linear(tf.keras.Model):\r\n> \r\n> AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\r\n\r\n"]}]