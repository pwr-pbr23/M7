[{"number": 34636, "title": "Different behaviour in keras.layers.TimeDistributed() and tf.keras.layers.TimeDistributed()?", "body": "Environment:\r\nWindows 10\r\nGPU\r\nTensorFlow 1.13.1 (install from pip)\r\nAnaconda\r\nIDE: Spyder\r\npython 3.6\r\n\r\nI am going to convert matterport's MaskRCNN implementation from keras to tf.keras. It gives the different output, as for example, for the following code-segment:\r\n```\r\nx = PyramidROIAlign([pool_size, pool_size],name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)\r\nprint('x1:',x)\r\n# Two 1024 FC layers (implemented with Conv2D for consistency)\r\n x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\"valid\"),\r\n                           name=\"mrcnn_class_conv1\")(x)\r\nprint('x2:', x)\r\n```\r\nThe printed output for keras:\r\n```\r\nx1: Tensor(\"roi_align_classifier_10/Reshape:0\", shape=(1, ?, 7, 7, 256), dtype=float32, device=/device:GPU:0)\r\nx2: Tensor(\"mrcnn_class_conv1_10/Reshape_1:0\", shape=(?, 200, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\r\n```\r\n\r\nThe printed output for tf.keras:\r\n```\r\nx1: Tensor(\"roi_align_classifier_11/Reshape:0\", shape=(1, ?, 7, 7, 256), dtype=float32, device=/device:GPU:0)\r\nx2: Tensor(\"mrcnn_class_conv1_11/transpose_1:0\", shape=(1, ?, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\r\n\r\nthe TimeDistributed gives different results.\r\n\r\nIs there any suggestion and solution pleas?\r\n\r\nThanks\r\n```\r\n\r\n", "comments": ["@ibrahimLearning,\r\nCould you post your complete code snippet for us to analyze the issue better?", "@gadagashwini \r\nThanks for your reply. Actually, I used Matterport's implementation MaskRCNN. \r\nhttps://github.com/matterport/Mask_RCNN\r\nHowever, the model is past here. Please check the function, **def fpn_classifier_graph()**\r\n\r\n```\r\n\r\n\"\"\"\r\nMask R-CNN\r\nThe main Mask R-CNN model implementation.\r\n\r\nCopyright (c) 2017 Matterport, Inc.\r\nLicensed under the MIT License (see LICENSE for details)\r\nWritten by Waleed Abdulla\r\n\"\"\"\r\n\r\nimport os\r\nimport random\r\nimport datetime\r\nimport re\r\nimport math\r\nimport logging\r\nfrom collections import OrderedDict\r\nimport multiprocessing\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nuse_tf_keras=True\r\n\r\nversion=1       ####change utils\r\n\r\n\r\nif use_tf_keras:\r\n    from tensorflow import keras\r\n    import tensorflow.keras.backend as K    \r\n    import tensorflow.keras.layers as KL\r\n    ####from tensorflow.compat.v1.keras import layers as KL\r\n    import tensorflow.keras.layers as KE    \r\n    import tensorflow.keras.models as KM\r\n\r\nelse:\r\n            \r\n    import keras\r\n    import keras.backend as K\r\n    import keras.layers as KL\r\n    import keras.engine as KE\r\n    import keras.models as KM\r\n\r\n\r\nimport utils\r\n\r\n# Requires TensorFlow 1.3+ and Keras 2.0.8+.\r\nfrom distutils.version import LooseVersion\r\nassert LooseVersion(tf.__version__) >= LooseVersion(\"1.3\")\r\nassert LooseVersion(keras.__version__) >= LooseVersion('2.0.8')\r\n\r\n\r\n############################################################\r\n#  Utility Functions\r\n############################################################\r\n\r\ndef log(text, array=None):\r\n    \"\"\"Prints a text message. And, optionally, if a Numpy array is provided it\r\n    prints it's shape, min, and max values.\r\n    \"\"\"\r\n    if array is not None:\r\n        text = text.ljust(25)\r\n        text += (\"shape: {:20}  \".format(str(array.shape)))\r\n        if array.size:\r\n            text += (\"min: {:10.5f}  max: {:10.5f}\".format(array.min(),array.max()))\r\n        else:\r\n            text += (\"min: {:10}  max: {:10}\".format(\"\",\"\"))\r\n        text += \"  {}\".format(array.dtype)\r\n    print(text)\r\n\r\n\r\nclass BatchNorm(KL.BatchNormalization):\r\n    \"\"\"Extends the Keras BatchNormalization class to allow a central place\r\n    to make changes if needed.\r\n\r\n    Batch normalization has a negative effect on training if batches are small\r\n    so this layer is often frozen (via setting in Config class) and functions\r\n    as linear layer.\r\n    \"\"\"\r\n    def call(self, inputs, training=None):\r\n        \"\"\"\r\n        Note about training values:\r\n            None: Train BN layers. This is the normal mode\r\n            False: Freeze BN layers. Good when batch size is small\r\n            True: (don't use). Set layer in training mode even when making inferences\r\n        \"\"\"\r\n        return super(self.__class__, self).call(inputs, training=training)\r\n\r\n\r\ndef compute_backbone_shapes(config, image_shape):\r\n    \"\"\"Computes the width and height of each stage of the backbone network.\r\n\r\n    Returns:\r\n        [N, (height, width)]. Where N is the number of stages\r\n    \"\"\"\r\n    if callable(config.BACKBONE):\r\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\r\n\r\n    # Currently supports ResNet only\r\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\r\n    return np.array(\r\n        [[int(math.ceil(image_shape[0] / stride)),\r\n            int(math.ceil(image_shape[1] / stride))]\r\n            for stride in config.BACKBONE_STRIDES])\r\n\r\n\r\n############################################################\r\n#  Resnet Graph\r\n############################################################\r\n\r\n# Code adopted from:\r\n# https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\r\n\r\ndef identity_block(input_tensor, kernel_size, filters, stage, block,\r\n                   use_bias=True, train_bn=True):\r\n    \"\"\"The identity_block is the block that has no conv layer at shortcut\r\n    # Arguments\r\n        input_tensor: input tensor\r\n        kernel_size: default 3, the kernel size of middle conv layer at main path\r\n        filters: list of integers, the nb_filters of 3 conv layer at main path\r\n        stage: integer, current stage label, used for generating layer names\r\n        block: 'a','b'..., current block label, used for generating layer names\r\n        use_bias: Boolean. To use or not use a bias in conv layers.\r\n        train_bn: Boolean. Train or freeze Batch Norm layers\r\n    \"\"\"\r\n    nb_filter1, nb_filter2, nb_filter3 = filters\r\n    conv_name_base = 'res' + str(stage) + block + '_branch'\r\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n\r\n    x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a',\r\n                  use_bias=use_bias)(input_tensor)\r\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\r\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\r\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c',\r\n                  use_bias=use_bias)(x)\r\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\r\n\r\n    x = KL.Add()([x, input_tensor])\r\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\r\n    return x\r\n\r\n\r\ndef conv_block(input_tensor, kernel_size, filters, stage, block,\r\n               strides=(2, 2), use_bias=True, train_bn=True):\r\n    \"\"\"conv_block is the block that has a conv layer at shortcut\r\n    # Arguments\r\n        input_tensor: input tensor\r\n        kernel_size: default 3, the kernel size of middle conv layer at main path\r\n        filters: list of integers, the nb_filters of 3 conv layer at main path\r\n        stage: integer, current stage label, used for generating layer names\r\n        block: 'a','b'..., current block label, used for generating layer names\r\n        use_bias: Boolean. To use or not use a bias in conv layers.\r\n        train_bn: Boolean. Train or freeze Batch Norm layers\r\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\r\n    And the shortcut should have subsample=(2,2) as well\r\n    \"\"\"\r\n    nb_filter1, nb_filter2, nb_filter3 = filters\r\n    conv_name_base = 'res' + str(stage) + block + '_branch'\r\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n\r\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\r\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\r\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\r\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\r\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\r\n                  '2c', use_bias=use_bias)(x)\r\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\r\n\r\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\r\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\r\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\r\n\r\n    x = KL.Add()([x, shortcut])\r\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\r\n    return x\r\n\r\n\r\ndef resnet_graph(input_image, architecture, stage5=False, train_bn=True):\r\n    \"\"\"Build a ResNet graph.\r\n        architecture: Can be resnet50 or resnet101\r\n        stage5: Boolean. If False, stage5 of the network is not created\r\n        train_bn: Boolean. Train or freeze Batch Norm layers\r\n    \"\"\"\r\n    assert architecture in [\"resnet50\", \"resnet101\"]\r\n    # Stage 1\r\n    x = KL.ZeroPadding2D((3, 3))(input_image)\r\n    x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x)\r\n    x = BatchNorm(name='bn_conv1')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\r\n    # Stage 2\r\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)\r\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn)\r\n    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn)\r\n    # Stage 3\r\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn)\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn)\r\n    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn)\r\n    # Stage 4\r\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)\r\n    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]\r\n    for i in range(block_count):\r\n        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_bn=train_bn)\r\n    C4 = x\r\n    # Stage 5\r\n    if stage5:\r\n        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)\r\n        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn)\r\n        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn)\r\n    else:\r\n        C5 = None\r\n    return [C1, C2, C3, C4, C5]\r\n\r\n\r\n############################################################\r\n#  Proposal Layer\r\n############################################################\r\n\r\ndef apply_box_deltas_graph(boxes, deltas):\r\n    \"\"\"Applies the given deltas to the given boxes.\r\n    boxes: [N, (y1, x1, y2, x2)] boxes to update\r\n    deltas: [N, (dy, dx, log(dh), log(dw))] refinements to apply\r\n    \"\"\"\r\n    # Convert to y, x, h, w\r\n    height = boxes[:, 2] - boxes[:, 0]\r\n    width = boxes[:, 3] - boxes[:, 1]\r\n    center_y = boxes[:, 0] + 0.5 * height\r\n    center_x = boxes[:, 1] + 0.5 * width\r\n    # Apply deltas\r\n    center_y += deltas[:, 0] * height\r\n    center_x += deltas[:, 1] * width\r\n    height *= tf.exp(deltas[:, 2])\r\n    width *= tf.exp(deltas[:, 3])\r\n    # Convert back to y1, x1, y2, x2\r\n    y1 = center_y - 0.5 * height\r\n    x1 = center_x - 0.5 * width\r\n    y2 = y1 + height\r\n    x2 = x1 + width\r\n    result = tf.stack([y1, x1, y2, x2], axis=1, name=\"apply_box_deltas_out\")\r\n    return result\r\n\r\n\r\ndef clip_boxes_graph(boxes, window):\r\n    \"\"\"\r\n    boxes: [N, (y1, x1, y2, x2)]\r\n    window: [4] in the form y1, x1, y2, x2\r\n    \"\"\"\r\n    # Split\r\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\r\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\r\n    # Clip\r\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\r\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\r\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\r\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\r\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\r\n    clipped.set_shape((clipped.shape[0], 4))\r\n    return clipped\r\n\r\n\r\nclass ProposalLayer(KE.Layer):\r\n    \"\"\"Receives anchor scores and selects a subset to pass as proposals\r\n    to the second stage. Filtering is done based on anchor scores and\r\n    non-max suppression to remove overlaps. It also applies bounding\r\n    box refinement deltas to anchors.\r\n\r\n    Inputs:\r\n        rpn_probs: [batch, num_anchors, (bg prob, fg prob)]\r\n        rpn_bbox: [batch, num_anchors, (dy, dx, log(dh), log(dw))]\r\n        anchors: [batch, num_anchors, (y1, x1, y2, x2)] anchors in normalized coordinates\r\n\r\n    Returns:\r\n        Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]\r\n    \"\"\"\r\n\r\n    def __init__(self, proposal_count, nms_threshold, config=None, **kwargs):\r\n        super(ProposalLayer, self).__init__(**kwargs)\r\n        self.config = config\r\n        self.proposal_count = proposal_count\r\n        self.nms_threshold = nms_threshold\r\n\r\n    def call(self, inputs):\r\n        # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]\r\n        scores = inputs[0][:, :, 1]\r\n        # Box deltas [batch, num_rois, 4]\r\n        deltas = inputs[1]\r\n        deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, [1, 1, 4])\r\n        # Anchors\r\n        anchors = inputs[2]\r\n\r\n        # Improve performance by trimming to top anchors by score\r\n        # and doing the rest on the smaller subset.\r\n        pre_nms_limit = tf.minimum(self.config.PRE_NMS_LIMIT, tf.shape(anchors)[1])\r\n        ix = tf.nn.top_k(scores, pre_nms_limit, sorted=True,\r\n                         name=\"top_anchors\").indices\r\n        scores = utils.batch_slice([scores, ix], lambda x, y: tf.gather(x, y),\r\n                                   self.config.IMAGES_PER_GPU)\r\n        deltas = utils.batch_slice([deltas, ix], lambda x, y: tf.gather(x, y),\r\n                                   self.config.IMAGES_PER_GPU)\r\n        pre_nms_anchors = utils.batch_slice([anchors, ix], lambda a, x: tf.gather(a, x),\r\n                                    self.config.IMAGES_PER_GPU,\r\n                                    names=[\"pre_nms_anchors\"])\r\n\r\n        # Apply deltas to anchors to get refined anchors.\r\n        # [batch, N, (y1, x1, y2, x2)]\r\n        boxes = utils.batch_slice([pre_nms_anchors, deltas],\r\n                                  lambda x, y: apply_box_deltas_graph(x, y),\r\n                                  self.config.IMAGES_PER_GPU,\r\n                                  names=[\"refined_anchors\"])\r\n\r\n        # Clip to image boundaries. Since we're in normalized coordinates,\r\n        # clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]\r\n        window = np.array([0, 0, 1, 1], dtype=np.float32)\r\n        boxes = utils.batch_slice(boxes,\r\n                                  lambda x: clip_boxes_graph(x, window),\r\n                                  self.config.IMAGES_PER_GPU,\r\n                                  names=[\"refined_anchors_clipped\"])\r\n\r\n        # Filter out small boxes\r\n        # According to Xinlei Chen's paper, this reduces detection accuracy\r\n        # for small objects, so we're skipping it.\r\n\r\n        # Non-max suppression\r\n        def nms(boxes, scores):\r\n            indices = tf.image.non_max_suppression(\r\n                boxes, scores, self.proposal_count,\r\n                self.nms_threshold, name=\"rpn_non_max_suppression\")\r\n            proposals = tf.gather(boxes, indices)\r\n            # Pad if needed\r\n            padding = tf.maximum(self.proposal_count - tf.shape(proposals)[0], 0)\r\n            proposals = tf.pad(proposals, [(0, padding), (0, 0)])\r\n            return proposals\r\n        proposals = utils.batch_slice([boxes, scores], nms,\r\n                                      self.config.IMAGES_PER_GPU)\r\n        return proposals\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (None, self.proposal_count, 4)\r\n\r\n\r\n############################################################\r\n#  ROIAlign Layer\r\n############################################################\r\n\r\ndef log2_graph(x):\r\n    \"\"\"Implementation of Log2. TF doesn't have a native implementation.\"\"\"\r\n    if version==1:\r\n        return tf.log(x) / tf.log(2.0)\r\n    elif version==2:\r\n        return tf.math.log(x) / tf.math.log(2.0)\r\n\r\n\r\nclass PyramidROIAlign(KE.Layer):\r\n    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\r\n\r\n    Params:\r\n    - pool_shape: [pool_height, pool_width] of the output pooled regions. Usually [7, 7]\r\n\r\n    Inputs:\r\n    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\r\n             coordinates. Possibly padded with zeros if not enough\r\n             boxes to fill the array.\r\n    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()\r\n    - feature_maps: List of feature maps from different levels of the pyramid.\r\n                    Each is [batch, height, width, channels]\r\n\r\n    Output:\r\n    Pooled regions in the shape: [batch, num_boxes, pool_height, pool_width, channels].\r\n    The width and height are those specific in the pool_shape in the layer\r\n    constructor.\r\n    \"\"\"\r\n\r\n    def __init__(self, pool_shape, **kwargs):\r\n        super(PyramidROIAlign, self).__init__(**kwargs)\r\n        self.pool_shape = tuple(pool_shape)\r\n\r\n    def call(self, inputs):\r\n        # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\r\n        boxes = inputs[0]\r\n\r\n        # Image meta\r\n        # Holds details about the image. See compose_image_meta()\r\n        image_meta = inputs[1]\r\n\r\n        # Feature Maps. List of feature maps from different level of the\r\n        # feature pyramid. Each is [batch, height, width, channels]\r\n        feature_maps = inputs[2:]\r\n\r\n        # Assign each ROI to a level in the pyramid based on the ROI area.\r\n        y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\r\n        h = y2 - y1\r\n        w = x2 - x1\r\n        # Use shape of first image. Images in a batch must have the same size.\r\n        image_shape = parse_image_meta_graph(image_meta)['image_shape'][0]\r\n        # Equation 1 in the Feature Pyramid Networks paper. Account for\r\n        # the fact that our coordinates are normalized here.\r\n        # e.g. a 224x224 ROI (in pixels) maps to P4\r\n        image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)\r\n        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\r\n        roi_level = tf.minimum(5, tf.maximum(\r\n            2, 4 + tf.cast(tf.round(roi_level), tf.int32)))\r\n        roi_level = tf.squeeze(roi_level, 2)\r\n\r\n        # Loop through levels and apply ROI pooling to each. P2 to P5.\r\n        pooled = []\r\n        box_to_level = []\r\n        for i, level in enumerate(range(2, 6)):\r\n            ix = tf.where(tf.equal(roi_level, level))\r\n            level_boxes = tf.gather_nd(boxes, ix)\r\n\r\n            # Box indices for crop_and_resize.\r\n            box_indices = tf.cast(ix[:, 0], tf.int32)\r\n\r\n            # Keep track of which box is mapped to which level\r\n            box_to_level.append(ix)\r\n\r\n            # Stop gradient propogation to ROI proposals\r\n            level_boxes = tf.stop_gradient(level_boxes)\r\n            box_indices = tf.stop_gradient(box_indices)\r\n\r\n            # Crop and Resize\r\n            # From Mask R-CNN paper: \"We sample four regular locations, so\r\n            # that we can evaluate either max or average pooling. In fact,\r\n            # interpolating only a single value at each bin center (without\r\n            # pooling) is nearly as effective.\"\r\n            #\r\n            # Here we use the simplified approach of a single value per bin,\r\n            # which is how it's done in tf.crop_and_resize()\r\n            # Result: [batch * num_boxes, pool_height, pool_width, channels]\r\n            pooled.append(tf.image.crop_and_resize(\r\n                feature_maps[i], level_boxes, box_indices, self.pool_shape,\r\n                method=\"bilinear\"))\r\n\r\n        # Pack pooled features into one tensor\r\n        pooled = tf.concat(pooled, axis=0)\r\n\r\n        # Pack box_to_level mapping into one array and add another\r\n        # column representing the order of pooled boxes\r\n        box_to_level = tf.concat(box_to_level, axis=0)\r\n        box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1)\r\n        box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range],\r\n                                 axis=1)\r\n\r\n        # Rearrange pooled features to match the order of the original boxes\r\n        # Sort box_to_level by batch then box index\r\n        # TF doesn't have a way to sort by two columns, so merge them and sort.\r\n        sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]\r\n        ix = tf.nn.top_k(sorting_tensor, k=tf.shape(\r\n            box_to_level)[0]).indices[::-1]\r\n        ix = tf.gather(box_to_level[:, 2], ix)\r\n        pooled = tf.gather(pooled, ix)\r\n\r\n        # Re-add the batch dimension\r\n        shape = tf.concat([tf.shape(boxes)[:2], tf.shape(pooled)[1:]], axis=0)\r\n        pooled = tf.reshape(pooled, shape)\r\n        return pooled\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape[0][:2] + self.pool_shape + (input_shape[2][-1], )\r\n\r\n\r\n############################################################\r\n#  Detection Target Layer\r\n############################################################\r\n\r\ndef overlaps_graph(boxes1, boxes2):\r\n    \"\"\"Computes IoU overlaps between two sets of boxes.\r\n    boxes1, boxes2: [N, (y1, x1, y2, x2)].\r\n    \"\"\"\r\n    # 1. Tile boxes2 and repeat boxes1. This allows us to compare\r\n    # every boxes1 against every boxes2 without loops.\r\n    # TF doesn't have an equivalent to np.repeat() so simulate it\r\n    # using tf.tile() and tf.reshape.\r\n    b1 = tf.reshape(tf.tile(tf.expand_dims(boxes1, 1),\r\n                            [1, 1, tf.shape(boxes2)[0]]), [-1, 4])\r\n    b2 = tf.tile(boxes2, [tf.shape(boxes1)[0], 1])\r\n    # 2. Compute intersections\r\n    b1_y1, b1_x1, b1_y2, b1_x2 = tf.split(b1, 4, axis=1)\r\n    b2_y1, b2_x1, b2_y2, b2_x2 = tf.split(b2, 4, axis=1)\r\n    y1 = tf.maximum(b1_y1, b2_y1)\r\n    x1 = tf.maximum(b1_x1, b2_x1)\r\n    y2 = tf.minimum(b1_y2, b2_y2)\r\n    x2 = tf.minimum(b1_x2, b2_x2)\r\n    intersection = tf.maximum(x2 - x1, 0) * tf.maximum(y2 - y1, 0)\r\n    # 3. Compute unions\r\n    b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\r\n    b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\r\n    union = b1_area + b2_area - intersection\r\n    # 4. Compute IoU and reshape to [boxes1, boxes2]\r\n    iou = intersection / union\r\n    overlaps = tf.reshape(iou, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\r\n    return overlaps\r\n\r\n\r\ndef detection_targets_graph(proposals, gt_class_ids, gt_boxes, gt_masks, config):\r\n    \"\"\"Generates detection targets for one image. Subsamples proposals and\r\n    generates target class IDs, bounding box deltas, and masks for each.\r\n\r\n    Inputs:\r\n    proposals: [POST_NMS_ROIS_TRAINING, (y1, x1, y2, x2)] in normalized coordinates. Might\r\n               be zero padded if there are not enough proposals.\r\n    gt_class_ids: [MAX_GT_INSTANCES] int class IDs\r\n    gt_boxes: [MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized coordinates.\r\n    gt_masks: [height, width, MAX_GT_INSTANCES] of boolean type.\r\n\r\n    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\r\n    and masks.\r\n    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates\r\n    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs. Zero padded.\r\n    deltas: [TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw))]\r\n    masks: [TRAIN_ROIS_PER_IMAGE, height, width]. Masks cropped to bbox\r\n           boundaries and resized to neural network output size.\r\n\r\n    Note: Returned arrays might be zero padded if not enough target ROIs.\r\n    \"\"\"\r\n    # Assertions\r\n    asserts = [\r\n        tf.Assert(tf.greater(tf.shape(proposals)[0], 0), [proposals],\r\n                  name=\"roi_assertion\"),\r\n    ]\r\n    with tf.control_dependencies(asserts):\r\n        proposals = tf.identity(proposals)\r\n\r\n    # Remove zero padding\r\n    proposals, _ = trim_zeros_graph(proposals, name=\"trim_proposals\")\r\n    gt_boxes, non_zeros = trim_zeros_graph(gt_boxes, name=\"trim_gt_boxes\")\r\n    gt_class_ids = tf.boolean_mask(gt_class_ids, non_zeros,\r\n                                   name=\"trim_gt_class_ids\")\r\n    gt_masks = tf.gather(gt_masks, tf.where(non_zeros)[:, 0], axis=2,\r\n                         name=\"trim_gt_masks\")\r\n\r\n    # Handle COCO crowds\r\n    # A crowd box in COCO is a bounding box around several instances. Exclude\r\n    # them from training. A crowd box is given a negative class ID.\r\n    crowd_ix = tf.where(gt_class_ids < 0)[:, 0]\r\n    non_crowd_ix = tf.where(gt_class_ids > 0)[:, 0]\r\n    crowd_boxes = tf.gather(gt_boxes, crowd_ix)\r\n    gt_class_ids = tf.gather(gt_class_ids, non_crowd_ix)\r\n    gt_boxes = tf.gather(gt_boxes, non_crowd_ix)\r\n    gt_masks = tf.gather(gt_masks, non_crowd_ix, axis=2)\r\n\r\n    # Compute overlaps matrix [proposals, gt_boxes]\r\n    overlaps = overlaps_graph(proposals, gt_boxes)\r\n\r\n    # Compute overlaps with crowd boxes [proposals, crowd_boxes]\r\n    crowd_overlaps = overlaps_graph(proposals, crowd_boxes)\r\n    crowd_iou_max = tf.reduce_max(crowd_overlaps, axis=1)\r\n    no_crowd_bool = (crowd_iou_max < 0.001)\r\n\r\n    # Determine positive and negative ROIs\r\n    roi_iou_max = tf.reduce_max(overlaps, axis=1)\r\n    # 1. Positive ROIs are those with >= 0.5 IoU with a GT box\r\n    positive_roi_bool = (roi_iou_max >= 0.5)\r\n    positive_indices = tf.where(positive_roi_bool)[:, 0]\r\n    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.\r\n    negative_indices = tf.where(tf.logical_and(roi_iou_max < 0.5, no_crowd_bool))[:, 0]\r\n\r\n    # Subsample ROIs. Aim for 33% positive\r\n    # Positive ROIs\r\n    positive_count = int(config.TRAIN_ROIS_PER_IMAGE *\r\n                         config.ROI_POSITIVE_RATIO)\r\n    if version==1:\r\n        positive_indices = tf.random_shuffle(positive_indices)[:positive_count]\r\n    elif version==2:\r\n        positive_indices = tf.random.shuffle(positive_indices)[:positive_count]\r\n        \r\n    positive_count = tf.shape(positive_indices)[0]\r\n    # Negative ROIs. Add enough to maintain positive:negative ratio.\r\n    r = 1.0 / config.ROI_POSITIVE_RATIO\r\n    negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count\r\n    if version==1:\r\n        negative_indices = tf.random_shuffle(negative_indices)[:negative_count]\r\n    elif version==2:\r\n        negative_indices = tf.random.shuffle(negative_indices)[:negative_count]\r\n            \r\n    # Gather selected ROIs\r\n    positive_rois = tf.gather(proposals, positive_indices)\r\n    negative_rois = tf.gather(proposals, negative_indices)\r\n\r\n    # Assign positive ROIs to GT boxes.\r\n    positive_overlaps = tf.gather(overlaps, positive_indices)\r\n    roi_gt_box_assignment = tf.cond(\r\n        tf.greater(tf.shape(positive_overlaps)[1], 0),\r\n        true_fn = lambda: tf.argmax(positive_overlaps, axis=1),\r\n        false_fn = lambda: tf.cast(tf.constant([]),tf.int64)\r\n    )\r\n    roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)\r\n    roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)\r\n\r\n    # Compute bbox refinement for positive ROIs\r\n    deltas = utils.box_refinement_graph(positive_rois, roi_gt_boxes)\r\n    deltas /= config.BBOX_STD_DEV\r\n\r\n    # Assign positive ROIs to GT masks\r\n    # Permute masks to [N, height, width, 1]\r\n    transposed_masks = tf.expand_dims(tf.transpose(gt_masks, [2, 0, 1]), -1)\r\n    # Pick the right mask for each ROI\r\n    roi_masks = tf.gather(transposed_masks, roi_gt_box_assignment)\r\n\r\n    # Compute mask targets\r\n    boxes = positive_rois\r\n    if config.USE_MINI_MASK:\r\n        # Transform ROI coordinates from normalized image space\r\n        # to normalized mini-mask space.\r\n        y1, x1, y2, x2 = tf.split(positive_rois, 4, axis=1)\r\n        gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(roi_gt_boxes, 4, axis=1)\r\n        gt_h = gt_y2 - gt_y1\r\n        gt_w = gt_x2 - gt_x1\r\n        y1 = (y1 - gt_y1) / gt_h\r\n        x1 = (x1 - gt_x1) / gt_w\r\n        y2 = (y2 - gt_y1) / gt_h\r\n        x2 = (x2 - gt_x1) / gt_w\r\n        boxes = tf.concat([y1, x1, y2, x2], 1)\r\n    box_ids = tf.range(0, tf.shape(roi_masks)[0])\r\n    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes,\r\n                                     box_ids,\r\n                                     config.MASK_SHAPE)\r\n    # Remove the extra dimension from masks.\r\n    masks = tf.squeeze(masks, axis=3)\r\n\r\n    # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with\r\n    # binary cross entropy loss.\r\n    masks = tf.round(masks)\r\n\r\n    # Append negative ROIs and pad bbox deltas and masks that\r\n    # are not used for negative ROIs with zeros.\r\n    rois = tf.concat([positive_rois, negative_rois], axis=0)\r\n    N = tf.shape(negative_rois)[0]\r\n    P = tf.maximum(config.TRAIN_ROIS_PER_IMAGE - tf.shape(rois)[0], 0)\r\n    rois = tf.pad(rois, [(0, P), (0, 0)])\r\n    roi_gt_boxes = tf.pad(roi_gt_boxes, [(0, N + P), (0, 0)])\r\n    roi_gt_class_ids = tf.pad(roi_gt_class_ids, [(0, N + P)])\r\n    deltas = tf.pad(deltas, [(0, N + P), (0, 0)])\r\n    masks = tf.pad(masks, [[0, N + P], (0, 0), (0, 0)])\r\n\r\n    return rois, roi_gt_class_ids, deltas, masks\r\n\r\n\r\nclass DetectionTargetLayer(KE.Layer):\r\n    \"\"\"Subsamples proposals and generates target box refinement, class_ids,\r\n    and masks for each.\r\n\r\n    Inputs:\r\n    proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might\r\n               be zero padded if there are not enough proposals.\r\n    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.\r\n    gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized\r\n              coordinates.\r\n    gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type\r\n\r\n    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\r\n    and masks.\r\n    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized\r\n          coordinates\r\n    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.\r\n    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw)]\r\n    target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width]\r\n                 Masks cropped to bbox boundaries and resized to neural\r\n                 network output size.\r\n\r\n    Note: Returned arrays might be zero padded if not enough target ROIs.\r\n    \"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        super(DetectionTargetLayer, self).__init__(**kwargs)\r\n        self.config = config\r\n\r\n    def call(self, inputs):\r\n        proposals = inputs[0]\r\n        gt_class_ids = inputs[1]\r\n        gt_boxes = inputs[2]\r\n        gt_masks = inputs[3]\r\n\r\n        # Slice the batch and run a graph for each slice\r\n        # TODO: Rename target_bbox to target_deltas for clarity\r\n        names = [\"rois\", \"target_class_ids\", \"target_bbox\", \"target_mask\"]\r\n        outputs = utils.batch_slice(\r\n            [proposals, gt_class_ids, gt_boxes, gt_masks],\r\n            lambda w, x, y, z: detection_targets_graph(\r\n                w, x, y, z, self.config),\r\n            self.config.IMAGES_PER_GPU, names=names)\r\n        return outputs\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return [\r\n            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # rois\r\n            (None, self.config.TRAIN_ROIS_PER_IMAGE),  # class_ids\r\n            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # deltas\r\n            (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.MASK_SHAPE[0],\r\n             self.config.MASK_SHAPE[1])  # masks\r\n        ]\r\n\r\n    def compute_mask(self, inputs, mask=None):\r\n        return [None, None, None, None]\r\n\r\n\r\n############################################################\r\n#  Detection Layer\r\n############################################################\r\n\r\ndef refine_detections_graph(rois, probs, deltas, window, config):\r\n    \"\"\"Refine classified proposals and filter overlaps and return final\r\n    detections.\r\n\r\n    Inputs:\r\n        rois: [N, (y1, x1, y2, x2)] in normalized coordinates\r\n        probs: [N, num_classes]. Class probabilities.\r\n        deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Class-specific\r\n                bounding box deltas.\r\n        window: (y1, x1, y2, x2) in normalized coordinates. The part of the image\r\n            that contains the image excluding the padding.\r\n\r\n    Returns detections shaped: [num_detections, (y1, x1, y2, x2, class_id, score)] where\r\n        coordinates are normalized.\r\n    \"\"\"\r\n    # Class IDs per ROI\r\n    class_ids = tf.argmax(probs, axis=1, output_type=tf.int32)\r\n    # Class probability of the top class of each ROI\r\n    indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)\r\n    class_scores = tf.gather_nd(probs, indices)\r\n    # Class-specific bounding box deltas\r\n    deltas_specific = tf.gather_nd(deltas, indices)\r\n    # Apply bounding box deltas\r\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\r\n    refined_rois = apply_box_deltas_graph(\r\n        rois, deltas_specific * config.BBOX_STD_DEV)\r\n    # Clip boxes to image window\r\n    refined_rois = clip_boxes_graph(refined_rois, window)\r\n\r\n    # TODO: Filter out boxes with zero area\r\n\r\n    # Filter out background boxes\r\n    keep = tf.where(class_ids > 0)[:, 0]\r\n    # Filter out low confidence boxes\r\n    if config.DETECTION_MIN_CONFIDENCE:\r\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\r\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\r\n                                        tf.expand_dims(conf_keep, 0))\r\n        keep = tf.sparse_tensor_to_dense(keep)[0]\r\n\r\n    # Apply per-class NMS\r\n    # 1. Prepare variables\r\n    pre_nms_class_ids = tf.gather(class_ids, keep)\r\n    pre_nms_scores = tf.gather(class_scores, keep)\r\n    pre_nms_rois = tf.gather(refined_rois,   keep)\r\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\r\n\r\n    def nms_keep_map(class_id):\r\n        \"\"\"Apply Non-Maximum Suppression on ROIs of the given class.\"\"\"\r\n        # Indices of ROIs of the given class\r\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\r\n        # Apply NMS\r\n        class_keep = tf.image.non_max_suppression(\r\n                tf.gather(pre_nms_rois, ixs),\r\n                tf.gather(pre_nms_scores, ixs),\r\n                max_output_size=config.DETECTION_MAX_INSTANCES,\r\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\r\n        # Map indices\r\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\r\n        # Pad with -1 so returned tensors have the same shape\r\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\r\n        class_keep = tf.pad(class_keep, [(0, gap)],\r\n                            mode='CONSTANT', constant_values=-1)\r\n        # Set shape so map_fn() can infer result shape\r\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\r\n        return class_keep\r\n\r\n    # 2. Map over class IDs\r\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\r\n                         dtype=tf.int64)\r\n    # 3. Merge results into one list, and remove -1 padding\r\n    nms_keep = tf.reshape(nms_keep, [-1])\r\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\r\n    # 4. Compute intersection between keep and nms_keep\r\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\r\n                                    tf.expand_dims(nms_keep, 0))\r\n    keep = tf.sparse_tensor_to_dense(keep)[0]\r\n    # Keep top detections\r\n    roi_count = config.DETECTION_MAX_INSTANCES\r\n    class_scores_keep = tf.gather(class_scores, keep)\r\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\r\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\r\n    keep = tf.gather(keep, top_ids)\r\n\r\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\r\n    # Coordinates are normalized.\r\n    detections = tf.concat([\r\n        tf.gather(refined_rois, keep),\r\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\r\n        tf.gather(class_scores, keep)[..., tf.newaxis]\r\n        ], axis=1)\r\n\r\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\r\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\r\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \"CONSTANT\")\r\n    return detections\r\n\r\n\r\nclass DetectionLayer(KE.Layer):\r\n    \"\"\"Takes classified proposal boxes and their bounding box deltas and\r\n    returns the final detection boxes.\r\n\r\n    Returns:\r\n    [batch, num_detections, (y1, x1, y2, x2, class_id, class_score)] where\r\n    coordinates are normalized.\r\n    \"\"\"\r\n\r\n    def __init__(self, config=None, **kwargs):\r\n        super(DetectionLayer, self).__init__(**kwargs)\r\n        self.config = config\r\n\r\n    def call(self, inputs):\r\n        rois = inputs[0]\r\n        mrcnn_class = inputs[1]\r\n        mrcnn_bbox = inputs[2]\r\n        image_meta = inputs[3]\r\n\r\n        # Get windows of images in normalized coordinates. Windows are the area\r\n        # in the image that excludes the padding.\r\n        # Use the shape of the first image in the batch to normalize the window\r\n        # because we know that all images get resized to the same size.\r\n        m = parse_image_meta_graph(image_meta)\r\n        image_shape = m['image_shape'][0]\r\n        window = norm_boxes_graph(m['window'], image_shape[:2])\r\n\r\n        # Run detection refinement graph on each item in the batch\r\n        detections_batch = utils.batch_slice(\r\n            [rois, mrcnn_class, mrcnn_bbox, window],\r\n            lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config),\r\n            self.config.IMAGES_PER_GPU)\r\n\r\n        # Reshape output\r\n        # [batch, num_detections, (y1, x1, y2, x2, class_id, class_score)] in\r\n        # normalized coordinates\r\n        return tf.reshape(\r\n            detections_batch,\r\n            [self.config.BATCH_SIZE, self.config.DETECTION_MAX_INSTANCES, 6])\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (None, self.config.DETECTION_MAX_INSTANCES, 6)\r\n\r\n\r\n############################################################\r\n#  Region Proposal Network (RPN)\r\n############################################################\r\n\r\ndef rpn_graph(feature_map, anchors_per_location, anchor_stride):\r\n    \"\"\"Builds the computation graph of Region Proposal Network.\r\n\r\n    feature_map: backbone features [batch, height, width, depth]\r\n    anchors_per_location: number of anchors per pixel in the feature map\r\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\r\n                   every pixel in the feature map), or 2 (every other pixel).\r\n\r\n    Returns:\r\n        rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits (before softmax)\r\n        rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities.\r\n        rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be\r\n                  applied to anchors.\r\n    \"\"\"\r\n    # TODO: check if stride of 2 causes alignment issues if the feature map\r\n    # is not even.\r\n    # Shared convolutional base of the RPN\r\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\r\n                       strides=anchor_stride,\r\n                       name='rpn_conv_shared')(feature_map)\r\n\r\n    # Anchor Score. [batch, height, width, anchors per location * 2].\r\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\r\n                  activation='linear', name='rpn_class_raw')(shared)\r\n\r\n    # Reshape to [batch, anchors, 2]\r\n    rpn_class_logits = KL.Lambda(\r\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\r\n\r\n    # Softmax on last dimension of BG/FG.\r\n    rpn_probs = KL.Activation(\r\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\r\n\r\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\r\n    # where depth is [x, y, log(w), log(h)]\r\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\r\n                  activation='linear', name='rpn_bbox_pred')(shared)\r\n\r\n    # Reshape to [batch, anchors, 4]\r\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\r\n\r\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\r\n\r\n\r\ndef build_rpn_model(anchor_stride, anchors_per_location, depth):\r\n    \"\"\"Builds a Keras model of the Region Proposal Network.\r\n    It wraps the RPN graph so it can be used multiple times with shared\r\n    weights.\r\n\r\n    anchors_per_location: number of anchors per pixel in the feature map\r\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\r\n                   every pixel in the feature map), or 2 (every other pixel).\r\n    depth: Depth of the backbone feature map.\r\n\r\n    Returns a Keras Model object. The model outputs, when called, are:\r\n    rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits (before softmax)\r\n    rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities.\r\n    rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be\r\n                applied to anchors.\r\n    \"\"\"\r\n    input_feature_map = KL.Input(shape=[None, None, depth],\r\n                                 name=\"input_rpn_feature_map\")\r\n    outputs = rpn_graph(input_feature_map, anchors_per_location, anchor_stride)\r\n    return KM.Model([input_feature_map], outputs, name=\"rpn_model\")\r\n\r\n\r\n############################################################\r\n#  Feature Pyramid Network Heads\r\n############################################################\r\n\r\ndef fpn_classifier_graph(rois, feature_maps, image_meta,\r\n                         pool_size, num_classes, train_bn=True,\r\n                         fc_layers_size=1024):\r\n    \"\"\"Builds the computation graph of the feature pyramid network classifier\r\n    and regressor heads.\r\n\r\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\r\n          coordinates.\r\n    feature_maps: List of feature maps from different layers of the pyramid,\r\n                  [P2, P3, P4, P5]. Each has a different resolution.\r\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\r\n    pool_size: The width of the square feature map generated from ROI Pooling.\r\n    num_classes: number of classes, which determines the depth of the results\r\n    train_bn: Boolean. Train or freeze Batch Norm layers\r\n    fc_layers_size: Size of the 2 FC layers\r\n\r\n    Returns:\r\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\r\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\r\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\r\n                     proposal boxes\r\n    \"\"\"\r\n    # ROI Pooling\r\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\r\n    x = PyramidROIAlign([pool_size, pool_size],\r\n                        name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)\r\n    print('x1:',x)\r\n    \r\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\r\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\"valid\"),\r\n                           name=\"mrcnn_class_conv1\")(x)\r\n    print('x2:',x)\r\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\r\n    print('x3:',x)\r\n    x = KL.Activation('relu')(x)\r\n    print('x4:',x)\r\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\r\n                           name=\"mrcnn_class_conv2\")(x)\r\n    print('x5:',x)\r\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\r\n    print('x6:',x)\r\n    x = KL.Activation('relu')(x)\r\n    \r\n    print('x7: ', x)\r\n\r\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\r\n                       name=\"pool_squeeze\")(x)\r\n    \r\n    print('shared: ', shared)\r\n\r\n    # Classifier head\r\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\r\n                                            name='mrcnn_class_logits')(shared)\r\n    print(mrcnn_class_logits)\r\n    \r\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\"softmax\"),\r\n                                     name=\"mrcnn_class\")(mrcnn_class_logits)\r\n\r\n    print('mrcnn_probs: ', mrcnn_probs)\r\n    # BBox head\r\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\r\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\r\n                           name='mrcnn_bbox_fc')(shared)\r\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\r\n    \r\n    print('x: ', x)\r\n    s = K.int_shape(x)\r\n    print('s[1]: ', s[1])\r\n    print('s: ', s)\r\n    \r\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\"mrcnn_bbox\")(x)\r\n    print('mrcnn_bbox: ', mrcnn_bbox)\r\n\r\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\r\n\r\n\r\ndef build_fpn_mask_graph(rois, feature_maps, image_meta,\r\n                         pool_size, num_classes, train_bn=True):\r\n    \"\"\"Builds the computation graph of the mask head of Feature Pyramid Network.\r\n\r\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\r\n          coordinates.\r\n    feature_maps: List of feature maps from different layers of the pyramid,\r\n                  [P2, P3, P4, P5]. Each has a different resolution.\r\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\r\n    pool_size: The width of the square feature map generated from ROI Pooling.\r\n    num_classes: number of classes, which determines the depth of the results\r\n    train_bn: Boolean. Train or freeze Batch Norm layers\r\n\r\n    Returns: Masks [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, NUM_CLASSES]\r\n    \"\"\"\r\n    # ROI Pooling\r\n    # Shape: [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, channels]\r\n    x = PyramidROIAlign([pool_size, pool_size],\r\n                        name=\"roi_align_mask\")([rois, image_meta] + feature_maps)\r\n\r\n    # Conv layers\r\n    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\r\n                           name=\"mrcnn_mask_conv1\")(x)\r\n    x = KL.TimeDistributed(BatchNorm(),\r\n                           name='mrcnn_mask_bn1')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\r\n                           name=\"mrcnn_mask_conv2\")(x)\r\n    x = KL.TimeDistributed(BatchNorm(),\r\n                           name='mrcnn_mask_bn2')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\r\n                           name=\"mrcnn_mask_conv3\")(x)\r\n    x = KL.TimeDistributed(BatchNorm(),\r\n                           name='mrcnn_mask_bn3')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\r\n                           name=\"mrcnn_mask_conv4\")(x)\r\n    x = KL.TimeDistributed(BatchNorm(),\r\n                           name='mrcnn_mask_bn4')(x, training=train_bn)\r\n    x = KL.Activation('relu')(x)\r\n\r\n    x = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation=\"relu\"),\r\n                           name=\"mrcnn_mask_deconv\")(x)\r\n    x = KL.TimeDistributed(KL.Conv2D(num_classes, (1, 1), strides=1, activation=\"sigmoid\"),\r\n                           name=\"mrcnn_mask\")(x)\r\n    return x\r\n\r\n\r\n############################################################\r\n#  Loss Functions\r\n############################################################\r\n\r\ndef smooth_l1_loss(y_true, y_pred):\r\n    \"\"\"Implements Smooth-L1 loss.\r\n    y_true and y_pred are typically: [N, 4], but could be any shape.\r\n    \"\"\"\r\n    diff = K.abs(y_true - y_pred)\r\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\r\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\r\n    return loss\r\n\r\n\r\ndef rpn_class_loss_graph(rpn_match, rpn_class_logits):\r\n    \"\"\"RPN anchor classifier loss.\r\n\r\n    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\r\n               -1=negative, 0=neutral anchor.\r\n    rpn_class_logits: [batch, anchors, 2]. RPN classifier logits for BG/FG.\r\n    \"\"\"\r\n    # Squeeze last dim to simplify\r\n    rpn_match = tf.squeeze(rpn_match, -1)\r\n    # Get anchor classes. Convert the -1/+1 match to 0/1 values.\r\n    anchor_class = K.cast(K.equal(rpn_match, 1), tf.int32)\r\n    # Positive and Negative anchors contribute to the loss,\r\n    # but neutral anchors (match value = 0) don't.\r\n    indices = tf.where(K.not_equal(rpn_match, 0))\r\n    # Pick rows that contribute to the loss and filter out the rest.\r\n    rpn_class_logits = tf.gather_nd(rpn_class_logits, indices)\r\n    anchor_class = tf.gather_nd(anchor_class, indices)\r\n    # Cross entropy loss\r\n    loss = K.sparse_categorical_crossentropy(target=anchor_class,\r\n                                             output=rpn_class_logits,\r\n                                             from_logits=True)\r\n    loss = K.switch(tf.size(loss) > 0, K.mean(loss), tf.constant(0.0))\r\n    return loss\r\n\r\n\r\ndef rpn_bbox_loss_graph(config, target_bbox, rpn_match, rpn_bbox):\r\n    \"\"\"Return the RPN bounding box loss graph.\r\n\r\n    config: the model config object.\r\n    target_bbox: [batch, max positive anchors, (dy, dx, log(dh), log(dw))].\r\n        Uses 0 padding to fill in unsed bbox deltas.\r\n    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\r\n               -1=negative, 0=neutral anchor.\r\n    rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\r\n    \"\"\"\r\n    # Positive anchors contribute to the loss, but negative and\r\n    # neutral anchors (match value of 0 or -1) don't.\r\n    rpn_match = K.squeeze(rpn_match, -1)\r\n    indices = tf.where(K.equal(rpn_match, 1))\r\n\r\n    # Pick bbox deltas that contribute to the loss\r\n    rpn_bbox = tf.gather_nd(rpn_bbox, indices)\r\n\r\n    # Trim target bounding box deltas to the same length as rpn_bbox.\r\n    batch_counts = K.sum(K.cast(K.equal(rpn_match, 1), tf.int32), axis=1)\r\n    target_bbox = batch_pack_graph(target_bbox, batch_counts,\r\n                                   config.IMAGES_PER_GPU)\r\n\r\n    loss = smooth_l1_loss(target_bbox, rpn_bbox)\r\n    \r\n    loss = K.switch(tf.size(loss) > 0, K.mean(loss), tf.constant(0.0))\r\n    return loss\r\n\r\n\r\ndef mrcnn_class_loss_graph(target_class_ids, pred_class_logits,\r\n                           active_class_ids):\r\n    \"\"\"Loss for the classifier head of Mask RCNN.\r\n\r\n    target_class_ids: [batch, num_rois]. Integer class IDs. Uses zero\r\n        padding to fill in the array.\r\n    pred_class_logits: [batch, num_rois, num_classes]\r\n    active_class_ids: [batch, num_classes]. Has a value of 1 for\r\n        classes that are in the dataset of the image, and 0\r\n        for classes that are not in the dataset.\r\n    \"\"\"\r\n    # During model building, Keras calls this function with\r\n    # target_class_ids of type float32. Unclear why. Cast it\r\n    # to int to get around it.\r\n    target_class_ids = tf.cast(target_class_ids, 'int64')\r\n\r\n    # Find predictions of classes that are not in the dataset.\r\n    pred_class_ids = tf.argmax(pred_class_logits, axis=2)\r\n    # TODO: Update this line to work with batch > 1. Right now it assumes all\r\n    #       images in a batch have the same active_class_ids\r\n    pred_active = tf.gather(active_class_ids[0], pred_class_ids)\r\n\r\n    # Loss\r\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n        labels=target_class_ids, logits=pred_class_logits)\r\n\r\n    # Erase losses of predictions of classes that are not in the active\r\n    # classes of the image.\r\n    loss = loss * pred_active\r\n\r\n    # Computer loss mean. Use only predictions that contribute\r\n    # to the loss to get a correct mean.\r\n    loss = tf.reduce_sum(loss) / tf.reduce_sum(pred_active)\r\n    return loss\r\n\r\n\r\ndef mrcnn_bbox_loss_graph(target_bbox, target_class_ids, pred_bbox):\r\n    \"\"\"Loss for Mask R-CNN bounding box refinement.\r\n\r\n    target_bbox: [batch, num_rois, (dy, dx, log(dh), log(dw))]\r\n    target_class_ids: [batch, num_rois]. Integer class IDs.\r\n    pred_bbox: [batch, num_rois, num_classes, (dy, dx, log(dh), log(dw))]\r\n    \"\"\"\r\n    # Reshape to merge batch and roi dimensions for simplicity.\r\n    target_class_ids = K.reshape(target_class_ids, (-1,))\r\n    target_bbox = K.reshape(target_bbox, (-1, 4))\r\n    pred_bbox = K.reshape(pred_bbox, (-1, K.int_shape(pred_bbox)[2], 4))\r\n\r\n    # Only positive ROIs contribute to the loss. And only\r\n    # the right class_id of each ROI. Get their indices.\r\n    positive_roi_ix = tf.where(target_class_ids > 0)[:, 0]\r\n    positive_roi_class_ids = tf.cast(\r\n        tf.gather(target_class_ids, positive_roi_ix), tf.int64)\r\n    indices = tf.stack([positive_roi_ix, positive_roi_class_ids], axis=1)\r\n\r\n    # Gather the deltas (predicted and true) that contribute to loss\r\n    target_bbox = tf.gather(target_bbox, positive_roi_ix)\r\n    pred_bbox = tf.gather_nd(pred_bbox, indices)\r\n\r\n    # Smooth-L1 Loss\r\n    loss = K.switch(tf.size(target_bbox) > 0,\r\n                    smooth_l1_loss(y_true=target_bbox, y_pred=pred_bbox),\r\n                    tf.constant(0.0))\r\n    loss = K.mean(loss)\r\n    return loss\r\n\r\n\r\ndef mrcnn_mask_loss_graph(target_masks, target_class_ids, pred_masks):\r\n    \"\"\"Mask binary cross-entropy loss for the masks head.\r\n\r\n    target_masks: [batch, num_rois, height, width].\r\n        A float32 tensor of values 0 or 1. Uses zero padding to fill array.\r\n    target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded.\r\n    pred_masks: [batch, proposals, height, width, num_classes] float32 tensor\r\n                with values from 0 to 1.\r\n    \"\"\"\r\n    # Reshape for simplicity. Merge first two dimensions into one.\r\n    target_class_ids = K.reshape(target_class_ids, (-1,))\r\n    mask_shape = tf.shape(target_masks)\r\n    target_masks = K.reshape(target_masks, (-1, mask_shape[2], mask_shape[3]))\r\n    pred_shape = tf.shape(pred_masks)\r\n    pred_masks = K.reshape(pred_masks,\r\n                           (-1, pred_shape[2], pred_shape[3], pred_shape[4]))\r\n    # Permute predicted masks to [N, num_classes, height, width]\r\n    pred_masks = tf.transpose(pred_masks, [0, 3, 1, 2])\r\n\r\n    # Only positive ROIs contribute to the loss. And only\r\n    # the class specific mask of each ROI.\r\n    positive_ix = tf.where(target_class_ids > 0)[:, 0]\r\n    positive_class_ids = tf.cast(\r\n        tf.gather(target_class_ids, positive_ix), tf.int64)\r\n    indices = tf.stack([positive_ix, positive_class_ids], axis=1)\r\n\r\n    # Gather the masks (predicted and true) that contribute to loss\r\n    y_true = tf.gather(target_masks, positive_ix)\r\n    y_pred = tf.gather_nd(pred_masks, indices)\r\n\r\n    # Compute binary cross entropy. If no positive ROIs, then return 0.\r\n    # shape: [batch, roi, num_classes]\r\n    loss = K.switch(tf.size(y_true) > 0,\r\n                    K.binary_crossentropy(target=y_true, output=y_pred),\r\n                    tf.constant(0.0))\r\n    loss = K.mean(loss)\r\n    return loss\r\n\r\n\r\n############################################################\r\n#  Data Generator\r\n############################################################\r\n\r\ndef load_image_gt(dataset, config, image_id, augment=False, augmentation=None,\r\n                  use_mini_mask=False):\r\n    \"\"\"Load and return ground truth data for an image (image, mask, bounding boxes).\r\n\r\n    augment: (deprecated. Use augmentation instead). If true, apply random\r\n        image augmentation. Currently, only horizontal flipping is offered.\r\n    augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.\r\n        For example, passing imgaug.augmenters.Fliplr(0.5) flips images\r\n        right/left 50% of the time.\r\n    use_mini_mask: If False, returns full-size masks that are the same height\r\n        and width as the original image. These can be big, for example\r\n        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,\r\n        224x224 and are generated by extracting the bounding box of the\r\n        object and resizing it to MINI_MASK_SHAPE.\r\n\r\n    Returns:\r\n    image: [height, width, 3]\r\n    shape: the original shape of the image before resizing and cropping.\r\n    class_ids: [instance_count] Integer class IDs\r\n    bbox: [instance_count, (y1, x1, y2, x2)]\r\n    mask: [height, width, instance_count]. The height and width are those\r\n        of the image unless use_mini_mask is True, in which case they are\r\n        defined in MINI_MASK_SHAPE.\r\n    \"\"\"\r\n    # Load image and mask\r\n    image = dataset.load_image(image_id)\r\n    mask, class_ids = dataset.load_mask(image_id)\r\n    original_shape = image.shape\r\n    image, window, scale, padding, crop = utils.resize_image(\r\n        image,\r\n        min_dim=config.IMAGE_MIN_DIM,\r\n        min_scale=config.IMAGE_MIN_SCALE,\r\n        max_dim=config.IMAGE_MAX_DIM,\r\n        mode=config.IMAGE_RESIZE_MODE)\r\n    mask = utils.resize_mask(mask, scale, padding, crop)\r\n\r\n    # Random horizontal flips.\r\n    # TODO: will be removed in a future update in favor of augmentation\r\n    if augment:\r\n        logging.warning(\"'augment' is deprecated. Use 'augmentation' instead.\")\r\n        if random.randint(0, 1):\r\n            image = np.fliplr(image)\r\n            mask = np.fliplr(mask)\r\n\r\n    # Augmentation\r\n    # This requires the imgaug lib (https://github.com/aleju/imgaug)\r\n    if augmentation:\r\n        import imgaug\r\n\r\n        # Augmenters that are safe to apply to masks\r\n        # Some, such as Affine, have settings that make them unsafe, so always\r\n        # test your augmentation on masks\r\n        MASK_AUGMENTERS = [\"Sequential\", \"SomeOf\", \"OneOf\", \"Sometimes\",\r\n                           \"Fliplr\", \"Flipud\", \"CropAndPad\",\r\n                           \"Affine\", \"PiecewiseAffine\"]\r\n\r\n        def hook(images, augmenter, parents, default):\r\n            \"\"\"Determines which augmenters to apply to masks.\"\"\"\r\n            return augmenter.__class__.__name__ in MASK_AUGMENTERS\r\n\r\n        # Store shapes before augmentation to compare\r\n        image_shape = image.shape\r\n        mask_shape = mask.shape\r\n        # Make augmenters deterministic to apply similarly to images and masks\r\n        det = augmentation.to_deterministic()\r\n        image = det.augment_image(image)\r\n        # Change mask to np.uint8 because imgaug doesn't support np.bool\r\n        mask = det.augment_image(mask.astype(np.uint8),\r\n                                 hooks=imgaug.HooksImages(activator=hook))\r\n        # Verify that shapes didn't change\r\n        assert image.shape == image_shape, \"Augmentation shouldn't change image size\"\r\n        assert mask.shape == mask_shape, \"Augmentation shouldn't change mask size\"\r\n        # Change mask back to bool\r\n        mask = mask.astype(np.bool)\r\n\r\n    # Note that some boxes might be all zeros if the corresponding mask got cropped out.\r\n    # and here is to filter them out\r\n    _idx = np.sum(mask, axis=(0, 1)) > 0\r\n    mask = mask[:, :, _idx]\r\n    class_ids = class_ids[_idx]\r\n    # Bounding boxes. Note that some boxes might be all zeros\r\n    # if the corresponding mask got cropped out.\r\n    # bbox: [num_instances, (y1, x1, y2, x2)]\r\n    bbox = utils.extract_bboxes(mask)\r\n\r\n    # Active classes\r\n    # Different datasets have different classes, so track the\r\n    # classes supported in the dataset of this image.\r\n    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)\r\n    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][\"source\"]]\r\n    active_class_ids[source_class_ids] = 1\r\n\r\n    # Resize masks to smaller size to reduce memory usage\r\n    if use_mini_mask:\r\n        mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\r\n\r\n    # Image meta data\r\n    image_meta = compose_image_meta(image_id, original_shape, image.shape,\r\n                                    window, scale, active_class_ids)\r\n\r\n    return image, image_meta, class_ids, bbox, mask\r\n\r\n\r\ndef build_detection_targets(rpn_rois, gt_class_ids, gt_boxes, gt_masks, config):\r\n    \"\"\"Generate targets for training Stage 2 classifier and mask heads.\r\n    This is not used in normal training. It's useful for debugging or to train\r\n    the Mask RCNN heads without using the RPN head.\r\n\r\n    Inputs:\r\n    rpn_rois: [N, (y1, x1, y2, x2)] proposal boxes.\r\n    gt_class_ids: [instance count] Integer class IDs\r\n    gt_boxes: [instance count, (y1, x1, y2, x2)]\r\n    gt_masks: [height, width, instance count] Ground truth masks. Can be full\r\n              size or mini-masks.\r\n\r\n    Returns:\r\n    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)]\r\n    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs.\r\n    bboxes: [TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (y, x, log(h), log(w))]. Class-specific\r\n            bbox refinements.\r\n    masks: [TRAIN_ROIS_PER_IMAGE, height, width, NUM_CLASSES). Class specific masks cropped\r\n           to bbox boundaries and resized to neural network output size.\r\n    \"\"\"\r\n    assert rpn_rois.shape[0] > 0\r\n    assert gt_class_ids.dtype == np.int32, \"Expected int but got {}\".format(\r\n        gt_class_ids.dtype)\r\n    assert gt_boxes.dtype == np.int32, \"Expected int but got {}\".format(\r\n        gt_boxes.dtype)\r\n    assert gt_masks.dtype == np.bool_, \"Expected bool but got {}\".format(\r\n        gt_masks.dtype)\r\n\r\n    # It's common to add GT Boxes to ROIs but we don't do that here because\r\n    # according to XinLei Chen's paper, it doesn't help.\r\n\r\n    # Trim empty padding in gt_boxes and gt_masks parts\r\n    instance_ids = np.where(gt_class_ids > 0)[0]\r\n    assert instance_ids.shape[0] > 0, \"Image must contain instances.\"\r\n    gt_class_ids = gt_class_ids[instance_ids]\r\n    gt_boxes = gt_boxes[instance_ids]\r\n    gt_masks = gt_masks[:, :, instance_ids]\r\n\r\n    # Compute areas of ROIs and ground truth boxes.\r\n    rpn_roi_area = (rpn_rois[:, 2] - rpn_rois[:, 0]) * \\\r\n        (rpn_rois[:, 3] - rpn_rois[:, 1])\r\n    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * \\\r\n        (gt_boxes[:, 3] - gt_boxes[:, 1])\r\n\r\n    # Compute overlaps [rpn_rois, gt_boxes]\r\n    overlaps = np.zeros((rpn_rois.shape[0], gt_boxes.shape[0]))\r\n    for i in range(overlaps.shape[1]):\r\n        gt = gt_boxes[i]\r\n        overlaps[:, i] = utils.compute_iou(\r\n            gt, rpn_rois, gt_box_area[i], rpn_roi_area)\r\n\r\n    # Assign ROIs to GT boxes\r\n    rpn_roi_iou_argmax = np.argmax(overlaps, axis=1)\r\n    rpn_roi_iou_max = overlaps[np.arange(\r\n        overlaps.shape[0]), rpn_roi_iou_argmax]\r\n    # GT box assigned to each ROI\r\n    rpn_roi_gt_boxes = gt_boxes[rpn_roi_iou_argmax]\r\n    rpn_roi_gt_class_ids = gt_class_ids[rpn_roi_iou_argmax]\r\n\r\n    # Positive ROIs are those with >= 0.5 IoU with a GT box.\r\n    fg_ids = np.where(rpn_roi_iou_max > 0.5)[0]\r\n\r\n    # Negative ROIs are those with max IoU 0.1-0.5 (hard example mining)\r\n    # TODO: To hard example mine or not to hard example mine, that's the question\r\n    # bg_ids = np.where((rpn_roi_iou_max >= 0.1) & (rpn_roi_iou_max < 0.5))[0]\r\n    bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]\r\n\r\n    # Subsample ROIs. Aim for 33% foreground.\r\n    # FG\r\n    fg_roi_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)\r\n    if fg_ids.shape[0] > fg_roi_count:\r\n        keep_fg_ids = np.random.choice(fg_ids, fg_roi_count, replace=False)\r\n    else:\r\n        keep_fg_ids = fg_ids\r\n    # BG\r\n    remaining = config.TRAIN_ROIS_PER_IMAGE - keep_fg_ids.shape[0]\r\n    if bg_ids.shape[0] > remaining:\r\n        keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)\r\n    else:\r\n        keep_bg_ids = bg_ids\r\n    # Combine indices of ROIs to keep\r\n    keep = np.concatenate([keep_fg_ids, keep_bg_ids])\r\n    # Need more?\r\n    remaining = config.TRAIN_ROIS_PER_IMAGE - keep.shape[0]\r\n    if remaining > 0:\r\n        # Looks like we don't have enough samples to maintain the desired\r\n        # balance. Reduce requirements and fill in the rest. This is\r\n        # likely different from the Mask RCNN paper.\r\n\r\n        # There is a small chance we have neither fg nor bg samples.\r\n        if keep.shape[0] == 0:\r\n            # Pick bg regions with easier IoU threshold\r\n            bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]\r\n            assert bg_ids.shape[0] >= remaining\r\n            keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)\r\n            assert keep_bg_ids.shape[0] == remaining\r\n            keep = np.concatenate([keep, keep_bg_ids])\r\n        else:\r\n            # Fill the rest with repeated bg rois.\r\n            keep_extra_ids = np.random.choice(\r\n                keep_bg_ids, remaining, replace=True)\r\n            keep = np.concatenate([keep, keep_extra_ids])\r\n    assert keep.shape[0] == config.TRAIN_ROIS_PER_IMAGE, \\\r\n        \"keep doesn't match ROI batch size {}, {}\".format(\r\n            keep.shape[0], config.TRAIN_ROIS_PER_IMAGE)\r\n\r\n    # Reset the gt boxes assigned to BG ROIs.\r\n    rpn_roi_gt_boxes[keep_bg_ids, :] = 0\r\n    rpn_roi_gt_class_ids[keep_bg_ids] = 0\r\n\r\n    # For each kept ROI, assign a class_id, and for FG ROIs also add bbox refinement.\r\n    rois = rpn_rois[keep]\r\n    roi_gt_boxes = rpn_roi_gt_boxes[keep]\r\n    roi_gt_class_ids = rpn_roi_gt_class_ids[keep]\r\n    roi_gt_assignment = rpn_roi_iou_argmax[keep]\r\n\r\n    # Class-aware bbox deltas. [y, x, log(h), log(w)]\r\n    bboxes = np.zeros((config.TRAIN_ROIS_PER_IMAGE,\r\n                       config.NUM_CLASSES, 4), dtype=np.float32)\r\n    pos_ids = np.where(roi_gt_class_ids > 0)[0]\r\n    bboxes[pos_ids, roi_gt_class_ids[pos_ids]] = utils.box_refinement(\r\n        rois[pos_ids], roi_gt_boxes[pos_ids, :4])\r\n    # Normalize bbox refinements\r\n    bboxes /= config.BBOX_STD_DEV\r\n\r\n    # Generate class-specific target masks\r\n    masks = np.zeros((config.TRAIN_ROIS_PER_IMAGE, config.MASK_SHAPE[0], config.MASK_SHAPE[1], config.NUM_CLASSES),\r\n                     dtype=np.float32)\r\n    for i in pos_ids:\r\n        class_id = roi_gt_class_ids[i]\r\n        assert class_id > 0, \"class id must be greater than 0\"\r\n        gt_id = roi_gt_assignment[i]\r\n        class_mask = gt_masks[:, :, gt_id]\r\n\r\n        if config.USE_MINI_MASK:\r\n            # Create a mask placeholder, the size of the image\r\n            placeholder = np.zeros(config.IMAGE_SHAPE[:2], dtype=bool)\r\n            # GT box\r\n            gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[gt_id]\r\n            gt_w = gt_x2 - gt_x1\r\n            gt_h = gt_y2 - gt_y1\r\n            # Resize mini mask to size of GT box\r\n            placeholder[gt_y1:gt_y2, gt_x1:gt_x2] = \\\r\n                np.round(utils.resize(class_mask, (gt_h, gt_w))).astype(bool)\r\n            # Place the mini batch in the placeholder\r\n            class_mask = placeholder\r\n\r\n        # Pick part of the mask and resize it\r\n        y1, x1, y2, x2 = rois[i].astype(np.int32)\r\n        m = class_mask[y1:y2, x1:x2]\r\n        mask = utils.resize(m, config.MASK_SHAPE)\r\n        masks[i, :, :, class_id] = mask\r\n\r\n    return rois, roi_gt_class_ids, bboxes, masks\r\n\r\n\r\ndef build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):\r\n    \"\"\"Given the anchors and GT boxes, compute overlaps and identify positive\r\n    anchors and deltas to refine them to match their corresponding GT boxes.\r\n\r\n    anchors: [num_anchors, (y1, x1, y2, x2)]\r\n    gt_class_ids: [num_gt_boxes] Integer class IDs.\r\n    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]\r\n\r\n    Returns:\r\n    rpn_match: [N] (int32) matches between anchors and GT boxes.\r\n               1 = positive anchor, -1 = negative anchor, 0 = neutral\r\n    rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\r\n    \"\"\"\r\n    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral\r\n    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)\r\n    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]\r\n    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))\r\n\r\n    # Handle COCO crowds\r\n    # A crowd box in COCO is a bounding box around several instances. Exclude\r\n    # them from training. A crowd box is given a negative class ID.\r\n    crowd_ix = np.where(gt_class_ids < 0)[0]\r\n    if crowd_ix.shape[0] > 0:\r\n        # Filter out crowds from ground truth class IDs and boxes\r\n        non_crowd_ix = np.where(gt_class_ids > 0)[0]\r\n        crowd_boxes = gt_boxes[crowd_ix]\r\n        gt_class_ids = gt_class_ids[non_crowd_ix]\r\n        gt_boxes = gt_boxes[non_crowd_ix]\r\n        # Compute overlaps with crowd boxes [anchors, crowds]\r\n        crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)\r\n        crowd_iou_max = np.amax(crowd_overlaps, axis=1)\r\n        no_crowd_bool = (crowd_iou_max < 0.001)\r\n    else:\r\n        # All anchors don't intersect a crowd\r\n        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)\r\n\r\n    # Compute overlaps [num_anchors, num_gt_boxes]\r\n    overlaps = utils.compute_overlaps(anchors, gt_boxes)\r\n\r\n    # Match anchors to GT Boxes\r\n    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.\r\n    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.\r\n    # Neutral anchors are those that don't match the conditions above,\r\n    # and they don't influence the loss function.\r\n    # However, don't keep any GT box unmatched (rare, but happens). Instead,\r\n    # match it to the closest anchor (even if its max IoU is < 0.3).\r\n    #\r\n    # 1. Set negative anchors first. They get overwritten below if a GT box is\r\n    # matched to them. Skip boxes in crowd areas.\r\n    anchor_iou_argmax = np.argmax(overlaps, axis=1)\r\n    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]\r\n    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1\r\n    # 2. Set an anchor for each GT box (regardless of IoU value).\r\n    # If multiple anchors have the same IoU match all of them\r\n    gt_iou_argmax = np.argwhere(overlaps == np.max(overlaps, axis=0))[:,0]\r\n    rpn_match[gt_iou_argmax] = 1\r\n    # 3. Set anchors with high overlap as positive.\r\n    rpn_match[anchor_iou_max >= 0.7] = 1\r\n\r\n    # Subsample to balance positive and negative anchors\r\n    # Don't let positives be more than half the anchors\r\n    ids = np.where(rpn_match == 1)[0]\r\n    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)\r\n    if extra > 0:\r\n        # Reset the extra ones to neutral\r\n        ids = np.random.choice(ids, extra, replace=False)\r\n        rpn_match[ids] = 0\r\n    # Same for negative proposals\r\n    ids = np.where(rpn_match == -1)[0]\r\n    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -\r\n                        np.sum(rpn_match == 1))\r\n    if extra > 0:\r\n        # Rest the extra ones to neutral\r\n        ids = np.random.choice(ids, extra, replace=False)\r\n        rpn_match[ids] = 0\r\n\r\n    # For positive anchors, compute shift and scale needed to transform them\r\n    # to match the corresponding GT boxes.\r\n    ids = np.where(rpn_match == 1)[0]\r\n    ix = 0  # index into rpn_bbox\r\n    # TODO: use box_refinement() rather than duplicating the code here\r\n    for i, a in zip(ids, anchors[ids]):\r\n        # Closest gt box (it might have IoU < 0.7)\r\n        gt = gt_boxes[anchor_iou_argmax[i]]\r\n\r\n        # Convert coordinates to center plus width/height.\r\n        # GT Box\r\n        gt_h = gt[2] - gt[0]\r\n        gt_w = gt[3] - gt[1]\r\n        gt_center_y = gt[0] + 0.5 * gt_h\r\n        gt_center_x = gt[1] + 0.5 * gt_w\r\n        # Anchor\r\n        a_h = a[2] - a[0]\r\n        a_w = a[3] - a[1]\r\n        a_center_y = a[0] + 0.5 * a_h\r\n        a_center_x = a[1] + 0.5 * a_w\r\n\r\n        # Compute the bbox refinement that the RPN should predict.\r\n        rpn_bbox[ix] = [\r\n            (gt_center_y - a_center_y) / a_h,\r\n            (gt_center_x - a_center_x) / a_w,\r\n            np.log(gt_h / a_h),\r\n            np.log(gt_w / a_w),\r\n        ]\r\n        # Normalize\r\n        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV\r\n        ix += 1\r\n\r\n    return rpn_match, rpn_bbox\r\n\r\n\r\ndef generate_random_rois(image_shape, count, gt_class_ids, gt_boxes):\r\n    \"\"\"Generates ROI proposals similar to what a region proposal network\r\n    would generate.\r\n\r\n    image_shape: [Height, Width, Depth]\r\n    count: Number of ROIs to generate\r\n    gt_class_ids: [N] Integer ground truth class IDs\r\n    gt_boxes: [N, (y1, x1, y2, x2)] Ground truth boxes in pixels.\r\n\r\n    Returns: [count, (y1, x1, y2, x2)] ROI boxes in pixels.\r\n    \"\"\"\r\n    # placeholder\r\n    rois = np.zeros((count, 4), dtype=np.int32)\r\n\r\n    # Generate random ROIs around GT boxes (90% of count)\r\n    rois_per_box = int(0.9 * count / gt_boxes.shape[0])\r\n    for i in range(gt_boxes.shape[0]):\r\n        gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[i]\r\n        h = gt_y2 - gt_y1\r\n        w = gt_x2 - gt_x1\r\n        # random boundaries\r\n        r_y1 = max(gt_y1 - h, 0)\r\n        r_y2 = min(gt_y2 + h, image_shape[0])\r\n        r_x1 = max(gt_x1 - w, 0)\r\n        r_x2 = min(gt_x2 + w, image_shape[1])\r\n\r\n        # To avoid generating boxes with zero area, we generate double what\r\n        # we need and filter out the extra. If we get fewer valid boxes\r\n        # than we need, we loop and try again.\r\n        while True:\r\n            y1y2 = np.random.randint(r_y1, r_y2, (rois_per_box * 2, 2))\r\n            x1x2 = np.random.randint(r_x1, r_x2, (rois_per_box * 2, 2))\r\n            # Filter out zero area boxes\r\n            threshold = 1\r\n            y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >=\r\n                        threshold][:rois_per_box]\r\n            x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >=\r\n                        threshold][:rois_per_box]\r\n            if y1y2.shape[0] == rois_per_box and x1x2.shape[0] == rois_per_box:\r\n                break\r\n\r\n        # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape\r\n        # into x1, y1, x2, y2 order\r\n        x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)\r\n        y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)\r\n        box_rois = np.hstack([y1, x1, y2, x2])\r\n        rois[rois_per_box * i:rois_per_box * (i + 1)] = box_rois\r\n\r\n    # Generate random ROIs anywhere in the image (10% of count)\r\n    remaining_count = count - (rois_per_box * gt_boxes.shape[0])\r\n    # To avoid generating boxes with zero area, we generate double what\r\n    # we need and filter out the extra. If we get fewer valid boxes\r\n    # than we need, we loop and try again.\r\n    while True:\r\n        y1y2 = np.random.randint(0, image_shape[0], (remaining_count * 2, 2))\r\n        x1x2 = np.random.randint(0, image_shape[1], (remaining_count * 2, 2))\r\n        # Filter out zero area boxes\r\n        threshold = 1\r\n        y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >=\r\n                    threshold][:remaining_count]\r\n        x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >=\r\n                    threshold][:remaining_count]\r\n        if y1y2.shape[0] == remaining_count and x1x2.shape[0] == remaining_count:\r\n            break\r\n\r\n    # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape\r\n    # into x1, y1, x2, y2 order\r\n    x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)\r\n    y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)\r\n    global_rois = np.hstack([y1, x1, y2, x2])\r\n    rois[-remaining_count:] = global_rois\r\n    return rois\r\n\r\n\r\ndef data_generator(dataset, config, shuffle=True, augment=False, augmentation=None,\r\n                   random_rois=0, batch_size=1, detection_targets=False,\r\n                   no_augmentation_sources=None):\r\n    \"\"\"A generator that returns images and corresponding target class ids,\r\n    bounding box deltas, and masks.\r\n\r\n    dataset: The Dataset object to pick data from\r\n    config: The model config object\r\n    shuffle: If True, shuffles the samples before every epoch\r\n    augment: (deprecated. Use augmentation instead). If true, apply random\r\n        image augmentation. Currently, only horizontal flipping is offered.\r\n    augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.\r\n        For example, passing imgaug.augmenters.Fliplr(0.5) flips images\r\n        right/left 50% of the time.\r\n    random_rois: If > 0 then generate proposals to be used to train the\r\n                 network classifier and mask heads. Useful if training\r\n                 the Mask RCNN part without the RPN.\r\n    batch_size: How many images to return in each call\r\n    detection_targets: If True, generate detection targets (class IDs, bbox\r\n        deltas, and masks). Typically for debugging or visualizations because\r\n        in trainig detection targets are generated by DetectionTargetLayer.\r\n    no_augmentation_sources: Optional. List of sources to exclude for\r\n        augmentation. A source is string that identifies a dataset and is\r\n        defined in the Dataset class.\r\n\r\n    Returns a Python generator. Upon calling next() on it, the\r\n    generator returns two lists, inputs and outputs. The contents\r\n    of the lists differs depending on the received arguments:\r\n    inputs list:\r\n    - images: [batch, H, W, C]\r\n    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()\r\n    - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)\r\n    - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\r\n    - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs\r\n    - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]\r\n    - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width\r\n                are those of the image unless use_mini_mask is True, in which\r\n                case they are defined in MINI_MASK_SHAPE.\r\n\r\n    outputs list: Usually empty in regular training. But if detection_targets\r\n        is True then the outputs list contains target class_ids, bbox deltas,\r\n        and masks.\r\n    \"\"\"\r\n    b = 0  # batch item index\r\n    image_index = -1\r\n    image_ids = np.copy(dataset.image_ids)\r\n    error_count = 0\r\n    no_augmentation_sources = no_augmentation_sources or []\r\n\r\n    # Anchors\r\n    # [anchor_count, (y1, x1, y2, x2)]\r\n    backbone_shapes = compute_backbone_shapes(config, config.IMAGE_SHAPE)\r\n    anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\r\n                                             config.RPN_ANCHOR_RATIOS,\r\n                                             backbone_shapes,\r\n                                             config.BACKBONE_STRIDES,\r\n                                             config.RPN_ANCHOR_STRIDE)\r\n\r\n    # Keras requires a generator to run indefinitely.\r\n    while True:\r\n        try:\r\n            # Increment index to pick next image. Shuffle if at the start of an epoch.\r\n            image_index = (image_index + 1) % len(image_ids)\r\n            if shuffle and image_index == 0:\r\n                np.random.shuffle(image_ids)\r\n\r\n            # Get GT bounding boxes and masks for image.\r\n            image_id = image_ids[image_index]\r\n\r\n            # If the image source is not to be augmented pass None as augmentation\r\n            if dataset.image_info[image_id]['source'] in no_augmentation_sources:\r\n                image, image_meta, gt_class_ids, gt_boxes, gt_masks = \\\r\n                load_image_gt(dataset, config, image_id, augment=augment,\r\n                              augmentation=None,\r\n                              use_mini_mask=config.USE_MINI_MASK)\r\n            else:\r\n                image, image_meta, gt_class_ids, gt_boxes, gt_masks = \\\r\n                    load_image_gt(dataset, config, image_id, augment=augment,\r\n                                augmentation=augmentation,\r\n                                use_mini_mask=config.USE_MINI_MASK)\r\n\r\n            # Skip images that have no instances. This can happen in cases\r\n            # where we train on a subset of classes and the image doesn't\r\n            # have any of the classes we care about.\r\n            if not np.any(gt_class_ids > 0):\r\n                continue\r\n\r\n            # RPN Targets\r\n            rpn_match, rpn_bbox = build_rpn_targets(image.shape, anchors,\r\n                                                    gt_class_ids, gt_boxes, config)\r\n\r\n            # Mask R-CNN Targets\r\n            if random_rois:\r\n                rpn_rois = generate_random_rois(\r\n                    image.shape, random_rois, gt_class_ids, gt_boxes)\r\n                if detection_targets:\r\n                    rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask =\\\r\n                        build_detection_targets(\r\n                            rpn_rois, gt_class_ids, gt_boxes, gt_masks, config)\r\n\r\n            # Init batch arrays\r\n            if b == 0:\r\n                batch_image_meta = np.zeros(\r\n                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype)\r\n                batch_rpn_match = np.zeros(\r\n                    [batch_size, anchors.shape[0], 1], dtype=rpn_match.dtype)\r\n                batch_rpn_bbox = np.zeros(\r\n                    [batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)\r\n                batch_images = np.zeros(\r\n                    (batch_size,) + image.shape, dtype=np.float32)\r\n                batch_gt_class_ids = np.zeros(\r\n                    (batch_size, config.MAX_GT_INSTANCES), dtype=np.int32)\r\n                batch_gt_boxes = np.zeros(\r\n                    (batch_size, config.MAX_GT_INSTANCES, 4), dtype=np.int32)\r\n                batch_gt_masks = np.zeros(\r\n                    (batch_size, gt_masks.shape[0], gt_masks.shape[1],\r\n                     config.MAX_GT_INSTANCES), dtype=gt_masks.dtype)\r\n                if random_rois:\r\n                    batch_rpn_rois = np.zeros(\r\n                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)\r\n                    if detection_targets:\r\n                        batch_rois = np.zeros(\r\n                            (batch_size,) + rois.shape, dtype=rois.dtype)\r\n                        batch_mrcnn_class_ids = np.zeros(\r\n                            (batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)\r\n                        batch_mrcnn_bbox = np.zeros(\r\n                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)\r\n                        batch_mrcnn_mask = np.zeros(\r\n                            (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)\r\n\r\n            # If more instances than fits in the array, sub-sample from them.\r\n            if gt_boxes.shape[0] > config.MAX_GT_INSTANCES:\r\n                ids = np.random.choice(\r\n                    np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)\r\n                gt_class_ids = gt_class_ids[ids]\r\n                gt_boxes = gt_boxes[ids]\r\n                gt_masks = gt_masks[:, :, ids]\r\n\r\n            # Add to batch\r\n            batch_image_meta[b] = image_meta\r\n            batch_rpn_match[b] = rpn_match[:, np.newaxis]\r\n            batch_rpn_bbox[b] = rpn_bbox\r\n            batch_images[b] = mold_image(image.astype(np.float32), config)\r\n            batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids\r\n            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes\r\n            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks\r\n            if random_rois:\r\n                batch_rpn_rois[b] = rpn_rois\r\n                if detection_targets:\r\n                    batch_rois[b] = rois\r\n                    batch_mrcnn_class_ids[b] = mrcnn_class_ids\r\n                    batch_mrcnn_bbox[b] = mrcnn_bbox\r\n                    batch_mrcnn_mask[b] = mrcnn_mask\r\n            b += 1\r\n\r\n            # Batch full?\r\n            if b >= batch_size:\r\n                inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,\r\n                          batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]\r\n                outputs = []\r\n\r\n                if random_rois:\r\n                    inputs.extend([batch_rpn_rois])\r\n                    if detection_targets:\r\n                        inputs.extend([batch_rois])\r\n                        # Keras requires that output and targets have the same number of dimensions\r\n                        batch_mrcnn_class_ids = np.expand_dims(\r\n                            batch_mrcnn_class_ids, -1)\r\n                        outputs.extend(\r\n                            [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])\r\n\r\n                yield inputs, outputs\r\n\r\n                # start a new batch\r\n                b = 0\r\n        except (GeneratorExit, KeyboardInterrupt):\r\n            raise\r\n        except:\r\n            # Log it and skip the image\r\n            logging.exception(\"Error processing image {}\".format(\r\n                dataset.image_info[image_id]))\r\n            error_count += 1\r\n            if error_count > 5:\r\n                raise\r\n\r\n\r\n############################################################\r\n#  MaskRCNN Class\r\n############################################################\r\n\r\nclass MaskRCNN():\r\n    \"\"\"Encapsulates the Mask RCNN model functionality.\r\n\r\n    The actual Keras model is in the keras_model property.\r\n    \"\"\"\r\n\r\n    def __init__(self, mode, config, model_dir):\r\n        \"\"\"\r\n        mode: Either \"training\" or \"inference\"\r\n        config: A Sub-class of the Config class\r\n        model_dir: Directory to save training logs and trained weights\r\n        \"\"\"\r\n        assert mode in ['training', 'inference']\r\n        self.mode = mode\r\n        self.config = config\r\n        self.model_dir = model_dir\r\n        self.set_log_dir()\r\n        self.keras_model = self.build(mode=mode, config=config)\r\n\r\n    def build(self, mode, config):\r\n        \"\"\"Build Mask R-CNN architecture.\r\n            input_shape: The shape of the input image.\r\n            mode: Either \"training\" or \"inference\". The inputs and\r\n                outputs of the model differ accordingly.\r\n        \"\"\"\r\n        assert mode in ['training', 'inference']\r\n\r\n        # Image size must be dividable by 2 multiple times\r\n        h, w = config.IMAGE_SHAPE[:2]\r\n        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\r\n            raise Exception(\"Image size must be dividable by 2 at least 6 times \"\r\n                            \"to avoid fractions when downscaling and upscaling.\"\r\n                            \"For example, use 256, 320, 384, 448, 512, ... etc. \")\r\n\r\n        # Inputs\r\n        input_image = KL.Input(\r\n            shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\")\r\n        input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],\r\n                                    name=\"input_image_meta\")\r\n        if mode == \"training\":\r\n            # RPN GT\r\n            input_rpn_match = KL.Input(\r\n                shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32)\r\n            input_rpn_bbox = KL.Input(\r\n                shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\r\n\r\n            # Detection GT (class IDs, bounding boxes, and masks)\r\n            # 1. GT Class IDs (zero padded)\r\n            input_gt_class_ids = KL.Input(\r\n                shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32)\r\n            # 2. GT Boxes in pixels (zero padded)\r\n            # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates\r\n            input_gt_boxes = KL.Input(\r\n                shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\r\n            # Normalize coordinates\r\n            gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(\r\n                x, K.shape(input_image)[1:3]))(input_gt_boxes)\r\n            # 3. GT Masks (zero padded)\r\n            # [batch, height, width, MAX_GT_INSTANCES]\r\n            if config.USE_MINI_MASK:\r\n                input_gt_masks = KL.Input(\r\n                    shape=[config.MINI_MASK_SHAPE[0],\r\n                           config.MINI_MASK_SHAPE[1], None],\r\n                    name=\"input_gt_masks\", dtype=bool)\r\n            else:\r\n                input_gt_masks = KL.Input(\r\n                    shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None],\r\n                    name=\"input_gt_masks\", dtype=bool)\r\n        elif mode == \"inference\":\r\n            # Anchors in normalized coordinates\r\n            input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\")\r\n\r\n        # Build the shared convolutional layers.\r\n        # Bottom-up Layers\r\n        # Returns a list of the last layers of each stage, 5 in total.\r\n        # Don't create the thead (stage 5), so we pick the 4th item in the list.\r\n        if callable(config.BACKBONE):\r\n            _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,\r\n                                                train_bn=config.TRAIN_BN)\r\n        else:\r\n            _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,\r\n                                             stage5=True, train_bn=config.TRAIN_BN)\r\n        # Top-down Layers\r\n        # TODO: add assert to varify feature map sizes match what's in config\r\n        P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5)\r\n        P4 = KL.Add(name=\"fpn_p4add\")([\r\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\r\n            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)])\r\n        P3 = KL.Add(name=\"fpn_p3add\")([\r\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\r\n            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)])\r\n        P2 = KL.Add(name=\"fpn_p2add\")([\r\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\r\n            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)])\r\n        # Attach 3x3 conv to all P layers to get the final feature maps.\r\n        P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\r\n        P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\r\n        P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\r\n        P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\r\n        # P6 is used for the 5th anchor scale in RPN. Generated by\r\n        # subsampling from P5 with stride of 2.\r\n        P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\r\n\r\n        # Note that P6 is used in RPN, but not in the classifier heads.\r\n        rpn_feature_maps = [P2, P3, P4, P5, P6]\r\n        mrcnn_feature_maps = [P2, P3, P4, P5]\r\n\r\n        # Anchors\r\n        if mode == \"training\":\r\n            anchors = self.get_anchors(config.IMAGE_SHAPE)\r\n            # Duplicate across the batch dimension because Keras requires it\r\n            # TODO: can this be optimized to avoid duplicating the anchors?\r\n            anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\r\n            # A hack to get around Keras's bad support for constants\r\n            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)\r\n        else:\r\n            anchors = input_anchors\r\n\r\n        # RPN Model\r\n        rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,\r\n                              len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE)\r\n        # Loop through pyramid layers\r\n        layer_outputs = []  # list of lists\r\n        for p in rpn_feature_maps:\r\n            layer_outputs.append(rpn([p]))\r\n        # Concatenate layer outputs\r\n        # Convert from list of lists of level outputs to list of lists\r\n        # of outputs across levels.\r\n        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\r\n        output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\r\n        outputs = list(zip(*layer_outputs))\r\n        outputs = [KL.Concatenate(axis=1, name=n)(list(o))\r\n                   for o, n in zip(outputs, output_names)]\r\n\r\n        rpn_class_logits, rpn_class, rpn_bbox = outputs\r\n\r\n        # Generate proposals\r\n        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\r\n        # and zero padded.\r\n        proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\r\n            else config.POST_NMS_ROIS_INFERENCE\r\n        rpn_rois = ProposalLayer(\r\n            proposal_count=proposal_count,\r\n            nms_threshold=config.RPN_NMS_THRESHOLD,\r\n            name=\"ROI\",\r\n            config=config)([rpn_class, rpn_bbox, anchors])\r\n\r\n        if mode == \"training\":\r\n            # Class ID mask to mark class IDs supported by the dataset the image\r\n            # came from.\r\n            active_class_ids = KL.Lambda(\r\n                lambda x: parse_image_meta_graph(x)[\"active_class_ids\"]\r\n                )(input_image_meta)\r\n\r\n            if not config.USE_RPN_ROIS:\r\n                # Ignore predicted ROIs and use ROIs provided as an input.\r\n                input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\r\n                                      name=\"input_roi\", dtype=np.int32)\r\n                # Normalize coordinates\r\n                target_rois = KL.Lambda(lambda x: norm_boxes_graph(\r\n                    x, K.shape(input_image)[1:3]))(input_rois)\r\n            else:\r\n                target_rois = rpn_rois\r\n\r\n            # Generate detection targets\r\n            # Subsamples proposals and generates target outputs for training\r\n            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\r\n            # padded. Equally, returned rois and targets are zero padded.\r\n            rois, target_class_ids, target_bbox, target_mask =\\\r\n                DetectionTargetLayer(config, name=\"proposal_targets\")([\r\n                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])\r\n\r\n            # Network Heads\r\n            # TODO: verify that this handles zero padded ROIs\r\n            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\r\n                fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta,\r\n                                     config.POOL_SIZE, config.NUM_CLASSES,\r\n                                     train_bn=config.TRAIN_BN,\r\n                                     fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)\r\n\r\n            mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps,\r\n                                              input_image_meta,\r\n                                              config.MASK_POOL_SIZE,\r\n                                              config.NUM_CLASSES,\r\n                                              train_bn=config.TRAIN_BN)\r\n\r\n            # TODO: clean up (use tf.identify if necessary)\r\n            output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois)\r\n\r\n            # Losses\r\n            rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\r\n                [input_rpn_match, rpn_class_logits])\r\n            rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")(\r\n                [input_rpn_bbox, input_rpn_match, rpn_bbox])\r\n            class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")(\r\n                [target_class_ids, mrcnn_class_logits, active_class_ids])\r\n            bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")(\r\n                [target_bbox, target_class_ids, mrcnn_bbox])\r\n            mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")(\r\n                [target_mask, target_class_ids, mrcnn_mask])\r\n\r\n            # Model\r\n            inputs = [input_image, input_image_meta,\r\n                      input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks]\r\n            if not config.USE_RPN_ROIS:\r\n                inputs.append(input_rois)\r\n            outputs = [rpn_class_logits, rpn_class, rpn_bbox,\r\n                       mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask,\r\n                       rpn_rois, output_rois,\r\n                       rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss]\r\n            model = KM.Model(inputs, outputs, name='mask_rcnn')\r\n        else:\r\n            # Network Heads\r\n            # Proposal classifier and BBox regressor heads\r\n            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\r\n                fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\r\n                                     config.POOL_SIZE, config.NUM_CLASSES,\r\n                                     train_bn=config.TRAIN_BN,\r\n                                     fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)\r\n\r\n            # Detections\r\n            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in\r\n            # normalized coordinates\r\n            detections = DetectionLayer(config, name=\"mrcnn_detection\")(\r\n                [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\r\n\r\n            # Create masks for detections\r\n            detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections)\r\n            mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps,\r\n                                              input_image_meta,\r\n                                              config.MASK_POOL_SIZE,\r\n                                              config.NUM_CLASSES,\r\n                                              train_bn=config.TRAIN_BN)\r\n\r\n            model = KM.Model([input_image, input_image_meta, input_anchors],\r\n                             [detections, mrcnn_class, mrcnn_bbox,\r\n                                 mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],\r\n                             name='mask_rcnn')\r\n\r\n        # Add multi-GPU support.\r\n        if config.GPU_COUNT > 1:\r\n            from mrcnn.parallel_model import ParallelModel\r\n            model = ParallelModel(model, config.GPU_COUNT)\r\n\r\n        return model\r\n\r\n    def find_last(self):\r\n        \"\"\"Finds the last checkpoint file of the last trained model in the\r\n        model directory.\r\n        Returns:\r\n            The path of the last checkpoint file\r\n        \"\"\"\r\n        # Get directory names. Each directory corresponds to a model\r\n        dir_names = next(os.walk(self.model_dir))[1]\r\n        key = self.config.NAME.lower()\r\n        dir_names = filter(lambda f: f.startswith(key), dir_names)\r\n        dir_names = sorted(dir_names)\r\n        if not dir_names:\r\n            import errno\r\n            raise FileNotFoundError(\r\n                errno.ENOENT,\r\n                \"Could not find model directory under {}\".format(self.model_dir))\r\n        # Pick last directory\r\n        dir_name = os.path.join(self.model_dir, dir_names[-1])\r\n        # Find the last checkpoint\r\n        checkpoints = next(os.walk(dir_name))[2]\r\n        checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\r\n        checkpoints = sorted(checkpoints)\r\n        if not checkpoints:\r\n            import errno\r\n            raise FileNotFoundError(\r\n                errno.ENOENT, \"Could not find weight files in {}\".format(dir_name))\r\n        checkpoint = os.path.join(dir_name, checkpoints[-1])\r\n        return checkpoint\r\n\r\n    def load_weights(self, filepath, by_name=False, exclude=None):\r\n        \"\"\"Modified version of the corresponding Keras function with\r\n        the addition of multi-GPU support and the ability to exclude\r\n        some layers from loading.\r\n        exclude: list of layer names to exclude\r\n        \"\"\"\r\n        import h5py\r\n        # Conditional import to support versions of Keras before 2.2\r\n        # TODO: remove in about 6 months (end of 2018)\r\n        try:\r\n            from keras.engine import saving\r\n        except ImportError:\r\n            # Keras before 2.2 used the 'topology' namespace.\r\n            from keras.engine import topology as saving\r\n\r\n        if exclude:\r\n            by_name = True\r\n\r\n        if h5py is None:\r\n            raise ImportError('`load_weights` requires h5py.')\r\n        f = h5py.File(filepath, mode='r')\r\n        if 'layer_names' not in f.attrs and 'model_weights' in f:\r\n            f = f['model_weights']\r\n\r\n        # In multi-GPU training, we wrap the model. Get layers\r\n        # of the inner model because they have the weights.\r\n        keras_model = self.keras_model\r\n        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\r\n            else keras_model.layers\r\n\r\n        # Exclude some layers\r\n        if exclude:\r\n            layers = filter(lambda l: l.name not in exclude, layers)\r\n\r\n        if by_name:\r\n            saving.load_weights_from_hdf5_group_by_name(f, layers)\r\n        else:\r\n            saving.load_weights_from_hdf5_group(f, layers)\r\n        if hasattr(f, 'close'):\r\n            f.close()\r\n\r\n        # Update the log directory\r\n        self.set_log_dir(filepath)\r\n\r\n    def get_imagenet_weights(self):\r\n        \"\"\"Downloads ImageNet trained weights from Keras.\r\n        Returns path to weights file.\r\n        \"\"\"\r\n        from keras.utils.data_utils import get_file\r\n        TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/'\\\r\n                                 'releases/download/v0.2/'\\\r\n                                 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\r\n        weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\r\n                                TF_WEIGHTS_PATH_NO_TOP,\r\n                                cache_subdir='models',\r\n                                md5_hash='a268eb855778b3df3c7506639542a6af')\r\n        return weights_path\r\n\r\n    def compile(self, learning_rate, momentum):\r\n        \"\"\"Gets the model ready for training. Adds losses, regularization, and\r\n        metrics. Then calls the Keras compile() function.\r\n        \"\"\"\r\n        # Optimizer object\r\n        optimizer = keras.optimizers.SGD(\r\n            lr=learning_rate, momentum=momentum,\r\n            clipnorm=self.config.GRADIENT_CLIP_NORM)\r\n        # Add Losses\r\n        # First, clear previously set losses to avoid duplication\r\n        self.keras_model._losses = []\r\n        self.keras_model._per_input_losses = {}\r\n        loss_names = [\r\n            \"rpn_class_loss\",  \"rpn_bbox_loss\",\r\n            \"mrcnn_class_loss\", \"mrcnn_bbox_loss\", \"mrcnn_mask_loss\"]\r\n        for name in loss_names:\r\n            layer = self.keras_model.get_layer(name)\r\n            if layer.output in self.keras_model.losses:\r\n                continue\r\n            loss = (\r\n                tf.reduce_mean(layer.output, keepdims=True)\r\n                * self.config.LOSS_WEIGHTS.get(name, 1.))\r\n            self.keras_model.add_loss(loss)\r\n\r\n        # Add L2 Regularization\r\n        # Skip gamma and beta weights of batch normalization layers.\r\n        reg_losses = [\r\n            keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)\r\n            for w in self.keras_model.trainable_weights\r\n            if 'gamma' not in w.name and 'beta' not in w.name]\r\n        self.keras_model.add_loss(tf.add_n(reg_losses))\r\n\r\n        # Compile\r\n        self.keras_model.compile(\r\n            optimizer=optimizer,\r\n            loss=[None] * len(self.keras_model.outputs))\r\n\r\n        # Add metrics for losses\r\n        for name in loss_names:\r\n            if name in self.keras_model.metrics_names:\r\n                continue\r\n            layer = self.keras_model.get_layer(name)\r\n            self.keras_model.metrics_names.append(name)\r\n            loss = (\r\n                tf.reduce_mean(layer.output, keepdims=True)\r\n                * self.config.LOSS_WEIGHTS.get(name, 1.))\r\n            self.keras_model.metrics_tensors.append(loss)\r\n\r\n    def set_trainable(self, layer_regex, keras_model=None, indent=0, verbose=1):\r\n        \"\"\"Sets model layers as trainable if their names match\r\n        the given regular expression.\r\n        \"\"\"\r\n        # Print message on the first call (but not on recursive calls)\r\n        if verbose > 0 and keras_model is None:\r\n            log(\"Selecting layers to train\")\r\n\r\n        keras_model = keras_model or self.keras_model\r\n\r\n        # In multi-GPU training, we wrap the model. Get layers\r\n        # of the inner model because they have the weights.\r\n        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\r\n            else keras_model.layers\r\n\r\n        for layer in layers:\r\n            # Is the layer a model?\r\n            if layer.__class__.__name__ == 'Model':\r\n                print(\"In model: \", layer.name)\r\n                self.set_trainable(\r\n                    layer_regex, keras_model=layer, indent=indent + 4)\r\n                continue\r\n\r\n            if not layer.weights:\r\n                continue\r\n            # Is it trainable?\r\n            trainable = bool(re.fullmatch(layer_regex, layer.name))\r\n            # Update layer. If layer is a container, update inner layer.\r\n            if layer.__class__.__name__ == 'TimeDistributed':\r\n                layer.layer.trainable = trainable\r\n            else:\r\n                layer.trainable = trainable\r\n            # Print trainable layer names\r\n            if trainable and verbose > 0:\r\n                log(\"{}{:20}   ({})\".format(\" \" * indent, layer.name,\r\n                                            layer.__class__.__name__))\r\n\r\n    def set_log_dir(self, model_path=None):\r\n        \"\"\"Sets the model log directory and epoch counter.\r\n\r\n        model_path: If None, or a format different from what this code uses\r\n            then set a new log directory and start epochs from 0. Otherwise,\r\n            extract the log directory and the epoch counter from the file\r\n            name.\r\n        \"\"\"\r\n        # Set date and epoch counter as if starting a new model\r\n        self.epoch = 0\r\n        now = datetime.datetime.now()\r\n\r\n        # If we have a model path with date and epochs use them\r\n        if model_path:\r\n            # Continue from we left of. Get epoch and date from the file name\r\n            # A sample model path might look like:\r\n            # \\path\\to\\logs\\coco20171029T2315\\mask_rcnn_coco_0001.h5 (Windows)\r\n            # /path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5 (Linux)\r\n            regex = r\".*[/\\\\][\\w-]+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})[/\\\\]mask\\_rcnn\\_[\\w-]+(\\d{4})\\.h5\"\r\n            m = re.match(regex, model_path)\r\n            if m:\r\n                now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),\r\n                                        int(m.group(4)), int(m.group(5)))\r\n                # Epoch number in file is 1-based, and in Keras code it's 0-based.\r\n                # So, adjust for that then increment by one to start from the next epoch\r\n                self.epoch = int(m.group(6)) - 1 + 1\r\n                print('Re-starting from epoch %d' % self.epoch)\r\n\r\n        # Directory for training logs\r\n        self.log_dir = os.path.join(self.model_dir, \"{}{:%Y%m%dT%H%M}\".format(\r\n            self.config.NAME.lower(), now))\r\n\r\n        # Path to save after each epoch. Include placeholders that get filled by Keras.\r\n        self.checkpoint_path = os.path.join(self.log_dir, \"mask_rcnn_{}_*epoch*.h5\".format(\r\n            self.config.NAME.lower()))\r\n        self.checkpoint_path = self.checkpoint_path.replace(\r\n            \"*epoch*\", \"{epoch:04d}\")\r\n\r\n    def train(self, train_dataset, val_dataset, learning_rate, epochs, layers,\r\n              augmentation=None, custom_callbacks=None, no_augmentation_sources=None):\r\n        \"\"\"Train the model.\r\n        train_dataset, val_dataset: Training and validation Dataset objects.\r\n        learning_rate: The learning rate to train with\r\n        epochs: Number of training epochs. Note that previous training epochs\r\n                are considered to be done alreay, so this actually determines\r\n                the epochs to train in total rather than in this particaular\r\n                call.\r\n        layers: Allows selecting wich layers to train. It can be:\r\n            - A regular expression to match layer names to train\r\n            - One of these predefined values:\r\n              heads: The RPN, classifier and mask heads of the network\r\n              all: All the layers\r\n              3+: Train Resnet stage 3 and up\r\n              4+: Train Resnet stage 4 and up\r\n              5+: Train Resnet stage 5 and up\r\n        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug)\r\n            augmentation. For example, passing imgaug.augmenters.Fliplr(0.5)\r\n            flips images right/left 50% of the time. You can pass complex\r\n            augmentations as well. This augmentation applies 50% of the\r\n            time, and when it does it flips images right/left half the time\r\n            and adds a Gaussian blur with a random sigma in range 0 to 5.\r\n\r\n                augmentation = imgaug.augmenters.Sometimes(0.5, [\r\n                    imgaug.augmenters.Fliplr(0.5),\r\n                    imgaug.augmenters.GaussianBlur(sigma=(0.0, 5.0))\r\n                ])\r\n\t    custom_callbacks: Optional. Add custom callbacks to be called\r\n\t        with the keras fit_generator method. Must be list of type keras.callbacks.\r\n        no_augmentation_sources: Optional. List of sources to exclude for\r\n            augmentation. A source is string that identifies a dataset and is\r\n            defined in the Dataset class.\r\n        \"\"\"\r\n        assert self.mode == \"training\", \"Create model in training mode.\"\r\n\r\n        # Pre-defined layer regular expressions\r\n        layer_regex = {\r\n            # all layers but the backbone\r\n            \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\r\n            # From a specific Resnet stage and up\r\n            \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\r\n            \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\r\n            \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\r\n            # All layers\r\n            \"all\": \".*\",\r\n        }\r\n        if layers in layer_regex.keys():\r\n            layers = layer_regex[layers]\r\n\r\n        # Data generators\r\n        train_generator = data_generator(train_dataset, self.config, shuffle=True,\r\n                                         augmentation=augmentation,\r\n                                         batch_size=self.config.BATCH_SIZE,\r\n                                         no_augmentation_sources=no_augmentation_sources)\r\n        val_generator = data_generator(val_dataset, self.config, shuffle=True,\r\n                                       batch_size=self.config.BATCH_SIZE)\r\n\r\n        # Create log_dir if it does not exist\r\n        if not os.path.exists(self.log_dir):\r\n            os.makedirs(self.log_dir)\r\n\r\n        # Callbacks\r\n        callbacks = [\r\n            keras.callbacks.TensorBoard(log_dir=self.log_dir,\r\n                                        histogram_freq=0, write_graph=True, write_images=False),\r\n            keras.callbacks.ModelCheckpoint(self.checkpoint_path,\r\n                                            verbose=0, save_weights_only=True),\r\n        ]\r\n\r\n        # Add custom callbacks to the list\r\n        if custom_callbacks:\r\n            callbacks += custom_callbacks\r\n\r\n        # Train\r\n        log(\"\\nStarting at epoch {}. LR={}\\n\".format(self.epoch, learning_rate))\r\n        log(\"Checkpoint Path: {}\".format(self.checkpoint_path))\r\n        self.set_trainable(layers)\r\n        self.compile(learning_rate, self.config.LEARNING_MOMENTUM)\r\n\r\n        # Work-around for Windows: Keras fails on Windows when using\r\n        # multiprocessing workers. See discussion here:\r\n        # https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009\r\n        if os.name is 'nt':\r\n            workers = 0\r\n        else:\r\n            workers = multiprocessing.cpu_count()\r\n\r\n        self.keras_model.fit_generator(\r\n            train_generator,\r\n            initial_epoch=self.epoch,\r\n            epochs=epochs,\r\n            steps_per_epoch=self.config.STEPS_PER_EPOCH,\r\n            callbacks=callbacks,\r\n            validation_data=val_generator,\r\n            validation_steps=self.config.VALIDATION_STEPS,\r\n            max_queue_size=100,\r\n            workers=workers,\r\n            use_multiprocessing=True,\r\n        )\r\n        self.epoch = max(self.epoch, epochs)\r\n\r\n    def mold_inputs(self, images):\r\n        \"\"\"Takes a list of images and modifies them to the format expected\r\n        as an input to the neural network.\r\n        images: List of image matrices [height,width,depth]. Images can have\r\n            different sizes.\r\n\r\n        Returns 3 Numpy matrices:\r\n        molded_images: [N, h, w, 3]. Images resized and normalized.\r\n        image_metas: [N, length of meta data]. Details about each image.\r\n        windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\r\n            original image (padding excluded).\r\n        \"\"\"\r\n        molded_images = []\r\n        image_metas = []\r\n        windows = []\r\n        for image in images:\r\n            # Resize image\r\n            # TODO: move resizing to mold_image()\r\n            molded_image, window, scale, padding, crop = utils.resize_image(\r\n                image,\r\n                min_dim=self.config.IMAGE_MIN_DIM,\r\n                min_scale=self.config.IMAGE_MIN_SCALE,\r\n                max_dim=self.config.IMAGE_MAX_DIM,\r\n                mode=self.config.IMAGE_RESIZE_MODE)\r\n            molded_image = mold_image(molded_image, self.config)\r\n            # Build image_meta\r\n            image_meta = compose_image_meta(\r\n                0, image.shape, molded_image.shape, window, scale,\r\n                np.zeros([self.config.NUM_CLASSES], dtype=np.int32))\r\n            # Append\r\n            molded_images.append(molded_image)\r\n            windows.append(window)\r\n            image_metas.append(image_meta)\r\n        # Pack into arrays\r\n        molded_images = np.stack(molded_images)\r\n        image_metas = np.stack(image_metas)\r\n        windows = np.stack(windows)\r\n        return molded_images, image_metas, windows\r\n\r\n    def unmold_detections(self, detections, mrcnn_mask, original_image_shape,\r\n                          image_shape, window):\r\n        \"\"\"Reformats the detections of one image from the format of the neural\r\n        network output to a format suitable for use in the rest of the\r\n        application.\r\n\r\n        detections: [N, (y1, x1, y2, x2, class_id, score)] in normalized coordinates\r\n        mrcnn_mask: [N, height, width, num_classes]\r\n        original_image_shape: [H, W, C] Original image shape before resizing\r\n        image_shape: [H, W, C] Shape of the image after resizing and padding\r\n        window: [y1, x1, y2, x2] Pixel coordinates of box in the image where the real\r\n                image is excluding the padding.\r\n\r\n        Returns:\r\n        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\r\n        class_ids: [N] Integer class IDs for each bounding box\r\n        scores: [N] Float probability scores of the class_id\r\n        masks: [height, width, num_instances] Instance masks\r\n        \"\"\"\r\n        # How many detections do we have?\r\n        # Detections array is padded with zeros. Find the first class_id == 0.\r\n        zero_ix = np.where(detections[:, 4] == 0)[0]\r\n        N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\r\n\r\n        # Extract boxes, class_ids, scores, and class-specific masks\r\n        boxes = detections[:N, :4]\r\n        class_ids = detections[:N, 4].astype(np.int32)\r\n        scores = detections[:N, 5]\r\n        masks = mrcnn_mask[np.arange(N), :, :, class_ids]\r\n\r\n        # Translate normalized coordinates in the resized image to pixel\r\n        # coordinates in the original image before resizing\r\n        window = utils.norm_boxes(window, image_shape[:2])\r\n        wy1, wx1, wy2, wx2 = window\r\n        shift = np.array([wy1, wx1, wy1, wx1])\r\n        wh = wy2 - wy1  # window height\r\n        ww = wx2 - wx1  # window width\r\n        scale = np.array([wh, ww, wh, ww])\r\n        # Convert boxes to normalized coordinates on the window\r\n        boxes = np.divide(boxes - shift, scale)\r\n        # Convert boxes to pixel coordinates on the original image\r\n        boxes = utils.denorm_boxes(boxes, original_image_shape[:2])\r\n\r\n        # Filter out detections with zero area. Happens in early training when\r\n        # network weights are still random\r\n        exclude_ix = np.where(\r\n            (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\r\n        if exclude_ix.shape[0] > 0:\r\n            boxes = np.delete(boxes, exclude_ix, axis=0)\r\n            class_ids = np.delete(class_ids, exclude_ix, axis=0)\r\n            scores = np.delete(scores, exclude_ix, axis=0)\r\n            masks = np.delete(masks, exclude_ix, axis=0)\r\n            N = class_ids.shape[0]\r\n\r\n        # Resize masks to original image size and set boundary threshold.\r\n        full_masks = []\r\n        for i in range(N):\r\n            # Convert neural network mask to full size mask\r\n            full_mask = utils.unmold_mask(masks[i], boxes[i], original_image_shape)\r\n            full_masks.append(full_mask)\r\n        full_masks = np.stack(full_masks, axis=-1)\\\r\n            if full_masks else np.empty(original_image_shape[:2] + (0,))\r\n\r\n        return boxes, class_ids, scores, full_masks\r\n\r\n    def detect(self, images, verbose=0):\r\n        \"\"\"Runs the detection pipeline.\r\n\r\n        images: List of images, potentially of different sizes.\r\n\r\n        Returns a list of dicts, one dict per image. The dict contains:\r\n        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\r\n        class_ids: [N] int class IDs\r\n        scores: [N] float probability scores for the class IDs\r\n        masks: [H, W, N] instance binary masks\r\n        \"\"\"\r\n        assert self.mode == \"inference\", \"Create model in inference mode.\"\r\n        assert len(\r\n            images) == self.config.BATCH_SIZE, \"len(images) must be equal to BATCH_SIZE\"\r\n\r\n        if verbose:\r\n            log(\"Processing {} images\".format(len(images)))\r\n            for image in images:\r\n                log(\"image\", image)\r\n\r\n        # Mold inputs to format expected by the neural network\r\n        molded_images, image_metas, windows = self.mold_inputs(images)\r\n\r\n        # Validate image sizes\r\n        # All images in a batch MUST be of the same size\r\n        image_shape = molded_images[0].shape\r\n        for g in molded_images[1:]:\r\n            assert g.shape == image_shape,\\\r\n                \"After resizing, all images must have the same size. Check IMAGE_RESIZE_MODE and image sizes.\"\r\n\r\n        # Anchors\r\n        anchors = self.get_anchors(image_shape)\r\n        # Duplicate across the batch dimension because Keras requires it\r\n        # TODO: can this be optimized to avoid duplicating the anchors?\r\n        anchors = np.broadcast_to(anchors, (self.config.BATCH_SIZE,) + anchors.shape)\r\n\r\n        if verbose:\r\n            log(\"molded_images\", molded_images)\r\n            log(\"image_metas\", image_metas)\r\n            log(\"anchors\", anchors)\r\n        # Run object detection\r\n        detections, _, _, mrcnn_mask, _, _, _ =\\\r\n            self.keras_model.predict([molded_images, image_metas, anchors], verbose=0)\r\n        # Process detections\r\n        results = []\r\n        for i, image in enumerate(images):\r\n            final_rois, final_class_ids, final_scores, final_masks =\\\r\n                self.unmold_detections(detections[i], mrcnn_mask[i],\r\n                                       image.shape, molded_images[i].shape,\r\n                                       windows[i])\r\n            results.append({\r\n                \"rois\": final_rois,\r\n                \"class_ids\": final_class_ids,\r\n                \"scores\": final_scores,\r\n                \"masks\": final_masks,\r\n            })\r\n        return results\r\n\r\n    def detect_molded(self, molded_images, image_metas, verbose=0):\r\n        \"\"\"Runs the detection pipeline, but expect inputs that are\r\n        molded already. Used mostly for debugging and inspecting\r\n        the model.\r\n\r\n        molded_images: List of images loaded using load_image_gt()\r\n        image_metas: image meta data, also returned by load_image_gt()\r\n\r\n        Returns a list of dicts, one dict per image. The dict contains:\r\n        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\r\n        class_ids: [N] int class IDs\r\n        scores: [N] float probability scores for the class IDs\r\n        masks: [H, W, N] instance binary masks\r\n        \"\"\"\r\n        assert self.mode == \"inference\", \"Create model in inference mode.\"\r\n        assert len(molded_images) == self.config.BATCH_SIZE,\\\r\n            \"Number of images must be equal to BATCH_SIZE\"\r\n\r\n        if verbose:\r\n            log(\"Processing {} images\".format(len(molded_images)))\r\n            for image in molded_images:\r\n                log(\"image\", image)\r\n\r\n        # Validate image sizes\r\n        # All images in a batch MUST be of the same size\r\n        image_shape = molded_images[0].shape\r\n        for g in molded_images[1:]:\r\n            assert g.shape == image_shape, \"Images must have the same size\"\r\n\r\n        # Anchors\r\n        anchors = self.get_anchors(image_shape)\r\n        # Duplicate across the batch dimension because Keras requires it\r\n        # TODO: can this be optimized to avoid duplicating the anchors?\r\n        anchors = np.broadcast_to(anchors, (self.config.BATCH_SIZE,) + anchors.shape)\r\n\r\n        if verbose:\r\n            log(\"molded_images\", molded_images)\r\n            log(\"image_metas\", image_metas)\r\n            log(\"anchors\", anchors)\r\n        # Run object detection\r\n        detections, _, _, mrcnn_mask, _, _, _ =\\\r\n            self.keras_model.predict([molded_images, image_metas, anchors], verbose=0)\r\n        # Process detections\r\n        results = []\r\n        for i, image in enumerate(molded_images):\r\n            window = [0, 0, image.shape[0], image.shape[1]]\r\n            final_rois, final_class_ids, final_scores, final_masks =\\\r\n                self.unmold_detections(detections[i], mrcnn_mask[i],\r\n                                       image.shape, molded_images[i].shape,\r\n                                       window)\r\n            results.append({\r\n                \"rois\": final_rois,\r\n                \"class_ids\": final_class_ids,\r\n                \"scores\": final_scores,\r\n                \"masks\": final_masks,\r\n            })\r\n        return results\r\n\r\n    def get_anchors(self, image_shape):\r\n        \"\"\"Returns anchor pyramid for the given image size.\"\"\"\r\n        backbone_shapes = compute_backbone_shapes(self.config, image_shape)\r\n        # Cache anchors and reuse if image shape is the same\r\n        if not hasattr(self, \"_anchor_cache\"):\r\n            self._anchor_cache = {}\r\n        if not tuple(image_shape) in self._anchor_cache:\r\n            # Generate Anchors\r\n            a = utils.generate_pyramid_anchors(\r\n                self.config.RPN_ANCHOR_SCALES,\r\n                self.config.RPN_ANCHOR_RATIOS,\r\n                backbone_shapes,\r\n                self.config.BACKBONE_STRIDES,\r\n                self.config.RPN_ANCHOR_STRIDE)\r\n            # Keep a copy of the latest anchors in pixel coordinates because\r\n            # it's used in inspect_model notebooks.\r\n            # TODO: Remove this after the notebook are refactored to not use it\r\n            self.anchors = a\r\n            # Normalize coordinates\r\n            self._anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2])\r\n        return self._anchor_cache[tuple(image_shape)]\r\n\r\n    def ancestor(self, tensor, name, checked=None):\r\n        \"\"\"Finds the ancestor of a TF tensor in the computation graph.\r\n        tensor: TensorFlow symbolic tensor.\r\n        name: Name of ancestor tensor to find\r\n        checked: For internal use. A list of tensors that were already\r\n                 searched to avoid loops in traversing the graph.\r\n        \"\"\"\r\n        checked = checked if checked is not None else []\r\n        # Put a limit on how deep we go to avoid very long loops\r\n        if len(checked) > 500:\r\n            return None\r\n        # Convert name to a regex and allow matching a number prefix\r\n        # because Keras adds them automatically\r\n        if isinstance(name, str):\r\n            name = re.compile(name.replace(\"/\", r\"(\\_\\d+)*/\"))\r\n\r\n        parents = tensor.op.inputs\r\n        for p in parents:\r\n            if p in checked:\r\n                continue\r\n            if bool(re.fullmatch(name, p.name)):\r\n                return p\r\n            checked.append(p)\r\n            a = self.ancestor(p, name, checked)\r\n            if a is not None:\r\n                return a\r\n        return None\r\n\r\n    def find_trainable_layer(self, layer):\r\n        \"\"\"If a layer is encapsulated by another layer, this function\r\n        digs through the encapsulation and returns the layer that holds\r\n        the weights.\r\n        \"\"\"\r\n        if layer.__class__.__name__ == 'TimeDistributed':\r\n            return self.find_trainable_layer(layer.layer)\r\n        return layer\r\n\r\n    def get_trainable_layers(self):\r\n        \"\"\"Returns a list of layers that have weights.\"\"\"\r\n        layers = []\r\n        # Loop through all layers\r\n        for l in self.keras_model.layers:\r\n            # If layer is a wrapper, find inner trainable layer\r\n            l = self.find_trainable_layer(l)\r\n            # Include layer if it has weights\r\n            if l.get_weights():\r\n                layers.append(l)\r\n        return layers\r\n\r\n    def run_graph(self, images, outputs, image_metas=None):\r\n        \"\"\"Runs a sub-set of the computation graph that computes the given\r\n        outputs.\r\n\r\n        image_metas: If provided, the images are assumed to be already\r\n            molded (i.e. resized, padded, and normalized)\r\n\r\n        outputs: List of tuples (name, tensor) to compute. The tensors are\r\n            symbolic TensorFlow tensors and the names are for easy tracking.\r\n\r\n        Returns an ordered dict of results. Keys are the names received in the\r\n        input and values are Numpy arrays.\r\n        \"\"\"\r\n        model = self.keras_model\r\n\r\n        # Organize desired outputs into an ordered dict\r\n        outputs = OrderedDict(outputs)\r\n        for o in outputs.values():\r\n            assert o is not None\r\n\r\n        # Build a Keras function to run parts of the computation graph\r\n        inputs = model.inputs\r\n        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\r\n            inputs += [K.learning_phase()]\r\n        kf = K.function(model.inputs, list(outputs.values()))\r\n\r\n        # Prepare inputs\r\n        if image_metas is None:\r\n            molded_images, image_metas, _ = self.mold_inputs(images)\r\n        else:\r\n            molded_images = images\r\n        image_shape = molded_images[0].shape\r\n        # Anchors\r\n        anchors = self.get_anchors(image_shape)\r\n        # Duplicate across the batch dimension because Keras requires it\r\n        # TODO: can this be optimized to avoid duplicating the anchors?\r\n        anchors = np.broadcast_to(anchors, (self.config.BATCH_SIZE,) + anchors.shape)\r\n        model_in = [molded_images, image_metas, anchors]\r\n\r\n        # Run inference\r\n        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\r\n            model_in.append(0.)\r\n        outputs_np = kf(model_in)\r\n\r\n        # Pack the generated Numpy arrays into a a dict and log the results.\r\n        outputs_np = OrderedDict([(k, v)\r\n                                  for k, v in zip(outputs.keys(), outputs_np)])\r\n        for k, v in outputs_np.items():\r\n            log(k, v)\r\n        return outputs_np\r\n\r\n\r\n############################################################\r\n#  Data Formatting\r\n############################################################\r\n\r\ndef compose_image_meta(image_id, original_image_shape, image_shape,\r\n                       window, scale, active_class_ids):\r\n    \"\"\"Takes attributes of an image and puts them in one 1D array.\r\n\r\n    image_id: An int ID of the image. Useful for debugging.\r\n    original_image_shape: [H, W, C] before resizing or padding.\r\n    image_shape: [H, W, C] after resizing and padding\r\n    window: (y1, x1, y2, x2) in pixels. The area of the image where the real\r\n            image is (excluding the padding)\r\n    scale: The scaling factor applied to the original image (float32)\r\n    active_class_ids: List of class_ids available in the dataset from which\r\n        the image came. Useful if training on images from multiple datasets\r\n        where not all classes are present in all datasets.\r\n    \"\"\"\r\n    meta = np.array(\r\n        [image_id] +                  # size=1\r\n        list(original_image_shape) +  # size=3\r\n        list(image_shape) +           # size=3\r\n        list(window) +                # size=4 (y1, x1, y2, x2) in image cooredinates\r\n        [scale] +                     # size=1\r\n        list(active_class_ids)        # size=num_classes\r\n    )\r\n    return meta\r\n\r\n\r\ndef parse_image_meta(meta):\r\n    \"\"\"Parses an array that contains image attributes to its components.\r\n    See compose_image_meta() for more details.\r\n\r\n    meta: [batch, meta length] where meta length depends on NUM_CLASSES\r\n\r\n    Returns a dict of the parsed values.\r\n    \"\"\"\r\n    image_id = meta[:, 0]\r\n    original_image_shape = meta[:, 1:4]\r\n    image_shape = meta[:, 4:7]\r\n    window = meta[:, 7:11]  # (y1, x1, y2, x2) window of image in in pixels\r\n    scale = meta[:, 11]\r\n    active_class_ids = meta[:, 12:]\r\n    return {\r\n        \"image_id\": image_id.astype(np.int32),\r\n        \"original_image_shape\": original_image_shape.astype(np.int32),\r\n        \"image_shape\": image_shape.astype(np.int32),\r\n        \"window\": window.astype(np.int32),\r\n        \"scale\": scale.astype(np.float32),\r\n        \"active_class_ids\": active_class_ids.astype(np.int32),\r\n    }\r\n\r\n\r\ndef parse_image_meta_graph(meta):\r\n    \"\"\"Parses a tensor that contains image attributes to its components.\r\n    See compose_image_meta() for more details.\r\n\r\n    meta: [batch, meta length] where meta length depends on NUM_CLASSES\r\n\r\n    Returns a dict of the parsed tensors.\r\n    \"\"\"\r\n    image_id = meta[:, 0]\r\n    original_image_shape = meta[:, 1:4]\r\n    image_shape = meta[:, 4:7]\r\n    window = meta[:, 7:11]  # (y1, x1, y2, x2) window of image in in pixels\r\n    scale = meta[:, 11]\r\n    active_class_ids = meta[:, 12:]\r\n    return {\r\n        \"image_id\": image_id,\r\n        \"original_image_shape\": original_image_shape,\r\n        \"image_shape\": image_shape,\r\n        \"window\": window,\r\n        \"scale\": scale,\r\n        \"active_class_ids\": active_class_ids,\r\n    }\r\n\r\n\r\ndef mold_image(images, config):\r\n    \"\"\"Expects an RGB image (or array of images) and subtracts\r\n    the mean pixel and converts it to float. Expects image\r\n    colors in RGB order.\r\n    \"\"\"\r\n    return images.astype(np.float32) - config.MEAN_PIXEL\r\n\r\n\r\ndef unmold_image(normalized_images, config):\r\n    \"\"\"Takes a image normalized with mold() and returns the original.\"\"\"\r\n    return (normalized_images + config.MEAN_PIXEL).astype(np.uint8)\r\n\r\n\r\n############################################################\r\n#  Miscellenous Graph Functions\r\n############################################################\r\n\r\ndef trim_zeros_graph(boxes, name='trim_zeros'):\r\n    \"\"\"Often boxes are represented with matrices of shape [N, 4] and\r\n    are padded with zeros. This removes zero boxes.\r\n\r\n    boxes: [N, 4] matrix of boxes.\r\n    non_zeros: [N] a 1D boolean mask identifying the rows to keep\r\n    \"\"\"\r\n    non_zeros = tf.cast(tf.reduce_sum(tf.abs(boxes), axis=1), tf.bool)\r\n    boxes = tf.boolean_mask(boxes, non_zeros, name=name)\r\n    return boxes, non_zeros\r\n\r\n\r\ndef batch_pack_graph(x, counts, num_rows):\r\n    \"\"\"Picks different number of values from each row\r\n    in x depending on the values in counts.\r\n    \"\"\"\r\n    outputs = []\r\n    for i in range(num_rows):\r\n        outputs.append(x[i, :counts[i]])\r\n    return tf.concat(outputs, axis=0)\r\n\r\n\r\ndef norm_boxes_graph(boxes, shape):\r\n    \"\"\"Converts boxes from pixel coordinates to normalized coordinates.\r\n    boxes: [..., (y1, x1, y2, x2)] in pixel coordinates\r\n    shape: [..., (height, width)] in pixels\r\n\r\n    Note: In pixel coordinates (y2, x2) is outside the box. But in normalized\r\n    coordinates it's inside the box.\r\n\r\n    Returns:\r\n        [..., (y1, x1, y2, x2)] in normalized coordinates\r\n    \"\"\"\r\n    h, w = tf.split(tf.cast(shape, tf.float32), 2)\r\n    scale = tf.concat([h, w, h, w], axis=-1) - tf.constant(1.0)\r\n    shift = tf.constant([0., 0., 1., 1.])\r\n    return tf.divide(boxes - shift, scale)\r\n\r\n\r\ndef denorm_boxes_graph(boxes, shape):\r\n    \"\"\"Converts boxes from normalized coordinates to pixel coordinates.\r\n    boxes: [..., (y1, x1, y2, x2)] in normalized coordinates\r\n    shape: [..., (height, width)] in pixels\r\n\r\n    Note: In pixel coordinates (y2, x2) is outside the box. But in normalized\r\n    coordinates it's inside the box.\r\n\r\n    Returns:\r\n        [..., (y1, x1, y2, x2)] in pixel coordinates\r\n    \"\"\"\r\n    h, w = tf.split(tf.cast(shape, tf.float32), 2)\r\n    scale = tf.concat([h, w, h, w], axis=-1) - tf.constant(1.0)\r\n    shift = tf.constant([0., 0., 1., 1.])\r\n    return tf.cast(tf.round(tf.multiply(boxes, scale) + shift), tf.int32)\r\n```\r\n", "@ibrahimLearning, Thanks for the complete code. Will it be possible to share the minimal code snippet to reproduce the reported issue. Thanks!", "@ibrahimLearning, Any update on issue!", "@gadagashwini \r\nThanks for comment.\r\n\r\n>  Any update on issue!\r\n\r\nNot yet. I will share minimal code snippet as soon as possible.", "@ibrahimLearning, Did you get a chance to reproduce the minimal code snippet. Thanks!", "@ibrahimLearning, Closing the issue now. Please feel free to comment when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi , @gadagashwini  the problem is \r\nif we use keras - 2.2.4 \r\nthe output produced by timedistributed layer is \r\nx - inputshape --> (1,None,7,7,256)\r\nx = keras.Timedistributed(conv2(1024 , (7,7))) (x)\r\nx - output shape --> (None , 200, 1,1,1024)\r\n\r\n_______________________________________\r\nfor tensorflow 2.0 \r\nthe output produced by timedistributed layer is \r\nx - inputshape --> (1,None,7,7,256)\r\nx = tf.keras.Timedistributed(conv2(1024 , (7,7))) (x)\r\nx - output shape --> (None , None, 1,1,1024)\r\n\r\n\r\n\r\nso basically the output shape is wrong in new tf 2.0 /timesteps calculated wrongly\r\n\r\nany ideas how to solvethis\r\nthanks", "> Hi , @gadagashwini the problem is if we use keras - 2.2.4 the output produced by timedistributed layer is x - inputshape --> (1,None,7,7,256) x = keras.Timedistributed(conv2(1024 , (7,7))) (x) x - output shape --> (None , 200, 1,1,1024)\r\n> \r\n> for tensorflow 2.0 the output produced by timedistributed layer is x - inputshape --> (1,None,7,7,256) x = tf.keras.Timedistributed(conv2(1024 , (7,7))) (x) x - output shape --> (None , None, 1,1,1024)\r\n> \r\n> so basically the output shape is wrong in new tf 2.0 /timesteps calculated wrongly\r\n> \r\n> any ideas how to solvethis thanks\r\n\r\nHi @kartik4949, do you find any solution? \r\nFor me, I tried on TF 2.6, the explanation in its document about the \"TimeDistributed\" function is logic. I'm so curious how the original Keras can produce \"200\" in (None, 200, 1, 1, 1024) while the input has no value as 200.", "> > Hi , @gadagashwini the problem is if we use keras - 2.2.4 the output produced by timedistributed layer is x - inputshape --> (1,None,7,7,256) x = keras.Timedistributed(conv2(1024 , (7,7))) (x) x - output shape --> (None , 200, 1,1,1024)\r\n> > for tensorflow 2.0 the output produced by timedistributed layer is x - inputshape --> (1,None,7,7,256) x = tf.keras.Timedistributed(conv2(1024 , (7,7))) (x) x - output shape --> (None , None, 1,1,1024)\r\n> > so basically the output shape is wrong in new tf 2.0 /timesteps calculated wrongly\r\n> > any ideas how to solvethis thanks\r\n> \r\n> Hi @kartik4949, do you find any solution? For me, I tried on TF 2.6, the explanation in its document about the \"TimeDistributed\" function is logic. I'm so curious how the original Keras can produce \"200\" in (None, 200, 1, 1, 1024) while the input has no value as 200.\r\n\r\nI found that this issue only happens in TF 2.6, when I downgrade to TF2.4 and 2.5, it works as expected."]}, {"number": 34635, "title": "optimizer.apply_gradients() logs warnings using Tensor.name which is not supported by eager execution", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: RTX 2070 super 8gb\r\n\r\n**Describe the current behavior**\r\nWhen using a gradient tape in eager mode, if the gradient computation fails and returns `None`, the `apply_gradients()` function will attempt to log a warning using `Tensor.name` which isn't supported in eager execution. The exact line can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1043). This is a breaking issue because it is simply a logged warning, and the code should continue to execute; however in eager mode it raises an `AttributeError` due to `Tensor.name`. A similar issue can be found above on [line 1039](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1039) however that one is less serious, as the code would terminate due to the `ValueError` anyway.\r\n\r\n**Describe the expected behavior**\r\nA warning is logged and the code continues to execute.\r\n\r\n**Code to reproduce the issue**\r\nThere is a workaround for RTX GPUs at the top per the comment in #24828 \r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nconv1_filters = 32\r\nconv1_window = 3\r\ninput_dims = 50\r\nnum_classes = 10\r\n\r\nrandom_normal = tf.initializers.RandomNormal()\r\n\r\n# Magic fix for RTX GPUs\r\n# gpus = tf.config.experimental.list_physical_devices('GPU')\r\n# for gpu in gpus:\r\n#   tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\nweights = {\r\n  'wc1': tf.Variable(random_normal([conv1_window, input_dims, conv1_filters])),\r\n  'out': tf.Variable(random_normal([conv1_filters, num_classes]))\r\n}\r\n\r\nbiases = {\r\n  # This line is the one that's wrong. Here I forgot to wrap the tf.zeros in a tf.Variable which is how I discovered the issue.\r\n  'bc1': tf.zeros(conv1_filters),\r\n  'out': tf.Variable(tf.zeros(num_classes))\r\n}\r\n\r\ndef conv1d(x, W, b, stride=1):\r\n  \"\"\"\r\n  Conv1D wrapper, with bias and relu activation.\r\n  \"\"\"\r\n  x = tf.nn.conv1d(x, W, stride=stride, padding='SAME')\r\n  x = tf.nn.bias_add(x, b)\r\n  return tf.nn.relu(x)\r\n\r\ndef model(inputs):\r\n  x = inputs\r\n  x = conv1d(x, weights['wc1'], biases['bc1'])\r\n  x = tf.add(tf.matmul(x, weights['out']), biases['out'])\r\n  return tf.nn.softmax(x)\r\n\r\ndef cross_entropy(y_pred, y_true):\r\n  y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\r\n  return -tf.reduce_sum(y_true * tf.math.log(y_pred)) / tf.reduce_sum(y_true)\r\n\r\ndef train_one_batch(optimizer, minibatch_x, minibatch_y):\r\n  with tf.GradientTape() as g:\r\n    pred = model(minibatch_x)\r\n    loss = cross_entropy(pred, minibatch_y)\r\n  trainable_variables = list(weights.values()) + list(biases.values())\r\n  gradients = g.gradient(loss, trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n\r\nbatch_size = 2\r\nsequence_len = 4\r\nx = tf.zeros([batch_size, sequence_len, input_dims], dtype=tf.float32)\r\ny = tf.ones([batch_size, sequence_len], dtype=tf.int64)\r\ny_onehot = tf.one_hot(y, depth=num_classes)\r\noptimizer = tf.optimizers.Adam()\r\ntrain_one_batch(optimizer, x, y_onehot)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"code/tf_testcase.py\", line 58, in <module>\r\n    train_one_batch(optimizer, x, y_onehot)\r\n  File \"code/tf_testcase.py\", line 50, in train_one_batch\r\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n  File \"/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 427, in apply_gradients\r\n    grads_and_vars = _filter_grads(grads_and_vars)\r\n  File \"/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 1029, in _filter_grads\r\n    ([v.name for v in vars_with_empty_grads]))\r\n  File \"/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 1029, in <listcomp>\r\n    ([v.name for v in vars_with_empty_grads]))\r\n  File \"/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1090, in name\r\n    \"Tensor.name is meaningless when eager execution is enabled.\")\r\nAttributeError: Tensor.name is meaningless when eager execution is enabled.\r\n```\r\n ", "comments": ["I have tried on colab with TF version 2.0 and was able to reproduce the issue.If i disable eager execution (`tf.compat.v1.disable_eager_execution()` and run the code i am not seeing any issue.I have tried using TF  2.1.0dev20191127 version and i am seeing different error message.`InternalError: Blas xGEMMBatched launch failed : a.shape=[2,4,32], b.shape=[1,32,10], m=4, n=10, k=32, batch_size=2 [Op:BatchMatMulV2]`. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9d44e8521891131fb3a3d4a179d3cf01/untitled413.ipynb). Thanks!", "@ikhatri,\r\nCan you please check Stack Overflow [Link1](https://stackoverflow.com/questions/55552538/tensorflow-2-0-attributeerror-tensor-name-is-meaningless-when-eager-execution?rq=1)  and [Link2](https://stackoverflow.com/questions/52340101/tensor-name-is-meaningless-in-eager-execution) and let us know if it resolves your issue. Thanks!", "@rmothukuru The code sample above is intentionally incorrect in order to demonstrate a bug with the functionality of the `optimizer_v2` under eager execution. The issue can be \"resolved\" in the code sample by wrapping the call to `tf.zeros` in a `tf.Variable` which will allow the auto-grad to differentiate the full network and provide gradients for every variable.\r\n\r\nHowever the intention with the above code snippet is to demonstrate the bug where the `apply_gradients()` function calls `_filter_grads()` which logs a warning using `Tensor.name`. The intended functionality of this function is to filter out variables that have un-computable gradients and warn the user (in case they have made a mistake, as I did above). The issue is that under eager execution, this warning message fails to print due to eager tensors not having a name. This behavior should be fixed to be consistent under both eager execution and non-eager execution so that the warning message does not crash the users code and is instead logged.\r\n\r\nI'm not sure what the best way to go about this would be, as I'm unsure what the intended design pattern for code under eager execution should be. Having `if eager_execution:` blocks everywhere seems like the wrong way to do it, however it would fix the issue.\r\n\r\nThe below code snippet is from [line 1040 of optimizer_v2.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1040) and I've modified it to fix the issue, though again I'm unsure if this is the best way to do so.\r\n``` python\r\nif vars_with_empty_grads:\r\n\tif context.executing_eagerly():\r\n\t\tlogging.warning(\r\n\t\t\"%i of your variables had that gradients could not be computed\",\r\n\t\tlen(vars_with_empty_grads))\r\n\telse:\r\n\t\tlogging.warning(\r\n\t\t\t(\"Gradients do not exist for variables %s when minimizing the loss.\"),\r\n\t\t\t([v.name for v in vars_with_empty_grads]))\r\n```", "This is not really an optimizer issue when the \"variable\" is actually a \"tensor\".", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34635\">No</a>\n", "I have the same issue, but only when I try to train on the TPU, it works fine with GPU\r\n\r\nThis is the full error message:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-24-1f67c891bad4> in <module>()\r\n      5         validation_steps=val_steps,\r\n      6         validation_freq=1,\r\n----> 7         callbacks=callbacks)\r\n      8 \r\n      9 # else:\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    625       # This is the first call of __call__, so we have to initialize.\r\n    626       initializers = []\r\n--> 627       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    628     finally:\r\n    629       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    504     self._concrete_stateful_fn = (\r\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 506             *args, **kwds))\r\n    507 \r\n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2444       args, kwargs = None, None\r\n   2445     with self._lock:\r\n-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2447     return graph_function\r\n   2448 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2775 \r\n   2776       self._function_cache.missed.add(call_context_key)\r\n-> 2777       graph_function = self._create_graph_function(args, kwargs)\r\n   2778       self._function_cache.primary[cache_key] = graph_function\r\n   2779       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665             arg_names=arg_names,\r\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2667             capture_by_value=self._capture_by_value),\r\n   2668         self._function_attributes,\r\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    440         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    443 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nAttributeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    <ipython-input-7-9641c6148b59>:21 train_step  *\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py:149 apply_gradients  *\r\n        return super().apply_gradients(grads_and_vars, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:472 apply_gradients  **\r\n        grads_and_vars = _filter_grads(grads_and_vars)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 _filter_grads\r\n        ([v.name for v in vars_with_empty_grads]))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 <listcomp>\r\n        ([v.name for v in vars_with_empty_grads]))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1123 name\r\n        \"Tensor.name is meaningless when eager execution is enabled.\")\r\n\r\n    AttributeError: Tensor.name is meaningless when eager execution is enabled.\r\n```", "Yeah this issue has been closed prematurely in my opinion. I guess a better way to explain it would be like so:\r\n\r\nThe user (me!) makes an error by switching a \"variable\" and a \"tensor\". Just like @tanzhenyu said, it's not an optimizer issue because you the user has made the mistake. TensorFlow tries to inform the user of their error. It does so by printing an error message stating that the \"Gradients do not exist for xyz\". This is a perfectly adequate error message because \"variables\" don't have gradients and if this message was printed the user could be expected to fix their code.\r\n\r\nThe actual problem here though is that the error message itself never prints because in eager mode tensors don't have names. So if you are in eager mode and you make this mistake you'll be left with a cryptic error message about \"Tensor.name is meaningless\" that has nothing to do with your actual mistake. The bug here is with the error message not the actual optimizer and I still think it should be fixed, but that's not my call \ud83e\udd37 .\r\n\r\nEDIT: A quick clarification that I should probably make is that this error message could be printed for a variety of reasons that aren't the user switching a \"tensor\" and a \"variable\" like I did in my original post. The fact that the error messaging is broken is the issue regardless of what code-path the user takes to get there. @santosh-gupta it looks like you got here via different code path (did you have eager execution enabled on both the GPU and TPU?)", "> Yeah this issue has been closed prematurely in my opinion. I guess a better way to explain it would be like so:\r\n> \r\n> The user (me!) makes an error by switching a \"variable\" and a \"tensor\". Just like @tanzhenyu said, it's not an optimizer issue because you the user has made the mistake. TensorFlow tries to inform the user of their error. It does so by printing an error message stating that the \"Gradients do not exist for xyz\". This is a perfectly adequate error message because \"variables\" don't have gradients and if this message was printed the user could be expected to fix their code.\r\n> \r\n> The actual problem here though is that the error message itself never prints because in eager mode tensors don't have names. So if you are in eager mode and you make this mistake you'll be left with a cryptic error message about \"Tensor.name is meaningless\" that has nothing to do with your actual mistake. The bug here is with the error message not the actual optimizer and I still think it should be fixed, but that's not my call \ud83e\udd37 .\r\n> \r\n> EDIT: A quick clarification that I should probably make is that this error message could be printed for a variety of reasons that aren't the user switching a \"tensor\" and a \"variable\" like I did in my original post. The fact that the error messaging is broken is the issue regardless of what code-path the user takes to get there. @Santosh-Gupta it looks like you got here via different code path (did you have eager execution enabled on both the GPU and TPU?)\r\n\r\nI am using TF 2.0 where eager is on by default, and is discouraged not to turn it off. \r\n\r\nI made a minimal example of my code for others to debug, but in my minimal example, I am getting a completely different error message. \r\n\r\n>NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function or strategy.run is called inside a tf.function if eager behavior is enabled.\r\n\r\nEven though I believe I removed all pytorch functions in the code, and I am only using tensorflow operations. \r\n\r\nI detail the issue in this stackoverflow post\r\n\r\nhttps://stackoverflow.com/questions/62650379/error-when-running-on-tpu-notimplementederror-tpustrategy-runfn-does-n"]}, {"number": 34634, "title": "Why model.Summary() shows all layers in sub model.", "body": "In old version(I forget which one). Sub model just shows model's name but not all layers.\r\nBut now,it shows all layers.\r\n\r\nMy versoin: TF2.0(GPU)\r\n\r\ndemo code:\r\n```\r\nclass Swish(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(Swish, self).__init__()\r\n        self.weight = self.add_weight(initializer='uniform',trainable=True)\r\n\r\n    def __call__(self, inputs):\r\n        return inputs+tf.sigmoid(self.weight*inputs)\r\n```", "comments": ["the full code :[https://github.com/tensorflow/tensorflow/issues/34637](https://github.com/tensorflow/tensorflow/issues/34637)", "@HLSS-Hen, Tried replicating the issue but got error at `from Models import MCN`. Looks like file or class method is missing. Please provide us more information to reproduce the issue. Thanks!", "> @HLSS-Hen, Tried replicating the issue but got error at `from Models import MCN`. Looks like file or class method is missing. Please provide us more information to reproduce the issue. Thanks!\r\n\r\nNo, there is. \r\nMCN.py is the file you miss.", "@HLSS-Hen, MCN.py file doesn't have Models class method. ", "@gadagashwini Models is just a file.", "@HLSS-Hen, Just to verify, MCN.py and Models are they same file or different. ", "@gadagashwini \r\nyou can use following code(use any database with shape [?,256,256,3]):\r\n```\r\nimport os\r\nos.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'] = '1'\r\nimport math\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass Swish(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(Swish, self).__init__()\r\n        self.weight = self.add_weight(initializer='uniform',trainable=True)\r\n\r\n    def __call__(self, inputs):\r\n        return inputs+tf.sigmoid(self.weight*inputs)\r\n\r\n\r\nclass Conv(keras.Model):\r\n    def __init__(self,filters,kernel_size=1,strides=1,padding='valid'):\r\n        super(Conv, self).__init__()\r\n        self.conv = keras.layers.Conv2D(filters,kernel_size,strides,padding)\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        return self.ac(self.bn(self.conv(inputs)))\r\n\r\n\r\nclass SEBlock(keras.Model):\r\n    def __init__(self, filters):\r\n        super(SEBlock, self).__init__()\r\n        self.conv0 = keras.layers.Conv2D(filters//4,1,1)\r\n        self.drop = keras.layers.Dropout(0.25)\r\n        self.conv1 = keras.layers.Conv2D(filters,1,1)\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        x = self.conv1(self.drop(self.conv0(tf.reduce_mean(inputs,[1,2],keepdims=True))))\r\n        return self.ac(self.bn(tf.sigmoid(x)*inputs))\r\n\r\n\r\nclass ResBlock(keras.Model):\r\n    def __init__(self, filters):\r\n        super(ResBlock, self).__init__()\r\n        self.conv0 = keras.layers.Conv2D(filters//4,1,1)\r\n        self.drop = keras.layers.Dropout(0.25)\r\n        self.conv1 = keras.layers.Conv2D(filters,3,1,'same')\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        x = self.conv1(self.drop(self.conv0(inputs)))\r\n        return self.ac(self.bn(inputs+x))\r\n\r\ndef mcn_520(width, growth,input_shape=[256,256,3]):\r\n    fs = int(width*growth)\r\n    inputs=keras.layers.Input(input_shape)\r\n    x=keras.layers.Conv2D(fs,8,2)(inputs)\r\n    x=keras.layers.MaxPool2D(2)(x)\r\n    x1=Conv(fs//width)(SEBlock(fs)(x))\r\n    x2=Conv(fs//width)(ResBlock(fs)(x))\r\n    for i, depth in enumerate([2, 3, 5, 4]):\r\n        for _ in range(int(6*depth)):\r\n            fs+=int(math.sqrt(fs*width))\r\n            t=keras.layers.Concatenate()([x,x1,x2])\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            t=Conv(fs//width, 1, 1)(t)\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            x1=SEBlock(fs//width)(t)\r\n            x2=ResBlock(fs//width)(t)\r\n            t=keras.layers.Concatenate()([t,x1,x2])\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            t=Conv(growth,1,1)(t)\r\n            x=keras.layers.Concatenate()([x,t])\r\n        if i != 3:\r\n            fs //= 2\r\n            x=keras.layers.MaxPool2D(2)(Conv(fs)(x))\r\n            x1=keras.layers.MaxPool2D(2)(Conv(fs//width)(x1))\r\n            x2=keras.layers.MaxPool2D(2)(Conv(fs//width)(x2))\r\n    x=keras.layers.GlobalMaxPool2D()(x)\r\n    x=keras.layers.Dropout(0.25)(x)\r\n    outputs=keras.layers.Dense(1000,activation='softmax')(x)\r\n    return keras.Model(inputs=inputs,outputs=outputs,name='MCN520')\r\n\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\nlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n\r\nstarategy=tf.distribute.MirroredStrategy()\r\nwith starategy.scope():\r\n    model=MCN.mcn_520(2,24)\r\n    model.summary()\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        metrics=[tf.keras.metrics.TopKCategoricalAccuracy(1,'top1'),tf.keras.metrics.TopKCategoricalAccuracy(5,'top5')]\r\n        )\r\n    fit_ds,val_ds=ds(BATCH_SIZE)\r\n    model.fit(\r\n        fit_ds, # please use your database\r\n        epochs=1000000,\r\n        steps_per_epoch=BATCHS_PER_APLY_GRADIENTS*200,\r\n    )\r\n```", "@HLSS-Hen, Please find the attached the [gist](https://colab.sandbox.google.com/gist/gadagashwini/68712f12f85f31da6a6fce73cde257e4/untitled295.ipynb) and help us in reproducing the issue. Thanks!", "@gadagashwini  , sorry I took some time to  visit the page. And my google account have some wrong.\r\n\r\nuse full code here:\r\n\r\n```\r\nimport os\r\nos.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'] = '1'\r\nimport math\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass Swish(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(Swish, self).__init__()\r\n        self.weight = self.add_weight(initializer='uniform',trainable=True)\r\n\r\n    def __call__(self, inputs):\r\n        return inputs+tf.sigmoid(self.weight*inputs)\r\n\r\n\r\nclass Conv(keras.Model):\r\n    def __init__(self,filters,kernel_size=1,strides=1,padding='valid'):\r\n        super(Conv, self).__init__()\r\n        self.conv = keras.layers.Conv2D(filters,kernel_size,strides,padding)\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        return self.ac(self.bn(self.conv(inputs)))\r\n\r\n\r\nclass SEBlock(keras.Model):\r\n    def __init__(self, filters):\r\n        super(SEBlock, self).__init__()\r\n        self.conv0 = keras.layers.Conv2D(filters//4,1,1)\r\n        self.drop = keras.layers.Dropout(0.25)\r\n        self.conv1 = keras.layers.Conv2D(filters,1,1)\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        x = self.conv1(self.drop(self.conv0(tf.reduce_mean(inputs,[1,2],keepdims=True))))\r\n        return self.ac(self.bn(tf.sigmoid(x)*inputs))\r\n\r\n\r\nclass ResBlock(keras.Model):\r\n    def __init__(self, filters):\r\n        super(ResBlock, self).__init__()\r\n        self.conv0 = keras.layers.Conv2D(filters//4,1,1)\r\n        self.drop = keras.layers.Dropout(0.25)\r\n        self.conv1 = keras.layers.Conv2D(filters,3,1,'same')\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        x = self.conv1(self.drop(self.conv0(inputs)))\r\n        return self.ac(self.bn(inputs+x))\r\n\r\ndef mcn_520(width, growth,input_shape=[256,256,3]):\r\n    fs = int(width*growth)\r\n    inputs=keras.layers.Input(input_shape)\r\n    x=keras.layers.Conv2D(fs,8,2)(inputs)\r\n    x=keras.layers.MaxPool2D(2)(x)\r\n    x1=Conv(fs//width)(SEBlock(fs)(x))\r\n    x2=Conv(fs//width)(ResBlock(fs)(x))\r\n    for i, depth in enumerate([2, 3, 5, 4]):\r\n        for _ in range(int(6*depth)):\r\n            fs+=int(math.sqrt(fs*width))\r\n            t=keras.layers.Concatenate()([x,x1,x2])\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            t=Conv(fs//width, 1, 1)(t)\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            x1=SEBlock(fs//width)(t)\r\n            x2=ResBlock(fs//width)(t)\r\n            t=keras.layers.Concatenate()([t,x1,x2])\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            t=Conv(growth,1,1)(t)\r\n            x=keras.layers.Concatenate()([x,t])\r\n        if i != 3:\r\n            fs //= 2\r\n            x=keras.layers.MaxPool2D(2)(Conv(fs)(x))\r\n            x1=keras.layers.MaxPool2D(2)(Conv(fs//width)(x1))\r\n            x2=keras.layers.MaxPool2D(2)(Conv(fs//width)(x2))\r\n    x=keras.layers.GlobalMaxPool2D()(x)\r\n    x=keras.layers.Dropout(0.25)(x)\r\n    outputs=keras.layers.Dense(1000,activation='softmax')(x)\r\n    return keras.Model(inputs=inputs,outputs=outputs,name='MCN520')\r\n\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\nlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n\r\nstarategy=tf.distribute.MirroredStrategy()\r\nwith starategy.scope():\r\n    model=mcn_520(2,24)\r\n    model.summary()\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        metrics=[tf.keras.metrics.TopKCategoricalAccuracy(1,'top1'),tf.keras.metrics.TopKCategoricalAccuracy(5,'top5')]\r\n        )\r\n```\r\n", "I could replicate the issue with Tf 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/955ccf74deb3065fb10a17cd08a0939d/untitled295.ipynb#scrollTo=chYTIWSIrgzX). Thanks!", "@HLSS-Hen [This](https://github.com/tensorflow/tensorflow/issues/29472#issue-452812370) model is related to your issue but check the approach ('base_model') used to show pretrained model as one layer. Here is the colab [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/0b278a807fa9b91e9ad4fd5d701c3081/untitled722.ipynb).  Thanks.\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34634\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34634\">No</a>\n"]}, {"number": 34633, "title": "Profiling with Tensorboard is unable to catch GPU operations", "body": "**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow version (use command below):v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version:3.7.3\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.2\r\n- GPU : P40\r\n\r\n**Describe the current behavior**\r\nFollowing the description in doc, I use the code snippet below to profile model.\r\n```python3\r\nlog_dir=\"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = 3)\r\n\r\nmerge_model.fit(\r\n    dataset, epochs=2, steps_per_epoch=100,\r\n    validation_data=eval_dataset,\r\n    validation_steps=66,\r\n    validation_freq=1,\r\n    callbacks=[tensorboard_callback]\r\n)\r\n```\r\nBut tensorboard callback only captured  cpu ops.\r\n![image](https://user-images.githubusercontent.com/14145834/69692741-f459e300-110d-11ea-9391-e7900280ef96.png)\r\n\r\nIt's for sure that i'm using a gpu version tf and the device is used during training.\r\n\r\n", "comments": ["I suspect your TF does not recognize gpu.  TF 2.0 pre built binary supports cuda 10.0. \r\nYou may try printing;\r\n```python\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n```\r\nIf prints ```False```, then switch to cuda 10.0 and update cuda,cupti, cudnn environment variables.", "Please do check the console output (in the Terminal window if you are using Jupyter) and see if TensorFlow reports that CUPTI is not found. ", "> I suspect your TF does not recognize gpu. TF 2.0 pre built binary supports cuda 10.0.\r\n> You may try printing;\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> tf.test.is_gpu_available()\r\n> ```\r\n> \r\n> If prints `False`, then switch to cuda 10.0 and update cuda,cupti, cudnn environment variables.\r\n\r\nThanks for reply, I think tf recognized gpu correctly, and gpu is utilized for training.\r\n![image](https://user-images.githubusercontent.com/14145834/69933774-daa00d80-150a-11ea-8d42-c9bb2c578b64.png)\r\n![image](https://user-images.githubusercontent.com/14145834/69933851-1c30b880-150b-11ea-8796-0d76327ddd32.png)\r\n\r\n", "> Please do check the console output (in the Terminal window if you are using Jupyter) and see if TensorFlow reports that CUPTI is not found.\r\n\r\nThanks for reply, I do find log as you said. \r\n```\r\n2019-12-02 13:59:56.583464: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.\r\n```\r\n\r\nAdding `/usr/local/cuda/extras/CUPTI/lib64` to `LD_LIBRARY_PATH` fix the problem.\r\nThanks a lot!", "@littlebeandog \r\nPlease, let me know if we can close this issue since it looks to be fixed. Thanks!", "> @littlebeandog\r\n> Please, let me know if we can close this issue since it looks to be fixed. Thanks!\r\n\r\nYes, problem solved, thanks.", " I am closing the issue since the query is been resolved. Thanks!"]}, {"number": 34632, "title": "[Performance Problem] The tf.image.crop_and_resize op costs much time during training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.6.4\r\n- GPU model and memory: GTX 1080Ti, 11Gb\r\n\r\n**Problem Description**\r\nHi Dear experts, my question is about the performance problem on the `tf.image.crop_and_resize` op. My code is as follows:\r\n\r\n    def roi_pooling(featuremap, boxes, box_inds, crop_size, data_format='channels_last'):\r\n        if data_format == 'channels_first':\r\n            shp2d = tf.shape(featuremap)[2:]\r\n            featuremap = tf.transpose(featuremap, [0, 2, 3, 1])\r\n        else:\r\n            shp2d = tf.shape(featuremap)[1:3]\r\n        feat_h, feat_w = tf.cast(shp2d[0], tf.float32), tf.cast(shp2d[1], tf.float32)\r\n        xmin, ymin, xmax, ymax = tf.split(boxes, num_or_size_splits=4, axis=1)\r\n        xmin /= feat_w\r\n        ymin /= feat_h\r\n        xmax /= feat_w\r\n        ymax /= feat_h\r\n        normalized_boxes = tf.concat([ymin, xmin, ymax, xmax], axis=1)\r\n        normalized_boxes = tf.stop_gradient(normalized_boxes)\r\n    \r\n        box_features = tf.image.crop_and_resize(image=featuremap,\r\n                                                boxes=normalized_boxes,\r\n                                                box_ind=box_inds,\r\n                                                crop_size=crop_size)  # nhwc\r\n        box_features = tf.layers.average_pooling2d(inputs=box_features, pool_size=2,\r\n                                                   strides=2, padding='valid',\r\n                                                   data_format='channels_last')\r\n    \r\n        if data_format == 'channels_first':\r\n            box_features = tf.transpose(box_features, [0, 3, 1, 2])  # nhwc --> nchw\r\n    \r\n        return box_features\r\n    \r\nAnd I've take the full trace of the training procedure using the `tensorflow.python.client.timeline` object and got the following tracing graph. \r\n![A0587B57-0D3A-49ba-81C9-56DC9332CB57](https://user-images.githubusercontent.com/43327429/69691790-01c19e00-110b-11ea-8eb3-85e3988eb209.png)\r\n\r\nFrom this graph, we can see that the `tf.image.crop_and_resize` ops costs much time during one training step. So is this reasonable? Could you please give some advice on how to improve the performance? Thanks.", "comments": ["@Remember2018 ,\r\nCan you provide a standalone code to reproduce the issue reported here as the given code is not sufficient to replicate it?Thanks!", "@oanush thanks a lot for your kind reply! I've found the cause. In the project, the number of input `boxes` of `tf.image.crop_and_resize` are about 12000, which seems too large and leads to slow performance. When decreasing the number, we can get the reasonable performance. "]}, {"number": 34631, "title": "Tutorial on basic classification needs some documentation improvement ", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: \r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/keras/classification/ \r\n\r\n## Description of issue (what needs changing):\r\nOne of the first step-by-step tutorial which explains a neural network is on Basic Classification [here](https://www.tensorflow.org/tutorials/keras/classification/). However, it can benefit from additional explanations on few terms like overfitting, optimizer etc.  \r\n\r\n### Clear description\r\nWe add one line description for *overfitting*  to help user get a first hand idea of what *overfitting* does. Later, we add an extra line with a link to TensorFlow definition of overfit where user can find more information. The suggested changes  are expected to make it easier for users to get an intuitive understanding of the term.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? Yes\r\n\r\n### Submit a pull request? \r\n\r\nYes.\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Thanks @copperwiring , do you have a pull request in the works for this? Please link when ready for review", "Oh! Yes. I missed this. I'll send a PR in a few days. Thanks Billy!", "Can i take this one plz??\r\n", "Hi @ManishAradwad Please go ahead and work on it. It'll take me some time to pick up this issue. If needed, I can help you later in the documentation improvement process. Thanks!\r\n\r\nCC: @8bitmp3 ", "Thanks, Srishti!!", "This was already resolved by the https://github.com/tensorflow/docs/pull/1347 Thanks to @ManishAradwad \r\n\r\nAs this is already reflecting in the TF website, I am closing this issue. Thanks!"]}, {"number": 34630, "title": "[Intel MKL]Fix MatMul and Elu fusion issue relate to #33451.", "body": "In previous work we split MatMul and Relu/Relu6/Elu fusion(#33451 ) and Elu issue fix(#34535 ) to 2 different PR, but I wrongly reverted code after the splitting. It will get random failure because we didn't set post operator for MKL-DNN primitive.\r\n\r\nHere's the PR to fix the random failures of MatMul and Elu fusion - in fact it should be part of #33451 .\r\n\r\nModified:\r\n- tensorflow/core/kernels/mkl_matmul_ops_common.h\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["Hi @penpornk , could you take a quick look on this PR? I made a mistake when separate the original MatMul and Elu fusion PR to #33451 and #34535 . The lost condition will prevent `key_creator` to  store `post_op_param.name`, then primitive cache may map `Elu` to `Relu` because they only have different name but their shape are the same here."]}, {"number": 34629, "title": "Fix pip package API generation", "body": "This PR fixes the API generation of the pip package and removes the need for workarounds like `keras = _sys.modules[\"tensorflow.keras\"]` which break IDE autocompletion (see #31973).\r\n\r\n@mihaimaruseac added workarounds to the [`root.__init__`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/virtual_root_template_v2.__init__.py) templates to properly export lazily loaded modules in 18c2cf989a2263ee212fbd5ac0b3085d9450b80a and b7db3f4ae43f5228952f3a1bb480a3e52a2006ed. Unfortunately this breaks autocompletion in many IDEs and therfore can make using TensorFlow tricky.\r\n\r\nThe core issue is that generated [`tensorflow_core.__init__.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py) looks something like this:\r\n```python\r\n# many lines of top level imports\r\n\r\n_names_with_underscore = ['__version__', '__git_version__', '__compiler_version__', '__cxx11_abi_flag__', '__monolithic_build__']\r\n__all__ = [_s for _s in dir() if not _s.startswith('_')]\r\n__all__.extend([_s for _s in _names_with_underscore])\r\n\r\n# lazy loading of keras, estimator, losses, metrics, initializers and optimizers\r\n```\r\n\r\n`__all__` was added by @yifeif in 584a64fa377d3070a05753344a410a5c96685ffd to fix underscore symbol imports.\r\n\r\nSince lazily loaded modules like `keras` are imported after computing `__all__` (which relies on `dir()`), they won't be visible in the root template which imports them via:\r\nhttps://github.com/tensorflow/tensorflow/blob/222f2431b113f2b4726dcf321f3b79688da68e03/tensorflow/virtual_root_template_v2.__init__.py#L101\r\n\r\nThis PR fixes the issue by moving `__all__` to the bottom of the file.\r\n\r\nNote: Please review with care since I wasn't able to build TF from source and thus couldn't properly test the changes in `create_python_api.py`.\r\n\r\nFixes #31973", "comments": ["@lgeiger  Thank you for the change!\r\nYou can verify if this helps without building TensorFlow but instead copying the changes into pip package files.\r\nFor e.g.\r\n```\r\npython3 -m venv venv\r\nsource venv/bin/activate\r\npip install tensorflow\r\n~$ python\r\n>>> import tensorflow as tf\r\n>>> tf\r\n<module 'tensorflow' from '/path/to/python3.7/site-packages/tensorflow/__init__.py'>\r\n```\r\nOpen /path/to/venv/lib/python3.7/site-packages/tensorflow_core/__init__.py\r\nMove `__all__` to the end of the file.\r\nNow you can check if this installation of TensorFlow fixes the issue you see.\r\n(I can also help out with building TensorFlow if you have specific questions).\r\n\r\nThat being said, I am actually looking right now at fixing the autocomplete issues by removing the virtual package and changing tensorflow_core/ to tensorflow/. Specifically, there are other issues as well, not just `__all__` location.\r\n", "@annarev Thanks for the fast reply.\r\n\r\nI did indeed manually test my changes: they fix autocomplete and don't break imports. I just wasn't able to test the pip package generation via `tensorflow/python/tools/api/generator/create_python_api.py`.\r\n\r\n> (I can also help out with building TensorFlow if you have specific questions).\r\n\r\nThanks for the offer, the main issue is just that it takes too long to be feasible on my machine without a remote build cache. I'll build it overnight just to make sure.", "> That being said, I am actually looking right now at fixing the autocomplete issues by removing the virtual package and changing tensorflow_core/ to tensorflow/. Specifically, there are other issues as well, not just `__all__` location.\r\n\r\nThat's great to hear! Do you have an ETA for this?\r\nIt would be really cool from a user perspective to get this into the upcoming 2.1 release, even if it is only my solution that doesn't fully solve the problem.", "This looks good to me. Thank you", "b98db48be1b8238e213d1d6b9ab607ea96088224 will hopefully fix the CI failure. Sorry about that", "> That's great to hear! Do you have an ETA for this?\r\n\r\nI put up a proposal here:\r\nhttps://github.com/tensorflow/community/pull/182/files?short_path=5c4ad87#diff-5c4ad8728e4355930b6444d235b602d0\r\nThe change itself should be relatively quick. However, I need to get some feedback first to see if this is the right way to proceed.\r\n\r\n\r\n"]}, {"number": 34628, "title": "[INTEL MKL] Optimization for MirrorPad op.", "body": "In the original code the index was not being checked correctly for SIMD vectorization. For example, if it is a row-major matrix, the index is correctly checked for vectorization (packets) only for the first row. We need to use `%` operator to make sure the index is correct for each row.\r\n\r\nAlso, several test cases are added to test the correctness and performance. Some outputs of benchmark test:\r\n\r\n| test case|previous|optimized|\r\n| ------ | ------ |------ |\r\n|BM_MirrorPad_cpu_1_16_16_32_1| 1.9M items/s  |6.2M items/s|\r\n|BM_MirrorPad_cpu_1_16_16_32_8| 10.0M items/s  |13.4M items/s|\r\n|BM_MirrorPad_cpu_1_512_512_16_1| 106.1M items/s  |422.1M items/s|\r\n|BM_MirrorPad_cpu_1_512_512_16_256| 118.7M items/s | 196.9M items/s|", "comments": ["Any updates?", "Any update? @penpornk ", "@penpornk  Fixed."]}, {"number": 34627, "title": "Improve resize_bilinear CPU back-prop kernel comment", "body": "I believe that the proposed change is both easier to understand and more correct.", "comments": []}, {"number": 34626, "title": "Move gpu script update to the correct file.", "body": "PiperOrigin-RevId: 282646719\r\nChange-Id: Idbec43ef97473209cf301ef66ec3f323952e42ad", "comments": []}, {"number": 34625, "title": "Fixing test that fails on AVX512", "body": "The operation categorical_crossentropy requires taking log as an intermediate\r\nstep. Due to the rank (2) and shape (3, 3) of the tensors used in this example,\r\non AVX2 and older builds, the log operation uses plog, Eigen's packet log\r\nmethod, whereas on AVX512 build, the log operation is not vectorized and ends\r\nup using std::log.\r\n\r\nDue to the precision mismatch between std::log and Eigen's plog, the results do\r\nnot match exactly. The loss values comes out to be equal to [0.10536055\r\n0.8046685 0.06187541], instead of [0.10536055 0.8046684 0.06187541]. This is an\r\nexpected mismatch and should not fail the test.\r\n\r\nThe absolutely correct way to test would be to compare hex values and make sure\r\nthat the results are within the expected range of the ULP error. An easier fix\r\nwould be to reduce the precision of the test to account for such mismatches\r\nbetween the implementation of operators in the underlying math libraries.\r\n\r\nWe are taking the second approach and will compare results after rounding to 5\r\ndecimal places.", "comments": ["@anuj-rawat Can you please resolve conflicts? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34625) for more info**.\n\n<!-- need_author_cla -->", "> @anuj-rawat Can you please resolve conflicts? Thanks!\r\n\r\nDone. Thanks!", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34625) for more info**.\n\n<!-- ok -->"]}, {"number": 34624, "title": "Added the ability to multiply a SparseTensor with a Dense Matrix on the left side", "body": "At the moment, functionality for `sp_a x dense_b` is implemented. \r\n\r\nIt is easy to show using some linear algebra identities that `dense_a x sp_b` can be acquired by multiplication of the adjoints of the two constituent matrices, and transposing the result.\r\n\r\nWe implement this so that future users do not have to prove to themselves that the implementations of the above operations in TensorFlow are adequate, and that the linear algebra techniques may apply.\r\n\r\nThis enables those users who wish to compute `dense_a x sp_b` to quickly do so.", "comments": ["isinstance(a, SparseTensor)?\n\nOn Mon, Dec 2, 2019 at 2:08 PM Archis Joglekar <notifications@github.com>\nwrote:\n\n> *@joglekara* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/sparse_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/34624#discussion_r352885008>\n> :\n>\n> > @@ -2404,6 +2404,34 @@ def sparse_tensor_dense_matmul(sp_a,\n>          adjoint_a=adjoint_a,\n>          adjoint_b=adjoint_b)\n>\n> +tf_export(\"sparse.dense_sparse_matmul\",\n> +           v1=[\"sparse.dense_sparse_matmul\",\n> +               \"dense_sparse_tensor_matmul\"])\n> +@deprecation.deprecated_endpoints(\"dense_sparse_tensor_matmul\")\n> +def dense_sparse_tensor_matmul(a,\n>\n> do you have a recommendation for how to implement a check that determines\n> whether a is sparse or b?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34624?email_source=notifications&email_token=AAABHRP6YSZ5X25FI2DSRGDQWWBOPA5CNFSM4JR5UYS2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCNVPH6I#discussion_r352885008>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNU6SEZ66Y3AGFY7KTQWWBOPANCNFSM4JR5UYSQ>\n> .\n>\n\n\n-- \n - Alex\n", "Thanks.\r\n\r\nI have merged the two functions as recommended. This has had an unintended benefit of enabling a greater level of generalization.", "Hi @alextp, I had to update the tests. This commit passes `bazel run //tensorflow/python:sparse_ops_test` locally.", "hey @alextp , I had to update a few of the tests. Nothing's changed in terms of the actual implementation. Please let me know if you have any further comments or thoughts.", "@joglekara Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned It's an `api_compatability_test` error. I'm working on reproducing the error locally. If you have any thoughts on what might help, please let me know. \r\n\r\n", "Update: I've found the instructions to run the Sanity test, and it is running now.", "hi @alextp, again, no code changes. The unit tests are good to go as well, it is just style this time. This commit addressed some `line-too-long` warnings from `pylint`. \r\n\r\nHope this passes Ubuntu Sanity. If not, I am running the Sanity test locally in the meantime and should be able to get this through soon.", "Thanks for pushing that through. Looks like Sanity is fine now, but `Ubuntu CPU` is still having issues with the `API_compatibility_test`. \r\nIt would be useful if it could show where the incompatibility was in addition to how many. Anyway, I'm on it!\r\n\r\nAlso, if anyone could tell me what went wrong with `import/copybara`, I can try to address that as well", "I got to the bottom of the `api_compatibility_test` problems and have updated the golden labels accordingly. \r\n\r\nPlease review the new changes.\r\n\r\nFor reference:\r\nThe API has been modified due to a change in the name of variables. The new variable names reflect the fact that either input can be sparse or dense. The pre-existing variable names became misleading with this added functionality.\r\n", "Thanks for the feedback, makes sense. I will choose the former.\r\n\r\nCan you tell me exactly where to document the new meaning? At least the `docstring`, but is there anywhere else this should go?", "Just the docstring.\n\nOn Thu, Jan 2, 2020 at 8:58 AM Archis Joglekar <notifications@github.com>\nwrote:\n\n> Thanks for the feedback, makes sense. I will choose the former.\n>\n> Can you tell me exactly where to document the new meaning? At least the\n> docstring, but is there anywhere else this should go?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34624?email_source=notifications&email_token=AAABHRNRAMAKFLH2IOX73PLQ3YMMNA5CNFSM4JR5UYS2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEH6Z6TQ#issuecomment-570269518>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMYW4UTEY2DSKETKY3Q3YMMNANCNFSM4JR5UYSQ>\n> .\n>\n\n\n-- \n - Alex\n", "Thanks. Just pushed the requested changes. ", "@rthadur, any idea on what the `copybara` error might be?", "Can you please rebase on master?", "Done", "@mihaimaruseac @rthadur please let me know if there's something else you need me to do on my side. this is quite mysterious to me!", "Windows failures are unrelated, so let's wait for internal review and import + additional testing after that", "do you mean @alextp or is someone else internal taking a look at it?", "All code from GitHub gets imported internally at Google, integrated with existing code there and reviewed again. There are more tests being run there.", "hey @mihaimaruseac, let me know if there is any feedback for me that I can use to improve my submission. Thanks!", "I think this is good to go, but internal testing hasn't been run yet due to some tooling issue. Sorry about that.", "Ah alright. Do you happen to have an idea of when that might be resolved?", "The tooling issues have just been resolved. So we're going through the pipeline of blocked PRs and merging them", "Great. Thanks for the update!", "Looks like something's getting stuck, let me know if there's something I can do to help!", "@mihaimaruseac, I made both of the proposed changes. Feel free to take another look. ", "Gotcha. Just ran the linter. Let's try again", "This now landed. Unfortunately the PR couldn't have been closed as the formatting tool formatted the code differently.\r\n\r\nYou can look at the f0cf1dc to see how the commited code looks like.\r\n\r\nClosing this manually.\r\n\r\nThank you for the PR.", "Thank you for working with me to get this in @mihaimaruseac "]}, {"number": 34623, "title": "Fix bug in if -v check.", "body": "PiperOrigin-RevId: 282598769\r\nChange-Id: Ic71c23ba3983c397cac81a44cd4a71f76e5ffef3", "comments": []}, {"number": 34622, "title": "Fold ADD with TRANSPOSECONV in tflite converter.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n- This is a feature request and also relative to performance issue.\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\ntf-nightly '2.1.0-dev20191126'\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFor generating _.tflite_ file with [_TFLiteConverter_](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter), when model contains [_Conv2DTranspose_](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) layers, bias cannot fold into Operator TRANSPOSECONV. It will result with extra Op ADD following Op TRANSPOSECONV.\r\nBut with other CONV-like layers (Conv2D, DepthwiseConv2D), bias will be fold into CONV layer.\r\n\r\n- [ ] **bias should fold into Op TRANSPOSECONV in tflite.**\r\n\r\nReproduce current behaviors with example code and model graph:\r\n```\r\ndef get_keras_model():\r\n    input_0 = tf.keras.layers.Input(shape=[100,100,3])\r\n    # Conv2D with bias\r\n    conv_0 = tf.keras.layers.Conv2D(filters=3, kernel_size=[3, 3],\r\n                                    use_bias=True, padding=\"same\",\r\n                                    kernel_initializer=tf.random_uniform_initializer,\r\n                                    bias_initializer=tf.random_uniform_initializer)(input_0)\r\n    # Conv2D without bias\r\n    conv_1 = tf.keras.layers.Conv2D(filters=3, kernel_size=[3, 3],\r\n                                    use_bias=False, padding=\"same\",\r\n                                    kernel_initializer=tf.random_uniform_initializer)(conv_0)\r\n    # DepthwiseConv2D with bias\r\n    depthwise_conv_0 = tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3],\r\n                                                       use_bias=True, padding=\"same\",\r\n                                                       kernel_initializer=tf.random_uniform_initializer,\r\n                                                       bias_initializer=tf.random_uniform_initializer)(conv_1)\r\n    # DepthiseConv2D without bias\r\n    depthwise_conv_1 = tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3],\r\n                                                       use_bias=False, padding=\"same\",\r\n                                                       kernel_initializer=tf.random_uniform_initializer)(depthwise_conv_0)\r\n    # TrasposeConv with bias\r\n    transpose_conv_0 = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=[3,3],\r\n                                                       use_bias=True, padding=\"same\",\r\n                                                       kernel_initializer=tf.random_uniform_initializer,\r\n                                                       bias_initializer=tf.random_uniform_initializer)(depthwise_conv_1)\r\n    # TrasposeConv without bias\r\n    transpose_conv_1 = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=[3, 3],\r\n                                                       use_bias=False, padding=\"same\",\r\n                                                       kernel_initializer=tf.random_uniform_initializer)(transpose_conv_0)\r\n\r\n    keras_model = tf.keras.models.Model(inputs=[input_0], outputs=[transpose_conv_1])\r\n\r\n    return keras_model\r\n\r\n\r\ndef gen_tflite():\r\n\r\n    keras_model = get_keras_model()\r\n    keras_model.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n                        optimizer='adam',\r\n                        metrics=['accuracy'])\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n    tflite_quant_model = converter.convert()\r\n    open('conv.tflite', 'wb').write(tflite_quant_model)\r\n\r\ngen_tflite()\r\n```\r\nAs the graph below shows, _TFLiteConverter_ will fold bias with Conv2D and Depthwise2D, but not fold bias with TransposeConv:\r\n![image](https://user-images.githubusercontent.com/55463253/69658482-2f0f3d00-1074-11ea-944f-f46fe3723453.png)\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will not affect any TF python api.\r\nIt will affect [tflite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/transpose_conv.cc#L341) as bias added.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers of TensorFlow, as \r\n\r\n- consistent implementation/behavior between conv-like layers\r\n\r\n- Performance boost when tflite running on device, as TRANSPOSECONV + ADD folded into TRANSPOSECONV\r\n\r\n**Any Other info.**\r\nN/A", "comments": ["@liyunlu0618 Sorry to bother you, I saw your recent update with INT8 TRANSPOSE_CONV reference kernel.\r\n\r\nDo you mind to have a look of this issue? I'm wondering why transpose_conv doesn't come with bias in tflite Op kernel. Meantime, conv and depthwise_conv does.\r\n\r\nI'm trying to contribute regarding this issue, so that transpose_conv will has bias inside. And tflite graph transformer will behave the same for {conv, depthwise_conv, transpose_conv}, which fuse bias add with \"conv\" Op.\r\n\r\nIt will be very helpful if you can give some suggestions. Thanks.", "@gadagashwini @haozha111 Hi guys, sorry to bother you. Can you give some suggestions?", "Pull request https://github.com/tensorflow/tensorflow/pull/34903 trying to resolve this issue.", "thanks!", "Is this issue resolved??\r\nIt does not seem to work in tf versions: 2.4.0-dev20200916, 2.3.0 !!!\r\nAn extra shape node seems to be created ..."]}, {"number": 34621, "title": "R2.1", "body": "pulling TF", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34621) for more info**.\n\n<!-- need_sender_cla -->", "We don't merge release branches back into master."]}, {"number": 34620, "title": "Batch size reset to 1 after conversion", "body": "**System information**\r\n- OS Platform and Distribution Windows 10 and Ubuntu 64 bit\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```python\r\nx=tf.keras.Input(shape=(256,1), batch_size=50)\r\ny=tf.keras.layers.Convolution1D(kernel_size=5,filters=8,input_shape=(50,256,1))(x)\r\nmodel=tf.keras.Model(x,y)\r\n# compile and train\r\n# ... \r\nconverter=tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.representative_dataset = setgen\r\nmodelq=converter.convert()\r\n\r\nconv_interpreter=tf.lite.Interpreter(model_content=modelq)\r\ninput_details = conv_interpreter.get_input_details()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n{'name': 'input_5',\r\n 'index': 8,\r\n 'shape': array([   1, 256,    1]),\r\n 'dtype': numpy.float32,\r\n 'quantization': (0.0, 0)}\r\n```\r\n\r\n**Failure details**\r\nDespite before conversion the batch size is explicitly set to 50, after conversion the batch size is always 1.\r\nThe reason why I am trying to have batch size >1 is because the model has to be executed in the edge TPU and I want to exploit as best the parallelism offered by the chip, thus performing the convolution on as many inputs as possible in parallel.\r\nIf not supported by TF Lite, any suggestion to achieve the same result is welcome and appreciated\r\n", "comments": ["@antofara I ran your code in `tf-nightly` and I see batch_size set as 50. There were lots of improvements recently and team is continuously improving the converter. \r\n\r\nI have also added new experimental new converter as `converter.experimental_new_converter = True`. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/c7f8adcc2be802258e522540b0c61472/untitled686.ipynb).\r\n\r\nPlease close the issue if it was resolved for you. Otherwise, please share a colab gist with a standalone code to reproduce the issue. Thanks!", "Thanks @jvishnuvardhan \r\nSolved with the following fixes:\r\nTF version: '2.1.0-dev20191204'\r\n`converter.experimental_new_converter = True\r\n`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34620\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34620\">No</a>\n"]}, {"number": 34619, "title": "Wrong output printing when using fit_generator", "body": "On TensorFlow 1.15 (OS Ubuntu 16.04), when I'm using generators to train a model via the tf.keras.model fit_generator() function call prints a wrong and duplicated output of \"Epoch 1\" when starting the validation steps on every epoch.  The function fit_generator() prints:\r\n\r\n```\r\nEpoch 1/2\r\nEpoch 1/2\r\n32/32 - 5s - loss: 2.3132 - val_loss: 2.3116\r\nEpoch 2/2\r\nEpoch 1/2\r\n32/32 - 1s - loss: 2.3070 - val_loss: 2.3075\r\n```\r\n\r\nThe minimal code to reproduce is:\r\n\r\n```\r\nimport math\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.utils import to_categorical\r\n\r\n\r\ndef data_generator(X, Y, batch_size, start=0, end=None):\r\n    end = len(X) if end is None else end\r\n    num_batches = int(math.ceil((end-start)/batch_size))\r\n    while True:\r\n        lob = list(range(num_batches))\r\n        for bi in lob:\r\n            sb = start + bi*batch_size\r\n            eb = sb + batch_size\r\n            eb = end if eb > end else eb\r\n            Xb = X[sb:eb]\r\n            Yb = Y[sb:eb]\r\n            yield Xb, Yb\r\n\r\n\r\n(Xtr, Ytr), (Xva, Yva) = tf.keras.datasets.cifar10.load_data()\r\nXtr, Ytr, Xva, Yva, nc = Xtr[:1000], Ytr[:1000], Xva[:100], Yva[:100], 10\r\nXtr, Xva = Xtr.astype('float32') / 255, Xva.astype('float32') / 255\r\nYtr, Yva, ins = to_categorical(Ytr, nc), to_categorical(Yva, nc), Xtr.shape[1:]\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Input(ins))\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3)))\r\nmodel.add(tf.keras.layers.Activation('relu'))\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(32, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))\r\nmodel.add(tf.keras.layers.Dense(nc, activation='softmax'))\r\nopt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[])\r\n\r\nbatch_size = 32\r\ntr_gen = data_generator(Xtr, Ytr, batch_size)\r\nva_gen = data_generator(Xva, Yva, batch_size)\r\ntr_steps = int(math.ceil(len(Xtr)/batch_size))\r\nva_steps = int(math.ceil(len(Xva)/batch_size))\r\n\r\nmodel.fit_generator(tr_gen, steps_per_epoch=tr_steps, epochs=2, verbose=2,\r\n                    validation_data=va_gen, validation_steps=va_steps)\r\n```\r\n\r\nIf I run it on TensorFlow 2.0, the duplicated and wrong prints disappear.", "comments": ["Could replicate the issue with Tf 1.15 and its working fine with Tf 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/aca26f3a33c9dabd1819dcce2d4af363/untitled275.ipynb). Thanks!", "@andmax Thanks for the issue! This is fixed in the latest nightly, unfortunately we can't backport changes like this to older version.\r\n\r\nPlease try it out with `pip install -U tf-nightly`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34619\">No</a>\n"]}, {"number": 34618, "title": "Support exporting the transform graph from tf.Transform together with the model graph defined using Keras to SavedModel in TensorFlow 2.0", "body": "**System information**\r\n- TensorFlow version (you are using): TensorFlow 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe feature is:  improve `tf.saved.saved_model` Api in Keras and support exporting the transform graph from tf.Transform and the graph from the Keras model together into a single SavedModel.\r\n\r\n**Current behavior/state** about how to export transform graph and model graph together to a SavedModel?  \r\nEstimator (Work well):  \r\nFrom [tf.transform official tutorial](https://github.com/tensorflow/transform/blob/cf517f1a8765e6552d09cee65efea80e26c05f5e/examples/census_example.py#L380-L382), at the stage of exporting model, it will call [estimator.export_saved_model(exported_model_dir, serving_input_fn)](https://github.com/tensorflow/estimator/blob/b22a912de2693322622d6f50e3a19e98fecac441/tensorflow_estimator/python/estimator/estimator.py#L661) to complete the model exporting work. Inside its implementation, it calls serving_input_fn to load the transform graph from SavedModel at first, and then calls estimator's model_fn to generate the model graph, combines these two graph into one graph and finally exports it into one SavedModel. Please check the [code snippet](https://github.com/tensorflow/estimator/blob/43921b4552d1c30acc31d3b5989112cb397383e0/tensorflow_estimator/python/estimator/estimator.py#L926-L937).  \r\n\r\nKeras (Can not):  \r\nFor TF2.0, we define a model using keras and exports it by calling [tf.saved.saved_model](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). This SavedModel only contains the model definition including feature columns and network structure. `tf.saved.saved_model` api doesn't have the parameter `serving_input_fn` just like estimator and **it lacks the flexibility to combine transform graph and model graph together for inference**.\r\n\r\n**Will this change the current api? How?**\r\nWill change the Api `tf.saved.save_model`, add a new parameter `serving_input_fn`.\r\n\r\n**Who will benefit with this feature?**\r\nThe users of tf.Transform and TensorFlow 2.0\r\n\r\n**Any Other info.**\r\n", "comments": ["The same question has been asked in [Tensorflow Transform](https://github.com/tensorflow/transform/issues/150) Repository as well. ", "Hi, @rmothukuru and @k-w-w , can you please share more information about this issue?", "@brightcoder01,\r\nCan you please let us know if [this comment](https://github.com/tensorflow/transform/issues/150#issuecomment-694769276) has resolved your issue? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34617, "title": "ValueError: model_fn (<tensorflow.python.eager.def_function.Function object at 0x7f0391abc4a8>) must include features argument.", "body": "I have been trying to solve this for 5 hours but to no avails. Kindly help if any of you have solved this. I have renamed all the variable accordingly but it still says, value error. I can share my code with you if you want. \r\nI am using **Tensorflow 2.0**", "comments": ["@muhammadfahid51 , please share your code, preferably as a Google Colab gist.", "> @muhammadfahid51 , please share your code, preferably as a Google Colab gist.\r\n\r\nnikochiko here is my code,\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\n\r\n@tf.function\r\ndef model_fn(features, labels, mode, params):\r\n    img = tf.keras.layers.Input(features, shape=(28, 28, 1))\r\n    img = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(img)\r\n    img = tf.keras.layers.MaxPool2D((2, 2), 2)(img)\r\n    img = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(img)\r\n    img = tf.keras.layers.MaxPool2D((2, 2), 2)(img)\r\n    img = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(img)\r\n    img = tf.keras.layers.Flatten()(img)\r\n    img = tf.keras.layers.Dense(units=256, activation=\"relu\")(img)\r\n    img = tf.keras.layers.Dense(units=128, activation=\"relu\")(img)\r\n    logits = tf.keras.layers.Dense(units=10, activation=None)(img)\r\n\r\n    probs = tf.nn.softmax(logits)\r\n    predicted_classes = tf.argmax(probs, 1)\r\n\r\n    loss = tf.losses.sparse_categorical_crossentropy(labels, logits)\r\n\r\n    acc = tf.metrics.Accuracy(labels, predicted_classes)\r\n    metrics = {\"accuracy\": acc}\r\n    tf.summary.scaler('accuracy', acc[1])\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode,\r\n                                          loss=loss, eval_metric_ops=metrics)\r\n\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss)\r\n    return tf.estimator.EstimatorSpec(mode,\r\n                                      loss=loss, train_op=train_op)\r\n\r\n\r\ndef load_mnist():\r\n    train, test = mnist.load_data()\r\n    return train, test\r\n\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\r\n    dataset = dataset.shuffle(500)\r\n    dataset = dataset.repeat().batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef eval_input_fn(imgs, labels, batch_size):\r\n    dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\r\n    dataset = dataset.batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef main(m_dir=None):\r\n    (x_train, y_train), (x_test, y_test) = load_mnist()\r\n    classifier = tf.estimator.Estimator(model_fn=model_fn,\r\n                                        params=['features', 'labels', 'mode', 'params', 'self', 'config'])\r\n\r\n\r\nmain()", "@muhammadfahid51 , TensorFlow is unable to recognize the arguments included in `model_fn` due to the  `@tf.function` wrapper.  You need to remove the it. See [gist](https://colab.research.google.com/gist/nikochiko/c3e8173b553510106753e5f9fe4f8b3a/untitled34.ipynb) ", "Thanks a lot bro. Thanks a LOT", "> Thanks a lot bro. Thanks a LOT\r\n\r\n`", "> Thanks a lot bro. Thanks a LOT\r\n\r\n\r\n\r\n> @muhammadfahid51 , TensorFlow is unable to recognize the arguments included in `model_fn` due to the `@tf.function` wrapper. You need to remove the it. See [gist](https://colab.research.google.com/gist/nikochiko/c3e8173b553510106753e5f9fe4f8b3a/untitled34.ipynb)\r\n\r\nTypeError: minimize() missing 1 required positional argument: 'var_list'\r\n\r\n\r\nHere is my code, Can you solve this bro. I don't know what to put there,\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    features = tf.cast(features, dtype=tf.float32)\r\n    print(features.dtype)\r\n    img = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(features)\r\n    img = tf.keras.layers.MaxPool2D((2, 2), 2)(img)\r\n    img = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(img)\r\n    img = tf.keras.layers.MaxPool2D((2, 2), 2)(img)\r\n    img = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(img)\r\n    img = tf.keras.layers.Flatten()(img)\r\n    img = tf.keras.layers.Dense(units=256, activation=\"relu\")(img)\r\n    img = tf.keras.layers.Dense(units=128, activation=\"relu\")(img)\r\n    logits = tf.keras.layers.Dense(units=10, activation=None)(img)\r\n\r\n    probs = tf.keras.activations.softmax(logits)\r\n    predicted_classes = tf.argmax(probs, 1)\r\n\r\n    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, probs)\r\n\r\n    acc = tf.compat.v1.metrics.accuracy(labels, predicted_classes)\r\n    metrics = {\"accuracy\": acc}\r\n    tf.summary.scalar(\"accuracy\", acc[1])\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode,\r\n                                          loss=loss, eval_metric_ops=metrics)\r\n\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n\r\n    optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss)\r\n    return tf.estimator.EstimatorSpec(mode,\r\n                                      loss=loss, train_op=train_op)\r\n\r\n\r\ndef load_mnist():\r\n    train, test = mnist.load_data()\r\n    return train, test\r\n\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    # features = tf.convert_to_tensor(features, dtype=tf.float32)\r\n    # labels = tf.convert_to_tensor(labels, dtype=tf.int32)\r\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\r\n    dataset = dataset.shuffle(500)\r\n    dataset = dataset.repeat().batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef eval_input_fn(imgs, labels, batch_size):\r\n    dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\r\n    dataset = dataset.batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef main(m_dir=None):\r\n    (x_train, y_train), (x_test, y_test) = load_mnist()\r\n    x_train = x_train.reshape(60000, 28, 28, 1)\r\n    x_test = x_test.reshape(10000, 28, 28, 1)\r\n    classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=m_dir)\r\n\r\n    classifier.train(input_fn=lambda: train_input_fn(x_train, y_train, 500), steps=100)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n", "@muhammadfahid51 , please open another issue and feel free to tag me there. Please keep the issue comments related. Also, enclose all of your code in one ``` codeblock."]}, {"number": 34615, "title": "Fail to load frozen model converted from FP32 to FP16 because of type mismatch", "body": "Hi, I was converting tensorflow frozen pb model from fp32 to fp16 using a method in this [post](https://stackoverflow.com/questions/55827368/loading-tensorflow-object-detection-model-post-training-quantization-to-fp16-is?noredirect=1#comment104009065_55827368). The conversion succeeded (the model I used was [ssd mobilenet v2 coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)). But when I try to load the converted model, error `ValueError: Input 0 of node Preprocessor/map/while/Enter_2 was passed float from Preprocessor/map/TensorArray_1:1 incompatible with expected half.` was raised. Then I checked the node information in the converted model in pbtxt format, the node `Preprocessor/map/TensorArray_1` was a `TensorArrayV3` as this one:\r\n```\r\nnode {\r\n  name: \"Preprocessor/map/TensorArray_1\"\r\n  op: \"TensorArrayV3\"\r\n  input: \"Preprocessor/map/strided_slice\"\r\n  attr {\r\n    key: \"clear_after_read\"\r\n    value {\r\n      b: true\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_HALF\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dynamic_size\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"element_shape\"\r\n    value {\r\n      shape {\r\n        unknown_rank: true\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"identical_element_shapes\"\r\n    value {\r\n      b: true\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_array_name\"\r\n    value {\r\n      s: \"\"\r\n    }\r\n  }\r\n}\r\n```\r\nAnd the output node `Preprocessor/map/while/Enter_2` was:\r\n```\r\nnode {\r\n  name: \"Preprocessor/map/while/Enter_2\"\r\n  op: \"Enter\"\r\n  input: \"Preprocessor/map/TensorArray_1:1\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_HALF\r\n    }\r\n  }\r\n  attr {\r\n    key: \"frame_name\"\r\n    value {\r\n      s: \"Preprocessor/map/while/while_context\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"is_constant\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"parallel_iterations\"\r\n    value {\r\n      i: 32\r\n    }\r\n  }\r\n}\r\n```\r\nThe node's [type](https://www.tensorflow.org/api_docs/python/tf/TensorArray) was DT_HALF, which indicated FP16 as expected. But according to the error information, the first node seemed export a FP32 node rather than FP16 value. So I'm confused about the source of FP32: where does this FP32 info come from? If the op `Enter` requires FP16? Any explanation about loading frozen model is really appreciated! Thanks!", "comments": ["@CasiaFan ,\r\nHi, can you please provide a standalone code to reproduce the error reported here?Thanks!", "```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.framework import types_pb2, graph_pb2, attr_value_pb2\r\nfrom tensorflow.tools.graph_transforms import TransformGraph\r\nfrom google.protobuf import text_format\r\nimport numpy as np\r\n\r\n# object detection api input and output nodes\r\ninput_name = \"image_tensor\"\r\noutput_names = [\"detection_boxes\", \"detection_classes\", \"detection_scores\", \"num_detections\"]\r\n# Const should be float32 in object detection api during nms (see here: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/non-max-suppression-v4.html)\r\nkeep_fp32_node_name = [\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold\",\r\n                       \"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/non_max_suppression/score_threshold\"]\r\n\r\ndef load_graph(model_path):\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        graph_def = tf.GraphDef()\r\n        if model_path.endswith(\"pb\"):\r\n            with open(model_path, \"rb\") as f:\r\n                graph_def.ParseFromString(f.read())\r\n        else:\r\n            with open(model_path, \"r\") as pf:\r\n                text_format.Parse(pf.read(), graph_def)\r\n        tf.import_graph_def(graph_def, name=\"\")\r\n        sess = tf.Session(graph=graph)\r\n        return sess\r\n\r\ndef rewrite_batch_norm_node_v2(node, graph_def, target_type='fp16'):\r\n    \"\"\"\r\n    Rewrite FusedBatchNorm with FusedBatchNormV2 for reserve_space_1 and reserve_space_2 in FusedBatchNorm require float32 for \r\n    gradient calculation (See here: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fused-batch-norm)\r\n    \"\"\"\r\n    if target_type == 'fp16':\r\n        dtype = types_pb2.DT_HALF\r\n    elif target_type == 'fp64':\r\n        dtype = types_pb2.DT_DOUBLE\r\n    else:\r\n        dtype = types_pb2.DT_FLOAT\r\n    new_node = graph_def.node.add()\r\n    new_node.op = \"FusedBatchNormV2\"\r\n    new_node.name = node.name\r\n    new_node.input.extend(node.input)\r\n    new_node.attr[\"U\"].CopyFrom(attr_value_pb2.AttrValue(type=types_pb2.DT_FLOAT))\r\n    for attr in list(node.attr.keys()):\r\n        if attr == \"T\":\r\n            node.attr[attr].type = dtype\r\n        new_node.attr[attr].CopyFrom(node.attr[attr])\r\n    print(\"rewrite fused_batch_norm done!\")\r\n\r\ndef convert_graph_to_fp16(model_path, save_path, name, as_text=False, target_type='fp16', input_name=None, output_names=None):\r\n    if target_type == 'fp16':\r\n        dtype = types_pb2.DT_HALF\r\n    elif target_type == 'fp64':\r\n        dtype = types_pb2.DT_DOUBLE\r\n    else:\r\n        dtype = types_pb2.DT_FLOAT\r\n    source_sess = load_graph(model_path)\r\n    source_graph_def = source_sess.graph.as_graph_def()\r\n    target_graph_def = graph_pb2.GraphDef()\r\n    target_graph_def.versions.CopyFrom(source_graph_def.versions)\r\n    for node in source_graph_def.node:\r\n        # fused batch norm node\r\n        if node.op == \"FusedBatchNorm\":\r\n            rewrite_batch_norm_node_v2(node, target_graph_def, target_type=target_type)\r\n            continue\r\n        # replicate node\r\n        new_node = target_graph_def.node.add()\r\n        new_node.op = node.op\r\n        new_node.name = node.name\r\n        new_node.input.extend(node.input)\r\n        attrs = list(node.attr.keys())\r\n        # keep batch norm params node\r\n        if (\"BatchNorm\" in node.name) or ('batch_normalization' in node.name):\r\n            for attr in attrs:\r\n                new_node.attr[attr].CopyFrom(node.attr[attr])\r\n            continue\r\n        # replace dtype in node attr with target dtype\r\n        for attr in attrs:\r\n            # keep special node in fp32\r\n            if node.name in keep_fp32_node_name:\r\n                new_node.attr[attr].CopyFrom(node.attr[attr])\r\n                continue\r\n            if node.attr[attr].type == types_pb2.DT_FLOAT:\r\n                # modify node dtype\r\n                node.attr[attr].type = dtype\r\n            if attr == \"value\":\r\n                tensor = node.attr[attr].tensor\r\n                if tensor.dtype == types_pb2.DT_FLOAT:\r\n                    # if float_val exists\r\n                    if tensor.float_val:\r\n                        float_val = tf.make_ndarray(node.attr[attr].tensor)\r\n                        new_node.attr[attr].tensor.CopyFrom(tf.make_tensor_proto(float_val, dtype=dtype))\r\n                        continue\r\n                    # if tensor content exists\r\n                    if tensor.tensor_content:\r\n                        tensor_shape = [x.size for x in tensor.tensor_shape.dim]\r\n                        tensor_weights = tf.make_ndarray(tensor)\r\n                        # reshape tensor\r\n                        tensor_weights = np.reshape(tensor_weights, tensor_shape)\r\n                        tensor_proto = tf.make_tensor_proto(tensor_weights, dtype=dtype)\r\n                        new_node.attr[attr].tensor.CopyFrom(tensor_proto)\r\n                        continue\r\n            new_node.attr[attr].CopyFrom(node.attr[attr])\r\n    # transform graph\r\n    if output_names:\r\n        if not input_name:\r\n            input_name = []\r\n        transforms = [\"strip_unused_nodes\"]\r\n        target_graph_def = TransformGraph(target_graph_def, input_name, output_names, transforms)\r\n    # write graph_def to model\r\n    tf.io.write_graph(target_graph_def, logdir=save_path, name=name, as_text=as_text)\r\n    print(\"Converting done ...\")\r\n\r\nmodel_path = \"ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\"\r\nsave_path = \"test\"\r\nname = \"test.pb\"\r\nas_text = False\r\ntarget_type = 'fp16'\r\nconvert_graph_to_fp16(model_path, save_path, name, as_text=as_text, target_type=target_type, input_name=input_name, output_names=output_names)\r\nsess = load_graph(save_path+\"/\"+name)\r\n```\r\nMy tf version is 1.14. To visualize the node info, just turn `as_text` to `True`. And then check the node `Preprocessor/map/TensorArray_2`.\r\n@oanush \r\n", "@CasiaFan ,\r\nWhen tried running the code, i got error `NameError: name 'model_path' is not defined`.Thanks!", "@oanush Oh sorry, my fault. Fixed it by changing `save_path` to `model_path` and the code above had been updated.", "Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/0f28c3f52975696bba8ae7f8f28d2f49/34615.ipynb#scrollTo=3YJQNP6eGQ-q) of colab when tried replicating the issue for given code.Thanks!", "@oanush Use `tensorflow-gpu==1.14.0` and that error is due to tf2.0. ", "I am running into an error. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/1a3eeb94556be57dd331cd198f0df240/copy-of-34615.ipynb). Thanks!", "The official model weights should be downloaded and unzipped first. Here is\nthe link:\nhttp://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n\ngowthamkpr <notifications@github.com> \u4e8e2019\u5e7412\u670810\u65e5\u5468\u4e8c \u4e0a\u53486:12\u5199\u9053\uff1a\n\n> I am running into an error. Please find my gist here\n> <https://colab.sandbox.google.com/gist/gowthamkpr/1a3eeb94556be57dd331cd198f0df240/copy-of-34615.ipynb>.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34615?email_source=notifications&email_token=ACQ6CWGYQHOUZAZIFUR3NDLQX27DJA5CNFSM4JRXKTMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGK4OWY#issuecomment-563464027>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACQ6CWDKKQZKCHAEAYMAB5TQX27DJANCNFSM4JRXKTMA>\n> .\n>\n", "@CasiaFan Were you able to resolve this issue?", "Closing this issue as it has been inactive for more than 4 weeks. Please add additional comments to open this issue. Thanks!", "@CasiaFan Have you fixed this issue? I met the same issue, but have no idea about solution.", "@zhangxiaoli73 @CasiaFan Do you have any solution for this problem? thanks"]}, {"number": 34614, "title": "\u9047\u5230ValueError: Invalid tensors 'input/input_data:0' were found.", "body": "\u9047\u5230ValueError: Invalid tensors 'input/input_data:0' were found.\r\n\u540c\u6837\u95ee\u9898\uff0c\u8bf7\u95ee\u6709\u8c01\u89e3\u51b3\u4e86\u5417\uff1f\u5e2e\u6211\uff0c 'input/input_data:0' \u662f\u6211\u6d4b\u8bd5pb\u6a21\u578b\u662f\u7684\u8f93\u5165\u53d8\u91cf\u540d\uff0c\u4f46\u662f\u8f6ctflite\u65f6\uff0c\u62a5\u4e0a\u8ff0\u9519\u8bef\u3002\u8c22\u8c22\u3002\u3002\u3002\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new/choose). Please provide all the information it asks. Thank you."]}, {"number": 34613, "title": "try", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 34612, "title": "Build a static tensorflow on Alpine linux ", "body": "Any instructions, documents or blogs show us how to build static library of tensorflow on Alpine linux? I want to port the object detection model(and other) to aws lambda, build tensorflow as a static library could save a lot of troubles", "comments": ["@stereomatchingkiss \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34612\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34612\">No</a>\n"]}, {"number": 34611, "title": "Implement reference kernel for Add and Mul using CMSIS-NN", "body": "", "comments": ["@petewarden @gbaned gentle push for review"]}, {"number": 34610, "title": "Difference output when dumb freeze .pb from .hdf5", "body": "Hi I'm trying to dumb frozen .pb graph from .hdf5 file but I my .pb have difference output with .h5 file \r\n### input and output name of .hdf5 file\r\n```bash\r\nAll input nodes: [<tf.Tensor 'input_1:0' shape=(?, 256, 320, 3) dtype=float32>]\r\nAll output nodes: [<tf.Tensor 'fcn17/truediv:0' shape=(?, ?, ?, 4) dtype=float32>]\r\n```\r\n### and output value of hdf5 file like this: \r\n```bash\r\narray([[[[0.95520484, 0.02454368, 0.01151993, 0.0087315 ],\r\n         [0.96743697, 0.01915975, 0.00736554, 0.00603788],\r\n         [0.97429854, 0.01614887, 0.00556678, 0.00398591],\r\n         ...,\r\n         [0.96292394, 0.01786121, 0.00704672, 0.01216816],\r\n         [0.95855474, 0.02005052, 0.00870537, 0.0126894 ],\r\n         [0.9483133 , 0.02560406, 0.01250436, 0.01357825]],\r\n         ...,\r\n         [0.6439966 , 0.33929765, 0.01019213, 0.00651362],\r\n         [0.64723253, 0.3306629 , 0.01337334, 0.00873124],\r\n         [0.649275  , 0.3211698 , 0.01852187, 0.0110333 ]]]],\r\n      dtype=float32)\r\n```\r\n###  this is output of .pb file ( 'fcn17/truediv:0' ) : \r\n```bash\r\n[[[0.24999999 0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.24999999 0.24999999]\r\n  ...\r\n  [0.25       0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.25       0.25      ]]]\r\n```\r\nand i see my output of hdf5 file difference pb file\r\n### this is my code i try to dumb freeze graph from hdf5 file \r\n```bash\r\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\r\n    \"\"\"\r\n    Freezes the state of a session into a pruned computation graph.\r\n\r\n    Creates a new computation graph where variable nodes are replaced by\r\n    constants taking their current value in the session. The new graph will be\r\n    pruned so subgraphs that are not necessary to compute the requested\r\n    outputs are removed.\r\n    @param session The TensorFlow session to be frozen.\r\n    @param keep_var_names A list of variable names that should not be frozen,\r\n                          or None to freeze all the variables in the graph.\r\n    @param output_names Names of the relevant graph outputs.\r\n    @param clear_devices Remove the device directives from the graph for better portability.\r\n    @return The frozen graph definition.\r\n    \"\"\"\r\n    graph = session.graph\r\n    with graph.as_default():\r\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\r\n        output_names = output_names or []\r\n        output_names += [v.op.name for v in tf.global_variables()]\r\n        input_graph_def = graph.as_graph_def()\r\n        if clear_devices:\r\n            for node in input_graph_def.node:\r\n                node.device = \"\"\r\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n            session, input_graph_def, output_names, freeze_var_names)\r\n        return frozen_graph\r\n\r\ndef freeze_graph_keras(net, model_dir):\r\n    \"\"\"Extract the sub graph defined by the output nodes and convert \r\n    all its variables into constant \r\n    Args:\r\n        model_dir: the root folder containing the checkpoint state file\r\n        output_node_names: a string, containing all the output node's names, \r\n                            comma separated\r\n    \"\"\"\r\n\r\n    # The export path contains the name and the version of the model\r\n    tf.keras.backend.set_learning_phase(0)  # Ignore dropout at inference\r\n    file_name = os.path.basename(model_dir).replace('.hdf5', '.pb')\r\n    model_dir = os.path.dirname(model_dir)\r\n    print(os.path.join(model_dir, file_name))\r\n    with tf.keras.backend.get_session() as sess:\r\n        tf.initialize_all_variables().run()\r\n        frozen_graph = freeze_session(K.get_session(),\r\n                                      output_names=[out.op.name for out in net.outputs])\r\n        tf.train.write_graph(frozen_graph, model_dir,\r\n                             file_name, as_text=False)\r\n    print('All input nodes:', net.inputs)\r\n    print('All output nodes:', net.outputs)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_dir\", type=str,\r\n                        default=\"path.hdf5\", help=\"Model folder to export\")\r\n\r\n    args = parser.parse_args()                    \r\n\r\n    model = build_my_model((256,320,3), num_classes=4,\r\n                    lr_init=1e-3, lr_decay=5e-4)\r\n    model.load_weights('path')\r\n    freeze_graph_keras(model, args.model_dir)\r\n```\r\nI'm using tensorflow version 1.14.0 and keras 2.2.4", "comments": ["@BuiNgocHai,\r\nCan you please clarify the reason for Freezing the Graph i.e., if you are planning to deploy the model in Mobile Device, etc..", "I want to Freezing the Graph for my project with ROS, i have some problem with run Keras with ROS but .pb is OK and .pb will be faster also I don't want show my summary model when i submit my project so i think i need Freezing the Graph for this.", "@BuiNgocHai Can you try `TF1.15.0` and also share a simple standalone code to reproduce the issue. Thanks!", "Here is my model\r\n```bash\r\ndef focal_loss(gamma=2., alpha=.25):\r\n\tdef focal_loss_fixed(y_true, y_pred):\r\n\t\tpt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\r\n\t\tpt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\r\n\t\treturn -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\r\n\treturn focal_loss_fixed\r\n\r\ndef dice_coef(y_true, y_pred):\r\n    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\r\n\r\ndef mean_iou(num_classes):\r\n    def iou(y_true, y_pred):\r\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred, num_classes)\r\n        K.get_session().run(tf.local_variables_initializer())\r\n        with tf.control_dependencies([up_opt]):\r\n            score = tf.identity(score)\r\n        return score\r\n    return iou\r\n\r\n\r\ndef model(shape, num_classes, lr_init, lr_decay,  alpha=1.0, include_top=True, weights=None):\r\n    mbl = applications.mobilenet.MobileNet(weights=None, include_top=False, input_shape=shape)\r\n    x = mbl.output\r\n\r\n    model_tmp =  Model(inputs = mbl.input, outputs = x)\r\n    layer5, layer8, layer13 = model_tmp.get_layer('conv_pw_5_relu').output, model_tmp.get_layer('conv_pw_8_relu').output, model_tmp.get_layer('conv_pw_13_relu').output\r\n\r\n    fcn14 = Conv2D(filters=2 , kernel_size=1, name='fcn14')(layer13)\r\n    fcn15 = Conv2DTranspose(filters=layer8.get_shape().as_list()[-1] , kernel_size=4, strides=2, padding='same', name='fcn15')(fcn14)\r\n    fcn15_skip_connected = Add(name=\"fcn15_plus_vgg_layer8\")([fcn15, layer8])\r\n    fcn16 = Conv2DTranspose(filters=layer5.get_shape().as_list()[-1], kernel_size=4, strides=2, padding='same', name=\"fcn16_conv2d\")(fcn15_skip_connected)\r\n    # Add skip connection\r\n    fcn16_skip_connected = Add(name=\"fcn16_plus_vgg_layer5\")([fcn16, layer5])\r\n    # Upsample again\r\n    fcn17 = Conv2DTranspose(filters=4, kernel_size=16, strides=(8, 8), padding='same', name=\"fcn17\", activation=\"softmax\")(fcn16_skip_connected)\r\n\r\n    \r\n    model = Model(inputs = mbl.input, outputs = fcn17)\r\n\r\n    model.compile(optimizer=Adam(lr=lr_init, decay=lr_decay),\r\n                  loss='categorical_crossentropy',\r\n                  metrics=[mean_iou(num_classes=num_classes), dice_coef])\r\n    \r\n    return model\r\n```", "@BuiNgocHai Can you please provide a standalone code. I cannot run the current code you shared. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34610\">No</a>\n"]}, {"number": 34609, "title": "tf.function retracing for multigpu(mirrored_strategy) operations(tf.keras.models)", "body": "**System information**\r\n- Have I [written]() custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ('CentOS Linux', '7.6.1810', 'Core')\r\n- os platform: Linux-3.10.0-862.14.4.el7.x86_64-x86_64-with-centos-7.6.1810-Core\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below): unknown 2.0.0\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  cudatoolkit-10.0.130 / cudnn-7.6.4 \r\n- GPU model and memory: TITAN Xp   (12G) * 8\r\n\r\n**Describe the current behavior**\r\n\r\nCurrently I have 8 gpus in develop environment(titanxp's), and converting single gpu training model using tf.distributed.mirror_strategy & tf.function gives slow training(gpu uses) and print WARNINGS below.\r\nSince I confirmed model trains well in single gpu environment, I Wrote simple test code similar to the structure of my actual code of my model, which shows the same WARNINGS.\r\n\r\n`W1126 17:46:13.538742 139942541895424 def_function.py:474] 5 out of the last 5 calls to <function inference at 0x7f4a76ab12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxesargument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nW1126 17:46:13.584210 139942533502720 def_function.py:474] 6 out of the last 6 calls to <function inference at 0x7f4a76ab12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxesargument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nW1126 17:46:13.634693 139941979879168 def_function.py:474] 7 out of the last 7 calls to <function inference at 0x7f4a76ab12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxesargument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nW1126 17:46:13.689692 139941971486464 def_function.py:474] 8 out of the last 8 calls to <function inference at 0x7f4a76ab12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxesargument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.`\r\n\r\nI wonder if this retracing could occur from restricted network policy or outdated kernels. Since this workstation is originally intended for the service purpose so it is under strong monitoring for network packets(ports). I found mirrored_strategy open several network ports randomly at startup.\r\n\r\n**Describe the expected behavior**\r\nNo retracing warnings with fully dedicated GPU usages.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nimport os, sys\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.python.keras import backend, layers, models, utils\r\nfrom os.path import join, basename, dirname\r\nfrom os import listdir\r\ntf.__version__\r\ntf.executing_eagerly()\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()#devices=[\"/gpu:0\", \"/gpu:1\"])#devices=[\"/gpu:0\", \"/gpu:1\"])#,\r\n                                                  #cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n    \r\ndef create_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu'),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Conv2D(64, 3, activation='relu'),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n  return model\r\n\r\n\r\nwith mirrored_strategy.scope():\r\n    \r\n    inp = layers.Input([240,320,6])\r\n    \r\n    \r\n    test = create_model()\r\n    \r\n\r\n@tf.function\r\ndef inference(input):\r\n    return test(input)\r\n@tf.function\r\ndef inf():\r\n    with mirrored_strategy.scope():\r\n            mirrored_strategy.experimental_run_v2(inference, args=(tf.random.normal(shape=[32,240,320,3]),)) \r\n\r\ninf()\r\n\r\n```\r\n**Other info / logs**\r\nAlthough I remove the loss calculation and optimizations, I checked this retracing occurs irrelevant to the loss or optimization steps.", "comments": ["I checked the code on gcp compute instance with K80*8 gpu environment, and it works fine.\r\n(Same anaconda environment with tensorflow-gpu 2.0.) \r\nNow I guess this issue is caused by my development environment, but I still wonder why my environment cause retracing problem for multigpu training.", "I am not able to repro this either. Is it possible that you have somewhat different versions of TF? I tested with TF @ master.", "No, I am using tensorflow provided by anaconda.", "I would appreciate if you could tell me a effective way for tracing the problem and find the issue for myself, since it is obvious the error originates from my development environment.\r\nAt current it seems the tf.function decorator spawn another process so actually impossible to trace directly from my code.", "tf.function does not spawn another process - why do you say that? ", "Currently I am using another deeplearning framework and it runs as I expected. I cannot stop training to reproduce the results or recheck your comments since training occupies all gpus available.\r\nYou could close the issue or I will update anytime one I recheck the debugging comments.\r\nThank you for your kind support."]}, {"number": 34608, "title": " whether tflite can not support the two operator ( MaxPool2D and QUANTIZE) in TensorFlow 2.0.0 when running the mode in embed freeRTOS system?", "body": "**System information**\r\n- OS Platform and Distribution : Window 7\r\n- TensorFlow installed from : Install TensorFlow 2.0.0  by anaconda. It should be binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nBelow is the tf.lite.convert output information after convert finished.\r\n```\r\n2019-11-26 16:16:17.608537: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-11-26 16:16:17.609537: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-26 16:16:17.712547: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-26 16:16:17.712547: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 8.001ms.\r\n2019-11-26 16:16:17.712547: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-11-26 16:16:17.738550: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-11-26 16:16:17.739550: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-26 16:16:17.807557: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-26 16:16:17.808557: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 15 nodes (-4), 18 edges (-4), time = 52.005ms.\r\n2019-11-26 16:16:17.808557: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 15 nodes (0), 18 edges (0), time = 1ms.\r\nINFO: Initialized TensorFlow Lite runtime.\r\ngenerate: ./cnn.tflite\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nMy first model code:\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(filters=32,   kernel_size=[5, 5],  padding='same',\r\n                                activation=tf.nn.relu,input_shape=(feature_cnt_max, feature_dim_cnt,1)))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\r\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=[3, 3],padding='same',activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(units=num_fc_hide,activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Dense(units=num_class,activation=tf.nn.softmax))\r\nmodel.build(input_shape=(None,feature_cnt_max, feature_dim_cnt,1))\r\n\r\nMy second model code:\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(filters=186,\r\n                                kernel_size=[first_filter_height, first_filter_width],\r\n                                padding='valid',\r\n                                activation=tf.nn.relu,input_shape=(feature_cnt_max, feature_dim_cnt,1)))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(units=num_class,activation=tf.nn.softmax))\r\nmodel.build(input_shape=(None,feature_cnt_max, feature_dim_cnt,1))\r\n\r\n\r\n**Any other info / logs**\r\nI convert the two model into *tflite by below code:\r\n(https://github.com/tensorflow/tensorflow/files/3890814/covert_h5_to_tflite.txt)\r\n\r\ncovert_h5_to_tflite.py\r\nMy question is:\r\n1: When i use the first model and do not do quantization, i meet the the err info report \"Didn't find op for builtin opcode 'MaxPool2D' version '1'\" when run the the model in my Embedded freeRTOS system.\r\n2: When i use the second model and do quantization, i meet the the err info report \"Didn't find op for builtin opcode 'QUANTIZE' version '1'\" when run the the model in my Embedded freeRTOS system.\r\n3: Below code add the TF_OPS, but the tflite are same with no TF_OPS, why?\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\nSo i want to know whether tflite can not support the two operator ( MaxPool2D and QUANTIZE) in tensorflow 2.0.0? How about tensorflow 1.1x?\r\n\r\n", "comments": ["The latest status is that: i can do inference in my embed board when use float32 model( just do not quantization).  but will report below error info from board when run the quantization model by TFLiteConverter. So will i close this issue and report a new one? or going on this issue?\r\nError info:\r\n\r\n> tensorflow/lite/kernels/kernel_util.cc:54 affine_quantization->scale->size != filter->dims->data[affine_quantization->quantized_dimension] (186 != 1)\r\n> Node DEPTHWISE_CONV_2D (number 1) failed to invoke with status 1", "@raygong1207  @liyunlu0618 @haozha111 \r\nDoes anyone have a fix for this?\r\nI'm experiencing the same issue. The float model works fine. But extremely slow.\r\nI would prefer a quantized tflite model running on tflite 2.0.0 if possible.\r\n\r\nThanks in advance.", "@raygong1207 It looks like you are using an older Version of Tensorflow (2.0). Can you please execute your code using Latest Version (2.6.0) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34608\">No</a>\n"]}, {"number": 34607, "title": "ModuleNotFoundError: No module named 'tensorflow_core.keras' in Flask", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS X 10.15.1 Catalina\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.7.3\r\n- virtualenv (Flask)\r\n\r\n**Describe the problem**\r\nI'm unable to import and run a model using Flask services because of this error: ModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- I have pip installed keras and tensorflow into my virtualenv (I've also tried pip3 install)\r\n- My code is like this:\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import load_model\r\nmodel = load_model(\"model.hdf5\")\r\n# ... preprocess images\r\npredictions = model.predict(processed_images)\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/bin/flask\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/flask/cli.py\", line 966, in main\r\n    cli.main(prog_name=\"python -m flask\" if as_module else None)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/flask/cli.py\", line 586, in main\r\n    return super(FlaskGroup, self).main(*args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 717, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 1137, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 956, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/decorators.py\", line 64, in new_func\r\n    return ctx.invoke(f, obj, *args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/flask/cli.py\", line 860, in run_command\r\n    extra_files=extra_files,\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/serving.py\", line 1008, in run_simple\r\n    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 337, in run_with_reloader\r\n    reloader.run()\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 202, in run\r\n    for filename in chain(_iter_module_files(), self.extra_files):\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 24, in _iter_module_files\r\n    filename = getattr(module, \"__file__\", None)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\nUsing TensorFlow backend.\r\nUsing TensorFlow backend.\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\n2019-11-26 15:33:29.703531: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb84403e4f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-26 15:33:29.703585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-26 15:33:29.705059: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa1f9bbcd00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-26 15:33:29.705389: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\r\n\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\r\n```", "comments": ["@zzeniale , refer https://github.com/tensorflow/tensorflow/issues/32768. Looks similar. Let me know if this helps. :)", "Hi @nikochiko. Thanks for the reply. Unfortunately the solution mentioned in that thread isn't quite clear: \r\n\r\n> I was missing CUDA/cudnn on my PATH, adding those then rerunning pip install fixed the issue.\r\n> \r\n> (pip install without CUDA on path does not install: libtensorflow_framework.so.2)\r\n\r\nI was unable to find any solution online as to what path was and how to add CUDA/cudnn to it.", "I have had a similar problem when loading a model.  it seems to break  but I don't have cuda installed and need to install this on a ubuntu server with no access to cuda / gpu.  \r\nI have noticed if i turn off my debugging it will run the first time through the code but the second time break.  this is installed inside flask and I can't reproduce it outside of flask\r\n\r\n`Traceback (most recent call last):\r\n  File \"E:\\Dropbox\\Code\\wld\\API\\v1\\local_run.py\", line 3, in <module>\r\n    app.run(host='127.0.0.1', port=80, debug=True)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 990, in run\r\n    run_simple(host, port, self, **options)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\werkzeug\\serving.py\", line 1008, in run_simple\r\n    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\werkzeug\\_reloader.py\", line 337, in run_with_reloader\r\n    reloader.run()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\werkzeug\\_reloader.py\", line 202, in run\r\n    for filename in chain(_iter_module_files(), self.extra_files):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\werkzeug\\_reloader.py\", line 24, in _iter_module_files\r\n    filename = getattr(module, \"__file__\", None)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Program Files\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n`", "The Trace on the second time through shows this error...\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 1949, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 1935, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\flask_restful\\__init__.py\", line 458, in wrapper\r\n    resp = resource(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\flask\\views.py\", line 89, in view\r\n    return self.dispatch_request(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\flask_restful\\__init__.py\", line 573, in dispatch_request\r\n    resp = meth(*args, **kwargs)\r\n  File \"E:\\Dropbox\\Code\\wld\\API\\v1\\routes\\functions.py\", line 24, in post\r\n    response = local(post_data)\r\n  File \"E:\\Dropbox\\Code\\wld\\API\\v1\\functions\\pofg_xrf_nn\\v1\\__init__.py\", line 20, in local\r\n    output = xrd_nn_model(event,'tmp')\r\n  File \"E:\\Dropbox\\Code\\wld\\API\\v1\\functions\\pofg_xrf_nn\\v1\\main.py\", line 28, in xrd_nn_model\r\n    new_model = load_model(some_model,compile=False)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\saving.py\", line 492, in load_wrapper\r\n    return load_function(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\saving.py\", line 584, in load_model\r\n    model = _deserialize_model(h5dict, custom_objects, compile)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\saving.py\", line 274, in _deserialize_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\saving.py\", line 627, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\layers\\__init__.py\", line 168, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 147, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\sequential.py\", line 302, in from_config\r\n    model.add(layer)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\sequential.py\", line 162, in add\r\n    name=layer.name + '_input')\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\input_layer.py\", line 178, in Input\r\n    input_tensor=tensor)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\engine\\input_layer.py\", line 87, in __init__\r\n    name=self.name)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 73, in symbolic_fn_wrapper\r\n    if _SYMBOLIC_SCOPE.value:\r\nAttributeError: '_thread._local' object has no attribute 'value'`", "@zzeniale \r\n\r\nCan you try with `from tensorflow import keras` and see how it progresses.Also , can you check\r\n```\r\n keras_applications.__version__\r\nkeras_preprocessing.__version__\r\n\r\n```\r\nThanks!", "this seems to have worked \r\n\r\n```\r\nfrom tensorflow import keras\r\nmodel = keras.models.load_model(new_model)\r\n```\r\n\r\nvs failing\r\n```\r\nfrom keras.models import load_model\r\nmodel = load_model(new_model)\r\n```", "@ravikyram, I'm still getting the same error unfortunately:\r\n\r\nmy code is:\r\n```\r\nfrom tensorflow import keras\r\nmodel = keras.models.load_model(\"model.hdf5\")\r\n```\r\nMy `keras_applications` version is 1.0.8 and `keras_preprocessing` version is 1.1.0.\r\n\r\nThe full error traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/bin/flask\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/flask/cli.py\", line 966, in main\r\n    cli.main(prog_name=\"python -m flask\" if as_module else None)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/flask/cli.py\", line 586, in main\r\n    return super(FlaskGroup, self).main(*args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 717, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 1137, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 956, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/decorators.py\", line 64, in new_func\r\n    return ctx.invoke(f, obj, *args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/flask/cli.py\", line 860, in run_command\r\n    extra_files=extra_files,\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/serving.py\", line 1008, in run_simple\r\n    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 337, in run_with_reloader\r\n    reloader.run()\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 202, in run\r\n    for filename in chain(_iter_module_files(), self.extra_files):\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 24, in _iter_module_files\r\n    filename = getattr(module, \"__file__\", None)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2019-11-27 19:29:43.399134: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f99c9539650 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-27 19:29:43.399187: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n2019-11-27 19:29:43.769712: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd831f1ae30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-27 19:29:43.769750: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /Users/llemonthyme/Desktop/traffic-app/flaskservice/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n```", "Any solution?", "I'm getting the same issue. Simply adding the line \"from tensorflow import keras\" into my flask app.py will cause the error: \r\n`ModuleNotFoundError: No module named 'tensorflow_core.keras'`", "I am facing the same issue when running my flask app in a docker container.", "I've been struggling with this issue for the past few days, I just tried to run flask with debug mode off and the exception no longer occurs. As an annoying work around setting `FLASK_DEBUG=1` allows you to continue working but you'll need to restart your server after every change. \r\n\r\nI'm still trying to figure out why flask debug mode is causing this issue.", "This issue happens with me running a falcon app inside the `tensorflow/tensorflow:nightly-py3` docker, just when it uses `tensorflow.keras.models.load_model` method.", "I have found that down grading the tensorflow version to 1.x resolved my issue - is everyone experiencing issues using TF 2?", "@alexespencer I'm experiencing the issue in TF2, What 1.x version worked for you? I tried 1.15 and 1.14 but still had the error.", "I've got the same error, but with django though.\r\nPython 3.7.5\r\nTF 1.15.0.\r\nWith TF 1.14.0 it works fine.\r\n```\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"manage.py\", line 10, in <module>\r\n    execute_from_command_line(sys.argv)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/__init__.py\", line 364, in execute_from_command_line\r\n    utility.execute()\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/__init__.py\", line 356, in execute\r\n    self.fetch_command(subcommand).run_from_argv(self.argv)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/base.py\", line 283, in run_from_argv\r\n    self.execute(*args, **cmd_options)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/commands/runserver.py\", line 61, in execute\r\n    super(Command, self).execute(*args, **options)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/base.py\", line 330, in execute\r\n    output = self.handle(*args, **options)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/commands/runserver.py\", line 100, in handle\r\n    self.run(**options)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/commands/runserver.py\", line 109, in run\r\n    autoreload.main(self.inner_run, None, options)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 341, in main\r\n    reloader(wrapped_main_func, args, kwargs)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 307, in python_reloader\r\n    reloader_thread()\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 277, in reloader_thread\r\n    change = fn()\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 206, in code_changed\r\n    for filename in gen_filenames():\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 104, in gen_filenames\r\n    [filename.__file__ for filename in new_modules\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 105, in <listcomp>\r\n    if hasattr(filename, '__file__')])\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n```\r\n\r\nWhich is super strange, because when I call the import directly from a python interpreter in terminal - it works:\r\n```\r\nfrom tensorflow import keras\r\n```\r\nEven loading a model works.", "I have the same error with Python 3.7.5, TensorFlow 2.0.0, Flask 1.1.1 on macOS 10.14.6.\r\n\r\nUnfortunately, the error does not occur every time.  Sometimes it will take up to 100 reload/req cycles to cause the exception:\r\n\r\n```bash\r\ni=0 ; while true ; do i=$((i+1)); echo ${i}; touch app.py ; sleep 1 ; curl -s -o /dev/null http://localhost:5000 ; done\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/testuser/web/v/bin/flask\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/flask/cli.py\", line 966, in main\r\n    cli.main(prog_name=\"python -m flask\" if as_module else None)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/flask/cli.py\", line 586, in main\r\n    return super(FlaskGroup, self).main(*args, **kwargs)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/click/core.py\", line 717, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/click/core.py\", line 1137, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/click/core.py\", line 956, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/click/decorators.py\", line 64, in new_func\r\n    return ctx.invoke(f, obj, *args, **kwargs)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/flask/cli.py\", line 860, in run_command\r\n    extra_files=extra_files,\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/werkzeug/serving.py\", line 1008, in run_simple\r\n    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 337, in run_with_reloader\r\n    reloader.run()\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 202, in run\r\n    for filename in chain(_iter_module_files(), self.extra_files):\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/werkzeug/_reloader.py\", line 24, in _iter_module_files\r\n    filename = getattr(module, \"__file__\", None)\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/Users/testuser/web/v/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/Users/testuser/.pyenv/versions/3.7.5/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n```\r\n\r\nThe Werkzeug [`_reloader` is looping over `sys.modules.values()`](https://github.com/pallets/werkzeug/blob/master/src/werkzeug/_reloader.py#L21).\r\n\r\nEvery keras submodule is in the list.  Is it interesting that `tensorflow_core.keras` occasionally appears twice?  It is the only one duplicated in the occurrence below.\r\n\r\n```python\r\nimport sys\r\nk = [ str(m) for m in list(sys.modules.values()) if 'tensorflow_core.keras' in str(m) ]\r\nlen(k) # => 47\r\nk[0:3]\r\n[<module 'tensorflow_core.keras' from '/tmp/app/v/lib/python3.7/site-packages/tensorflow_core/python/keras/api/_v2/keras/__init__.py'>,\r\n <module 'tensorflow_core.keras' from '/tmp/app/v/lib/python3.7/site-packages/tensorflow_core/python/keras/api/_v2/keras/__init__.py'>,\r\n <module 'tensorflow_core.keras.activations' from '/tmp/app/v/lib/python3.7/site-packages/tensorflow_core/python/keras/api/_v2/keras/activations/__init__.py'>]\r\n```\r\n\r\nAlso, without `list()`, the above code warns the `sys.modules` dictionary has changed during iteration.  Werkzeug knows about this and uses `list()` as well.\r\n\r\nI am not using Django, but it was mentioned up-thread, so I briefly skimmed its reloader.  It similarly watches all modules by [looping over `sys.modules` keys](https://github.com/django/django/blob/master/django/utils/autoreload.py#L103-L105).\r\n\r\nI'm not sure if this is a \"bug\" in Tensorflow, or a consequence of how other code is reloading modules.  I know very little about the Python module system.\r\n\r\n", "> @alexespencer I'm experiencing the issue in TF2, What 1.x version worked for you? I tried 1.15 and 1.14 but still had the error.\r\n\r\nSorry for the delay. I had to roll back to Tensorflow 1.13.1 to get this to work.", "I also tested with `tensorflow==2.0.0` both with python `3.7.5` and `3.7.3` - still the same :(", "..Also tested on `tensorflow-cpu==1.5.0`, python `3.7.5` same story.\r\n@ymodak - added labels does not reflects the real state. For sure not only mac are affected. I work on Linux Ubuntu 16.04 - same problem. \r\nFor sure this is a cross-version issue and it started between tensorflow `1.14.0` and `1.15.0` and it is still valid in `2.0.0`.", "Hi!\r\nI have the same problem but on Raspbian.\r\n\r\nSystem:\r\n```\r\nOS: Raspbian GNU/Linux 10 (buster) armv7l \r\nHost: Raspberry Pi 4 Model B Rev 1.1 \r\nKernel: 4.19.75-v7l+ \r\nUptime: 3 days, 2 hours, 5 mins \r\nPackages: 662 (dpkg) \r\nShell: bash 5.0.3 \r\nTerminal: /dev/pts/0 \r\nCPU: BCM2835 (4) @ 1.500GHz \r\nMemory: 125MiB / 3906MiB \r\n```\r\nPython:\r\n```\r\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n[GCC 8.2.0] on linux\r\n```\r\n\r\nEnv:\r\n```\r\nabsl-py==0.9.0                                                                                                                                                                                                       \r\nastor==0.8.1                                                                                                                                                                                                         \r\nattrs==19.3.0                                                                                                                                                                                                        \r\nbleach==3.1.0                                                                                                                                                                                                        \r\ncachetools==4.0.0                                                                                                                                                                                                    \r\ncertifi==2019.11.28                                                                                                                                                                                                  \r\ncffi==1.13.2                                                                                                                                                                                                         \r\nchardet==3.0.4                                                                                                                                                                                                       \r\nClick==7.0                                                                                                                                                                                                           \r\ncryptography==2.8                                                                                                                                                                                                    \r\ncycler==0.10.0                                                                                                                                                                                                       \r\ndecorator==4.4.1                                                                                                                                                                                                     \r\ndocutils==0.15.2                                                                                                                                                                                                     \r\nFlask==1.1.1                                                                                                                                                                                                         \r\ngast==0.2.2                                                                                                                                                                                                          \r\ngoogle-auth==1.10.0                                                                                                                                                                                                  \r\ngoogle-auth-oauthlib==0.4.1                                                                                                                                                                                          \r\ngoogle-pasta==0.1.8                                                                                                                                                                                                  \r\ngrpcio==1.26.0                                                                                                                                                                                                       \r\nh5py==2.10.0                                                                                                                                                                                                         \r\nidna==2.8                                                                                                                                                                                                            \r\nimageio==2.6.1                                                                                                                                                                                                       \r\nimportlib-metadata==1.3.0                                                                                                                                                                                            \r\nitsdangerous==1.1.0                                                                                                                                                                                                  \r\njeepney==0.4.1                                                                                                                                                                                                       \r\nJinja2==2.10.3                                                                                                                                                                                                       \r\njoblib==0.14.1                                                                                                                                                                                                       \r\nKeras==2.2.4                                                                                                                                                                                                         \r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeyring==20.0.0\r\nkiwisolver==1.1.0\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.1.2\r\nmore-itertools==8.0.2\r\nnetworkx==2.4\r\nnumpy==1.16.2\r\noauthlib==3.1.0\r\nopencv-python==3.4.7.28\r\nopt-einsum==3.1.0\r\npackaging==19.2\r\nPassportEye==1.4.1\r\npdfminer3k==1.3.1\r\nPillow==6.2.1\r\npkg-resources==0.0.0\r\npkginfo==1.5.0.1\r\npluggy==0.13.1\r\nply==3.11\r\nprotobuf==3.11.2\r\npy==1.8.0\r\npy-agender==0.0.9\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.7\r\npycparser==2.19\r\nPygments==2.5.2\r\npyparsing==2.4.5\r\npytesseract==0.3.1\r\npytest==5.3.2\r\npython-dateutil==2.8.1\r\nPyWavelets==1.1.1\r\nPyYAML==5.2\r\nreadme-renderer==24.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nrequests-toolbelt==0.9.1\r\nrsa==4.0\r\nscikit-image==0.16.2\r\nscikit-learn==0.22\r\nscipy==1.4.1\r\nSecretStorage==3.1.1\r\nsix==1.13.0\r\ntensorboard==2.0.2\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntermcolor==1.1.0\r\ntqdm==4.41.0\r\ntwine==3.1.1\r\nurllib3==1.25.7\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nWerkzeug==0.16.0\r\nwrapt==1.11.2\r\nzipp==0.6.0\r\n```", "same here with `Flask==1.1.1` `tensorflow==1.15.0` in a ubuntu 18.04 docker. I have to set `FLASK_DEBUG=0` to let it work.", "same here with `tensorflow==2.0.0` and `django==2.1.13`.\r\n\r\nPlacing this in my `manage.py` this seems to temporarily remove the issue:\r\n```\r\nfrom gevent import monkey\r\nmonkey.patch_all()\r\n```", "> \r\n> \r\n> I am facing the same issue when running my flask app in a docker container.\r\n\r\nIt worked when using python 3.6.", "As other comments have mentioned, a similar error happens in Django as well.\r\nI am facing the same issue with:\r\n\r\n```\r\nPython 3.6.1\r\nDjango 2.0.3\r\n```\r\n\r\nThanks to other comments I have found it only happens with\r\n`tensorflow==1.15.0` and `tensorflow==2.0`\r\nHence, a possible workaround is to downgrade to `tensorflow==1.14.0` or below.\r\n\r\nAnother workaround I have found, which is ugly as well, is to include\r\n\r\n`import tensorflow as tf`\r\n\r\nin the import section of the app's `manage.py`\r\n\r\nI hope that information could also help to eventually resolve the bug.", "Thanks @enriqueav I worked I was facing this same error with:\r\n\r\npython 3.6.4\r\nDjango 1.11\r\nTensorflow 2\r\n\r\nApparently as you said when placing the import:\r\n\r\nimport tensorflow as tf\r\n\r\nIn the apps section manage.py\r\nThe django server runs without any problem.\r\n\r\n\r\n", "Any functioning workaround for flask yet? Same error here when flask debug mode is tuned on:\r\n`    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n`\r\n\r\nThanks!", "@seabass86 see my comment above it works with the most recent release of Tensor Flow and Flask", "> I've been struggling with this issue for the past few days, I just tried to run flask with debug mode off and the exception no longer occurs. As an annoying work around setting `FLASK_DEBUG=1` allows you to continue working but you'll need to restart your server after every change.\r\n> \r\n> I'm still trying to figure out why flask debug mode is causing this issue.\r\n\r\n\r\n@alexdibattista Thank you for quick fix.\r\n", "Same issue here. Just FLASK_DEBUG=0 helps", "> @seabass86 see my comment above it works with the most recent release of Tensor Flow and Flask\r\n\r\nThanks @adclose , but the issue sadly remains even with version 2.1:\r\n```\r\n  return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n```\r\n", "Hello everyone, \r\nEven I was getting the same error in a flask application.\r\nI did the following and it worked,\r\n`pip uninstall tensorflow`\r\n`pip install tensorflow==2.0`.", "> Hello everyone,\r\n> Even I was getting the same error in a flask application.\r\n> I did the following and it worked,\r\n> `pip uninstall tensorflow`\r\n> `pip install tensorflow==2.0`.\r\n\r\nDoes not work. I tried this and still getting same error.", "Has anybody gleaned any insight into the problem?\r\n\r\nIt seems that enough people are taxed by it to warrant an economic incentive for its fix.  Any interest in pooling together to establish a reasonable bug bounty?", "I retrained the model with `tensorflow==1.14.0` then saved the model and used `gunicorn` to run flask. This solved my problem. ", "> I retrained the model with `tensorflow==1.14.0` then saved the model and used `gunicorn` to run flask. This solved my problem.\r\n\r\nThis does not solve the problem - you should be able to work with TensorFlow and flask in debug mode", "I've been having this same issue while using the docker container of tensorflow-latest-gpu-py3 when trying to load anything from keras, it shows the ModuleNotFoundError\r\n\r\nEDIT: I tried with different versions of TensorFlow official docker containers and at least in all of 2.x-gpu I've tried, the module keras doesn't even appear as an available option when autocompleting after `tf.k`. Strangely enough, the tf-nightly container works ok.", "I'm also facing the same issue when using TensorFlow v2.1.0 and 2.0.0 with Flask in Docker & Docker-Compose. As mentioned in above threads, setting FLASK_DEBUG=0 worked for my case.\r\n\r\nAnother way that worked for my case and allows keeping FLASK_DEBUG=1 during development is to use package versions `Flask==1.1.1` and `tensorflow==2.0.0-beta1`. By the way, I'm using this flask+docker image from [uwsgi-nginx-flask-docker](https://github.com/tiangolo/uwsgi-nginx-flask-docker)", "I encountered the same error recently, using Tensorflow 2.1 GPU in Ubuntu 19.10.\r\n\r\nWhen I ran:\r\n`sudo python3 -c 'import tensorflow.keras as keras; print(keras.__version__)'`\r\nit worked; but, running: \r\n`python3 -c 'import tensorflow.keras as keras; print(keras.__version__)'`\r\nwithout the `sudo` prefix gave me this error.\r\n\r\nHere is how I fixed it. In my case, I correctly identified that the user account did not have the necessary tensorflow.keras python dependencies installed.\r\n\r\nI resolved this issue by running:\r\n`python3 -m pip install grpcio  grpcio-tools cycler kiwisolver \\\r\n                                     absl-py astunparse gast matplotlib     \\\r\n                                     google-pasta opt-einsum                   \\\r\n                                     tensorboard tensorflow-estimator      \\\r\n                                     termcolor wrapt Cython                      \\\r\n                                     setuptools pybind11 Sphinx tables`\r\n`python3 -m pip install -U --user pip six numpy wheel setuptools mock 'future>=0.17.1'`\r\n`python3 -m pip install -U --user keras_applications --no-deps keras_preprocessing --no-deps`\r\n\r\nNOTE: All of the python package installation steps above DO NOT have the `sudo` prefix.", "The recommended way to prevent this is to:\r\n\r\n1. install using `python -m pip install ...`\r\n2. check using `python -m pip list`\r\n\r\nOr, to use virtualenvs or similar.", "As @enriqueav commented in the workaround for \"manage.py\" Django.\r\n\r\nIn the Flask you should put \"`import tensorflow as tf`\" in the \"`__init__.py`\" file.\r\n\r\nWorks perfectly for Flask==1.1.1 and tensorflow==2.1.0", "A workaround for this issue that works for me is to run the TensorFlow part within a separate process\r\nSomething like;\r\n```\r\nfrom multiprocessing import Pool\r\nwith Pool() as pool:\r\n    result = pool.starmap(function_that_runs_tf, [(var1, var2..._),(...)]\r\n```", "**UPDATE: for those using Flask, TF2, and gunicorn** \r\n\r\nFor those running flask through gunicorn, I found that running\r\n```\r\ngunicorn -b 0.0.0.0:5000 -w 4\r\n        --access-logfile -\r\n        --reload\r\n        app:app\r\n```\r\nwas causing issues with the `--reload` flag. So basically you would get \r\nthe `ModuleNotFoundError: No module named 'tensorflow_core.keras'` error.\r\nBut if you removed the `--reload` flag it would fix the error. However, while developing your app, you want to be able to use `--reload`. I was using gunicorn = \"==19.9.0\". \r\n\r\nBut, when I upgraded to `gunicorn = \"==20.0.4\"` everything was fixed. Here are some of the package versions I was using.\r\n\r\n```\r\nFlask==1.1.1\r\nFlask-CeleryExt==0.3.4\r\nFlask-Cors==3.0.8\r\nFlask-SQLAlchemy==2.4.1\r\ngunicorn==20.0.4\r\nnumpy==1.18.1\r\nPillow==7.0.0\r\nprotobuf==3.11.3\r\nscikit-learn==0.22.1\r\nscipy==1.4.1\r\ntensorboard==2.1.0\r\ntensorflow==2.1.0\r\ntensorflow-estimator==2.1.0\r\n```", "> **UPDATE: for those using Flask, TF2, and gunicorn**\r\n> \r\n> For those running flask through gunicorn, I found that running\r\n> \r\n> ```\r\n> gunicorn -b 0.0.0.0:5000 -w 4\r\n>         --access-logfile -\r\n>         --reload\r\n>         app:app\r\n> ```\r\n> \r\n> was causing issues with the `--reload` flag. So basically you would get\r\n> the `ModuleNotFoundError: No module named 'tensorflow_core.keras'` error.\r\n> But if you removed the `--reload` flag it would fix the error. However, while developing your app, you want to be able to use `--reload`. I was using gunicorn = \"==19.9.0\".\r\n> \r\n> But, when I upgraded to `gunicorn = \"==20.0.4\"` everything was fixed. Here are some of the package versions I was using.\r\n> \r\n> ```\r\n> Flask==1.1.1\r\n> Flask-CeleryExt==0.3.4\r\n> Flask-Cors==3.0.8\r\n> Flask-SQLAlchemy==2.4.1\r\n> gunicorn==20.0.4\r\n> numpy==1.18.1\r\n> Pillow==7.0.0\r\n> protobuf==3.11.3\r\n> scikit-learn==0.22.1\r\n> scipy==1.4.1\r\n> tensorboard==2.1.0\r\n> tensorflow==2.1.0\r\n> tensorflow-estimator==2.1.0\r\n> ```\r\n\r\nSame here! Also giving more ram to Docker made my flask app start, otherwise the workers would just restart indefinitely", "I'm also running into this issue with the `flask run` command in development with the latest versions of tensorflow and flask:\r\n`FLASK_ENV=development flask run`\r\n\r\n\r\n", "Flask==1.1.1\r\npdfminer.six==20200124\r\nlmdb==0.98\r\nflask-cors==3.0.8\r\nnumpy==1.17.4\r\ntensorflow==2.1.0\r\nkeras\r\nnltk\r\n\r\nenvironment:\r\n- FLASK_DEBUG=0\r\n- FLASK_APP_CONFIG=Product\r\n\r\ndid the trick for me.\r\n\r\n", "`pip uninstall tensorflow` after that `pip install tensorflow` worked for me", "I stopped getting ModuleNotFoundError: No module named 'tensorflow_core.keras' in Flask when I used\r\n\r\n```\r\nexport FLASK_APP=your_app_name\r\n\r\nflask run\r\n```\r\ni.e didn't use `export FLASK_ENV=development`", "@svensevenslow Thanks man, it's solved my issue.", "Can we close this then, since it is solved and it is actually a flask issue?", "@mihaimaruseac I've seen several people mention that it also affects the Django reloader. I'm wondering if there's a way to modify TensorFlow so that both the Flask and Django reloaders will work with it.", "It is worth noting that Flask uses Werkzeug. I wonder if this issue is reproducible with the Werkzeug reloader. My bet is that it would be.", "I do have an app that uses TF Lite with Flask and I haven't be able to reproduce this error with that combination. That might be another work around for folks.", "Filled this issue on the Flask project in case they can help:\r\nhttps://github.com/pallets/flask/issues/3549", "It looks like passing --eager-loading to the `flask run` command is another work around.", "Here's a minimal, reproducible example of this error that only imports tensorflow. EDIT: made even smaller\r\n\r\n```python\r\nimport sys\r\nfrom threading import Thread\r\nimport time\r\n\r\ndef loader():\r\n    import tensorflow\r\n\r\nt = Thread(target=loader, args=())\r\nt.start()\r\n\r\nwhile True:\r\n    for module in list(sys.modules.values()):\r\n        getattr(module, \"__file__\", None)\r\n    time.sleep(1)\r\n```", "@mulka :sparkles: Beautiful!  From your minimal repro, I wondered if import is thread-safe. There is an [unconfirmed bug on the CPython tracker](https://bugs.python.org/issue38884).  The repro there also uses tensorflow, so it's as yet unclear if this is a tensorflow problem or a Python problem.", "Thank you for the reproduction.\r\n\r\nIt seems this is going to be solved in 2.2, but it happens in 1.15 and 2.1. Unfortunately, I'm not sure if we can backport a fix safely.\r\n\r\n```\r\n(env) [tf] \u03bb pip install -q tensorflow-cpu==2.2.0rc2\r\n(env) [tf] \u03bb python test.py \r\n(env) [tf] \u03bb pip uninstall -y tensorflow-cpu\r\nFound existing installation: tensorflow-cpu 2.2.0rc2\r\nUninstalling tensorflow-cpu-2.2.0rc2:\r\n  Successfully uninstalled tensorflow-cpu-2.2.0rc2\r\n(env) [tf] \u03bb pip install -q tensorflow-cpu==2.1\r\n(env) [tf] \u03bb python test.py \r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    getattr(module, \"__file__\", None)\r\n  File \"/tmp/tf/env/lib64/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/tmp/tf/env/lib64/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib64/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n(env) [tf] \u03bb pip uninstall -y tensorflow-cpu\r\nFound existing installation: tensorflow-cpu 2.1.0\r\nUninstalling tensorflow-cpu-2.1.0:\r\n  Successfully uninstalled tensorflow-cpu-2.1.0\r\n(env) [tf] \u03bb pip install -q tensorflow-cpu==1.15\r\n(env) [tf] \u03bb python test.py \r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    getattr(module, \"__file__\", None)\r\n  File \"/tmp/tf/env/lib64/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/tmp/tf/env/lib64/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib64/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n```\r\n\r\n(where `test.py` is the code in the reproducer)", "Decreasing the tensorflow version to version 1.14.0 by command pip install tensorflow == 1.14.0 solved the issue for me\r\n", "Recommend upgrading to 2.2 where this issue is also fixed (rc3 is available for testing now, final release will be early next week). This is because 1.14 release has a large number of bugs and inconsistencies and is already outside the support window. Sure, switching to 2.2 means upgrading to use TF2.0 APIs but this is also something we recommend.", "> I've got the same error, but with django though.\r\n> Python 3.7.5\r\n> TF 1.15.0.\r\n> With TF 1.14.0 it works fine.\r\n> \r\n> ```\r\n> Using TensorFlow backend.\r\n> Traceback (most recent call last):\r\n>   File \"manage.py\", line 10, in <module>\r\n>     execute_from_command_line(sys.argv)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/__init__.py\", line 364, in execute_from_command_line\r\n>     utility.execute()\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/__init__.py\", line 356, in execute\r\n>     self.fetch_command(subcommand).run_from_argv(self.argv)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/base.py\", line 283, in run_from_argv\r\n>     self.execute(*args, **cmd_options)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/commands/runserver.py\", line 61, in execute\r\n>     super(Command, self).execute(*args, **options)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/base.py\", line 330, in execute\r\n>     output = self.handle(*args, **options)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/commands/runserver.py\", line 100, in handle\r\n>     self.run(**options)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/core/management/commands/runserver.py\", line 109, in run\r\n>     autoreload.main(self.inner_run, None, options)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 341, in main\r\n>     reloader(wrapped_main_func, args, kwargs)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 307, in python_reloader\r\n>     reloader_thread()\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 277, in reloader_thread\r\n>     change = fn()\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 206, in code_changed\r\n>     for filename in gen_filenames():\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 104, in gen_filenames\r\n>     [filename.__file__ for filename in new_modules\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/django/utils/autoreload.py\", line 105, in <listcomp>\r\n>     if hasattr(filename, '__file__')])\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"/opt/anaconda2/envs/ocr_ws_dev/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\n> ModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n> ```\r\n> \r\n> Which is super strange, because when I call the import directly from a python interpreter in terminal - it works:\r\n> \r\n> ```\r\n> from tensorflow import keras\r\n> ```\r\n> \r\n> Even loading a model works.\r\n\r\nI has a similar problem while integrating keras model to django framework.\r\nUsing TensorFlow backend.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"manage.py\", line 21, in <module>\r\n    main()\r\n  File \"manage.py\", line 17, in main\r\n    execute_from_command_line(sys.argv)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 381, in execute\r\n_from_command_line\r\n    utility.execute()\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 375, in execute\r\n\r\n    self.fetch_command(subcommand).run_from_argv(self.argv)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\core\\management\\base.py\", line 316, in run_from_ar\r\ngv\r\n    self.execute(*args, **cmd_options)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\core\\management\\commands\\runserver.py\", line 60, i\r\nn execute\r\n    super().execute(*args, **options)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\core\\management\\base.py\", line 353, in execute\r\n    output = self.handle(*args, **options)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\core\\management\\commands\\runserver.py\", line 95, i\r\nn handle\r\n    self.run(**options)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\core\\management\\commands\\runserver.py\", line 102,\r\nin run\r\n    autoreload.main(self.inner_run, None, options)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\utils\\autoreload.py\", line 323, in main\r\n    python_reloader(wrapped_main_func, args, kwargs)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\utils\\autoreload.py\", line 302, in python_reloader\r\n\r\n    reloader_thread()\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\utils\\autoreload.py\", line 274, in reloader_thread\r\n\r\n    change = fn()\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\utils\\autoreload.py\", line 203, in code_changed\r\n    for filename in gen_filenames():\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\utils\\autoreload.py\", line 101, in gen_filenames\r\n    [filename.__file__ for filename in new_modules\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\django\\utils\\autoreload.py\", line 102, in <listcomp>\r\n    if hasattr(filename, '__file__')])\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\pc\\Anaconda3\\envs\\kr\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n```\r\n\r\nBut I don't know how to fix it. Longing for your reply!", "@TDYe123 solution was mentioned 1 and 2 comments above yours. Either downgrade to 1.14 (not recommended) or upgrade to 2.2 release candidate (final to be released soon)", "It works! Thanks a lot!\n\n\n| |\n\u53f6\u7530\u5730\n|\n|\n\u90ae\u7bb1\uff1a18365225454@163.com\n|\n\n\u7b7e\u540d\u7531 \u7f51\u6613\u90ae\u7bb1\u5927\u5e08 \u5b9a\u5236\n\nOn 04/22/2020 00:43, Mihai Maruseac wrote:\n\n@TDYe123 solution was mentioned 1 and 2 comments above yours. Either downgrade to 1.14 (not recommended) or upgrade to 2.2 release candidate (final to be released soon)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.", "@mihaimaruseac I upgraded to 2.2.0-rc3 but I'm running into the same issue\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py\", line 583, in spawn_worker\r\n    worker.init_process()\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py\", line 162, in init_process\r\n    super().init_process()\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py\", line 119, in init_process\r\n    self.load_wsgi()\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py\", line 144, in load_wsgi\r\n    self.wsgi = self.app.wsgi()\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py\", line 67, in wsgi\r\n    self.callable = self.load()\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py\", line 49, in load\r\n    return self.load_wsgiapp()\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py\", line 39, in load_wsgiapp\r\n    return util.import_app(self.app_uri)\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py\", line 358, in import_app\r\n    mod = importlib.import_module(module)\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py\", line 3, in <module>\r\n    app = scoring_server.init(pyfunc.load_model(\"/opt/ml/model/\"))\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py\", line 292, in load_model\r\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/keras.py\", line 338, in _load_pyfunc\r\n    keras_module = importlib.import_module(f.read())\r\n  File \"/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core'\r\n```\r\n\r\nTrying to deploy a keras model to sagemaker", "@Ben-Epstein `pip list` should not list any packages that depend on TF before 2.2", "@MihailSalnikov  Thanks, you're correct in your reply [here](https://github.com/tensorflow/tensorflow/issues/36441) \r\n\r\nI had old dependencies. Uninstalling everything tf and reinstalling with 2.2.0rc3 fixed my problem. Thank you", "Closing this issue as it is solved in 2.2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34607\">No</a>\n", "Ran into the same issue using docker container and I have solved it. The problem was caused when my ENTRYPOINT in the dockerfile was:\r\n`FLASK_APP=api.py  FLASK_ENV=development flask run --host=0.0.0.0`\r\nI have salved the issue (running in development) by simply just setting the entrypoint to `[\"python\", \"api.py\"]`"]}, {"number": 34606, "title": "Memory leak when append predictions to list", "body": "When append prediction tensors to list, gpu memory will gradually increase, the occupied memory (model memory) will not be released (even when move the model prediction in @tf.function)\r\n```\r\nfor data in dataset:\r\n    pred = model(...)\r\n    preds.append(pred)\r\n```\r\nthis works as expected\r\n```\r\nfor data in dataset:\r\n  pred = model(...)\r\n  preds.append(np.array(pred))\r\n```", "comments": ["@kindernerd, Please provide the complete standalone code to analyze the reported issue and also mention Tensorflow version. Thanks!", "@kindernerd, Could you elaborate your issue by providing complete code.Thanks  ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]