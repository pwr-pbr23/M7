[{"number": 8330, "title": "Uncatchable exception messages when using slice_input_producer and batch", "body": "Hello,\r\n\r\n`Out of range` messages are being thrown by the code below, and they seem to be uncatchable \u2014 I even tried enclosing the whole code with `try:`, `except: pass`, but the messages are still printed. While they don't really seem to affect the rest of the code, since we are taking the exact amount of examples available, these messages are quite annoying because with a big pipeline they can get really messy and ruin real-time visualization of logs (the number of errors has a relation with the number of threads).\r\n\r\nIf I try to evaluate an extra epoch, then TF raises an exception I can catch, because this time I tried to evaluate an example I don't have, but this isn't the case here.\r\n\r\n**Operating System:** Debian 4.8.15-2\r\n**Installed version of CUDA and cuDNN:** CUDA 8, cuDNN 5\r\n**python3 -c \"import tensorflow; print(tensorflow.__version__)\"**: 1.0.0\r\n\r\n### Reproducible example\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nnum_epochs = 6\r\n\r\na = ([tf.constant(i) for i in range(2)],[tf.constant(i) for i in range(2)])\r\n\r\nq1 = tf.train.slice_input_producer(a, num_epochs=num_epochs, shuffle=True, capacity=4)\r\nq2 = tf.train.batch(q1, batch_size=2, num_threads=2, enqueue_many=False, capacity=4, allow_smaller_final_batch=True)\r\n\r\ninit = [tf.global_variables_initializer(), tf.local_variables_initializer()]\r\nsess = tf.Session()\r\ncoord = tf.train.Coordinator()\r\nsess.run(init)\r\nthreads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\ntest_number = 1\r\nfor i in range(num_epochs):\r\n\tprint('Testing %d' % test_number)\r\n\tignore = sess.run(q2)\r\n\ttest_number = test_number + 1\r\n\ttime.sleep(3)\r\nprint('Done.')\r\n```\r\n\r\n### Output\r\n\r\n```\r\n(...initialization messages...)\r\nTesting 1\r\nW tensorflow/core/framework/op_kernel.cc:993] Out of range: Reached limit of 6\r\n         [[Node: input_producer/input_producer/fraction_of_4_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/input_producer/fraction_of_4_full/limit_epochs/epochs\"], limit=6, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/input_producer/fraction_of_4_full/limit_epochs/epochs)]]\r\nTesting 2\r\nTesting 3\r\nTesting 4\r\nW tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_input_producer/input_producer/fraction_of_4_full/fraction_of_4_full' is closed and has insufficient elements (requested 1, current size 0)\r\n         [[Node: input_producer/fraction_of_4_full_Dequeue = QueueDequeueV2[component_types=[DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/input_producer/fraction_of_4_full/fraction_of_4_full)]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_input_producer/input_producer/fraction_of_4_full/fraction_of_4_full' is closed and has insufficient elements (requested 1, current size 0)\r\n         [[Node: input_producer/fraction_of_4_full_Dequeue = QueueDequeueV2[component_types=[DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/input_producer/fraction_of_4_full/fraction_of_4_full)]]\r\nTesting 5\r\nTesting 6\r\nDone.\r\n```\r\n\r\nThese messages may vary: on multiple runs of the same code, the first one sometimes isn't printed, and the number of messages after `Testing 4` also changes.\r\n\r\nThe documentation of `batch` states:\r\n> The returned operation is a dequeue operation and will throw tf.errors.OutOfRangeError if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.\r\n\r\nHowever, no further instruction is given. And since a `try: except:` didn't work, I'm guessing this is a bug. Could anyone clarify this behavior?\r\n\r\nThanks in advance.", "comments": ["Complementary information: These messages are being thrown by the threads created with `start_queue_runners`, that is why they aren't linked to the main program and cannot be catched/traced. There seems to be no parameter of `start_queue_runners` to treat this. There are a few options I can think of in order to contour the problem, but they are _waaaaaaaay too hacky_ \u2014 like manually creating threads to feed the queues or intercepting the threads created by TF before they start running and encapsulating them.", "@mrry, could you please take a look?", "@MicaelCarvalho \r\nI had the same uncatchable error with tf.train.string_input_producer. \r\nIt was solved when I set num_epochs to None.\r\nFrom the [documentation](https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer), it seems to have an 'automation' effect: \r\n\r\n> num_epochs: An integer (optional). If specified, string_input_producer produces each string from string_tensor num_epochs times before generating an OutOfRange error. If not specified, string_input_producer can cycle through the strings in string_tensor an unlimited number of times.\r\n\r\nBut I am not yet sure that this change is correct from an application perspective and I don't have an explanation of why it solved the uncatchable exception. \r\nLooking forward to any precise insights on this issue..", "@ymrabet that is not the solution. :-)\r\n\r\nSetting num_epochs to None makes the threads continuously feed the queue, without ever stopping. This way, the queue never gets emptied and the problem doesn't happen.\r\n\r\nHowever, in some situations it is desirable to precisely control the number of epochs, specially when shuffle is enabled, to ensure every sample was seen N times; and in these situations the uncatchable exception messages occur. It won't actually pose big problems, since it's just a message being printed, but it seems to be impossible to block the printing of it (except with some hacky solution like setting high log levels throughout the application, and losing all important logs of tensorflow).", "@MicaelCarvalho Thanks for the explanation :-)\r\nHow can we be sure that it's only a display issue with no other impact?\r\n", "@ymrabet the problem happens because `slice_input_producer` only generates the specified number of epochs of input, while `batch` has no parameter to control the number of epochs, therefore it \"runs forever\".\r\n\r\nInternally, `batch` creates threads to feed an input queue, from which batches are taken. These threads keep pulling samples from the input tensor indefinitely. The problem occurs when they try to pull a sample and `slice_input_producer` has finished its job (i.e. there are no samples left, `slice_input_producer`'s pool is empty).\r\n\r\nThe exception is then uncatchable and intractable because it is raised inside each of these threads, when they try to get a new sample and there is none left. Each thread, after raising the exception, has no predefined way of treating it (and that's the problem), so it goes all the way up to the interpreter, which by default prints the exception and kills \"the program\". Since each thread is individually handled, \"the program\" means \"the thread\"; so the thread gets terminated and the error message printed.\r\n\r\nThat being said, the number of messages one sees should match exactly the number of threads created by `batch`, because it is only printed when a thread dies (but weirdly it doesn't always match, and that's something to be investigated maybe). It has no other impact because each thread dies individually, without affecting the main program, and the expected behavior _in this case_ would be closing the threads, which is happening anyway because of the exception.", "We're replacing Queues with tf.contrib.datasets in the near future. This should take care of weaknesses such as this.\r\n\r\nI'm closing this issue, thanks @MicaelCarvalho for the great explanations! "]}, {"number": 8329, "title": "set out_depth and pad_depth in PoolParameters if depth_window == 1", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/8313", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 8328, "title": "orthogonal_initializer() on GPU, horrible error message", "body": "Works fine on CPU.\r\n\r\n**Error message is horrible**, without any hints towards `orthogonal_initializer`, or line it is used on, or GPU, or essence of the problem.\r\n\r\nProblems like this (not the first one for me) can only be handled as theory-by-theory manual search by user.\r\n\r\nIsolated example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef strange_assign():\r\n\tconfig = tf.ConfigProto()\r\n\tconfig.allow_soft_placement = True\r\n\tsess = tf.InteractiveSession(config=config)\r\n\twith tf.device(\"gpu:0\"):\r\n\t\tg1 = tf.get_variable(\"g1\", [2,2], tf.float32, tf.constant_initializer(1.0))\r\n\t\tg2 = tf.get_variable(\"g2\", [2,2], tf.float32, tf.zeros_initializer())\r\n\t\tg3 = tf.get_variable(\"g3\", [2,2], tf.float32, tf.ones_initializer())\r\n\t\tg4 = tf.get_variable(\"g4\", [2,2], tf.float32, tf.orthogonal_initializer(1.0))\r\n\t\tg5 = tf.get_variable(\"g5\", [2,2], tf.float32, tf.random_normal_initializer())\r\n\t\tg6 = tf.get_variable(\"g6\", [2,2], tf.float32, tf.random_uniform_initializer())\r\n\r\n\ttf.global_variables_initializer().run()\r\n\tfor test in [g1,g2,g3,g4,g5,g6]:\r\n\t\tt = sess.run(test)\r\n\t\tprint(\"ASSIGN TEST\", test.name)\r\n\t\tph = tf.placeholder(tf.float32, t.shape)\r\n\t\ttry:\r\n\t\t\tsess.run( [tf.assign(test, ph)] , feed_dict = { ph: t })\r\n\t\t\tprint(\"OK\")\r\n\t\texcept:\r\n\t\t\tprint(\"FAIL\")\r\n\r\nstrange_assign()\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nASSIGN TEST g1:0\r\nOK\r\nASSIGN TEST g2:0\r\nOK\r\nASSIGN TEST g3:0\r\nOK\r\nASSIGN TEST g4:0\r\nE tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n\t [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25)]]\r\nFAIL\r\nASSIGN TEST g5:0\r\nOK\r\nASSIGN TEST g6:0\r\nOK\r\n```\r\n\r\n\r\nError message:\r\n\r\n```\r\nE tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n\t [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25)]]\r\nTraceback (most recent call last):\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n\t [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"wtf.py\", line 29, in <module>\r\n    strange_assign()\r\n  File \"wtf.py\", line 23, in strange_assign\r\n    sess.run( [tf.assign(test, ph)] , feed_dict = { ph: t })\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n\t [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_g1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^_recv_Placeholder_3_0/_25)]]\r\n```\r\n\r\nVersions:\r\n\r\n1.0.1\r\n\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n\r\n", "comments": ["@zheng-xq, I don't know if this is a gpu issue, but is there anyway we can improve the error message here?", "I'm getting the same error message (`AttrValue must not have reference type value of float_ref`), also related to using Variables on the GPU, but unrelated to `orthogonal_initializer`.  I'm not sure what exactly is triggering it, due to the difficulty debugging this error (for the reasons pointed out by @olegklimov).  It'd be great if the error in general could be made more informative, not just a a fix for `orthogonal_initializer` . A stack trace/line number pointing to the code that created the op that's throwing the error would be ideal, but if that isn't possible even just the name of the Op associated with the `_Recv` node would help.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I ran the example given by @olegklimov with tf-1.5.0, and couldn't reproduce the error any more.\r\n\r\nClose it due to the inactivity for a long time."]}, {"number": 8327, "title": "Add SingleImageRandomDotStereogramsOp to Contrib", "body": "Updated code, closed 8074 pull request.  Could not merge to master without README file in the pull request so pulling R1.0.\r\n\r\nThis update add an Op to convert 3D data into a 2D image SIRDS (Single Image Random Dot Stereogram) for scientific data review.\r\n\r\nIt is related to this discussion:\r\nhttps://github.com/tensorflow/tensorflow/issues/8022\r\n", "comments": ["Can one of the admins verify this patch?", "Can you change this to commit to master? We don't plan to update master with r1.0 changes anymore.", "Sure, when I tried that, a README.md file showed and I couldn't get rid of that as I didn't know how to remove it from my changes (it was someone else update).  Sorry, I am not that good with git yet.\r\n\r\nShould I close this and try again?  I am not sure how to change this one in particular.", "That's too little information to go off of :) Yes, let's close this and make a new pull request on master."]}, {"number": 8326, "title": "TensorBoard develop build failed due to npm dead loop and typescript error", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.10\r\nnpm 4.4.1\r\nnode.js v7.7.2\r\n\r\nInstalled version of CUDA and cuDNN:  CUDA 8.0, cuDNN 5.1\r\n-rw-r--r-- 1 root root   558720 9\u6708  15 07:02 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 9\u6708  15 07:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 9\u6708  15 07:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 9\u6708  15 07:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 9\u6708  15 07:02 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 12\u6708 12 18:46 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 12\u6708 12 18:46 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 12\u6708 12 18:46 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 12\u6708 12 18:46 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\nf1ffbc548906e379d423633f76edc558be8284e7\r\n2. The output of `bazel version`\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n\r\n### minimal reproducible example\r\nget a fresh clone\r\ncd tensorflow/tensorboard\r\n`npm run prepare` \r\n\r\nand npm install some packages and goes into a dead loop without the bower and typings\r\n  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard\r\n  > npm install && bower install && typings install\r\n  \r\n  \r\n  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard\r\n  > npm install && bower install && typings install \r\n  \r\n  \r\n  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard\r\n  > npm install && bower install && typings install\r\n  \r\n  \r\n  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard\r\n  > npm install && bower install && typings install\r\n\r\n### What other attempted solutions have you tried?\r\nI have moved global typings and typescript, but nothing works.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nIf I kill the prepare after it goes into dead loop than use `bower install && typings install` \r\neverything looks fine. \r\nbut  `gulp` fail due to some semantic errors\r\n  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2403: Subsequent variable declarations   must have the same type.  Variable 'shadowRoot' must be of type 'ShadowRoot', but here has  type'ShadowRootPolyfill'.\r\n  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2687: All declarations of 'shadowRoot' must have identical modifiers.\r\n  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2403: Subsequent variable declarations must have the same type.  Variable 'shadowRoot' must be of type 'ShadowRoot', but here has type 'ShadowRootPolyfill'.\r\n  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2687: All declarations of 'shadowRoot' must have identical modifiers.\r\n  __lib.d.ts(7452,14): error TS2687: All declarations of 'shadowRoot' must have identical modifiers.\r\n  [23:27:41] TypeScript: 2 semantic errors\r\n  [23:27:41] TypeScript: 3 emit errors\r\n  [23:27:41] TypeScript: emit failed\r\nthen the demo/index.html is just a blank page.\r\n", "comments": ["[webcomponents.js typescript binding issue at DefinitelyTyped](https://github.com/DefinitelyTyped/DefinitelyTyped/issues/15064)", "[webcomponents.js typescript binding issue at DefinitelyTyped](https://github.com/DefinitelyTyped/DefinitelyTyped/issues/15064)", "@dandelionmane, do you have any insights on this?", "Any update to this? It seems like the TensorBoard development workflow (at least on recent checkouts) is not possible due to this.", "the `npm run prepare` issue is a result of npm 4.3. `npm install` will now run the prepare script, which runs install, which runs prepare... and so on. I am about to submit a PR for this.", "Here's a relevant NPM issue regarding `npm prepare` https://github.com/npm/npm/issues/15226", "@jart @chihuahua \r\nThe fact that the open-source TensorBoard build is broken is a known issue and one that frustrates us a lot. I think on the master branch chihuahua put out some new instructions in development.md that solve some of this. We're hard at work on getting a bazel build working so we can leave all this gulp/npm stuff behind.", "The development.md file is quite helpful. I'm not sure if I should open an issue for each thing I'm currently encountering in \"master\" (some might be because I have no experience with the npm/bower ecosystem) but:\r\n\r\n1. The webcomponents.js typings introduced that \"shadowRoot\" issue that people are hitting even outside of TensorBoard. See the \"FYI, this line\" part at https://github.com/DefinitelyTyped/DefinitelyTyped/pull/10330/files/4a7781d7e31eb4b9d7b97ecc81c55bb630fd1ddc\r\n\r\nRemoving that line seemed to allow me to proceed further.\r\n\r\n2. TypeScript does not like the use of \"number|null\" within vz_project/data.ts:97\r\n\r\nRemoving the \"null\" allowed me to proceed further.\r\n\r\n3. The tf_storage component was updated at some point to have a \"useLocalStorage\" parameter on the get*/set* functions. However, not all of these methods use a default parameter for this. Therefore, storageTests.ts is now broken.\r\n\r\nEnsuring this parameter is passed allowed me to proceed further.\r\n\r\n4. The demo now loads partially but fails soon after because these scripts cannot be found (d3, lodash, etc). I figured these would be handled by bower, but I don't know why it's not placing those in some accessible place.\r\n\r\nFurthermore, the \"tf_imports\" folder/dependency is not created and so many other tasks (gulp regenerate, bazel) all then fail at this point.\r\n\r\nEDIT: My mistake on (4). I didn't realize I needed to edit the tf_imports component HTML files to point to the correct .js files. Once I did that, the TensorBoard demo is now loading.\r\n\r\n", "I'm in the process of upgrading TypeScript in package.json to 2.2.1, which might help. But there's a lot going on here. As @dandelionmane mentioned, we're going to be deprecating the Gulp build soon.", "tsify and gulp-typescript both use their own versions of typescript as well. These seem to be the packages causing most of the issues.", "So the upgrade to TypeScript `2.2.1` happened. Can we close this?", "@jart fixed this; tensorboard now uses bazel to build instead of gulp/npm!\r\n\r\nCheck it out here: https://github.com/tensorflow/tensorboard"]}, {"number": 8325, "title": "A typo in lstm_ops.py", "body": "https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/rnn/python/ops/lstm_ops.py#L73\r\n\r\nThis line o = sigmoid(cs * wco + f) looks like a typo. It should be o = sigmoid(cs * wco + o).", "comments": []}, {"number": 8324, "title": "i cant get tensorflow to install", "body": "i tried following the steps in the tutorial after installing python 3.6 and 3.5 then i went to cmd and typed in pip3 install --upgrade tensorflow and the gpu version i was told that it could not find a version that satisfies the requierments tensorflow (from versions: ) no matching distribution found for tensorflow. (same for the gpu one)\r\ni also tried this: pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl : and i was told that it is not supported for this wheel which is understandable considering i have a nividia gpu and intel cpu but other than that i really dont know what to do if any one is able to give me commands to put in to the cmd terminal that would be great ill list my pc specs here: windows 7 pro, intel Q8300, nividia 560TI, 12GB DDR3 ram. as far as i know i have all the latest drivers installed and such and more than enough room for any files that might need to be downloaded. any help is appreciated.", "comments": ["Your pip version might be too old. You could try upgrading it with \r\n`pip install --upgrade pip`\r\n\r\nIf your distribution doesn't allow this consider using a virtual environment (check [this](http://docs.python-guide.org/en/latest/dev/virtualenvs/) out)", "it says that it is already up to date when i put that in it doesn't say anything about the distribution.", "What's the output of `pip --version` (or `pip3 --version ` in your case)?", "pip 9.0.1 (then file directory)", "Which python version is pip3 linked to? You mentioned installing 3.5 AND 3.6. Maybe you're trying to install the tensorflow version for python3.5 while pip3 links to python3.6\r\n\r\nAlso  32bit vs 64bit might be a problem, [this](http://stackoverflow.com/questions/28568070/filename-whl-is-not-supported-wheel-on-this-platform) stackoverflow post lists various causes for this error message.", "its linked to python 3.6 and its on 32 bit for both", "The documentation says:\r\n\r\n> TensorFlow supports only 64-bit Python 3.5 on Windows\r\n\r\nSo you need to install 64-bit python **3.5** for tensorflow to work.", "ahh ok ill get that done, thanks allot for you're help :)", "Did this work for you? If yes, please close this issue!", "Looks like this was an issue on user side.\r\nClosing issue."]}, {"number": 8323, "title": "tensorflow bazel build error ", "body": "I am using ubuntu 16.04 , I have anaconda and tensorflow installed ,  I was following along this tutorial ->  https://www.tensorflow.org/tutorials/image_retraining#training_on_flowers\r\n\r\nI am getting the following error \r\n\r\n```\r\nsaurabhorange@orangepc:~/tensorflow$ \r\nsaurabhorange@orangepc:~/tensorflow$ bazel build tensorflow/examples/image_retraining:retrain\r\nERROR: /home/saurabhorange/tensorflow/tensorflow/core/BUILD:1315:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/saurabhorange/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/saurabhorange/tensorflow/tensorflow/core/BUILD:1315:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/saurabhorange/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/saurabhorange/tensorflow/tensorflow/core/BUILD:1315:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/saurabhorange/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted.\r\nINFO: Elapsed time: 1.480s\r\nsaurabhorange@orangepc:~/tensorflow$ \r\n\r\n```", "comments": ["may be your bazel is too old", "I just installed it before entering bazel build command following official instructions from bazel website", "did you run `./configure` before trying to build?", "Ping!\r\nCould you try running `./configure` first, then building?\r\nIf it works, please close the issue.", "Closing for now, please reopen with detailed info if this doesn't work.", "Yes I re ran configuration and it solved the problem , Thanks :)"]}, {"number": 8322, "title": "Using a `tf.Tensor` as a Python `bool` is not allowed.  ", "body": "I want to do \uff1a IF True: return a ; else : return b (in python) . But there is a error : Using a `tf.Tensor` as a Python `bool` is not allowed.  How can I do this", "comments": ["Try use print(a) and print(b)\n\nEl 12 mar. 2017 12:41, \"danlutan\" <notifications@github.com> escribi\u00f3:\n\n> I want to do \uff1a IF True: return a ; else : return b (in python) . But there\n> is a error : Using a tf.Tensor as a Python bool is not allowed. How can I\n> do this\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8322>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AI289dG1ZSUqlWzKtF_8zTwcF14WMxy4ks5rk9nogaJpZM4Mag59>\n> .\n>\n", "`c=tf.placeholder(tf.bool)\r\nif c  :\r\n    loss=-tf.reduce_sum(lable * tf.log(lable2))`\r\n\r\nraise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n", "This is intended.  Please use `tf.cond`."]}, {"number": 8321, "title": "it seems that sparse_to_dense can only work in CPU?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["currently I want to generate one Dense Tensor by Sparse Tensor, it is very usual that generating one dense tensor with big dim(such as 100k dim) by some word indexs. but it seems sparse_to_dense op only works for CPU. so the problem is that if I have 8 GPUs,  I have to run sparse_to_dense 8 times in the CPU serially and send them to 8 GPUs, the  Parallel running performance is very bad!", "You're correct that TF only has a CPU kernel for SparseToDense (see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_to_dense_op.cc#L139)).  I don't think anyone is actively working on adding a GPU kernel for it -- we're happy to accept a pull request!", "Thanks for your comment, do you think is there working around method? because as I said, expanding the short words idx vector into large dense vector is very usual for ML.", "Do you need to use `SparseTensor` to represent the indices, though?  Would something like `tf.one_hot` work for you, which does have a GPU kernel?", "Thanks very much! I think the tf.one_hot can work for me."]}, {"number": 8320, "title": "Please remove all code visibility restrictions", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI am trying to use grpc_util to convert ::tensorflow::Status to ::grpc::Status, and found that I cannot build the target because of package visibility issues.\r\n\r\nHere is my recommendation:\r\n\r\nRemove all visibility when open source tensorflow. \r\n\r\nReason:\r\n\r\n1. It is pointless to have the package visibility as we have the source code, and we can modify it.\r\n\r\n2. If everybody has to modify their own copy of tensorflow to depend on portions of the source code, it only makes people's life harder as there would be millions of different forks all trying to hack the BUILD file. And every time a new tensorflow is launched, it breaks other people's build, which seems quite counter productive.\r\n\r\n3. In Google we want to make the dependency as accurate as possible. In the open source world, because of the visibility issue, we have to try to pull in much bigger build target to get something working, which is again, counter productive.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@rosun82 I think we want to go in other direction. We should be removing access to internals from external libraries. This is quite customary for libraries, where the API surface is limited and maintained with an SLA (compatibility guarantees). Internals can be refactored and changed without breaking users.\r\n\r\nAn example of not doing this is the Win32 API, and more recently the LLVM API. If you have ever upgraded LLVM, this can be quite painful. And the Win32 API is cluttered with symbols and undocumented behavior that must be honored forever in the future.", "If tensorflow is published as a dll file, I agree.\r\n\r\nHowever, since it is released as source code, there is no real 'internal'. Whoever downloaded the code can modify it to make use of the internal code, which is often very necessary (like the tf status to grpc status conversion) My point is simply, if it is impossible to enforce what is 'internal', why not just remove the visibility constraints, making advanced user's life a bit simpler? After all, people who only use the public interface would not care whether the code is internal or not", "I agree with you that this is something only very advanced users _might_ do.  As such, I don't think we'll introduce complexities (AFAICT there are a lot of consequences of the purposed change) by doing this.  ", "We are using package visibility to indicate what parts of the API are public and can be relied on to not change. We do not want people relying on implementation details, so we would like to keep visibility rules in place. \r\n\r\nIf you think a specific rule should be public, please file a bug for that rule. Of course, as you noted, you also have the ability to modify the code to modify visibility yourself."]}, {"number": 8319, "title": "Update macOS GPU installation instructions", "body": "### Environment info\r\n\r\nOperating System: macOS 10.12\r\nTensorflow: 1.0\r\nCUDA/cuDNN: 8.0, 5.1\r\n\r\n---\r\n\r\nI was setting up Tensorflow with GPU, reading the instructions: https://www.tensorflow.org/install/install_mac. It seems that a few things can be made clearer. I'm not sure where the source for the website lives so I want to file an issue so it can possibly be updated and people searching for related issues can have a pointer.\r\n\r\nThe issues are with the **Requirements to run TensorFlow with GPU support** section.\r\n\r\n1. It mentioned that one should set `LD_LIBRARY_PATH` according to NVIDIA docs, but NVIDIA docs only asks to specify `DYLD_LIBRARY_PATH`, which actually doesn't work. Can we say on the website how exactly it should be set?\r\n2. Same for cuDNN. The linked page actually has no instruction about how to install cuDNN. It may be useful to put on the website that the **include** and **lib** files should be manually copied over to **/usr/local/cuda**.\r\n3. The following should be run to avoid segfault: `sudo ln -sf /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`.\r\n\r\nThe paths for 1 should be:\r\n\r\n```sh\r\nexport CUDA_HOME=/usr/local/cuda\r\nexport LD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0/lib/\r\nexport PATH=\"/Developer/NVIDIA/CUDA-8.0/bin:${PATH}\"\r\n```", "comments": ["About 1 and 2: CUDA and cuDNN are software built by NVIDIA. Their installation instructions may change for reasons beyond our control.\r\nTherefore instead of us rewriting NVIDIA's instructions to install their software, we elected to point to NVIDIA for latest instructions for these.\r\n\r\nMac GPU is actually a difficult environment to test, as Apple has not produced a mac with NVIDIA GPUs for three years now. I think there may be two bugs from what you described:\r\n-Setting `DYLD_LIBRARY_PATH` does not work.\r\n-We need to create a symlink to avoid a segfault (when loading cuda libraries)\r\n\r\nI will investigate these two.\r\n\r\n", "re. 1 and 2, understood. The problem is that they don't really have documentation, unfortunately (esp. for cuDNN).\r\n\r\nfor `DYLD_LIBRARY_PATH`, I guess it's WAI, since you did mention in the instructions that we should set `LD_LIBRARY_PATH` correctly. it's just nvidia somehow doesn't have that in their docs..\r\n\r\nat any rate, hopefully someone having trouble will see this issue and find some help :) Thanks for looking into it!\r\n\r\n(someone holding onto an old mbp just for the GPU..)", "So here is my progress. My macbook pro is gone, due to old age, but these I was able to figure out in its last breath:\r\nThe need to create `/usr/local/cuda/lib/libcuda.1.dylib` should now be gone if you use TF from head or TF 1.1 (rc at the moment).\r\n\r\n`LD_LIBRARY_PATH` I did not need to set at all to get TF to run.\r\n`DYLD_LIBRARY_PATH`, this was bad. I got everything to work with this aggressive variable:\r\n```\r\nexport DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0:/Developer/NVIDIA/CUDA-8.0/lib:/usr/local/cuda:/usr/local/cuda/lib:$DYLD_LIBRARY_PATH\r\n```\r\n"]}, {"number": 8318, "title": "#FR CG pipeline in tf", "body": "#in brief\r\nbuild computer graphic render pipeline with tf\r\n\r\nIn GDC2017's talk ['FrameGraph: Extensible Rendering Architecture in Frostbite'](http://www.frostbite.com/2017/03/framegraph-extensible-rendering-architecture-in-frostbite), a few slide inspire me, why don't CG and CV use the same computation platform, share 'GPU cluster' 'memory management' and 'operation oplimization' code, iteract with each other directly.  I am going to implement a simple ray trace with tf, is there any comment or suggestion?\r\n\r\n<img width=\"860\" alt=\"screenshot\" src=\"https://cloud.githubusercontent.com/assets/7426917/23828883/29b7d1f6-071b-11e7-80c4-99a95aaa2c4d.png\">\r\n\r\n<img width=\"869\" alt=\"screenshot\" src=\"https://cloud.githubusercontent.com/assets/7426917/23828862/89d7e978-071a-11e7-8195-5bb6353e01c6.png\">\r\n", "comments": ["This seems very speculative and this is not supposed to be a discussion forum. \r\nI suspect that your question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is also a larger community that reads questions there.\r\n\r\nPlease feel free to reopen once you have more specifics."]}, {"number": 8317, "title": "Building from source on macOS: invalid command 'bdist_wheel'", "body": "I found two other issues mention this error, though none of the proposed solutions fixed the problem for me.\r\n\r\nOperating System: macOS Sierra 10.12.3\r\n\r\nInstalled version of CUDA and cuDNN: None\r\n\r\nOn a fresh install of Sierra, I've used brew to install bazel, python, python3 (which gives me 3.6.0), pyenv, and the virtualenv plugin for pyenv. I'm following the \"Build from source\" instructions for Mac, created a virtual environment specifically for building tensorflow, and when running ./configure using all the defaults and answering N to all the questions. At the final step, building the wheel, I get:\r\n\r\n```\r\n(build-tensorflow) \u276f bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nSat Mar 11 18:35:17 PST 2017 : === Using tmpdir: /var/folders/z0/l02zt19x3rvdq8ns17x5tznm0000gn/T/tmp.XXXXXXXXXX.Jfkzndt3\r\n~/Code/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/Code/tensorflow\r\n~/Code/tensorflow\r\n/var/folders/z0/l02zt19x3rvdq8ns17x5tznm0000gn/T/tmp.XXXXXXXXXX.Jfkzndt3 ~/Code/tensorflow\r\nSat Mar 11 18:35:19 PST 2017 : === Building wheel\r\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n   or: setup.py --help [cmd1 cmd2 ...]\r\n   or: setup.py --help-commands\r\n   or: setup.py cmd --help\r\n\r\nerror: invalid command 'bdist_wheel'\r\n```\r\n\r\nI thought this was maybe a pip or setuptools version issue, but they all seem on the latest version. `pip list` outputs:\r\n\r\n```\r\nappdirs (1.4.3)\r\nnumpy (1.12.0)\r\npackaging (16.8)\r\npip (9.0.1)\r\npyparsing (2.2.0)\r\nsetuptools (34.3.2)\r\nsix (1.10.0)\r\nwheel (0.29.0)\r\n```\r\n\r\nAny help to understand how to resolve the bdist_wheel error will be much appreciated, thanks!", "comments": ["Can you confirm that the solutions in #7402  #348 didn't work for you?\r\n\r\nPlease can you confirm your python version?  (if you're using python3 you need to `pip3 install wheel`)\r\n\r\nI assume you are following https://www.tensorflow.org/install/install_sources#prepare_environment_for_mac_os ?\r\n\r\n", "Correct, the solutions I saw in those two issues (`pip install wheel` and `pip3 install wheel`) didn't fix it. Python version is 3.6.0, I created the environment using `pyenv virtualenv 3.6.0 build-tensorflow`. Created this way, python / pip are already mapped to python3 / pip3:\r\n\r\n```\r\n(build-tensorflow) \u276f python --version                       \r\nPython 3.6.0\r\n```", "i also have the same problem.\r\n<img width=\"938\" alt=\"wx20170314-132402 2x\" src=\"https://cloud.githubusercontent.com/assets/18029500/23886932/8acf3d4a-08b9-11e7-88a4-da094be884f8.png\">\r\ni add `sys.path.append('/Users/CongWeilin/anaconda2/lib/python2.7/site-packages')` in setup.py and it worked! ", "I encountered the same problem and worked it out. It seems that python can't find wheel module.\r\nDoes you use pyenv-virtualenv? you should install wheel on global python\r\n\r\n```\r\npyenv global 3.6.0\r\npip3 install wheel\r\n```", "@turbojava You're spot on, installing wheel for the global pyenv version resolved the problem. So, do you know... is that a bug with pyenv or tensorflow?", "Looks like a glitch outside of core TensorFlow itself.  Since you've found a workaround, I'm closing this.", "@CongWeilin ,\r\nsetting pyenv global did nt help.\r\n\r\nWhich line of setup.py do we need to add sys.path.append.?\r\n", "Depending how the required dependencies were installed Pip may be associated with a different Python installation that TensorFlow is trying to find e.g. TF may be looking into `/Library/Python/2.7/site-packages/` but the Pip used was associated to `/usr/local/lib/python2.7/site-packages` (the default that comes with macOS).\r\n\r\nI exported `PYTHONPATH` to reflect the corresponding Python packages location, in my case, by adding the correct `site-packages` path as:\r\n`export PYTHONPATH=$PYTHONPATH:/usr/local/lib/python2.7/site-packages`", "@Carmezim Thanks, you saved my day."]}, {"number": 8316, "title": "Where is the document of tf.keras?", "body": "It is said that Google have released tf.keras in the lastest version,but I can't find the document.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@arsenal0502 It is said that tf.keras will be supported since TensorFlow Version 1.2.", "Thank you very much!\n\nAt 2017-03-17 12:04:36, \"Pai Peng\" <notifications@github.com> wrote:\n\n\n@arsenal0502 It is said that tf.keras will be supported since TensorFlow Version 1.2.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread."]}, {"number": 8315, "title": "[feature]: support for trigonometric functions in XLA", "body": "I'd like to request support for basic trigonometric functions in XLA, things like cos, sin, etc, as well as basic geometry options like cross products. These are already supported in TF and I imagine there's native support in eigen. Looking through tf2xla I didn't see them anywhere.", "comments": ["@learyg @eliben We currently only have Tanh - any reason not to have the rest?\r\n", "Any updates on this?", "I think element-wise trigonometric functions can be added - I don't see a strong reason not to support them, because building them out of other XLA primitives would be challenging", "Does the \"contributions welcome\" tag mean no one at Google has any plans to add this? Also what about things like `cross`? It's currently a builtin TF function.", "@fxsuper Yep, no one at Google is working on this.", "Hi! I've just added sin(x) support and it looks like cos(x) support was there. Should be available in the next merge. I'm going to mark this as fixed once that lands, feel free to open new requests for other things, like the cross products you mention, and if possible it'd be helpful to describe why they'd be difficult/impossible to construct with existing XLA operations. (In general it'd also be helpful to have motivating use cases for feature requests, helps us prioritize.) Thanks for filing, sorry for the delay!", "Thanks for the update, and I'm glad that `sin` will be in there soon!\r\n\r\nRegarding `cross`--it's certainly possible to build out of existing XLA blocks, but my concern was speed. Is that not a valid concern? TF-level cross can be defined pretty easily indeed:\r\n\r\n```\r\nwith tf.op_scope([u, v], name, 'cross') as scope:\r\n    u_t = tf.transpose(u)\r\n    v_t = tf.transpose(v)\r\n\r\n    u1, u2, u3 = tf.unstack(u_t)\r\n    v1, v2, v3 = tf.unstack(v_t)\r\n\r\n    return tf.transpose(tf.stack([(u2 * v3) - (u3 * v2), (u3 * v1) - (u1 * v3), (u1 * v2) - (u2 * v1)]))\r\n```\r\n\r\nI'm asking so that I know whether to start a new issue or not. Thanks.", "Merged as 2661f6841d0ad9ec1381d177a1f9df02e73d001c\r\n\r\nHmm I'm not familiar with how those stack/unstacks will lower from TF to XLA. Could you open a new issue for that and we can check it out, since it's fairly distinct, and address it if there are missing pieces? Thanks much!", "Also, @fxsuper, looks like it works, but please file the bug to add the op-registered support for tf.cross that does the \"desugaring\" you've suggested: https://gist.github.com/learyg/4019bae1cfc5bacbcdb318fd882b4d5d\r\n\r\nLooks like it fuses somewhat reasonably, but the CPU backend doesn't seem to currently support fusing the slice operation into its consumers, so it could be better. Thanks for filing!\r\n", "Thanks for investigating this! I just opened a new issue."]}, {"number": 8314, "title": "[Windows Build] \"RECURSIVE_PKG:rootedPath\" Path Casing Error", "body": "Windows 10\r\nBazel 0.4.5\r\nVisual C++ 2015\r\nMsys2 v20160205\r\nPython 3.5 (python.org distribution)\r\n\r\nConfiguring TensorFlow on Windows generates an error related to path case sensitivity:\r\n\r\n```\r\nAdriano@Adriano MSYS /c/Users/Adriano/Documents/tensorflow\r\n$ ./configure\r\nPlease specify the location of python. [Default is /c/Users/Adriano/AppData/Local/Programs/Python/Python35/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\n  C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35]\r\n\r\nUsing python library path: C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for util\\python\\python_include <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\include\r\nJunction created for util\\python\\python_lib <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for third_party\\py\\numpy\\numpy_include <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\numpy\\core\\include\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1\r\nPlease specify the location where cuDNN 5.1 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 5.0\r\n..................................................\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'FILE:[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head]' (requested by nodes 'RECURSIVE_PKG:rootedPath=[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head], excludedPaths=<omitted>)')\r\n        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:448)\r\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: relativePath 'C:/Users/Adriano/Documents/tensorflow/.git/HEAD' is absolute, but it's not under root 'C:/users/adriano/documents/tensorflow'\r\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:383)\r\n        at com.google.devtools.build.lib.util.Preconditions.checkArgument(Preconditions.java:90)\r\n        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:56)\r\n        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:73)\r\n        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPathMaybeUnderRoot(RootedPath.java:83)\r\n        at com.google.devtools.build.lib.skyframe.FileFunction.getSymlinkTargetRootedPath(FileFunction.java:176)\r\n        at com.google.devtools.build.lib.skyframe.FileFunction.compute(FileFunction.java:101)\r\n        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:374)\r\n        ... 4 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'FILE:[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head]' (requested by nodes 'RECURSIVE_PKG:rootedPath=[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head], excludedPaths=<omitted>)')\r\n        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:448)\r\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: relativePath 'C:/Users/Adriano/Documents/tensorflow/.git/HEAD' is absolute, but it's not under root 'C:/users/adriano/documents/tensorflow'\r\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:383)\r\n        at com.google.devtools.build.lib.util.Preconditions.checkArgument(Preconditions.java:90)\r\n        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:56)\r\n        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:73)\r\n        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPathMaybeUnderRoot(RootedPath.java:83)\r\n        at com.google.devtools.build.lib.skyframe.FileFunction.getSymlinkTargetRootedPath(FileFunction.java:176)\r\n        at com.google.devtools.build.lib.skyframe.FileFunction.compute(FileFunction.java:101)\r\n        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:374)\r\n        ... 4 more\r\n```\r\n\r\nSteps to reproduce:\r\n- Clone TensorFlow repo\r\n- `./configure`", "comments": ["@mrry Have you encountered anything like this before?", "@Carmezim Could you also please provide the commit version of your repo.", "Nope, sounds like Bazel's sandboxing is unhappy. As a short-term workaround, the CMake build in `tensorflow/contrib/cmake` should work fine.\r\n\r\n@meteorcloudy: have you seen this error before?", "@prb12 Sure a91be11e5d228848e9648f1e63e6d509754befb9", "I think this issue is fixed on bazel current rc. I received this error on rc3 if not mistaken and on current it does not occur so I am closing this issue."]}, {"number": 8313, "title": "out_depth not set in PoolParameters if depth_window == 1", "body": "In `PoolParameters`, `out_depth` is not set if `depth_window == 1`.\r\nSee: https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/pooling_ops_common.cc#L61-L66.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!"]}, {"number": 8312, "title": "tf.Graph.get_operations() expects an undocumented parameter", "body": "Based on the API [documentation](https://www.tensorflow.org/versions/master/api_docs/python/framework/core_graph_data_structures#Graph.get_operations), I can call tf.Graph.get_operations(), without arguments, to fetch the list of operations in the graph. When I do, I get:\r\n\r\n```\r\n----> 1 tf.Graph.get_operations()\r\n\r\nTypeError: get_operations() missing 1 required positional argument: 'self'\r\n\r\n```\r\nUsing tensorflow-gpu (1.0.1) under Ubuntu, installed with pip3.\r\n\r\n```\r\n> python3 -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.1\r\n> \r\n\r\n```\r\n", "comments": ["I tried to use it as a class method, while it is not. My mistake."]}, {"number": 8311, "title": "Different output using CudnnGRU vs GRUCell", "body": "Operating System: Arch Linux\r\n\r\nInstalled version of CUDA and cuDNN:  libcudart.so.8.0.44, libcudnn.so.5.1.5\r\n\r\n1. The commit hash (`git rev-parse HEAD`): 57e40363eb40a692f7c5dfea3f53031a52024321\r\n2. The output of `bazel version`: 0.4.4\r\n\r\n\r\n\r\nI set x = 1, previous_h = 0, all weights and biases = 0, output should be 0. Traditional tensorflow GRU returns 0, but CudnnGRU returns 0.20482421\r\n\r\nNOTE: need to use GPU for CudnnGRU to work properly\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n\r\n# GRU with x=1, h_t_minus_1=0, all weights and biases = 0; output (h) should equal 0\r\n# u = sig(Wx + Rh + b)\r\n# r = sig(Wx + Rh + b)\r\n# c = tanh(Wx + r(Rh + b) + b)\r\n# h = (1-u)c + uh\r\n\r\n\r\n# u = sig(0*1 + 0*0 + 0)\r\n# u = .5\r\n# r = sig(0*1 + 0*0 + 0)\r\n# r = .5\r\n# c = tanh(0*1 + .5(0*0 + 0) + 0)\r\n# c = 0\r\n# h = (1-.5)*0 + .5*0\r\n# h = 0\r\n\r\n\r\n\r\nbatch_size = 1\r\nn_time = 1\r\nx_depth = 1\r\nn_cell = 1\r\n\r\nx = tf.ones([batch_size, n_time, x_depth])\r\n\r\ny_tf, _ = tf.nn.dynamic_rnn(tf.contrib.rnn.GRUCell(n_cell), x, dtype=tf.float32)\r\nparam_tf = [v.assign(tf.zeros(tf.shape(v))) for v in tf.trainable_variables()] # y_tf uses these zeroed out params\r\n\r\nn_layer = 1\r\nrnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, n_cell, x_depth, 'skip_input')\r\nparam_cudnn = tf.Variable(tf.zeros([rnn_cudnn.params_size()]), validate_shape=False)\r\ny_cudnn, state_cudnn = rnn_cudnn(tf.transpose(x, [1,0,2]), tf.zeros([n_layer, batch_size, n_cell]), param_cudnn)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    # NOTE: cudnn has more params because they use twice as many biases\r\n    print('y_tf: {}\\ny_cudnn: {}\\nparam_tf\\n{}\\n\\nparam_cudnn\\n{}\\n\\n'.format(*sess.run([y_tf, y_cudnn, param_tf, param_cudnn])))\r\n\r\n\r\n\r\n# Output:\r\n# y_tf: [[[ 0.]]]\r\n# y_cudnn: [[[ 0.20482421]]]\r\n# param_tf\r\n# [array([[ 0.,  0.],\r\n#        [ 0.,  0.]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[ 0.],\r\n#        [ 0.]], dtype=float32), array([ 0.], dtype=float32)]\r\n\r\n# param_cudnn\r\n# [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n```\r\n", "comments": ["@zheng-xq This seems like something is wrong?", "Try putting `linear_input` instead of `skip_input`?", "`linear_input` does correctly output 0. I think there's still a bug with `skip_input` though because in my example input_size == num_units, so it shouldn't require a linear projection before the GRU layer. When I try `auto_select`, it returns 0.20482421. Here's the [argument definition](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L377).\r\n\r\n\r\n> input_mode: indicate whether there is a linear projection between the\r\n>           input and The actual computation before the first layer. It could be\r\n>           'skip_input', 'linear_input' or 'auto_select'.\r\n>           'skip_input' is only allowed when input_size == num_units;\r\n>           'auto_select' implies 'skip_input' when input_size == num_units;\r\n>           otherwise, it implies 'linear_input'.", "`skip_input`, as far as I can tell, skips the `Wx` linear transformation of the input and performs roughly half the computation as the other approach. This is probably in case you want to do batch norm through depth and so need flexibility to modify the outputs of `Wx`.  It does NOT appear to apply an extra linear transform \"outside\" of the GRU.\r\n\r\nAs you're expecting to apply `Wx`, you should enable `linear_input`. \r\n\r\nAs a side note, there still appears to be bug in `skip_input`, since for GRUs if you skip the initial linear transformation you need to pass in an input matrix that has dimensionality 3x that of the hidden dimension (each gate in the GRU takes a separate linear transformation). CuDNN still checks for equality in dimensionality, however, and appears to just copy the input to all three gates. This is likely incorrect and I can't make sense of it. Using `linear_input` will make the GRU behave as expected.", "@eddiepierce is your issue resolved? I'm gonna close this, but please let me know if it should be reopened. Thanks."]}, {"number": 8310, "title": "XLA/AOT on Windows build?", "body": "I know windows is low on the priority list (I don't blame you), but will there be support in the future for AOT compilation for Windows, or is it possible now in theory?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nThere aren't a lot of questions anywhere about this yet :/\r\n\r\n### Environment info\r\nOperating System:\r\nWindows 10\r\n\r\n### Installed version of CUDA and cuDNN: \r\nNone\r\n\r\n### Installation \r\nI installed from cmake using the tutorial here and it worked fine, the stock mnist example ran and I couldn't find a fault with the installation. \r\n\r\nI also tried building with the windows Bazel build. I didn't have much success with this (lots of errors on configure) but if I know it can work than I can put for effort here. \r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI attemped the JIT example [here](https://www.tensorflow.org/versions/master/experimental/xla/jit) but I got the same output using XLA and not using XLA. Checking chrome://tracing revealed no XLA startup. This is where I assumed that the cmake build does not include tensorflow/compiler/xla, indeed I found no corresponding MSVC build files. \r\n\r\nAttempting to build AOT binaries with bazel didn't work because configure had not been run. I know you can build these binaries with alternate build systems (in the works?) but there's no docs on how to do that. \r\n\r\nI just want to know if I should even bother trying while this part of the project is so young. I should add that the JIT works very well for me on ubuntu, in places where it should work. \r\n\r\nIf I can compile tf_library and get the tfcompile tool then I would appreciate some pointers on how to get there. I don't know how productive it is to spend time decoding bazel build files and try to construct my own alternate build the replicates it.\r\n\r\n### Logs or other output that would be helpful\r\nLog of JIT example output\r\n```\r\n$ TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py\r\nExtracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\r\n0.9179\r\n```\r\n`bazel ./configure` output\r\n```\r\n$ ./configure\r\nPlease specify the location of python. [Default is /c/Users//AppData/Local/Programs/Python/Python35/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35\r\n  C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35]\r\n\r\nUsing python library path: C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for util\\python\\python_include <<===>> C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35\\include\r\nJunction created for util\\python\\python_lib <<===>> C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for third_party\\py\\numpy\\numpy_include <<===>> C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\numpy\\core\\include\r\nDo you wish to build TensorFlow with CUDA support? [y/N] N\r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\nWarning: ignoring _JAVA_OPTIONS in environment.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:6:6: invalid character: '!'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:6:14: invalid character: '?'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:23:4: invalid character: '@'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:23:41: invalid character: '@'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:24:4: invalid character: '@'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:34:2: indentation error.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:38:6: invalid character: '!'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:38:18: invalid character: '?'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:44:2: indentation error.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:68:57: invalid character: '&'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:68:58: invalid character: '&'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:87:2: indentation error.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:92:39: invalid character: '?'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:95:44: invalid character: '?'.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:3:2: indentation error.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:7:4: indentation error.\r\nERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:16:2: indentation error.\r\n```", "comments": ["@tatatodd Do you know what the status is of tf_compile on Windows?\r\n\r\n@neale Thanks for following the issue reporting template ... I agree that the output (especially from bazel configuration) is sort of irrelevant and unrelated.\r\n", "@neale I don't believe anyone has tried to get `tfcompile` / `tf_library` to work on Windows.\r\n\r\nHonestly I'm not even sure what the right approach should be; on linux we rely on the `tf_library` bazel macro to kick off `tfcompile` binary, but I'm not sure whether this will fly on Windows.  In the worst case, as long as we can get the `tfcompile` binary built, the user can invoke it themselves.  I'll self-assign and follow-up to figure out how to best do this.", "Thanks!", "One question. If you manage to build a native Windows version of tfcompile with bazel, would generated static libraries be compatible with MSVC? If not, is there any way to generate C/C++ source code rather than static libraries so they can built and linked across platforms and compilers?\r\n\r\nThanks!", "@leandro-gracia-gil \r\n\r\nYes, if we add tfcompile support for Windows, the generated libraries would be compatible with MSVC.  Note that the only ABI requirement for the generated computation is a single C (not C++) function, which is definitely supported.  We also generate a C++ header to make it easier to call the C function, which performs some buffer management, and would just be compiled by MSVC as usual.", "That would streamline so much of my development process. Deep learning on windows platforms is definitely hamstringed right now. Its more of a hack then a science. ", "I have managed to successfully use tfcompile to run a Tensorflow graph in MSVC. The key seems to be using the target triple \"x86_64-pc-windows-msvc\" from a Linux build of tfcompile to generate a .h/.lib pair. You also need to build tensorflow/compiler/aot/runtime.cc with MSVC (it needs a couple minor fixes) and link it together with everything else. Also, one more trivial fix is required in the generated headers.\r\n\r\nIs it ok if I send a patch addressing these minor issues? If there are any instructions about sending patches that would be very helpful.", "@leandro-gracia-gil Yes, patches are definitely welcome!\r\n\r\nSome info on legal stuff here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\r\n\r\nOther than that, just send the patch like a regular github pull request (PR).  Some github links on this topic:\r\nhttps://help.github.com/articles/about-pull-requests/\r\nhttps://help.github.com/articles/creating-a-pull-request/\r\n\r\nThanks!", "@leandro-gracia-gil do you plan to add the patch soon? It would be very helpful.  And thanks for reporting the solution.", "@pawarrick I've found that depending on what you do (your graph operations) you might actually need to build more than just the aot runtime.cc in MSVC. That leads to changes in a few more Tensorflow files, and transitively to one more trivial change in the Eigen library.\r\n\r\nI'm currently trying to figure out a cleaner approach considering the required changes, which are small but scattered across various files. The core issue is that with the generated static library alone there are linker errors because of missing symbols (generally the memory management functions of the runtime and some XLA/JIT CPU kernel ops). This gets solved by building more and more bits of Tensorflow using MSVC, but it also makes the snowball of MSVC fixes bigger and bigger without knowing if your next graph will be missing something else.\r\n\r\nIt would be great if we could somehow include them in the generated static library itself, or perhaps have a separate static library for MSVC containing all the symbols the first one might need regardless of the graph we use.\r\n\r\nAny suggestions on good ways to approach the problem are most welcome.", "By the way, I've also noticed that there are a lot of kernels not implemented for the DEVICE_CPU_XLA_JIT device, which limits a lot what you can actually do with the AoT compiler. This seems to include multiple control flow operations (enter, exit, merge, loopcond, switch, nextiteration), TensorArray ops (size, read, gather, scatter) and some ops like argmin (although there's an argmax XLA kernel). You can hit most of these by simply using tf.map_fn in your graph. Kernels for any quantized data types are missing too.\r\n\r\nAre there there any plans about implementing these kernels for the AoT compiler? If I wanted to try implementing some of these myself, is there something I should keep in mind?\r\n\r\nThanks.", "After looking into further detail I've crafted the list of the source files that might need to be built and linked with MSVC in order to use static libraries generated with tfcompile. Unless I'm missing something this list should be exhaustive at the moment of writing this.\r\n\r\nRequired by generated headers:\r\n- compiler/aot/runtime.cc  (For tfcompile::runtime::MallocContiguousBuffers and FreeContiguous)\r\n- compiler/xla/executable_run_options.cc  (For the implementation of xla::ExecutableRunOptions)\r\n\r\nRequired to provide XLA custom ops:\r\n- compiler/tf2xla/kernels/gather_op_kernel_float_int32.cc  (for gather_float_int32_xla_impl)\r\n- compiler/tf2xla/kernels/gather_op_kernel_float_int64.cc  (for gather_float_int64_xla_impl)\r\n- compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc  (argmax_float_1d_xla_impl)\r\n- compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc  (argmax_float_2d_xla_impl)\r\n\r\nRequired by SimpleResolver in compiler/xla/service/cpu/simple_orc_jit.cc:\r\n- compiler/xla/service/cpu/runtime_matmul.cc  (for __xla_cpu_runtime_EigenMatMulF32)\r\n- compiler/xla/service/cpu/runtime_single_threaded_matmul.cc  (for __xla_cpu_runtime_EigenSingleThreadedMatMulF32)\r\n- compiler/xla/service/cpu/runtime_conv2d.cc  (for __xla_cpu_runtime_EigenConvF32)\r\n- compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc  (for __xla_cpu_runtime_EigenSingleThreadedConvF32)\r\n- compiler/xla/service/cpu/cpu_runtime.cc  (for __xla_cpu_runtime_AcquireInfeedBufferForDequeue and __xla_cpu_runtime_ReleaseInfeedBufferAfterDequeue)\r\n- compiler/xla/service/cpu/cpu_runtime_sse4_1.cc  (for runtime::ExpV4F32, runtime::LogV4F32 and runtime::TanhV4F32)\r\n- compiler/xla/service/cpu/cpu_runtime_avx.cc  (for runtime::ExpV8F32, runtime::LogV8F32 and runtime::TanhV8F32)\r\n\r\nI can also confirm that no changes in Eigen seem to be needed at the end.\r\n\r\nThe exact set of sources needed from that list depends on what the static library does and how the generated headers are used. I can confirm that I was able to successfully build all these source files in MSVC with the exception of:\r\n- compiler/xla/service/cpu/cpu_runtime_sse4_1.cc\r\n- compiler/xla/service/cpu/cpu_runtime_avx.cc\r\n\r\nWhich means no SSE4.1 and AVX support for the exp, log and tanh functions. The reason for these is the lack of good alternatives to \\_\\_attribute__((\\_\\_vector_size__(x))) and \\_\\_attribute__((weak)) in MSVC, particularly the former. All other files can build with minor fixes that should not affect other platforms.\r\n\r\nI can try preparing a pull request with the fixes that make the other sources compatible with MSVC, although we probably need some way to make sure they *keep* being compatible. Not having to gather all these by hand would be great too, but I'm not sure how that might be doable.\r\n\r\nAny feedback or suggestions are most welcome.", "Thanks for merging the fix! Here are detailed instructions on how to build and run a static library generated by tfcompile in MSVC.\r\n\r\n1. In a Linux machine (tfcompile doesn't work on Windows yet) checkout tensorflow and build tfcompile. You will need to [install Bazel](https://bazel.build/versions/master/docs/install.html) if you haven't done so already.\r\n> git clone https://github.com/tensorflow/tensorflow.git\r\n> cd tensorflow\r\n> bazel build -c opt //tensorflow/compiler/aot:tfcompile\r\n\r\n2. Run tfcompile to generate the static library for your graph and config.\r\n> bazel-bin/tensorflow/compiler/aot/tfcompile --target_triple=\"**x86_64-pc-windows-msvc**\" --graph=\"_your_graph.pb_\" --config=\"_your_graph_config.pbtxt_\" --entry_point=\"_your_C_entry_point_name_\" --cpp_class=\"_YourNamespace::YourCppClass_\" --out_object=\"output.lib\" --out_header=\"output.h\"\r\n\r\nYou might want to add some other arguments to tfcompile, like for example:\r\n> --xla_cpu_llvm_opt_level=3 --target_features=\"+avx,+avx2,+sse4.2\"\r\n\r\n3. Copy your generated .lib and .h files to the Windows machine where you run MSVC.\r\n\r\n4. In the MSVC solution you want to use the static library, create a new empty Win32 Static Library project.\r\n\r\n5. Checkout the Tensorflow source again in Windows (e.g., your_project\\third_party\\tensorflow)\r\n\r\n6. Open tensorflow\\workspace.bzl in your tensorflow checkout and look for \"eigen_archive\". Download one of the URLs there and extract the contents into a folder (e.g., your_project\\third_party\\eigen).\r\n\r\n7. Open tensorflow\\contrib\\cmake\\CMakeLists.txt in your tensorflow checkout and look for \"add_definitions\". Take a look to the preprocessor macros defined there (starting with -D) and add them manually in MSVC to your Tensorflow project properties -> C/C++ -> Preprocessor. You will need to define at least NOMINMAX, COMPILER_MSVC and TF_COMPILE_LIBRARY. Similarly, you might want to add some other compiler options from CMakeLists.txt into C/C++ -> Command Line -> Additional Options. None of these should be a strong requirement AFAIK.\r\n\r\n8. In your Tensorflow project properties -> C/C++ -> General -> Additional Include Directories add the following entries in this order:\r\n- If needed, the path where your generated .h file is.\r\n- The path where you extracted eigen (e.g., your_project\\third_party\\eigen).\r\n- The path where you extracted tensorflow (e.g., your_project\\third_party\\tensorflow).\r\n\r\n9. In your Tensorflow project properties -> Librarian, add your generated .lib to Additional Dependencies, and the path to it in Additional Library Directories.\r\n\r\n10. Manually add the following files to the \"Source Files\" section of your Tensorflow project:\r\n- tensorflow/compiler/aot/runtime.cc\r\n- tensorflow/compiler/xla/executable_run_options.cc\r\n- tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int32.cc\r\n- tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int64.cc\r\n- tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc\r\n- tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_matmul.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_single_threaded_matmul.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_conv2d.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc\r\n- tensorflow/compiler/xla/service/cpu/cpu_runtime.cc\r\n\r\n11. Set your project dependencies in your solution so that the projects you want to use the static library in depend on your Tensorflow project. (Right-click in your project -> Build Dependencies -> Project dependencies).\r\n\r\n12. You're ready to use the generated static library in your code. Here's an example snippet:\r\n```\r\n#define EIGEN_USE_THREADS\r\n#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"  // From your Tensorflow checkout.\r\n#include \"your_generated_header.h\"\r\n\r\neigen::ThreadPool thread_pool(2);  // Number of threads in your pool. Set as you want.\r\neigen::ThreadPoolDevice thread_pool_device(&thread_pool, thread_pool.NumThreads());\r\nYourNamespace::YourCppClass helper_object;\r\n\r\nhelper_object.set_thread_pool(&thread_pool_device);\r\nhelper_object.set_arg0_data(your_arg_data);\r\nhelper_object.Run();\r\nauto your_result = helper_object.result0();\r\n```\r\n\r\nKeep in mind that if you enable vector instructions (by passing args like +sse4.2, +avx or +avx2 to target_features when running tfcompile) you should ensure that any pointers passed to set_arg*_data(...) have the appropriate memory alignment. Otherwise you are likely to experience crashes when calling Run().\r\n\r\nEnjoy!", "Looks like this issue was resolved.\r\nClosing issue."]}, {"number": 8309, "title": "Kernel Died Issue", "body": "Can anybody know why kernel is always died? When I excute this cell, a part of Lenet CNN.\r\n\r\n1. Code : \r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    num_examples = len(X_train)\r\n    \r\n    print(\"Training...\")\r\n    print()\r\n    for i in range(EPOCHS):\r\n        X_train, y_train = shuffle(X_train, y_train)\r\n        for offset in range(0, num_examples, BATCH_SIZE):\r\n            end = offset + BATCH_SIZE\r\n            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\r\n            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\r\n            \r\n        validation_accuracy = evaluate(X_validation, y_validation)\r\n        print(\"EPOCH {} ...\".format(i+1))\r\n        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\r\n        print()\r\n    try:\r\n        saver\r\n    except NameError:\r\n        saver=tf.train.Saver()       \r\n    saver.save(sess, './lenet')\r\n    print(\"Model saved\")\r\n```\r\n2. Error Message :\r\n``` \r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\nThen, the kernel is died....it is crazy. Please let me know the solution \r\n\r\n3. Environment \r\nWindow 10 (No Ubantu)\r\nAnaconda w/Jupyter Notebook \r\nTensorflow w/GPU (Cuda 8.0, cuDNN 5.1)\r\n\r\nThank you!!\r\n", "comments": ["This does look like TensorFlow is unable to find the cuDNN library.  \r\nCould you please double check that you followed the instructions on [this](https://www.tensorflow.org/install/install_windows) page for installation of cuDNN and adding the installation directory to  your `%PATH%` environment variable?\r\n", "Potentially a duplicate of #6698 ", "Sometimes you get unrelated GPU errors when you run of RAM, as in https://github.com/tensorflow/tensorflow/issues/7025", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8308, "title": "Indentation in word2vec_basic.py", "body": "Running the raw word2vec_basic.py in a python terminal threw a number of `IndentationError: unexpected indent` exceptions, and it looks like the cause is the indentation of the blank lines.\r\n\r\nI\u2019ve added the expected spacing into the blank lines here, and can confirm that the script now runs correctly.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@brchristian , thank you for sending the PR. However, these changes are not consistent with Google Python style. Therefore I will close this PR. With regard to the issue you are experiencing, maybe you can do `run <file_name>` instead of copy-pasting the code."]}, {"number": 8307, "title": "Fixed typo", "body": "Running the example didn't produce any output.\r\nAdded print on estimator.evaluate to comply with the expected behavior.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 8306, "title": "tf.contrib.distributions.Multinomial initialization problem", "body": "Hi\r\nI can't initialize a multinomial distribution with more than 1 dimension probabilities. The version of installed TF is 1.0.0. I tried the example code in tensorflow website (https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Multinomial), it gets the same problem and the error doesn't make any sense to require probabilities to be integers. I'm not sure if the guide in the website is obsolete or I got something wrong. Please give some help here, thanks.\r\n```\r\nr=tf.contrib.distributions.Multinomial(n=[4, 5], p=[[.1, .3, .6], [.4, .05, .55]])\r\n\r\nValueErrorTraceback (most recent call last)\r\n<ipython-input-92-7d05643707f2> in <module>()\r\n----> 1 r=tf.contrib.distributions.Multinomial([4, 5], p=[[.1, .3, .6], [.4, .05, .55]])\r\n\r\n/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/multinomial.pyc in __init__(self, n, logits, p, validate_args, allow_nan_stats, name)\r\n    164             multidimensional=True)\r\n    165         self._n = array_ops.identity(n, name=\"convert_n\")\r\n--> 166         self._mean_val = array_ops.expand_dims(n, -1) * self._p\r\n    167         self._broadcast_shape = math_ops.reduce_sum(\r\n    168             self._mean_val, reduction_indices=[-1], keep_dims=False)\r\n\r\n/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in binary_op_wrapper(x, y)\r\n    881     with ops.name_scope(None, op_name, [x, y]) as name:\r\n    882       if not isinstance(y, sparse_tensor.SparseTensor):\r\n--> 883         y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n    884       return func(x, y, name=name)\r\n    885 \r\n\r\n/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    649       name=name,\r\n    650       preferred_dtype=preferred_dtype,\r\n--> 651       as_ref=False)\r\n    652 \r\n    653 \r\n\r\n/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    714 \r\n    715         if ret is None:\r\n--> 716           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    717 \r\n    718         if ret is NotImplemented:\r\n\r\n/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _TensorTensorConversionFunction(t, dtype, name, as_ref)\r\n    587     raise ValueError(\r\n    588         \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\r\n--> 589         % (dtype.name, t.dtype.name, str(t)))\r\n    590   return t\r\n    591 \r\n\r\nValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"Multinomial_26/Multinomial/p/Identity:0\", shape=(2, 3), dtype=float32)'\r\n```\r\n", "comments": ["This isn't a bug.  You appear to have missed a subtle but important character when copying the example... \r\n\r\nIn your version, the value passed to `n` is an int32 tensor: \r\n```\r\nr=tf.contrib.distributions.Multinomial(n=[4, 5], p=[[.1, .3, .6], [.4, .05, .55]])\r\n```\r\nThe example has a decimal point after the `4` which makes the argument a float32 tensor:\r\n```\r\nr=tf.contrib.distributions.Multinomial(n=[4., 5], p=[[.1, .3, .6], [.4, .05, .55]])\r\n                                           ^\r\n```\r\n\r\nI think that this would have been clearer in documentation:\r\n```\r\nr=tf.contrib.distributions.Multinomial(n=[4.0, 5.0], p=[[.1, .3, .6], [.4, .05, .55]])\r\n```\r\n\r\nPlease let me know if this does not solve your problem.\r\n\r\n@xmbrst This could probably be clearer in the docs!", "Yes, it does work now. It seems the library I used to wrap the distribution has some bug about this parameter. Thanks!"]}, {"number": 8305, "title": "tf.image.resize_image_with_crop_or_pad", "body": "hi, \r\nI`m moving a project from r0.11 to r0.10, and I have some bug in 0.10 running this script:\r\n```\r\n      print('before', label.get_shape())\r\n      label = tf.image.resize_image_with_crop_or_pad(label, h, w)\r\n      print('after',label.get_shape())\r\n```\r\nthe results is :\r\n```\r\n('before', TensorShape([Dimension(None), Dimension(None), Dimension(1)]))\r\n('after', TensorShape([Dimension(321), Dimension(321), Dimension(None)]))\r\n```\r\nbut when I test in r0,11, the results:\r\n```\r\n('before', TensorShape([Dimension(None), Dimension(None), Dimension(1)]))\r\n('after', TensorShape([Dimension(321), Dimension(321), Dimension(1)]))\r\n```\r\nthe input of the above two functions is the same.\r\nI can not find where it gets wrong.\r\nany one can help me?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8304, "title": "tf_upgrade should rename tf.batch_fft2d to tf.fft2d", "body": "Updating a module with `tf_upgrade.py` should rename tf.batch_fft2d to tf.fft2d similarly to the others fft / ifft functions.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 8303, "title": "Readded build rule for image resize benchmarks, extended for bilinear.", "body": "Build rule was deleted somewhere along the way. Connected to discussion of issue #533. Benchmark results on my configuration (GTX 960):\r\n\r\n```\r\nBenchmark                                  Time(ns) Iterations\r\n--------------------------------------------------------------\r\nBM_Resize_NearestNeighbor_cpu_10_499_499   36575420        100 204.2M items/s\r\nBM_Resize_NearestNeighbor_gpu_10_499_499    5042070        100 1481.5M items/s\r\nBM_Resize_Bilinear_cpu_10_499_499          31493990        100 237.2M items/s\r\nBM_Resize_Bilinear_gpu_10_499_499           5754130        100 1298.2M items/s\r\n```", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@panmari. Sanity check on the BUILD file failed. I think it's because you are using 3-space indent, instead of the required 4-space indent. Please fix.", "@tensorflow-jenkins test this please", "Anything else you need from my side?"]}, {"number": 8302, "title": "Can't generate documentations", "body": "I cannot run gen_docs.sh on my machine.\r\n\r\nI already run the configuration script.\r\n\r\nThis is the last error message\r\nhttp://sprunge.us/MVJS\r\n\r\nI am using tensorflow 1.0 with CUDA capability\r\nhttp://sprunge.us/dhQT\r\n\r\nThis is my list of CUDA libs\r\nhttp://sprunge.us/Gdfh\r\n\r\nI am on arch linux btw.\r\n\r\nWhat sould i do ?", "comments": ["@marulitua None of the links in your issue report work for me.\r\n\r\nPlease provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "I am on Arch Linux 64 bit\r\n`uname -a`\r\n```\r\nLinux singularity 4.10.1-1-ARCH #1 SMP PREEMPT Sun Feb 26 21:08:53 UTC 2017 x86_64 GNU/Linux\r\n```\r\n\r\nI am using tensorflow 1.0 with CUDA capability\r\n`python -c \"import tensorflow; print(tensorflow.__version__)\"`\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```\r\n\r\n`ls -l /opt/cuda/lib64`\r\n```\r\ntotal 1415152\r\n-rw-r--r-- 1 root root  53464088 Feb 28 18:43 libcublas_device.a\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libcublas.so -> libcublas.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libcublas.so.8.0 -> libcublas.so.8.0.61\r\n-rwxr-xr-x 1 root root  42505456 Feb 28 18:43 libcublas.so.8.0.61\r\n-rw-r--r-- 1 root root  49080634 Feb 28 18:43 libcublas_static.a\r\n-rw-r--r-- 1 root root    556000 Feb 28 18:43 libcudadevrt.a\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root root    415432 Feb 28 18:43 libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root    775162 Feb 28 18:43 libcudart_static.a\r\n-rwxr-xr-x 1 root root  79337624 Dec 25 15:14 libcudnn.so\r\n-rwxr-xr-x 1 root root  79337624 Dec 25 15:14 libcudnn.so.5\r\n-rwxr-xr-x 1 root root  79337624 Dec 25 15:14 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root  69756172 Dec 25 15:14 libcudnn_static.a\r\nlrwxrwxrwx 1 root root        15 Feb 28 18:43 libcufft.so -> libcufft.so.8.0\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libcufft.so.8.0 -> libcufft.so.8.0.61\r\n-rwxr-xr-x 1 root root 146765440 Feb 28 18:43 libcufft.so.8.0.61\r\n-rw-r--r-- 1 root root 129661426 Feb 28 18:43 libcufft_static.a\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libcufftw.so -> libcufftw.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libcufftw.so.8.0 -> libcufftw.so.8.0.61\r\n-rwxr-xr-x 1 root root    476840 Feb 28 18:43 libcufftw.so.8.0.61\r\n-rw-r--r-- 1 root root     42294 Feb 28 18:43 libcufftw_static.a\r\nlrwxrwxrwx 1 root root        17 Feb 28 18:43 libcuinj64.so -> libcuinj64.so.8.0\r\nlrwxrwxrwx 1 root root        20 Feb 28 18:43 libcuinj64.so.8.0 -> libcuinj64.so.8.0.61\r\n-rwxr-xr-x 1 root root   6403784 Feb 28 18:43 libcuinj64.so.8.0.61\r\n-rw-r--r-- 1 root root   1649302 Feb 28 18:43 libculibos.a\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libcurand.so -> libcurand.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libcurand.so.8.0 -> libcurand.so.8.0.61\r\n-rwxr-xr-x 1 root root  59163968 Feb 28 18:43 libcurand.so.8.0.61\r\n-rw-r--r-- 1 root root  59359140 Feb 28 18:43 libcurand_static.a\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libcusolver.so -> libcusolver.so.8.0\r\nlrwxrwxrwx 1 root root        21 Feb 28 18:43 libcusolver.so.8.0 -> libcusolver.so.8.0.61\r\n-rwxr-xr-x 1 root root  53866552 Feb 28 18:43 libcusolver.so.8.0.61\r\n-rw-r--r-- 1 root root  22386284 Feb 28 18:43 libcusolver_static.a\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libcusparse.so -> libcusparse.so.8.0\r\nlrwxrwxrwx 1 root root        21 Feb 28 18:43 libcusparse.so.8.0 -> libcusparse.so.8.0.61\r\n-rwxr-xr-x 1 root root  43034120 Feb 28 18:43 libcusparse.so.8.0.61\r\n-rw-r--r-- 1 root root  51650816 Feb 28 18:43 libcusparse_static.a\r\nlrwxrwxrwx 1 root root        14 Feb 28 18:43 libnppc.so -> libnppc.so.8.0\r\nlrwxrwxrwx 1 root root        17 Feb 28 18:43 libnppc.so.8.0 -> libnppc.so.8.0.61\r\n-rwxr-xr-x 1 root root    456512 Feb 28 18:43 libnppc.so.8.0.61\r\n-rw-r--r-- 1 root root     24512 Feb 28 18:43 libnppc_static.a\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libnppial.so -> libnppial.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libnppial.so.8.0 -> libnppial.so.8.0.61\r\n-rwxr-xr-x 1 root root  10116032 Feb 28 18:43 libnppial.so.8.0.61\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libnppicc.so -> libnppicc.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libnppicc.so.8.0 -> libnppicc.so.8.0.61\r\n-rwxr-xr-x 1 root root   3790368 Feb 28 18:43 libnppicc.so.8.0.61\r\nlrwxrwxrwx 1 root root        17 Feb 28 18:43 libnppicom.so -> libnppicom.so.8.0\r\nlrwxrwxrwx 1 root root        20 Feb 28 18:43 libnppicom.so.8.0 -> libnppicom.so.8.0.61\r\n-rwxr-xr-x 1 root root   1030800 Feb 28 18:43 libnppicom.so.8.0.61\r\nlrwxrwxrwx 1 root root        17 Feb 28 18:43 libnppidei.so -> libnppidei.so.8.0\r\nlrwxrwxrwx 1 root root        20 Feb 28 18:43 libnppidei.so.8.0 -> libnppidei.so.8.0.61\r\n-rwxr-xr-x 1 root root   7098600 Feb 28 18:43 libnppidei.so.8.0.61\r\nlrwxrwxrwx 1 root root        15 Feb 28 18:43 libnppif.so -> libnppif.so.8.0\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libnppif.so.8.0 -> libnppif.so.8.0.61\r\n-rwxr-xr-x 1 root root  47868176 Feb 28 18:43 libnppif.so.8.0.61\r\nlrwxrwxrwx 1 root root        15 Feb 28 18:43 libnppig.so -> libnppig.so.8.0\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libnppig.so.8.0 -> libnppig.so.8.0.61\r\n-rwxr-xr-x 1 root root  21052640 Feb 28 18:43 libnppig.so.8.0.61\r\nlrwxrwxrwx 1 root root        15 Feb 28 18:43 libnppim.so -> libnppim.so.8.0\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libnppim.so.8.0 -> libnppim.so.8.0.61\r\n-rwxr-xr-x 1 root root   4333920 Feb 28 18:43 libnppim.so.8.0.61\r\nlrwxrwxrwx 1 root root        14 Feb 28 18:43 libnppi.so -> libnppi.so.8.0\r\nlrwxrwxrwx 1 root root        17 Feb 28 18:43 libnppi.so.8.0 -> libnppi.so.8.0.61\r\n-rwxr-xr-x 1 root root 108924816 Feb 28 18:43 libnppi.so.8.0.61\r\n-rw-r--r-- 1 root root 136645168 Feb 28 18:43 libnppi_static.a\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libnppist.so -> libnppist.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libnppist.so.8.0 -> libnppist.so.8.0.61\r\n-rwxr-xr-x 1 root root  14270440 Feb 28 18:43 libnppist.so.8.0.61\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libnppisu.so -> libnppisu.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libnppisu.so.8.0 -> libnppisu.so.8.0.61\r\n-rwxr-xr-x 1 root root    448016 Feb 28 18:43 libnppisu.so.8.0.61\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libnppitc.so -> libnppitc.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libnppitc.so.8.0 -> libnppitc.so.8.0.61\r\n-rwxr-xr-x 1 root root   2897184 Feb 28 18:43 libnppitc.so.8.0.61\r\nlrwxrwxrwx 1 root root        14 Feb 28 18:43 libnpps.so -> libnpps.so.8.0\r\nlrwxrwxrwx 1 root root        17 Feb 28 18:43 libnpps.so.8.0 -> libnpps.so.8.0.61\r\n-rwxr-xr-x 1 root root   8178184 Feb 28 18:43 libnpps.so.8.0.61\r\n-rw-r--r-- 1 root root  10741440 Feb 28 18:43 libnpps_static.a\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libnvblas.so -> libnvblas.so.8.0\r\nlrwxrwxrwx 1 root root        19 Feb 28 18:43 libnvblas.so.8.0 -> libnvblas.so.8.0.61\r\n-rwxr-xr-x 1 root root    498088 Feb 28 18:43 libnvblas.so.8.0.61\r\nlrwxrwxrwx 1 root root        17 Feb 28 18:43 libnvgraph.so -> libnvgraph.so.8.0\r\nlrwxrwxrwx 1 root root        20 Feb 28 18:43 libnvgraph.so.8.0 -> libnvgraph.so.8.0.61\r\n-rwxr-xr-x 1 root root   5213656 Feb 28 18:43 libnvgraph.so.8.0.61\r\n-rw-r--r-- 1 root root   8169056 Feb 28 18:43 libnvgraph_static.a\r\nlrwxrwxrwx 1 root root        24 Feb 28 18:43 libnvrtc-builtins.so -> libnvrtc-builtins.so.8.0\r\nlrwxrwxrwx 1 root root        27 Feb 28 18:43 libnvrtc-builtins.so.8.0 -> libnvrtc-builtins.so.8.0.61\r\n-rwxr-xr-x 1 root root   9656680 Feb 28 18:43 libnvrtc-builtins.so.8.0.61\r\nlrwxrwxrwx 1 root root        15 Feb 28 18:43 libnvrtc.so -> libnvrtc.so.8.0\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libnvrtc.so.8.0 -> libnvrtc.so.8.0.61\r\n-rwxr-xr-x 1 root root  18512120 Feb 28 18:43 libnvrtc.so.8.0.61\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libnvToolsExt.so -> libnvToolsExt.so.1\r\nlrwxrwxrwx 1 root root        22 Feb 28 18:43 libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\r\n-rwxr-xr-x 1 root root     37136 Feb 28 18:43 libnvToolsExt.so.1.0.0\r\nlrwxrwxrwx 1 root root        14 Feb 28 18:43 libOpenCL.so -> libOpenCL.so.1\r\nlrwxrwxrwx 1 root root        16 Feb 28 18:43 libOpenCL.so.1 -> libOpenCL.so.1.0\r\nlrwxrwxrwx 1 root root        18 Feb 28 18:43 libOpenCL.so.1.0 -> libOpenCL.so.1.0.0\r\n-rw-r--r-- 1 root root     25840 Feb 28 18:43 libOpenCL.so.1.0.0\r\ndrwxr-xr-x 2 root root      4096 Mar  5 16:29 stubs\r\n```\r\n\r\n`ls -lah /usr/lib/libcud*`\r\n```\r\nlrwxrwxrwx 1 root root   12 Feb 27 17:36 /usr/lib/libcuda.so -> libcuda.so.1\r\nlrwxrwxrwx 1 root root   17 Feb 27 17:36 /usr/lib/libcuda.so.1 -> libcuda.so.378.13\r\n-rwxr-xr-x 2 root root 8.3M Feb 27 17:36 /usr/lib/libcuda.so.378.13\r\n```\r\n\r\n`bazel info`\r\n```\r\nWarning: ignoring _JAVA_OPTIONS in environment.\r\nbazel-bin: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin\r\nbazel-genfiles: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/genfiles\r\nbazel-testlogs: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/testlogs\r\ncommand_log: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/command.log\r\ncommitted-heap-size: 838MB\r\nexecution_root: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow\r\ngc-count: 24\r\ngc-time: 1347ms\r\ninstall_base: /home/maruli/.cache/bazel/_bazel_maruli/install/b9af16fe5222092c9bdd576f898b1e18\r\nmax-heap-size: 1825MB\r\nmessage_log: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/message.log\r\noutput_base: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a\r\noutput_path: /home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out\r\npackage_path: %workspace%\r\nrelease: release 0.4.4- (@non-git)\r\nserver_pid: 12761\r\nused-heap-size: 422MB\r\nworkspace: /media/uno/repos/tensorflow\r\n```\r\nGit commit hash\r\n`git rev-parse HEAD`\r\n```\r\nf1ffbc548906e379d423633f76edc558be8284e7\r\n```\r\n\r\nI got the following error when i run gen_docs.sh\r\n\r\n```\r\n/media/uno/repos/tensorflow/tensorflow /media/uno/repos/tensorflow/tensorflow/tools/docs\r\nWarning: ignoring _JAVA_OPTIONS in environment.\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /media/uno/repos/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /media/uno/repos/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nTarget //tensorflow/python:gen_docs_combined up-to-date:\r\n  bazel-bin/tensorflow/python/gen_docs_combined\r\nINFO: Elapsed time: 7.777s, Critical Path: 6.52s\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/python/gen_docs_combined '--out_dir=/media/uno/repos/tensorflow/tensorflow/g3doc/api_docs/python'\r\nTraceback (most recent call last):\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/gen_docs_combined.runfiles/org_tensorflow/tensorflow/python/framework/gen_docs_combined.py\", line 332, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/gen_docs_combined.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/gen_docs_combined.runfiles/org_tensorflow/tensorflow/python/framework/gen_docs_combined.py\", line 286, in main\r\n    module_to_name = get_module_to_name(module_names())\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/gen_docs_combined.runfiles/org_tensorflow/tensorflow/python/framework/gen_docs_combined.py\", line 112, in get_module_to_name\r\n    return collections.OrderedDict([(find_module(tf, x), x) for x in names])\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/gen_docs_combined.runfiles/org_tensorflow/tensorflow/python/framework/gen_docs_combined.py\", line 112, in <listcomp>\r\n    return collections.OrderedDict([(find_module(tf, x), x) for x in names])\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/gen_docs_combined.runfiles/org_tensorflow/tensorflow/python/framework/gen_docs_combined.py\", line 102, in find_module\r\n    name, s, parent_module.__name__, dir(parent_module)))\r\nValueError: Module not found: tf.contrib.distributions.bijector. Submodule bijector not found in parent module tensorflow.contrib.distributions. Possible candidates are ['Bernoulli', 'BernoulliWithSigmoidProbs', 'Beta', 'BetaWithSoftplusConcentration', 'Binomial', 'Categorical', 'Chi2', 'Chi2WithAbsDf', 'ConditionalDistribution', 'ConditionalTransformedDistribution', 'Dirichlet', 'DirichletMultinomial', 'Distribution', 'ExpRelaxedOneHotCategorical', 'Exponential', 'ExponentialWithSoftplusRate', 'FULLY_REPARAMETERIZED', 'Gamma', 'GammaWithSoftplusConcentrationRate', 'Geometric', 'InverseGamma', 'InverseGammaWithSoftplusConcentrationRate', 'Laplace', 'LaplaceWithSoftplusScale', 'Logistic', 'Mixture', 'Multinomial', 'MultivariateNormalDiag', 'MultivariateNormalDiagPlusLowRank', 'MultivariateNormalDiagWithSoftplusScale', 'MultivariateNormalTriL', 'NOT_REPARAMETERIZED', 'NegativeBinomial', 'Normal', 'NormalWithSoftplusScale', 'OneHotCategorical', 'Poisson', 'QuantizedDistribution', 'RegisterKL', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReparameterizationType', 'StudentT', 'StudentTWithAbsDfSoftplusScale', 'TransformedDistribution', 'Uniform', 'WishartCholesky', 'WishartFull', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_allowed_symbols', 'kl', 'matrix_diag_transform', 'normal_conjugates_known_scale_posterior', 'normal_conjugates_known_scale_predictive', 'softplus_inverse']\r\nERROR: Non-zero return code '1' from command: Process exited with status 1.\r\n```\r\n\r\nBtw, when i run the configuration script i choose to not build tensorflow with cuda capability.\r\nI don't know if it have any correlation with the error. \r\n", "@langmore The error seems to be related to `tf.contrib.distributions`.  Have you any idea why this might be happening?", "This is probably related to [this change](https://github.com/tensorflow/tensorflow/commit/988a443383a56619aeaa73354146579addfb13ad)", "@langmore Thanks.", "I don't think it's caused by that change. Wonder where the reference to distributions.bijector is from... Perhaps an outdated me file?  Josh?", "gen_docs.sh is no longer the way to generate documentation.  See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc\r\nfor the new commands to run.", "@josh11b I am on commit 03d31c77703304087d66bd0f608a3de745fadf67 \r\nbut when i run \r\n`\r\nbazel run -- tensorflow/tools/docs:generate \\\r\n    --src_dir=tensorflow/docs_src/ \\\r\n    --output_dir=/tmp/tfdocs/\r\n`\r\n\r\nit produced the following error\r\n```\r\ntensorflow \u27a4 bazel run -- tensorflow/tools/docs:generate \\                                                                              git:master\r\n    --src_dir=tensorflow/docs_src/ \\\r\n    --output_dir=/tmp/tfdocs/\r\nWarning: ignoring _JAVA_OPTIONS in environment.\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /media/uno/repos/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /media/uno/repos/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/docs:generate up-to-date:\r\n  bazel-bin/tensorflow/tools/docs/generate\r\nINFO: Elapsed time: 0.496s, Critical Path: 0.00s\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/tools/docs/generate '--src_dir=tensorflow/docs_src/' '--output_dir=/tmp/tfdocs/'\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWriting docs for tf (<module 'tensorflow' from '/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/__init__.py'>).\r\nERROR: Handle doc reference \"@{$adding_an_op}\"\r\nERROR: Handle doc reference \"@{$variables}\"\r\nERROR: Handle doc reference \"@{$variable_scope}\"\r\nERROR: Handle doc reference \"@{$variable_scope}\"\r\nERROR: Handle doc reference \"@{$python/image}\"\r\nERROR: Handle doc reference \"@{$python/nn}\"\r\nERROR: Handle doc reference \"@{$python/python_io}\"\r\nERROR: Handle doc reference \"@{$python/summary}\"\r\nERROR: Handle doc reference \"@{$python/test}\"\r\nERROR: Handle doc reference \"@{$python/train}\"\r\nERROR: Handle doc reference \"@{$variable_scope}\"\r\nWriting docs for tf.AggregationMethod (<class 'tensorflow.python.ops.gradients_impl.AggregationMethod'>).\r\nWriting docs for tf.Assert (<function Assert at 0x7f76c12a99d8>).\r\nWriting docs for tf.AttrValue (<class 'tensorflow.core.framework.attr_value_pb2.AttrValue'>).\r\nWriting docs for tf.ConditionalAccumulator (<class 'tensorflow.python.ops.data_flow_ops.ConditionalAccumulator'>).\r\nTraceback (most recent call last):\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate.py\", line 51, in <module>\r\n    sys.exit(doc_generator.build(flags))\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py\", line 494, in build\r\n    yaml_toc=self.yaml_toc)\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py\", line 131, in write_docs\r\n    f.write(pretty_docs.build_md_page(page_info))\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/pretty_docs.py\", line 41, in build_md_page\r\n    return _build_class_page(page_info)\r\n  File \"/home/maruli/.cache/bazel/_bazel_maruli/07eb648f17e77e5bc907dc6be9df5c3a/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/pretty_docs.py\", line 128, in _build_class_page\r\n    parts.append(h3.format(**method_info.__dict__))\r\nAttributeError: '_MethodInfo' object has no attribute '__dict__'\r\nERROR: Non-zero return code '1' from command: Process exited with status 1.\r\n```\r\nI think we still have the problem.", "I think Martin is looking at this.", "Are you running this on python3? I doubt the doc generator works on python3 at the moment. ", "Yes, indeed. I am using python3. I am sorry but  i cannot try it on python2.\r\nSo, i think we can close this issue.\r\nThanks all."]}, {"number": 8301, "title": "Feedback on \"A Guide to TF Layers: Building a Convolutional Neural Network\" tutorial", "body": "This [tutorial](https://www.tensorflow.org/tutorials/layers) seems to contain some errors:\r\n\r\n# Convolutional Layer #1\r\n\r\n> The filters argument specifies the number of filters to apply (here, 32)\r\n\r\nAre the filters learnt? In that case, you could mention it. Else, how do you choose the types of the filters?\r\n\r\n> Our output tensor produced by conv2d() has a shape of [batch_size, 28, 28, 1]: the same width and height dimensions as the input, but now with 32 channels holding the output from each of the filters.\r\n\r\nIs not the shape `[batch_size, 28, 28, 32]` instead?\r\n\r\n# Pooling Layer #1\r\n\r\n> Our output tensor produced by max_pooling2d() (pool1) has a shape of [batch_size, 14, 14, 1]\r\n\r\nIsn't it `[batch_size, 14, 14, 32]`, as the pooling does not reduce the number of channels?\r\n\r\n> the 2x2 filter reduces width and height by 50%.\r\n\r\nMaybe you could say that the total size is reduced by 75% (we keep one pixel out of four).", "comments": ["Hi @Vayel, thanks for reporting.\r\n\r\n_Are the filters learnt? In that case, you could mention it. Else, how do you choose the types of the filters?_\r\n\r\nNot sure I understand your question. These are typical convolution filters where the weights will be learned via gradient descent during training.\r\n\r\n_Is not the shape `[batch_size, 28, 28, 32]` instead?_\r\n\r\nYes. Looks like this is already fixed.\r\n\r\n_Isn't it `[batch_size, 14, 14, 32]`, as the pooling does not reduce the number of channels?_\r\n\r\nYep. Just fixed this.\r\n\r\n_Maybe you could say that the total size is reduced by 75% (we keep one pixel out of four)._\r\n\r\nI changed the text to read \"the 2x2 filter reduces width and height by 50% each.\"\r\n\r\nYou should see the updates pushed out to the website within 3-4 days."]}]