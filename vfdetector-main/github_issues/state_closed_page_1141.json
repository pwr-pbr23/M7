[{"number": 18984, "title": " returned non-zero exit status -9", "body": " I am using gcloud ml-engine. I can run tensorflow package locally and on the cloud with non-distributed version. When I run in the distributed using STANDARD_1 it gives me the following error:\r\n{\r\n insertId:  \"tfdl1gg1hisuso\"  \r\n jsonPayload: {\r\n  created:  1525101438.307529   \r\n  levelname:  \"ERROR\"   \r\n  lineno:  889   \r\n  message:  \"Command '['python', '-m', u'trainer_1_4.task', u'--train-data-file', u'gs://main-175118-accern/tf_data/train_1*', u'--num-epochs', u'1', u'--train-batch-size', u'10', u'--eval-batch-size', u'10', u'--eval-data-file', u'gs://main-175118-accern/tf_data/test_1*', '--job-dir', u'gs://main-175118-accern/accern_1_4']' returned non-zero exit status -9\"   \r\n  pathname:  \"/runcloudml.py\"   \r\n }\r\n labels: {\r\n  compute.googleapis.com/resource_id:  \"1659456639612915557\"   \r\n  compute.googleapis.com/resource_name:  \"cmle-training-worker-7bc72ccf46-0-qz5zj\"   \r\n  compute.googleapis.com/zone:  \"us-central1-f\"   \r\n  ml.googleapis.com/job_id:  \"accern_1_4_dist_1\"   \r\n  ml.googleapis.com/job_id/log_area:  \"root\"   \r\n  ml.googleapis.com/task_name:  \"worker-replica-0\"   \r\n  ml.googleapis.com/trial_id:  \"\"   \r\n }\r\n logName:  \"projects/main-175118/logs/worker-replica-0\"  \r\n receiveTimestamp:  \"2018-04-30T15:17:23.426890073Z\"  \r\n resource: {\r\n  labels: {\u2026}   \r\n  type:  \"ml_job\"   \r\n }\r\n severity:  \"ERROR\"  \r\n timestamp:  \"2018-04-30T15:17:18.307528972Z\"  \r\n}", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: YES\r\nOS Platform and Distribution: LINUX 16.04\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.5\r\nBazel version: 0.10.1\r\nCUDA/cuDNN version: RUN ON THE CLOUD\r\nGPU model and memory: RUN ON THE CLOUD\r\nExact command to reproduce: gcloud ml-engine jobs submit training \r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18983, "title": "Branch 194768567", "body": "", "comments": []}, {"number": 18982, "title": "TensorFlow Error : java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows", "body": "Hello everyone I have this error when I compile my project tensorflow on netbeans\r\n\r\n```\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)\r\n at org.tensorflow.TensorFlow.init(TensorFlow.java:66) at org.tensorflow.TensorFlow.(TensorFlow.java:70)\r\n at org.tensorflow.Graph.(Graph.java:258) at HelloTF.main(HelloTF.java:8)\r\n C:\\Users\\HP Notebook\\AppData\\Local\\NetBeans\\Cache\\8.2\\executor-enter codeheresnippets\\run.xml:53: Java returned: 1 BUILD FAILED (total time: 0 seconds)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "As suggested with the error message, could you try after setting `-Dorg.tensorflow.NativeLibrary.DEBUG=1` in the JVM? That will provide additional information about what's missing.\r\n\r\nRegardless, do fill out the issue template including ideally a [minimal, complete, verifiable](https://stackoverflow.com/help/mcve) set of steps to reproduce.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "\r\nUnfortunately I still have not been able to fix the problem, so I will try to test with another work environment to see what will happen", "Nagging Assignee @skye: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@OrionWambert : please update this issue with the results of https://github.com/tensorflow/tensorflow/issues/18982#issuecomment-385728771 when you get a chance.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "This is still an issue..\r\n\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n    at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)\r\n    at org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n    at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n    at org.tensorflow.Graph.<clinit>(Graph.java:361)\r\n    at org.tf.sample.hellotensorflow.HelloTensorFlow.main(HelloTensorFlow.java:10)\r\n\r\nI am trying to run sample code from tensorflow.org", "@kishorecs : You might want to outline the exact set of steps to reproduce the problem as suggested in https://github.com/tensorflow/tensorflow/issues/18982#issuecomment-385728771", "Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:479)\r\n\tat forecast.HelloTensorFlow.main(HelloTensorFlow.java:19)\r\nJava Result: 1", "> Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n> at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77) at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)\r\n> at org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n> at org.tensorflow.TensorFlow.(TensorFlow.java:70)\r\n> at org.tensorflow.Graph.(Graph.java:479)\r\n> at forecast.HelloTensorFlow.main(HelloTensorFlow.java:19)\r\n> Java Result: 1\r\n\r\nUnfortunately, I still meet the problem too. It's sad.", "When you have tensor flow dependency in your pom.xml file as below\r\n\r\n<dependencies>\r\n<dependency>\r\n  <groupId>org.tensorflow</groupId>\r\n  <artifactId>tensorflow</artifactId>\r\n  <version>1.14.0</version>\r\n</dependency>\r\nIt will download required 3 libraries tensorflow-1.14.0.jar libtensorflow-1.14.0.jar libtensorflow_jni-1.14.0.jar\r\n\r\nSo you have to manually extract the 3rd jar(libtensorflow_jni) and get your OS compatible file like for windows you have to copy the tensorflow_jni.dll file and paste in your project root directory that will solve your problem."]}, {"number": 18981, "title": "Update README", "body": "Adding an additional demo link.", "comments": []}, {"number": 18980, "title": "error when comiling Tensorflow 1.8", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow) N/A:\r\n- **OS Platform and Distribution:Linux Debian 9.1:\r\n- **TensorFlow installed from (source or binary) bazel:\r\n- **TensorFlow version (use command below) v1.8.0:\r\n- **Python version 3.7: \r\n- **Bazel version (if compiling from source)0.11:\r\n- **GCC/Compiler version (if compiling from source) 6.3 and 4.9:\r\n- **CUDA/cuDNN version 7.1:\r\n- **GPU model and memory GTX 1080 TI, 11GB, 48GB:\r\n- **Exact command to reproduce: gcc:\r\n\r\n### Describe the problem\r\nI could not able to install Tensorflow V1.8.0 on my machine. I used different gcc and g++ versions 4.9 and 6.1 in Debian 9. Furthermore, I got the same errors.\r\n\r\n### Error:\r\nERROR: /home/pm/local/cpp/TENSOR_FLOW_180/tensorflow/tensorflow/core/kernels/BUILD:1864:1: output 'tensorflow/core/kernels/_objs/eye_functor_gpu/tensorflow/core/kernels/eye_functor_gpu.cu.pic.o' was not created\r\nERROR: /home/pm/local/cpp/TENSOR_FLOW_180/tensorflow/tensorflow/core/kernels/BUILD:1864:1: not all outputs were created or valid\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 66.847s, Critical Path: 57.74s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n", "comments": ["Same error here. Tried different versions of Bazel (0.12.0/0.11.1) and GCC (7.3.1/6.4.1) but the issue persisted.\r\n\r\nI was building TF against CUDA 9.1, CuDNN 7.2 and NCCL 2.1.15.\r\n\r\nPerhaps related to #18522?\r\n\r\nEdit: Btw, Arch already has the `_BITS_FLOATN_H` patch in their official repo.", "I am also facing the same issue. I am using similar software versions as @Frederick888 ", "@annarev Any ideas here?", "@Frederick888 can you try an earlier version of gcc? gcc 6.4 and later are not listed in compatibility table for cuda 9.1: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements.", "The error messages you copied are helpful, but does not contain the actual failure message.\r\nCould you copy your full terminal output to pastebin and share here?\r\n\r\nAlso, as @annarev stated CUDA does not seem to support GCC 6.4. As we are building on top of CUDA, there is not much we can do until cuda fully supports the gcc version.", "Tried GCC 5.5.0 and failed with\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/adjust_saturation_op_gpu.cu.cc:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9220): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9231): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9244): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9255): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9268): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9279): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9292): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9303): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9316): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9327): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9340): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9352): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9365): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9376): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9389): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9401): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9410): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9419): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9428): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9437): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9445): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9454): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9463): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9472): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9481): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9490): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9499): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9508): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9517): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9526): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9535): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512fintrin.h(9544): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(55): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(63): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(73): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(81): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(91): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(100): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(109): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(117): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(127): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(136): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(145): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512pfintrin.h(153): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10799): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10811): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10823): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10835): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10847): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10859): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10871): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10883): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10895): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10907): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10919): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10931): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10943): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10955): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10967): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10979): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(10989): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11000): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11009): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11020): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11029): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11040): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11049): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11060): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11069): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11080): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11089): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11100): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11109): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11120): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11129): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11140): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11149): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11160): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11169): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11180): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11189): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11200): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11209): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11220): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11229): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11240): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11249): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11260): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11269): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11280): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11289): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/5.5.0/include/avx512vlintrin.h(11300): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n92 errors detected in the compilation of \"/tmp/tmpxft_00005ce1_00000000-6_adjust_saturation_op_gpu.cu.cpp1.ii\".\r\nERROR: /home/frederick/.virtualenvs/pylab/tensorflow/tensorflow/core/kernels/BUILD:2091:1: output 'tensorflow/core/kernels/_objs/adjust_saturation_op_gpu/tensorflow/core/kernels/adjust_saturation_op_gpu.cu.o' was not created\r\nERROR: /home/frederick/.virtualenvs/pylab/tensorflow/tensorflow/core/kernels/BUILD:2091:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 472.571s, Critical Path: 42.11s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n...should I get a GCC copy of exactly 6.3.0?", "You are trying to build with avx512 support I think.\r\nThat is still not officially supported.", "~building with avx512 not being officially supported is a bit weird given that it's done automatically if you use the `-march=native` configuration option~ woops, just realized that the `-march=native` option is passed directly to gcc as opposed to being something that bazel processes itself\r\n\r\ni get the same error as @Frederick888 when I compile the _r1.7_ branch with GCC 5.5.0 (CUDA 9.1, cuDNN 7.1, Bazel 0.13.0) (I was also getting the same error on r1.8)\r\n\r\n", "@gunan You meant that I should disable it via any arguments?\r\nFYI my build command was\r\n```\r\nbazel build --config=opt --config=cuda --copt=-march=native --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n```", "I have the same issue on mac os with LLVM 8.1.0, XCode 8.3.3, CUDA 9.0 and cuDNN 7.0", "if you add `--config=opt` on a machine that has AVX512 instruction set, you will still get the error.\r\nSo I recommend:\r\n```\r\nbazel build --config=cuda --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n", "@gunan Same error with this build command :(", "i managed to get it to build with gcc-4.8, and using the build command:\r\n```\r\nbazel build --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nunfortunately, this doesn't perform any additional compiler optimizations :(\r\n\r\nUPDATE: ok so i managed to get it to build (under gcc-4.8) with `--config=opt` as well (using the default `-march=native` configuration option)\r\n\r\nfor the record, i'm using bazel 0.12.0, cuDNN 7.1, CUDA 9.1\r\n\r\nnot sure if this fixes OP's original issue, but it does fix the `avx512fintrin.h` issue", "@emerali Thanks for the info! I also successfully compiled with the following setup:\r\n```\r\nBazel          0.13.0\r\nCUDA           9.1.85.3\r\ncuDNN          7.1.2\r\nNCCL           2.1.15\r\nComputeCpp-CE  0.7.0\r\nGCC            4.9.4\r\n\r\nbazel build --config=opt --config=cuda --copt=-march=native --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n```", "I was able to sucessfully compile with:\r\n\r\n```\r\nBazel      0.13.0\r\nCUDA       9.1.85.3\r\ncuDNN      7.1.3\r\nNCCL       2.1.15\r\nGCC        5.4 (5.5 did not work)\r\n\r\n./configure\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n```", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18979, "title": "TOCO converter requires min/max information for taking output from intermediate nodes?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:No.\r\n- **TensorFlow version (use command below)**:v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:Nvidia 840M\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nI have a quantized mobilenet downloaded from the official repository([here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)) , I want to collect output from the last pointwise convolutional layer of the network and run on a mobile.\r\n\r\nThe quantized frozen model contains additional fc,softmax etc layers that are of no use for my application.\r\n\r\nTo convert my quantized and frozen graph to specified input & output i run the below command\r\n```\r\ntoco \\\r\n  --input_file=mobilenet_v1_1.0_224_quant_frozen.pb \\\r\n  --output_file=corrected_mobilenet_v1_1.0_224_frozen.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=\"1,224,224,3\" \\\r\n  --input_array=input \\\r\n  --output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold \\\r\n  --std_value=127.5 --mean_value=127.5\r\n```\r\nand receive the following output from the converter\r\n```\r\nWARNING:tensorflow:From /home/sam/.virtualenvs/tf6/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\n2018-04-30 16:31:24.735099: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 598 operators, 889 arrays (0 quantized)\r\n2018-04-30 16:31:24.753483: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 571 operators, 851 arrays (0 quantized)\r\n2018-04-30 16:31:24.773414: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 571 operators, 851 arrays (0 quantized)\r\n2018-04-30 16:31:25.122835: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 54 operators, 109 arrays (1 quantized)\r\n2018-04-30 16:31:25.123855: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 54 operators, 109 arrays (1 quantized)\r\n2018-04-30 16:31:25.124206: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 28 operators, 83 arrays (1 quantized)\r\n2018-04-30 16:31:25.124569: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 28 operators, 83 arrays (1 quantized)\r\n2018-04-30 16:31:25.155063: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:169] Array MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\r\nAborted (core dumped)\r\n```\r\n\r\nWhy does this quantized graph requires min max information for intermediate nodes when it can resolve the final nodes correctly without additional min/max information?\r\n\r\nI also tried manually giving default min/max information but i suffer a huge performance drop if i do this.\r\n\r\n\r\n\r\n\r\n", "comments": ["The flag,\r\n\r\n```--output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold```\r\n\r\nis asking toco to cut this graph at this point, i.e. immediately after the Conv2D op, before the subsequent BiasAdd, ReLU6... and FakeQuantWithMinMaxVars.\r\n\r\nThe resulting cropped graph thus does not have anymore the required min-max info for the output node.\r\n\r\nPassing arbitrary --input_arrays , --output_arrays to cut graphs is fine, as long as the required granularity of the graph is preserved. When quantizing (--inference_type=QUANTIZED_UINT8), that required granularity is whichever granularity FakeQuant nodes are inserted at, e.g. typically whole fused layers (Conv2D+BiasAdd+Relu6).\r\n\r\nTry this instead:\r\n\r\n```\r\n--output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars\r\n```", "@bjacob Thanks for the response,\r\nYes , I have tried \r\n`--output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars\r\n`\r\nBut the output i want should be directly coming from` Conv2d_13_pointwise/Conv2D_Fold` layer without the activation.\r\n\r\nI was thinking if we are performing quantized inference than quantization on a mobile device would be happening after each step and not after a certain number of operations like Convolution followed by Batchnormalization and RELU6, so it must be possible to get this intermediate quantized result?\r\n\r\nOr is it that at inference time some float operations are still being performed which get changed to 8bits after certain steps?\r\n\r\nI might be wrong on this , but thats what i understood.", "There is no 1:1 mapping of TensorFlow nodes to quantized operations.\r\nTensorFlow nodes are fused in a specific way (Conv+Bias+Relu) that corresponds to the placement of FakeQuant nodes; this is what allows the training process to effectively train for quantization.\r\nThus, when performing quantized inference, it is not possible to let --output_array point to arbitrary nodes in the original TF graph, as these nodes just don't exist in the quantized inference workload.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to inactivity. Please re-open if there are remaining questions we have missed. Thanks!"]}, {"number": 18978, "title": "Tensorflow on imx6", "body": "Hi,\r\nI am trying to port tensorflow on imx6 board,\r\nFirst i installed bazel and tensorflow from source it got installed but when i did import tensorflow i was getting this:\r\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python2.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python2.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\nThen in tried to install using tensorflow makefile it also got installed but when i ran my own .pb file it gave this error:\r\n [[Node: MultipleGridAnchorGenerator/assert_equal/Equal = Equal[T=DT_INT32](MultipleGridAnchorGenerator/assert_equal/x, MultipleGridAnchorGenerator/strided_slice_2)]]\r\n2018-04-30 03:21:07.874606: I tensorflow/tools/benchmark/benchmark_model.cc:468] Initialized session in 2.16979s\r\ncan anyone tell  how to install tensorflow on imx6", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi,\r\nOS Platform and Distribution : Ubuntu 16.04 installed from https://boundarydevices.com/ubuntu-xenial-mx67-boards-august-2016-kernel-4-1-15/\r\nTensorFlow installed from source \r\nTensorFlow version : 1.7\r\nBazel version: 10\r\nCUDA/cuDNN version : N/A\r\nGPU model and memory : GPU- N/A and memory 1Gb of Ram and 16Gb Rom\r\nExact command to reproduce: For installing i used https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md", "Then i tried to install from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile followed the steps given for raspberry pi, it got installed i got library file and the benchmark but when i ran the executable file it gave error as:\r\n2018-05-01 21:22:46.664393: I tensorflow/tools/benchmark/benchmark_model.cc:308] Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences\r\nIllegal instruction", "@snehaku I'm not seeing much through web searches: \r\nhttps://github.com/tensorflow/tensorflow/issues/9259 suggests that @gunasekaran7 got one working; I'd suggest asking them. ", "@cy89, \r\nI followed the instruction in this [link](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md). It worked without any issues.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@cy89 Closing this now, feel free to open a new issue if any error comes up."]}, {"number": 18977, "title": "R1.4.add java u int16", "body": "Add UInt16 support to Java Tensorflow API.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 18976, "title": "TFlite TransposeConv op supported but unregistered", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.3\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.8.0-rc1-1107-g8a428cd 1.8.0-rc1\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 9.1.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI'm trying to convert my custom model to tflite using tf.contrib.lite.toco_convert. \r\nThe model has some TransposeConv in it. I expect these to convert as looking at commit https://github.com/tensorflow/tensorflow/commit/58fe7d26afa435560e7a0d8ca6fc8d670d2477da there does seem to be quite a bit of code in place to support transposeconv, including the actual implementation and graph transformations. However when I run the conversion it tells me it's not supported (see below output).\r\n\r\n\r\nIt seems like this op is supported but simply unregistered for some reason, which might be a bug or maybe there is some mysterious reason it cannot be registered?\r\n\r\nThanks\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\n2018-04-30 11:11:07.554671: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 204 operators, 314 arrays (0 quantized)\r\n2018-04-30 11:11:07.557853: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 204 operators, 314 arrays (0 quantized)\r\n2018-04-30 11:11:07.562549: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 52 operators, 108 arrays (0 quantized)\r\n2018-04-30 11:11:07.566535: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 48 operators, 103 arrays (0 quantized)\r\n2018-04-30 11:11:07.567162: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 43 operators, 95 arrays (0 quantized)\r\n2018-04-30 11:11:07.567617: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 43 operators, 95 arrays (0 quantized)\r\n2018-04-30 11:11:07.568085: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 934336 bytes, theoretical optimalvalue: 933120 bytes.\r\n2018-04-3011:11:07.569157: F tensorflow/contrib/lite/toco/tflite/export.cc:315] Some of the operators in the model are not supported by the standard TensorFlow Literuntime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need customimplementations: TransposeConv.\r\n", "comments": ["I compiled last tensorflow code (from master) and it works for we! I got exactly the same error.", "TransposeConv has been wired but there are still shape issues with it. We are tracking these issues. ", "Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this as it is in \"awaiting response\" status for more than 7 days. Feel free to add your comments and we will reopen(if required)."]}, {"number": 18975, "title": "``sequence_bucketized_column`` is missing", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nTensorFlow 1.8 introduced a new API for sequence feature columns. The API covers categorical, numeric columns, but lacks `sequence_bucketized_column`. It should not be hard to implement, so if that's OK with you, I can do it myself and submit a PR.\r\n\r\n### Source code / logs\r\n\r\nN/A.\r\n", "comments": ["@ispirmustafa please review for API addition. We're ok with it if you are.", "it should work this way: `bucketized_column(sequence_numeric_column(...), ...)`\r\nTo do that, you need to update `_BucketizedColumn`. You can check the implementation in `_EmbeddingColumn`.\r\nI'm okay with adding sequence support to bucketized column.\r\nThank you for contribution!\r\nSince this is a new feature, It would be great to hear your experience too.", "Thanks for the pointers @ispirmustafa. Should I make `_BucketizedColumn` dense or categorical, or both? The latter will require removing `_CategoricalColumn` from the base classes of `_SequenceCategoricalColumn` because otherwise the MRO for `_BucketizedColumn` will be ambiguous:\r\n\r\n```\r\n                 _____________________  _FeatureColumn -----------\r\n                /           /                |                     \\\r\n               /           |         _CategoricalColumn             |\r\n               |           |                 |                      |\r\n_CategoricalColumn  _DenseColumn  _SequenceCategoricalColumn _SequenceDenseColumn\r\n                \\          |       |                         /\r\n                 -- _BucketizedColumn -----------------------\r\n```", "Hi @superbobry \r\ngood point. let's remove _CategoricalColumn inheritance from _SequenceCategoricalColumn. that will be better.", "Hi @superbobry \r\nCould you please update this thread? thanks", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing the issue due to inactivity. ", "What's missing on your WIP implementation @superbobry?", "Any progress for this feature?"]}, {"number": 18974, "title": "Permission denied: '/Library/Python/2.7/site-packages/pbr-4.0.2.dist-info'", "body": "pip install tensorflow\r\nCollecting tensorflow\r\n  Using cached https://files.pythonhosted.org/packages/9b/1e/d89f1369b5b8045e5aedf43718b45d2396d3c61e9cc56123c24b7758dd9f/tensorflow-1.8.0-cp27-cp27m-macosx_10_11_x86_64.whl\r\nRequirement already satisfied: numpy>=1.13.3 in /Library/Python/2.7/site-packages (from tensorflow) (1.14.3)\r\nCollecting mock>=2.0.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\r\nCollecting enum34>=1.1.6 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\r\nCollecting wheel (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl\r\nCollecting astor>=0.6.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\r\nCollecting backports.weakref>=1.0rc1 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\r\nCollecting termcolor>=1.1.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\r\nCollecting gast>=0.2.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\r\nCollecting grpcio>=1.8.6 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/47/75/c8b6a98b3a8de062e9454a3b796344bb5a8f36caa558578c8bbc584ec901/grpcio-1.11.0-cp27-cp27m-macosx_10_11_x86_64.whl\r\nRequirement already satisfied: six>=1.10.0 in /Library/Python/2.7/site-packages/six-1.11.0-py2.7.egg (from tensorflow) (1.11.0)\r\nCollecting absl-py>=0.1.6 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz\r\nCollecting protobuf>=3.4.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/c7/15/e21b9597043ecdc586b76b29608b30212658d239d66407621a642aedb41f/protobuf-3.5.2.post1-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\r\nCollecting tensorboard<1.9.0,>=1.8.0 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/4d/1e/3bfb48ff165e331c0c5fdca5de79d497fa5d71f3cb2eee2733ff22e898df/tensorboard-1.8.0-py2-none-any.whl\r\nCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/e1/ba/f95e3ec83f93919b1437028e989cf3fa5ff4f5cae4a1f62255f71deddb5b/pbr-4.0.2-py2.py3-none-any.whl\r\nCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\r\nRequirement already satisfied: futures>=2.2.0 in /Library/Python/2.7/site-packages/futures-3.2.0-py2.7.egg (from grpcio>=1.8.6->tensorflow) (3.2.0)\r\nRequirement already satisfied: setuptools in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from protobuf>=3.4.0->tensorflow) (18.5)\r\nCollecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl\r\nCollecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\r\nCollecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\r\nCollecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz\r\nInstalling collected packages: pbr, funcsigs, mock, enum34, wheel, astor, backports.weakref, termcolor, gast, protobuf, grpcio, absl-py, markdown, html5lib, bleach, werkzeug, tensorboard, tensorflow\r\nCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/pbr-4.0.2.dist-info'\r\nConsider using the `--user` option or check the permissions.", "comments": ["Hi @chuanjiadan. It seems your install command does not have permissions to install in the Library directory. This can be fixed by using a [virtualenv](http://docs.python-guide.org/en/latest/dev/virtualenvs/) or adding the `--user` flag to your pip install command.", "add --user flag.  this worked for me.", "Use like this **pip install packagename --user**", "Use \r\nPip install anisble(pkg name) --user"]}, {"number": 18973, "title": "Fix MSVC openmp flag", "body": "CMake is checking the flag \"-fopenmp\" but the flag is \"/openmp\" at least on my setup (MSVC 2015 and 2017 with Intel compilers_and_libraries_2018.2.185).\r\n\r\nThis PR checks for both \"-fopenmp\" and \"/openmp\" if WIN32.", "comments": ["When is \"-fopenmp\" valid? Is it for GCC/mingw or something?\r\n\r\nI will rename the variables something semantic like COMPILER_OPT_OPENMP_SUPPORT_GCC and COMPILER_OPT_OPENMP_SUPPORT_MSVC if that makes more sense.", "@gunan fixed the variables as suggested. Cleaned out the logic even more:\r\n\r\ntensorflow_ENABLE_MKL_SUPPORT and openmp was dependent on tensorflow_WIN_CPU_SIMD_OPTIONS. I mean yeah you should probably have both but the logic doesn't exactly make sense. Also, there were two separate MKL blocks.\r\n\r\nNow just several separate if statements:\r\n- if MKL support\r\n- if openmp gcc\r\n- if openmp vc\r\n- if win simd\r\n\r\nI combined the 2 MKL blocks as well", "Thanks for the changes! \"-fopenmp\" is mostly used in GCC. I think it may partially work for clang too, but I have seen clang warnings on macos that complain about the flag.\r\nAs for variable names I am OK with either this, or your suggestions.\r\n\r\nI agree that the logic for the above options are confusing, I think one way to make things cleaner could be to make them as independent as possible. Yes, it would make sense to have MKL + SIMD options, but then you may want to just have MKL without SIMD options to have a portable library, vs MKL + SIMD for the fastest possible library. Or even just SIMD without MKL because of licensing restrictions on your system.\r\nBut this PR s a step in the right direction.", "@gunan Thanks for the quick response! Now everything is independent, so you can do any combination of MKL, SIMD and OpenMP you want. Not sure why that structure was there originally.\r\n\r\nDoes it make sense to add a flag to enable openMP conditionally or should it just always be used if available? Not having a flag for openMP makes it harder to cross-compile. I'll add that in another PR if it makes sense. I would think default to true.", "It does make sense, especially if there are problems cross compiling. I would be happy to review the PR.", "I think that use cmake to detect the compiler flags is better than hard code the compiler flags in CMakeList.txt.\r\n\r\nhttps://github.com/fo40225/tensorflow/commit/32da37bb0325a23a0725f3f0e7c360beee0a3dda#diff-1d799fa350437420218e5e5aa680c481R134", "@fo40225 your suggestions make sense too. Posted a few comments there. \r\n\r\nActually would be better to use something like FindMKL instead of all this custom stuff. Should consistently use find_package for mkl, openmp, and anything else.", "CMake didn't have officially FIndMKL, I think use MKL_ROOT is enough.\r\n\r\nUse the functions provided by cmake as much as possible because that there is not only msvc compiler on windows.\r\n\r\nFor example, if you use Intel compiler on windows, you need use /Qopenmp to enable OpenMP; it is different from msvc or gnu-compliant compiler.\r\n\r\nI have successfully built tensorflow on windows using the intel compiler. I think it is also possible to build tensorflow on windows by gcc in MSYS2 or clang for windows.", "Not officially, but could copy FindMKL or something similar into\ntensorflow. Would be better organization but would need to check licensing.\nBTW, it is MKLROOT that intel sets (at least in my experiments).\n\nSeems like we should check for -fopenmp /Qopenmp and /openmp? I assume\nFindOpenMP hits those correctly.\n\nOn Mon, Apr 30, 2018 at 10:29 PM, fo40225 <notifications@github.com> wrote:\n\n> CMake didn't have officially FIndMKL, I think use MKL_ROOT is enough.\n>\n> Use the functions provided by cmake as much as possible because that there\n> is not only msvc compiler on windows.\n>\n> For example, if you use Intel compiler on windows, you need use /Qopenmp\n> to enable OpenMP; it is different from msvc or gnu-compliant compiler.\n>\n> I have successfully built tensorflow on windows using the intel compiler.\n> I think it is also possible to build tensorflow on windows by gcc in MSYS2\n> or clang for windows.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18973#issuecomment-385584224>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL4rbIu6BgaAYaa4EVvsx9yC4nn1LQApks5tt8iSgaJpZM4TsLFx>\n> .\n>\n", "Also, I'm curious, see any performance diff with intel compiler v msvc?\n\nOn Mon, Apr 30, 2018 at 10:52 PM, Benjamin Striner <bstriner@gmail.com>\nwrote:\n\n> Not officially, but could copy FindMKL or something similar into\n> tensorflow. Would be better organization but would need to check licensing.\n> BTW, it is MKLROOT that intel sets (at least in my experiments).\n>\n> Seems like we should check for -fopenmp /Qopenmp and /openmp? I assume\n> FindOpenMP hits those correctly.\n>\n> On Mon, Apr 30, 2018 at 10:29 PM, fo40225 <notifications@github.com>\n> wrote:\n>\n>> CMake didn't have officially FIndMKL, I think use MKL_ROOT is enough.\n>>\n>> Use the functions provided by cmake as much as possible because that\n>> there is not only msvc compiler on windows.\n>>\n>> For example, if you use Intel compiler on windows, you need use /Qopenmp\n>> to enable OpenMP; it is different from msvc or gnu-compliant compiler.\n>>\n>> I have successfully built tensorflow on windows using the intel compiler.\n>> I think it is also possible to build tensorflow on windows by gcc in MSYS2\n>> or clang for windows.\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/18973#issuecomment-385584224>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AL4rbIu6BgaAYaa4EVvsx9yC4nn1LQApks5tt8iSgaJpZM4TsLFx>\n>> .\n>>\n>\n>\n", "FindOpenMP is cmake built-in function, it works very well. Cmake is born for cross-platform and compilers, cmake can handle it.\r\n\r\nhttps://cmake.org/cmake/help/latest/module/FindOpenMP.html\r\n\r\nI don't have a modern Intel CPU, it is hard to say it has performance improve from intel compiler.\r\n\r\nCrack intel compiler & mkl 's cpu brand check has too much effort for tensorflow on windows.", "I agree the find_package way would be better."]}, {"number": 18972, "title": "Use MKLROOT", "body": "Search ENV{MKLROOT} in addition to MKL_HOME. MKLROOT is the standard environment variable set by Intel scripts.\r\n\r\nShould make install a little easier. No need to set MKL_HOME if you run the Intel environment setup.\r\n\r\nCheers", "comments": []}, {"number": 18971, "title": "Problems about loading SavedModelBundle", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.6\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA Version 9.0.176\r\n- **GPU model and memory**:\r\nTesla K80, total memory 11.17GiB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n- **Definitions**:\r\n**Update N** - Update status of proposed issues.\r\n**Additional N** - New relevant issues met during development.\r\n- **Background**: \r\nI saved a trained model under Tensorflow 1.0 using Saver.save(), then updated some variables and loaded it into Tensorflow 1.6. Now I saved it again using SavedModelBundle APIs and want to load it in Java. \r\n- **Current Status**: \r\nThe model has been exported as .pb and variables files, with tags and SignatureDefs. But when I tried to SavedModelBundle.load() it in Java, it seemed to be blocked until manually terminated. Prompts after termination was like:\r\n![error screenshot - github](https://user-images.githubusercontent.com/22216143/39414218-aaa3f24e-4c67-11e8-802a-93900c7b173a.jpg)\r\n**Update 1**: (transfer the import procedure from Win to Ubuntu)\r\nException in thread \"main\" java.lang.IllegalArgumentException: Cannot assign a device for operation ... : Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n\r\n- **Further Description**: \r\nWhat I want to emphasize is that, speaking of the SignatureDef settings, I attempted to give several inputs and outputs at one time, in which case some outputs may also be inputs again.\r\nMore clearly in Seq2Seq model, I have outputs from the encoder which may also be inputs to the decoder. It also means that I want to fetch, modify and feed some intermediate variables of the model instead of playing with a simple model just with 1 input and 1 output. Hence the situation now is a little complex.\r\nI read the documents but still have not found information describing this situation. Also searching the community gave no suitable answer. Therefore, could you help me to figure out if my implementation is right? or how to use the APIs to serve this situation?\r\nThank you!\r\n**Update 1**: \r\nIt seems like SavedModelBundle failed to register GPU device for operations? or there are some configurations I should do before using it? \r\nBesides, I have set \"**tf.ConfigProto(allow_soft_placement=True)**\".\r\nThanks.\r\n\r\n### Source code / logs\r\nPython codes:\r\n``` python\r\nsaved_model_builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\r\na = tf.saved_model.utils.build_tensor_info(model.a)\r\nb = tf.saved_model.utils.build_tensor_info(model.b)\r\nc = tf.saved_model.utils.build_tensor_info(model.c)\r\nd = tf.saved_model.utils.build_tensor_info(model.d)\r\ne = tf.saved_model.utils.build_tensor_info(model.e)\r\nsignature = tf.saved_model.signature_def_utils.build_signature_def(\r\n        inputs = {\r\n            \"a\": a,\r\n            \"b\": b,\r\n            \"c\": c},\r\n        outputs = {\r\n            \"d\": d,\r\n            \"e\": e,\r\n            \"a\": a,\r\n            \"b\": b},\r\n        method_name = tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\r\nasset_file = tf.constant(FLAGS.vocab_path, name=\"Vocabulary\")\r\ntf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, asset_file)\r\nsaved_model_builder.add_meta_graph_and_variables(\r\n        self._sess, [tf.saved_model.tag_constants.SERVING],\r\n        signature_def_map = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},\r\n            assets_collection = tf.get_collection(tf.GraphKeys.ASSET_FILEPATHS))\r\nsaved_model_builder.save()\r\n```\r\nJava code:\r\n``` java\r\nprivate static final String MODEL_TAG = \"serve\";\r\nSavedModelBundle model = SavedModelBundle.load(modelDir, MODEL_TAG);\r\n```", "comments": ["Just based on the error message, it appears that your Java binary isn't setup to use the GPU, so it only has CPU kernels registered. While your model requires a GPU and thus can't be loaded.\r\n\r\nIs that plausible? If so, could you elaborate on how you're building your Java program? In particular, see [the installation instructions for GPU](https://www.tensorflow.org/install/install_java#gpu_support).\r\n\r\nLet us know if that helps.", "Hi @asimshankar ,\r\nThank you for the reply!\r\n\r\nI followed [Installing TensorFlow for Java - Using TensorFlow with JDK](https://www.tensorflow.org/versions/r1.6/install/install_java#using_tensorflow_with_jdk) with **TF_TYPE=\"gpu\"** and also successfully ran the HelloTF example, of which results are as shown below:\r\n\r\n``` java\r\n$ java HelloTF\r\n2018-05-04 01:46:37.195042: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2018-05-04 01:46:37.668711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:88:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-04 01:46:37.668869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-05-04 01:46:38.096256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10764 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:88:00.0, compute capability: 3.7)\r\nHello from 1.6.0\r\n```\r\n\r\nDoes it mean that I have configure and register the GPU successfully? \r\nFurthermore, I built the Java program using standard OpenJDK instead of Maven, that is:\r\n``` java\r\n$ java -version\r\nopenjdk version \"1.8.0_162\"\r\nOpenJDK Runtime Environment (build 1.8.0_162-8u162-b12-0ubuntu0.16.04.2-b12)\r\nOpenJDK 64-Bit Server VM (build 25.162-b12, mixed mode)\r\n```\r\n``` java\r\njavac -d ../bin/ *.java\r\njava -Djava.library.path=../../lib/jni Main\r\n```\r\nCLASSPATH has been exported into environment.\r\nBy the way, would it be better if I develop it under Maven?\r\n\r\nLooking forward to your reply.\r\nThanks!\r\n\r\n**Update 1**: (tested under Maven)\r\nI also tried to run the programs under Maven but still got the same results.\r\nHelloTF:\r\n``` java\r\n$ mvn -q -e compile exec:java\r\n2018-05-04 03:39:14.899898: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2018-05-04 03:39:15.369079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:44:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-04 03:39:15.369201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-05-04 03:39:15.809895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10764 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:44:00.0, compute capability: 3.7)\r\nHello from 1.6.0\r\n```\r\nMy program:\r\n``` java\r\n2018-05-04 07:16:31.631928: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2018-05-04 07:16:31.646627: I tensorflow/cc/saved_model/loader.cc:240] Loading SavedModel with tags: { serve }; from: ../saved_models/updated_model/for_java\r\n2018-05-04 07:16:32.088859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:44:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-04 07:16:32.089152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-05-04 07:16:32.490601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10764 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:44:00.0, compute capability: 3.7)\r\n2018-05-04 07:16:32.684505: I tensorflow/cc/saved_model/loader.cc:289] SavedModel load for tags { serve }; Status: fail. Took 1037733 microseconds.\r\n[WARNING]\r\njava.lang.IllegalArgumentException: Cannot assign a device for operation 'gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n\r\n         [[Node: gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/device:GPU:0\"](gen/seq2seq/encoder/bidirectional_rnn/bw/bw/All, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert/data_0, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/stack, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert/data_2, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Shape)]]\r\n    at org.tensorflow.SavedModelBundle.load (Native Method)\r\n    at org.tensorflow.SavedModelBundle.load (SavedModelBundle.java:39)\r\n    ......\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 2.953 s\r\n[INFO] Finished at: 2018-05-04T07:16:32Z\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:java (default-cli) on project ...: An exception occured while executing the Java class. Cannot assign a device for operation 'gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n[ERROR] Registered kernels:\r\n[ERROR]   device='CPU'\r\n[ERROR]\r\n[ERROR]        [[Node: gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/device:GPU:0\"](gen/seq2seq/encoder/bidirectional_rnn/bw/bw/All, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert/data_0, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/stack, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Assert/Assert/data_2, gen/seq2seq/encoder/bidirectional_rnn/bw/bw/Shape)]]\r\n[ERROR] -> [Help 1]\r\n[ERROR]\r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR]\r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\n```\r\n\r\n**Additional 1**:\r\nBesides, I could load the load if abandon GPUs by **add_meta_graph_and_variables(clear_devices=True)**. However, in Java, I still cannot retrieve the variables using names I defined in **signature_def_utils.build_signature_def()**. So I really wonder if there is any wrong with my use of **tf.saved_model.signature_def_utils.build_signature_def()**, as shown in my Python code? \r\n\r\nThank you so much!", "Nagging Assignee @asimshankar: It has been 22 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 37 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@gypleon : From the error message, it seems that the way the graph is defined, it is asking the `tf.assert` operation to execute on GPU, which isn't possible. Without knowing more about what the tensors `a`, `b` etc. in the code snippet are, it's going to be hard to diagnose what's going on. Is it possible for you to share the saved model?\r\n\r\nIf not, another thing to try would be to load and execute the saved model from within Python (e.g., using `tf.saved_model.loader.load`). I suspect that you'll end up with the same error message there too. Suggesting that perhaps the path from the input to output tensors is forcing an `assert` on the GPU. Perhaps the model could be edited to avoid that assert?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18970, "title": "v", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18969, "title": "Error (windows 10 - Anaconda)", "body": "problemes to install\r\n------------\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Uma rotina de inicializa\u00e7\u00e3o da biblioteca de v\u00ednculo din\u00e2mico (DLL) falhou.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Usuario\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi Enrique, \r\nthe problem may be that your CPU don't have AVX instruction set, \r\nplease take a look at this thread with the same problem:\r\n[https://github.com/tensorflow/tensorflow/issues/18503](https://github.com/tensorflow/tensorflow/issues/18503)", "IIUC only newer binary releases of TensorFlow require AVX support on x86_64. One solution could be using an older binary release of TensorFlow, or compiling it from source."]}, {"number": 18968, "title": "Update mobile prepare models documentation", "body": "Correct the command line examples to match the actual location of freeze_graph.\r\n- Old command line example showed freeze_graph in `tensorflow/tools`\r\n- freeze_graph is actually located in `tensorflow/python/tools`. The documentation immediately above the command line example states this.\r\n- Modify the command line example to run using the `tensorflow/python/tools` location.\r\n- The command line for graph_transforms does not need to be updated.", "comments": ["Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18967, "title": "r", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18966, "title": ".", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18965, "title": "add missing equality", "body": "Change > to >= (2000 doesn't belong to a bucket otherwise)", "comments": []}, {"number": 18964, "title": "Add profile binary to dev image", "body": "Why `tensorflow/core/profiler:profiler` was not added at least in dev images?\r\n\r\nI.e. for the gpu one:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L103", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Do you meant 30 days?", "@tensorflowbutler is a bot and won't respond. You should fill out the template, as @tensorflowbutler suggested, to get a response.", "@tensorflowbutler I know it is a bot but probably some human registered it to github. I hope that its mentions will be notified somewhere.", "Well I did notice since I'm assigned to triage this issue. But, if you want us to respond to the issue, just fill out the issue template, like the bot suggested, and we will take a look.", "@reedwm The direct link on master source code in the comment is not enough or was just an internal burocracy request? \r\n", "In this case, it's purely bureaucratic. We have an internal list of issues that we need to respond to, and issues only appear if the template is filled out. We do this because most of the time, the template info is useful, and people still tend not to fill it out even when it is useful.\r\n\r\n/CC @gunan, can you take a look at this issue?\r\n", "The exclusion of the binary is mostly a size optimization.\r\n@martinwicke wdyt?", "It makes sense for the dev image to contain it. Probably not the non-dev image. What's the impact in terms of size?", "Probably for the non-dev one we could at least complete the docs on high level api cases https://github.com/tensorflow/tensorflow/issues/18732", "Sure, but that's independent of the docker image, right?", "Yes but could be a valid runtime solution. Generally my evaluation is that the non dev image could be a little bit more related to hight level APi users than the devel one. So if we finally exclude perf from not dev images we can let at least a complete solution with the docs coverage completion.", "@martinwicke I know that with this comment I will reset the Nagging Assignee counting days stats but if this has a low priority can you prioritize https://github.com/tensorflow/tensorflow/issues/18732 so that beginners will have a quick path to profile theirs high level API models?", "I've recently proposed a change to TensorFlow that obsoletes parameterized_docker_build.sh, which may help alleviate this issue. If anyone following this thread is interested in making TensorFlow's Dockerfile story better for everyone, [please take a look at the RFC](https://github.com/tensorflow/community/pull/8).", "I think this is no longer relevant, since we have new docker images now. If it still is a problem, please comment again."]}, {"number": 18963, "title": "fix missing type conversion", "body": "Hi,\r\n\r\nI was following the getting started and noticed that the else case of `eval_input_fn` needs an extra dict(features). Without it I get:\r\n\r\n```\r\n(tensorflow) rodrigo@rodrigo-XPS-13-9360:~/dev/tensorflow$ python python/iris_graph.py\r\nTraceback (most recent call last):\r\n  File \"python/iris_graph.py\", line 52, in <module>\r\n    eval_result = classifier.evaluate(input_fn=lambda:eval_input_fn(test_features, test_labels, batch_size))\r\n  File \"/home/rodrigo/dev/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 425, in evaluate\r\n    name=name)\r\n  File \"/home/rodrigo/dev/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1087, in _evaluate_model\r\n    features, labels, model_fn_lib.ModeKeys.EVAL, self.config)\r\n  File \"/home/rodrigo/dev/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 831, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/rodrigo/dev/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 347, in _model_fn\r\n    config=config)\r\n  File \"/home/rodrigo/dev/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 159, in _dnn_model_fn\r\n    'Given type: {}'.format(type(features)))\r\nValueError: features should be a dictionary of `Tensor`s. Given type: <class 'tensorflow.python.framework.ops.Tensor'>\r\n(tensorflow) rodrigo@rodrigo-XPS-13-9360:~/dev/tensorflow$\r\n```\r\n\r\nAfter adding the extra type conversion (line 39) as following everything works fine:\r\n\r\n```\r\n 35 def eval_input_fn(features, labels, batch_size):\r\n 36     if labels is None:\r\n 37         input = features\r\n 38     else:\r\n 39         input = (dict(features), labels)\r\n 40     dataset = tf.data.Dataset.from_tensor_slices(input)\r\n 41     dataset = dataset.batch(batch_size)\r\n 42     return dataset.make_one_shot_iterator().get_next()\r\n```\r\n\r\nNote that this is also consistent with `train_input_fn`", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "I looked briefly thru the example, seems like there might be two variables, train_features (which looks like a pandas dataframe) and train_x (which looks like the feature dict). Might have just omitted the conversion btw them in the example. Unsure. But I dont think this is the best fix.\r\n\r\nThanks for the PR though"]}, {"number": 18962, "title": "bug of tf.extract_image_patches", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu  16.04\r\n- **TensorFlow installed from pip**:\r\n- **TensorFlow version**: 1.5\r\n- **Python version**: 2.7 \r\n- **CUDA/cuDNN version**: 8.0\r\n- **Have I written custom code**: N/A\r\n- **Bazel version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### the problem\r\nI have a input \"x\" which size is (batch, width, height, chanel), because the height of data is variable. so x.get_shape() is (?, 20, ?, 64). I use the following code to do something like im2col:\r\n`im2col = tf.extract_image_patches(x, ksizes=[1, 1, KERNEL_SIZE, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding=\"SAME\")\r\n`\r\nthe output \"im2col\" lost some shape, so I use the following code to set_shape:\r\n `im2col.set_shape([shape1, shape2, shape3, KERNEL_SIZE * shape4])`\r\nand then reshape im2col to (shape1, shape2, shape3, KERNEL_SIZE, shape4)\r\n\r\nIn the process of building graph, errors arise when processing optimization algorithms (Adam).\r\n\r\n> TypeError: unsupported operand type(s) for /: 'NoneType' and 'long'\r\n\r\nI'm sure the error is produced by tf.extract_image_patches. If I replace tf.extract_image_patches with tf.tile (to make the output size of tf.tile same as tf.extract_image_patches), graph will be built sucessfully.\r\n\r\n### Error message\r\nTraceback (most recent call last):\r\n  File \"/home/anaconda2/bin/t2t-trainer\", line 32, in <module>\r\n    tf.app.run()\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 129, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/anaconda2/bin/t2t-trainer\", line 28, in main\r\n    t2t_trainer.main(argv)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py\", line 338, in main\r\n    execute_schedule(exp)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py\", line 288, in execute_schedule\r\n    getattr(exp, FLAGS.schedule)()\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 635, in train_and_evaluate\r\n    self.train(delay_secs=0)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 377, in train\r\n    saving_listeners=self._saving_listeners)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 824, in _call_train\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 314, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 743, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 725, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py\", line 886, in wrapping_model_fn\r\n    use_tpu=use_tpu)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py\", line 980, in estimator_model_fn\r\n    loss, num_async_replicas=num_async_replicas)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py\", line 984, in estimator_spec_train\r\n    train_op = self.optimize(loss, num_async_replicas=num_async_replicas)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py\", line 401, in optimize\r\n    loss, lr, self.hparams, use_tpu=common_layers.is_on_tpu())\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py\", line 68, in optimize\r\n    colocate_gradients_with_ops=True)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 241, in optimize_loss\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py\", line 110, in compute_gradients\r\n    gradients = self._opt.compute_gradients(loss, var_list, **kwargs)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 608, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 374, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 608, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 724, in _ExtractImagePatchesGrad\r\n    cols_out = int(ceil(cols_in / stride_h))\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'long'\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler These fields are added now.", "The gradient of extract_image_patches currently doesn't support variable sized input. See if PR #17420 works for you.", "@fmannan Does extract_image_patches support a input whose first dimension(batch dimension) is a variable? e.g. (?, 20, step, 64)\r\nAre there plans to make extract_image_patches support the variable sized input?", "@DC-Swind  Yes, the PR that I linked supports variable sized input like the one you have.", "@fmannan \r\nI test tf.extract_image_patches in tensorflow 1.5, it supports input size (?, 20, step, 64) but doesn't support (?, 20, ?, 64).\r\nI notice that your PR hasn't been merged. Does it exist any problems or just an approving review is expected? \r\nIf I use this PR to support (?, 20, ?, 64), the only thing is to replace the file \"tensorflow/python/ops/array_grad.py\" in my tf 1.5?", "@DC-Swind I'm not aware of any issues with PR, so I'm expecting it to be merged. You can build TF with the PR and install, or update your `array_grad.py`.", "@DC-Swind,\r\nSorry for the delayed response. This issue has been fixed in [this PR](https://github.com/tensorflow/tensorflow/pull/29815). Can you please confirm so that we can close this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18962\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18962\">No</a>\n"]}, {"number": 18961, "title": "CUDA 9.1 Support", "body": "I am using NVIDIA GeForce GTX 1050 and installed NVIDIA 388.19. I installed cuDNN 7.0.5 and CUDA 9.1. As of my understanding, I know that, tensorflow is not supported in CUDA 9.1. My question is when I can expect the next build/release of TF to support CUDA 9.1. For the time being, shall I make a link from CUDA 9.0 to CUDA 9.1 and expect to work? Or is there any better way to solve the problem?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@StevenGann, this is addressed in #15140. The tentative plan, AFAIK, is to move to 9.2 w/TF 1.9. See #18906 for the latest.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "It still is an issue to me, I haven't found the answer yet.", "for me its also an issue the problem should stay open  because the is not easy solution ", "Hi.\r\nI installed tensorflow gpu on an HPC server, I have installed the dependencies cudnn and cuda 9.1. But when i import tensorflow it get an error. I read that tensorflow-gpu installed via pip does not support cuda 9.1. Could that be the issue here? The error message is below\r\n\r\n File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/apps/Python/Python-3.5/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "Same here. I have multiple version of cuda 7.5, 8.0, 9.1 They are working fine for various projects. \r\nI have tried installing 9.0 without installing driver and the geforce experience. It's still failed. It would be nice if someone could help me.", "I solved my issue. I just installed CUDA 9.0 to a different directory. \r\n\r\nAnd before i import tensorflow i make sure to set my PATH to add cuda 9.0. After doing that i get no error when i run tensorflow gpu "]}, {"number": 18960, "title": "Please clarify the documentation on eager execution and available host language functionalities", "body": "The documentation on eager execution [here](https://www.tensorflow.org/programmers_guide/eager), particularly for the \"fizzbuzz\" example, does not reflect what tensorflow does. Specifically, all the comparisons num % x == y  do not work as the reader is expecting them to. According to me, it is not true that \"all the functionality of the host language is available\" because in order to make that code work I need to convert all the comparisons into num.numpy() % x == y (I have enabled eager execution).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This is true. I have observed the similar pattern. Apparently it needs to be x.numpy().", "Answering to \"tensorflowbutler\".\r\n\r\nActually, because this is an issue with tensorflow documentation, if tensorflow developers had taken the time to run the \"fizzbuzz\" code as it is reported into the tensorflow documentation, at the link I have provided, which points to tensorflow web site, the issue would have come out.\r\n\r\nTrying to fill out the relevant information:\r\nHave I written custom code: No. I cut and pasted the code on the tensorflow documentation.\r\nOS Platform and Distribution: It happens both on Windows 10 (tensorflow 1.8, downloaded with pip) and on Linux (built from sources).\r\nTensorFlow installed from: on Windows, through pip. On Linux, from source.\r\nTensorFlow version 1.7 and 1.8\r\nBazel version: NA\r\nCUDA/cuDNN version: No CUDA. I am not using GPU.\r\nGPU model and memory: Not using GPU\r\nExact command to reproduce: try to run the code you report on your documentation (see above).", "+1 Also you can just cast it to e.g. int: `int(num % x) == y`. Though, the bare FizzBuzz doesn't work as given in the example. BTW same behavior on GPU version, so it is unrelated (as expected really). ", "When I ran the fizzbuzz example in the programmer's guide on tf 1.7.0 as `fizzbuzz(10)` it worked; it only failed when run as `fizzbuzz(tf.constant(10))`. I'm updating the docs to make it work in both cases, thanks for the heads up."]}, {"number": 18959, "title": "Use `get_cosine_decay_fn` to match the description", "body": "", "comments": ["Nagging Assignee @ekelsen: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18958, "title": "[eature request", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18957, "title": "fix typo", "body": "Compliling -> Compiling", "comments": []}, {"number": 18956, "title": "Fail to build cmake with gpu for _lstm and _rnn ops in windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: github master (should be 1.8?)\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: CMake build\r\n- **GCC/Compiler version (if compiling from source)**: MSVC2015\r\n- **CUDA/cuDNN version**:  CUDA 9.0 with CUDNN 7.0.5\r\n- **GPU model and memory**: GTX 1060 6GB\r\n- **Exact command to reproduce**:\r\n\r\ncmake \\\r\n    \t-DCMAKE_INSTALL_PREFIX=../install/ \\\r\n    \t-DCMAKE_BUILD_TYPE=Release \\\r\n        -Dtensorflow_BUILD_SHARED_LIB=ON \\\r\n    \t-Dtensorflow_BUILD_ALL_KERNELS=ON \\\r\n    \t-Dtensorflow_BUILD_CONTRIB_KERNELS=ON \\\r\n    \t-Dtensorflow_BUILD_CC_EXAMPLE=ON \\\r\n    \t-Dtensorflow_BUILD_PYTHON_BINDINGS=ON \\\r\n    \t-Dtensorflow_ENABLE_GRPC_SUPPORT=ON \\\r\n    \t-Dtensorflow_ENABLE_SSL_SUPPORT=OFF \\\r\n    \t-Dtensorflow_BUILD_CC_TESTS=OFF \\\r\n    \t-Dtensorflow_BUILD_PYTHON_TESTS=OFF \\\r\n        -Dtensorflow_ENABLE_GPU=ON ..\r\n\r\n### Describe the problem\r\nIn the [contrib/rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/BUILD) build, the gpu resource requires `blas_gemm.h` . I found this is missed in the cmake \r\n[relevent position](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake)\r\n\r\n I have built pass in older version of tensorflow release (1.5) but not in 1.8. The cuda object built in rnn should be linked with blas_gemm? How can I modified the cmake files to do this?\r\n\r\n### Source code / logs\r\nThe cmake file that fails to build\r\n\r\n```cmake\r\nif(WIN32)\r\n    # include contrib/rnn as .so\r\n    #\r\n    set(tf_gru_srcs\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.cc\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.h\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops.cc\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops.h\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/gru_ops.cc\"\r\n    )\r\n    set(tf_gru_gpu_srcs\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.cc\"\r\n    )\r\n\r\n    set(tf_lstm_srcs\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.cc\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.h\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops.cc\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops.h\"\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/lstm_ops.cc\"\r\n    )\r\n    set(tf_lstm_gpu_srcs\r\n        \"${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.cc\"\r\n    )\r\n\r\n    AddUserOps(TARGET _gru_ops\r\n        SOURCES \"${tf_gru_srcs}\"\r\n        GPUSOURCES ${tf_gru_gpu_srcs}\r\n        DEPENDS pywrap_tensorflow_internal tf_python_ops\r\n        DISTCOPY ${CMAKE_CURRENT_BINARY_DIR}/tf_python/tensorflow/contrib/rnn/python/ops/)\r\n\r\n    AddUserOps(TARGET _lstm_ops\r\n        SOURCES \"${tf_lstm_srcs}\"\r\n        GPUSOURCES ${tf_lstm_gpu_srcs}\r\n        DEPENDS pywrap_tensorflow_internal tf_python_ops\r\n        DISTCOPY ${CMAKE_CURRENT_BINARY_DIR}/tf_python/tensorflow/contrib/rnn/python/ops/)\r\nendif(WIN32)\r\n```\r\n\r\nBazel BUILD file in contrib/rnn:\r\n\r\n```BAZEL\r\ntf_custom_op_library(\r\n    name = \"python/ops/_lstm_ops.so\",\r\n    srcs = [\r\n        \"kernels/blas_gemm.cc\",\r\n        \"kernels/blas_gemm.h\",\r\n        \"kernels/lstm_ops.cc\",\r\n        \"kernels/lstm_ops.h\",\r\n        \"ops/lstm_ops.cc\",\r\n    ],\r\n    gpu_srcs = [\r\n        \"kernels/blas_gemm.h\",\r\n        \"kernels/lstm_ops_gpu.cu.cc\",\r\n        \"kernels/lstm_ops.h\",\r\n    ],\r\n    deps = [\"//tensorflow/core/kernels:eigen_helpers\"],\r\n)\r\n\r\n...\r\n\r\ntf_custom_op_library(\r\n    name = \"python/ops/_gru_ops.so\",\r\n    srcs = [\r\n        \"kernels/blas_gemm.cc\",\r\n        \"kernels/blas_gemm.h\",\r\n        \"kernels/gru_ops.cc\",\r\n        \"kernels/gru_ops.h\",\r\n        \"ops/gru_ops.cc\",\r\n    ],\r\n    gpu_srcs = [\r\n        \"kernels/blas_gemm.h\",\r\n        \"kernels/gru_ops_gpu.cu.cc\",\r\n        \"kernels/gru_ops.h\",\r\n    ],\r\n    deps = [\"//tensorflow/core/kernels:eigen_helpers\"],\r\n)\r\n\r\n```", "comments": ["Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think I resolved this at head, but at the moment we have another breakage."]}, {"number": 18955, "title": "contrib: minor spelling tweaks", "body": "", "comments": ["@protoget rebased to master, thanks!", "Nagging Assignee @ekelsen: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 48 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 63 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 78 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 93 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "dropped conflicting part of commit, rebased to master, hope this helps!"]}]