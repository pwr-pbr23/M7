[{"number": 14266, "title": "Tensorflow splitting?", "body": "I want to split tensor into two parts:   \r\n \r\n    ipdb> mean_log_std\r\n    <tf.Tensor 'pi/add_5:0' shape=(?, 2) dtype=float32>\r\n  \r\nContext: ? is for number of samples and the other dimension is 2. I want to split along the second dimension into two tensorflow of shape 1 along that dimension.\r\n\r\nWhat I tried?(https://www.tensorflow.org/api_docs/python/tf/slice)\r\n \r\n    ipdb> tf.slice(mean_log_std,[0,2],[0,1])\r\n    <tf.Tensor 'pi/Slice_6:0' shape=(0, 1) dtype=float32>\r\n    ipdb> tf.slice(mean_log_std,[0,1],[0,1])\r\n    <tf.Tensor 'pi/Slice_7:0' shape=(0, 1) dtype=float32>\r\n    ipdb>\r\n\r\nI would expect the shape to be (?,1) and (?,1) for the above two splits.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14265, "title": "tf.layers generates an extra op when not specifying the name", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\npip install\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: \r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nNA\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNA\r\n- **CUDA/cuDNN version**:\r\n8.0.61\r\n- **GPU model and memory**:\r\nGeForce GTX 1080 Ti, 11GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen using tf.layers, I notice that an extra op is generated when not specifying the name. \r\n\r\nFor example, here is an example using tf.layers.dense. \"dense_1\" is an extra op. Similar problem has been observed for tf.layers.conv2d also. \r\n```\r\nx = tf.placeholder(tf.float32, [None, 16], name='x')\r\nhidden = tf.layers.dense(x, 32)\r\nhidden = tf.layers.dense(hidden, 32)\r\ny = tf.layers.dense(hidden, 1, name='y')\r\n```\r\nThe graph in tensorboard is as follows. \r\n![image](https://user-images.githubusercontent.com/13603534/32421364-b057853e-c24c-11e7-8aa7-865a42fc22ee.png)\r\n\r\nIf I add the name, \r\n```\r\nx = tf.placeholder(tf.float32, [None, 16], name='x')\r\nhidden = tf.layers.dense(x, 32, name='h1')\r\nhidden = tf.layers.dense(hidden, 32, name='h2')\r\ny = tf.layers.dense(hidden, 1, name='y')\r\n```\r\nthen the graph looks as expected,\r\n![image](https://user-images.githubusercontent.com/13603534/32421378-daedd956-c24c-11e7-9487-ecf9ed0d8027.png)\r\n\r\n\r\n", "comments": ["@fchollet Could you take a look at this?", "I believe this is the same issue as https://github.com/tensorflow/tensorflow/issues/13429\r\n\r\nIt sounds like a variable scope issue?", "I think @fchollet is right . The best solution is to specify name by yourself nowadays.", "@facaiy thanks for confirming this. Not really a big issue. Just to make sure I am using it properly. ", "This appears to be resolved (and the underlying issue is a duplicate of #13429). I'm closing the issue to keep the tracker focused. Feel free to reopen if needed."]}, {"number": 14264, "title": "the version for windows", "body": "Hi, could you build the new version of tensorflow for windows? the cuda version just tensorflow 1.1.", "comments": ["Version 1.4.0 of the `tensorflow-gpu` package is available [from PyPI](https://pypi.python.org/pypi/tensorflow-gpu/1.4.0) and should be installed if you enter the command `pip install tensorflow-gpu`."]}, {"number": 14263, "title": "R1.4", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14262, "title": "Revert \"Upgrade gRPC (#13958)\"", "body": "This reverts commit fccc3d2365fca265d3c6cecf367a3b147b7b51dc.\r\n\r\n@vjpai FYI\r\n\r\n\r\nThis fixes the build failure we see here:\r\nhttp://ci.tensorflow.org/job/tensorflow-master-cpu/3166/\r\n\r\nThis was missed, because we do not build verbs in our PR tests, which we should fix.\r\nI verified this rollback fixes the build failures:\r\nhttp://ci.tensorflow.org/job/tensorflow-gunan-cpu/4/", "comments": []}, {"number": 14261, "title": "'MultivariateNormalDiag' object has no attribute 'pdf' ERROR", "body": "Hi,\r\n\r\nI am trying to use tf.contrib.distributions.MultivariateNormalDiag \r\n as explained  in the API https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.distributions/multivariate_distributions \r\n\r\n- macOS High Sierra\r\n- **TensorFlow version**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**:  Python 3.6.1\r\n\r\nHere is the code that throws the error:\r\n\r\n\tmu1 = tf.to_float(tf.fill([batch_size, z_dim], 0))\r\n\tdiag_stdev1 = tf.to_float(tf.fill([batch_size, z_dim], 1))\r\n\r\n\tdist1 = tf.contrib.distributions.MultivariateNormalDiag(mu1, diag_stdev1)\r\n\r\n\tmu2 = tf.to_float(tf.fill([batch_size, z_dim], -1))\r\n\tdiag_stdev2 = tf.to_float(tf.fill([batch_size, z_dim], 1))\r\n\r\n\tdist2 = tf.contrib.distributions.MultivariateNormalDiag(mu2, diag_stdev2)\r\n\r\n\tz_gmm = dist1.pdf(z) + dist2.pdf(z)\r\n\r\nAnd the error I get is: \r\nAttributeError: 'MultivariateNormalDiag' object has no attribute 'pdf'\r\n\r\nDoes anybody have the same problem?\r\nThank you for you time.", "comments": ["It seems you were looking at the documentation for release 0.12 but running 1.3.\r\nNote that, as per [semantic versioning](http://semver.org/) followed [by TensorFlow](https://www.tensorflow.org/programmers_guide/version_compat) - there were no API stability guarantees for versions prior to 1.0\r\n\r\nIf you're using release 1.3, then please refer to the [documentation for that version](https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/contrib/distributions/MultivariateNormalDiag), which mentions that there is no `pdf` field/function.\r\n\r\nHope that helps.\r\nPlease feel free to re-open if I'm mistaken.\r\n\r\nThanks.", "Thank you for your response. It helps me a lot!"]}, {"number": 14260, "title": "consistency in function name..", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 14259, "title": "Implement backpropagation for SVD when full_matrices is False", "body": "This pull request implements backpropagation for the singular value decomposition operation when `full_matrices` is False.  A request for this feature is in #13641.\r\n\r\nThe algorithm I use is the same one as in the [autograd implementation](https://github.com/HIPS/autograd/blob/77387abc6d5d57a2688d6fa04e932f4a9d5613d9/autograd/numpy/linalg.py#L127-L217).  There is a very nice derivation of the formula, written (I think) by @j-towns, at https://j-towns.github.io/papers/svd-derivative.pdf.  (@j-towns I hope you don't mind that I include this link here; I found it in the description of your autograd pull request that added this feature.)\r\n\r\nThanks in advance for taking the time to review this!", "comments": ["Can one of the admins verify this patch?", ">@j-towns I hope you don't mind that I include this link here; I found it in the description of your autograd pull request that added this feature.\r\n\r\nNo problem. Glad you found it useful!", "Will review this on the next couple of days. Thanks for adding this!", "sure, no hurry, thanks!", "ping @rmlarsen ", "@tensorflow-jenkins Test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n", "Hello I am trying to use SVD single components in my training process, however, occasionally the training I get the following error maybe due to back propagation of SVD being unstable, I am using compute_uv=False as I only care about s for training   \r\n```\r\n InvalidArgumentError (see above for traceback): slice index 0 of dimension 0 out of bounds.\r\n         [[node worker_0/strided_slice (defined at g:\\NiftyNet\\niftynet\\layer\\loss_added.py:61)  = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](worker_0/Svd, ConstantFolding/worker_0/gradients/worker_0/NoNewNet/R4/120_bn_relu/bn_/batch_norm/add_grad/BroadcastGradientArgs-folded-1, worker_0/strided_slice/stack_1, worker_0/strided_slice/stack_1)]]\r\n\r\n```\r\nany advice on how to bypass this problem would be great, thanks and thanks for your work on the tool. "]}, {"number": 14258, "title": "Fix typo in `mobile/prepare_models.md`", "body": "This fix fixes a typo in `mobile/prepare_models.md`:\r\n`targetting` -> `targeting`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14257, "title": "TFGAN gan_model tensor conversion necessary?", "body": "I would like to use the `Dataset` API with the `GANEstimator`/`TFGAN`.\r\n\r\nI know that `MakeIterator` cannot be cast to a tensor, but I would like to pass it to `generator_fn` anyways. Is the conversion of `generator_inputs` to tensors really necessary?\r\n\r\nWith the plain `Estimator` API I also do not have this restriction.\r\n\r\nI am passing the following object to `gan_model` through `generator_inputs`:\r\n```python\r\nTextInput(initializer=<tf.Operation 'MakeIterator' type=MakeIterator>, batch_size=<tf.Tensor 'Size:0' shape=() dtype=int32>, source=<tf.Tensor 'IteratorGetNext:0' shape=(?, ?) dtype=int32>, target_input=<tf.Tensor 'IteratorGetNext:1' shape=(?, ?) dtype=int32>, target_output=<tf.Tensor 'IteratorGetNext:2' shape=(?, ?) dtype=int32>, source_sequence_length=<tf.Tensor 'IteratorGetNext:3' shape=(?,) dtype=int32>, target_sequence_length=<tf.Tensor 'IteratorGetNext:4' shape=(?,) dtype=int32>)\r\n``` \r\n\r\nWhen running my program `_convert_tensor_or_l_or_d` throws an error:\r\n```\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\tf-backbone\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\train.py\", line 103, in gan_model\r\n    with variable_scope.variable_scope(generator_scope) as gen_scope:\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\tf-backbone\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\train.py\", line 789, in _convert_tensor_or_l_or_d\r\n    return [ops.convert_to_tensor(x) for x in tensor_or_l_or_d]\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\tf-backbone\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\train.py\", line 789, in <listcomp>\r\n    return [ops.convert_to_tensor(x) for x in tensor_or_l_or_d]\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\tf-backbone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 836, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\tf-backbone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 926, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\tf-backbone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 5069, in _operation_conversion_error\r\n    name, as_ref))\r\nTypeError: Can't convert Operation 'MakeIterator' to Tensor (target dtype=None, name=None, as_ref=False)\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7f84d88d39f236e5c0cea492a2248782e696c972/tensorflow/contrib/gan/python/train.py#L104", "comments": ["Same goes for `real_data`.\r\nLike in the regular `Estimator` API I would like to pass in `None` or any other value I like and do the handling of the input data myself instead of the framework trying to convert it to a tensor for me.", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I am on Windows 10, Tensorflow 1.4.0 (pip).\r\nI do not see how my operating system and architecture are related to this question, though.\r\n\r\nMaybe you misunderstood my question.\r\nI would like to pass arbitrary data to my `generator_fn` and `discriminator_fn` as it is possible with `tf.estimator.Estimator`. This is a matter of consistency for me. I would allow reusing most of my data preprocessing already implemented for `tf.estimator.Estimator`.\r\n\r\nA solution is to just remove the conversions entirely. I just don't know why they are there in the first place.", "Yes, I misread this as an error during invocation. Sorry about that.\r\n\r\n", "@mrry, can you take a look at this?", "Seems specific to TFGAN, so I'll defer to @joel-shor.", "In general, I would like full control over the data flow in `GANEstimator`. The framework should not prevent me from passing data between the functions I implement.\r\n\r\nSame problem occurs with `gan_estimator_impl.py` where `_use_check_shapes(real_data)` is used to validate the shape of my generated data preventing me from passing `[batch_size]` shaped real_data to the `GANEstimator` while passing `[batch_size, x]` shaped data from the `generator_fn` to `generator_loss_fn`.", "As a side note, is there an easy way to pretrain generator and discriminator independently using `GANEstimator`?", "In general, `GANEstimator` is absolutely not meant to be a fully-flexible GAN solution; none of the canned Estimators are. For full control, you should use `TFGAN` or write your own `model_fn`, based on `GANEstimator` code. \r\n\r\nRe your specific questions: \r\n1) Tensor conversions are standard in publicly accessible functions. However, I think all this code should be compatible with the new Dataset API, and I'll work with mrry@ on the best way to make that happen.\r\n\r\n2) I don't understand your use-case for passing real data of shape `[batch_size]` and generated data of `[batch_size, x]`, so I can't say whether or not this is a common-enough use-case to support in GANEstimator. Can you motivate this a bit more? Also, note that TFGAN already supports this behavior.\r\n", "1. I can use the Dataset API with `GANEstimator` using a hook that runs the Dataset initializer.\r\n\r\n2. My description was not quite correct. I have generator output of shape `[time, batch_size, vocab_size]` and real data of shape `[time, batch_size]` as would be the case when using a sequence approach with e.g. `RNN`s. `GANEstimator` complains about mismatching shapes but I need them that way for my loss calcuation `tf.nn.sparse_softmax_cross_entropy_with_logits`. There is probably a way to convert the generator output to shape `[time, batch_size]` as well using softmax but I would rather use the inbuilt `softmax_cross_..` function then performing `softmax` beforehand just to overcome a \"shape mismatch\" error. ", "For the pretraining I see the best possible solution is to write modular code where I plug my `Generator` and `Discriminator` either in `GANEstimator` together or each one in a single `Estimator` for pretraining. This was my initial approach but that fails due to different shape checks and slightly different behavior (as reported above) of `GANEstimator` compared to `Estimator`. That is also why I suggest to make them more alike.", "Sorry for the delay.\r\n\r\n1) The Dataset API folks are trying to make `MakeIterator` more standard. Currently, it seems that you are passing the initialization Op where a Tensor is expected. If instead you store the initialization op somewhere (like a collection) and pass the input Tensor instead, things should work.\r\n\r\n2) I'm about to submit a CL removing the shape check.\r\n\r\n3) There isn't currently first class infra support for pretraining just the discriminator or generator. I think you could either train them separately and initialize the weights to be what you want inside the generator_fn/discriminator_fn itself, or use a custom train loop (which is about to be added).\r\n\r\nDo these comments/changes address your issues?", "1. I have found a good solution to circumvent that problem.\r\n2. Great :)\r\n3. For now I am using two plain `Estimator`s for both the pretraining of generator and discriminator.  Let me know when you come up with a better solution for that problem :)", "Your PR #14723 solves this, no? Instead of using the default train step, just set it up to train the generator for M steps then the discriminator for the next N steps, then do whatever you want with training. Doesn't that work?", "I haven't tested that yet but it should. I finished the two separate Estimators before there was any traction with my pull request.", "I opened up a new issue for the pre-training/training with GANEstimator here #15271.", "By the way why is the real data\r\nhttps://github.com/tensorflow/tensorflow/blob/f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/gan/python/train.py#L110\r\nnot handled like\r\nhttps://github.com/tensorflow/tensorflow/blob/f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/gan/python/train.py#L104\r\n?", "Real data and generator inputs are different kinds of things. The former\ngoes to the discriminator, the latter goes to the generator. The analogy\nisn't between real data and generator inputs, but between real data and\ngenerator outputs, which are handled the same way.\n\nOn Fri, Dec 15, 2017 at 2:23 PM, Julian Niedermeier <\nnotifications@github.com> wrote:\n\n> By the way why is the real data\n> https://github.com/tensorflow/tensorflow/blob/\n> f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/\n> gan/python/train.py#L110\n> not handled like\n> https://github.com/tensorflow/tensorflow/blob/\n> f5f2f789ea395e585ddcbc43e088fa63d6b41d0e/tensorflow/contrib/\n> gan/python/train.py#L104\n> ?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14257#issuecomment-352017214>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFvffIpFv7_9LN5ZSUXi1jBDoCMvg1T4ks5tAoDkgaJpZM4QSYA3>\n> .\n>\n", "I was just wondering that in the conditioned GAN case I could pass generator input as a dictionary whereas the real data has to be a list because dicts are not supported by `ops.convert_to_tensor`.\r\nFor my condition I can pass information like \"sequence lengths before padding\" along but I cannot do that for the real_data.", "@joel-shor I'm trying out TFGAN in eager execution mode. Unfortunately, [the TFGAN MNIST tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) is not compatible because `download_and_convert_mnist.py` from `models/research/slim` [uses placeholders](https://github.com/tensorflow/models/blob/a3669a934961f71894f959f2765182d6d5062931/research/slim/datasets/download_and_convert_mnist.py#L117).\r\n\r\nThe Eager MNIST example [uses the Dataset API](https://github.com/tensorflow/tensorflow/blob/d87a9fbbc5f49ec5ae8eb52c62628f0b1a0bf67f/tensorflow/contrib/eager/python/examples/mnist/mnist.py#L135).\r\n\r\nIs it possible to use the Dataset API with TFGAN at this moment? If so, is there an example of this? Thanks!", "joel-shor: \"The Dataset API folks are trying to make MakeIterator more standard. Currently, it seems that you are passing the initialization Op where a Tensor is expected. If instead you store the initialization op somewhere (like a collection) and pass the input Tensor instead, things should work.\" \r\n\r\nsleighsoft: \"I have found a good solution to circumvent that problem.\"\r\n\r\n@sleighsoft, what is the solution you found for this?\r\n  ", "FWIW, in 1.5, `tf.estimator.Estimator` lets you return a `tf.data.Dataset` from an input function and takes care of creating and initializing the iterator for you. Other `Estimator`-based APIs should probably be updated to follow suit.", "I created a separate issue for the eager compatibility: #15821.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe this issue has been solved, as it isn't actually a problem with TFGAN or the dataset API."]}, {"number": 14256, "title": "Update estimator.py", "body": "Replace contrib metrics with core metrics", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@martinwicke I believe my commit was passing all the tests a couple of days ago, but now they are failing four.", "This looks like infra failure.", "@sb2nov FYI, ready to merge."]}, {"number": 14255, "title": "Update learning.py", "body": "-Replace variables for training_util\r\n-Replace tf_variables with variables", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14254, "title": "Tensors erroneously zero when an existing session is running on the same device", "body": "UPDATE: Restarting my machine fixed the issue.\r\n\r\nTo reproduce, run two processes with\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\nz = tf.constant([1.8, 2.2], dtype=tf.float32)\r\nz = tf.cast(z, tf.int32)\r\nwith tf.Session() as sess:\r\n    while True:\r\n        print('z is', sess.run(z), 'but should be [1, 2]')\r\n        time.sleep(1)\r\n```\r\nSecond process output\r\n```\r\nz is [0 0] but should be [1, 2]\r\nz is [0 0] but should be [1, 2]\r\n...\r\n```\r\nStopping the first process will fix the problem immediately without needing to restart the second process!\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: CUDA 8 cuDNN 5.1.10\r\n- **GPU model and memory**: GeForce GTX 980 4GB", "comments": ["I can't reproduce it on my local machine either. Thanks for the update. I will close this issue.\r\n"]}, {"number": 14253, "title": "Fixes for Raspberry Pi cross-compilation in CI Build", "body": "", "comments": ["Jenkins test this please (Windows Cmake error looks like a github flake?)", "Correct, cmake issue is a flake:\r\n```\r\nCUSTOMBUILD : error : downloading 'http://www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz' failed\r\n```"]}, {"number": 14252, "title": "[Feature request] tf.edit_distance return deletions, substitutions, insertions", "body": "Hi,\r\nWhen analysing the performance of a speech recognition model, it is very useful to know the distribution of deletions, substitutions and insertions on the test set.\r\n\r\nCould you make `tf.edit_distance` return these three metrics too, in addition to the cheapest cost?\r\nIt appears to me that the [cpp code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/gtl/edit_distance.h#L47) already computes them. What do you think, @ebrevdo ?", "comments": ["Sounds like we need to create a new op that returns all three\n\nOn Sat, Nov 4, 2017, 4:11 PM George Sterpu <notifications@github.com> wrote:\n\n> Hi,\n> When analysing the performance of a speech recognition model, it is very\n> useful to know the distribution of deletions, substitutions and insertions\n> on the test set.\n>\n> Could you make tf.edit_distance return these three metrics too, in\n> addition to the cheapest cost?\n> It appears to me that the cpp code\n> <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/gtl/edit_distance.h#L47>\n> already computes them. What do you think, @ebrevdo\n> <https://github.com/ebrevdo> ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14252>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzIsf2YOJoqPEJXx3aRDsLqMW3RPks5szO8vgaJpZM4QSMcA>\n> .\n>\n", "Hi @ebrevdo ,\r\nExcuse me for this rookie question.\r\nHow do I find the gradient implementation of tf.edit_distance (or any other op) ?\r\n\r\nI added the tensor returned by tf.edit_distance to the traditional cross entropy loss, and can see that it is possible to pass this to an optimiser. The aim is to obtain an implementation that is conformant with Google's paper https://arxiv.org/abs/1712.01818\r\n\r\nThank you", "edit_distance does not have a gradient, because it has integer outputs and\noutputs and is not differentiable.  only ctc_loss is differentiable.\n\nOn Fri, Jan 19, 2018 at 2:53 PM, George Sterpu <notifications@github.com>\nwrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo> ,\n> Excuse me for this rookie question.\n> How do I find the gradient implementation of tf.edit_distance (or any\n> other op) ?\n>\n> I added the tensor returned by tf.edit_distance to the traditional cross\n> entropy loss, and can see that it is possible to pass this to an optimiser.\n> The aim is to obtain an implementation that is conformant with Google's\n> paper https://arxiv.org/abs/1712.01818\n>\n> Thank you\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14252#issuecomment-359112300>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5Utj71DD4t_KBNjPfJiSGtgzzkdks5tMRz4gaJpZM4QSMcA>\n> .\n>\n", "hi @ebrevdo , since this issue has been closed, just wondering if the feature has been implemented. thanks much!", "@ebrevdo Would it be possible to get a gradient for edit_distance when the output is normalized?\r\nThis way the output would be a float."]}, {"number": 14251, "title": "Add int64 support of `axis` (`Tidx`) for ConcatV2", "body": "In `array_ops.cc`, it was specified that ConcatV2 support both int32 and int64 data types of `axis` (`Tidx`):\r\n```\r\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\r\n```\r\n\r\nHowever, in actual kernel implementations only int32 is supported as there is an unnecessary `.TypeConstraint<int32>(\"Tidx\")` specified.\r\n\r\nThis fix tries to address the discrepancy between the ops declaration and kernel registration by adding the int64 axis (`Tidx`) support for `ConcatV2`.\r\n\r\nThis fix removes the TypeConstraint and adds additional processing so that differnt types (int32 or int64) of `axis` could be processed correctly.\r\n\r\nAdditional test cases have been added to cover the changes as well.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@aselle could you look at this, especially whether the conditionals in the kernels is how we want this implemented?", "@aselle any chance to take a look?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "```\r\nERROR: testConcatAxisType (__main__.ConcatOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/concat_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/concat_op_test.py\", line 615, in testConcatAxisType\r\n    c = gen_array_ops._concat_v2([t1, t2],\r\nAttributeError: 'module' object has no attribute '_concat_v2'\r\n\r\n----------------------------------------------------------------------\r\nRan 36 tests in 7.705s\r\n\r\nFAILED (errors=1)\r\n```", "@martinwicke @drpngx Thanks for the review, the PR has been updated and all tests passed now.", "Hit the wrong button\ud83d\ude05,so reopen."]}, {"number": 14250, "title": "[Feature request] tf.data.Dataset sort and skip buckets", "body": "Hi\r\nIt would be useful to sort the variable length inputs by their lengths in order to accelerate the training process. However I cannot find this functionality yet.\r\n\r\nIn [[1]](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308789560), @guillaumekln already suggested something similar through his code snippet, yet the requested feature was batching inputs of similar length together, regardless of the processing order of the batches, and the solution of @mrry in [[2]](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-326098305) using `group_by_window()` addressed this request just fine. _First question_: Would it be possible to make the iterator return the batches in the ascending order of their ids (given by `key_func`), while maintaining the shuffling operation applied before batching?\r\n\r\nAdditionally, I would like to skip the longer sentences early in training, with a length threshold that would gradually increase depending on the `global_step`. _Second question_: Could you reserve one batch id (e.g. -1) in `group_by_window` to tag the batches that will be skipped ? At the moment, it seems that all the ids are considered, even the negative values, and it would not be restrictive at all to allow only positive values (as there would still be 63 bits left to group the inputs). Thus, in `key_func` we could simply compare the input length with the threshold and return a negative value when it is above it.\r\n\r\nApologies if both functionalities are already available, feel free to stackoverflow me.", "comments": ["> feel free to stackoverflow me\r\n\r\nI haven't heard that one before. [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) is probably your best bet when there's any uncertainty about whether or not a feature is necessary. We triagers can also take care of the work of tagging folks on the team for you in the future. I will note that you made me smile, so I'm going to try my best to fill in for the community.\r\n\r\nSpeaking as someone who only recently began learning `contrib.data`, from what I understand these are stateful iterative ops, so I'm not sure if the sort of sorting you described would be possible. Have you considered collecting all the data and using [tf.nn.top_k](https://www.tensorflow.org/api_docs/python/tf/nn/top_k)? I'm also assuming you're also familiar with `tf.cond` which has been traditionally used to implement skipping type logic.\r\n\r\nIf those things don't help, please give StackOverflow a try. I'll be happy to reopen this issue if provided new information.", "You are completely right, @jart. It would be more appropriate if I rephrase the last sentence as : \"After going through the entire support thread in [[1]](https://github.com/tensorflow/tensorflow/issues/7951) and searching related content on StackOverflow too, to the best of my knowledge these functionalities have not been implemented yet.\" \r\n\r\nIf you are unsure about the possibility of sorting a Dataset after applying the `group_by_window()` transformation, perhaps it would be fair to re-open the issue. Could you also help me find out what is the implicit order of the batches when this transformation is applied, please ?\r\n\r\nOn batch skipping, could you please provide me an example of ignoring the batches grouped by a range of ids within `group_by_window` ? I am probably overthinking this, yet it seems that the id returned by `key_func` would not be available anymore at runtime. Maybe `reduce_func` could discard some batches instead ?\r\n\r\nThank you", "Just so there's no misunderstanding, our team is always grateful when members of the community take the time to leave us feedback. My goal is to be friendly and helpful in this process. Per your request, I'm happy to reopen this issue, so the next triager can take a look in a day or so.", "Thanks, Justine. One workaround that crossed my mind is storing the inputs in separate TFRecords based on their lengths and concatenating several Datasets when creating the iterator. Having this feature on the iterator would only make things look more compact. ", "I also meet this problem. And more I want to use distribute tf to train my deepspeech2 model,So It requests dataset.get_next() to yield the same bucket_id data in distributed machines.I also try dataset.shard(), but this function returned dataset's bucket_id are not same in one batch.How do I implement this function? @mrry ", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Hello, as a future reference, skipping examples longer than some `max_length` could be done like this:\r\n\r\n`dataset = dataset.filter(lambda elem: tf.shape(elem)[0] < max_length)`"]}, {"number": 14249, "title": "Issue with installing TensorFlow with pip", "body": "Hi!\r\n\r\nI'm using Python 3.6.3\r\nI'm trying to install TensorFlow using:\r\n_pip3 install --upgrade tensorflow_\r\n\r\nI get the following error:\r\n_Collecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versi\r\nons: )\r\nNo matching distribution found for tensorflow_\r\n\r\nCould someone help to fix this?\r\n", "comments": ["@av8ramit will have details on Python 3.6 support.", "Additionally I downloaded .whl file\r\ntensorflow-1.4.0-cp36-cp36m-win_amd64.whl\r\nand try to install it with \r\n_pip3 install tensorflow-1.4.0-cp36-cp36m-win_amd64.whl_\r\nI had thefollowing error:\r\n_tensorflow-1.4.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platf\r\norm._\r\nI'm using Win7\r\n", "Can you provide more details on the platform you're using? (e.g., are you using 32-bit Windows by any chance?)", "TensorFlow has pip packages available for Python 3.6 on Linux, Mac and Win64.\r\nhttps://pypi.python.org/pypi/tensorflow", "Hmm I am trying to do this on macOS and I am also having issues. ", "I tried using a virtual environment that runs with both python 3.5 and 3.6 and pip did not work in both cases. What is going on. ", "@asimshankar \r\nI'm using Win7 64 bit.\r\nPython 3.6.3\r\nThe issue is still actual, can't install using pip", "@caisq \r\nI already tried to install from package:\r\nAdditionally I downloaded .whl file\r\ntensorflow-1.4.0-cp36-cp36m-win_amd64.whl\r\nand try to install it with\r\npip3 install tensorflow-1.4.0-cp36-cp36m-win_amd64.whl\r\nI had thefollowing error:\r\ntensorflow-1.4.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platf\r\norm.", "All,\r\nclosing the issue. I made a mistake and install 32 bit python. With 64 it's OK"]}, {"number": 14248, "title": "Keras backend functionality changed?", "body": "Hi,\r\n\r\nI have been using keras from within tensorflow since it was included into the contrib package - but it seems that in the 1.4 release the keras backend is missing some functionality ...\r\n\r\nFor example:\r\n\r\n    >> from tensorflow.python.keras import backend as K\r\n    >> K.tile\r\n    Traceback (most recent call last):\r\n      File \"<stdin>\", line 1, in <module>\r\n    AttributeError: module 'tensorflow.python.keras.backend' has no attribute 'tile'\r\n\r\nWhereas in tensorflow 1.1:\r\n\r\n    >>> from tensorflow.contrib.keras.python.keras import backend as K\r\n    >>> K.tile\r\n    <function tile at 0x7fbd9024fb70>\r\n\r\nAnd using pure keras:\r\n\r\n    >>> from keras import backend as K\r\n    Using TensorFlow backend.\r\n    \r\n    >>> K.tile\r\n    <function tile at 0x7fe9743c5950>\r\n\r\n\r\nIs this just an omission, or has the functionality been deliberately removed?  I know I can use the equivalent tensorflow operation - but it's nice to be able to use a reasonably portable api and if I wanted to switch backend I could just change the import path.  I also try not too mix keras and tensorflow too much as I think the code is more readable just using one!\r\n\r\nRegards,\r\n\r\nAlex\r\n\r\n", "comments": ["@fchollet any reason tile not included in tensorflow/python/keras/backend/__init__.py?\r\n\r\n", "Thanks for reporting this, it seems like it was omitted from the list of symbols that are part of the public API. We'll fix it in the next release.\r\n\r\nHave you noticed this for any other symbol than `tile`?", "Hi @fchollet - thanks for the quick response!\r\n\r\nAside from tile, the following are missing:\r\nconv3d_transpose\r\ncumprod\r\ncumsum\r\ndepthwise_conv2d\r\nidentity\r\nlocal_conv1d\r\nlocal_conv2d\r\nlogsumexp\r\n\r\nI have added these imports into __init__.py and created a pull request.\r\n\r\n#14318 \r\n\r\nRegards,\r\n\r\nAlex", "@fchollet Could the tf keras and backend import become simpler for the next release? \r\n\r\nPerhaps:\r\n`import tf.keras` and `from tf.keras import backend as K`\r\n\r\nAny reasonable and easy to use option would be fantastic! If nothing else better documentation of the current preferred way to import keras and the backend would also be helpful.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "According to your Python or Anaconda version ....install perfect version of tensorflow and keras otherwise it will give error. ", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Closing since it looks like the issue was resolved. Feel free to reopen.", "I've run into this using tf1.13.1 and 1.12 (not sure minor version) on osx:\r\n\r\n   import tensorflow.keras.backend as K\r\n   'cumsum' in dir(K)     # False\r\n\r\nOn ubuntu, it's present in 1.13.0-dev20190208 but not in 1.12.0\r\n\r\n"]}, {"number": 14247, "title": "BUG: ImportError: No module named 'tensorflow.contrib.eager'", "body": "### System information\r\n\r\n##### I tried to `import tensorflow.contrib.eager as tfe` after installation and it throws No module error.\r\n##### OS Platform and Distribution Linux Ubuntu 16.04:\r\n##### TensorFlow installed from python pip in virtualenv:\r\n##### TensorFlow version 1.1.0:\r\n##### Python version 3.5:\r\n##### GPU model and memory: GeForce GTX 1080\r\n##### CUDA 8.0 /cuDNN 6.0:\r\n\r\n\r\n### Describe the problem\r\nAfter I install the eager, i.e. `pip install tf-nightly-gpu` in my virtualenv activated I run\r\n```\r\n(tensorflow-gpu-3.5) marija@dhcp-90-160:~/DCGAN-tensorflow$ python\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import tensorflow.contrib.eager as tfe\r\n```\r\nand get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named 'tensorflow.contrib.eager'\r\n```\r\n", "comments": ["That seems like it isn't using the latest nightly builds. Could you try the following to ensure that you're not using an old, cached nightly build:\r\n\r\n```sh\r\npip install -U tf-nightly-gpu\r\n```\r\n\r\nIf that still doesn't work, do add more detailed version information by running the following in a Python shell:\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nprint(tf.__git_version__)\r\n```\r\n\r\nThanks\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "The nightly built wheels are sometimes less than 30mb, which does not make sense to me. And I confirm that tf_nightly-1.5.0.dev20171216-cp35-cp35m-win_amd64.whl (29.46 MB) does not have eager mode. \r\n", "I run into this same error\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-fb328cb2bd94> in <module>()\r\n----> 1 import tensorflow.contrib.eager as tfe\r\n      2 tfe.enable_eager_execution()\r\n      3 import tensorflow as tf\r\n\r\nImportError: No module named 'tensorflow.contrib.eager'\r\n\r\n\r\nI am running \r\nMacOS 10.13.2\r\nPython 3.5.2 :: Anaconda 4.2.0 (x86_64)\r\nI installed Tensorflow using docker\r\n", "Have someone fixed this question?\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n1.4.0\r\n>>> print(tf.__git_version__)\r\nb'unknown'\r\n>>>\r\nEmmm, what the 'git' used for contrib.eager?\r\nThanks.", "@pinemosquito @EricKani - `tensorflow.contrib.eager` was not included in TensorFlow 1.4. It is included in TensorFlow 1.5.", "Thank you!  My version was very old 0.12.1  \r\nI updated with docker run -it gcr.io/tensorflow/tensorflow:latest-devel to 1.5 and it is working now\r\n", "Thank you\uff01", "Upgrading TensorFlow solved the issue."]}, {"number": 14246, "title": "typo fixed in CONTRIBUTING.md", "body": "The noun phrase [contribution - decision - code] seems to be missing a determiner before it. Consider adding an article.\r\n", "comments": ["Can one of the admins verify this patch?", "The indeterminate \"code\", as in, \"some of the code\", is ok here. "]}, {"number": 14245, "title": "Issue building libtensorflow_cc.so for arm64-v8a", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 17.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **GCC/Compiler version (if compiling from source)**: 6.0.3\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n> bazel build -c opt //tensorflow:libtensorflow_cc.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a\r\n\r\n\r\n### Describe the problem\r\nI am trying to build libtensorflow_cc.so for android for arm64-v8a  architecture.\r\nI needed it to create and train a model file in Android NDK through C++ at runtime.\r\nI was able do so on Desktop using C++ with the help of 'libtensorflow_cc.so' generated by \r\n\r\n> bazel build -c opt //tensorflow:libtensorflow_cc.so\r\n\r\n, but wanted to integrate with Android NDK now and train on mobile.\r\n\r\nI tried with 'libtensorflow.so' , but i get below error\r\n\r\n> tensorflow_jni.cc:359 Non-OK-status: session->Create (graph_def) status: Invalid argument: No OpKernel was registered to support Op 'SparseSoftmaxCrossEntropyWithLogits' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n>                                                                    <no registered kernels>\r\n>                                                                  \r\n>                                                                  \t [[Node: SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64](add, Cast)]]\r\n\r\nI also tried with 'libtensorflow_inference.so' but I get many undefined reference errors like below\r\n\r\n> In function `tensorflow::TensorShapeRep::~TensorShapeRep()':\r\n> /home/ashok/AndroidStudioProjects/Android-arm64-v8a-dnn/facerecognitionlibrary/jni-build/jni/include/include1/tensorflow/core/framework/tensor_shape.h:492: undefined reference to `tensorflow::TensorShapeRep::DestructorOutOfLine()'\r\n> ./obj/local/arm64-v8a/objs-debug/tensorflow_cc1/tensorflowTrian_jni.o: In function `std::pair<std::string, tensorflow::Tensor>::~pair()':\r\n> /home/ashok/Ashok/android-ndk-r12b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/stl_pair.h:96: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n> ./obj/local/arm64-v8a/objs-debug/tensorflow_cc1/tensorflowTrian_jni.o: In function `std::string* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\n> /home/ashok/AndroidStudioProjects/Android-arm64-v8a-dnn/facerecognitionlibrary/jni-build/jni/include/include1/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\n\r\n\r\n\r\n### Source code / logs for\r\n\r\n> bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a\r\n\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> WARNING: /home/ashok/Ashok/tensorflow/tensorflow/core/BUILD:952:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n> ERROR: /home/ashok/Ashok/tensorflow/tensorflow/c/eager/BUILD:11:1: in deps attribute of cc_library rule //tensorflow/c/eager:c_api: target '//tensorflow/c/eager:c_api_internal' does not exist. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation in /home/ashok/Ashok/tensorflow/tensorflow/tensorflow.bzl:667:12\r\n> ERROR: Analysis of target '//tensorflow:libtensorflow_cc.so' failed; build aborted\r\n> INFO: Elapsed time: 0.252s\r\n> FAILED: Build did NOT complete successfully (0 packages loaded)\r\n> \r\n\r\nStackoverflow - https://stackoverflow.com/questions/47108420/issue-building-libtensorflow-cc-so-for-arm64-v8a", "comments": ["Unfortunately, the TensorFlow maintainers do not have the bandwidth to provide support for building from source using cross-compilers and rely on community support for that.\r\n\r\nThat said, one thing that does come to mind from your description:\r\nThe linking error might be because you're not including `libtensorflow_framework.so`, which `libtensorflow_cc.so` depends on. However, you could build a single library that doesn't have this dependency by adding `--config=monolithic` to the `bazel` commandline, so:\r\n\r\n```sh\r\nbazel build -c opt \\\r\n  --config=monolithic \\\r\n  //tensorflow:libtensorflow_cc.so \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  --cpu=arm64-v8a\r\n```\r\n\r\n@andrewharp may have some more insights.", "@asimshankar  Thanks for your time and response\r\n\r\nI tried to execute the command as suggested\r\n\r\n\r\n`bazel build -c opt --config=monolithic //tensorflow:libtensorflow_cc.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a`\r\n\r\nBut unfortunately I receive same set of errors with an additional warning\r\n\r\n`> WARNING: Config values are not defined in any .rc file: monolithic`\r\n\r\nCould I please know if i need to add any set of lines in \r\n` /tensorflow/tools/bazel.rc` to resolve the warning OR Can I please know if there is any other way to resolve it\r\nThanks", "Hello,\r\n\r\nI try to compile tensorflow for arm64-v8a. I use the previous posted command:\r\n`bazel build -c opt \\\r\n  --config=monolithic \\\r\n  //tensorflow:libtensorflow_cc.so \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  --cpu=arm64-v8a\r\n\r\nbazel build --config=android_arm64 //tensorflow:libtensorflow.so`\r\n\r\n\r\nI get that error:\r\n\r\n`bazel build --config=android_arm64 //tensorflow:libtensorflow.so\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1825:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/tensorflow.bzl:1152:30\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:batch_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/BUILD:1035:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nINFO: Analysed target //tensorflow:libtensorflow.so (54 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/xavier/.cache/bazel/_bazel_xavier/ef54af8645dec4f38c438d8e1c779747/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1)\r\nIn file included from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nIn file included from external/highwayhash/highwayhash/sip_hash.h:25:\r\nexternal/highwayhash/highwayhash/state_helpers.h:76:3: error: use of undeclared identifier 'static_assert'; did you mean 'static_cast'?\r\n  static_assert((kPacketSize & (kPacketSize - 1)) == 0, \"Size must be 2^i.\");\r\n  ^\r\nIn file included from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nexternal/highwayhash/highwayhash/sip_hash.h:33:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\n  using Key = HH_U64[2];\r\n              ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:104:22: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing SipHashState = SipHashStateT<2, 4>;\r\n                     ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:105:24: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing SipHash13State = SipHashStateT<1, 3>;\r\n                       ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:20:13: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing Key = highwayhash::SipHashState::Key;\r\n            ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:21:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing Key13 = highwayhash::SipHash13State::Key;\r\n              ^\r\n5 warnings and 1 error generated.\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 164.508s, Critical Path: 13.57s\r\nFAILED: Build did NOT complete successfully\r\n`\r\n\r\n\r\nAnd when I try that:\r\n`bazel build -c opt \\\r\n  --config=monolithic \\\r\n  //tensorflow:libtensorflow_cc.so \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  --cpu=arm64-v8a`\r\n\r\nI get that error:\r\n\r\n`[462 / 815] Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc.runfiles [for host]ERROR: /home/xavier/.cache/bazel/_bazel_xavier/ef54af8645dec4f38c438d8e1c779747/external/com_googlesource_code_re2/BUILD:11:1: C++ compilation of rule '@com_googlesource_code_re2//:re2' failed (Exit 1)\r\nIn file included from external/com_googlesource_code_re2/re2/prefilter.cc:5:\r\nIn file included from external/com_googlesource_code_re2/re2/prefilter.h:17:\r\nexternal/com_googlesource_code_re2/util/logging.h:81:35: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  LogMessage(const LogMessage&) = delete;\r\n                                  ^\r\nexternal/com_googlesource_code_re2/util/logging.h:82:46: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  LogMessage& operator=(const LogMessage&) = delete;\r\n                                             ^\r\nexternal/com_googlesource_code_re2/util/logging.h:101:45: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  LogMessageFatal(const LogMessageFatal&) = delete;\r\n                                            ^\r\nexternal/com_googlesource_code_re2/util/logging.h:102:56: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  LogMessageFatal& operator=(const LogMessageFatal&) = delete;\r\n                                                       ^\r\nIn file included from external/com_googlesource_code_re2/re2/prefilter.cc:5:\r\nexternal/com_googlesource_code_re2/re2/prefilter.h:102:33: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Prefilter(const Prefilter&) = delete;\r\n                                ^\r\nexternal/com_googlesource_code_re2/re2/prefilter.h:103:44: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Prefilter& operator=(const Prefilter&) = delete;\r\n                                           ^\r\nIn file included from external/com_googlesource_code_re2/re2/prefilter.cc:16:\r\nIn file included from external/com_googlesource_code_re2/re2/re2.h:186:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/mutex:35:\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/c++0x_warning.h:32:2: error: This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.\r\n#error This file requires compiler and library support for the \\\r\n ^\r\nIn file included from external/com_googlesource_code_re2/re2/prefilter.cc:16:\r\nexternal/com_googlesource_code_re2/re2/re2.h:350:46: warning: variadic templates are a C++11 extension [-Wc++11-extensions]\r\n  template <typename F, typename SP, typename... A>\r\n                                             ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:363:21: warning: variadic templates are a C++11 extension [-Wc++11-extensions]\r\n  template <typename... A>\r\n                    ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:364:66: warning: rvalue references are a C++11 extension [-Wc++11-extensions]\r\n  static bool FullMatch(const StringPiece& text, const RE2& re, A&&... a) {\r\n                                                                 ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:368:21: warning: variadic templates are a C++11 extension [-Wc++11-extensions]\r\n  template <typename... A>\r\n                    ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:369:69: warning: rvalue references are a C++11 extension [-Wc++11-extensions]\r\n  static bool PartialMatch(const StringPiece& text, const RE2& re, A&&... a) {\r\n                                                                    ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:373:21: warning: variadic templates are a C++11 extension [-Wc++11-extensions]\r\n  template <typename... A>\r\n                    ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:374:59: warning: rvalue references are a C++11 extension [-Wc++11-extensions]\r\n  static bool Consume(StringPiece* input, const RE2& re, A&&... a) {\r\n                                                          ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:378:21: warning: variadic templates are a C++11 extension [-Wc++11-extensions]\r\n  template <typename... A>\r\n                    ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:379:66: warning: rvalue references are a C++11 extension [-Wc++11-extensions]\r\n  static bool FindAndConsume(StringPiece* input, const RE2& re, A&&... a) {\r\n                                                                 ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:746:16: error: no type named 'once_flag' in namespace 'std'\r\n  mutable std::once_flag rprog_once_;\r\n          ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:747:16: error: no type named 'once_flag' in namespace 'std'\r\n  mutable std::once_flag num_captures_once_;\r\n          ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:748:16: error: no type named 'once_flag' in namespace 'std'\r\n  mutable std::once_flag named_groups_once_;\r\n          ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:749:16: error: no type named 'once_flag' in namespace 'std'\r\n  mutable std::once_flag group_names_once_;\r\n          ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:751:21: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  RE2(const RE2&) = delete;\r\n                    ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:752:32: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  RE2& operator=(const RE2&) = delete;\r\n                               ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:365:49: error: no member named 'forward' in namespace 'std'\r\n    return Apply(FullMatchN, text, re, Arg(std::forward<A>(a))...);\r\n                                           ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:365:57: error: 'A' does not refer to a value\r\n    return Apply(FullMatchN, text, re, Arg(std::forward<A>(a))...);\r\n                                                        ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:363:25: note: declared here\r\n  template <typename... A>\r\n                        ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:370:52: error: no member named 'forward' in namespace 'std'\r\n    return Apply(PartialMatchN, text, re, Arg(std::forward<A>(a))...);\r\n                                              ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:370:60: error: 'A' does not refer to a value\r\n    return Apply(PartialMatchN, text, re, Arg(std::forward<A>(a))...);\r\n                                                           ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:368:25: note: declared here\r\n  template <typename... A>\r\n                        ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:375:48: error: no member named 'forward' in namespace 'std'\r\n    return Apply(ConsumeN, input, re, Arg(std::forward<A>(a))...);\r\n                                          ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:375:56: error: 'A' does not refer to a value\r\n    return Apply(ConsumeN, input, re, Arg(std::forward<A>(a))...);\r\n                                                       ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:373:25: note: declared here\r\n  template <typename... A>\r\n                        ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:380:55: error: no member named 'forward' in namespace 'std'\r\n    return Apply(FindAndConsumeN, input, re, Arg(std::forward<A>(a))...);\r\n                                                 ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:380:63: error: 'A' does not refer to a value\r\n    return Apply(FindAndConsumeN, input, re, Arg(std::forward<A>(a))...);\r\n                                                              ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:378:25: note: declared here\r\n  template <typename... A>\r\n                        ^\r\nexternal/com_googlesource_code_re2/re2/re2.h:925:16: error: no type named 'once_flag' in namespace 'std'\r\n  mutable std::once_flag once_;\r\n          ~~~~~^\r\nexternal/com_googlesource_code_re2/re2/re2.h:915:10: error: no member named 'call_once' in namespace 'std'; did you mean 'calloc'?\r\n    std::call_once(once_, &LazyRE2::Init, this);\r\n    ~~~~~^~~~~~~~~\r\n         calloc\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cstdlib:133:11: note: 'calloc' declared here\r\n  using ::calloc;\r\n          ^\r\nIn file included from external/com_googlesource_code_re2/re2/prefilter.cc:18:\r\nIn file included from external/com_googlesource_code_re2/re2/walker-inl.h:19:\r\nexternal/com_googlesource_code_re2/re2/regexp.h:218:39: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  RegexpStatus(const RegexpStatus&) = delete;\r\n                                      ^\r\nexternal/com_googlesource_code_re2/re2/regexp.h:219:50: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  RegexpStatus& operator=(const RegexpStatus&) = delete;\r\n                                                 ^\r\nexternal/com_googlesource_code_re2/re2/regexp.h:270:33: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  CharClass(const CharClass&) = delete;\r\n                                ^\r\nexternal/com_googlesource_code_re2/re2/regexp.h:271:44: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  CharClass& operator=(const CharClass&) = delete;\r\n                                           ^\r\nexternal/com_googlesource_code_re2/re2/regexp.h:586:27: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Regexp(const Regexp&) = delete;\r\n                          ^\r\nexternal/com_googlesource_code_re2/re2/regexp.h:587:38: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Regexp& operator=(const Regexp&) = delete;\r\n                                     ^\r\nexternal/com_googlesource_code_re2/re2/regexp.h:622:47: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  CharClassBuilder(const CharClassBuilder&) = delete;\r\n                                              ^\r\nexternal/com_googlesource_code_re2/re2/regexp.h:623:58: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  CharClassBuilder& operator=(const CharClassBuilder&) = delete;\r\n                                                         ^\r\nIn file included from external/com_googlesource_code_re2/re2/prefilter.cc:18:\r\nexternal/com_googlesource_code_re2/re2/walker-inl.h:98:27: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Walker(const Walker&) = delete;\r\n                          ^\r\nexternal/com_googlesource_code_re2/re2/walker-inl.h:99:38: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Walker& operator=(const Walker&) = delete;\r\n                                     ^\r\nexternal/com_googlesource_code_re2/re2/prefilter.cc:502:27: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Walker(const Walker&) = delete;\r\n                          ^\r\nexternal/com_googlesource_code_re2/re2/prefilter.cc:503:38: warning: deleted function definitions are a C++11 extension [-Wc++11-extensions]\r\n  Walker& operator=(const Walker&) = delete;\r\n                                     ^\r\n29 warnings and 15 errors generated.\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 240.404s, Critical Path: 15.29s\r\nFAILED: Build did NOT complete successfully\r\n`\r\n\r\nHere are the infos about my computer\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.4\r\nPython version: 2.7\r\nBazel version (if compiling from source): 0.9.0\r\nGCC/Compiler version (if compiling from source): \r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce:", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!", "> --config=android_arm64\r\n\r\nDId u resolved this ? What is step to build tensorflow on android ( not tensorflow lite)", "try compiling libtensorflow-core.a using ndk14, tensorflow Makefiles and bazel are desinged to be used by older NDK , unless guys at google or someone modifies it for newer ndk supporting unified headers in newer ndk "]}, {"number": 14244, "title": "Can Tensorflow 1.4 use CUDA 9?", "body": "I upgraded to tensorflow 1.4. I found that this version can not load \"cudart64_90.dll\" when i import tensorflow in python.\r\n\r\nIt can work when i use  tensorflow 1.4 rc-0. But now it goes error:   Could not find 'cudart64_80.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 8.0 from this URL: https://developer.nvidia.com/cuda-toolkit.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["No, Tensorflow 1.4 doesn't support cuda 9.0. You may check the release docs here https://github.com/tensorflow/tensorflow/releases. They plan to release tensoflow 1.5 with cuda 9.0 and cuDNN 7.", "Yeah, cant use CUDA 9 yet. Just got this error\r\n\r\n`ImportError: Could not find 'cudart64_80.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 8.0 from this URL: https://developer.nvidia.com/cuda-toolkit`\r\n ", "Correct, the release builds of TensorFlow use CUDA 8 right now.\r\nThe next release will support CUDA 9.", "We've added CUDA 9 wheels here: https://github.com/mind/wheels/releases/tag/tf1.4-gpu-cuda9", "if you install it from anaconda:\r\n\r\npip install --ignore-installed --upgrade \\ https://github.com/mind/wheels/releases/download/tf1.4-gpu-cuda9/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n\r\ndanqing, Thanks a lot!!", "danqing, Great work\uff01", "@Danquing . Any plans for developing  the same  patch for windows 10? Thanks ", "@Fulvioo unfortunately not :( we don't have any windows machines.", "I'm working on Ubuntu 17.10, with cuda-9.0 and cuDNN 7.0 for cuda 9. I followed the instruction above. Running\r\n```\r\npip install --ignore-installed --upgrade  https://github.com/mind/wheels/releases/download/tf1.4-gpu-cuda9/tensorflow-1.4.0-cp27-cp27mu-linux_x86_64.whl \r\n```\r\nSuccesfully installed TF:\r\n```\r\nSuccessfully installed backports.weakref-1.0.post1 bleach-1.5.0 enum34-1.1.6 funcsigs-1.0.2 futures-3.1.1 html5lib-0.9999999 markdown-2.6.9 mock-2.0.0 numpy-1.13.3 pbr-3.1.1 protobuf-3.4.0 setuptools-36.7.2 six-1.11.0 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0rc2 werkzeug-0.12.2 wheel-0.30.0\r\n```\r\nbut when I try to import I get the following error :\r\n\r\n```\r\nPython 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:09:15)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://anaconda.org\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rodrigo/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/rodrigo/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/rodrigo/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/rodrigo/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/rodrigo/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/rodrigo/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\nAny suggestions would be very appreciated :)", "You want to follow the **NOTE** section in my url above: https://github.com/mind/wheels/releases/tag/tf1.4-gpu-cuda9", "Thanks for the cuda wheels!\r\n\r\nBtw I have Ubuntu 16.04 with anaconda and python 3.6, cuda 9, cudnn 7 and a GPU with compute capability 5.0. I also finished installing the additional MKL-dnn library. However I tried to install the cp36-cp36mu wheel yet it says my platform does not support that wheel. What might be lacking in my system? Thanks in advance! ", "How are you installing it? Can you paste the command you run? Also to make sure, do you have a reasonably new Intel CPU (past 3 years)?\r\n\r\nPlease feel free to open an issue in our repo and I can further assist you. Replying to this thread will email 10 people and don't want to create too much spam.", "so i take for window 10, tf 1.4 cuda 9 cudnn 7 is not an option yet?", "@colmantse yes you have to build it from source", "thanks", "@quarkleptonboson you need to write cp36-cp36m instead of cp36-cp36mu . I just had the same problem ;)", "It worked! Thanks!", "Can someone help me? I pretty new to this.\r\nI already installed cuda9.0 and cudnn7.0 then found out that cuda 9 isn't supported by tensorflow 1.4 then I came across to this thread.\r\nI've downloaded the whl and installed it using:\r\n`pip3 install tensorflow-1.4.0-cp35-cp35m-linux_x86_64.whl`\r\nit says that it was successful but when I try to import tensorflow I get the following error:\r\n`ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`\r\n\r\nI can't just download the cuda 8 since it will take me 1/4 of my day because of slow internet connection :|", "I would assume that you installed the wheel I provided above.\r\n\r\nIf so, follow the debug guide in our readme (https://github.com/mind/wheels#cuda) and make sure cuda 9 is in your path. It looks like your cuda 9 isn't correctly installed.\r\n\r\nIf you need further assistance, open an issue in our repo and I can help you there. 10+ people are subscribed to this thread and let's not do multiple back-and-forth's to spam them.", "it works fine on the terminal but if I import tensorflow on Pycharm, it doesn't work with ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory.\r\nCan I get any advice?", "After I added this at Environmental variables, it worked.\r\nName: LD_LIBRARY_PATH\r\nValue: /usr/local/cuda-9.0/lib64:${LD_LIBRARY_PATH}:/usr/local/lib\r\nThanks a lot @danqing by the way.", "hey how do I install tensorflow for Cuda 9 in Windows? Awesome work for Linux...", "@helloiamsourav  maybe you should build it from source~", "If you build from source , it does supprot CUDA 9\r\nI have built tensorflow binary wheels both for python2.x and python3.x , check out my repo or my blog\r\n`https://repo.puyuwang.me/pub/`\r\n", "Hello,  \r\nI have installed tensorflow with `pip install --ignore-installed --upgrade \\ https://github.com/mind/wheels/releases/download/tf1.4-gpu-cuda9/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl` and MKL.   \r\nHowever, I have this error when I import tensorflow\r\n`[libprotobuf FATAL google/protobuf/stubs/common.cc:61] This program requires version 3.4.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"external/protobuf_archive/src/google/protobuf/any.pb.cc\".)`.\r\nIt seems that Protocol Buffer is not updated although it was installed during the tensorflow installation : `Successfully installed (...) protobuf-3.5.0.post1  (...)`.\r\nThank you in advance", "@alexattia \r\nthe C++ version of google protobuff is 2.6.1 with this command:\r\n\r\n`sudo apt-get install libprotobuf-dev`", "Hi everyone I'm just having the same issue as @rcmf123. My cuda-9.1 installation is located in `/usr/local/cuda-9.1` next to `/usr/local/cuda-8.0` and I have updated the path accordingly in my `bashrc` but I still keep getting the same error when trying to import tensorflow.\r\n\r\nAfter some digging around I found that I have the following:\r\n```\r\nldconfig -v | grep libcublas*\r\nlibcublas.so.8.0 -> libcublas.so.8.0.88\r\nlibcublas.so.9.1 -> libcublas.so.9.1.85\r\n```\r\n\r\nShould I rename `libcublas.so.9.1` to `libcublas.so.9.0` in order to resolve the issue?\r\nI am not quite sure? Any thoughts?", " @kirk86 \r\nHi,I dont know if renaming the file could resovle the issue.However I am going build a tensorflow binary working with cuda 9.1 to reslove this. (my repo is https://repo.puyuwang.me/)\r\nIf you have CUDA 8.0 installed , you could just use the CUDA 8.0. Google`s official build only support CUDA 8.0 now.(1.4 version)", "@PaulWang1905 hi, I presume you have already builded `tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`? Is this one with support for cuda 9.1?", "@kirk86  I am still working on the bugs. The one you mentioned is for CUDA 9.0 . \r\nSorry. Give me one more day. ", "I could not build with cuda-9\n\nOn Tue, Dec 26, 2017 at 9:43 AM, Puyu Wang <notifications@github.com> wrote:\n\n> @kirk86 <https://github.com/kirk86> I am still working on the bugs. The\n> one you mentioned is for CUDA 9.0 .\n> Sorry. Give me one more day.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14244#issuecomment-353914592>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASmrRfikc-zheStQ2b-NQhV_yqItV6vaks5tEHJ0gaJpZM4QR5aL>\n> .\n>\n", "@helloiamsourav @kirk86 \r\nHi. Good news. I have built the tensorflow working with CUDA9.1\r\njust go to my repo: https://repo.puyuwang.me/ to download it. \r\nMake sure you download the right version  (I built for both python2.x and 3.6 environments)\r\nAnd if you want to build the binary on your own , you probably would encounter some errors with CUDA9.1.(most likely a header files error) It could be resovled by a quick hack:simply link the header files to the cuda/include directory.\r\n", "@PaulWang1905 that's awesome ;). Good job \ud83d\udc4d ", "@PaulWang1905 Thank you for doing this! Unfortunately, I am getting:\r\n`tensorflow-1.4.1-cp27-cp27mu-linux_x86_64_cuda9.1.whl is not a supported wheel on this platform.\r\n`\r\nAny ideas?", "@conduit242 Are you using python 2.7 ? If you are using python 3.6 (check with `pip --version`) you need  `tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`", "I am using Python 2.7, I was trying to use the Cuda 9.1 wheel that was mentioned above.\n\n> On Dec 28, 2017, at 12:28 PM, Alexandre Attia <notifications@github.com> wrote:\n> \n> @conduit242 Are you using python 2.7 ? If you are using python 3.6 (check with pip --version) you need tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@conduit242  What is your CPU type? I am going to built another whl for you. This time it should work. give me a few hours,:-)\r\n", "@PaulWang1905 It's an AMD Ryzen 1600, so x86_64...thank you! :+1: \r\n\r\nOn Thu, Dec 28, 2017 at 3:08 PM, Puyu Wang <notifications@github.com> wrote:\r\n\r\n> @conduit242 <https://github.com/conduit242> What is your CPU type?\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/14244#issuecomment-354350320>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AGCSLWBqXfD26x3HCv2BeqbO5hVS82wZks5tE_UggaJpZM4QR5aL>\r\n> .\r\n>\r\n", "@conduit242 \r\nI am done. This time it should work on both AMD and intel CPU. Do not forget to check the sha256sum.:-)\r\n`tensorflow-1.4.1-cp27-cp27mu-linux-generic-x86_64-cuda9.1.whl`", "@PaulWang1905 Thanks Paul, unfortunately I am still getting:\r\n`tensorflow-1.4.1-cp27-cp27mu-linux-generic-x86_64-cuda9.1.whl is not a supported wheel on this platform.`", "@conduit242 see if their build works ?  https://github.com/mind/wheels/releases/download/tf1.4.1-gpu-cuda91-generic/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl", "That worked! Thanks Paul.", "i work on this for three days ..cant found supported wheel for my platform..\r\n  tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whlthis one doesn't worked too\r\n", "got this error when trying python 3.5 \r\n File \"/usr/lib/python3.5/zipfile.py\", line 1093, in _RealGetContents\r\n    raise BadZipFile(\"File is not a zip file\")\r\nzipfile.BadZipFile: File is not a zip file\r\n\r\nany suggestions \r\n", "Consider to update the python version. The build you mean is for python 3.6 \r\nOr using virtualenv ,conda,or docker.\r\nOr use  this whl.: https://github.com/mind/wheels/releases/download/tf1.4.1-gpu-cuda91-generic/tensorflow-1.4.1-cp35-cp35m-linux_x86_64.whl\r\n", "I am on Windows 10 and have VS 2017 Community Edition. I tried to compile TensorFlow 1.4 with CUDA 9 but the compilation failed.\r\nIn particular, I have configured and generated the project files with the CMake build system. Then, as suggested in the guide, I tried to compile the project `tf_tutorials_example_trainer`.\r\nThe first linked project that fails to compile seems to be `tf_core_kernels`. I attach the compilation logs.\r\n[tf_core_kernels.log](https://github.com/tensorflow/tensorflow/files/1594891/tf_core_kernels.log)\r\n\r\nIn particular, there are the following errors:\r\n\r\n> D:\\Programs\\VS2017Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\\data\\programmi\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n>   D:\\data\\programmi\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(54): note: could be 'const __int64 &tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator::operator *(void)'\r\n>   d:\\Programs\\VS2017Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): note: while trying to match the argument list '(const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator)' (compiling source file D:\\data\\programmi\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n>   d:\\Programs\\VS2017Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2439): note: see reference to function template instantiation '_FwdIt std::_Lower_bound_unchecked<_Iter,_Ty,_Fn>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled\r\n>           with\r\n>           [\r\n>               _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n>               _Iter=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n>               _Ty=tensorflow::int64,\r\n>               _Fn=std::less<void>,\r\n>               _Pr=std::less<void>\r\n>           ] (compiling source file D:\\data\\programmi\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n>   d:\\Programs\\VS2017Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2447): note: see reference to function template instantiation '_FwdIt std::lower_bound<_FwdIt,_Ty,std::less<void>>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled\r\n>           with\r\n>           [\r\n>               _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n>               _Ty=tensorflow::int64,\r\n>               _Pr=std::less<void>\r\n>           ] (compiling source file D:\\data\\programmi\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n>   D:\\data\\programmi\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(115): note: see reference to function template instantiation '_FwdIt std::lower_bound<tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,tensorflow::int64>(_FwdIt,_FwdIt,const _Ty &)' being compiled\r\n>           with\r\n>           [\r\n>               _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n>               _Ty=tensorflow::int64\r\n>           ]\r\n> d:\\Programs\\VS2017Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2100: illegal indirection (compiling source file D:\\data\\programmi\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n> \r\n\r\nMoreover, various buffer overflows warnings are generated, which doesn't look good at all.\r\n", "PaulWang1905 , I had both python 3.5 and 3.6 ,,thanks for ( https://github.com/mind/wheels/releases/download/tf1.4.1-gpu-cuda91-generic/tensorflow-1.4.1-cp35-cp35m-linux_x86_64.whl\r\n) but it is not optimized  \r\ni mean when trying 3.6 wheel i got not supported platform ,,when trying 3.5 got the above error\r\nlet me have another try  ", "The guide provided in TinyMind's [link](https://github.com/mind/wheels#cuda) installs CUDA 9.1, which seems to be not compatible with this wheel. You need to run\r\n\r\n`sudo apt-get install cuda-9-0`\r\n\r\nand not\r\n\r\n`sudo apt-get install cuda`\r\n\r\nEDIT: sorry there is a [wheel](https://github.com/mind/wheels/releases/tag/tf1.4.1-gpu-cuda91) for CUDA 9.1 although it is not listed in the \"wheels you will most likely need\".", "i got it installed but had the same  issues as https://github.com/tensorflow/tensorflow/issues/13711 ,, ", "Go to http://www.python36.com/install-tensorflow141-gpu/ for step by step installation of tensorflow with cuda 9.1 and cudnn7.05 on ubuntu. ", "@arunmandal53  Hi, thanks for your link, did this guide work on your machine?  \r\n BTW, to all people having troubles with CUDA9.1, if you have to use CUDA9.1, consider to use the master branch or r1.5 branch. Some bugs have been fixed on the master branch. ", "I have not checked master or r1.5 branch. But i build tensorflow 1.4 on my ubuntu16.04 with gpu 840m geforce and conformed myself it worked. I have written all the required details step by step to build tensorflow from source with cuda 9.1 and cudnn 7.05.", " arunmandal53 thanks it works", "Windows 10 wheels please! Eagerly waiting for windows Cuda 9 support.", "@neilpanchal   The newest 1.5 support the CUDA 9 and CUDNN 7", "Please build it for Windows (Win 7, Win 8 and Win 10 if the OS version is important) with CUDA 9.1 support.", "@Yagun I have successfully built Tensorflow 1.5.0 on Windows 10 with CUDA 9.1 and CUDNN 7.0.5 using cmake and Visual Studio 2015 Update 3. Tested. Goto http://www.python36.com/install-tensorflow-gpu-windows for step by step tutorial on \"How to install Tensorflow 1.5.0 GPU with CUDA Toolkit 9.1 and cuDNN 7.0.5 for Python 3 on Windows OS\". Let me know if it worked for you.", "@arunmandal53 Thanks. I will try to build the 1.5 version on Windows 10.\r\nBut in your webiste there is written that \"Currently, SSE or other are not supported by cl (C/CPP compiler came with visual studio 2015) only AVX and AVX2 supported.\". Where did you take this information from?\r\nFor example, [here](https://stackoverflow.com/questions/1480916/how-do-i-enable-the-sse-sse2-instruction-set-in-visual-studio-2008-using-cmake) it says that it is supported at least since VS2005. Are you referring to TS Windows implementation in particular?", "@raffam actually i have used x64 based native cl compiler which does not have /arch:sse flags and it will throw ignoring warning. Though x86 build tools have sse enabled by default. But i have choosed x64 build tool due to out of memory errors.", "Hi could some one help me out , i have installed cuda 9.1 with cuddn 7.1 and all the dlls are present in the folder yet when i try to use tensorflow gpu it give me \"ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\" but when i download the cuda 9.0 it again gives the same error saying cudart64_80.dll is not available . \r\n\r\nI have tired installing all the version of the tensorflow and cuda yet i get error i have checked the path variable to not sure why i am getting this error on Windows 10 1060q gpu\r\n", "YES\uff0cjust download the correct version of cuda(as tf1.7.0_gpu need cuda9.0 and cudnn7).\r\nDon't forget to log out your computer after add the variables of cuda.\r\n", "@alexattia Do you have an import of any package using some ui components (matplotlib...) before your tensorflow import? That was the problem in my case, see #2903", "Anyone want to point me to the code that compiles models? Just ran into the slowness myself. It seems worse on CUDA 9."]}, {"number": 14243, "title": "SpaceToDepthGrad and DepthToSpaceGrad are not aware of data_format", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 8.0 / cuDNN 6\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n`tf.depth_to_space` and `tf.space_to_depth` support `data_format='NCHW'` on GPU. However, [`_SpaceToDepthGrad`](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/array_grad.py#L626) and [`_DepthToSpaceGrad`](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/array_grad.py#L633) are not aware of `data_format`. Maybe they would need to propagate `op.get_attr('data_format')`.\r\n\r\n### Source code / logs\r\n\r\n``` python\r\nimport tensorflow as tf\r\nx = tf.zeros([1, 4, 1, 1])\r\ny = tf.depth_to_space(x, 2, data_format='NCHW')\r\n# ValueError: Dimension size must be evenly divisible by 2 but is 1 for 'gradients ...\r\ng = tf.gradients(y, x)\r\n```\r\n", "comments": ["Thanks for the report, this indeed seems to be an oversight in https://github.com/tensorflow/tensorflow/commit/4d69d0408da946096163ee1d8ea068ae6698ae9d which introduced `data_format`.\r\n\r\n@yzhwang @wujingyue : Could you take a look?"]}, {"number": 14242, "title": "Concert keras model to tf.Keras model", "body": "here is my original keras model\r\n\r\ndef inference(image_tensor):\r\n    # the input size is 90*30*3\r\n    x = Conv2D(32,(3,3),name='conv1')(image_tensor)\r\n    x = MaxPool2D((2,2),name='pool1')(x)\r\n    x = BatchNormalization(name='bn1')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(64,(3,3),name='conv2')(x)\r\n    x = MaxPool2D((2, 2),name='pool2')(x)\r\n    x = BatchNormalization(name='bn2')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(64,(3,3), name='conv3')(x)\r\n    x = MaxPool2D((2, 2), name='pool3')(x)\r\n    x = BatchNormalization(name='bn3')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Flatten(name='flatte1',inputs=x)\r\n\r\n    x = Dense(256,name='fc1')(x)\r\n\r\n    # max_length of sentence\r\n    x = RepeatVector(7,name='repeat')(x)\r\n\r\n    #2*GRU the output shape is (None,7,256)\r\n    x = Bidirectional(GRU(units=128,name='GRU',return_sequences=True))(x)\r\n\r\n    #apply (None,7,256) to all GRUs output (None,7,16)\r\n    x = TimeDistributed(Dense(16,name='fc2'))(x)\r\n    x = Activation('softmax')(x)\r\n    x = Flatten(name='Flatten2',inputs=x)\r\n\r\nwhen I show my summary, it print like this\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 90, 30, 3)         0         \r\n_________________________________________________________________\r\nconv1 (Conv2D)               (None, 88, 28, 32)        896       \r\n_________________________________________________________________\r\npool1 (MaxPooling2D)         (None, 44, 14, 32)        0         \r\n_________________________________________________________________\r\nbn1 (BatchNormalization)     (None, 44, 14, 32)        128       \r\n_________________________________________________________________\r\nactivation_1 (Activation)    (None, 44, 14, 32)        0         \r\n_________________________________________________________________\r\nconv2 (Conv2D)               (None, 42, 12, 64)        18496     \r\n_________________________________________________________________\r\npool2 (MaxPooling2D)         (None, 21, 6, 64)         0         \r\n_________________________________________________________________\r\nbn2 (BatchNormalization)     (None, 21, 6, 64)         256       \r\n_________________________________________________________________\r\nactivation_2 (Activation)    (None, 21, 6, 64)         0         \r\n_________________________________________________________________\r\nconv3 (Conv2D)               (None, 19, 4, 64)         36928     \r\n_________________________________________________________________\r\npool3 (MaxPooling2D)         (None, 9, 2, 64)          0         \r\n_________________________________________________________________\r\nbn3 (BatchNormalization)     (None, 9, 2, 64)          256       \r\n_________________________________________________________________\r\nactivation_3 (Activation)    (None, 9, 2, 64)          0         \r\n_________________________________________________________________\r\nflatte1 (Flatten)            (None, 1152)              0         \r\n_________________________________________________________________\r\nfc1 (Dense)                  (None, 256)               295168    \r\n_________________________________________________________________\r\nrepeat (RepeatVector)        (None, 7, 256)            0         \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, 7, 256)            295680    \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, 7, 16)             4112      \r\n_________________________________________________________________\r\nFlatten2 (Flatten)           (None, 112)               0         \r\n=================================================================\r\n\r\nbut when I change my codes to tf.keras like this(the architecture do not change! )\r\n_________________________________________________________________\r\nrepeat (RepeatVector)        (None, 7, 256)            0         \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, None, 256)         295680    \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, None, 16)          4112      \r\n_________________________________________________________________\r\nactivation_4 (Activation)    (None, None, 16)          0         \r\n_________________________________________________________________\r\nFlatten2 (Flatten)           (None, None)              0         \r\n=================================================================\r\nTotal params: 651,920\r\nTrainable params: 651,600\r\nNon-trainable params: 320\r\n_________________________________________________________________\r\n\r\n\r\nthe output of bidirectional_1 is (None, None, 256), I just wonder why the GRU output was flase.\r\nAnd how can I change my code?\r\nThanks a lot!\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14241, "title": "`Network` behaviour inconsistent with `Layer` w.r.t `build`", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (provided below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary, `pip install tf-nightly-gpu`\r\n- **TensorFlow version (use command below)**: git: v1.3.0-rc1-4090-g4e75ae1, tf: 1.5.0-dev20171103\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX-1070 8GB\r\n\r\n### Describe the problem\r\nThe `tf.layers.Layer` class exposes a `build` method which can optionally be called prior to first `__call__` e.g. to construct variables in a different scope to where the call is made. The documentation does not explicitly promise that layers are built during `build` and not elsewhere, but this seems to be the pattern with the implemented layers and is very useful in a narrow set of circumstances.\r\n\r\nThe `tensorflow.python.layers.base.Network` class implements `tf.layers.Layer` and exposes the same method, but there is no way to construct the constructor arguments without building constituent layers. This is unexpected and misleading, and prevents use of `Network`s where unbuilt `Layer`s are expected/required.\r\n\r\nA lazily-built `Network` implementation would be greatly appreciated, as would clarity on whether it is the intention that all classes implementing `Layer` should be `build` separately from their constructor. \r\n\r\nIllustrative example/ basic lazy implementation below.\r\n\r\n### Source code\r\n```\r\nfrom tensorflow.python.layers import base\r\nimport tensorflow as tf\r\n\r\nn_features = 16\r\nn_hidden = 8\r\nn_out = 3\r\n\r\n\r\ndef n_vars():\r\n    return len(tf.trainable_variables())\r\n\r\n\r\n# Layers, variable created on build\r\nx = tf.placeholder(shape=(None, n_features), dtype=tf.float32)\r\nl0 = tf.layers.Dense(n_hidden)\r\nl1 = tf.layers.Dense(n_out)\r\nprint(n_vars())  # 0\r\nl0.build((n_features,))\r\nl1.build((n_hidden,))\r\nprint(n_vars())  # 4\r\n\r\n\r\nprint('---------')\r\n# Network, variables must be created prior to Network construction.\r\ntf.reset_default_graph()\r\nx = tf.placeholder(shape=(None, n_features), dtype=tf.float32)\r\n\r\nl0 = tf.layers.Dense(n_hidden)\r\nl1 = tf.layers.Dense(n_out)\r\n\r\ninp = base.Input((n_features,))\r\noutputs = l1(l0(inp))\r\nprint(n_vars())  # 4\r\nnetwork = base.Network(inp, outputs)\r\n\r\nprint(network.built)  # Already true: no way around with constructor signature\r\nnetwork.build(None)\r\nprint(network.built)  # True as expected\r\n\r\n\r\ndef _transform_inputs(inputs):\r\n    if isinstance(inputs, base.InputSpec):\r\n        return inputs\r\n    elif isinstance(inputs, (tuple, tf.TensorShape)):\r\n        return base.Input(shape=inputs[1:], batch_size=inputs[0])\r\n    elif isinstance(inputs, tf.Tensor):\r\n        return base.Input(tensor=inputs)\r\n    elif isinstance(inputs, list):\r\n        return [_transform_inputs(inp) for inp in inputs]\r\n    else:\r\n        raise NotImplementedError(\r\n            'Unrecognized type for _transform_inputs: %s' % type(inputs))\r\n\r\n\r\nclass LazyNetwork(base.Network):\r\n    def __init__(self, outputs_fn, **kwargs):\r\n        self._outputs_fn = outputs_fn\r\n        self._kwargs = kwargs\r\n        self.built = False\r\n\r\n    def build(self, inputs):\r\n        if self.built:\r\n            return\r\n        inputs = _transform_inputs(inputs)\r\n        outputs = self._outputs_fn(inputs)\r\n        super(LazyNetwork, self).__init__(\r\n            inputs, outputs, **self._kwargs)\r\n\r\n    def __call__(self, inputs):\r\n        if not self.built:\r\n            self.build(_transform_inputs(inputs))\r\n        return super(LazyNetwork, self).__call__(inputs)\r\n\r\n\r\nprint('---------')\r\n# LazyNetwork, variables created on build\r\ntf.reset_default_graph()\r\nx = tf.placeholder(shape=(None, n_features), dtype=tf.float32)\r\n\r\nl0 = tf.layers.Dense(n_hidden)\r\nl1 = tf.layers.Dense(n_out)\r\nnetwork = LazyNetwork(lambda x: l1(l0(x)))\r\n\r\nprint(n_vars())  # 0\r\nprint(network.built)  # False\r\nnetwork.build(base.Input((n_features,)))\r\nprint(n_vars())  # 4\r\nprint(network.built)  # True\r\n```", "comments": ["Note that `Network` is not part of the supported public API (as per our [versioning guarantees](https://www.tensorflow.org/programmers_guide/version_compat#what_is_covered), since the symbol is not directly accessible via the `tensorflow` module and isn't documented on the website, it isn't supported).\r\n\r\nHowever, it is something that is being worked on.\r\n@fchollet might be able to comment on the design choices here.", "For the time being, `Network` can only be built from a set of already built layers. Thus its `build` method does nothing (the network is always already built). Also, it is not yet part of the public API. If you want to implement something custom, I recommend subclassing `Layer` instead (they have the same API anyway).\r\n\r\nIn addition, `__call__` is never a method you should override (unless you know what you are doing, and call the parent's `__call__` in your implementation).  Custom layers should just implement `__init__`, `build`, and `call`.\r\n\r\n"]}, {"number": 14240, "title": "Remove duplicated com_google_absl workspace object (#14238)", "body": "", "comments": []}, {"number": 14239, "title": "Disable flaky prefetching_ops_test", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 14238, "title": "Remove duplicated com_google_absl workspace object", "body": "", "comments": ["Both failures are flakes, if the remaining two builds return healthy we can merge the PR."]}, {"number": 14237, "title": "Added an option to `kernel` to `skip_metropolis_step`.", "body": "Fix #14221 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@jvdillon Could you point me out where are the tests for this?", "HMC tests live in:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/bayesflow/python/kernel_tests/hmc_test.py", "@jvdillon Do I have to do something for the Test suit to execute?", "Sorry, could you resolve the conflicts? The test ones is easy, but I don't want to mess with the _impl file.", "@martinwicke I've resolved the conflicts.", "Jenkins, test this please.", "I can't say that I understand what the failure on the CPU Test Python 3 is. Could someone take a look and elaborate on it?", "Infra failure. Nothing for you to worry about.", "The Contrib test does look like it could be related though: hmc_test failing?", "I agree there is some chance, but all other tests pass... also I don't see any more comprehensive report on which part of the test has failed. Could you rerun it?", "Here is the test log for the failed test https://source.cloud.google.com/results/invocations/d109530b-8561-434d-8315-6f4639cf70de/targets/%2F%2Ftensorflow%2Fcontrib%2Fbayesflow:hmc_test/log. @botev could you take a look?", "I think the link should actually be https://source.cloud.google.com/results/invocations/d109530b-8561-434d-8315-6f4639cf70de/targets/%2F%2Ftensorflow%2Fcontrib%2Fbayesflow:hmc_test?page=log\r\n\r\n/log doesn't work for me, maybe it relies on a referrer.", "Im seeing \"test log not found\".", "Could you try click on one of the blue `Details` link, and then clicked the failing target bayesflow:hmc_test, then TARGET LOG tab?", "(Looks like the accept_probs are quite different from the skip_accept_probs, although Im not sure why.)", "Ah you are right, since the `hmc.kernel` samples the initial velocity vectors inside, it means it is stochastic on each call. Does any of you know of any way to synchronize somehow the samples of `random_ops.random_normal(x_shape, dtype=x.dtype)` in two consecutive calls?", "@jvdillon I've used `set_random_seed` in order to go around this. Does that seem reasonable? \r\nAlso, need to see the results of the tests.", "The current impl has a design flaw; every function should take a seed so that way seed can be propagated all the way down to `kernel`. Unfortunately you'll have to make this change to get your cl in.\r\n\r\n(Side-note: we're actually revamping HMC and our Metropolis Hastings code based on user feedback. Having said that, Id like to get you the feature you need ASAP and so we're still supporting this version.)", "@jvdillon Could you run jenkins? I think my tests should now pass.\r\n\r\nOn a related note for improvements, you might want to have a distribution argument for the velocities rather than to always use a fixed Gaussian. \r\nEven, further following some of the new advances, like https://openreview.net/forum?id=B1n8LexRZ, you can add transformations in as well. ", "@martinwicke: can you rerun tests for @botev.\r\n\r\n@botev: thank you for this feedback! ill make sure to accommodate this in the new design. (Ive shared the new design with you as well.)", "@tensorflow-jenkins test this please", "Ah, apparently this does not work, meaning I need to set the op seed, not the graph one.... should I just drop this test? Currently, the test is impossible to do due to:\r\n```\r\nm = random_ops.random_normal(x_shape, dtype=x.dtype)\r\n```\r\nin `kernel`.", "@jvdillon WDYT?", "Josh is on leave for a while; so this may stall for now.  @langmore can you recommend someone to finish review?", "I can finish it next week.  I'm out until then.", "@botev can you address @langmore's comments?", "Actually, internally we are getting closer to a new version since Josh came back earlier than expected.  @jvdillon , would you say that @botev is better off not bothering with these changes in this version?", "I'm a bit busy until next week, so won't do it until then. If by any chance indeed there is no point bothering let me know. ", "Yes @botev , Josh has confirmed that the new version will be in soon enough that you need not worry.  I'll try to update this thread when the new version is in.", "One thing when you design that is to mention currently there is an inconsistency between `leapfrog_step` and `kernel`. Specifically one accepts the log_density and its gradient, while the other accepts the \"potential\" which is the negative log_density. I think it would be better both to accept the same thing.", "@jvdillon  Given that this issue #5802 is still open, is there any way currently to run anything in the bayesflow module with:\r\n\r\n1. The log-joint has more than one variable (currently it seems that `leapfrog_step` assumes that the inputs cannot be a list)\r\n2. Don't have access to a function mapping x -> log p(x), but rather we have x and the graph node representing log p(x)? E.g. are there any workarounds for constructing a function from the graph? If so where can I find examples?\r\n\r\nAtm given that I just found out about these it's making my life hell, as we want to test HMC on a small network... ", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@jvdillon @langmore what should happen here?", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We're just about wrapping up a major refactoring of HMC (and MCMC) code.  The new version will have an UncalibratedHMC (which has no MetropolisHastings adjustment) as well as what is currently HMC.\r\n\r\nAlso, we're moving MCMC code to: github.com/tensorflow/probability"]}]