[{"number": 4401, "title": "Broken blog link on tensroflow.org dated 26 Aug 2016", "body": "https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html.html\n\nShould be \n\nhttps://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html\n", "comments": ["Fixed.\n"]}, {"number": 4400, "title": "Update bazel version in devel dockerfiles.", "body": "This should also fix nightly docker breakages.\n", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @caisq to be potential reviewers\n", "Jenkins, test this please\n", "should we ignore the mac build failure?\n", "Test machine ran out of disk in the first run.\nI cleaned up the disk issue on that machine.\nRerunning tests now.\n"]}, {"number": 4399, "title": "Android build issue: permission denied", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nNone relating to permission denied issue\n### Environment info\n\nOperating System: Linux Ubuntu 64-bit \n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nandroid_sdk_repository(\n    name = \"androidsdk\",\n    api_level = 23,\n    build_tools_version = \"24.0.2\",\n    # Replace with path to Android SDK on your system\n    path = \"/home/flavia/Documents/android-sdk-linux\",\n)\n# \n\nandroid_ndk_repository(\n    name=\"androidndk\",\n    path=\"/home/flavia/Documents/ndk\",\n    api_level=21)\n### What other attempted solutions have you tried?\n\nDownloaded NDK version: android-ndk-r12b, unzipped it and renamed it to ndk and pointed to it in workspace\nDownloade SDK: 24.4.1, run SDK manager to install build-tools, pointed to the main sdk directory\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nERROR: /home/flavia/.cache/bazel/_bazel_flavia/1fe576225e857ae381761d2049465367/external/protobuf/BUILD:71:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: arm-linux-androideabi-gcc failed: error executing command \n  (cd /home/flavia/.cache/bazel/_bazel_flavia/1fe576225e857ae381761d2049465367/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.d '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.o' -iquote external/protobuf -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/protobuf/src/google/protobuf/repeated_field.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nprocess-wrapper: execvp(\"external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc\", ...): Permission denied\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 1.330s, Critical Path: 0.43s\n\nBasically, i've failed to decipher the permission denied. I'm no expert at android, but i'm experimenting with tensorflow.\n", "comments": ["hi @fninsiima,\n\nFirst, can you verify that `/home/flavia/Documents/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc` exists on your system and is executable?\n\nAs a potential quick fix, I would suggest that you try downloading/extracting r11c from here: https://dl.google.com/android/repository/android-ndk-r11c-linux-x86_64.zip, as occasionally Bazel will exhibit incompatibilities with the latest NDK.\n", "Also, did you clone with `--recurse-submodules`, and can you verify that `<git_root>/tensorflow/bazel-tensorflow/external/protobuf` is not empty?\n", "Bazel protobuf is not empty. Ive listed the output here\n\nflavia@fninsiima-ThinkPad-T440p:~/Documents/tensorflow/bazel-tensorflow/external/protobuf$ ls\nappveyor.bat      generate_descriptor_proto.sh  protobuf-lite.pc.in\nappveyor.yml      gmock.BUILD                   protobuf.pc.in\nautogen.sh        java                          Protobuf.podspec\nbenchmarks        javanano                      protoc-artifacts\nBUILD             jenkins                       python\nCHANGES.txt       js                            README.md\ncmake             LICENSE                       ruby\nconfigure.ac      m4                            six.BUILD\nconformance       Makefile.am                   src\nCONTRIBUTORS.txt  more_tests                    tests.sh\ncsharp            objectivec                    update_file_lists.sh\ndocs              php                           util\neditors           post_process_dist.sh          WORKSPACE\nexamples          protobuf.bzl\n\nYes, ive verified that arm-linux-androideabi-gcc  exists on my system.\nBut I'm not sure how to verify that its executable...\n\n## On opening it(double clicking), it says \n\n## 'There is no application installed for \u201cexecutable\u201d files.'\n\n## On doing so in terminal, it says\n\nThe program 'arm-linux-androideabi-gcc' is currently not installed. You can install it by typing:\n\n## sudo apt-get install gcc-arm-linux-androideabi\n\nNo, i do not recall cloning with --recurse-submodules.\n", "No, i have yet to try out the quick fix of ndk r11\n\nsorry for the bold. dint know typing hyphens would do that!\n", "```\nYes, ive verified that arm-linux-androideabi-gcc exists on my system.\nBut I'm not sure how to verify that its executable...\n```\n\nYou would just run `ls -la /home/flavia/Documents/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc` and make sure you have read/execute permissions, or simply try executing it directly from the command line.\n", "Okay.\n\nI followed your instructions.\nThen later on i run into an issue. something to do with \n\narm-linux-androideabi-gcc: error trying to exec 'cc1': execvp: No such file...\n\nThen i googled a bit. and found a solution here...\n\nhttp://stackoverflow.com/questions/10207280/error-compiling-android-on-ubuntu-11-10\n\nIn short, i did not extract my ndk with the right tool. so i did not have executable permissions .\n\nSo i re-did the extraction using unzip.\n\nVoila! Problems solved with permissions.\n\nSUCCESSFUL BUILD ACCOMPLISHED!\n\nHope you don't mind if i close this issue.\n\nThanks very much Andrew. God bless.\n"]}, {"number": 4398, "title": "Update README.md to refer to 0.10.0 nightly images.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @caisq, @tensorflower-gardener and @keveman to be potential reviewers\n"]}, {"number": 4397, "title": "Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib)", "body": "**Summary**: Why is it looking for the header file under the lib directory?\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n- https://github.com/tensorflow/tensorflow/issues/3989 (and references within)\n### Environment info\n\nOperating System: Ubuntu 16.04. Docker 1.12.1.\n### If possible, provide a minimal reproducible example\n1. Modify [devel-gpu](https://github.com/tensorflow/tensorflow/blob/4addf4b5806cd731949c6582a83f5824599cd1ef/tensorflow/tools/docker/Dockerfile.devel-gpu) to read \"`FROM nvidia/cuda:8.0-cudnn5-devel`\".\n2. Run `docker build -f Dockerfile.devel-gpu -t tf` from the `/tensorflow/tools/docker` directory. (HEAD @ 4addf4b5806cd731949c6582a83f5824599cd1ef at time of posting)\n### What other attempted solutions have you tried?\n\nNone yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.\n### Logs or other output that would be helpful\n\n```\n$ docker build -f Dockerfile.devel-gpu -t tf .\n\n[snip]\n\nStep 23 : RUN ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl\n ---> Running in 1f99527c7748\nNo Google Cloud Platform support will be enabled for TensorFlow\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\n____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.\nConfiguration finished\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/lib/x86_64-linux-gnu.\n____Elapsed time: 0.391s\nThe command '/bin/sh -c ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1\n```\n", "comments": ["I pieced together that I could make it work by modifying `CUDNN_INSTALL_PATH` to `/usr/local/cuda` in the Dockerfile.\n", "The error message is confusing, because it looks like it almost finds the file since it looks in `/usr/local/cuda-8.0/targets/x86_64-linux/lib` but the header file is in fact in `/usr/local/cuda-8.0/targets/x86_64-linux/include`.\n", "This is expected.\nCuda8 still does not have APT packages we can install.\nAPT packages deploy all cuda libraries/headers to different locations than nvidia's installer.\nThus our cuda7.5 images wont work out of the box with cuda 8 images.\n\n@flx42, could you verify the above information?\n", "Correct, we don't have deb packages of cuDNN v5 on CUDA 8.0 since CUDA 8.0 is not final yet. But, as I said [here](https://github.com/tensorflow/tensorflow/issues/3989#issuecomment-241933622), ideally the configure script should look in both `/usr/local/cuda` and `/usr`.\n", "@davidzchen @martinwicke \n", "Would this be as simple as including both possible directories here (and lines below and above, all the environment variables we use to point to cuda library locations)\nhttps://github.com/tensorflow/tensorflow/blob/4addf4b5806cd731949c6582a83f5824599cd1ef/tensorflow/tools/docker/Dockerfile.devel-gpu#L91\n", "@gunan well, there is already a `CUDNN_INSTALL_PATH` line here, simply removing it also work.\nIdeally, the Dockerfile should not have to specify `CUDNN_INSTALL_PATH`, `CUDA_PATH` or `CUDA_TOOLKIT_PATH`, if the configure step use sensible default values when looking for CUDA/cuDNN. Those variables should only be needed when you have a truly unusual CUDA install path like `/home/felix/cuda`.\n\nAlso, the `LD_LIBRARY_PATH` line you mentioned is bogus, see #4363\n", "Just tried removing those, and but it looks like we need the path:\n/usr/local/cuda/extras/CUPTI/lib64\nAs without it my tests are failing with:\n tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcupti.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nF tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: /workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks)could not find cuptiActivityRegisterCallbacksin libcupti DSO\n\nShould we fix the dso_loader.cc instead?\n", "Don't remove the CUPTI path, just don't erase the previous value of `LD_LIBRARY_PATH` ;)\n\nMy fix:\n\n```\n-ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64                                                                                                                                                                                                  \n+ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH  \n```\n", "@flx42 This is also fixed, right?\n", "@gunan: yes, I think it is.\n", "My minimum reproduce (running docker build) still fails when I run on the master branch.\n\n<details>\n\n```\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 506\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 435, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 291, in _find_cudnn_header_dir\n        auto_configure_fail(\"Cannot find cudnn.h under %s\" %...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 93, in auto_configure_fail\n        fail(\"\n%sAuto-Configuration Error:%s ...))\n\nAuto-Configuration Error: Cannot find cudnn.h under /usr/lib/x86_64-linux-gnu\n```\n\n</details>\n", "This patch makes it work (updating the `CUDNN_INSTALL_PATH` to `/usr/local/cuda` as before).\n\n<details>\n\n``` diff\n--- a/tensorflow/tools/docker/Dockerfile.devel-gpu\n+++ b/tensorflow/tools/docker/Dockerfile.devel-gpu\n@@ -89,7 +89,7 @@ WORKDIR /tensorflow\n\n # Configure the build for our CUDA configuration.\n ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n-ENV CUDNN_INSTALL_PATH /usr/lib/x86_64-linux-gnu\n+ENV CUDNN_INSTALL_PATH /usr/local/cuda\n ENV TF_NEED_CUDA 1\n ENV TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2\n\n@@ -108,4 +108,13 @@ EXPOSE 6006\n```\n\n</details>\n", "@pwaller: you are not using the right Dockerfile.\nOn master, there is no more `CUDNN_INSTALL_PATH`.\n", "@flx42, I mis-spoke. I have to re-add it in order to make it work. If it is absent, I get the above error.\n", "Sorry, one moment while I understand what has happened, clearly this is at odds with the diff I posted.\n", "I had a dirty worktree from trying a few things. Here is the correct diff (against master) which makes it work for me:\n\n``` diff\n--- a/tensorflow/tools/docker/Dockerfile.devel-gpu\n+++ b/tensorflow/tools/docker/Dockerfile.devel-gpu\n@@ -89,6 +89,7 @@ WORKDIR /tensorflow\n\n # Configure the build for our CUDA configuration.\n ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n+ENV CUDNN_INSTALL_PATH /usr/local/cuda\n ENV TF_NEED_CUDA 1\n ENV TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2\n\n@@ -107,4 +108,13 @@ EXPOSE 6006\n```\n", "I am sorry, however I cannot reproduce the problem you are seeing.\nHere is what I did:\n\n$ git clone git@github.com:tensorflow/tensorflow\n$ cd tensorflow\n$ nvidia-docker build  -f tensorflow/tools/docker/Dockerfile.devel-gpu -t tf tensorflow/tools/docker\n\nOutcome:\nSuccessfully built c8163ce55259\n\nThen:\n$ nvidia-docker run -it c8163ce55259 bash\n$ python -c \"import tensorflow as tf; print tf.test.is_gpu_available()\"\n....\nTrue\n\nMaybe you are not using nvidia docker?\nAs I cannot reproduce the problem, I am inclined to leave the issue as closed.\n", "@gunan: same for me, it works. \nAlso, `nvidia-docker` is actually passthrough to `docker` when doing a `build`. Using nvidia-docker is only important for `run`.\n", "In the minimal reproduce in the original post, it says:\n\n> 1. Modify devel-gpu to read \"FROM nvidia/cuda:8.0-cudnn5-devel\".\n\nDid you guys do that?\n", "Master is already changed to use that image, you do not need to edit\nanymore.\n\nOn Oct 21, 2016 2:28 AM, \"Peter Waller\" notifications@github.com wrote:\n\n> In the minimal reproduce in the original post, it says:\n> 1. Modify devel-gpu to read \"FROM nvidia/cuda:8.0-cudnn5-devel\".\n> \n> Did you guys do that?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4397#issuecomment-255335847,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOdcvwDP5GghIxU7iHq-l7Usb0Y3iks5q2IW2gaJpZM4J-C-T\n> .\n", "A key part of the minimal reproduce example is to modify the dockerfile to be based off cudnn8. Did you spot that? Apols for terse phone reply.\n\nOn 20 October 2016 23:16:03 BST, gunan notifications@github.com wrote:\n\n> I am sorry, however I cannot reproduce the problem you are seeing.\n> Here is what I did:\n> \n> $ git clone git@github.com:tensorflow/tensorflow\n> $ cd tensorflow\n> $ nvidia-docker build  -f tensorflow/tools/docker/Dockerfile.devel-gpu\n> -t tf tensorflow/tools/docker\n> \n> Outcome:\n> Successfully built c8163ce55259\n> \n> Then:\n> $ nvidia-docker run -it c8163ce55259 bash\n> $ python -c \"import tensorflow as tf; print tf.test.is_gpu_available()\"\n> ....\n> True\n> \n> Maybe you are not using nvidia docker?\n> As I cannot reproduce the problem, I am inclined to leave the issue as\n> closed.\n> \n> ## \n> \n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub:\n> https://github.com/tensorflow/tensorflow/issues/4397#issuecomment-255243145\n\n## \n\nSent from my Android device with K-9 Mail. Please excuse my brevity.\n", "All tensorflow dockerfiles should be upgraded to use nvidia/cuda:8.0-cudnn5-devel.\nAnd our CI is able to build fine with all of these dockerfiles.\nMaybe your repository is outdated?\nSince I cannot reproduce the problem, I am sorry but I have no idea how to help.\n", "Thanks very much @gunan et al for taking the time to try and reproduce and reply.\n\nThe r0.11 branch now works, where it was not before. (Even though I had an uptodate repository and an uptodate docker base image pulled before, but doing it this time it worked).\n\nAs an aside, the master branch @ 446b1cb fails with the below error message. It's not stopping me from getting anything working though because r0.11 appears to be building fine.\n\n```\nStep 20 : RUN tensorflow/tools/ci_build/builds/configured GPU     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl &&     rm -rf /tmp/pip &&     rm -rf /root/.cache\n ---> Running in fe048e7cdc07\n/tensorflow /tensorflow\n/tensorflow /tensorflow\nCan't find swig.  Ensure swig is in $PATH or set $SWIG_PATH.\nThe command '/bin/sh -c tensorflow/tools/ci_build/builds/configured GPU     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl &&     rm -rf /tmp/pip &&     rm -rf /root/.cache' returned a non-zero code: 1\n```\n", "@jart I think you were working on a fix for the above issue, right?\nOr am I confusing this with another change?\n", "I commented on a CL internally regarding this. I'm not sure why that error message is being displayed in the docker environment. The \"Can't find swig\" error message was in our configure file and it was deleted by my SWIG change. Somehow that old code is lingering around, and failing, because we no longer apt-get swig in the Dockerfile.\n", "@jart: note that the dockerfile checks out the r0.11 branch, rather than running the repository at the revision you have checked out, so this could be some weird interaction there. When I do a docker build with `r0.11` as the repository-outside-of-docker, it's fine, but not with 446b1cb, even though inside docker it should be building exactly the same thing (r0.11).\n\nhttps://github.com/tensorflow/tensorflow/blob/7de3419cd7504ebd94eaad273f9919eff334ece7/tensorflow/tools/docker/Dockerfile.devel-gpu#L86\n", "Ah, that is the problem.\nWe have recently changed swig to be built by bazel rather than using it as a system dependency.\nr0.11 needs it as a system dependency, but master branch does not download it anymore. That is the root cause of that error message.\nLooks like @jart already found the problem and sent a cherrypick request to r0.11\n", "Can you link to that PR? I am still seeing this issue.\n"]}, {"number": 4396, "title": "Problem in using docker tensorflow installation for GPU", "body": "Operating System: Ubuntu 16.04\nI installed the gpu-docker tensorflow using these commands:\n\n```\nwget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.0-rc.3/nvidia-docker_1.0.0.rc.3-1_amd64.deb\nsudo dpkg -i /tmp/nvidia-docker*.deb && rm /tmp/nvidia-docker*.deb\nnvidia-docker run --rm nvidia/cuda nvidia-smi\n```\n\nNow, I have cuda installed but when I try to run code with gpu support, I get this:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:1562] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 25e49b6d892e\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  370.28  Thu Sep  1 19:45:04 PDT 2016\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 370.28\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1051] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n```\n\nIt cant open libcuda even though I installed it. Also, how do I install cuDNN here?\noutput of `ldconfig -p | grep cuda`:\n\n```\n    libnvrtc.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc.so.7.5\n    libnvrtc.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc.so\n    libnvrtc-builtins.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc-builtins.so.7.5\n    libnvrtc-builtins.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvrtc-builtins.so\n    libnvblas.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnvblas.so.7.5\n    libnvblas.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvblas.so\n    libnvToolsExt.so.1 (libc6,x86-64) => /usr/local/cuda/lib64/libnvToolsExt.so.1\n    libnvToolsExt.so (libc6,x86-64) => /usr/local/cuda/lib64/libnvToolsExt.so\n    libnpps.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnpps.so.7.5\n    libnpps.so (libc6,x86-64) => /usr/local/cuda/lib64/libnpps.so\n    libnppi.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnppi.so.7.5\n    libnppi.so (libc6,x86-64) => /usr/local/cuda/lib64/libnppi.so\n    libnppc.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libnppc.so.7.5\n    libnppc.so (libc6,x86-64) => /usr/local/cuda/lib64/libnppc.so\n    libcusparse.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcusparse.so.7.5\n    libcusparse.so (libc6,x86-64) => /usr/local/cuda/lib64/libcusparse.so\n    libcusolver.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcusolver.so.7.5\n    libcusolver.so (libc6,x86-64) => /usr/local/cuda/lib64/libcusolver.so\n    libcurand.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcurand.so.7.5\n    libcurand.so (libc6,x86-64) => /usr/local/cuda/lib64/libcurand.so\n    libcufftw.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcufftw.so.7.5\n    libcufftw.so (libc6,x86-64) => /usr/local/cuda/lib64/libcufftw.so\n    libcufft.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcufft.so.7.5\n    libcufft.so (libc6,x86-64) => /usr/local/cuda/lib64/libcufft.so\n    libcudart.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcudart.so.7.5\n    libcudart.so (libc6,x86-64) => /usr/local/cuda/lib64/libcudart.so\n    libcublas.so.7.5 (libc6,x86-64) => /usr/local/cuda/lib64/libcublas.so.7.5\n    libcublas.so (libc6,x86-64) => /usr/local/cuda/lib64/libcublas.so\n```\n", "comments": ["Tensorflow uses `LD_LIBRARY_PATH` to locate the CUDA lib files. In your environment, the paths in that variable do not contain `libcuda.so` nor cuDNN files, that's why tensorflow can't open/find those files.\n\nI think the problem is with the nvidia-docker installation, make sure you installed it correctly for tensorflow, perhaps [this doc](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#docker-installation) may be helpful for you.\n\nTo answer your question on how to install cuDNN with nvidia-docker, they provide tags with CUDA + cuDNN.\nI think nvidia-docker provides a symlink to `libcuda.so`.\n", "How can I make sure that my nvidia-docker is installed correctly? I reran the whole process (installing the .deb files). I do have cuda installed in my environment. Thing is, I have `CUDA 8.0` installed from nvidia's repo and `CUDA 7.5` using `sudo apt-get install nvidia-cuda-dev nvidia-cuda-toolkit`.\n(In my env : Not docker) `ldconfig -p | grep cud` \n\n```\n    libicudata.so.55 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libicudata.so.55\n    libicudata.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libicudata.so\n    libcudnn.so.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so.5\n    libcudnn.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so\n    libcudart.so.7.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so.7.5\n    libcudart.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so\n    libcuda.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so.1\n    libcuda.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so\n```\n", "You want tensorflow to find those files? then you'll need to export `/usr/lib/x86_64-linux-gnu/` from your env, into `LD_LIBRARY_PATH` in your docker, which is the variable tensorflow uses for locating cuda libs.\nYou can create symlinks and export those if it's more comfortable to you.\n\nI download CUDA and cuDNN from NVIDIA's site as runfiles (deb is pretty much broken right now) and install with that, so I'm not familiar with your installing convention, hopefully I'm being helpful.\n", "I had already set the env variables. Tried that again, but it didn't help. I figured it might be that cuda 8.0 might not be supported by the docker image. So I have installed cuda 7.5 on my system and it still gives the same errors.\n", "My understanding of the problem is that you are trying to run tensorflow with GPU support while your `LD_LIBRARY_PATH` does not point to paths that contain `libcuda.so` nor cuDNN files.\n\nTherefore, the issue is not directly with tensorflow, but with the CUDA and cuDNN installation.\nTo solve this, you need to install CUDA and cuDNN such that the variable `LD_LIBRARY_PATH` in the environment you're trying to run tensorflow will point to paths that contain all required CUDA and cuDNN files.\n"]}, {"number": 4395, "title": "proper message for new contributor", "body": "There is a space left between \"howto\"\nAlso, it will be better if you write as \"getting started\"\n", "comments": ["@aj07, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke and @keveman to be potential reviewers\n", "Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 4394, "title": "Proper mesage for new contributor", "body": "On page CONTRIBUTING.md there is a link for new contributor.\nThere is a need of space between how and to which i missing. But, it will look good if we change the message to getting started.\n", "comments": ["If I am interpreting your issue correctly, I assume you mean the [howto]. Howto is actually now a word!\nhttps://www.google.com/search?q=howto%20definition\nIt's pretty similar to how electronic mail became email.\n"]}, {"number": 4393, "title": "AttributeError: 'GFile' object has no attribute 'Size'", "body": "Getting above error when running mnist_softmax.py\n\n(tensorflow) :~/softwares/tensorflow/tensorflow/examples/tutorials/mnist$  python mnist_softmax.py \n\nTemporary work around would be to do following edit in the \"tensorflow/tensorflow/contrib/learn/python/learn/datasets/base.py\"\n\nReplace  Line # 160    '    size = f.Size()  '        with       '   size = f.size()  '\n\nAs 'tensorflow/tensorflow/python/lib/io/file_io.py' has the function define for **size** not **Size**.\n\n  def size(self):\n    \"\"\"Returns the size of the file.\"\"\"\n    return stat(self.__name).length\n\nFailure is seen on running the tensorflow for the first time or cleaning up /tmp/data\n", "comments": ["Thanks for reporting, this but it looks like its already been fixed:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/base.py\n", "The same error occured on tensorflow r0.10 with `/models/image/mnist/convolutional.py`. Changing `f.Size()` to `f.size()` on line 72 solved it for me.", "I can confirm that f.size() works, where f.Size() was throwing this error.", "yep Chage `f.Size()` to` f.size()` then it works. "]}, {"number": 4392, "title": "LMDB Reading Feature", "body": "I would like to see a native feature where LMDB files are read by tensorflow inside the graph. I know it can be done with placeholders but that's suboptimal.\n\nIn general, there are three ways of approaching this in TF.\n(1) Native Implementation / Custom Data Reader\n[https://www.tensorflow.org/versions/r0.10/how_tos/new_data_formats/index.html](https://www.tensorflow.org/versions/r0.10/how_tos/new_data_formats/index.html)\n(2) Python Function Wrapping\n[https://www.tensorflow.org/versions/r0.9/api_docs/python/script_ops.html](https://www.tensorflow.org/versions/r0.9/api_docs/python/script_ops.html)\n(3) Placeholders\nThe functionality is neat but I require better performance..\n\nIf I were to do this myself, what are from a performance standpoint be the main differences between (1) and (2). In any case, it would be great if (1) ended up in the master branch.\n", "comments": ["thanks for posting. I think that it would be best to post your questions about relative performance differences between options (1) and (2) to stackoverflow first (tag 'tensorflow').\n", "Thanks Andy.\nBut the feature request is 'LMDB reading feature', so the question can be considered an extra. Can you open this feature request up again, or are you excluding this from ever being implemented? That would make me sad.\nI did open the stackoverflow question: http://stackoverflow.com/questions/39525951/custom-data-reader-lmdb-in-tensorflow\n", "My thinking was that you could first ask the question portion of this issue on StackOverflow (sometimes people may have a solution for you). If nothing useful turns up there, then feel free to enter another feature request (or we can reopen this one).\n", "An open PR implementing the requested feature.\r\nhttps://github.com/tensorflow/tensorflow/pull/9950"]}, {"number": 4391, "title": "bazel build error ", "body": "Ubuntu16.04 Cuda8.0 cudnn4 gcc-5.4 g++5.4 tensorflow0.9.0,\ni experienced the issue blow when using bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  and there also is a warning.\nwhat should i do to correct this? i am new to tensorflow .  Need Help\nWARNING: /home/keithyin/workspace/tensorflow-0.9.0/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\n\nERROR: /home/keithyin/workspace/tensorflow-0.9.0/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 100 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::DoBlasGemm(perftools::gputools::Stream_, perftools::gputools::blas::Transpose, perftools::gputools::blas::Transpose, tensorflow::uint64, tensorflow::uint64, tensorflow::uint64, float, const perftools::gputools::DeviceMemoryEigen::half&, int, const perftools::gputools::DeviceMemoryEigen::half&, int, float, perftools::gputools::DeviceMemoryEigen::half_, int)':\ntensorflow/stream_executor/cuda/cuda_blas.cc:1683:22: error: 'CUBLAS_DATA_HALF' was not declared in this scope\n       CUDAMemory(a), CUBLAS_DATA_HALF, lda,\n                      ^\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasOperation_t perftools::gputools::cuda::{anonymous}::CUDABlasTranspose(perftools::gputools::blas::Transpose)':\ntensorflow/stream_executor/cuda/cuda_blas.cc:406:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasFillMode_t perftools::gputools::cuda::{anonymous}::CUDABlasUpperLower(perftools::gputools::blas::UpperLower)':\ntensorflow/stream_executor/cuda/cuda_blas.cc:417:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasDiagType_t perftools::gputools::cuda::{anonymous}::CUDABlasDiagonal(perftools::gputools::blas::Diagonal)':\ntensorflow/stream_executor/cuda/cuda_blas.cc:428:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasSideMode_t perftools::gputools::cuda::{anonymous}::CUDABlasSide(perftools::gputools::blas::Side)':\ntensorflow/stream_executor/cuda/cuda_blas.cc:439:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 5928.835s, Critical Path: 282.48s\n", "comments": ["this is my bazel version .  i dont know what is wrong with it\n\nkeithyin@keithyin-PC:~$ bazel version\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n", "I am also having the same issue.\n", "Is anyone working on this? Please help me to resolve this. @jart \n", "Hi @asispatra. Out of curiosity, how did you build Bazel? I'm not seeing the version in the output above. Could you try installing 0.3.1?\n", "CUDA 8.0 isn't officially supported yet. Please see #2559. We're working on it now.\n", "Please have a look at this set of changes. https://github.com/tensorflow/tensorflow/pull/2556/files. This fixes the problem.\n"]}, {"number": 4390, "title": "Set operation name in SyncReplicasOptimizer.apply_gradients.", "body": "The `name` in `SyncReplicasOptimizer.apply_gradients` and ( `SyncReplicasOptimizer.minimize` ) was not being used for some reason.\n", "comments": ["@daeyun, thanks for your PR! By analyzing the annotation information on this pull request, we identified @lukaszkaiser, @jmchen-g and @ilblackdragon to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins Test this please.\n", "Jenkins, test this please!\n"]}, {"number": 4389, "title": "correct batch_norm docstring", "body": "In response to #4361 \n", "comments": ["Can one of the admins verify this patch?\n", "@thuyen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @sguada, @tensorflower-gardener and @xodus7 to be potential reviewers\n", "@tensorflow-jenkins Test this please.\n"]}, {"number": 4388, "title": "Branch 133202022", "body": "Pushing internal changes.\n", "comments": ["@tensorflow-jenkins Test this please.\n"]}, {"number": 4387, "title": "Significant performance overhead with gpu kernel launches in a while_loop", "body": "### Environment info\n\nOperating System:\nubuntu 14.04\nGeForce GTX TITAN  Driver Version: 367.44\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 Feb 23  2016 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 Feb 23  2016 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 Feb 23  2016 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Feb 23  2016 /usr/local/cuda/lib64/libcudnn_static.a\n1. A link to the pip package you installed:\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\nstart = tf.constant(0, dtype=tf.float64)\nend = tf.constant(100, dtype=tf.float64)\ninit = tf.ones([100], dtype=tf.float64)\nb = tf.ones([100], dtype=tf.float64)\n\ndef condForWhile(i, a):\n    return tf.less(i, end)\n\ndef bodyForWhile(i, a):\n    i = i + 1\n    a = tf.mul(a, b)\n    return [i, a]\n\n_, prod = tf.while_loop(condForWhile, bodyForWhile, [start, init])\n\nwith tf.Session() as sess:\n    tf.initialize_all_variables().run()\n    # run once to eliminate start-up costs\n    sess.run(prod)\n    # now run with tracing\n    run_metadata = tf.RunMetadata()\n    result = sess.run(prod,\n        options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n        run_metadata=run_metadata)\n\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\ntrace_file = open('/tmp/timeline.tfwhile.json', 'w')\ntrace_file.write(trace.generate_chrome_trace_format(show_dataflow=False, show_memory=False))\n\nThis is a simple app that does a = a \\* b 100 times in a while_loop using the tf.mul() operator. I am using the timeline feature to profile it. Inspecting the timeline shows that for each iteration, the time in the gpu stream for the multiply is about .003ms, but the time between launches is .3ms-.5ms. This seems like a lot - is that expected? \n\nNote that my while condition uses floats in the less function. I see the less operation happening on the gpu stream and a memcpyDtoH time for each loop iteration presumably to copy the loop counter back and forth to the gpu, which seems kind of inefficient. If instead I use  type int64 in the loop conditions (start and end), then the gap between kernels goes down to .11 - .17ms and I no longer see the less on the gpu stream or the memcpy. \n\nNote that in the original case where we discovered this, our gpu operator was much more complicated (and uses floats in the condition) and the time between kernel launches was still relatively constant at around .4ms. In contrast, if I write this same app in straight cuda, and launch the kernel 100 times in cuda, the gap between launches is about .007ms (about 50x improvement). I would expect some overhead in the tensorflow conditional operators, but this really seems like a lot. \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nAttaching the timeline traces for the above code for both floats and ints in the conditional. \n\n[timelines.zip](https://github.com/tensorflow/tensorflow/files/473495/timelines.zip)\n", "comments": ["@yuanbyu can probably clarify- but it does seem like you are executing the loop condition on the GPU. This is always going to be very inefficient since the host needs to know the output of the condition before it can dispatch the loop body for the next iteration.\n\nBy making the condition function use integers you are effectively causing it to be executed on the CPU - there is a special case for integer ops assigned to /gpu:0 to compute on tensors kept in host memory.  This avoids the memcpyD2H and having to sync against the GPU stream each iteration.\n\nHaving said that - all of the performance numbers in your post look **way** too large to me! \n\nPS @kbrems TF Version 0.10rc0 had some nasty performance regressions.  I don't believe any of them would affect this code, but I would definitely advise updating to the release 0.10\n", "I modified your code to compare it with static unrolling in TF:\n\n```\nimport google3\nimport tensorflow.google as tf\nimport numpy as np\nimport time\n\ncfg = tf.ConfigProto(graph_options=tf.GraphOptions(\n    optimizer_options=tf.OptimizerOptions(\n        opt_level=tf.OptimizerOptions.L0)))\n\ndim = 100\nstart = tf.constant(0)\nend = tf.placeholder(tf.int32)\ninit = tf.ones([dim], dtype=tf.float64)\nb = tf.ones([dim], dtype=tf.float64)\n\ndef while_run():\n  def condForWhile(i, a):\n    return tf.less(i, end)\n\n  def bodyForWhile(i, a):\n    i = i + 1\n    a = tf.mul(a, b)\n    return [i, a]\n\n  _, prod = tf.while_loop(condForWhile, bodyForWhile, [start, init])\n  with tf.control_dependencies([prod]):\n    result = tf.no_op()\n\n  with tf.Session(config=cfg) as sess:\n    # warm up to eliminate start-up costs\n    sess.run(result, feed_dict={end: 100})\n\n    # time it.\n    t0 = time.time()\n    sess.run(result, feed_dict={end: 10000})\n    print(\"Iterations/second (dynamic):\", 10000.0 / (time.time() - t0))\n\ndef for_run():\n  prod = init\n  for i in xrange(10000):\n    prod = tf.mul(prod, b)\n  with tf.control_dependencies([prod]):\n    result = tf.no_op()\n\n  with tf.Session(config=cfg) as sess:\n    # warm up to eliminate start-up costs\n    sess.run(result)\n\n    # time it.\n    t0 = time.time()\n    sess.run(result)\n    print(\"Iterations/second (static):\", 10000.0 / (time.time() - t0))\n\ndef main(_):\n  for_run()\n  while_run()\n\nif __name__ == '__main__':\n  tf.app.run() \n```\n\nHere are the results. When the loop body is tiny, the run time is dominated by the book keeping overhead of while loop. (We need to run a lot more ops such as tf.add, tf.less, ...) When the loop body has a bit non-trivial computation, they are roughly equal.\n\ndim = 100:\nIterations/second (static): 183921.174835\nIterations/second (dynamic): 79751.9009581\n\ndim = 100000:\nIterations/second (static): 69104.7188479\nIterations/second (dynamic): 69447.4082463\n", "Automatically closing due to lack of recent activity. Please reopen when additional information becomes available.\n"]}, {"number": 4386, "title": "bazel build should warn you if you haven't run ./configure", "body": "The errors when not running configure are not terribly intuitive (see e.g. #4279 ). \n", "comments": ["I'm not sure how it worked, but there used to be a helpful message. If I get a chance I'll sync back to an August github build and see if I can trace where that message used to be triggered from.\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 4385, "title": "MatMul operation would cost 4ms", "body": "By stats the time costs using stats_collector_ in Executor class, I found that a matrix multiply operation of 150*150 matrix with a 150 vector would cost 4ms on CPU, And a 3 layers of DNN would cost 20ms, is that expected?\n", "comments": ["4ms for such a small Matrix-Vector product sounds high. Can you include more information about your environment, a reproducible test case, and a timeline?\n", "Thanks for you reply. I just want a answer for question \"is 4ms expected\". if not, I will try more investigation on it.\n", "Hi andydavis1,\nThanks for you reply. I can give more details, I can give the stack by gdb, It's like:\n\n26 template <typename T>\n 27 struct MatMulTypes {\n 28   typedef Eigen::TensorMap<Eigen::Tensor<T, 2, Eigen::RowMajor>, Eigen::Aligned>\n 29       out_type;\n 30   typedef Eigen::TensorMap<Eigen::Tensor<const T, 2, Eigen::RowMajor>,\n 31                            Eigen::Aligned> in_type;\n 32 };  \n 33 \n 34 template <typename Device, typename In0, typename In1, typename Out,\n 35           typename DimPair>\n 36 void MatMul(const Device& d, Out out, In0 in0, In1 in1,\n 37             const DimPair& dim_pair) {\n 38   out.device(d) = in0.contract(in1, dim_pair);\n 39 }\n\n248 template <typename T>\n249 struct MatMulFunctor<CPUDevice, T> {\n250   void operator()(\n251       const CPUDevice& d, typename MatMulTypes<T>::out_type out,\n252       typename MatMulTypes<T>::in_type in0,\n253       typename MatMulTypes<T>::in_type in1,\n254       const Eigen::arrayEigen::IndexPair<Eigen::DenseIndex, 1>& dim_pair) {\n255     MatMul<CPUDevice>(d, out, in0, in1, dim_pair);\n256   }\n257 };\n\nAnd a 150*150 matrix with a 150 vector would cost 4ms here.\nCan you give any points for this?\n", "sorry, this is is misleading. it's a Matrix-Matrix multiplication. 150_150  Mul 150_100. I think it's reasonable to cost 4ms.\n"]}, {"number": 4384, "title": "Update Dockerfile.gpu to use Tensorflow 0.10.0", "body": "Tensorflow 0.10.0rc0 was compiled against CuDNN v4. Previous Dockerfile wasn't working.\n", "comments": ["@fsegouin, thanks for your PR! By analyzing the annotation information on this pull request, we identified @gunan, @keveman and @caisq to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA signed \ud83d\udc4d \n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "This is done as a part of #4344 sorry for the delay on that.\nmerging that PR now.\n"]}, {"number": 4383, "title": "Removed duplicate section in Tensorboard README", "body": "There were two sections present, both talking about multiple/distributed summary writes. The sections were almost identical. I have removed one of them.\n", "comments": ["@florianletsch, thanks for your PR! By analyzing the annotation information on this pull request, we identified @danmane, @keveman and @rryan to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the PR. However, this was fixed internally a few days ago, so this PR will be redundant once we sync changes. I'm closing it.\n"]}, {"number": 4382, "title": "List of 2Ds -> 3D Tensor, seq2seq_loss", "body": "Use full tensor and reshape instead of evaluating at every timestep, computationally more efficient and faster. Further, handles `None` in sequence length and `raw_rnn` style RNN classes does not work with a list of Tensors.\n\nImplemented handling of `list` as input, so it does not break backwards compatibility.\n", "comments": ["Can one of the admins verify this patch?\n", "@ebrevdo Made some updates to allowing mix of list (current style) and tensors (new style). Further correct indentation and raises for wrong dimensions on logits, targets and weights.\n", "Lukasz will have to comment, but I believe we're starting a deprecation process for `seq2seq` which is monolithic and hard to maintain.  New code will sit somewhere in contrib.\n", "(it's unfortunate that you are sending all these PRs now, after we've just decided to start deprecating; sorry for the bad timing)\n", "Oh, while static seq2seq will be deprecated, the sequence_loss functions will stay (we might just move them to another module). I'm therefore reopening this. alrojo -- maybe instead of improving the seq2seq module you could put these functions in a new module, maybe tf.contrib.seq2seq? That one could be fully dynamic and ultimately (but before TF 1.0) replace seq2seq. Does this sound ok? Great thanks for your CLs, they help a lot!\n", "Can one of the admins verify this patch?\n", "No problem, TensorFlow has helped me much. So I felt like giving back :)\n\nI could work on a tf.contrib.seq2seq.py file. I also have a `dynamic_rnn_decoder_attention` ready for PR (based on the `raw_rnn` as well). I will start by making tests as advised in #4686 for the `dynamic_rnn_decoder` and I assume you would like kernel tests for this seq2seq loss function as well? After thoese two tasks I can make a seq2seq folder [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib) with similar structures to the other contrib folders.\n\nI might have a few questions underway regarding setting up tests and correct folder structure. Would you like those questions to be on this thread (with an open PR), in the Issue section or another place? Thanks\n", "Yes, making a seq2seq folder in contrib sounds like the best way to go. Then you can have a separate file there for loss functions and maybe a separate one for decoders (in current seq2seq these things got somehow jumbled toegther). Great thanks for doing this, having a fully dynamic seq2seq will help a lot of people!\n", "P.S. As for questions, any thread is fine.\n", "Recommend focusing only on the dynamic case... Don't bother adding logic to\nhandle lists of inputs.\n\nOn Sep 30, 2016 2:45 PM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> P.S. As for questions, any thread is fine.\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4382#issuecomment-250859747,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim5DByCvYDSTLEtr6Nfgj-UJMGBUUks5qvYMDgaJpZM4J8vGd\n> .\n", "Any updates on this?  I'll close if this is no longer relevant.\n", "Yes, this will be a part of the loss function for the new dynamic decoder.\n\nI will update this as a part of the new tf.contrib.seq2seq section once we have finished the [dynamic_decoder](https://github.com/tensorflow/tensorflow/pull/4686) PR.\n", "Any update on this? Looks like maybe we should close it?", "Now alrojo is actively working on another related PR, I'm not sure we should close it yet.", "@danmane @lukaszkaiser I was waiting for #4686 to avoid merge conflicts. I will update this PR tomorrow. Thanks for your patience.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo @lukaszkaiser the code has now been updated accordingly to the recent `contrib.seq2seq` commits. The code compiles without issues on my own jenkins replica for the Linux CPU test.\r\n\r\nPlease review, thanks!\r\n\r\nAlso, let me know if you're at NIPS this week and want to meet :)", "Hi to all,\r\n\r\nI have seen the implementation of the seq2seq loss and it seems that it doesn't ignore possible padding values that can be used during training to make sequences of the same length. However, these values should not be considered in the loss function. Am I wrong?\r\n\r\nThank you for your great work @alrojo. I hope that this pull request will be merged as soon as possible :)\r\n\r\nBest regards,\r\nAlessandro", "@aleSuglia With padding do you mean masking? The function supports masking.", "Yes I mean masking. I've read again your code and I think that you apply masking here: https://github.com/alrojo/tensorflow/blob/a6eafe994bd4c3fd66b4d0f79083279f72d358d1/tensorflow/contrib/seq2seq/python/ops/loss.py#L76\r\n\r\nSo the correct way to ignore the error in the prediction for padding values is done by setting to zero the elements of *weights* corresponding to padding values. Is it right?\r\n\r\nThank you for your help!", "@aleSuglia yes, that is correct. I have added a more detailed docstring to avoid confusion.", "@martinwicke Any news?", "Hi Alex. Is this code ready for review? What is missing? I think it got lost between the PRs. Let us know what needs to be done please!", "Hi @lukaszkaiser , yes, the code is finalized and ready for review.", "@lukaszkaiser please review again.", "@tensorflow-jenkins test this please", "It looks like //tensorflow/contrib/seq2seq:loss_test  is failing, could you take a look?", "Of course, I will try and have it ready for monday. :)", "Please try and test again", "Jenkins, test this please.", "Jenkins, test this please.", "Oh, `seq2seq_loss` was my faulty merge. Fixing.", "Actually, it was in this CL. Fixed.\r\n\r\nJenkins, test this please.", "Jenkins, test this please."]}, {"number": 4381, "title": "fix comment grammar", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@kborer, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @caisq to be potential reviewers\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4380, "title": "TensorFlow master build failing: error: invalid initialization of reference ", "body": "We are building TensorFlow master (commit id  [461caa8](https://github.com/tensorflow/tensorflow/commit/461caa86137aee3d2e40843ea308c216f10c4655) ) using bazel on 64 bit platform. \n\nWe need a fix for 64 bit platform as mentioned in  [#1044](https://github.com/google/protobuf/pull/1044) in google/protobuf. So we tried to change the protobuf commit id used in `tensorflow/workspace.bzl` file with recent protobuf commit id as mentioned below.  \n\n```\n+++ b/tensorflow/workspace.bzl\n@@ -88,7 +88,8 @@ def tf_workspace(path_prefix = \"\", tf_repo_name = \"\"):\n\u00a0\u00a0\u00a0native.git_repository(\n\u00a0\u00a0\u00a0\u00a0\u00a0name = \"protobuf\",\n\u00a0\u00a0\u00a0\u00a0\u00a0remote = \"https://github.com/google/protobuf\",\n- commit = \"ed87c1fe2c6e1633cadb62cf54b2723b2b25c280\",\n+# commit = \"ed87c1fe2c6e1633cadb62cf54b2723b2b25c280\",\n+ commit = \"c59473d53eafadd126502657e5c5c33e952b67ed\",\n\u00a0\u00a0\u00a0)\n```\n\nBut TensorFlow build is failing with below errors: \n\n```\n\nERROR: /home/test/tensorflow_proto_commit/tensorflow/tensorflow/core/BUILD:941:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: gcc failed: error executing command /opt/gccgo/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/opt/gccgo/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 108 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\ntensorflow/core/example/feature_util.cc: In function 'const typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type& tensorflow::GetFeatureValues(const string&, const tensorflow::Example&) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type = google::protobuf::RepeatedField<long long int>; std::__cxx11::string = std::__cxx11::basic_string<char>]':\ntensorflow/core/example/feature_util.cc:54:66: error: invalid initialization of reference of type 'const google::protobuf::RepeatedField<long long int>&' from expression of type 'const google::protobuf::RepeatedField<long int>'\n   return example.features().feature().at(name).int64_list().value();\n                                                                  ^\ntensorflow/core/example/feature_util.cc: In function 'typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type* tensorflow::GetFeatureValues(const string&, tensorflow::Example*) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait<FeatureType>::Type = google::protobuf::RepeatedField<long long int>; std::__cxx11::string = std::__cxx11::basic_string<char>]':\ntensorflow/core/example/feature_util.cc:62:23: error: cannot convert 'google::protobuf::RepeatedField<long int>*' to 'google::protobuf::RepeatedField<long long int>*' in return\n       ->mutable_value();\n                       ^\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n\n```\n\nIs there any specific protobuf commit id to be used with TensorFlow?\nWe could build TensorFlow without making any change in `tensorflow/workspace.bzl` and manually adding fix for 64 bit platform in  `external/protobuf/src/google/protobuf/stubs/atomicops_internals_generic_gcc.h` .\n\nWe need to use protobuf with commit id [c59473d](https://github.com/google/protobuf/commit/c59473d53eafadd126502657e5c5c33e952b67ed) or recent , where a fix for 64 bit platform has been committed. \n\n**Environment info**\n\nOperating System: **RedHat , Ubuntu**\n\nInstalled version of CUDA and cuDNN:  **Not installed** \n\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. **master**\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)  [461caa8](https://github.com/tensorflow/tensorflow/commit/461caa86137aee3d2e40843ea308c216f10c4655)\n2. The output of `bazel version` **master branch (0.3.1-2016-09-12)**\n\nSteps to Reproduce:\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n", "comments": ["You actually quoted the canonical commit to be used:\n\n```\n+# commit = \"ed87c1fe2c6e1633cadb62cf54b2723b2b25c280\",\n```\n", "+1 \n\nSo TensorFlow needs specifically protobuf's `\"ed87c1fe2c6e1633cadb62cf54b2723b2b25c280\"` commit ID to build? \nOther protobuf commits don't work?\n", "I;m just a TF user and have no deeper insights into it's inner workings, \n\nYou can do an interval search between `ed87c1` and `HEAD` (see `git bisect`), but considering the commit of interest to you is rather small, you could probably just backport it.\n\n(and I believe you may have more luck at Stack Overflow considering this isn't a bug in TF).\n", "Please post any questions to StackOverflow with tag 'tensorflow'. Closing this for now.\n", "Thanks Andy.\nBut we wanted to know which protobuf commit id to be used for Tensorflow.\n\nI could build Protobuf with `c59473d53eafadd126502657e5c5c33e952b67ed` independently. \nDoesn't look like an issue with Protobuf. \n\nI could build TensorFlow master with  protobuf commit id  `ed87c1fe2c6e1633cadb62cf54b2723b2b25c280` successfully.  \nHowever, we want to use protobuf commit id `c59473d53eafadd126502657e5c5c33e952b67ed` or recent one in TensorFlow  as fix for 64 bit platform has been committed though `c59473d53eafadd126502657e5c5c33e952b67ed`. But building TensorFlow gives us an error as mentioned is description. \n\n@poxvoculi @girving Do you have any idea about this?\n"]}, {"number": 4379, "title": "having encounted an issue when i use ./configure to install tensorflow", "body": "Hi, all.\ni am new to tensorflow, and spent a lot of days to install tensorflow, but it didn't work. Need help\n\n sudo ./configure \n~/tensorflow ~/tensorflow\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 4\nPlease specify the location where cuDNN 4 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\nFound stale PID file (pid=3471). Server probably died abruptly, continuing...\n..\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nERROR: /home/keithyin/tensorflow/tensorflow/contrib/session_bundle/BUILD:134:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by /home/keithyin/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle_lite'.\nERROR: /home/keithyin/tensorflow/tensorflow/core/platform/default/build_config/BUILD:56:1: no such package '@jpeg_archive//': Error downloading from http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/keithyin/.cache/bazel/_bazel_root/9192340d7b606ddb9ea35b29a97154c1/external/jpeg_archive: Error downloading http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/keithyin/.cache/bazel/_bazel_root/9192340d7b606ddb9ea35b29a97154c1/external/jpeg_archive/jpegsrc.v9a.tar.gz: Connection timed out and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n", "comments": ["To quote https://github.com/tensorflow/tensorflow/issues/4352#issuecomment-246908728:\n\n> The errors regarding android_lib_lite and meta_graph_portable_proto should be fixed by f66b491, which was pushed today in #4360.\n\nThere may be a secondary problem you're encountering generating the download errors. I ran into this yesterday when I was struggling with the same android-reference problems. It appears that, if you run ./configure repeatedly because of a build problem, dependencies are also downloaded repeatedly and (some) servers react by blocking your requests.\n\nSolutions include (a) waiting, (b) changing your IP address or possibly (c) changing the `bazel clean --expunge` at the end of /configure to just `bazel clean`. Although the latter may create new problems if bazel isn't perfect with managing intermediate build results.\n\nNote that you also don't need to run configure with sudo.\n", "@MatthiasWinkelmann  i used bazel clean and did the suggestions as #4352 said , but i still got that issue\nERROR: /home/keithyin/workspace/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/keithyin/workspace/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\n", "Getting the exact same issue here, worked perfectly on exact same hardware a few days ago. It seems to be an issue with a recent commit.\n\nUbuntu 14.04, TITAN X, python2, CUDA 7.5, CuDNN 5.1\n\nEDIT: The issue no longer occurs when I try building v0.9.0. I also tried with v0.10.0, and while this issue did not occur, I experienced #4105 instead.\n", "@mxbi this issue occurs when i tried to build v0.9.0. Ubuntu16.04, GTX 960, python2, Cuda 8.0, cudnn 4. gcc-4.9 g++-4.9\nC++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 100 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n", "same here when configure both on my mac and my desktop , this is what it looks like when configure on my mac\n\n(tensorflow_anaconda/) KCs-MacBook-Pro:tensorflow kc$ ./configure \n~/tensor_versions/tensorflow ~/tensor_versions/tensorflow\nPlease specify the location of python. [Default is /Users/kc/anaconda2/envs/tensorflow_anaconda/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /Users/kc/anaconda2/envs/tensorflow_anaconda/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/Users/kc/anaconda2/envs/tensorflow_anaconda/lib/python2.7/site-packages]\n\n/Users/kc/anaconda2/envs/tensorflow_anaconda/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nExtracting Bazel installation...\n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nINFO: Waiting for response from Bazel server (pid 70793)...\nERROR: /Users/kc/tensor_versions/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /Users/kc/tensor_versions/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /Users/kc/tensor_versions/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /Users/kc/tensor_versions/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /Users/kc/tensor_versions/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /Users/kc/tensor_versions/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /Users/kc/tensor_versions/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n", "@squallssck  would you like to isntall the latest version of tensorflow? if not. you can try this method:[http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/os_setup.html](url) you need just change the version 0.8.0 to 0.9.0 just like this  `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl`. I hope it helps . \n", "Sorry, I was traveling for a bit. Is this an issue at head? There have been transient issues due to download issues, re-closing tensorflow mostly fixes those. Can you try with a fresh tensorflow?\n", "It's likely this was a duplicate of https://github.com/tensorflow/tensorflow/issues/4312 and https://github.com/tensorflow/tensorflow/issues/4352 which should have been fixed in https://github.com/tensorflow/tensorflow/commit/f66b491a06627510c1cf751fc11db2caf5aa1f25\n"]}, {"number": 4378, "title": "added einsum op and allow more broadcasting dims", "body": "This was meant to solve: https://github.com/tensorflow/tensorflow/issues/175\nAnd I think it addresses this too: https://github.com/tensorflow/tensorflow/issues/1519\n\nFixes #175, closing #1519.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins Test this please.\n", "I'm not too sure why `//tensorflow/contrib/learn:graph_io_test` is failing on python 3 only. Is there a log file from that specific test that I could take a look at? I can't seem to reproduce the failure locally...\n", "The //tensorflow/contrib/learn:graph_io_test failure is most likely unrelated. It's already failing on the nightly builds for Mac python3.\n", "@martinwicke done\n", "Thanks! Jenkins, test this please.\n", "only failure is `//tensorflow/contrib/learn:graph_io_test`, which we said was unrelated\n", "Hooray.\n", "Fantastic! \nAny reasons against an alias 'contraction' ?\n", "nope, I had named it `einsum` because of similarity to `numpy.einsum`\n"]}, {"number": 4377, "title": "tensorflow.contrib.learn.TensorFlowEstimator fails under sklearn.base.clone()", "body": "tensorflow.contrib.learn.TensorFlowEstimator fails under sklearn.base.clone()\n\nThis prevents the possibility to perform CV using sklearn.cross_validation.cross_val_Score over this estimator.\nThe clone fails due to the fact the TensorFlowEstimator.get_params() returns a key named \"params\" that is not part of the constructor input parameters.\nThis causes sklearn.base.clone to fail when trying to clone the estimator using it's constructor:\nTensorFlowEstimator(**params)\n", "comments": ["`TensorFlowEstimator` is deprecated and is removed in future version. Please use `Estimator`.\n\nThat said, the issue probably will persist with `Estimator` so we are going to address it there.\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4376, "title": "Update Makefile", "body": "-Wl,--no-whole-archive added fixes #3139\n-Wl,--allow-multiple-definition no longer necessary\nOnly changed the Linux configuration because i can only test that, but could be applicable to Android too\n", "comments": ["Can one of the admins verify this patch?\n", "@normads, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @tensorflower-gardener and @vrv to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins Test this please.\n"]}, {"number": 4375, "title": "Creating a \"Books\" section under Resources", "body": "Packt is proud to publish a new book on TensorFlow\n [Getting Started with TensorFlow](https://www.packtpub.com/big-data-and-business-intelligence/getting-started-tensorflow)\n\nWe would appreciate if you could create a \"Books\" section under Resources to feature books written on TensorFlow. \n\nLet me know if this can be done.\n\nThanks\n", "comments": ["BTW, there's a list of TensorFlow books on\nhttps://github.com/jtoy/awesome-tensorflow\n\nYou could do a pull request to add your book there\n\nOn Wed, Sep 14, 2016 at 12:05 AM, Packt notifications@github.com wrote:\n\n> Packt is proud to publish a new book on TensorFlow\n> Getting Started with TensorFlow\n> https://www.packtpub.com/big-data-and-business-intelligence/getting-started-tensorflow\n> \n> We would appreciate if you could create a \"Books\" section under Resources\n> to feature books written on TensorFlow.\n> \n> Let me know if this can be done.\n> \n> Thanks\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4375, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHDkL95uU-Gp1Aw9KjP2BLRMl7EtQks5qp5zWgaJpZM4J8dgj\n> .\n", "Thanks for the link @yaroslavvb \nBut I feel it will help alot more people if it is featured on tensorflow.org\n@andrewharp could you help me with this?\n", "@andydavis1 could you help me?\n", "We have not made such a books section since we would have trouble\nmaintaining it, and making sure that it's reasonably up to date.\n\nThere are many books on TensorFlow already and we haven't read any so we\nthink it would be somewhat inappropriate to endorse some of them.\nOn Wed, Sep 28, 2016 at 02:34 Packt notifications@github.com wrote:\n\n> @andydavis1 https://github.com/andydavis1 could you help me?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4375#issuecomment-250083481,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_e3tkQgnV7n4ILdIhqv828SPbIFYks5qugqIgaJpZM4J8dgj\n> .\n", "@martinwicke I totally agree with what you are saying. I could provide you with an eBook copy of the book for review, if you can put up the book.\n", "We don't have the bandwidth to review books. However, we are thinking about redoing our community page, so stay tuned. \n", "Sure @martinwicke . Keep me updated :)\n"]}, {"number": 4374, "title": "Remove deprecated load_csv", "body": "", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @ilblackdragon and @rohan100jain to be potential reviewers\n"]}, {"number": 4373, "title": "Remove deprecated learn.ops", "body": "These ops are deprecated and users need to use ops in `contrib.layers` instead. \n", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @martinwicke and @mrry to be potential reviewers\n", "@tensorflow-jenkins Test this please\n"]}, {"number": 4372, "title": "Typo fix in Android Example (gradle)", "body": "It seems like the `t` is a mistake, although it may be a feature which I'm unfamiliar with.\n", "comments": ["@eerwitt, thanks for your PR! By analyzing the annotation information on this pull request, we identified @andrewharp to be a potential reviewer\n", "Can one of the admins verify this patch?\n", "@eerwitt Correct, that was an accidental keypress just prior to submit. Thanks for the catch!\n"]}]