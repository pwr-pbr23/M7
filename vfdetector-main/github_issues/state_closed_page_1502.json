[{"number": 7875, "title": "Typos", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7874, "title": "strip_unused tool can't read all graph protobufs...", "body": "Hi, \r\n\r\nWhen trying to strip unused nodes from my graph, the tool fails in cases where the protobuf encodes a continuation byte, but fails to continue. For example, running \r\n\r\n`./bazel-bin/tensorflow/python/tools/strip_unused --input_graph=tensorflow/contrib/some/path/mnist.pb --output_graph=tensorflow/contrib/some/path/mnist.inference.pb --input_node_names=Reshape,dropout --output_node_names=prediction_onehot`\r\n\r\non the attached graph causes a: \r\n\r\n`UnicodeDecodeError: 'utf8' codec can't decode byte 0xc5 in position 45: invalid continuation byte`\r\n\r\nIt seems decoding the file as a UTF8 string is causing issues...\r\n\r\n[mnist.pb.zip](https://github.com/tensorflow/tensorflow/files/800990/mnist.pb.zip)\r\n\r\n", "comments": ["Are you using Python 3? It seems to be a `str` and `bytes` issue.", "I am not! 2.7.x", "I'm having exactly the same issue running `freeze_graph` - the files aren't loading due to utf-8 errors.\r\nI've tried saving in both Python 3 and Python 2, but there's no difference in the resulting errors.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I ran into a similar issue. For binary graph definitions you need to set the `--input_binary` argument to `True`. It defaults to `False` what causes the script to use a different parser."]}, {"number": 7873, "title": "The installing instructions for windows anaconda seems wrong", "body": "The URL in the command line is http://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_x86_64.whl\r\nI can't install from the URL\r\nhowever, I replaced x84_64 with amd64 and it works", "comments": ["I experienced the same issue and opted for the same \"fallback\" solution for now.\r\n\r\nThat being said, the resulting install still has the issue discussed under #7500, which as of now seems to be only addressed in the nightly build.", "@yifeif We've seen a few issues with confusion between `x86_64` and `amd64` in the PIP URLs. Do you happen to know what the definitive answer is? Thanks!", "The pip binaries have always had amd64 as the platform tag. \r\nThis seems to be a mistake in the URLs on the website, which we will update soon. Sorry for the confusion.", "website is updated with the correct urls. Closing issue."]}, {"number": 7872, "title": "updated word2vec basic example with current initialize_all_variables(\u2026", "body": "\u2026) from issue #5514\r\n\r\nBasic example wasn't running so I updated it with change recommended in [Issue 5514](https://github.com/tensorflow/tensorflow/issues/5514).", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7871, "title": " building target with GPU support ", "body": "bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\r\n\r\nERROR: /home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD.\r\nINFO: Elapsed time: 0.464s\r\n\r\nThonk you!\r\n", "comments": ["Did you run the configure.sh script from the master tf folder? As suggested please select the GPU option to be yes when asked if you need GPU support. Otherwise, please use this command instead: `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` to build without GPU support. I suggest closing this issue as this is a question to be asked at stackoverflow.", "Interesting, I just ran into the same issue when building from source and as shown in the log I configured for GPU support. \r\nEarlier I wasn't having this problem, I did have [this](https://github.com/bazelbuild/bazel/issues/2594) though.\r\n```\r\n$ ./configure\r\nPlease specify the location of python. [Default is /c/Users/Adriano/AppData/Local/Programs/Python/Python35/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] Y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\n  C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35]\r\n\r\nUsing python library path: C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for util\\python\\python_include <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\include\r\nJunction created for util\\python\\python_lib <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for third_party\\py\\numpy\\numpy_include <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\numpy\\core\\include\r\nDo you wish to build TensorFlow with CUDA support? [y/N] Y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1\r\nPlease specify the location where cuDNN 5.1 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 5.0\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n\r\nAdriano@Adriano MSYS /c/Users/Adriano/Documents/tensorflow\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nERROR: C:/tools/msys64/tmp/_bazel_Adriano/t5f2Q_w8/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n        File \"C:/tools/msys64/tmp/_bazel_Adriano/t5f2Q_w8/external/local_config_cuda/crosstool/BUILD\", line 4\r\n                error_gpu_disabled()\r\n        File \"C:/tools/msys64/tmp/_bazel_Adriano/t5f2Q_w8/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n                fail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by C:/tools/msys64/tmp/_bazel_Adriano/t5f2Q_w8/external/local_config_cuda/crosstool/BUILD.\r\nINFO: Elapsed time: 1.494s\r\n```\r\ncc @gunan\r\n**UPDATE:** I cloned the repository again and it works now.", "@molyswu All seems to be OK on our end.\r\nPlease follow the guide on installing from sources again:\r\nhttps://www.tensorflow.org/install/install_sources\r\n\r\nIf you still run into a problem, please use the issue template to file a new issue to provide more information for us.\r\nWe will need more debugging information that is required by the template if we are to help."]}, {"number": 7870, "title": "Branch 148513339", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7869, "title": "Test issue", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new)  We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!"]}, {"number": 7868, "title": "Hyper-parameter search in TensorFlow", "body": "Hi all,\r\n\r\nThanks for the great work on TensorFlow.\r\n\r\nIs anybody aware of any available TF feature for automatic hyper-parameter search? E.g., using Grid Search, or Random Search or Bayesian Optimization?\r\n\r\nThanks!\r\nHamid", "comments": ["I think this is something being worked on inside tf.contrib.learn together with Experiment. I don't know the detailed plan though, e.g. whether there will be Bayesian optimized param search method. @martinwicke might know more details. ", "Hey, is anyone working on this?  I just run into it, cause i need some of this, if no one is working on it, i can help to make a pr :)", "@xiejw FYI.\r\n\r\nLet us open-source some more tooling, but then it would be great if someone were to write some Tuning strategies. Or maybe an integration for something like spearmint. ", "Take a look at `learn_runner.tune`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_runner.py#L115\r\n\r\nIn theory, all that needs to be done is to implement one such `Tuner` object which does (say) grid search, or something more clever. \r\n\r\nI apologize -- the documentation on this is extremely sparse, but please ask questions if the intended use is unclear from the code and comments.", "@martinwicke i also need the same. I will have a shot at this if its okay.", "Go for it. Let me know if you have any questions. ", "Can you clarify the create_tuner(study_configuration, objective_key) function. Does it create tuner object? What are the study_configuration, objective_key. I saw the Tuner class which is just an abstract class with a default constructor so where are these parameters used. ", "Ah, that's just an example, you don't have to worry about it. The Tuner class is only used in `learn_runner.tune` and you can look at the tiny bit of code that uses it. Basically, you have to implement `Tuner.next_trial()` which asks the tuner to start a new \"trial\" (an experiment run on specific hyperparameters), and `Tuner.run_experiment`, which takes an experiment_fn with the signature documented in `learn_runner.tune`, and runs the experiments with the hyperparameters for the current trial.\r\n\r\nWe will add an abstract class to codify the requirements, so far it's all duck typing, yey.", "@martinwicke Very cool feature, I am looking forward to adapt it for hyperopt or something like that!\r\nBut for now, I stuck with very simple question: is there any simple way to give it ability to run several experiments in parallel (independently)? Or I **must** specify [tf.server](https://www.tensorflow.org/api_docs/python/tf/train/Server#class_tftrainserver) for that? Maybe you can help with some advice or best practices, guide?", "@xiejw for additional comments. A hyperopt tuner would be very cool.\r\n\r\nYou need `tf.Server` etc. only when you want each experiment to run on several machines. If you have one machine per experiment, you can distribute the python code and run it on each machine independently. You just have to make sure that your Tuner distributes different hyperparameter values to different workers.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial/blob/master/mnist.py\r\n\r\nThis guy from google implemented a small hyperparameter search. And print the different result in tensorboard.  Maybe it helps. \r\n  ", "For one of my projects I created a gridsearch similar to the one from  scikit-learn (http://scikit-learn.org/stable/modules/grid_search.html).\r\n\r\nYou can have a look at https://github.com/makoeppel/hyperParameterSearchTF/tree/master\r\n--> Please take in mind that for now the classes are not filled with a specific model and the repo is still under contraction (I will add a mnist example soon). If you want to test it you just need to fill the empty classes with your specific model.\r\n\r\n**The program works like this:**\r\n\r\nThe idea here is that you configure your search in the main.py. There you load your someClf.clfHandler and your evaluator.evaluatorHandler. With some default values. In the example code it is: \r\n\r\n`someClf = someClf.clfHandler(outputDir, outputName, layers=[5, 5, 1], feature_func=difference,\r\n                                         weight_func=summed_weight, cost_func=weighted_least_square,\r\n                                         optimizer=tf.train.AdamOptimizer(0.01), activation=tf.nn.tanh,\r\n                                         use_bias=False, kernel_initializer=tf.random_normal_initializer(),\r\n                                         steps=20, max_iters=2000, n_samples=1000, max_samples=3000)`\r\n\r\nand \r\n\r\n`clfEvaluator = evaluator.evalHandler(inputDir=outputDir,\r\n                                         inputName=outputName,\r\n                                         feature_func=difference,\r\n                                         weight_func=summed_weight\r\n                                         )`\r\n\r\nAfter this you pass everything to the multimodelsearch.MultiModelSearch. This class will generate for every parameter in your parameter dict clfHandler and evalHandler classes with the default values and will replace the default ones  with the ones from the dict.\r\n\r\nThe function MultiModelSearch.fit_and_eval() is using the multiprocessing library and will train and evaluate all generated models and after this it will return a dict with the results. \r\n\r\nAt the end I used this file (http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py) for plotting the results. \r\n\r\nIf you need more informations please ask me. I am open for contribution.\r\n\r\n  \r\n  ", "I used the mnist.py example from https://www.tensorflow.org/get_started/mnist/beginners and put that into my code now (https://github.com/makoeppel/hyperParameterSearchTF/tree/master).", "@martinwicke\r\n\r\nI now created this tuner class (https://github.com/makoeppel/tuner_object_for_learn_runner) and pass it to the `learn_runner.tune(experiment_fn=experiment_fn, tuner=tuner)` function. I used an example from https://gist.github.com/peterroelants/9956ec93a07ca4e9ba5bc415b014bcca for the model and experiment_fn.\r\n\r\nThe tuner class can be called with `default_param`, `hparams` and `model_dir`. When you call `next_trial()` it set everything to the `default_param` and goes to the next `hparam` in `hparams` and replaces the `default_param` with this one. When you call `run_experiment()` it runs \r\n`learn_runner.run(experiment_fn=experiment_fn, \r\n            run_config=self.run_config,\r\n            schedule=\"train_and_evaluate\", \r\n            hparams=hparams)` with the hparams after the current next_trial() call. The `run_config` uses the default `run_config = tf.contrib.learn.RunConfig()` the only parameter how can be changed is the `model_dir`.\r\n\r\nSo it is a really basic gridsearch.", "I almost never advertise my tutorials, but I came across this thread and saw it was still active, so you might like to know about my recent tutorial on hyper-parameter optimization in TensorFlow using Keras and scikit-optimize which has various forms of Bayesian optimization methods including Gaussian Processes and Random Forests:\r\n\r\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb\r\n\r\nhttps://www.youtube.com/watch?v=oaxf3rk0KGM\r\n", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "I need hyperparameter search for one of my projects and could spend some time on this issue. I did some prototyping for a single machine with multiple GPUs where a model instance is naively assigned to each GPU: [https://github.com/asprenger/tf-param-search/blob/master/param_search.py](https://github.com/asprenger/tf-param-search/blob/master/param_search.py). The parameter sets are currently generated using classes from Sklearn model selection. In the end I would like to be able to distribute parameter search over multiple machines. Maybe someone could provide some feedback.", "@cssndrx @ispirmustafa FYI.", "Hi @martinwicke is this issue being worked on or can I give it a try? Thanks", "There are several efforts around hyperparameter search for Keras already. Did you have a specific project in mind?", "Thanks @martinwicke , I was thinking of Bayesian optimization based method. As you rightly said it must be already part of the efforts. Wanted to mainly start as a beginner on the contribution, since I use these techniques I thought contributing here will be a good start.\r\nWill explore other issues on contribute label to start with.", "Yeah, this is a pretty complicated issue to start with. Thanks for getting\ninvolved!\n", "Going to close this issue, as [Keras Tuner](https://github.com/keras-team/keras-tuner) will support RandomSearch, Hyperband, and Bayesian optimization. Thank you for the request, and be on the lookout for KT's 1.0 release!", "Is there a recommended way to approach Bayesian optimization without Keras? I'm currently using the TensorBoard hparams plugin but I can only do grid/random search with that."]}, {"number": 7867, "title": "Branch 148500449", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7866, "title": "Build errors in versioned_computation_handle.{h,cc}", "body": "### Issue summary\r\n\r\nI'm running on Ubuntu 16.04, working from Tensorflow commit 904510eeaa40b0c8f982fbb679d827688cb35b01. I ./configure'd it to use XLA, CUDA 8.0, cuDNN 5.1, and compute capability 6.1 for a Titan X (Pascal). NVIDIA drivers and CUDA are installed from the NVIDIA repos, while the cuDNN 5.1 deb files are downloaded from NVIDIA's website and installed using dpkg. I'm trying to build tensorflow via\r\n\r\n`bazel build --config opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nEverything goes fine until it gets to compiling `./tensorflow/compiler/xla/service/versioned_computation_handle.h`\r\nand the associated cc file. The error seems to be that certain \"#includes\" aren't made in this file:\r\n\r\n```\r\nIn file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:0:\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:33:19: error: 'int64' does not name a type\r\n   using Version = int64;\r\n                   ^\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:36:3: error: 'Version' does not name a type\r\n   Version version;\r\n   ^\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:38:3: error: 'string' does not name a type\r\n   string ToString() const;\r\n   ^\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h: In member function 'bool xla::VersionedComputationHandle::operator==(const xla::VersionedComputationHandle&) const':\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:41:13: error: 'version' was not declared in this scope\r\n            (version == other.version);\r\n             ^\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:41:30: error: 'const struct xla::VersionedComputationHandle' has no member named 'version'\r\n            (version == other.version);\r\n                              ^\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h: In member function 'bool xla::VersionedComputationHandle::operator<(const xla::VersionedComputationHandle&) const':\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:46:15: error: 'version' was not declared in this scope\r\n              (version < other.version)));\r\n               ^\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:46:31: error: 'const struct xla::VersionedComputationHandle' has no member named 'version'\r\n              (version < other.version)));\r\n                               ^\r\ntensorflow/compiler/xla/service/versioned_computation_handle.cc: At global scope:\r\ntensorflow/compiler/xla/service/versioned_computation_handle.cc:22:1: error: 'string' does not name a type\r\n string VersionedComputationHandle::ToString() const {\r\n ^\r\ntensorflow/compiler/xla/service/versioned_computation_handle.cc: In function 'std::ostream& xla::operator<<(std::ostream&, const xla::VersionedComputationHandle&)':\r\ntensorflow/compiler/xla/service/versioned_computation_handle.cc:28:27: error: 'const struct xla::VersionedComputationHandle' has no member named 'ToString'\r\n   out << versioned_handle.ToString();\r\n                           ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n### Related GitHub issues\r\n\r\nSeems to be related to the following, except I'm not on MacOS:\r\nhttps://github.com/tensorflow/tensorflow/issues/7819\r\n\r\n### Environment info\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609 (from gcc --version)\r\nCuda compilation tools, release 8.0, V8.0.61 (from nvcc --version)\r\n\r\nCUDA 8.0 from NVIDIA repos (Linux, x86_64, Ubuntu, 16.04)\r\ncuDNN from nvidia downloads page (https://developer.nvidia.com/rdp/cudnn-download), runtime and developer debs for Ubuntu 14.04. Even though I'm running on 16.04 and the debs are for 14.04, this has worked for me in the past and seems unrelated to my issue. The cuDNN debs were installed with dpkg -i\r\n\r\nCUDA libs folder\r\n\r\n```\r\nuser@SYS-E300-8D-1:/usr/local/cuda-8.0/lib64$ ls -l libcud*\r\n-rw-r--r-- 1 root root 556000 Jan 26 18:48 libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Jan 26 18:51 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 Jan 26 18:51 libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root 415432 Jan 26 18:48 libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root 775162 Jan 26 18:48 libcudart_static.a\r\n```\r\n\r\n\r\n```\r\nuser@SYS-E300-8D-1:~/tensorflow$ bazel version\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n```\r\n\r\n### What I tried\r\n\r\nI'm playing around with including inttypes.h in the header in question, or possibly using an old school typedef instead of that fancy C++11 \"using fancyName = literalType\" syntax. Regardless, I wanted to file the bug before I get too deep into fixing it since it takes a while to recompile, even if I just change one line and run Bazel again.", "comments": ["I can confirm that disabling XLA doesn't cause the build error to occur.", "@jlebar is this a dupe of #7819, or an instance of the same problem on a different platform? \r\n\r\nAssigning you for XLA fu.", "Looks like a dupe to me."]}, {"number": 7865, "title": "Fixed documentation for false_negatives calculation", "body": "false positives -> false negatives in documentation", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Can one of the admins verify this patch?", "Jenkins, test this please.", "I signed the CLA two days ago. I wonder why it does not update in here.", "ci.tensorflow.org fails with `\r\nstderr: error: could not lock config file .git/config: No space left on device`"]}, {"number": 7864, "title": "Branch 148492669", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7863, "title": "Branch 148485205", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7862, "title": "RealDiv, VariableV2 ops still in system after uninstalling tf-gpu", "body": "Hi, thanks for making tf available! I'm having an issue reverting to tf-cpu. I'm using riga/tfdeploy to distribute trained models, which is not compatible with tf-gpu. I installed tf-gpu for an experiment, then reverted to cpu, all using pip install/uninstall. Now I'm getting error messages about unknown ops like RealDiv and VariableV2. \r\n\r\nMy question is: How can I remove any trace of tf-gpu from my system and get back to tf-cpu?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n#7322 and #7285\r\nand:\r\nhttp://stackoverflow.com/questions/36902457/how-to-uninstall-tensorflow-completely\r\n\r\n### Environment info\r\nOperating System: Win 10 64 bit, Python 3.5.3\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nNone\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: \r\nJust used \"pip install tensorflow\" (uses tensorflow-1.0.0-cp35-cp35m-win_amd64.whl)\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n1. pip install tensorflow\r\n2. run a tf.Session() - no ops like RealDiv etc., tfdeploy runs fine.\r\n3. pip install tensorflow-gpu\r\n4. pip uninstall tensorflow-gpu\r\n5. Some trace of these ops remains registered, causing warnings like `OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits` while runninng tf, and also causing frameworks like riga/tfdeploy to crash \r\n\r\n### What other attempted solutions have you tried?\r\nI have tried completely reinstalling Python 3, deleting all site-packages etc. I've installed and uninstalled CUDA. The ops seem to be registered somewhere, leading to different behavior of tf-cpu after installing and uninstalling tf-gpu.\r\n\r\n### Logs or other output that would be helpful\r\nThe best clue is maybe errors like `unknown op: BestSplits`, which did not occur in tf-cpu before I installed and uninstalled tf-gpu.\r\n\r\nAlso, tfdeploy gives error messages like `unknown operation: RealDiv`, which did not happen prior to installing and uninstalling tf-gpu.\r\n\r\nAny pointers on getting rid of/unregistering these ops is appreciated!\r\n\r\n", "comments": ["Hi, we are trying to keep this list for bugs/features in TensorFlow itself, could you ask on Stackoverflow instead? Personally I like to use virtual envs, this way I never have to worry about unistalling packages, just make a new env", "OK, thanks, I've posted the question here:\r\n\r\nhttp://stackoverflow.com/questions/42473745/remove-all-traces-of-tensorflow-gpu-installation-on-win10\r\n\r\nI think there is some trace of the GPU installation left after uninstalling, which might be viewed as a bug. Any pointers to unregistering these ops would be greatly appreciated!", "The GPU stuff is handled mostly by CUDA driver install. The TensorFlow Python uninstall is handled by whatever Python packager you use (ie, conda uninstall might leave some files, while pip uninstall not). If you think there's a bug in TensorFlow itself, rather than a bug in whatever Python manager you use to manage these packages, I can reopen this issue"]}, {"number": 7861, "title": "Internal compiler error using 1.0.0-devel docker image ", "body": "First pull the Docker image:\r\n\r\n```\r\ndocker pull tensorflow/tensorflow:1.0.0-devel\r\ndocker run -it tensorflow/tensorflow:1.0.0-devel /bin/bash\r\n```\r\n\r\nThen in the docker session,\r\n\r\n```\r\ncd /tensorflow\r\nbazel build -c opt //tensorflow:libtensorflow_c.so\r\n```\r\n\r\nThis succeeds for a while, but eventually outputs\r\n\r\n```\r\nERROR: /tensorflow/tensorflow/core/kernels/BUILD:1921:1: C++ compilation of rule '//tensorflow/core/kernels:svd_op' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers ... (remaining 114 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\r\nTarget //tensorflow:libtensorflow_c.so failed to build\r\n```\r\n\r\nThis is Docker 1.13.0 on OS X 10.12.3, if that makes a difference.", "comments": ["Actually, this is just the Docker VM running out of memory - increasing the memory from the Docker preferences resolves it.\r\n\r\nMaybe it would be worthwhile to add a warning to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md. ", "@caisq, could you add a warning about this to the docker readme?\r\n", "Talking to @caisq offline, it sounds like this is an unusual case because docker by default uses all available system memory (@malmaud did you have a default config or something that limited the memory? The commands you provided shouldn't limit the memory AFAIK). I'm gonna close this for now.", "If you use docker on Windows or OS X, you have to specify the memory the\nLinux virtual machine will use in advance and the default (at least at the\ntime) is too low for tensorflow.\nOn Fri, Apr 28, 2017 at 4:59 PM Skye Wanderman-Milne <\nnotifications@github.com> wrote:\n\n> Closed #7861 <https://github.com/tensorflow/tensorflow/issues/7861>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7861#event-1062802220>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA8SvV6Ilr9LzzR-MBPKxflaNtLc3Hj6ks5r0lMdgaJpZM4MLmLk>\n> .\n>\n", "Ah, thank you for that clarification! I'll add a note to the README."]}, {"number": 7860, "title": "Branch 148481141", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7859, "title": "Error using TF in Python", "body": "I have Python 3.5.2 and Anaconda 4.1.1 (64bit) and when i tried installing TF i got the same error earlier today.\r\nAfter following uninstall and then reinstall TF commands i was able to start Python app using TF; but now i am getting OpKernel error for unknown op: UpdateFertileSlots.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/11993393/23317661/58b8c90e-fa9d-11e6-8b95-975e2075e61d.png)\r\n\r\nAny idea how to fix this?\r\n\r\nN.B: i did searched online and could not find any answer in SO or any other resource to fix this issue to start using TF.", "comments": ["Could you please try to:\r\n1. Uninstall TensorFlow\r\n2. Download a [nightly build](http://ci.tensorflow.org/view/Nightly/job/nightly-win/) and install it\r\n\r\nthen follow up if worked for you?", "This rocked @Carmezim ! thanks a lot.", "Great it worked, thanks for the follow up. If there is no other doubts regarding this issue this can be closed now. ", "It also worked for me but is there a way to not use nightly builds?", "@atabakd You can actually ignore it as this issue is already fixed at HEAD but wasn't cherry picked for the current release. It will be present in the `1.1r`. To get rid of it, it's only upgrading to a nightly build.", "that means the problem can be resolved if I uninstall Tensorflow and install the nightly build version.  btw do you know when version 1.1 is released @Carmezim \r\n", "@mrnameless123 it's pre-released https://github.com/tensorflow/tensorflow/releases", "so I just simply need to wait until it's officially released and use 'pip3 --upgrade tensorflow' on my laptop right? @Carmezim cuz when I tried to upgrade 1.1 it said it's already up to date", "@mrnameless123 It's already available and you already have the 1.1 release if you installed through the PYPI wheel after March 24", "Thank you very much, @Carmezim ", "I got the same error. I thought the `pip3 --upgrade install` would install 1.1 by now?\r\nI tried: \r\n\r\n```\r\nPS C:\\Users\\kmand> pip3 install --upgrade tensorflow==1.1.0\r\nCollecting tensorflow==1.1.0-rc2\r\n  Downloading tensorflow-1.1.0rc2-cp35-cp35m-win_amd64.whl\r\n...\r\ntobuf>=3.2.0->tensorflow==1.1.0-rc2)\r\nInstalling collected packages: werkzeug, tensorf\r\n  Found existing installation: tensorflow 1.0.1\r\n    Uninstalling tensorflow-1.0.1:\r\n      Successfully uninstalled tensorflow-1.0.1\r\nSuccessfully installed tensorflow-1.1.0rc2 werkzeug-0.12.1\r\n```\r\n\r\nThat looks strange!?\r\n\r\nI also get \r\n\r\n```\r\n>>> sess = tf.Session()\r\n2017-04-21 17:10:34.351513: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFl\r\now library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\nSetting log level removes warning. Thanks\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\nimport tensorflow as tf\r\n```", "i have the same error mentioned above, yet, uninstalling TF and installing the build didn't solve the problem. what else can i do? \r\nthx", "how to install GPU and CPU of tensorflow on windows 7 for 32 bit .i have  installed cuda toolkit for  win 32   and now i am not able to install GPU and CPU versions for tensor i am getting the following error after creating a virtual environment on windows 7 for tensorflow.\r\n\r\n\r\n![error](https://cloud.githubusercontent.com/assets/28622848/25944384/bb8d2e36-3660-11e7-8de5-484bd6a0482e.png)\r\n", "@sreelathasamudrala  hi, I install CPU-only version of TensorFlow via native pip3 successfully according to the official page [link](https://www.tensorflow.org/install/install_windows), however it throws `warning: 43134753` when I execute `sess = tf.Session()`, but it does not affect the following execution. May be you can try it.\r\n", "i did pip3 installation also but still getting the same error.\r\n![pip3](https://cloud.githubusercontent.com/assets/28622848/25945777/cd202572-3665-11e7-9d65-61afd687c2c5.png)\r\nI tried extracting files of tensorflow 1.0.1.rc1 ionto the same folder as i have sent in the attachment but still not getting the  execution .\r\n", "i tried to run tensorflow program without cpu and gpu  installation and i am getting errors.\r\n![tfcode](https://cloud.githubusercontent.com/assets/28622848/25945926/799fad9a-3666-11e7-9b68-6c3ca8bc9a8f.png)\r\n", "please help me guys cant download tf in win 7 using cuda 5.5 release  download of cuda toolkit for win 32", "@sreelathasamudrala Even I got the same error, and it is solved using the solution at this link http://www.stefangordon.com/install-tensorflow-in-anaconda-on-windows/ "]}, {"number": 7858, "title": "Branch 148470369", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7857, "title": "\"Expected bool, got 1 of type 'int' instead\" when taking gradient through 'greater'", "body": "While I didn't expect this to work, a better error message might be in order:\r\n\r\n```python\r\nimport tensorflow as tf\r\nx=tf.constant(1)\r\ny=tf.constant(2)\r\nz=tf.greater(x,y)\r\ntf.gradients(z,x)\r\n```\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-4df05786a146> in <module>()\r\n----> 1 tf.gradients(z,x)\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    377     ys = ops.convert_n_to_tensor_or_indexed_slices(ys, name=\"y\")\r\n    378     xs = ops.convert_n_to_tensor_or_indexed_slices(xs, name=\"x\")\r\n--> 379     grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)\r\n    380\r\n    381     # The approach we take here is as follows: Create a list of all ops in the\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)\r\n    225         grad_ys[i] = array_ops.fill(\r\n    226             array_ops.shape(y), constant_op.constant(\r\n--> 227                 1, dtype=y.dtype))\r\n    228     else:\r\n    229       if grad_y.dtype != y.dtype:\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)\r\n    163   tensor_value = attr_value_pb2.AttrValue()\r\n    164   tensor_value.tensor.CopyFrom(\r\n--> 165       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    166   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    167   const_tensor = g.create_op(\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    365       nparray = np.empty(shape, dtype=np_dt)\r\n    366     else:\r\n--> 367       _AssertCompatible(values, dtype)\r\n    368       nparray = np.array(values, dtype=np_dt)\r\n    369       # check to them.\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in _AssertCompatible(values, dtype)\r\n    300     else:\r\n    301       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\r\n--> 302                       (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n    303\r\n    304\r\n\r\nTypeError: Expected bool, got 1 of type 'int' instead.\r\n```", "comments": ["What sort of error message do you think would be more helpful, and how could it be generated?", "Closing due to lack of activity.  Please reopen if necessary."]}, {"number": 7856, "title": "[Windows] tensorboard - needs to be started from same drive as logdir", "body": "OS: Windows 10 (64 bit)\r\nTensorFlow Version: 1.0.0\r\nCUDA: 8.0\r\nGPU: yes\r\n\r\nProblem: \r\n**C**:>tensorboard --logdir=**E**:\\tmp\\tensorflow\\mnist\\logs\r\n=> tensorboard starts without loading data (not working and difficult to detect reason)\r\n\r\n**E**:>tensorboard --logdir=**E**:\\tmp\\tensorflow\\mnist\\logs\r\n=> tensorboard starts with loading data (works perfectly) \r\n\r\n\r\n", "comments": ["ditto, win7 64 tensorflow v1.0.0 no gpu.\r\n\r\nas implied by @enpasos, there are no error messages in console or web\r\n\r\nETA: with the --inspect flag tensorboard seems to be finding and reading the files fine... some but not all file reading methods are drive letter aware?", "Unfortunately TensorBoard [uses a colon](https://github.com/tensorflow/tensorflow/blob/27a98083a6c16f263d668271889863596efbeb84/tensorflow/tensorboard/backend/application.py#L581) as the separator between the optional \"run name\" and the path in the `--logdir` flag. That means that `--logdir=E:\\tmp\\tensorflow\\mnist\\logs` is interpreted as a run named `E` with path `\\tmp\\tensorflow\\mnist\\logs`, which explains why it's sensitive to the current working directory.\r\n\r\nAs a quick workaround, you could avoid this problem by always specifying an explicit run name as part of the `--logdir` flag, e.g.:\r\n\r\n```\r\ntensorboard --logdir=training:E:\\tmp\\tensorflow\\mnist\\logs\r\n```\r\n\r\n(Reassigning to @dandelionmane, who shows up on the blame for the relevant code.)", "Migrated this to tensorflow/tensorboard.", "It still can't load data though using the same driver's name\r\n", "@mrry \r\nyep ,that worked (Windows 7,64bit )\r\ntensorboard --logdir=training:E:\\tmp\\tensorflow\\mnist\\logs\r\n", "@enpasos Works on Windows 10, 64-bit with Python 3.5. Point to be noted, you should be in the drive only and not even inside the log folder.\r\n\r\nWorks only when inside D:\\> not when in the complete log path.\r\n\r\nOnly issue I had was to I had to execute it with python instead using tensorboard directly.\r\npython -m tensorboard.main --logdir=\"D:/JupyterNotebooks/MultivariateTimeSeries/logs/\"", "I resolved the problem using double-quotes:\r\n--logdir=\"E:\\tmp\\tensorflow\\mnist\\logs\"\r\n\r\nWindows 7, 64bit, cygwin, python 3", "Windows 10, Pytorch 1.4.0 GPU, \r\nI tried the method above mentioned, but did not work. \r\nIs there any other way to solve it?", "`cd {logdir}`\r\n`tensorboard --logdir ./`\r\nIt works.", "> I resolved the problem using double-quotes:\r\n> --logdir=\"E:\\tmp\\tensorflow\\mnist\\logs\"\r\n> \r\n> Windows 7, 64bit, cygwin, python 3\r\n\r\nWorked perfectly for me.\r\n- Windows 10\r\n- Intel Core i9-7920X\r\n- 2x NVIDIA GeForce RTX 2080 Ti\r\n- TensorFlow 2.4.1"]}, {"number": 7855, "title": "Layer for 3D convolution transpose", "body": "I am wondering if someone could add a conv3d_transpose layer, similar to `tf.layers.conv2d_transpose`. A `tf.nn.conv3d_transpose` op already exists. Thanks!", "comments": ["Bump", "I would also be very interested in that", "Any update on when this might be available?", "This seems to have now been implemented as `tf.nn.conv3d_transpose`:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose\r\n\r\nGreat work!"]}, {"number": 7854, "title": "mnist.input_data.read_data_sets() cause Segmentation Fault upon exiting", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your \r\nWhile I was trying the [MNIST tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py), I found my tensorflow output Segmentation Fault (Core Dumped). \r\n\r\nHowever, my mnist_softmax.py can still operates normally, outputing correct output on the terminal. It prints out the core dumped when the program exits.\r\n\r\nAfter some try and error, I think it's this line \r\n`mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)` \r\ncausing program to Core dumped upon exiting. Maybe this is a r1.0 bug?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5 LTS\r\nCUDA 8.0\r\nCuDNN 5.1.5\r\n\r\nTensorflow r1.0 installed from source:\r\n1. The commit hash: 29a6b4661258ef99842904d7c54993c963a8c2c0\r\n2. The output of `bazel version`\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n\r\n### If possible, provide a minimal reproducible example\r\nThe same code as [MNIST tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py)\r\n\r\n### What other attempted solutions have you tried?\r\nI've tried out the [Getting Started tutorial](https://www.tensorflow.org/get_started/get_started#complete_program), and it worked perfectly. \r\nI've try to comment each line to figure out which line cause the error. The result is \r\n`mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)` seems to cause the problem.\r\n\r\n### Logs or other output that would be helpful\r\n```\r\n$ python example.py\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 770\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.1105\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.95GiB\r\nFree memory: 1.66GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0)\r\n0.917\r\nSegmentation fault (core dumped)\r\n```\r\n", "comments": ["The same problem hit me", "I tried with cuda 8.0 and cudnn 5.0 on ubuntu 14, and it works fine for me on the 1.0 release version. @gunan, @zheng-xq, have you seen any reports of this happening? Could you try from take r1.0 or from a pre-build binary.", "While I was trying \"https://www.tensorflow.org/get_started/mnist/beginners\" and trying to run below commands and facing time out exception. (I am using window 7 and tensorflow v.1\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)", "@aselle I have not seen any other reports of this.\r\n@cw1204772 Could you try to get a stack trace for us to inspect?\r\n\r\n@mkamranmalik You are seeing a different error, on a different Operating system. Your report here will just confuse us when trying to help out with the actual issue reported at the top.\r\nYou need to file a new issue by filling in all the information we are requesting for us to be able to help you out.", "Back trace result using gdb\r\n```\r\n#0  0x00007ffff782dc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\r\n#1  0x00007ffff7831028 in __GI_abort () at abort.c:89\r\n#2  0x00007ffff786a2a4 in __libc_message (do_abort=do_abort@entry=1, fmt=fmt@entry=0x7ffff79786b0 \"*** Error in `%s': %s: 0x%s ***\\n\") at ../sysdeps/posix/libc_fatal.c:175\r\n#3  0x00007ffff787655e in malloc_printerr (ptr=<optimized out>, str=0x7ffff7974801 \"free(): invalid pointer\", action=1) at malloc.c:4996\r\n#4  _int_free (av=<optimized out>, p=<optimized out>, have_lock=0) at malloc.c:3840\r\n#5  0x00007fffe19b53e3 in ufunc_dealloc (ufunc=0xa4c570) at numpy/core/src/umath/ufunc_object.c:4992\r\n#6  0x000000000055a657 in ?? ()\r\n#7  0x000000000055a52f in ?? ()\r\n#8  0x000000000045b2cf in _PyImport_Fini ()\r\n#9  0x0000000000437d51 in Py_Finalize ()\r\n#10 0x000000000044f993 in Py_Main ()\r\n#11 0x00007ffff7818f45 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffdde8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, \r\n    stack_end=0x7fffffffddd8) at libc-start.c:287\r\n#12 0x0000000000578c4e in _start ()\r\n```\r\n\r\nTerminal message during execution: (only use 10 epoch for inferencing mode)\r\n```\r\nStarting program: /usr/bin/python mnist.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff3647700 (LWP 9936)]\r\n[New Thread 0x7ffff2e46700 (LWP 9937)]\r\n[New Thread 0x7ffff0645700 (LWP 9938)]\r\n[New Thread 0x7fffede44700 (LWP 9939)]\r\n[New Thread 0x7fffeb643700 (LWP 9940)]\r\n[New Thread 0x7fffe8e42700 (LWP 9941)]\r\n[New Thread 0x7fffe6641700 (LWP 9942)]\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n[Thread 0x7fffe6641700 (LWP 9942) exited]\r\n[Thread 0x7ffff3647700 (LWP 9936) exited]\r\n[Thread 0x7ffff2e46700 (LWP 9937) exited]\r\n[Thread 0x7fffe8e42700 (LWP 9941) exited]\r\n[Thread 0x7fffeb643700 (LWP 9940) exited]\r\n[Thread 0x7fffede44700 (LWP 9939) exited]\r\n[Thread 0x7ffff0645700 (LWP 9938) exited]\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\n[New Thread 0x7fffe6641700 (LWP 9945)]\r\n[New Thread 0x7fffe8e42700 (LWP 9946)]\r\n[New Thread 0x7fffeb643700 (LWP 9947)]\r\n[New Thread 0x7fffede44700 (LWP 9948)]\r\n[New Thread 0x7fffbbf85700 (LWP 9949)]\r\n[New Thread 0x7fffbb784700 (LWP 9950)]\r\n[New Thread 0x7fffbaf83700 (LWP 9951)]\r\n[New Thread 0x7fffba782700 (LWP 9952)]\r\n[New Thread 0x7fffb9f81700 (LWP 9953)]\r\n[New Thread 0x7fffb9780700 (LWP 9954)]\r\n[New Thread 0x7fffb8f7f700 (LWP 9955)]\r\n[New Thread 0x7fffb877e700 (LWP 9956)]\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 770\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.1105\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.95GiB\r\nFree memory: 1.55GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0)\r\n[New Thread 0x7fffb7f7d700 (LWP 9957)]\r\n[New Thread 0x7fffb5756700 (LWP 9958)]\r\n[New Thread 0x7fffb4f55700 (LWP 9959)]\r\n[New Thread 0x7fffb4754700 (LWP 9960)]\r\n[New Thread 0x7fffb3f53700 (LWP 9961)]\r\n[New Thread 0x7fffb3752700 (LWP 9962)]\r\n[New Thread 0x7fffb2f51700 (LWP 9963)]\r\n[New Thread 0x7fffb2750700 (LWP 9964)]\r\n[New Thread 0x7fffb1f4f700 (LWP 9965)]\r\n[New Thread 0x7fffb174e700 (LWP 9966)]\r\n[New Thread 0x7fffb09ff700 (LWP 9967)]\r\n0/1000\r\n0.458\r\n[Thread 0x7fffb7f7d700 (LWP 9957) exited]\r\n[Thread 0x7fffb5756700 (LWP 9958) exited]\r\n```", "What really boggles me is when I try to back traced today, it come up with a slightly different back traced message. (Core dumped at the exit of the program)\r\n```\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007ffff787601f in _int_free (av=0x7ffff7bb5760 <main_arena>, p=<optimized out>,\r\n    have_lock=0) at malloc.c:3996\r\n3996    malloc.c: No such file or directory.\r\n(gdb) bt\r\n#0  0x00007ffff787601f in _int_free (av=0x7ffff7bb5760 <main_arena>, p=<optimized out>,\r\n    have_lock=0) at malloc.c:3996\r\n#1  0x00000000004f7c6c in ?? ()\r\n#2  0x00000000004f78c4 in ?? ()\r\n#3  0x000000000055a657 in ?? ()\r\n#4  0x000000000059fc62 in ?? ()\r\n#5  0x0000000000507eb0 in PyDict_SetItem ()\r\n#6  0x00000000004f7f71 in _PyModule_Clear ()\r\n#7  0x00000000004c7d61 in PyImport_Cleanup ()\r\n#8  0x0000000000437d4c in Py_Finalize ()\r\n#9  0x000000000044f993 in Py_Main ()\r\n#10 0x00007ffff7818f45 in __libc_start_main (main=0x44f9c2 <main>, argc=2,\r\n    argv=0x7fffffffe438, init=<optimized out>, fini=<optimized out>,\r\n    rtld_fini=<optimized out>, stack_end=0x7fffffffe428) at libc-start.c:287\r\n#11 0x0000000000578c4e in _start ()\r\n```", "@jhseu Any ideas on this one?", "Same issue as https://github.com/tensorflow/tensorflow/issues/6968. Known issue with Ubuntu 14.04.\r\n\r\nYou can either upgrade Ubuntu or install numpy from source with OpenBLAS disabled.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7853, "title": "TypeError: concat() got an unexpected keyword argument 'axis'", "body": "Traceback (most recent call last):\r\n  File \"train_image_classifier.py\", line 585, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"train_image_classifier.py\", line 482, in main\r\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\r\n  File \"/home/cloud/hoa/workspace/tensorflows/models/slim/deployment/model_deploy.py\", line 195, in create_clones\r\n    outputs = model_fn(*args, **kwargs)\r\n  File \"train_image_classifier.py\", line 466, in clone_fn\r\n    logits, end_points = network_fn(images)\r\n  File \"/home/cloud/hoa/workspace/tensorflows/models/slim/nets/nets_factory.py\", line 105, in network_fn\r\n    return func(images, num_classes, is_training=is_training)\r\n  File \"/home/cloud/hoa/workspace/tensorflows/models/slim/nets/inception_resnet_v2.py\", line 169, in inception_resnet_v2\r\n    tower_conv2_2, tower_pool_1])\r\nTypeError: concat() got an unexpected keyword argument 'axis'\r\n\r\n\r\nWhen i run restnet or inception_resnet of slim model, this error was raised. Can you hep me?\r\nThanks.", "comments": ["I believe these types of problems should not be posted here - ask them in StackOverflow instead. Also, you need to provide more information about your system. My guess is that you're using an older version of TensorFlow.", "Thank for your support. T tried on version r0.11 and r0.12. Both version raised this error", "Use version 1.0. They changed `tf.concat` in the newer version. The current `tf.concat` was previously called `tf.concat_v2` in the older versions if I remember correctly.", "Thanks you so much. I solved this problem by the way remove 'axis' and 'values' word", "removing the 'axis' and 'values' word from where??? @tranvanhoa533 \r\n\r\nfacing the same error in another project\r\nhttps://github.com/buriburisuri/speech-to-text-wavenet/issues/48", "I saw this instruction : https://www.tensorflow.org/api_docs/python/tf/concat\r\n", "Not downgrade, we need to upgrade the system. with keras = 2.1.5 and Tensor Flow = 1.6.0 with latest anaconda version, the softmax issue is resolved. In both windows and Linux.", "Correct @sumunthra , solved after upgrading TF: '1.7.0', while Keras was: '2.1.5'\r\nThanks 4 your efforts.", "Hi ! I have upgraded both tensorflow and keras , but still I am receiving this issue , what shall I do?"]}, {"number": 7852, "title": "Fix missing backquote in tf.norm description.", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7851, "title": "Retraining the inception model to remove objects", "body": "Hello I have been trying for a while now to find an answer to this question\r\nis there is anyway to retrain the inception model in a way that I remove objects from it (without doing the training from the scratch) and to be possible to integrate in the android demo", "comments": ["This question is better suited for StackOverflow as it's not a TF bug itself. They also monitor there under TensorFlow tag and it would get a wider exposure."]}, {"number": 7850, "title": "tf.variable_scope argument with some keywords makes TensorBoard work incorrectly", "body": "**Problem:**\r\nStep 1:\r\n`with tf.variable_scope('length'):`\r\n\r\nStep2:\r\nRun Tensorboard\r\n\r\nStep3:\r\nSwitch to Graphs tab, but no graph will show.\r\n", "comments": ["Please include all the information requested in the issues template.  For a bug report, a minimal complete reproducible example is very helpful."]}, {"number": 7849, "title": "Cannot build on a non-internet connected machine", "body": "I'm trying to build tensorflow on a machine that has no internet connection.  I've downloaded the\r\nlatest bazel (.0.4.4)  and installed and then run the configure script.  I downloaded the git repo with\r\n`git clone --recurse-submodues https://github.com/tensorflow/tensorflow`\r\nThe configure then fails with:\r\n\r\nThis then fails with:\r\nINFO: Unknown host: bazel-mirror.storage.googleapis.com\r\n\r\n`ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz, https://github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz] to /tmp/steven/.cache/bazel/_bazel_kelso/03aa5bcd96ce8a24592d7d0f1cdaf43b/external/io_bazel_rules_closure/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz: All mirrors are down: [Unknown host: bazel-mirror.storage.googleapis.com, Unknown host: github.com].`\r\n\r\nThis is to be expected as the machine is on a private network.\r\n\r\nHowever, I can manually install tarbalss of packages etc on this box, but I can't work out where they should be installed within the source tree.\r\n\r\nIs this possible, or can tensorflow only be built with an active connection to the internet?\r\n\r\nMany thanks,\r\nS.\r\n\r\n", "comments": ["You can build this locally, but you will have to replace all http_archive, git_archive, etc. in workspace.bzl with corresponding local_archive definitions, pointing to where you downloaded the files to.\r\n\r\nYou may want to disable tensorboard, otherwise you'll be downloading a lot of small things.", "@martinwicke How to disable tensorboard? I am also trying to build offline, but there is too much small git repositories to download.", "You can simply take the dependency on anything tensorboard out of the\nbuild_pip_package target. That should enable you to remove all the\ntensorboard dependencies from the workspace.\n"]}, {"number": 7847, "title": "Remove unnecessary code in master_session.cc", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7846, "title": "No 'T' attr on Logical op in GraphDef", "body": "Most graph nodes have a 'T' attr in their node definition.\r\n'LogicalAnd', 'LogicalOr', and others do not in TF 1.0. Is this because these nodes only operate on bools?\r\nAdding a 'T' attr would help tool builders have a uniform interface to ops and would allow easy future extension to logical operations on integer types.\r\n\r\nExample:\r\n```python\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\nx = tf.constant(1., name='x')\r\ny = tf.constant(2., name='y')\r\nge = tf.greater(x, y, name='ge')\r\nq = tf.logical_and(ge, ge, name='q')\r\nz = tf.logical_not(q, name='z')\r\n\r\nprint(tf.get_default_graph().as_graph_def())\r\n```\r\n\r\nOutput:\r\n```\r\nnode {\r\n  name: \"x\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_FLOAT\r\n        tensor_shape {\r\n        }\r\n        float_val: 1.0\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"y\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_FLOAT\r\n        tensor_shape {\r\n        }\r\n        float_val: 2.0\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"ge\"\r\n  op: \"Greater\"\r\n  input: \"x\"\r\n  input: \"y\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"q\"\r\n  op: \"LogicalAnd\"\r\n  input: \"ge\"\r\n  input: \"ge\"\r\n}\r\nnode {\r\n  name: \"z\"\r\n  op: \"LogicalNot\"\r\n  input: \"q\"\r\n}\r\nversions {\r\n  producer: 21\r\n}\r\n```", "comments": ["There are many ops that have fixed input types and those no `T` attrs.  Others have multiple `T` attrs or a single attr with a different name.  Often generalizing ops to handle more than one type is good, but `LogicalAnd` is pretty clearly a bool thing, so it doesn't make sense in this case."]}, {"number": 7845, "title": "Getting more done in GitHub with ZenHub", "body": "Hola! @watermelonboss has created a [ZenHub](http://www.zenhub.com) account for the **tensorflow** organization. ZenHub is the only project management tool integrated natively in GitHub \u2013 created specifically for fast-moving, software-driven teams.\n\n----\n\n#### How do I use ZenHub?\n\nTo get set up with ZenHub, all you have to do is **[download the browser extension](https://www.zenhub.com?utm_source=ZHOnboarding)** and log in with your GitHub account. Once you do, you\u2019ll get access to ZenHub\u2019s complete feature-set immediately.\n\n#### What can ZenHub do?\n\nZenHub adds a series of enhancements directly inside the GitHub UI:\n\n- Real-time, customizable task boards for GitHub issues;\n- Multi-Repository burndown charts, estimates, and velocity tracking based on GitHub Milestones;\n- Personal to-do lists and task prioritization;\n- Time-saving shortcuts \u2013 like a quick repo switcher, a \u201cMove issue\u201d button, and much more.\n\n### [Add ZenHub to GitHub](https://www.zenhub.com?utm_source=ZHOnboarding)\n\n_Still curious? See [more ZenHub features](https://www.zenhub.com/features?utm_source=ZHOnboarding) or read [user reviews](https://chrome.google.com/webstore/detail/zenhub-for-github/ogcgkffhplmphkaahpmffcafajaocjbd/reviews). This issue was written by your friendly ZenHub bot, posted by request from @watermelonboss._\n\n![ZenHub Board](https://cloud.githubusercontent.com/assets/8771909/11153956/233ac4a8-89f1-11e5-94b1-1569d3f38b4d.png)\n", "comments": ["Hola!"]}]