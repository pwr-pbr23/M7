[{"number": 41651, "title": "TensorFlowLite C++ Mobile", "body": "I am a iOS developer\r\n\r\nI know you have  \r\nTensorFlowLiteSwift, 2.2.0\r\nTensorFlowLiteC,2.2.0\r\nTensorFlowLiteOC,2.2.0\r\n\r\nbut  where i can get TensorFlowLite,2.2.0?  ,this only 1.31.0 verison\r\n\r\n1.31.0 not working result right answer\r\n\r\n", "comments": ["@wanglelewang \r\nCould you please share more details as what do you want to install.\r\nPlease confirm if you have referred to [this link](https://www.tensorflow.org/lite/guide/build_ios).", "I want to  build C++ Api used iOS/Android          but  TensorFlowLite only 1.31.0 version", "eg:   In iOS  use cocoapods  \r\n\r\npod 'TensorFlowLite', '1.31.0' \r\n\r\nthis version  can not work, I try it , but result is not right", "I hope \r\npod 'TensorFlowLite', '2.2.0'", "I try build new Lite  genreal libtensorflow-lite.a  \r\nbut  I can not  found Headers.h\r\n  \r\nI use 1.31.0 version Headers  but  error", "How general  new verison tensorflow_lite.framework  in iOS ?", "https://www.tensorflow.org/lite/guide/build_ios    I read this link  \r\nbut I can not  get framework  \r\nI only get .a not get header files\r\nHow I do can get header files???", "You can give me general TensorFlowLite.frameworks 2.0.0or2.2.0 verison   readme?", "I use 0.26.1 bazel  run r2.0's  create_ios_frameworks.sh   but  cp: /Users/wanglele/Desktop/TFL/tensorflow/tensorflow/lite/lib_package/../../../bazel-genfiles/tensorflow/tools/lib_package/include/tensorflow/c/LICENSE: No such file or directory  error\r\n\r\nHow I do ?", "help ", "@teijeong could you check this?", "Starting\r\nPackage dir:  /var/folders/sz/hpsf6s1j0px40kq9qdkb4f5w0000gn/T/tmp.AByuZJY5\r\nCreating target Headers directories\r\nHeaders, populating: TensorFlow Lite\r\nHeaders, populating: Flatbuffer\r\nGenerate master LICENSE file and copy to target\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /Users/wanglele/Desktop/TFLite/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --copt=-w --config=v2\r\nINFO: Reading rc options for 'build' from /Users/wanglele/Desktop/TFLite/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python --action_env PYTHON_LIB_PATH=/Library/Python/2.7/site-packages --python_path=/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python --config=xla --action_env TF_CONFIGURE_IOS=1\r\nINFO: Found applicable config definition build:v2 in file /Users/wanglele/Desktop/TFLite/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/wanglele/Desktop/TFLite/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:xla in file /Users/wanglele/Desktop/TFLite/tensorflow/.tf_configure.bazelrc: --define with_xla_support=true\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nINFO: Analyzed target //tensorflow/tools/lib_package:clicenses_generate (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/lib_package:clicenses_generate up-to-date:\r\n  bazel-bin/tensorflow/tools/lib_package/THIRD_PARTY_TF_C_LICENSES\r\nINFO: Elapsed time: 3.129s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\ncp: /Users/wanglele/Desktop/TFLite/tensorflow/tensorflow/lite/lib_package/../../../bazel-genfiles/tensorflow/tools/lib_package/include/tensorflow/c/LICENSE: No such file or directory", "Some people?", "\uff1f", "@yyoon any idea on this?", "@wanglelewang Is there any reason why you can't use either the `TensorFlowLiteSwift` or `TensorFlowLiteObjc` pods?\r\nAs you already mentioned, the `TensorFlowLite` CocoaPod is deprecated, and no longer actively maintained.\r\n\r\nAre you trying to use the C++ API in your app?", "I want to write one code  run iOS/Andriond  ...so  I need C++ API", "may be  i can use souce code  copy in my code ??  how i do?", "> I want to write one code run iOS/Andriond ...so I need C++ API\r\n\r\nIn general, you can't save that much duplicate code even when you use the C++ API. You still need to write the remaining part of your apps, such as the UI elements, pre/post data processing logic, etc.\r\n\r\nIf you're main goal is to write a single codebase and use it in both Android/iOS, I would suggest you to take a look at Flutter and its TFLite plugin.\r\n\r\n* Flutter: https://flutter.dev/\r\n* tflite_flutter plugin: https://pub.dev/packages/tflite_flutter", "can not use C++ ?  this feel not good  ,, may be support it?", "i want to write one C++ Api   code  , be used ios/android  app .. ,  then make easy my work..", "Thanks for the feedback. We are considering supporting C++ on iOS in the future, but it won't be ready in the near future.\r\nOne alternative that you can consider right now is to use the C API, which you can use in both Android/iOS.\r\n\r\n* Android: https://www.tensorflow.org/lite/guide/android#use_tflite_c_api\r\n* iOS: Use the `TensorFlowLiteC` CocoaPod and `#include \"c_api.h\"` in your code.", "ok  thans  ,    I hope it's faster\uff0c", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41651\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41651\">No</a>\n", "> Thanks for the feedback. We are considering supporting C++ on iOS in the future, but it won't be ready in the near future.\r\n> One alternative that you can consider right now is to use the C API, which you can use in both Android/iOS.\r\n> \r\n> * Android: https://www.tensorflow.org/lite/guide/android#use_tflite_c_api\r\n> * iOS: Use the `TensorFlowLiteC` CocoaPod and `#include \"c_api.h\"` in your code.\r\n\r\nI think tensorflowlite support C++ API in camera demo:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/ios/camera\r\nWhy not continue supporting?"]}, {"number": 41650, "title": "Dangerous data format default in SSIM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Linux 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 10 \r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n[SSIM](https://www.tensorflow.org/api_docs/python/tf/image/ssim) assumes that the image batches have a shape (h, w, batch_size). This is not documented anywhere and the only reason I was aware of it is when I had a batch size smaller than 11, which triggers the following assertion to fail:\r\n``` Python\r\n  checks = [\r\n      control_flow_ops.Assert(\r\n          math_ops.reduce_all(\r\n              math_ops.greater_equal(shape1[-3:-1], filter_size)),\r\n          [shape1, filter_size],\r\n          summarize=8),\r\n      control_flow_ops.Assert(\r\n          math_ops.reduce_all(\r\n              math_ops.greater_equal(shape2[-3:-1], filter_size)),\r\n          [shape2, filter_size],\r\n          summarize=8)\r\n  ]\r\n``` \r\nBecause:\r\n```\r\n>>> shape = (4, 255, 255)\r\n>>> shape[-3:-1]\r\n(4, 255)\r\n```\r\nand `filter_size = 11`\r\n\r\n**Describe the expected behavior**\r\nThis behaviour should be documented.\r\nThe user should have the choice to choose the data format : bath size first or last. By default, I think it should be batch size first as it's the way tf.data.Dataset handles the input pipeline. \r\n\r\n**Standalone code to reproduce the issue**\r\nThis works\r\n```\r\nshape = (255, 255, 4)\r\nimg1 = tf.zeros(shape)\r\nimg2 = tf.zeros(shape)\r\ntf.image.ssim(\r\n    img1, img2, 1\r\n)\r\n```\r\nBut this doesn't\r\n```\r\nshape = (4, 255, 255)\r\nimg1 = tf.zeros(shape)\r\nimg2 = tf.zeros(shape)\r\ntf.image.ssim(\r\n    img1, img2, 1\r\n)\r\n```\r\n", "comments": ["@khazit \r\n\r\nI have tried in colab with TF version 2.2,2.3-rc1.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b978e4ca53d728e584baa76ce67bfc4b/untitled177.ipynb).You are refering to same behavior?\r\nThanks!", "@ravikyram  Yes it's the same behavior", "@jvishnuvardhan any update on this ? I'm willing to work on it.", "I had the same question these days. After digging into the code \r\nhttps://github.com/tensorflow/tensorflow/blob/14246bd0280d1c5e5564eedd4927bf9af9339967/tensorflow/python/ops/image_ops_impl.py#L4307\r\n, it turns out that the implementation assumes the input data has a shape of `(batch size, h, w, channel)`. It calculate the ssim per channel and takes the average for each image, i.e the return is expected to have a shape of `(batch size,)`. \r\n\r\nHowever, if the input data happens to have a shape of `(batch size, h, w)`. It simply assume the input data has only 1 batch with a shape of `(h, w, channel)` and continue with the calculation. If the `batch_size` happens to be smaller than the `filter_size`, an exception will be raised during the shape check.\r\n\r\nI would still call the implementation dangerous since people will get the wrong result with an input data shape of `(batch size, h, w)`, when the `batch_size` is larger than the `filter_size`. It could still work nicely during training as a loss function (1 - ssim).\r\n\r\nFinally, the correct way is to reshape the data to `(batch size, h, w, 1)`.", "@khazit Example in the doc was updated to clearly mention the data shape required for this function. Please check the updated example [here](https://www.tensorflow.org/api_docs/python/tf/image/ssim?version=nightly). Thanks for raising this issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ee5449eed95302b912685d249440cb1d/untitled177.ipynb)\r\n\r\nExpected shape of a batch image is (batch_size, height, width, channels). The following works fine.\r\n\r\n```\r\nshape = (4, 255, 255,3)\r\nimg1 = tf.zeros(shape)\r\nimg2 = tf.zeros(shape)\r\ntf.image.ssim(\r\n    img1, img2, 1\r\n)\r\n\r\n\r\noutput \r\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>\r\n```\r\n\r\nI am closing this issue. Please feel free to reopen if I am mistaken. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41650\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41650\">No</a>\n"]}, {"number": 41649, "title": "Help!!!!!!!!!!   tf.keras.layers.Conv2d and slim.conv2d have different result(Though slim.conv2d had been deprecated)", "body": "TF 1.15.0\r\nUbuntu 16.0\r\npython 3.7.6\r\nwith same parameter(kernel , bias, padding etc.) tf.keras.layers.Conv2d and slim.conv2d have different output. Though the different is little, about 0.000001,  when i copy the model parameters training with slim.conv2d to the model builded with tf.keras.layers.Conv2d, I got a completely output", "comments": ["@zihao-lu,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41648, "title": "tf.data.Dataset - You must feed a value for placeholder tensor", "body": "**System information**\r\n- OS: Ubuntu 18.04.4 LTS (also tested in Windows 10 obtaining the same behaviour)\r\n- TensorFlow installed from: pypi, pip install tf-nightly\r\n- TensorFlow version (use command below): 2.4.0-dev20200718\r\n- Python version: 3.6.9 \r\n \r\n2020-07-23 09:58:13.497096: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nv1.12.1-36900-g829256e314 2.4.0-dev20200718\r\n\r\n**Describe the current behavior**\r\nComputations involving tf.data.Dataset stall. \r\nIf the dataset is small the execution continues properly but the error is still registered in the server logs.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nExcerpt from: https://www.tensorflow.org/tutorials/load_data/images\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pathlib\r\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\r\ndata_dir = tf.keras.utils.get_file(origin=dataset_url, \r\n                                   fname='flower_photos', \r\n                                   untar=True)\r\ndata_dir = pathlib.Path(data_dir)\r\nlist_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)\r\n# Triggers the abort message\r\nfor f in list_ds.take(5):\r\n    print(f.numpy()) \r\n```\r\n\r\n**Jupyter lab server log**\r\n\r\n**2020-07-23 09:52:42.605408: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [3670]\r\n         [[{{node Placeholder/_0}}]]**\r\n", "comments": ["@leaguilar \r\nCan you please refer to [this link](https://stackoverflow.com/questions/40521295/invalid-argument-you-must-feed-a-value-for-placeholder-tensor-with-dtype-float) and let us know if it helps.\r\nI ran your code on nightly and do not face any error, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/5c23ca410e27dc9635e844d1278b0f37/untitled284.ipynb)\r\n\r\n\r\n\r\n", "@Saduf2019 \r\nThanks for having a look. \r\n\r\nYes, I found the discussion you refer to in the link while trying to solve the issue, but the minimal code I posted doesn't use any feed_dict. \r\n\r\nRegarding your collab gist. The error message is logged on the server-side, I have experienced it with a Jupyter lab server both in Linux and Windows. On the client-side (the notebook) no error is logged **(as seen from your colab gist)**. I have no idea if there is a way of reading the server-side log in colab or if there is a way of redirecting that error output to the notebook. So there is no way to know if the error is happening in colab too. \r\n\r\nThe minimal code I posted is enough to consistently generate the error message in two of my systems but doesn't produce the stalling behaviour.  As I mentioned, with small enough computations seems like the abort \"signal\" is triggered after the computation finished with no effect, i.e. no stalling. I believe that the error message should be enough to track down the error.", "@Saduf2019 \r\nAfter further inspection, the error does get logged in colab's server-side too (using your gist).\r\n\r\nRuntime > View runtime logs\r\n\r\n![bug](https://user-images.githubusercontent.com/13425173/88288795-914c3a80-ccf4-11ea-940b-ff73cb7e0daf.png)", "I also faced the same issue here, any update?", "I just upgraded my computer so I had to install a new system now my own code I used to have it worked with no issues started giving me this error message today. Not sure why!!!!", "@MarkDaoust could you please take a look? thank you", "I found this issue only happens if you are using tf-nightly. Switching to tensorflow 2.3 resolved the issue", "> I found this issue only happens if you are using tf-nightly. Switching to tensorflow 2.3 resolved the issue\r\n\r\nAgreed, I can get the warning to pop up with the replication instructions and this nightly:\r\n\r\n```\r\n!pip install tf-nightly==2.4.0.dev20200722\r\n```\r\n\r\nBut not with the current nightly, or with the stable release, so whatever this problem was it seems to have been fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41648\">No</a>\n"]}, {"number": 41647, "title": "Can you optimize the text.text_to_word_sequence API\uff1f", "body": "**System information**\r\n- TensorFlow version (you are using):    2.2.0\r\n- Are you willing to contribute it (Yes/No):  Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n  \r\n   I want to use ```tf.keras.preprocessing.text.text_to_word_sequence(s, filters='{regex}', split='{regex}')  ```  API to split and filter text. but ```filter``` paramter mapping ```split``` paramter. I just want to ```split``` paramter only split and ```filters``` paramter only filter. It best supports ```regex``` from all paramters.eg:\r\n```python\r\nimport tensorflow as tf\r\ns = '\u8be5\u6587\u4ef6\u5939\u5305\u542b\uff0c\u8be5\u6587\u4ef6\u5b89\u88c5\u4e86\u5b89\u88c5\u4e86Keras2.1.5\u73af\u5883\u3002\u8be5\u73af\u5883\u53ef\u7528Docker'\r\ntext_to_word_sequence = tf.keras.preprocessing.text.text_to_word_sequence(s, filters='[0-9.]', split='[\uff0c\u3002]')\r\nprint(text_to_word_sequence)\r\n# ['\u8be5\u6587\u4ef6\u5939\u5305\u542b','\u8be5\u6587\u4ef6\u5b89\u88c5\u4e86\u5b89\u88c5\u4e86Keras\u73af\u5883','\u8be5\u73af\u5883\u53ef\u7528Docker']\r\n```", "comments": ["@jtyoui The reason why filter is apply first and then split is to optimize the process as filtering removes the records that are not necessary and then split is applied.\r\n\r\nCan you please let us know how applying split and filter separately is gonna help the community. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41646, "title": "How to cut off gradient in tensorflow2.0", "body": "I want to cut off gradient when training. For example: \r\n![image](https://user-images.githubusercontent.com/20520524/88247306-db6ef500-cccf-11ea-9da1-d53e5e27ebc6.png)\r\nIn picture, there are six ops. I want to loss7 only influence op5 and op6, so I need to cut off grandient flow between op2 and op5. \r\nDo you konw how to do it. I am looking forward your answer. thanks\r\n ", "comments": ["Is `tf.stop_gradient(op5)` what you're looking for?", "yes\uff0c**tf.stop_gradient(op5)** is what I need , Thank you very much."]}, {"number": 41645, "title": "[INTEL MKL] Changes to oneDNN build to remove binary blob when building with opensource components only.", "body": "The PR includes the following changes.\r\n1) Changes to the mkl_threapool build so it doesn't depend on the MKL binary blob.\r\n2) Added  the mkl_opensource_only config option that enables building of TF+oneDNN without the MKL binary blob.\r\n3) Cleaning up obsolete code.", "comments": ["@agramesh1 Can you please check build failures. Thanks!", "> @agramesh1 Can you please check build failures. Thanks!\r\n\r\n@gbaned thank you, I will take a look at it."]}, {"number": 41644, "title": "Updating estimator version after estimator final release", "body": "", "comments": []}, {"number": 41643, "title": "tf-slim ResNet v1 pre-trained models preprocessing", "body": "### Affected Doc URL\r\nhttps://github.com/tensorflow/models/blob/master/research/slim/README.md#pre-trained-models\r\n\r\n### Description\r\n\r\nBoth tf-slim VGG and Inception preprocessing cause the ResNet v1 models in the above link to have incorrect outputs.  The other ImageNet models run correctly with either the VGG or Inception preprocessing, however under the same code path, ResNet v1 models produce incorrect outputs.  This issue is also documented in [this unresolved issue](https://github.com/tensorflow/tensorflow/issues/17426).\r\n\r\nAre we aware of the correct steps to take to correctly run slim's ResNet v1 models and could this be updated in the documentation?\r\n\r\nThank you.", "comments": ["Hi @ymodak we have found that the model expects inputs not to be scaled by 255, however this does not get the expected top1 accuracy (67% vs 75% on resnet v1 50).  Could you point us to any information or documentation on the proper preprocessing?  Thank you.", "@neuralmagic Thanks for the issue and sharing your investigation. I think this issue is better raised on [tf-slim repo](https://github.com/google-research/tf-slim/issues) since the  [tf-slim VGG and Inception preprocessing](https://github.com/tensorflow/models/tree/master/research/slim/preprocessing) code is hosted on `tf-slim`.\r\nThe folks of `tf-slim` can provide a better guidance in this regard.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you @ymodak.  We will follow up there."]}, {"number": 41642, "title": "[ TF 2.0 ] tf.function throws error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): pip install tensorflow==2.0.0\r\n- Python version:3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI ported n gram computation for truecasing to tf-2.0 and it works fine when I run the function as standalone but fails when I try to wrap the function with tf.function. There is a get_score function that creates unigram/bigram combinations. When the function runs standalone with string tensor as input, everything works as expected\r\n\r\n**Describe the expected behavior**\r\n\r\nThe tf.function wrapper is needed so that the model can be exported to run with tf.serving. The code functions correctly when its provided an input. But the code fails with the error attached when tf.function wrapper is added\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n    def get_score(self, prev_token, possible_token, next_token):\r\n        possible_token_l = tf.strings.lower(possible_token)\r\n        alternative_tokens = self.get_alternative_tokens(possible_token_l)\r\n\r\n        unigram_score = self.compute_unigram_score(possible_token, alternative_tokens)\r\n        result = tf.math.log(unigram_score)\r\n\r\n        if prev_token is not None:\r\n            bigram_backward_score = self.compute_bigram_backward_score(possible_token, prev_token, alternative_tokens)\r\n            result += tf.math.log(bigram_backward_score)\r\n\r\n        if next_token is not None:\r\n            bigram_forward_score = self.compute_bigram_forward_score(possible_token, next_token, alternative_tokens)\r\n            result += tf.math.log(bigram_forward_score)\r\n\r\n        if prev_token is not None and next_token is not None:\r\n            trigram_score = self.compute_trigram_score(possible_token, prev_token, next_token, alternative_tokens)\r\n            result += tf.math.log(trigram_score)\r\n        return result\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None), dtype=tf.string, name=\"input_text\")])\r\n    def get_true_case(self, tokens_tensor):\r\n        cap_first_token = tf.reshape(tokens_tensor[0], [1])\r\n        trueCasedTokens = cap_first_token\r\n        tokens_tensor = tf.slice(tokens_tensor, [1], [-1])\r\n\r\n        condition = lambda tokens_tensor, trueCasedTokens: tf.greater(tf.size(tokens_tensor), 0)\r\n\r\n        def body(tokens_tensor, trueCasedTokens):\r\n            cur_tokens_tensor = tf.concat([trueCasedTokens[-1:], tokens_tensor], 0)\r\n            curToken = tf.get_static_value(tf.slice(cur_tokens_tensor, [1], [1]))[0]\r\n            curToken = tf.get_static_value(tf.strings.lower(curToken))\r\n\r\n            prevToken = tf.slice(cur_tokens_tensor, [0], [1])\r\n            if tf.get_static_value(tf.greater(tf.size(cur_tokens_tensor), [3]))[0]:\r\n                nextToken = tf.slice(cur_tokens_tensor, [2], [1])\r\n            else:\r\n                nextToken = None\r\n\r\n            wordCasingLookup = tf.reshape(self.get_alternative_tokens(tf.constant(curToken)), [-1])\r\n            if tf.get_static_value(tf.equal(tf.size(wordCasingLookup), [0]))[0]:\r\n                trueCasedTokens = tf.concat([trueCasedTokens, tf.constant(curToken)], 0)\r\n            if tf.get_static_value(tf.equal(tf.size(wordCasingLookup), [1]))[0]:\r\n                trueCasedTokens = tf.concat([trueCasedTokens, wordCasingLookup], 0)\r\n            else:\r\n                scores = tf.map_fn(lambda x: self.get_score(prevToken, x, nextToken),\r\n                                   wordCasingLookup, dtype=tf.float32)\r\n                maxElementIndx = tf.get_static_value(tf.argmax(scores))[0]\r\n                trueVariant = tf.slice(wordCasingLookup, [maxElementIndx], [1])\r\n                trueCasedTokens = tf.concat([trueCasedTokens, trueVariant], 0)\r\n\r\n            tokens_tensor = tf.slice(tokens_tensor, [1], [-1])\r\n            return tokens_tensor, trueCasedTokens\r\n\r\n        res = tf.while_loop(condition,\r\n                            lambda tokens_tensor, trueCasedTokens: body(tokens_tensor, trueCasedTokens),\r\n                            [tokens_tensor, trueCasedTokens])                    \r\n        res = res[1]\r\n        res = tf.gather(res, tf.where(res != b''))\r\n\r\n        return res\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n  File \"/Users/ayushc/transcript-post-processor/truecaser.py\", line 67, in truecase_tokenize\r\n    res = tf_model.get_true_case(tokens_tensor)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2658, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n\r\n    ./truecaser/truecaser_tf.py:160 body  *\r\n        curToken = tf.get_static_value(tf.slice(cur_tokens_tensor, [1], [1]))[0]\r\n    /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2478 while_loop_v2\r\n        return_same_structure=True)\r\n    /var/folders/4j/sdgl7s9j3w51c50b5wy4v_p00000gn/T/tmpw4oa0q3i.py:21 body\r\n        curToken = ag__.converted_call(tf.get_static_value, body_scope.callopts, (ag__.converted_call(tf.slice, body_scope.callopts, (cur_tokens_tensor, [1], [1]), None, body_scope),), None, body_scope)[0]\r\n\r\n    TypeError: 'NoneType' object is not subscriptable\r\n```", "comments": ["@ayushch3 \r\n\r\nI think code is incomplete.Request you to provide colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram I added all the relevant code in this git: https://github.com/ayushch3/truecaser_tf\r\n\r\nIf you try to run export_truecaser file, you will see all the relevant errors. The code basically needs to export the files that can be used directly in tf-serving", "@ayushch3 \r\n\r\nI think en_`truecasing_model.ob file is missing.Please, share the file to reproduce the issue.\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here.](https://github.com/tensorflow/serving/issues) Thanks!\r\n", "@ravikyram the link to the truecasing_model.obj file is in readme: https://drive.google.com/file/d/1DpmsDYm-gzcwXCJT4sMCUXmtHGSxKiIR/view?usp=sharing\r\n\r\nSharing it again just in case. I don't think its an issue with the serving itself, I haven't been able to export the model in the first place using tf.function wrapper. I have looked at bunch of configurations for the signature definition for tf.function, but none of them seem to work. The file that needs to run to export the model is export_truecaser.py from the git file. I'd appreciate if you can give any inputs on what's causing that behavior", "@ayushch3 \r\n\r\nI tried in colab with TF 2.2 .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/7d16a2073ea5ba72a49054084c91411a/untitled178.ipynb).Thanks!", "@ravikyram I added the requirements file so that our environments can match: https://github.com/ayushch3/truecaser_tf/blob/master/requirements.txt\r\n\r\nI am able to reproduce the same issue with these version of libraries as stated above. Let me know if you have any other issues running the code", "Also adding a try/except block in the body function within get_true_case leads to this error:\r\n\r\n```\r\n'NoneType' object is not subscriptable\r\nTraceback (most recent call last):\r\n  File \"export_model_truecaser.py\", line 26, in <module>\r\n    tf.TensorSpec(shape=(None), dtype=tf.string, name=\"input_text\"))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 959, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 865, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3299, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    ./truecaser/truecaser_tf.py:186 get_true_case  *\r\n        res = tf.while_loop(condition,\r\n    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:574 new_func  **\r\n        return func(*args, **kwargs)\r\n    /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2491 while_loop_v2\r\n        return_same_structure=True)\r\n    /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop\r\n        back_prop=back_prop)\r\n    /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:196 while_loop\r\n        add_control_dependencies=add_control_dependencies)\r\n    /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:180 wrapped_body\r\n        expand_composites=True)\r\n    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/nest.py:385 assert_same_structure\r\n        % (str(e), str1, str2))\r\n\r\n    ValueError: The two structures don't have the same nested structure.\r\n    \r\n    First structure: type=list str=[None]\r\n    \r\n    Second structure: type=list str=[<tf.Tensor 'Slice:0' shape=(None,) dtype=string>, <tf.Tensor 'Reshape:0' shape=(1,) dtype=string>]\r\n    \r\n    More specifically: The two structures don't have the same number of elements. First structure: type=list str=[None]. Second structure: type=list str=[<tf.Tensor 'Slice:0' shape=(None,) dtype=string>, <tf.Tensor 'Reshape:0' shape=(1,) dtype=string>]\r\n    Entire first structure:\r\n    [.]\r\n    Entire second structure:\r\n    [., .]\r\n```", "@gowthamkpr any update on this issue what's causing this error when using the tf.function wrapper", "Hi, I'm trying to run the reproduction code but keep running into various errors. At first the cell told me about a missing en_truecasing_model.obj, then after I found the link and downloaded it, I'm getting this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/content/truecaser_tf/export_truecaser.py\", line 20, in <module>\r\n    wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist = load_truecasing_model(truecaser_weights)\r\n  File \"/content/truecaser_tf/export_truecaser.py\", line 11, in load_truecasing_model\r\n    uni_dist = pickle.load(bin_file)\r\nValueError: could not convert string to int\r\n```\r\n\r\nPlease place the code to reproduce the bug into a single colab notebook, that can run independently, without needing to download any other files. For example, I doubt we need the saved model part and can just test the model with some random inputs.", "I finally got it to work, it seems that it takes a while for the file to upload in colab. So I'm running the code in TF 2.3 and it runs without error. For reference, this is the code I ran:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass NGramTF(tf.Module):\r\n    def __init__(self):\r\n        self.oov_score = 0\r\n        self.pseudo_count = 5.0\r\n        pass\r\n\r\n    def fit_tf_lookup_table(self, dist):\r\n        keys_tensor = tf.constant(list(dist.keys()))\r\n        vals_tensor = tf.constant(list(dist.values()))\r\n        initializer = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)\r\n        table = tf.lookup.StaticHashTable(initializer, self.oov_score)\r\n        return table\r\n\r\n    def fit_word_casing(self, wordCasingLookup):\r\n        indices = []\r\n        values = []\r\n        tokens = []\r\n        for idx, (token, items) in enumerate(wordCasingLookup.items()):\r\n            tokens.append(token)\r\n            for j, item in enumerate(items):\r\n                indices.append([idx, j])\r\n                values.append(item)\r\n        word_casing_lookup_shape = [len(tokens), max([len(item) for item in wordCasingLookup.values()])]\r\n        word_casing_lookup_tf = tf.SparseTensor(indices=indices, values=values, dense_shape=word_casing_lookup_shape)\r\n        word_casing_indices = tf.range(0, len(tokens))\r\n        dense_word_casing_lookup_tf = tf.sparse.to_dense(word_casing_lookup_tf, default_value='')\r\n\r\n        initializer = tf.lookup.KeyValueTensorInitializer(tf.constant(tokens), word_casing_indices)\r\n\r\n        # TODO: this does ot work\r\n        table = tf.lookup.StaticHashTable(initializer, -1)\r\n        return table, dense_word_casing_lookup_tf\r\n\r\n    def fit(self, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist):\r\n        self.word_casing, self.word_casing_lookup = self.fit_word_casing(wordCasingLookup)\r\n        self.tf_uni_dist = self.fit_tf_lookup_table(uniDist)\r\n        self.backwardBiDist = self.fit_tf_lookup_table(backwardBiDist)\r\n        self.forwardBiDist = self.fit_tf_lookup_table(forwardBiDist)\r\n        self.trigramDist = self.fit_tf_lookup_table(trigramDist)\r\n\r\n    def get_unigram_score(self, possible_token):\r\n\r\n        input_tensor = tf.constant(possible_token)\r\n\r\n        nominator = tf.dtypes.cast(self.tf_uni_dist.lookup(input_tensor), tf.float32) + self.pseudo_count\r\n\r\n        input_tensor_lower = tf.strings.lower(input_tensor)\r\n        idx = self.word_casing.lookup(input_tensor_lower)\r\n        alternative_tokens = self.word_casing_lookup[idx]\r\n        alternative_tokens = tf.gather(alternative_tokens, tf.where(alternative_tokens != b''))\r\n\r\n        # t_idx = self.word_casing_lookup.indices[idx]\r\n        # alternative_tokens = tf.gather(self.word_casing_lookup.values, t_idx)\r\n\r\n        denominator = self.tf_uni_dist.lookup(alternative_tokens)\r\n        denominator_sum = tf.math.reduce_sum(tf.dtypes.cast(denominator, tf.float32) + self.pseudo_count)\r\n        unigram_score = nominator / denominator_sum\r\n\r\n        return unigram_score\r\n\r\n    def get_alternative_tokens(self, possible_token_tensor):\r\n        idx = self.word_casing.lookup(possible_token_tensor)\r\n\r\n        def f1(): return tf.constant([], dtype=tf.dtypes.string)\r\n\r\n        def f2(): return tf.gather(self.word_casing_lookup[idx[0]], tf.where(self.word_casing_lookup[idx[0]] != b''))\r\n\r\n        alternative_tokens = tf.cond(tf.equal(idx, [-1]), f1, f2)\r\n        return alternative_tokens\r\n\r\n    def compute_unigram_score(self, possible_token_tensor, alternative_tokens):\r\n        nominator = tf.dtypes.cast(self.tf_uni_dist.lookup(possible_token_tensor), tf.float32) + self.pseudo_count\r\n        denominator = self.tf_uni_dist.lookup(alternative_tokens)\r\n        denominator_sum = tf.math.reduce_sum(tf.dtypes.cast(denominator, tf.float32) + self.pseudo_count)\r\n        score = nominator / denominator_sum\r\n        return score\r\n\r\n    def compute_bigram_backward_score(self, possible_token_tensor, prev_token_tensor, alternative_tokens):\r\n        x = prev_token_tensor + tf.constant('_') + possible_token_tensor\r\n        nominator = tf.dtypes.cast(self.backwardBiDist.lookup(x), tf.float32) + self.pseudo_count\r\n        alternative_tokens_m = prev_token_tensor + tf.constant('_') + alternative_tokens\r\n        denominator = self.backwardBiDist.lookup(alternative_tokens_m)\r\n        denominator_sum = tf.math.reduce_sum(tf.dtypes.cast(denominator, tf.float32) + self.pseudo_count)\r\n        score = nominator / denominator_sum\r\n        return score\r\n\r\n    def compute_bigram_forward_score(self, possible_token_tensor, next_token_tensor, alternative_tokens):\r\n        x = possible_token_tensor + tf.constant('_') + tf.strings.lower(next_token_tensor)\r\n        nominator = tf.dtypes.cast(self.forwardBiDist.lookup(x), tf.float32) + self.pseudo_count\r\n        alternative_tokens_m = alternative_tokens + tf.constant('_') + tf.strings.lower(next_token_tensor)\r\n        denominator = self.forwardBiDist.lookup(alternative_tokens_m)\r\n        denominator_sum = tf.math.reduce_sum(tf.dtypes.cast(denominator, tf.float32) + self.pseudo_count)\r\n        score = nominator / denominator_sum\r\n        return score\r\n\r\n    def compute_trigram_score(self, possible_token_tensor, prev_token_tensor, next_token_tensor, alternative_tokens):\r\n        x = prev_token_tensor + tf.constant('_') + possible_token_tensor + tf.constant('_') + tf.strings.lower(\r\n            next_token_tensor)\r\n        nominator = tf.dtypes.cast(self.trigramDist.lookup(x), tf.float32) + self.pseudo_count\r\n        alternative_tokens_m = prev_token_tensor + tf.constant('_') + alternative_tokens + tf.constant(\r\n            '_') + tf.strings.lower(next_token_tensor)\r\n        denominator = self.trigramDist.lookup(alternative_tokens_m)\r\n        denominator_sum = tf.math.reduce_sum(tf.dtypes.cast(denominator, tf.float32) + self.pseudo_count)\r\n        score = nominator / denominator_sum\r\n        return score\r\n\r\n    def get_score(self, prev_token, possible_token, next_token):\r\n\r\n        possible_token_l = tf.strings.lower(possible_token)\r\n        alternative_tokens = self.get_alternative_tokens(possible_token_l)\r\n\r\n        unigram_score = self.compute_unigram_score(possible_token, alternative_tokens)\r\n        result = tf.math.log(unigram_score)\r\n\r\n        if prev_token is not None:\r\n            bigram_backward_score = self.compute_bigram_backward_score(possible_token, prev_token, alternative_tokens)\r\n            result += tf.math.log(bigram_backward_score)\r\n\r\n        if next_token is not None:\r\n            bigram_forward_score = self.compute_bigram_forward_score(possible_token, next_token, alternative_tokens)\r\n            result += tf.math.log(bigram_forward_score)\r\n\r\n        if prev_token is not None and next_token is not None:\r\n            trigram_score = self.compute_trigram_score(possible_token, prev_token, next_token, alternative_tokens)\r\n            result += tf.math.log(trigram_score)\r\n        return result\r\n\r\n    def capitalize_str(self, token_tensor):\r\n        token_tensor = tf.reshape(token_tensor, [1])\r\n        char_tensor = tf.compat.v1.string_split(token_tensor, delimiter='').values\r\n\r\n        first_char = char_tensor[0]\r\n        first_char_cap = tf.strings.upper(first_char)\r\n\r\n        cap_char_tensor = tf.concat([tf.reshape(first_char_cap, [1]), char_tensor[1:]], 0)\r\n        cap_tensor = tf.strings.reduce_join(cap_char_tensor)\r\n        return tf.reshape(cap_tensor, [1])\r\n\r\n    @tf.function\r\n    def get_true_case(self, tokens_tensor):\r\n        cap_first_token = self.capitalize_str(tokens_tensor[0:1])\r\n        cur_token0 = tokens_tensor[0:1]\r\n\r\n        true_cased_tokens0 = cap_first_token\r\n        tokens_tensor0 = tokens_tensor[1:-1]\r\n        i0 = tf.constant(1)\r\n        condition = lambda i, true_cased_tokens, tokens_tensor: tf.less_equal(i, tf.size(tokens_tensor0) + 1)\r\n\r\n        def body(i, true_cased_tokens, tokens_tensor):\r\n            cur_token = tokens_tensor[i: i + 1]\r\n            prev_token = true_cased_tokens[-1:]\r\n\r\n            def f1(): return tokens_tensor[i + 1: i + 2]\r\n\r\n            def f2(): return tf.constant([b''])\r\n\r\n            next_token = tf.cond(tf.less(i, tf.size(tokens_tensor)),\r\n                                 f1,\r\n                                 f2)\r\n            ind = self.word_casing.lookup(cur_token)\r\n            word_casing_lookup = self.get_alternative_tokens(cur_token)\r\n\r\n            def f_true_cased_0():\r\n                return cur_token\r\n\r\n            def f_true_cased_1():\r\n                return word_casing_lookup[0]\r\n\r\n            cur_token_transformed = tf.cond(tf.equal(tf.size(word_casing_lookup), 0),\r\n                                            f_true_cased_0,\r\n                                            f_true_cased_1)\r\n\r\n            def f_return_cur():\r\n                return cur_token_transformed\r\n\r\n            def f_find_best():\r\n                scores = tf.map_fn(lambda x: self.get_score(prev_token, x, next_token),\r\n                                   word_casing_lookup, dtype=tf.float32)\r\n                max_el_ind = tf.argmax(scores)\r\n                truecased_token = word_casing_lookup[max_el_ind[0]: max_el_ind[0] + 1]\r\n                return truecased_token[0]\r\n\r\n            cur_token_transformed = tf.cond(tf.greater(tf.size(word_casing_lookup), 1), f_find_best, f_return_cur)\r\n\r\n            true_cased_tokens = tf.concat([true_cased_tokens, cur_token_transformed], 0)\r\n\r\n            return [tf.add(i, 1), true_cased_tokens, tokens_tensor]\r\n\r\n        res = tf.while_loop(condition,\r\n                            body,\r\n                            [i0, true_cased_tokens0, tokens_tensor],\r\n                            shape_invariants=[tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([None])])\r\n        return res[1]\r\n\r\nimport pickle\r\n\r\ndef load_truecasing_model(model_filename):\r\n    with open(model_filename, 'rb') as bin_file:\r\n        uni_dist = pickle.load(bin_file)\r\n        backward_bi_dist = pickle.load(bin_file)\r\n        forward_bi_dist = pickle.load(bin_file)\r\n        trigram_dist = pickle.load(bin_file)\r\n        word_casing_lookup = pickle.load(bin_file)\r\n        return word_casing_lookup, uni_dist, backward_bi_dist, forward_bi_dist, trigram_dist\r\n\r\ntruecaser_weights = 'en_truecasing_model.obj'\r\nwordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist = load_truecasing_model(truecaser_weights)\r\n\r\nmodel = NGramTF()\r\nexport_path = './truecaser_serving/1/'\r\nmodel.fit(wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist)\r\n\r\nsignature_def = model.get_true_case.get_concrete_function(tf.TensorSpec(shape=(None), dtype=tf.string, name=\"input_text\"))\r\ntf.saved_model.save(model,export_path,signatures={'serving_default': signature_def})\r\n```\r\n\r\n@ayushch3 , could you verify that this works in 2.3 on your end as well?", "On a related note, you can use AutoGraph in tf.function to write the control flow in a more Pyhtonic form. I rewrote the function as an example. I only tested that it works without error, please verify it on actual data before using it:\r\n\r\n```\r\n    @tf.function\r\n    def get_true_case(self, tokens_tensor):\r\n        cap_first_token = self.capitalize_str(tokens_tensor[0:1])\r\n        true_cased_tokens = cap_first_token\r\n        tokens_tensor = tokens_tensor[1:-1]\r\n\r\n        i = tf.constant(1)\r\n        while i <= (tf.size(tokens_tensor) + 1):\r\n            tf.autograph.experimental.set_loop_options(\r\n                shape_invariants=(\r\n                    (true_cased_tokens, tf.TensorShape([None])),\r\n                )\r\n            )\r\n\r\n            cur_token = tokens_tensor[i: i + 1]\r\n            prev_token = true_cased_tokens[-1:]\r\n\r\n            if i < tf.size(tokens_tensor):\r\n              next_token = tokens_tensor[i + 1: i + 2]\r\n            else:\r\n              next_token = tf.constant([b''])\r\n\r\n            ind = self.word_casing.lookup(cur_token)\r\n            word_casing_lookup = self.get_alternative_tokens(cur_token)\r\n\r\n            if tf.size(word_casing_lookup) == 0:\r\n              cur_token_transformed = cur_token\r\n            else:\r\n              cur_token_transformed = word_casing_lookup[0]\r\n\r\n            if tf.size(word_casing_lookup) > 1:\r\n                scores = tf.map_fn(lambda x: self.get_score(prev_token, x, next_token),\r\n                                   word_casing_lookup, dtype=tf.float32)\r\n                max_el_ind = tf.argmax(scores)\r\n                truecased_token = word_casing_lookup[max_el_ind[0]: max_el_ind[0] + 1]\r\n                cur_token_transformed = truecased_token[0]\r\n\r\n            true_cased_tokens = tf.concat([true_cased_tokens, cur_token_transformed], 0)\r\n            i += 1\r\n\r\n        return true_cased_tokens\r\n```", "@ayushch3  Seems the issue is resolved in latest TF version. Please feel free to reopen the issue if you still have a concern.Thanks!\r\n"]}, {"number": 41641, "title": "[ROCm] AMDGPU compiler fixes", "body": "This PR:\r\nEnsures that AMDGPU compiler's temporary files are all different from instance to instance (important for multi-GPU training, e.g. with Horovod) and that they are deleted after compilation\r\nAdds a HSACO cache, to speed up the compilation process when compilation of multiple identical IR's is requested", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41641) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41641) for more info**.\n\n<!-- ok -->"]}, {"number": 41640, "title": "Raise error when calling .fit() w/ batch_size and a tf dataset", "body": "", "comments": []}, {"number": 41639, "title": "Using learning-rate decay schedule and verbose=1 creates fatal TypeError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 and Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): Binary (pip3)\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nIn the following minimal example, setting `verbose=1` in the fitting creates a fatal error. I realize that `ReduceLROnPlateau` isn't needed when also using `ExponentialDecay` but the fact that this error happens if `verbose=1` and not when `verbose=0` in training shows there is a deeper problem. \r\n\r\n```\r\n# minimum keras example\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.optimizers import SGD\r\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.callbacks import TerminateOnNaN,EarlyStopping,ReduceLROnPlateau\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(8,input_shape=(16,)))\r\nmodel.add(tf.keras.layers.Dense(1))\r\ncallbacks=[#TerminateOnNaN(),EarlyStopping(monitor='val_loss'),\r\n          ReduceLROnPlateau(monitor='val_loss')]\r\noptimizer=SGD(learning_rate=ExponentialDecay(initial_learning_rate=1.e-3,\r\n                                            decay_steps=2,decay_rate=0.5))\r\nmodel.compile(optimizer=optimizer, loss='mse')\r\nx=tf.ones((32,16))\r\ny=tf.ones((32,1))\r\nv=tf.ones((8,1))\r\nmodel.fit(x, y, batch_size=4, epochs=8, callbacks=callbacks,\r\n         validation_split=0.2,steps_per_epoch=4,verbose=1)\r\n```\r\nyields the following error:\r\n```\r\nEpoch 1/8\r\n1/4 [======>.......................] - ETA: 0s - loss: 1.4363\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-138-2120310fffef> in <module>\r\n     19 v=tf.ones((8,1))\r\n     20 model.fit(x, y, batch_size=4, epochs=8, callbacks=callbacks,\r\n---> 21          validation_split=0.2,steps_per_epoch=4,verbose=1)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    874           epoch_logs.update(val_logs)\r\n    875 \r\n--> 876         callbacks.on_epoch_end(epoch, epoch_logs)\r\n    877         if self.stop_training:\r\n    878           break\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_epoch_end(self, epoch, logs)\r\n    363     logs = self._process_logs(logs)\r\n    364     for callback in self.callbacks:\r\n--> 365       callback.on_epoch_end(epoch, logs)\r\n    366 \r\n    367   def on_train_batch_begin(self, batch, logs=None):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_epoch_end(self, epoch, logs)\r\n    892 \r\n    893   def on_epoch_end(self, epoch, logs=None):\r\n--> 894     self._finalize_progbar(logs)\r\n    895 \r\n    896   def on_test_end(self, logs=None):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _finalize_progbar(self, logs)\r\n    933       self.progbar.target = self.seen\r\n    934     logs = logs or {}\r\n--> 935     self.progbar.update(self.seen, list(logs.items()), finalize=True)\r\n    936 \r\n    937 \r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py in update(self, current, values, finalize)\r\n    568         value_base = max(current - self._seen_so_far, 1)\r\n    569         if k not in self._values:\r\n--> 570           self._values[k] = [v * value_base, value_base]\r\n    571         else:\r\n    572           self._values[k][0] += v * value_base\r\n\r\nTypeError: unsupported operand type(s) for *: 'ExponentialDecay' and 'int'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nModel should fit without any errors. Here is what happens if I don't use the callbacks:\r\n\r\n```\r\nEpoch 1/8\r\n4/4 [==============================] - 0s 20ms/step - loss: 6.3209 - val_loss: 4.6678\r\nEpoch 2/8\r\n4/4 [==============================] - 0s 10ms/step - loss: 4.4087 - val_loss: 4.0870\r\nEpoch 3/8\r\n4/4 [==============================] - 0s 9ms/step - loss: 4.0231 - val_loss: 3.9547\r\nEpoch 4/8\r\n4/4 [==============================] - 0s 10ms/step - loss: 3.9385 - val_loss: 3.9224\r\nEpoch 5/8\r\n4/4 [==============================] - 0s 9ms/step - loss: 3.9185 - val_loss: 3.9143\r\nEpoch 6/8\r\n4/4 [==============================] - 0s 9ms/step - loss: 3.9131 - val_loss: 3.9123\r\nEpoch 7/8\r\n4/4 [==============================] - 0s 9ms/step - loss: 3.9121 - val_loss: 3.9118\r\nEpoch 8/8\r\n4/4 [==============================] - 0s 11ms/step - loss: 3.9117 - val_loss: 3.9117\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nsee above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@bsugerman,\r\nI was able to reproduce the issue with TF v2.2. However, the issue seems to be fixed in the latest TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41639\">No</a>\n", "Running the [gist](https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb) linked above with the latest nightly (`2.9.0-dev20220320`) fails with the exact same error."]}, {"number": 41638, "title": "Adding TF_UNUSED_VARIABLE", "body": "This PR adds a new macro for compiler warnings in case of unused variables. Similar to EIGEN_UNUSED_VARIABLE, it creates a void function that will be optimized away by the compiler. This comes in handy to reduce the warning messages while building and allows user to explicitly mark unused variable so that real warnings can be investigated.", "comments": ["Tagging @mihaimaruseac for notification."]}, {"number": 41637, "title": "TFLite quantization silently converts bias tensor to int8 (int32 expected), causing interpreter to crash", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version (or github SHA if from source): `tf-nightly`, version `2.4.0-dev20200721`\r\n\r\n**Code**\r\nI am converting using the options recommended in the docs:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [\r\n      tf.lite.OpsSet.TFLITE_BUILTINS_INT8\r\n]\r\nconverter.target_spec.supported_types = [tf.int8, tf.uint8, tf.int32]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n# Allocating tensors subsequently fails\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-07-22 11:51:55.215847: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:192] None of the MLIR optimization passes are enabled (registered 0 passes)\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nI0722 11:51:55.644716 140663456084352 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nI0722 11:51:55.645021 140663456084352 convert_saved_model.py:99] input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: input-to-image\r\nI0722 11:51:55.645142 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: input-to-image\r\nINFO:tensorflow: tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT\r\nI0722 11:51:55.645210 140663456084352 convert_saved_model.py:43]  tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT\r\nINFO:tensorflow:output tensors info: \r\nI0722 11:51:55.645267 140663456084352 convert_saved_model.py:101] output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_scores\r\nI0722 11:51:55.645350 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_scores\r\nINFO:tensorflow: tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT\r\nI0722 11:51:55.645412 140663456084352 convert_saved_model.py:43]  tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT\r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_boxes\r\nI0722 11:51:55.645479 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_boxes\r\nINFO:tensorflow: tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT\r\nI0722 11:51:55.645537 140663456084352 convert_saved_model.py:43]  tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT\r\n2020-07-22 11:51:55.646430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-22 11:51:55.646453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \r\nINFO:tensorflow:Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables\r\nI0722 11:51:56.537452 140663456084352 saver.py:1293] Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables\r\n2020-07-22 11:51:57.199313: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-07-22 11:51:57.199419: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-22 11:51:57.343124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561781c5b260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-22 11:51:57.343157: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2020-07-22 11:51:57.343674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-22 11:51:57.343771: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-07-22 11:51:57.343819: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:51:57.343863: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:51:57.343906: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:51:57.343949: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:51:57.343992: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:51:57.344035: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-07-22 11:51:57.344043: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-07-22 11:51:57.344057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-22 11:51:57.344064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-07-22 11:51:57.344070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-07-22 11:51:57.448338: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-07-22 11:51:57.448368: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-07-22 11:51:57.448373: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nWARNING:tensorflow:From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/util.py:276: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0722 11:51:57.642830 140663456084352 deprecation.py:323] From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/util.py:276: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nW0722 11:51:57.643322 140663456084352 deprecation.py:323] From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n2020-07-22 11:51:58.442749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-22 11:51:58.442781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \r\nINFO:tensorflow:Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables\r\nI0722 11:51:59.415019 140663456084352 saver.py:1293] Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nI0722 11:51:59.910989 140663456084352 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nI0722 11:51:59.911199 140663456084352 convert_saved_model.py:99] input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: input-to-image\r\nI0722 11:51:59.911319 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: input-to-image\r\nINFO:tensorflow: tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT\r\nI0722 11:51:59.911397 140663456084352 convert_saved_model.py:43]  tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT\r\nINFO:tensorflow:output tensors info: \r\nI0722 11:51:59.911462 140663456084352 convert_saved_model.py:101] output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_boxes\r\nI0722 11:51:59.912057 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_boxes\r\nINFO:tensorflow: tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT\r\nI0722 11:51:59.912123 140663456084352 convert_saved_model.py:43]  tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT\r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_scores\r\nI0722 11:51:59.912191 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_scores\r\nINFO:tensorflow: tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT\r\nI0722 11:51:59.912250 140663456084352 convert_saved_model.py:43]  tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT\r\n2020-07-22 11:51:59.913029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-22 11:51:59.913052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \r\nINFO:tensorflow:Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables\r\nI0722 11:52:00.961747 140663456084352 saver.py:1293] Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables\r\n2020-07-22 11:52:01.741302: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-07-22 11:52:01.741406: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-22 11:52:01.742209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-22 11:52:01.742323: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:01.742382: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:01.742436: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:01.742488: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:01.742540: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:01.742592: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:01.742644: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:01.742654: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-07-22 11:52:01.742672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-22 11:52:01.742680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-07-22 11:52:01.742688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-07-22 11:52:01.850868: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-07-22 11:52:01.850904: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-07-22 11:52:01.850913: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nI0722 11:52:03.153840 140663456084352 lite.py:1321] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n2020-07-22 11:52:03.415416: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.\r\n2020-07-22 11:52:03.415449: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.\r\n2020-07-22 11:52:03.791157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-22 11:52:03.791318: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:03.791376: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:03.791429: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:03.791480: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:03.791544: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:03.791592: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:03.791640: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-07-22 11:52:03.791649: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-07-22 11:52:03.791666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-22 11:52:03.791673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-07-22 11:52:03.791679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-07-22 11:52:04.930987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-22 11:52:04.931019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \r\n2020-07-22 11:52:05.066647: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [34600]\r\n\t [[{{node Placeholder/_0}}]]\r\n2020-07-22 11:52:05.066900: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [34600]\r\n\t [[{{node Placeholder/_0}}]]\r\nTraceback (most recent call last):\r\n  File \"./export_tflite_model.py\", line 91, in <module>\r\n    tf.app.run(main)\r\n  File \"/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"./export_tflite_model.py\", line 85, in main\r\n    export(FLAGS.saved_model_dir, FLAGS.output_dir)\r\n  File \"./export_tflite_model.py\", line 79, in export\r\n    interpreter.allocate_tensors()\r\n  File \"/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 243, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:334 bias->type != kTfLiteInt32 (INT8 != INT32)Node number 3 (CONV_2D) failed to prepare.\r\n```\r\n\r\n**Helpful Files**\r\nSaved model:\r\n[saved.zip](https://github.com/tensorflow/tensorflow/files/4961958/saved.zip)\r\n\r\nGraph after transformation:\r\n[after-transformation.zip](https://github.com/tensorflow/tensorflow/files/4961968/after-transformation.zip)\r\n\r\n\r\n**Failure details**\r\nInterestingly, the actual `TFLiteConverter` quantizes without crashing; reloading the generated model into the interpreter is what causes the crash. I am not sure why the bias tensor is being cast to int8; I am happy to provide any more debug files should they be necessary.", "comments": ["@rajansaini691 It looks like the `saved.zip` contains a frozen graph and not a saved model. If you have saved a model using `tf.saved_model.save` then you can use `from_saved_model` to load the model for conversion. I was also not able to load the file in `after-transformation.zip`.\r\n\r\nCan you please share a standalone code (colab or Jupyter notebook) to reproduce the issue? Thanks!", "@suharshs @jianlijianli \r\nI vaguely remember seeing this on another model too, where the quant tooling generates an int8 bias. I guess there is some issue with the consumers of that tensor?", "Can you try set experimental quantizer to be true and try again?\r\n\r\nThanks!\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L421", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41637\">No</a>\n", "any updates on this end? happens to a custom model of mine too (inherited from tf.keras.Model) when manually placing `tf.quantization.fake_quant_with_min_max_vars(x, -3.0, 3.0)`\r\n@rajansaini691 what model architecture did you use?"]}, {"number": 41636, "title": "[CherryPick 2.3] Going back to forcing embedding layer variables on the CPU even within a tf.function as this is breaking some user code.", "body": "PiperOrigin-RevId: 321607029\nChange-Id: Id159867f51b26e6604a1186d9ce526658ddd1e19", "comments": []}, {"number": 41635, "title": "Fitting sometimes leads to NaN loss on TPU, while on CPU doesn't", "body": "**System information**\r\n\r\n- TensorFlow version (use command below): 2.2.0 (v2.2.0-0-g2b96f3662b)\r\n- Python version: 3.6.9\r\n- GPU model and memory: Google Colab TPU\r\n\r\nI've found that sometimes fitting model on TPU leads to NaN loss, while fitting on CPU doesn't.\r\n\r\nI fit Xception network with custom top layers, using only 5 batches, one per epoch. Standalone code:\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import *\r\n\r\ntf.get_logger().propagate = False\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n#with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\r\n\r\n  xception = tf.keras.applications.Xception(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))\r\n\r\n  x = xception.output\r\n  x = GlobalAveragePooling2D()(x)\r\n  x = Dense(256, activation = 'relu')(x)\r\n  x = Dropout(0.25)(x)\r\n  predictions = Dense(10)(x)\r\n\r\n  model = tf.keras.Model(inputs = xception.input, outputs = predictions)\r\n\r\n  model.compile(\r\n      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\r\n      optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-8),\r\n      metrics = ['accuracy']\r\n  )\r\n\r\n  batch_size = 7\r\n  for epoch in range(5):\r\n    X = np.random.rand(batch_size, 256, 256, 3)\r\n    Y = np.zeros(batch_size)\r\n    model.fit(X, Y, batch_size = batch_size, epochs = 1)\r\n```\r\n\r\nWhat i've found?\r\n- if using TPU, batch_size < 8 fitting gives NaN loss after second epoch (or second batch, which is the same)\r\n- if using TPU, batch_size < 8 and X is multiplied by zero, fitting gives NaN loss after first epoch\r\n- if using TPU, batch_size >= 8, fitting doesn't give NaN loss\r\n- if using TPU, batch_size >= 8 and X is multiplied by zero, fitting gives NaN loss after first epoch\r\n- if using CPU, fitting doesn't give NaN loss in the cases described above\r\n\r\nSo, fitting gives NaN loss if we are using TPU and batch_size < 8 or X is an array of zeros.\r\nI tested batch sizes 1, 7, 8, 64.\r\nTo train on CPU i used this command:\r\n```\r\nwith tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\r\n```\r\nor just used Colab version with only CPU, which gives the same results.\r\n\r\nSame behaviour occured when I was training this model on real data: fitting immediately led to NaN loss when some batch had less than 8 image-label pairs in it, for example when last batch in dataset was smaller.\r\n\r\nI guess that it can be caused by less-precise float on TPU or by some error in parallel calculations on 8 TPU cores (/device:TPU:0, ..., /device:TPU:7).", "comments": ["I am able to reproduce the issue in colab with TF version 2.2.\r\nBut i am seeing different error message with TF version 2.3-rc1 and nightly version(`2.4.0-dev20200722`) (`InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}`).\r\nPlease, find the gist [here](https://colab.research.google.com/gist/ravikyram/e3ea5e72d7e2866d2e162b6b317173f9/untitled174.ipynb).Thanks!", "With tf-nightly TPU cannot be even initialized\r\nhttps://github.com/tensorflow/tensorflow/issues/41542", "Thanks for this report @sedol1339. Regarding the NaNs, I don't think this is too surprising given the small batch size. And I suspect they are caused by all the batch norm layers within Xception. At a batch size of 7 if you set:\r\n\r\n```\r\nfor layer in xception.layers:\r\n  layer.trainable = False\r\n```\r\n\r\nthen I am not seeing the NaNs. With batch norm, you're dividing by local batch statistics on each core, so with a global batch size < 8 on TPUs, the batch is being split for each core and you are more likely to divide by numbers close to 0.", "So isn't it a bug? I thought that using TPU should not affect such things. Why system cannot use less than 8 cores if batch size if less that 8?", "No, I don't think it's a bug because batch norm would have problems at such a low global batch size. There's no way for the system to utilize fewer cores even if the batch size is smaller than the number of cores.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41635\">No</a>\n", "@nikitamaia I think i was too hasty to close the ticket, because this problem happens not only when you specify small batch size. Yes, batch size should be large enough for TPU.\r\n\r\nBut when you split dataset into large batches, the last batch may be very small. In this case loss will immediately become NaN. It's may be hard to understand the reason of NaN loss in this situation. So this behaviour should be eigther fixed or clearly written in TPU documentation.", "I had this same issue and after reading your comment, I dropped the last remaining batch and it fixed it for me! Thanks!", "I ran into this problem too. I thought it must be data specific since i got this after constructing a bigger dataset and everything else was the same as before (it worked before). Reading this thread helps, it seems the solution is to not include the last batch? Is this still a bug or a hard requirement for TPU? I am guessing that my older dataset was lucky that its last batch is the same as the batch_size (i.e. no remainder smaller batch).\r\n\r\nCuriously, I got this error using a custom loss function, but not using just tf.keras.losses.BinaryCrossentropy", "I spent quite some time debugging the `NaN` issue in my system and finally realized this is the issue. For ex: If you have 101 classes and you encode labels from 1-101, your model will always have a `NaN`! Because 101 is greater than the size of the logits!\r\n\r\nCan we have at least a Warning popping up, instead of simply showing that the loss diverged with a NaN? That can bring down the debugging time to a great extent for many users.\r\n\r\n_Originally posted by @kartik-hegde in https://github.com/tensorflow/tensorflow/issues/8484#issuecomment-354376609_\r\n\r\nI also got the same error. It was because of the classes mismatch between the network output and the target classes"]}, {"number": 41634, "title": "batch_to_space & space_to_batch usage.", "body": "This is a request to improve the documentation to add some examples/use cases.\r\n\r\n## URL(s) with the issue:\r\nN/A\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n**https://www.tensorflow.org/api_docs/python/tf/space_to_batch\r\nhttps://www.tensorflow.org/api_docs/python/tf/batch_to_space**\r\n\r\n## Description of issue (what needs changing):\r\nIt's not very clear where/how these two (space_to_batch & batch_to_space) operations are used.\r\n\r\n### Clear description\r\nSome examples of how/where these operations are used will help.\r\n\r\n### Correct links\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_ops.py#L3676-L3678\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_ops.py#L3735-L3849\r\n\r\n### Parameters defined\r\nN/A\r\n\r\n### Returns defined\r\nN/A\r\n\r\n### Raises listed and defined\r\nN/A\r\n\r\n### Usage example\r\nThis is exactly what's required: usage example\r\n\r\n### Request visuals, if applicable\r\nPossibly..\r\n\r\n### Submit a pull request?\r\nN/A", "comments": ["@sudzam I think this is resolved. The updated pages of [space_to_batch](https://www.tensorflow.org/api_docs/python/tf/space_to_batch) and [batch_to_space](https://www.tensorflow.org/api_docs/python/tf/batch_to_space?version=nightly) include examples and an explanation. \r\n\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 41633, "title": "2.3.0 cherry-pick request: Update tensorboard dependency to 2.3", "body": "This cherrypick is required to depend on a version of TensorBoard that\r\nis compatible with TensorFlow 2.3.x, such that `tensorboard` starts\r\nsuccessfully.\r\n\r\nNote: starting from TF/TB 2.3, the tensorboard dependency is more\r\nrelaxed to allow TensorBoard to be released at a more frequent cadence.\r\nThe major version is still synced, while the minor version is not.\r\n\r\nPiperOrigin-RevId: 322474571", "comments": ["Thanks!  Merging"]}, {"number": 41632, "title": "ModelCheckpoint save_freq incompatible with Model.fit validation_freq", "body": "please see [this issue](https://github.com/keras-team/keras/issues/13689) filed on the now unsupported keras repo.  @gattia there has it [figured out](https://github.com/keras-team/keras/issues/13689#issuecomment-583758348) i believe.  would be nice to have a work around.\r\n\r\nin brief, when using `ModelCheckpoint(save_freq=<int>...` combined with `model.fit(validation_freq=<int>...` then \"WARNING:tensorflow:Can save best model only with val_loss available, skipping.\" is emitted. even when you've carefully calculated the two to be the same number of epochs.", "comments": ["@bjarthur \r\nCan you please refer to these links with reported to the error reported;\r\n[link](https://stackoverflow.com/questions/59706714/warningtensorflowearly-stopping-conditioned-on-metric-val-binary-accuracy-wh) [link1](https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in)\r\n\r\nPlease share simple stand alone code [indented with all dependencies] such that we can replicate the issue faced or if possible share a colab gist with the error for us to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41632\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41632\">No</a>\n", "@bjarthur can you please reopen the issue? \r\n\r\n@Saduf2019 the problem is not at the links you've presented. Yes, it originally seems like it could be associated with early stopping, but is not, it is a model checkpoint problem. \r\n\r\nAttached below is a MWE of the problem. I have tested this on TF 2.0.0. locally and 2.3.0 on colab. \r\n\r\nEssentially, if you try to set `save_freq` in `ModelCheckpoint` to be any integer it fails producing the warning: \r\n\r\n`WARNING:tensorflow:Can save best model only with val_loss available, skipping.`\r\n\r\neach time that it tries to save the model. \r\n\r\nThe logic and explicit code locations that cause this are outlined in this post:\r\nhttps://github.com/keras-team/keras/issues/13689#issuecomment-662598329\r\n\r\nThe gist of that post/the problem is that most people using `ModelCheckpoint` are benchmarking it against a validation metric (e.g., `val_loss`), and `val_loss` or any other validation metric is not calculated until after the epoch is completed (it's calculated when `.on_epoch_end`  is evoked). However, by nature of the code logic, `ModelCheckpoint` searches for the `val_loss` at the end of each batch, which is always before the `.on_epoch_end` code is invoked. Therefore, when `ModelCheckpoint` is looking for a `val_loss` it doesn't exist (and can't/won't) so this error will always appear if someone sets save_freq to be anything other than `epoch` which is the default and they want the value being monitored to be a validation metric. \r\n\r\nThe example sets up the save_freq to be at the end of an epoch. This will produce the error once per epoch. Though, it can be any value to produce the warning. \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nimport numpy as np\r\nimport tempfile\r\nimport os\r\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\r\n\r\n# Make Fake Data\r\nx_train = np.random.rand(256, 784).astype(\"float32\")\r\ny_train = np.random.randint(low=0, high=10, size=256)\r\n\r\nx_test = np.random.rand(64, 784).astype(\"float32\")\r\ny_test = np.random.randint(low=0, high=10, size=64)\r\n\r\n# Build toy model \r\ninputs = keras.Input(shape=(x_train.shape[1]))\r\ndense = layers.Dense(64, activation=\"relu\")(inputs)\r\ndense = layers.Dense(64, activation=\"relu\")(dense)\r\noutputs = layers.Dense(10)(dense)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nprint(model.summary())\r\n\r\nmodel.compile(\r\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    optimizer=keras.optimizers.Adam(),\r\n    metrics=[\"accuracy\"],\r\n)\r\n\r\n# Set Parameters\r\nbatch_size = 32 # divisible by number of examples in train (256)\r\nlocation_save = tempfile.gettempdir() # Get location of temp directory to not store data permanently\r\n\r\n# Align modelcheckpoint to end of epoch based on TF version. \r\n# Note only tf == 2.0 has been tried, as it was what is available on Conda (recommended install method)\r\nif ((tf.__version__[:3] == '2.0') or\r\n\t(tf.__version__[:3] == '2.1')\r\n\t):\r\n\tsave_freq = x_train.shape[0]\r\nelif ((tf.__version__[:3] == '2.2') or\r\n\t  (tf.__version__[:3] == '2.3')\r\n\t  ):\r\n\tsave_freq = x_train.shape[0] // batch_size\r\n\r\nverbose = 2 # To easily see number of epochs and \"warning\" print statements from TF. \r\n\r\n# Create model checkpoint\r\nmodel_checkpoint = ModelCheckpoint(filepath=os.path.join(location_save, \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t     'temp_mnist_weights.h5'),\r\n                                   monitor='val_loss',\r\n                                   verbose=verbose,\r\n                                   save_best_only=True,\r\n                                   save_weights_only=True,\r\n                                   save_freq=save_freq\r\n                                   )\r\n\r\n# Fit model \r\nmodel.fit(x_train, \r\n\t\t  y_train,\r\n\t\t  validation_data=(x_test, y_test),\r\n\t\t  batch_size=batch_size, \r\n\t\t  epochs=2, \r\n\t\t  callbacks=[model_checkpoint],\r\n\t\t  verbose=verbose)\r\n```", "@Saduf2019 can you please re-open this issue?  there is now code as you requested.  i don't appear to be able to reopen.  thanks!", "The same incompatibility exists with other callbacks like reduce lr on plateau", "@ntakouris thanks for adding that. \r\n\r\n@Saduf2019 can this be reopened? Or should I open a new issue? ", "Tensorflow issue handling is a freaking joke. Nobody takes anything serious. All issues get closed by some stupid bot just because nobody takes time to look at them."]}, {"number": 41631, "title": "[XLA] Extend the Algebraic Simplifier to convert Pow(x, 3) -> x*x*x. ", "body": "2 multiplication is faster then the full pow implementation.\r\nThis also make the GELU kernel vectorized.\r\n\r\nThis pattern happen in GELU in BERT. Now it only takes 81 us instead of the original 208 us on a V100.\r\n\r\n@thomasjoerg ", "comments": []}, {"number": 41630, "title": "CudnnLSTM variable sequence length sometimes fails with CUDNN_STATUS_EXECUTION_FAILED", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian/Sid (2020-07-01), Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source and binary\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6, 3.7.8\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 9.0\r\n- CUDA/cuDNN version: 10.0/7.4.1 ; 10.0/7.4.2.1 ; 10.0/7.5.1.10 ; 10.0/7.6.5.32\r\n- GPU model and memory: 2x RTX 2080 Ti ; 4x GTX 1080 Ti ; \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`: v1.15.3-0-g4386a6640c\r\n\r\n\r\n**Describe the current behavior**\r\nTraining with some dataset triggers:\r\n```\r\n2020-07-22 16:15:42.108252: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_EXECUTION_FAILED                                                                                                          \r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1778): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_de\r\nsc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2020-07-22 16:15:42.108385: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cudnn_rnn_ops.cc:1527 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 75, 2, 2048] \r\n```\r\n\r\n**Describe the expected behavior**\r\nTraining should succeed, or TensorFlow or CUDNN should expose a more actionable error\r\n\r\n**Standalone code to reproduce the issue**\r\nWill be provided after.\r\n\r\n**Other info / logs**\r\nWill be provided after. Some noisy debugging session can be seen at https://github.com/mozilla/DeepSpeech/issues/3088\r\n", "comments": ["Repro material.\r\n\r\nTo repro with r1.15 tensorflow on current Debian Sid, I needed to use pyenv to have a local 3.7 python (latest version supported by r1.15), but if you run it on a system-provided 3.7, you'd prefer remove those line and just setup a virtualenv.\r\n\r\nSTR:\r\n 1. Create `~/tmp/issue3308`\r\n 2. Placed this script in `~/tmp/issue3088/`\r\n```\r\n$ cat run_local.sh \r\n#!/bin/bash\r\n\r\nset -xe\r\n\r\nCOMMAND_PREFIX=$1\r\necho $COMMAND_PREFIX\r\n\r\nrm -fr ~/.local/share/deepspeech/\r\n\r\nexport PYENV_VERSION=3.7.8\r\nexport PYENV_ROOT=$HOME/pyenv/\r\nexport PATH=$PYENV_ROOT/bin/:$PATH\r\n\r\neval \"$(pyenv init -)\"\r\n\r\npip --version\r\n\r\nfor p in tensorboard tensorflow tensorflow-estimator tensorflow-gpu-local;\r\ndo\r\n    pip list $p && pip uninstall --yes $p\r\ndone;\r\n\r\ncd ds/ && \\\r\n        pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3 && \\\r\n        DS_NOTENSORFLOW=y pip3 install --upgrade -e . && \\\r\n        pip3 install --upgrade tensorflow-gpu==1.15.3\r\ncd ..\r\n\r\n#pip install --upgrade $HOME/tmp/issue3088/wheel_dst/tensorflow_gpu_local-1.15.0-cp37-cp37m-linux_x86_64.whl\r\n#pip install --upgrade $HOME/tmp/issue3088/wheel_dst/tensorflow_gpu_local-*-cp37-cp37m-linux_x86_64.whl\r\n\r\n### 1.13\r\n#patch -p1 -d $PYENV_ROOT/versions/$PYENV_VERSION < disable_tensorflow_contrib_cloud.patch\r\n# tpu hack\r\n#pip install --upgrade tensorflow-estimator==1.14.0\r\n\r\n### after\r\n#patch -p1 -d $PYENV_ROOT/versions/$PYENV_VERSION < disable_tensorflow_core_contrib_cloud.patch\r\n\r\nnvidia-smi\r\n\r\npython --version\r\n\r\n#TF_CPP_MIN_VLOG_LEVEL=1\r\npython -c 'import tensorflow as tf; tf.test.is_gpu_available()'\r\n\r\n$COMMAND_PREFIX $PYENV_ROOT/versions/$PYENV_VERSION/bin/python ds/DeepSpeech.py \\\r\n        --show_progressbar true \\\r\n        --log_level 0 \\\r\n        --train_cudnn true \\\r\n        --alphabet_config_path ds/data/alphabet.txt \\\r\n        --scorer \"\" \\\r\n        --train_files csvs/train_debug_mini_As_Bs_Cs_local.csv \\\r\n        --train_batch_size 2 \\\r\n        --n_hidden 2048 \\\r\n        --audio_sample_rate 16000 \\\r\n        --epochs 3 \\\r\n        --learning_rate 0.0001 \\\r\n        --dropout_rate 0.4 \\\r\n        --lm_alpha 0.75 \\\r\n        --lm_beta 1.85\r\n```\r\n 3. `git clone https://github.com/mozilla/DeepSpeech/ ds/` (you might also need git-lfs installed prior)\r\n 4. Extract attached zip file for repro dataset\r\n\r\nThe zip file also features some Dockerfile, if it helps for repro.\r\n[tensorflow_issue_41630.zip](https://github.com/tensorflow/tensorflow/files/4960626/tensorflow_issue_41630.zip)\r\n", "More infos / logs\r\n----------------------\r\n\r\nFailure log is (with some extra debug by myself):\r\n```\r\nD Session opened.                                                      \r\nI Could not find best validating checkpoint.\r\nI Could not find most recent checkpoint.                                                                                                                                                                    \r\nI Initializing all variables.                                                                                                                                                                               \r\n2020-07-22 16:06:28.419358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7   \r\nI STARTING Optimization                                                                                                                                                                                     \r\nEpoch 0 |   Training | Elapsed Time: 0:00:00 | Steps: 0 | Loss: 0.000000                                                                                                                                                                                                                                                                                                                                                2\r\n020-07-22 16:06:29.819427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0   \r\ngenerate_values <deepspeech_training.util.sample_collections.CSV object at 0x7f497418e610>                                                                                                                  \r\nyield generate_values 0 csvs/../data/A/163_5029_3498779ce37873475394654801cc3888-8fddd9522baf442463171802a7e57489.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7f4d1710a8d0>                                                                                                                                                                                                               \r\nyield generate_values 1 csvs/../data/A/155_4757_9bc6d6f754547a09bbcf70e42d8e2a27-b112945da6818223ab8e1daf80313a62.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7f4d1710a850>                                                                                                                                                                                                               \r\nyield generate_values 2 csvs/../data/B/98_2923_a387275540ba5f2159c37eaee3e4e9a0-651926517a6241fd9bb5942777b1f0ff.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7f4d1710ab10>\r\nbatch_fn <_VariantDataset shapes: <unknown>, types: tf.string> 2 <_VariantDataset shapes: (?, 26), types: tf.float32> <_VariantDataset shapes: (), types: tf.int32>\r\nyield generate_values 3 csvs/../data/B/154_4738_2f841fb1af523c579414e0358ab16295-6aea9aa95b1bdbfd80703754cd8a180c.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7f4d1710a9d0>                                                                                                                                                                                                               \r\nyield generate_values 4 csvs/../data/C/175_5429_67ed7914b9a3bac4e46dd42a5721a95f-e31a33c85ca8249476596c1ff7fc2f67.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7f4d1710a910>\r\nyield generate_values 5 csvs/../data/C/169_5271_3210ac3e97626f9c1515cb019e5fa36e-dd839274af12610f137398ddd01f85f8.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7f4d1710abd0>\r\nbatch_fn <_VariantDataset shapes: <unknown>, types: tf.string> 2 <_VariantDataset shapes: (?, 26), types: tf.float32> <_VariantDataset shapes: (), types: tf.int32>                                                                                                                                                                                                                                                      \r\nCudnnRNNForwardOp         \r\nShouldUsePaddedIO time_major=1                     \r\nShouldUsePaddedIO seq_array[0]=74                                                                                                                                                                                                                                                                                                                                                                                        \r\nShouldUsePaddedIO seq_array[1]=74\r\nShouldUsePaddedIO [0]: seq_array[i]=74              \r\nShouldUsePaddedIO [0]: model_shapes.max_seq_length=74                                                                                                                                                                                                                                                                                                                                                                    \r\nShouldUsePaddedIO [1]: seq_array[i]=74\r\nShouldUsePaddedIO [1]: model_shapes.max_seq_length=74\r\nShouldUsePaddedIO rv=false all_max_seq_length=true                                                                                                                                                                                                                                                                                                                                                                       \r\nfiles: [\"csvs/../data/A/163_5029_3498779ce37873475394654801cc3888-8fddd9522baf442463171802a7e57489.wav\" \"csvs/../data/A/155_4757_9bc6d6f754547a09bbcf70e42d8e2a27-b112945da6818223ab8e1daf80313a62.wav\"] [74 74] [[[-0.245490551 0.00717380643 0.10210821 ... 0.117108196 -0.0276376158 -0.126444399]\r\n  [0.0757513046 -0.0512054712 -0.291382492 ... 0.243732437 -0.0566715039 0.0398223847]]\r\n                                                                                                                                                                                                                                                                                                                                                                                                                         \r\n [[-0.195820108 0.138200715 0.0146510527 ... 0.177044764 0.0705049634 -0.189360529]\r\n  [-0.028521724 0.25693503 -0.152943641 ... 0.121622048 -0.342009 0.0648547485]]\r\n                                                                                                      \r\n [[-0.22990115 0.229410842 0.0562512279 ... 0.0885351449 -0.210647494 -0.115016311]\r\n  [-0.522053659 -0.152149498 -0.0655985773 ... 0.0328139216 -0.0536242127 0.0909850895]]\r\n                                                                                                      \r\n ...                                    \r\n                                                                                                      \r\n [[0.125317723 -0.201877266 0.342403591 ... -0.461629033 -0.7424016 -0.634134829]\r\n  [-0.495484978 0.0653357953 0.444910228 ... 0.356240422 0.116222136 -0.165624559]]\r\n                                                                                                      \r\n [[-0.559318066 -0.360073239 0.386285067 ... 0.250025511 -0.437686384 -0.0856590122]\r\n  [-0.429158121 -0.0336664394 -0.304231912 ... 0.120450795 -0.136865944 -0.0517320298]]\r\n                                                                                                      \r\n [[-0.30379957 0.109998763 0.230347365 ... 0.213008478 -0.100003451 -0.244150743]\r\n  [-0.19179742 -0.0341652408 0.393984377 ... 0.300181448 -0.380117238 -0.0264749527]]] 'SparseTensor(indices=[[0 0]\r\n [0 1]                                                                \r\n [0 2]\r\n ...                                                                                                                                                                                                                                                                                                                                                                                                                     \r\n [1 22]                   \r\n [1 23]                                                                                   \r\n [1 24]], values=[5 14 0 ... 8 9 10], shape=[2 27])'                                                                                                                                                                                                                                                                                                                                                                     \r\nCudnnRNNBackwardOp        \r\nShouldUsePaddedIO time_major=1                                                            \r\nShouldUsePaddedIO seq_array[0]=74                                                                                                                                                                                                                                                                                                                                                                                        \r\nShouldUsePaddedIO seq_array[1]=74\r\nShouldUsePaddedIO [0]: seq_array[i]=74                                                    \r\nShouldUsePaddedIO [0]: model_shapes.max_seq_length=74                                                                                                                                                                                                                                                                                                                                                                    \r\nShouldUsePaddedIO [1]: seq_array[i]=74\r\nShouldUsePaddedIO [1]: model_shapes.max_seq_length=74                                     \r\nShouldUsePaddedIO rv=false all_max_seq_length=true\r\nEpoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 1 | Loss: 188.119797                                                                                                                                                                                                                                                                                                                                              b\r\natch_fn <_VariantDataset shapes: <unknown>, types: tf.string> 2 <_VariantDataset shapes: (?, 26), types: tf.float32> <_VariantDataset shapes: (), types: tf.int32>\r\nCudnnRNNForwardOp\r\nShouldUsePaddedIO time_major=1\r\nShouldUsePaddedIO seq_array[0]=74\r\nShouldUsePaddedIO seq_array[1]=75\r\nShouldUsePaddedIO [0]: seq_array[i]=74\r\nShouldUsePaddedIO [0]: model_shapes.max_seq_length=75\r\nShouldUsePaddedIO rv=true all_max_seq_length=false\r\n2020-07-22 16:06:33.612395: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1778): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_de\r\nsc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2020-07-22 16:06:33.612438: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cudnn_rnn_ops.cc:1527 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 75, 2, 2048] \r\n```\r\n\r\nWe verified a few things:\r\n - fine with tensorflow r1.14\r\n - breaks on r1.15 with cudnn 7.6, 7.5, more intermittent with 7.4 (I got severel non repro, reporting contributor still hit the issue)\r\n - repro with several versions of the nvidia driver (currently using 440.100 packaged by debian sid, contributor using similar version tested down to 430.64: https://github.com/mozilla/DeepSpeech/issues/3088#issuecomment-656583170)\r\n - git bisected tensorflow several time to ensure the first commit really exposing the issue is https://github.com/tensorflow/tensorflow/pull/30889 that @kaixih pushed (confirmed by local revert and no repro at all over 25 runs)\r\n - forcing `ShouldUsePaddedIO` to return `false` would repro, forcing `true` would never repro in my case (not verified on big dataset from contributors)\r\n - tried changing the batch ordering when we feed / call `ctc_loss` but this would not help\r\n - a lot of other trial/experiments are to be found in the deepspeech bug report", "Please do not hesitate to ping myself or @applied-machinelearning if you have troubles reproducing the issue, but with the provided material it should be straightforward.", "@timshen91 can you PTAL?", "Can you try to fetch the cudnn logs with the following env vars? And attach the somefile.log (If this is too large, we might only need the last part which contains the cudnnRNNForwardTrainingEx). @lissyx \r\n\r\n```bash\r\nexport CUDNN_LOGINFO_DBG=1\r\nexport CUDNN_LOGDEST_DBG=somefile.log\r\n```\r\nMore details: https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging", "[issue3088.log](https://github.com/tensorflow/tensorflow/files/4968738/issue3088.log)\r\nI might not be able to react before tomorrow, it's getting later over there.", "> More details: https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging\r\n\r\nI swear I search a long time for a CUDNN-related debug env variable but never found that :/", "```\r\nI! CuDNN (v7600) function cudnnRNNForwardTrainingEx() called:\r\n...\r\ni!         paddingMode: type=cudnnRNNPaddingMode_t; val=CUDNN_RNN_PADDED_IO_DISABLED (0);\r\ni!         plan: type=cudnnPersistentRNNPlan_t; val=NULL_PTR;\r\ni!     xDesc: type=cudnnRNNDataDescriptor_t:\r\ni!         dataType: type=cudnnDataType_t; val=CUDNN_DATA_FLOAT (0);\r\ni!         dimA: type=int; val=[75,2,2048];\r\ni!         seqLengthArray: type=int; val=[74,75];\r\ni!         layout: type=cudnnRNNDataLayout_t; val=CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED (0);\r\ni!         paddingFill: type=CUDNN_DATA_FLOAT; val=0.000000;\r\n...\r\n```\r\nAn initial investigation shows the padding mode is suspicious. We shouldn't see CUDNN_RNN_PADDED_IO_DISABLED here, which is incompatible with the CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED below.\r\n\r\nThis case uses the variable sequence lengths [74, 75] and max seq length is 75. The time/seq major layout is used. From the logic here: https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cudnn_rnn_ops.cc#L944-L956, the use_padded_io should be set `True` and subsequently the CUDNN_RNN_PADDED_IO_ENABLED should have been used here https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1082. I checked how the use_padded_io gets passed in the TF codebase and it looks no issue to me.", "Just one more thing: can you give a shot with `TF_CUDNN_RESET_RND_GEN_STATE=1`? The only thing that I suspect now is this branch might not be taken. https://github.com/tensorflow/tensorflow/blob/a621845bd0256981a092681eb80f6e2cac6bb65e/tensorflow/core/kernels/cudnn_rnn_ops.cc#L1085", "Wait, I might have made a mistake when generating those logs and left a `false` being forced for `use_padded_io` (I should maybe not have done that being sleepy). I'm cross-checking and will provide newer logs.", "Here is the log, with debug enabled, after ensuring `ShouldUsePaddedIO` does not hardcode a `false`:\r\n[issue3088_new_noResetRnnGenState.log](https://github.com/tensorflow/tensorflow/files/4971314/issue3088_new_noResetRnnGenState.log)\r\nAs you can see, this is still failing the same way.\r\n\r\nNow, here are the logs with the same TensorFlow setup, and `TF_CUDNN_RESET_RND_GEN_STATE=1`\r\n[issue3088_new_withResetRnnGenState_0.log](https://github.com/tensorflow/tensorflow/files/4971313/issue3088_new_withResetRnnGenState_0.log)\r\n[issue3088_new_withResetRnnGenState_1.log](https://github.com/tensorflow/tensorflow/files/4971312/issue3088_new_withResetRnnGenState_1.log)\r\n[issue3088_new_withResetRnnGenState_2.log](https://github.com/tensorflow/tensorflow/files/4971311/issue3088_new_withResetRnnGenState_2.log)\r\n[issue3088_new_withResetRnnGenState_3.log](https://github.com/tensorflow/tensorflow/files/4971310/issue3088_new_withResetRnnGenState_3.log)\r\n[issue3088_new_withResetRnnGenState_4.log](https://github.com/tensorflow/tensorflow/files/4971308/issue3088_new_withResetRnnGenState_4.log)\r\nAs you can see, no repro at all now.\r\n\r\nThis does makes me wonder if we might not be in trouble because of how we use CudnnLSTM: https://github.com/mozilla/DeepSpeech/blob/9e023660ef1c0947b0f8c8db54b5cc9174c7076a/training/deepspeech_training/train.py#L108-L132\r\nContext, as much as I can remember (I'm not the one who wrote that) is that we needed to handle variables properly for being able to reload checkpoint on non-CuDNN setup. Maybe @reuben can elaborate if that's required.", "> This does makes me wonder if we might not be in trouble because of how we use CudnnLSTM: https://github.com/mozilla/DeepSpeech/blob/9e023660ef1c0947b0f8c8db54b5cc9174c7076a/training/deepspeech_training/train.py#L108-L132\r\n> Context, as much as I can remember (I'm not the one who wrote that) is that we needed to handle variables properly for being able to reload checkpoint on non-CuDNN setup. Maybe @reuben can elaborate if that's required.\r\n\r\nSo I verified, changed our code to avoid this like that:\r\n```\r\ndiff --git a/training/deepspeech_training/train.py b/training/deepspeech_training/train.py\r\nindex 93d0c727..85c581ae 100644\r\n--- a/training/deepspeech_training/train.py\r\n+++ b/training/deepspeech_training/train.py\r\n@@ -115,22 +115,19 @@ def rnn_impl_cudnn_rnn(x, seq_length, previous_state, _):\r\n     # reuse=True to reuse variables, we can't easily make use of the object oriented\r\n     # way CudnnLSTM is implemented, so we save a singleton instance in the function,\r\n     # emulating a static function variable.\r\n-    if not rnn_impl_cudnn_rnn.cell:\r\n+    with tfv1.variable_scope('cudnn_lstm/rnn/multi_rnn_cell'):\r\n         # Forward direction cell:\r\n         fw_cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=1,\r\n                                                  num_units=Config.n_cell_dim,\r\n                                                  input_mode='linear_input',\r\n                                                  direction='unidirectional',\r\n                                                  dtype=tf.float32)\r\n-        rnn_impl_cudnn_rnn.cell = fw_cell\r\n \r\n-    output, output_state = rnn_impl_cudnn_rnn.cell(inputs=x,\r\n-                                                   sequence_lengths=seq_length)\r\n+        output, output_state = fw_cell(inputs=x,\r\n+                                       sequence_lengths=seq_length)\r\n \r\n     return output, output_state\r\n \r\n-rnn_impl_cudnn_rnn.cell = None\r\n-\r\n \r\n def rnn_impl_static_rnn(x, seq_length, previous_state, reuse):\r\n     with tfv1.variable_scope('cudnn_lstm/rnn/multi_rnn_cell'):\r\n```\r\n\r\nUnfortunately, the issue does still repro.", "So @kaixih what's your take with those new logs and the extra verification I performed on our code ? Don't hesitate if you need more logging, I'll do it as quickly as I can :)", "So, for now two things can be verified: \r\n(1) If the TF_CUDNN_RESET_RND_GEN_STATE=1, there is no repro, correct?\r\n(2) If the TF_CUDNN_RESET_RND_GEN_STATE=0 (default setting), the `use_padded_io` is True, correct? If yes, can you track it and see if this branch is taken in your training: https://github.com/tensorflow/tensorflow/blob/05ab6a2afa2959410d48aab2336cea9dc1e2c13e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1082?\r\n\r\n", ">  (1) If the TF_CUDNN_RESET_RND_GEN_STATE=1, there is no repro, correct?\r\n\r\nYes, that is for sure.\r\n\r\n\r\n\r\n> (2) If the TF_CUDNN_RESET_RND_GEN_STATE=0 (default setting), the `use_padded_io` is True\r\n\r\nShould I check that in the cudnn logs or at tensorflow framework level (i.e., return value of `ShouldUsePaddedIO`) ?\r\n\r\n\r\n\r\n> If yes, can you track it and see if this branch is taken in your training:\r\n\r\nI will verify that.", "> (2) If the TF_CUDNN_RESET_RND_GEN_STATE=0 (default setting), the `use_padded_io` is True, correct? If yes, can you track it and see if this branch is taken in your training:\r\n> \r\n> [](https://github.com/tensorflow/tensorflow/blob/05ab6a2afa2959410d48aab2336cea9dc1e2c13e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1082)\r\n\r\nIt is indeed weird: so far, it looks like we do return `true` from `ShouldUsePaddedIO` but when we reach the code you link, the value is `false` there, and so we don't set the proper value for CUDNN side.", "So, it would looks like we are calling `GetCachedRnnDescriptor()` https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cudnn_rnn_ops.cc#L1077-L1096 with proper `use_padded_io=true`, but the cache returns a hit and the matching `rnn_desc` we get is with `CUDNN_RNN_PADDED_IO_DISABLED`.", "```\r\nI STARTING Optimization\r\nEpoch 0 |   Training | Elapsed Time: 0:00:00 | Steps: 0 | Loss: 0.000000                                                                                                                                                                                                                                                                                                                                                2\r\n020-07-28 13:51:55.003459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\ngenerate_values <deepspeech_training.util.sample_collections.CSV object at 0x7fce547bcc10>\r\nyield generate_values 0 csvs/../data/A/163_5029_3498779ce37873475394654801cc3888-8fddd9522baf442463171802a7e57489.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7fd2167e4090>\r\nyield generate_values 1 csvs/../data/A/155_4757_9bc6d6f754547a09bbcf70e42d8e2a27-b112945da6818223ab8e1daf80313a62.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7fd2167d8fd0>\r\nyield generate_values 2 csvs/../data/B/98_2923_a387275540ba5f2159c37eaee3e4e9a0-651926517a6241fd9bb5942777b1f0ff.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7fd2167d8cd0>\r\nyield generate_values 3 csvs/../data/B/154_4738_2f841fb1af523c579414e0358ab16295-6aea9aa95b1bdbfd80703754cd8a180c.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7fce547bcc50>\r\nbatch_fn <_VariantDataset shapes: <unknown>, types: tf.string> 2 <_VariantDataset shapes: (?, 26), types: tf.float32> <_VariantDataset shapes: (), types: tf.int32>\r\nbatch_fn <_VariantDataset shapes: <unknown>, types: tf.string> 2 <_VariantDataset shapes: (?, 26), types: tf.float32> <_VariantDataset shapes: (), types: tf.int32>\r\nyield generate_values 4 csvs/../data/C/175_5429_67ed7914b9a3bac4e46dd42a5721a95f-e31a33c85ca8249476596c1ff7fc2f67.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7fd2167d8f90>\r\nyield generate_values 5 csvs/../data/C/169_5271_3210ac3e97626f9c1515cb019e5fa36e-dd839274af12610f137398ddd01f85f8.wav <deepspeech_training.util.sample_collections.LabeledSample object at 0x7fd2167d8e50>\r\nShouldUsePaddedIO time_major=1                                                                    \r\nShouldUsePaddedIO seq_array[0]=74\r\nShouldUsePaddedIO seq_array[1]=74                                                                  \r\nShouldUsePaddedIO [0]: seq_array[i]=74                    \r\nShouldUsePaddedIO [0]: model_shapes.max_seq_length=74                                                \r\nShouldUsePaddedIO [1]: seq_array[i]=74\r\nShouldUsePaddedIO [1]: model_shapes.max_seq_length=74                                                                                                                                                       \r\nShouldUsePaddedIO rv=false all_max_seq_length=true\r\nCudnnRNNForwardOp use_padded_io=0                                                                                                                                                                           \r\nCudnnRNNForwardOp continue use_padded_io=0  \r\nCudnnRNNForwardOp GetCachedRnnDescriptor use_padded_io=0                                                                                                                                                    \r\nGetCachedRnnDescriptor call with use_padded_io=0\r\n--> GetCachedRnnDescriptor[num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 74, 2, 2048]                            \r\n--> GetCachedRnnDescriptor cache miss use_padded_io=0\r\n--> GetCachedRnnDescriptor call CreateRnnDescriptor use_padded_io=0              \r\n-->  CreateRnnDescriptor use_padded_io=0                                                                                                                                                                                                                                                                                                                                                                                      \r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) IN use_padded_io=0\r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) Create use_padded_io=0                                                                                                                                                                                                                                                                                                                                                                                          \r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) Set_v6 use_padded_io=0\r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) checking use_padded_io=0                                                      \r\nShouldUsePaddedIO time_major=1                 \r\nShouldUsePaddedIO seq_array[0]=74\r\nShouldUsePaddedIO seq_array[1]=74                                                                                                                                                                           \r\nShouldUsePaddedIO [0]: seq_array[i]=74\r\nShouldUsePaddedIO [0]: model_shapes.max_seq_length=74                                                                                                                                                       \r\nShouldUsePaddedIO [1]: seq_array[i]=74\r\nShouldUsePaddedIO [1]: model_shapes.max_seq_length=74                                                                                                                                                       \r\nShouldUsePaddedIO rv=false all_max_seq_length=true\r\nCudnnRNNBackwardOp use_padded_io=0                                                                \r\nGetCachedRnnDescriptor call with use_padded_io=0\r\n--> GetCachedRnnDescriptor[num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 74, 2, 2048] \r\n--> GetCachedRnnDescriptor cache miss use_padded_io=0                                            \r\n--> GetCachedRnnDescriptor call CreateRnnDescriptor use_padded_io=0                                                                                                                                             \r\n--> CreateRnnDescriptor use_padded_io=0                                                                                                                                                                         \r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) IN use_padded_io=0                                                                  \r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) Create use_padded_io=0                                                \r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) Set_v6 use_padded_io=0 \r\nstatic stream_executor::port::StatusOr<stream_executor::gpu::CudnnRnnDescriptor> stream_executor::gpu::CudnnRnnDescriptor::Create(const stream_executor::gpu::{anonymous}::CudnnHandle&, int, int, int, int, int, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const stream_executor::dnn::AlgorithmConfig&, float, tensorflow::uint64, stream_executor::ScratchAllocator\r\n*, bool) checking use_padded_io=0                                 \r\nEpoch 0 |   Training | Elapsed Time: 0:00:04 | Steps: 1 | Loss: 188.119797                                                                                                                                                                                                                                                                                                                                              b\r\natch_fn <_VariantDataset shapes: <unknown>, types: tf.string> 2 <_VariantDataset shapes: (?, 26), types: tf.float32> <_VariantDataset shapes: (), types: tf.int32>\r\nShouldUsePaddedIO time_major=1                                                                                                                                                                              \r\nShouldUsePaddedIO seq_array[0]=74                  \r\nShouldUsePaddedIO seq_array[1]=75                                                                                                                                                                           \r\nShouldUsePaddedIO [0]: seq_array[i]=74                \r\nShouldUsePaddedIO [0]: model_shapes.max_seq_length=75                                                                                                                                                       \r\nShouldUsePaddedIO rv=true all_max_seq_length=false\r\nCudnnRNNForwardOp use_padded_io=1                                                                                                                                                                           \r\nCudnnRNNForwardOp continue use_padded_io=1\r\nCudnnRNNForwardOp GetCachedRnnDescriptor use_padded_io=1                                                                                                                                                    \r\nGetCachedRnnDescriptor call with use_padded_io=1\r\n--> GetCachedRnnDescriptor[num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 75, 2, 2048]                                                   \r\n--> GetCachedRnnDescriptor cache hit use_padded_io=1                             \r\n2020-07-28 13:51:59.388865: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_EXECUTION_FAILED                                                                                                          \r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1783): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_de\r\nsc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2020-07-28 13:51:59.388909: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cudnn_rnn_ops.cc:1537 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 75, 2, 2048] \r\n```", "@kaixih So, after checking, I'm wondering if there's not something missing for creating the cache key:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cudnn_rnn_ops.cc#L1083\r\n\r\nWe use `model_shapes` there, which should NOT allow cache hit:\r\n> GetCachedRnnDescriptor[num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 74, 2, 2048]\r\n\r\nis obviously different from:\r\n> GetCachedRnnDescriptor[num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 75, 2, 2048]\r\n\r\nBut according to the log above, we still had a cache hit on this.\r\n\r\nChecking the definition of `CudnnRnnModelShapes` https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cudnn_rnn_ops.cc#L494-L520, it is my understanding that `IsCompatibleWith` is used to compare the model shape: https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cudnn_rnn_ops.cc#L507-L512\r\n\r\nOne can see that this method does **not** take into account `max_seq_length`, which would explain why we get a cache hit with padded disabled when we had it enabled.\r\n\r\nAt least, with that patch, I can't repro anymore the issue, and I can see cache misses **and** cache hits:\r\n```\r\ndiff --git a/tensorflow/core/kernels/cudnn_rnn_ops.cc b/tensorflow/core/kernels/cudnn_rnn_ops.cc\r\nindex 4a27394f28..9098b97462 100644\r\n--- a/tensorflow/core/kernels/cudnn_rnn_ops.cc\r\n+++ b/tensorflow/core/kernels/cudnn_rnn_ops.cc\r\n@@ -508,7 +508,7 @@ struct CudnnRnnModelShapes {\r\n   bool IsCompatibleWith(const CudnnRnnModelShapes& rhs) const {\r\n     return num_layers == rhs.num_layers && input_size == rhs.input_size &&\r\n            num_units == rhs.num_units && dir_count == rhs.dir_count &&\r\n-           cell_num_units == rhs.cell_num_units;\r\n+           cell_num_units == rhs.cell_num_units && max_seq_length == rhs.max_seq_length;\r\n   }\r\n   string DebugString() const {\r\n     return strings::Printf(\r\n```\r\n\r\nI'm unsure if this is correct, maybe we should just include `max_seq_length` into the hash itself?\r\nLooking at `CudnnRnnConfigHasher` defined at https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cudnn_rnn_ops.cc#L522-L550, it computes a hash from the model's shape https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/cudnn_rnn_ops.cc#L531-L533 which is directly used for computing the key.\r\n\r\nThis patch also works (and honestly, it does feel nicer):\r\n```\r\ndiff --git a/tensorflow/core/kernels/cudnn_rnn_ops.cc b/tensorflow/core/kernels/cudnn_rnn_ops.cc\r\nindex 4a27394f28..db7ff98df2 100644\r\n--- a/tensorflow/core/kernels/cudnn_rnn_ops.cc\r\n+++ b/tensorflow/core/kernels/cudnn_rnn_ops.cc\r\n@@ -530,7 +530,7 @@ struct CudnnRnnConfigHasher {\r\n \r\n     uint64 hash =\r\n         HashList({shapes.num_layers, shapes.input_size, shapes.num_units,\r\n-                  shapes.dir_count, shapes.batch_size});\r\n+                  shapes.dir_count, shapes.max_seq_length, shapes.batch_size});\r\n     if (algo_desc.has_value()) {\r\n       hash = Hash64Combine(hash, algo_desc->hash());\r\n     }\r\n```\r\n\r\nWhat's your take @kaixih on this? Does that looks like the root cause of the problem to you?", "@lissyx \r\nLooks very plausible to be the root cause to me! \r\nFrom a code point of view and it also fits with and explains all the patterns we saw while testing and debugging this issue. The same code seems also still present in TF2.x and master, which correlates with all the other reports you found about LSTM with CUDA/CUDNN being unstable while training and are likely related.\r\nGreat catch !\r\n", ">  Great catch !\r\n\r\nThanks, no offense but I'll claim victory once I get feedback from @kaixih :)", "@lissyx Thanks, I agree with you that this should be the root cause. (This also reminds me that I probably forgot to add corresponding changes to the cached RNN desc searching back when adding the variable sequence length feature for cuDNN).\r\n\r\nFor your two patches, I prefer to change both or only the first one. The first one about the IsCompatibleWith used here is mainly for the probing in the hash table, but might be used elsewhere to compare the RNN states. And I think it makes sense to distinguish RNNs with different max_seq_length. The second one about the hash function is mainly for generating the hash key for the RNN states and your patch could give us a little faster searching performance (because we can skip the probing). So, I think the first one is probably a necessity or we simply change both.\r\n\r\nFor the next step, can you create a PR to fix this bug and maybe link to this issue? Thanks.", "> For the next step, can you create a PR to fix this bug and maybe link to this issue? Thanks.\r\n\r\nSure!", "I suspect there won't be a 1.15 dot release including that kind of change, unfortunately?", "Right, I think Google only fixes major issues for 1.15. Can you first create a PR against master? And later they might pick it for 1.15 if necessary? @sanjoy ", "> Right, I think Google only fixes major issues for 1.15. Can you first create a PR against master? And later they might pick it for 1.15 if necessary? @sanjoy\r\n\r\nYep, it's done: https://github.com/tensorflow/tensorflow/pull/41832", "@kaixih So I have been running experiment on ~1000h of french training data, and setting `TF_CUDNN_RESET_RND_GEN_STATE=1` would make one epoch of training about 20 seconds longer, ~23:30 minutes on my hardware from the ~23:10 minutes when using `TF_CUDNN_RESET_RND_GEN_STATE=0`\r\n\r\nDoes that sounds reasonnable to you? Are there any side-effects to using that env variable we should know about?", "> @kaixih So I have been running experiment on ~1000h of french training data, and setting `TF_CUDNN_RESET_RND_GEN_STATE=1` would make one epoch of training about 20 seconds longer, ~23:30 minutes on my hardware from the ~23:10 minutes when using `TF_CUDNN_RESET_RND_GEN_STATE=0`\r\n> \r\n> Does that sounds reasonnable to you? Are there any side-effects to using that env variable we should know about?\r\n\r\n@kaixih Gentle ping: we are close to release 1.0, and we'd like to know exactly what are the consequences of that flag so that we can either force it by default until a (potential 1.15.4 is released) or at least recommend people to use it?\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41630\">No</a>\n", "@kaixih\r\nIs the fix from @lissyx  fix actually enough ?\r\nI have now rebuild tensorflow (1.15.3) with this fix applied and it now doesn't blow up in the training stage of DeepSpeech, but in the validation stage.\r\n\r\nIf i look at the code of:\r\nStatus GetCachedRnnDescriptor(\r\n    OpKernelContext* context,\r\n    const CudnnRnnModelShapes& model_shapes,\r\n    const RnnInputMode& input_mode, \r\n    const AlgorithmConfig& algo_config,\r\n    RnnStateCache* cache, RnnDescriptor** rnn_desc,\r\n    bool use_padded_io\r\n)\r\n\r\n- Is it really enough to only check if the model_shapes match and not input_mode, algo_config and use_padded_io ?\r\n- Another question is if the whole caching is really worthwhile, since lissyx has reported on a test run he didn't find a stellar difference in training duration with or without the caching ?", "Just let you know guys that in my system I need to set TF_CUDNN_RESET_RND_GEN_STATE=1  flag to make it to work even after upgrading to 1.15.4", "me aswell, it does not work without: \"export TF_CUDNN_RESET_RND_GEN_STATE=1 \" in the .sh file i run to fine-tune the model.\r\n\r\nBut it does work with it ! so i am a happy camper again.\r\n\r\nperhaps the documentation should be updated again @lissyx ?", "> me aswell, it does not work without: \"export TF_CUDNN_RESET_RND_GEN_STATE=1 \" in the .sh file i run to fine-tune the model.\r\n> \r\n> But it does work with it ! so i am a happy camper again.\r\n> \r\n> perhaps the documentation should be updated again @lissyx ?\r\n\r\nNo, it means there might be other cases triggering some issue. You are welcome to investigate the internals of TensorFlow / CUDNN.", "I am in the same boat as you, insanely busy with work and my toddler. \r\ni am writing my bachelor thesis now, and it is to be handed 4.th of January.\r\n\r\nwhat i meant is that it might be a good idea to mention this tensorflow cudnn flag in the documentation.\r\nbut i guess that must folks will google the error and find these 3 threads.\r\n\r\ni might look into this in February though.", "> I am in the same boat as you, insanely busy with work and my toddler.\r\n> i am writing my bachelor thesis now, and it is to be handed 4.th of January.\r\n\r\nGood luck, been there, done that, I know how you are right now.\r\n\r\n> \r\n> what i meant is that it might be a good idea to mention this tensorflow cudnn flag in the documentation.\r\n\r\nAs you can see from the discussion, this flag is a workaround not a fix. If there's a documentation that might benefit from it, it would be deepspeech, not tensorflow as much as I can tell.\r\n\r\n> but i guess that must folks will google the error and find these 3 threads.\r\n> \r\n> i might look into this in February though.\r\n\r\nPlease understand that tracking and fixing the root cause in TensorFlow to ensure DeepSpeech users are safe already took me weeks, specifically from the lack of reproductibility. That was when I was 100% of the time on the DeepSpeech project, which is not the case anymore, so, I'm sorry, but I really can't look into that."]}, {"number": 41629, "title": "Parallelize and prioritize model inference on ARM CPU with tensorflow lite.", "body": "I have two different neural networks running with tensorflow lite on an embedded device, using an arm cpu. \r\nMy questions are: How can I execute both models at parallel? How can I give one of the two models a higher priority than the other one? The background is: I do not want one model to block or interrept execution of the other model.\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n", "Excellent"]}, {"number": 41628, "title": "Support Batches in ForwardAccumulator", "body": "cc @allenlavoie", "comments": ["@abhichou4  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "Adding tests for nested Accumulator/GradientTape and tf.function soon.", "Can you check the lint errors?"]}, {"number": 41627, "title": "tf.debugging is not compatible with symbolic Keras tensors", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nFunctions from `tf.debugging` (such as `tf.debugging.assert_equal`) raise an exception when passing a Keras tensor as argument.\r\n\r\nIt appears that the functions run the eager code path even when one of the input is a symbolic tensor.\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.debugging` functions should work with Keras tensors.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.keras.Input(shape=[5], batch_size=2)\r\nbatch_size = tf.shape(x)[0]\r\ntf.debugging.assert_equal(batch_size, 2)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"assert.py\", line 4, in <module>\r\n    tf.debugging.assert_equal(batch_size, 2)\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 648, in assert_equal_v2\r\n    return assert_equal(x=x, y=y, summarize=summarize, message=message, name=name)\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 659, in assert_equal\r\n    data, summarize, message, name)\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 334, in _binary_assert\r\n    if condition:\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 778, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 548, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 537, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```", "comments": ["@guillaumekln \r\nPlease refer to these [links](https://stackoverflow.com/questions/59308263/using-a-tf-tensor-as-a-python-bool-is-not-allowed-in-graph-execution-use-ea) with same error and let us know if it helps\r\n#40916 #37393 #36848 [link](https://github.com/matterport/Mask_RCNN/issues/1911#issuecomment-653752919)", "Unless the usage above is incorrect, I think the issue is inside TensorFlow:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/check_ops.py#L334\r\n\r\n`if condition` is invalid when `condition` is a symbolic tensor.", "@guillaumekln I think we need to use static shape (`x.shape[0]`) during graph definition instead of dynamic shape (`tf.shape(x)[0]`). I changed one line in your code and everything worked as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/f07cf4aa74722a7ba556e5ed7ed4cd38/untitled957.ipynb#scrollTo=dYSub6fKnbpq).\r\n\r\n```\r\n#batch_size = tf.shape(x)[0]\r\nbatch_size = x.shape[0]\r\n\r\n```\r\nThere are couple of stackoverflow and other resources explaining difference between `static` and `dynamic` shape. [Here](https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/) one resource that is good. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks! \r\n", "`tf.shape` is required when the dimension is dynamic and not known at graph building time. Consider the following example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.keras.Input(shape=[5])\r\nassert x.shape[0] is None\r\nbatch_size = tf.shape(x)[0]\r\ntf.debugging.assert_equal(batch_size, 2)\r\n```", "Hi @guillaumekln,\r\n\r\nBy design you can only use tf APIs as layers in Keras Functional models when the APIs take tensors as input and output tensors. The various `tf.debugging` APIs return None (or an op that needs to be used as a control dependency), so they do not support Keras inputs.\r\n\r\nIf you're just looking to do a static shape check here you can statically get the shape instead of getting a symbolic tensor representing the shape by doing:\r\n```\r\nbatch_size = int(x.shape[0])\r\ntf.debugging.assert_equal(batch_size, 2)\r\n```\r\n\r\nOn the other hand, if you want to encode a `tf.debugging.assert` as part of your actual model you can:\r\n1. put it in a `tf.keras.Lambda` layer that returns the inputs (w/ the debugging assert as a side effect run in the lambda),\r\n or 2. Put the `tf.debugging` call in a custom layer / custom model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41627\">No</a>\n"]}, {"number": 41626, "title": "[XLA] Unintuitive behavior of jit_scope", "body": "As far as I can tell, the implementation of `MarkForCompilation` relates to `jit_scope` (`_XlaCompile` attribute) does the following steps:\r\n1. Remove the nodes whose `_XlaCompile` attribute is set to false from the compilation candidates (using `CompilationDisallowedByXlaCompileAttr`).\r\n2. Create a cluster for each candidate using `_XlaScope` as the scope (`absl::nullopt` otherwise).\r\n3. Cluster the one-node clusters and set `is_xla_compile_attr_true` to true if any node in the cluster has `_XlaCompile` true.\r\n4. Add the `kXlaClusterAttr` to node if the `is_xla_compile_attr_true` of its cluster is true (as in `ShouldCompileClusterImpl`).\r\n\r\nThe problem of the above steps is that if we want to only cluster part of the model, for example:\r\n```python\r\ndef stage1(input):\r\n  # stage 1 of the model\r\n\r\ndef stage2(x):\r\n  # stage2 of the model\r\n\r\ndef loss(x):\r\n  # ...\r\n\r\nx = stage1(input)\r\nwith tf.xla.experimental.jit_scope():\r\n  x = stage2(x)\r\nl = loss(x)\r\ntrain_op = optimizer.minimize(l)\r\n\r\n# ...\r\n```\r\nThe unlabeled part of the model may still be clustered and compiled. In practice, such situation happens a lot in the backprop part of the model, where the unlabeled loss gradient ops are clustered with the gradient ops inside `jit_scope`. I wonder if this is the expected behavior and I think that it is different from the intuitive idea that only the ops in `jit_scope` are compiled.\r\n\r\nOne walkaround to this problem is adding a global `jit_scope(compile_ops=False)`:\r\n\r\n```python\r\nwith tf.xla.experimental.jit_scope(compile_ops=False):\r\n  x = stage1(input)\r\n  with tf.xla.experimental.jit_scope():\r\n    x = stage2(x)\r\n  l = loss(x)\r\n  train_op = optimizer.minimize(l)\r\n```\r\n\r\nFollowing are the comparison between the origin result and the walkaround\r\n\r\nwithout the global `jit_scope(compile_ops=False)`:\r\n![image](https://user-images.githubusercontent.com/10428324/88171134-678e0780-cc51-11ea-8b96-32ad29245a35.png)\r\n\r\nwith the global `jit_scope(compile_ops=False)`:\r\n![image](https://user-images.githubusercontent.com/10428324/88171067-4b8a6600-cc51-11ea-9ad4-7895fc830b22.png)\r\n\r\nThe red part of the timeline is the `XlaRun` op.\r\n\r\nI think that to solve this problem, we only need to remove the unlabeled ops from the compilation candidates when global jit level is not set. It will be great if the XLA team would like to share some idea on the current behavior of `jit_scope`.\r\n\r\nAlso, is `global_jit_level` designed to be override the `jit_scope`? It seems that if `global_jit_level` is set, it will ignore the `_XlaScope` attribute and use `_XlaInternalScope` instead. In this way, if one want to compile the whole model, is he or she allowed to separate the cluster arbitrarily?\r\n\r\nThank you for your time on this issue :).\r\n\r\nGently ping @sanjoy @cheshire @tpopp .", "comments": ["@zhuzilin \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\nI believe this issue should be classified into the \"other\" category because it mainly relates to the internal structure of XLA. And because it relates to timeline, I'm not sure it can be easily reproduced through colab. Also, I think the content has already localized the issue... As for the tensorflow version, I think the implementation mentioned exists from at least 1.15.", "This is all running without autoclustering, right?\r\n\r\n> In practice, such situation happens a lot in the backprop part of the model, where the unlabeled loss gradient ops are clustered with the gradient ops inside jit_scope\r\n\r\nTo me it actually kind of makes sense: the computation is marked as \"best-effort compile\", so the gradient over the computation is also \"best-effort compile\"?.. Am I missing something?", "> It seems that if global_jit_level is set, it will ignore the _XlaScope attribute and use _XlaInternalScope instead\r\n\r\nYes, using explicit JIT scopes when autoclustering is on does not really make sense.\r\n\r\nAlso just to set expectations now we are mostly focusing on explicit compilation with `tf.function(experimental_compile=True)` as a preferred API for using XLA in TF.", "@cheshire \r\nThank you for your explanation :).  \r\n> To me it actually kind of makes sense: the computation is marked as \"best-effort compile\", so the gradient over the computation is also \"best-effort compile\"?.. Am I missing something?\r\n\r\nHere is where the misunderstanding emerges. The doc for `jit_scope` says:\r\n> Enable or disable JIT compilation of operators within the scope.\r\n> ...\r\n> The compilation is a hint and only supported on a best-effort basis.\r\n\r\nIt may be wrongly interpreted as \"XLA will try to compile the whole scope, but because XLA may not support compiling all of the scope, it will be regard as a hint.\" instead of \"XLA will may merge the ops in the scope with ones out of it for better performance.\" Also, it will be a little confusing that the compilation result will be different for ops in one place if we add a `jit_scope` at the other. The current behavior will be inconvenient for those who are using `jit_scope` to debug if any part of model is unsuitable to  XLA and has led to negative optimization, because the problem part may always been clustered and compiled. I think that it will be more consistent with the design of tensorflow api that a \"scope\" would only have influence to things inside it.\r\n\r\nI understand that we hope to have better performance by creating larger clusters. If there is little motivation to change the design of `jit_scope`, can I help to clarify this possible misunderstanding in the doc and provide an example for the users who only hope to compile the ops inside a certain scope?", "@zhuzilin For the doc, would you want to send a PR with examples? :P \r\n\r\nFrom a different angle, is `tf.function(experimental_compile=True)` unsuitable for your purposes?", "@cheshire \r\nI'd love to send a pr :). As for `tf.function`, many legacy models in our team are till using tf1.x... So we cannot use it for now... ", "@cheshire \r\nThere is another question relates to `jit_scope`. In `_MaybeCompile()`, we have:\r\n```python\r\ndef _MaybeCompile(scope, op, func, grad_fn):\r\n  \"\"\"Compile the calculation in grad_fn if op was marked as compiled.\"\"\"\r\n  scope = scope.rstrip(\"/\").replace(\"/\", \"_\")\r\n  if func is not None:\r\n    xla_compile = func.definition.attr[\"_XlaCompile\"].b\r\n    xla_separate_compiled_gradients = func.definition.attr[\r\n        \"_XlaSeparateCompiledGradients\"].b\r\n    xla_scope = func.definition.attr[\"_XlaScope\"].s.decode()\r\n  else:\r\n    try:\r\n      xla_compile = op.get_attr(\"_XlaCompile\")\r\n      xla_separate_compiled_gradients = op.get_attr(\r\n          \"_XlaSeparateCompiledGradients\")\r\n      xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    except ValueError:\r\n      return grad_fn()  # Exit early\r\n\r\n  if not xla_compile:\r\n    return grad_fn()  # Exit early\r\n  ...\r\n```\r\nI wonder if the latter \"Exit early\" (`jit_scope(compile_ops=False)` won't have influence to the grad fn) is the expected feature or a bug? Thank you.", "@zhuzilin,\r\n\r\nAs the PR you created for this issue has been merged, Can you confirm if we are good to close this issue? For your follow up question, You can open that as an issue in tf discussion forum, as there is a larger community to answer there. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41626\">No</a>\n"]}, {"number": 41624, "title": "[INTEL MKL] Fix MKlMaxPoolGrad", "body": "This PR fixes a bug in MklMaxPoolGrad.\r\n\r\nThe original implementation will generate error results if fwd and bwd have different memory formats, in which case bwd cannot recognize workspace with correct format. So in this PR we directly use src_md/dst_md to initiate bwd primitive.", "comments": ["I've verified that the two unsuccessful checks ([Windows Bazel](https://source.cloud.google.com/results/invocations/bee2331a-9e02-4b17-a5c6-ff988408bc07/log) and [Windows Bazel GPU](https://source.cloud.google.com/results/invocations/0db45987-2b32-4d43-ab12-7652691210c0/log)) are existing failures and aren't related to this PR."]}, {"number": 41623, "title": "tf error : Failed to load the native TensorFlow runtime.", "body": "\r\n![erreur](https://user-images.githubusercontent.com/68639810/88160941-a56a3c00-cc0f-11ea-8416-0a869e5818a1.png)\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.1\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: NVIDIA Quattro 2000\r\n\r\nI have, at the origins tf2.1, which works very well. For some Reason I have wanted to update to tf2.2. But when I did it \"import tensorflow\" make this error. So I thought that I aven't good composante like GPU, and I try to go back to tf1.15 but nothing changed. so I decided to comme back to the 2.1 version. And now I have also this error. I Don't undestand how a version could worked in the past  but not now.\r\n\r\nI precise that I read a lot of issues and try a lot of thinks but nothing works.\r\nPlease help me\r\n", "comments": ["@zolora \r\n\r\nPlease refer to [this link](https://github.com/tensorflow/tensorflow/issues/36683#issuecomment-585097726) and let us know as this is a common resolved issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41623\">No</a>\n"]}, {"number": 41622, "title": "S3 delete file delete/create dir", "body": "@mihaimaruseac \r\nThis PR adds delete/create dir and delete file for s3.", "comments": ["@shivaylamba Respectfully, I'd ask to refrain from approving PRs when the PR was already approved and the review you submitted has no power to trigger the CI/merge (as you are not an owner of the code / in the organisation). Note the grayed checkmark:\r\n![pr](https://user-images.githubusercontent.com/323199/88308541-00f50200-ccc2-11ea-9189-42833f6dfaf0.png)\r\n\r\nYour review in this case blocked CI and there was a need for another review to unblock. Please don't approve random PRs on GitHub.\r\n"]}, {"number": 41621, "title": "Not found: Key dense/kernel not found in checkpoint", "body": "I am trying to write an encoder/decoder model for a sequence using the TensorFlow estimator and tfa.seq2seq. It is running fine for training and loading the checkpoints when I am training it again but it is not working with the inference. Seems like the encoder is alright but there is some issue with the decoder. \r\n\r\nHere is the training decoder code which estimator.train calls and it is working fine:\r\n\r\n```\r\ndecoder_emb_inp = tf.compat.v1.nn.embedding_lookup(self.embedding_decoder, target_input)\r\nsampler =  tfa.seq2seq.TrainingSampler(time_major=False)\r\nmy_decoder = tfa.seq2seq.BasicDecoder(cell, sampler, output_layer=None)\r\n\r\noutputs, final_context_state, _ = tfa.seq2seq.dynamic_decode(decoder = my_decoder,swap_memory = False,scope ='decoder_scope',\r\n\toutput_time_major=False,decoder_init_input= decoder_emb_inp,\r\n\tdecoder_init_kwargs= {'initial_state' : decoder_initial_state,'sequence_length': tf.tile([self.length], [self.batch_size])})\r\n\r\nsample_id = outputs.sample_id\r\nlogits = self.output_layer(outputs.rnn_output) #self.output_layer=tf.keras.layers.Dense(self.vocab_size)\r\n```\r\n\r\nThis is the code I am using for inference:\r\n\r\n```\r\nstart_tokens = tf.fill([self.batch_size], tf.constant(0))\r\nend_token = tf.constant(0)\r\n\r\nsampler =  tfa.seq2seq.GreedyEmbeddingSampler()\r\nmy_decoder = MyDecoder(cell, sampler, output_layer=self.output_layer)        \r\noutputs, final_context_state, _ = tfa.seq2seq.dynamic_decode(decoder=my_decoder,maximum_iterations=self.length, \r\n\tswap_memory=True, scope='decoder_scope',decoder_init_input=self.embedding_decoder,\r\n\tdecoder_init_kwargs= {'initial_state' : decoder_initial_state,'start_tokens': start_tokens, 'end_token': end_token})\r\n        \r\nlogits = outputs.rnn_output\r\nsample_id = outputs.sample_id\r\n```\r\n\r\nIn inference I'm getting this error:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key BasicDecoderStep/my_dense/kernel not found in checkpoint\r\n         [[{{node save/RestoreV2}}]]\r\n```\r\n\r\nI have tried modifying the output layer, but it is not working. Moreover, if I delete all the checkpoints in the model_dir (passed to estimator), the inference code initializes everything and works fine but doesn't make right predictions.\r\n\r\nSo how do I make inference part read the checkpoints which were created during training and not throw the above error?\r\n\r\nI would appreciate any help to fix this error.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: NA", "comments": ["@mukurgupta \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\n\r\nI have made the miniature version of the model.\r\n\r\nFollowing is the main file for the model:\r\n\r\n\r\n```\r\nimport os\r\nimport numpy as np \r\nimport tensorflow as tf \r\nimport collections\r\n\r\nimport decoder\r\n\r\nmodel_dir='./models'\r\n\r\nparams={}\r\n\r\n#Change this to 'predict' during inference\r\nparams['mode'] = 'predict'\r\n\r\n\r\nparams['source_length']=10\r\nparams['train_epochs']= 5\r\nparams['lr']=0.001\r\n\r\nparams['encoder_length']=5\r\nparams['max_gradient_norm']=5\r\n\r\nparams['decoder_hidden_size']=6\r\nparams['decoder_length']=10\r\nparams['decoder_vocab_size']=10\r\n\r\ndef predict_from_file(estimator):\r\n\tdef infer_input_fn():\r\n\t\tt=np.array([[0,1,2,3,4,5,6,7,8,9]], dtype='int')\r\n\t\tdataset = tf.data.Dataset.from_tensor_slices(t)\r\n\r\n\t\tdef decode_record(record):\r\n\t\t\treturn record, tf.constant([0], dtype=tf.int32)\r\n\t\t\r\n\t\tdataset = dataset.map(decode_record)\r\n\t\tdataset = dataset.batch(1)\r\n\t\titerator = tf.compat.v1.data.make_one_shot_iterator(dataset)\r\n\t\tinputs, targets_inputs = iterator.get_next()\r\n\r\n\t\treturn {\r\n\t\t'decoder_input' : targets_inputs,\r\n\t\t}, None\r\n\tresults = []\r\n\tnew_ids = []\r\n\tperfs = []\r\n\tresult_iter = estimator.predict(infer_input_fn)\r\n\tfor result in result_iter:\r\n\t\toutput = result['sample_id'].flatten()\r\n\t\toutput = ' '.join(map(str, output))\r\n\t\tresults.append(output)\r\n\r\n\r\ndef input_fn(params, mode):\r\n\tt=np.array([[0,1,2,3,4,5,6,7,8,9]])\r\n\tdecoder_target_dataset = tf.data.Dataset.from_tensor_slices(t)\r\n\r\n\tdef decode(decoder_tgt):\r\n\t\tdecoder_src = tf.compat.v1.concat([tf.constant([0]) ,decoder_tgt[:-1]], axis=0)\r\n\t\treturn (decoder_src, decoder_tgt)\r\n\r\n\tdataset = decoder_target_dataset.map(decode)\r\n\t\r\n\tdataset = dataset.repeat(None)\r\n\tdataset = dataset.batch(1)\r\n\titerator = tf.compat.v1.data.make_one_shot_iterator(dataset)\r\n\tbatched = iterator.get_next()\r\n\r\n\tdecoder_input, decoder_target = batched\t\r\n\r\n\treturn{\r\n\t\t'decoder_input': decoder_input,\r\n\t\t'decoder_target': decoder_target\r\n\t}, None\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\tdecoder_input = features['decoder_input']\r\n\t\tdecoder_target = features['decoder_target']\r\n\r\n\t\tmy_decoder = decoder.Model(decoder_input, decoder_target, params, mode, 'Decoder')\r\n\t\ttotal_loss = my_decoder.loss\r\n\r\n\t\tglobal_step = tf.compat.v1.train.get_or_create_global_step()\r\n\t\tlearning_rate = tf.constant(params['lr'])\r\n\t\topt = tf.compat.v1.train.AdamOptimizer(learning_rate=tf.constant(params['lr']))\r\n\r\n\t\twith tf.control_dependencies(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)):\r\n\t\t\tgradients, variables = zip(*opt.compute_gradients(total_loss))\r\n\t\t\tclipped_gradients, _ = tf.clip_by_global_norm(gradients, params['max_gradient_norm'])\r\n\t\t\ttrain_op = opt.apply_gradients(zip(clipped_gradients, variables), global_step=global_step)\r\n\r\n\t\ttf.identity(learning_rate, 'learning_rate')\r\n\r\n\t\treturn tf.estimator.EstimatorSpec(mode=mode,loss=total_loss,train_op=train_op)\r\n\r\n\telif mode == tf.estimator.ModeKeys.PREDICT:\r\n\t\tdecoder_input = features.get('decoder_input', None)\r\n\t\tdecoder_target = features.get('decoder_target', None)\r\n\t\t\r\n\t\tmy_decoder = decoder.Model(decoder_input, decoder_target, params, mode, 'Decoder')\r\n\t\tres = my_decoder.decode()\r\n\t\tsample_id = res['sample_id']\r\n\r\n\t\tpredictions = {\r\n\t\t  'sample_id' : sample_id,\r\n\t\t}\r\n\t\treturn tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\ndef main():\r\n\tif params['mode']=='train':\r\n\t\testimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params)\r\n\t\testimator.train(input_fn=lambda: input_fn(params, 'train'),max_steps=params['train_epochs'])\r\n\t\tprint(\"\\nTraining Done\\n\")\r\n\r\n\telif params['mode']=='predict':\r\n\t\testimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params)\r\n\t\tpredict_from_file(estimator)\r\n\r\nif __name__ == '__main__':\r\n\ttf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\r\n\tmain()\r\n\r\n```\r\n\r\nThe above code runs fine if params['mode']='train' and also creates the checkpoints in the models directory. But on 'predict' mode, it throws error.\r\n\r\nHere is the code for the decoder.py:\r\n\r\n```\r\nimport collections\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow_addons.seq2seq.basic_decoder import BasicDecoderOutput\r\n\r\nclass MyDecoder(tfa.seq2seq.BasicDecoder):\r\n  def __init__(self, cell, sampler, output_layer=None):\r\n    super(MyDecoder, self).__init__(cell, sampler, output_layer)\r\n\r\n  def step(self, time, inputs, state, name=None):\r\n    cell_outputs, cell_state = self.cell(inputs, state)\r\n    if self.output_layer is not None:\r\n      cell_outputs = self.output_layer(cell_outputs)\r\n    sample_ids = self.sampler.sample(\r\n      time=time, outputs=cell_outputs, state=cell_state)\r\n    (finished, next_inputs, next_state) = self.sampler.next_inputs(\r\n      time=time,\r\n      outputs=cell_outputs,\r\n      state=cell_state,\r\n      sample_ids=sample_ids)\r\n    outputs = BasicDecoderOutput(cell_outputs, sample_ids)\r\n    return (outputs, next_state, next_inputs, finished)\r\n\r\nclass Decoder():\r\n  def __init__(self, params, mode, embedding_decoder, output_layer):\r\n    self.hidden_size = params['decoder_hidden_size']\r\n    self.length = params['decoder_length']\r\n    self.source_length = params['encoder_length']\r\n    self.vocab_size = params['decoder_vocab_size']\r\n    self.embedding_decoder = embedding_decoder\r\n    self.output_layer = output_layer\r\n    self.mode = mode\r\n\t\r\n  def build_decoder(self, target_input, batch_size):\t    \r\n    tgt_sos_id = tf.constant(0)\r\n    tgt_eos_id = tf.constant(0)\r\n\r\n    self.batch_size=batch_size\r\n\r\n    with tf.compat.v1.variable_scope('decoder'):\r\n      cell, decoder_initial_state = self.build_decoder_cell()\r\n      \r\n      if self.mode != tf.estimator.ModeKeys.PREDICT:\r\n        decoder_emb_inp = tf.compat.v1.nn.embedding_lookup(self.embedding_decoder, target_input)\r\n        sampler =  tfa.seq2seq.TrainingSampler(time_major=False)\r\n        my_decoder = MyDecoder(cell, sampler, output_layer=None)\r\n\r\n        outputs, final_context_state, _ = tfa.seq2seq.dynamic_decode(decoder = my_decoder,swap_memory = False,scope ='decoder_scope',output_time_major=False,decoder_init_input= decoder_emb_inp,decoder_init_kwargs= {'initial_state' : decoder_initial_state,'sequence_length': tf.tile([self.length], [self.batch_size])})\r\n        \r\n        sample_id = outputs.sample_id\r\n        logits = self.output_layer(outputs.rnn_output)\r\n\r\n      else:\r\n        start_tokens = tf.fill([self.batch_size], tgt_sos_id)\r\n        end_token = tgt_eos_id\r\n\r\n        sampler =  tfa.seq2seq.GreedyEmbeddingSampler()\r\n        my_decoder = MyDecoder(cell, sampler, output_layer=self.output_layer)        \r\n        outputs, final_context_state, _ = tfa.seq2seq.dynamic_decode(decoder=my_decoder,maximum_iterations=self.length, swap_memory=True, scope='decoder_scope',decoder_init_input=self.embedding_decoder,decoder_init_kwargs= {'initial_state' : decoder_initial_state,'start_tokens': start_tokens, 'end_token': end_token})\r\n        \r\n        logits = outputs.rnn_output\r\n        sample_id = outputs.sample_id    \r\n\t\t\r\n    return logits, sample_id, final_context_state\r\n\r\n  def build_decoder_cell(self):\r\n    batch_size= self.batch_size\r\n    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(self.hidden_size,initializer='orthogonal')\r\n    cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1)\r\n\r\n    decoder_initial_state = cell.zero_state(batch_size, tf.float32)\r\n\r\n    return cell, decoder_initial_state\r\n\r\n\r\nclass Model:\r\n  def __init__(self, target_input, target, params, mode, scope=None):\r\n    self.params = params\r\n    self.target_input = target_input\r\n    self.target = target\r\n    self.batch_size = tf.shape(self.target_input)[0]\r\n    self.mode = mode\r\n    self.vocab_size = params['decoder_vocab_size']\r\n    self.decoder_length = params['decoder_length']\r\n    self.hidden_size = params['decoder_hidden_size']\r\n\r\n    initializer = tf.random_uniform_initializer(-0.1,0.1)\r\n    tf.compat.v1.get_variable_scope().set_initializer(initializer)\r\n    self.build_graph(scope=scope)\r\n\r\n  def build_graph(self, scope=None):\r\n    with tf.compat.v1.variable_scope(scope):\r\n      self.W_emb = tf.compat.v1.get_variable('W_emb', [self.vocab_size, self.hidden_size])\r\n      with tf.compat.v1.variable_scope(\"decoder/output_projection\"):\r\n        self.output_layer = tf.keras.layers.Dense(self.vocab_size, use_bias=False)\r\n      \r\n      self.logits, self.sample_id, self.final_context_state = self.build_decoder()\r\n      \r\n      if self.mode != tf.estimator.ModeKeys.PREDICT:\r\n        self.compute_loss()\r\n      else:\r\n        self.loss=None\r\n        self.total_loss=None\r\n\r\n  def build_decoder(self):\r\n    decoder = Decoder(self.params, self.mode, self.W_emb, self.output_layer)\r\n    logits, sample_id, final_context_state = decoder.build_decoder(self.target_input, self.batch_size)\r\n    \t\r\n    return logits, sample_id, final_context_state\r\n\r\n  def compute_loss(self):\r\n    target_output = self.target\r\n    crossent = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=target_output, logits=self.logits)\r\n    tf.identity(crossent, 'cross_entropy')\r\n    self.loss = crossent\r\n\r\n  def decode(self):\r\n    return {'sample_id' : self.sample_id}\r\n\r\n```\r\n\r\nError is:\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key dense/kernel not found in checkpoint\r\n         [[{{node save/RestoreV2}}]]\r\n```\r\n\r\nI'd appreciate your help on this.", "@mukurgupta \r\n\r\nI have tried in colab with TF version 2.2.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/69ebcdb7e6ee4a2eaaa2ce31b3448861/untitled172.ipynb).You are also seeing same behavior?\r\nThanks!", "@ravikyram \r\n\r\nYes you are correct. But you ran it in prediction mode without any checkpoints so it is initializing and won't have any errors but this prediction is wrong as everything is being initialized here and there's no use of training then.\r\n\r\n> INFO:tensorflow:Could not find trained model in model_dir: ./models, running initialization to predict.\r\n\r\nFor right prediction you need to change the mode to train first, create checkpoints in './models' and then run it on predict mode.\r\n\r\n> params['mode'] = 'train'\r\n\r\nOr you can use the checkpoints I'm attaching below and put them in the './models' directory.\r\n\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/4960194/models.zip)\r\n\r\n", "I've also encountered this error, it seems like it is caused by the saver not correctly naming Keras model variables when they are restored in eager mode. \r\n\r\nUnfortunately, you can't seem to load the variables without eager either when the model structure differs, but the variables do not. This becomes particularly apparent in models like the LSTM, which have different encoding and decoding model structures (and thus, the model parameters are not restored properly).", "Thanks to @k-w-w, I was able to get this working by using the following scaffold in the return from the model_fn:\r\n```python\r\ndef model_fn(..., mode):\r\n    ...\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        ...\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            loss=loss,\r\n            train_op=train_op,\r\n            scaffold=tf.compat.v1.train.Scaffold(\r\n                saver=tf.train.Checkpoint(\r\n                    optimizer=optimizer,\r\n                    model=model,\r\n                    global_step=tf.compat.v1.train.get_or_create_global_step())))\r\n     if mode == tf.estimator.ModeKeys.EVAL:\r\n        ...\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            loss=loss,\r\n            scaffold=tf.compat.v1.train.Scaffold(\r\n                  saver=tf.train.Checkpoint(\r\n                      model=model,\r\n                      global_step=tf.compat.v1.train.get_or_create_global_step())))\r\n```\r\n\r\nIf you want to load the weights later, you could use:\r\n```\r\nmodel = tf.keras.Model(...)\r\nckpt = tf.train.Checkpoint(model=model)\r\nckpt.restore(model_dir)\r\n```", "@DavidMChan \r\n\r\nYes, this works for me. Thank you so much for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41621\">No</a>\n"]}]