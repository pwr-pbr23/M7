[{"number": 55358, "title": "MirroredStrategy was not working.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 20.04`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `No mobile device.`\r\n- TensorFlow installed from (source or binary): `Installed tensorflow from pip.`\r\n- TensorFlow version (use command below): tf.version.GIT_VERSION = v2.8.0-rc1-32-g3f878cff5b6; `tensorflow-gpu=2.8.0.`\r\n- Python version: `python=3.10`\r\n- Bazel version (if compiling from source): `Not compiled from source code.`\r\n- GCC/Compiler version (if compiling from source): `No.`\r\n- CUDA/cuDNN version: `cuda-11.5 ; cudnn-8.3.2.44.`\r\n- GPU model and memory: `RTX3090 * 2.`\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nMy neural network callback code was illustrated as follows:\r\n\r\n````\r\nclass ResnetBuilder(object):\r\n    @staticmethod\r\n    @tf.function\r\n    def build(block, blocks_num, im_width=224, im_height=224, num_classes=1000):\r\n        input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\", name=\"layers_inputs\")\r\n        x = layers.Conv2D(filters=64, kernel_size=7, strides=2, padding=\"SAME\",\r\n                          use_bias=False, name=\"layers_conv1\")(input_image)  \r\n        x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n        x = layers.ReLU()(x)\r\n        x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\")(x)\r\n\r\n        x = _make_layer(block, 64, blocks_num[0], layer_name=\"ml1\", strides=1)(x)\r\n        x = _make_layer(block, 128, blocks_num[1], layer_name=\"ml2\", strides=2)(x)\r\n        x = _make_layer(block, 256, blocks_num[2], layer_name=\"ml3\", strides=2)(x)\r\n        x = _make_layer(block, 512, blocks_num[3], layer_name=\"ml4\", strides=2)(x)\r\n\r\n        x = layers.GlobalAvgPool2D()(x)  # pool + flatten\r\n        x = layers.Dense(num_classes, name=\"logits\")(x)\r\n\r\n        predict = layers.Softmax()(x)\r\n        # predict = layers.Activation('softmax', dtype='float32', name='predictions')(x)\r\n\r\n        model = Model(inputs=input_image, outputs=predict)\r\n        return model\r\n\r\n    @staticmethod\r\n    @tf.function\r\n    def resnet101(im_width=224, im_height=224, num_classes=1000):\r\n        # tf.keras.backend.clear_session()\r\n        return ResnetBuilder.build(Bottleneck, [3, 4, 23, 3], im_width, im_height, num_classes)\r\n\r\n    @staticmethod\r\n    @tf.function\r\n    def resnet50(im_width=224, im_height=224, num_classes=1000):\r\n        # tf.keras.backend.clear_session()\r\n        return ResnetBuilder.build(Bottleneck, [3, 4, 6, 3], im_width, im_height, num_classes)\r\n````\r\n\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n` tf.version.GIT_VERSION = v2.8.0-rc1-32-g3f878cff5b6; tensorflow-gpu=2.8.0.`\r\n\r\n**Describe the current behavior**\r\nThe tf.distribute.MirroredStrategy was not working, I have checked cuda, cudnn, driver and nccl, all is well, however, distribution train has frozen, only one GPU worked in this distribution training scene.\r\n**Describe the expected behavior**\r\nI do not know which problem was caused this status.\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No, thanks very much.\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook\r\n\r\n```\r\nimport gc\r\nimport os\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\nfrom utils import save_appoint_csv, FGVCSave\r\nfrom BaseConstruct.ResnetFunctionDepthWise import ResnetBuilder\r\n\r\nprint(tf.compat.v1.executing_eagerly_outside_functions())\r\nnp.random.seed(0)\r\ntf.config.run_functions_eagerly(True) \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\n\r\ndef scheduler(epoch):\r\n    if epoch < 40:\r\n        return 0.1\r\n    if epoch < 100:\r\n        return 0.06\r\n    if epoch < 150:\r\n        return 0.01\r\n    if epoch < 200:\r\n        return 0.006\r\n    return 0.0001\r\n\r\n\r\ndef trainFGVC():\r\n    BATCH_SIZE = 10\r\n    num_classes = 30\r\n    all_labels = []\r\n    epochs = 250\r\n    save_path = r\"*********\"\r\n    save_train_data = r\"*********\"\r\n    train_img_path = r\"*********\"\r\n    test_img_path = r\"*********\"\r\n    train_data = pd.read_csv(r'*********, delimiter=\",\")\r\n\r\n    train_data['label'] = train_data['label'].astype('str')\r\n    Y = train_data[['label']]\r\n    test_data = pd.read_csv(r'*********, delimiter=\",\")\r\n    test_data['label'] = test_data['label'].astype('str')\r\n\r\n    generator = tf.keras.preprocessing.image.ImageDataGenerator(\r\n        rescale=1.0 / 255)  \r\n\r\n    for i in range(num_classes):\r\n        all_labels.append(str(i))\r\n    train_data_generator = generator.flow_from_dataframe(train_data, directory=train_img_path,\r\n                                                         x_col=\"filename\",\r\n                                                         batch_size=BATCH_SIZE, y_col=\"label\",\r\n                                                         class_mode='categorical',\r\n                                                         target_size=(448, 448), shuffle=True,\r\n                                                         classes=all_labels)\r\n\r\n    val_data_generator = generator.flow_from_dataframe(test_data, directory=test_img_path,\r\n                                                       x_col=\"filename\",\r\n                                                       batch_size=BATCH_SIZE, y_col=\"label\",\r\n                                                       class_mode='categorical',\r\n                                                       target_size=(448, 448), shuffle=False,\r\n                                                       classes=all_labels)\r\n    kept = tf.keras.callbacks.ModelCheckpoint(save_train_data + \"\\\\\" + \"model.{epoch:02d}-{val_loss:.4f}-\" +\r\n                                              str(0) + \".h5\", monitor='val_accuracy', verbose=1,\r\n                                              save_best_only=True, mode='max')\r\n\r\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=save_train_data + \"\\\\\" + r\"TensorBoard\",\r\n                                                          write_graph=True,\r\n                                                          histogram_freq=5)\r\n    lr_reducer = tf.keras.callbacks.LearningRateScheduler(scheduler)\r\n    loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\r\n\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    with strategy.scope():\r\n        model = ResnetBuilder.resnet101(im_width=448, im_height=448, num_classes=num_classes)\r\n        optimizer = tfa.optimizers.SGDW(weight_decay=1e-5, learning_rate=0.02, momentum=0.9)\r\n\r\n    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'], run_eagerly=True)\r\n\r\n    history = model.fit(x=train_data_generator, validation_data=val_data_generator, epochs=epochs,\r\n                        callbacks=[kept, tensorboard_callback, lr_reducer])\r\n\r\n    print(history.history[\"val_recall\"])\r\n    data_dic = {\"val_accuracy\": history.history['val_accuracy'],\r\n                \"val_loss\": history.history['val_loss'],\r\n                \"val_recall\": history.history[\"val_recall\"],\r\n                \"val_precision\": history.history[\"val_precision\"]\r\n                }\r\n\r\n    flag, path = save_appoint_csv(data_dic, 0, save_path)\r\n\r\n    if flag == -1:\r\n        print(\"save training data false! Please check function code!\")\r\n    elif flag == 1:\r\n        print(\"save training data successful! Save path is \", path)\r\n\r\n    del history\r\n    del model\r\n    for i in range(999):\r\n        gc.collect()\r\n\r\n\r\nif __name__ == '__main__':\r\n    trainFGVC()\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n`I am sorry, there was no log or information has collected from logging.`", "comments": ["@jiayugedede ,\r\nCan you please provide the error log which you are facing issue.It helps to analyse the issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55358\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55358\">No</a>\n"]}, {"number": 55357, "title": "@tensorflow/tfjs: Unable to load model", "body": "Using Webpack to build a node app with @tensorflow/tfjs included, when using tf.loadLayersModel() the following error is received:\r\n\r\n```\r\nwebpack:///./node_modules/@tensorflow/tfjs-core/dist/platforms/platform_node.js?:59\r\n        return systemFetch(path, requestInits);\r\n               ^\r\n\r\nTypeError: systemFetch is not a function\r\n```\r\n\r\nWhen looking into the node_module, it does appear that this is a bug in the tensorflowjs core code. This happens when trying to load a model via an http request or a direct file (since this is a node application)", "comments": ["@mcmanussean Could you please post this issue in [tensorflow/tfjs](https://github.com/tensorflow/tfjs/issues) repo to get the right help there?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mcmanussean  Could you please confirm if you have posted this issue in tfjs repo and please refer to [this](https://github.com/tensorflow/tfjs/issues/3883) thread with similar error?\r\nThanks!\r\n", "Closing this issue due to lack of recent activity.Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55357\">No</a>\n"]}, {"number": 55356, "title": "\"lengh\" -> \"length\"", "body": "Fix tiny typo", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac ", "Agreed that it is a relatively trivial single character document change.\nBut that's not the whole point.\n\nIt's typical for potential contributors to a project to dip their toes in\nwith small changes to get acquainted with the pull request process for a\nproject to gauge the appetite the maintainers have to onboard new folks.\nNobody is going to start with a massive body of work just to find out that\nthe project doesn't accept contributions.\n\nTo be honest, this is the first project I've come across that has willfully\nkept typos in documentation because the process of accepting commits was\ndeemed too expensive. If the process to incorporate pull requests is unduly\ncumbersome, perhaps that is a bug in the process?", "Hi @dweekly - appreciate your feedback! I'd agree we need a better solution for very small patches. Unfortunately like a lot of high profile projects we are the recipient of people opportunistically submitting trivial patches in order to boost their resume or other online reputation, vs being in actual earnest about growing their contribution.\r\n\r\nWe vet incoming patches for security and correctness, and the expense of doing this is not proportionate. The downside for contributors is however the upside for users: this process is what lets us ship the same TensorFlow in open source as the one that Google uses internally.\r\n\r\nHowever, we know there's a way to go. For folks who are wanting to start working with the project, we have some Getting Started guidance at https://www.tensorflow.org/community/contribute. In particular, it is often easy to start work within one of the [Special Interest Groups](https://www.tensorflow.org/community/contribute/sigs), where the burden is less and these strictures don't apply.\r\n\r\nThanks for engaging: please feel free to contact me if you'd like to discuss further.", "Thank you. I appreciate your being willing to engage on this - and sorry to write so much about a single character fix!\r\n\r\nI guess I didn't consider the angle of resume-building trivial contributions but was mainly thinking about it from the perspective of encouraging folks to onboard into real contributions.\r\n\r\nI'll take another look at your helpful getting started guide. Thank you.", "Hi @dweekly  Sorry, this change is part of [PR#55342](https://github.com/tensorflow/tensorflow/pull/55342) which is created by another contributor before this PR created. Can you please close this PR? Thank you!"]}, {"number": 55355, "title": "TF.data.dataset.cache(path) still uses memory despite the cache file path is given in tf2.8", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ntf2.8.0\r\n- Python version:\r\npython 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nCUDA v11.2. cuDNN v8.1\r\n- GPU model and memory:\r\nK80, gMem=16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI came across a weird problem when I read TFrecords files from S3 through tf.dataset and cached them to my local path. Here is my reading code\r\n\r\n    filenames=['s3s:path1', ''s3s:path2']\r\n    dataset = tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\r\n    parsed_dataset = (\r\n        dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE)\r\n        .map(decode, num_parallel_calls=tf.data.AUTOTUNE)\r\n        .cache(cache_file_path)\r\n        .prefetch(tf.data.AUTOTUNE)\r\n    )\r\nIt\u2019s very strange that cache() still uses the internal memory which results in OOM. Here is the memory usage I printed via callback during training.\r\n\r\n2022-03-08T22:19:40.154191003Z ...Training: end of batch 15700; got log keys: ['loss', 'copc', 'auc']\r\n2022-03-08T22:19:40.159188560Z totalmemor: 59.958843GB\r\n2022-03-08T22:19:40.159223737Z availablememory: 8.418320GB\r\n2022-03-08T22:19:40.159250296Z usedmemory: 50.959393GB\r\n2022-03-08T22:19:40.159257814Z percentof used memory: 86.000000\r\n2022-03-08T22:19:40.159263710Z freememory:1.072124GB\r\n2022-03-08T22:19:47.752077011Z Tue Mar  8 22:19:47 UTC 2022\tjob-submitter:\tjob run error: signal: killed\r\n\r\n**Describe the expected behavior**\r\nI have tested the code on TF2.3 which has no such an issue, but TF2.8 have such an OOM issue. It's supposed that when the path is given, cache(path) should use files to cache instead of memory.\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["If need any details, please let me know. Thanks", "@yusenzhan \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@yusenzhan \r\nCould you please respond  to the [comment](https://github.com/tensorflow/tensorflow/issues/55355#issuecomment-1077336476) above?\r\nIf you are using  this  \r\n```\r\nif cache: self.train_ds.cache()\r\n```\r\nPlease change this to following as it will increase the performance\r\n```\r\nif cache: train_hr_ds = train_hr_ds.cache()\r\n```\r\nThank you!", "Closing this issue due to lack of recent activity. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55355\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55355\">No</a>\n"]}, {"number": 55354, "title": "[oneDNN] Upgrade oneDNN to 2.6-rc", "body": "Update oneDNN from 2.5.1 to 2.6-rc", "comments": []}, {"number": 55353, "title": "[TF-TRT] Slice OP BugFix", "body": "@bixia1 for review\r\n\r\nThis PR fixes a bug where TF-TRT rejects a Slice OP in the conversion phase instead of the validation phase", "comments": ["Currently TF-TRT may reject Slice OPs during the conversion phase, which obviously leads to:\r\n\r\n```bash\r\n2022-03-24 01:00:01.700857: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1055] TF-TRT Warning: Engine creation for TRTEngineOp_0_2 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/tf2tensorrt/convert/ops/slice_ops.cc:116 TRT_ENSURE failure\r\n```\r\n\r\nCC: @tfeher ", "@bixia1 can you please review ?\r\n"]}, {"number": 55352, "title": "Update oneDNN version for mkl_dnn_acl_compatible.", "body": "Updates the mkl_dnn_acl_compatible for oneDNN 2.6.", "comments": ["Hi @penpornk,\r\nThe oneDNN 2.6 release isn't available yet, but a number of AArch64 patches are available in the RC, which this PR is currently using. \r\n", "Note: There's an equivalent PR for x86, updating mkldnn_v1.BUILD, here - https://github.com/tensorflow/tensorflow/pull/55354"]}, {"number": 55351, "title": "Integrating cuBLASLt into XLA", "body": "Adds support for the cuBLASLt library for GEMM operations to XLA. The library can be activated by setting the XLA flag `xla_gpu_enable_cublaslt=true`.\r\n\r\nAttn: @SandSnip3r", "comments": ["@philipphack It was again confirmed that this change caused drastically increased memory usage (out-of-memory errors). We are investigating.", "@philipphack As found by someone else, the suspicion is that the code seems to be restructured so that memory assignments in `gemm_algorithm_picker.cc` are done even if there is a cache hit in `DoGemmAutotune`. Looking at the code, previously, we used to check the cache before allocating this lhs_buffer, rhs_buffer, etc. Now, we always allocate them, then check the cache afterwards. The usecase that was triggering this out-of-memory error was seeing an accumulated allocation from this part of the code with no matching frees.", "@SandSnip3r I changed `DoGemmAutotune` and opened PR 55518. Can you take a look?", "> @SandSnip3r I changed `DoGemmAutotune` and opened PR 55518. Can you take a look?\r\n\r\nYeah, I see it. I'll check it out. Thanks."]}, {"number": 55350, "title": "Fix pfor defunc issue", "body": "This is trying to fix the indirect defunc case with a vectorized_map", "comments": ["@mdanatg  I've tried to add a dummy failing test (we could improve it as you like).\r\n\r\nThe goal here is to fix https://github.com/tensorflow/tensorflow/issues/55340\r\n\r\nI see that we are checking if we are in eager mode and we wrap the function in that case.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f62c8863b3e19b81d2484a95e96ddb854ea66b2f/tensorflow/python/ops/parallel_for/control_flow_ops.py#L183-L205\r\n\r\n/cc @wangpengmit\r\n\r\n\r\n", "@mdanatg I've tried to insert  `f = autograph.tf_convert(f, autograph_ctx.control_status_ctx())` there but it is failing with the same error\r\n", "I've moved `autograph.tf_convert` internally. I don't know if the coverage is complete but locally the test is passing.\r\nLet me know if and how we want to improve this.", "@gbaned Can you connect this PR to https://github.com/tensorflow/tensorflow/issues/55340. So when it is merged it will close the ticket.", "@mdanatg Let me know if you like this test or you want to test something else.", "/cc\u00a0@wangpengmit"]}, {"number": 55349, "title": "TF-lite conversion of complex abs layer not working with integer quantization with fallbacks", "body": "I'm trying to optimize a speech-to-text Conformer model for tflite usage. Quantization of the model is working for _default_ optimization mode, but not when a _representative dataset_ is used. In this case it fails in inference with: `type != kTfLiteFloat32 (INT8 != FLOAT32) Node number 46 (COMPLEX_ABS) failed to prepare`\r\n\r\nShouldn't the model use the _float32_ fallback in this case, since I'm not enforcing _int8_ operators?\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20:04\r\n- TensorFlow installation (pip package or built from source): Nvidia-Docker\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.7\r\n\r\n\r\n### 2. Failure after conversion\r\n\r\n```\r\nTranscribing the audio ...\r\nLabel:             they were subjected to constant surveillance and periodic searches\r\nTraceback (most recent call last):\r\n  File \"/Scribosermo/exporting/testing_file.py\", line 161, in <module>\r\n    main()\r\n  File \"/Scribosermo/exporting/testing_file.py\", line 155, in main\r\n    test_tflite(checkpoint_tflite_quant)\r\n  File \"/Scribosermo/exporting/testing_file.py\", line 118, in test_tflite\r\n    prediction = predict(interpreter, audio)\r\n  File \"/Scribosermo/exporting/testing_file.py\", line 92, in predict\r\n    interpreter.invoke()\r\n  File \"/usr/local/lib/python3.8/dist-packages/tflite_runtime/interpreter.py\", line 923, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: /workspace/tensorflow/lite/kernels/complex_support.cc:43 output-> \\\r\n  type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (COMPLEX_ABS) failed to prepare.\r\n```\r\n\r\n### 3. Code snippets\r\n\r\n```python3\r\n# Conversion settings\r\ndef export_tflite(model, save_path, optimize):\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n    if optimize:\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.representative_dataset = representative_dataset\r\n\r\n    tflite_model = converter.convert()\r\n\r\n    with open(save_path, \"wb+\") as file:\r\n        file.write(tflite_model)\r\n\r\n# Spectrogram definition\r\ndef audio_to_spect(self, audio):\r\n        \"\"\"Calculate the spectrogram\"\"\"\r\n\r\n        # Pytorch uses a slightly different spectrogram calculation which is matched to librosa\r\n        # unlike the default tensorflow implementation\r\n        n_fft = 512\r\n        nbatch = tf.shape(audio)[0]\r\n\r\n        # Add center padding\r\n        signal = tf.reshape(audio, [nbatch, -1])\r\n        pad_amount = int(self.audio_window_samples // 2)\r\n        signal = tf.pad(signal, [[0, 0], [pad_amount, pad_amount]], \"REFLECT\")\r\n        signal = tf.reshape(signal, [nbatch, 1, -1])\r\n\r\n        # Calculate short-time Fourier transforms with a differnt windowing approach\r\n        f = tf.signal.frame(\r\n            signal, self.audio_window_samples, self.audio_step_samples, pad_end=False\r\n        )\r\n        w = tf.signal.hann_window(self.audio_window_samples, periodic=False)\r\n        stfts = tf.signal.rfft(f * w, fft_length=[n_fft])\r\n\r\n        # Obtain the magnitude of the STFT.\r\n        spectrogram = tf.abs(stfts) ** 2\r\n        spectrogram = tf.squeeze(spectrogram, axis=1)\r\n\r\n        return spectrogram\r\n```\r\n\r\n<br>\r\n\r\nRelated issue: https://github.com/tensorflow/tensorflow/issues/53393#issuecomment-1009703703 (was closed without solution)", "comments": ["Fails as well when I'm replacing the `abs` with `spectrogram = tf.math.real(stfts) ** 2 + tf.math.imag(stfts) ** 2`:\r\n\r\n `RuntimeError: /workspace/tensorflow/lite/kernels/complex_support.cc:43 output->type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (IMAG) failed to prepare.`", "Hi @DanBmh ! Can you please share the model along with code containing representative dataset to replicate the issue? I was going to propose these changes for [integer quantization.](https://www.tensorflow.org/lite/performance/post_training_integer_quant)  Thanks!\r\n\r\n```\r\ndef export_tflite(model, save_path, optimize):\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n    if optimize:\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.representative_dataset = representative_dataset\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8 , tf.lite.OpsSet.SELECT_TF_OPS ]\r\n   # Set the input and output tensors to uint8 (APIs added in r2.3)\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_model = converter.convert()\r\n\r\n    with open(save_path, \"wb+\") as file:\r\n        file.write(tflite_model)\r\n```\r\n\r\n", "The actual model is quite complicated, but I could implement a shorter example:\r\n\r\n```\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tflite_runtime.interpreter as tflite\r\nfrom tensorflow.keras import layers as tfl\r\nfrom tensorflow.python.framework.ops import enable_eager_execution\r\n\r\n# ==================================================================================================\r\n\r\nexportdir = \"/tmp/tftests/\"\r\n\r\n# ==================================================================================================\r\n\r\n\r\nclass AudioLayer(tfl.Layer):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.audio_window_samples = int(16000 * 0.025)\r\n        self.audio_step_samples = int(16000 * 0.01)\r\n\r\n    # ==============================================================================================\r\n\r\n    def audio_to_spect(self, audio):\r\n        \"\"\"Calculate the spectrogram\"\"\"\r\n\r\n        # Pytorch uses a slightly different spectrogram calculation which is matched to librosa\r\n        # unlike the default tensorflow implementation\r\n        n_fft = 512\r\n        nbatch = tf.shape(audio)[0]\r\n\r\n        # Add center padding\r\n        signal = tf.reshape(audio, [nbatch, -1])\r\n        pad_amount = int(self.audio_window_samples // 2)\r\n        signal = tf.pad(signal, [[0, 0], [pad_amount, pad_amount]], \"REFLECT\")\r\n        signal = tf.reshape(signal, [nbatch, 1, -1])\r\n\r\n        # Calculate short-time Fourier transforms with a differnt windowing approach\r\n        f = tf.signal.frame(\r\n            signal, self.audio_window_samples, self.audio_step_samples, pad_end=False\r\n        )\r\n        w = tf.signal.hann_window(self.audio_window_samples, periodic=False)\r\n        stfts = tf.signal.rfft(f * w, fft_length=[n_fft])\r\n\r\n        # Obtain the magnitude of the STFT.\r\n        spectrogram = tf.math.real(stfts) ** 2 + tf.math.imag(stfts) ** 2\r\n        spectrogram = tf.squeeze(spectrogram, axis=1)\r\n\r\n        return spectrogram\r\n\r\n    # ==============================================================================================\r\n\r\n    def call(self, x, training=False):  # pylint: disable=arguments-differ\r\n\r\n        audio = tf.expand_dims(x, axis=-1)\r\n        spectrogram = self.audio_to_spect(audio)\r\n        return spectrogram\r\n\r\n\r\n# ==================================================================================================\r\n\r\n\r\nclass MyModel(tf.keras.Model):  # pylint: disable=abstract-method\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.model = self.make_model()\r\n\r\n    # ==============================================================================================\r\n\r\n    def make_model(self):\r\n        input_tensor = tfl.Input(shape=[None], name=\"input_samples\")\r\n\r\n        # Used for easier debugging changes\r\n        audio = tf.identity(input_tensor)\r\n\r\n        spectrogram = AudioLayer()(audio)\r\n\r\n        x = tf.reduce_sum(spectrogram)\r\n        output_tensor = tf.identity(x, name=\"logits\")\r\n\r\n        model = tf.keras.Model(input_tensor, output_tensor, name=\"name\")\r\n        return model\r\n\r\n    # ==============================================================================================\r\n\r\n    def summary(self):  # pylint: disable=arguments-differ\r\n        print(\"\")\r\n        self.model.summary(line_length=100)\r\n\r\n    # ==============================================================================================\r\n\r\n    # This input signature is required that we can export and load the model in \".pb\" format\r\n    # with a variable sequence length, instead of using the one of the first input.\r\n    @tf.function(input_signature=[tf.TensorSpec([None, None], tf.float32)])\r\n    def call(self, x):  # pylint: disable=arguments-differ\r\n        \"\"\"Call with input shape: [1, len_signal], output shape: [1, len_steps, n_alphabet]\"\"\"\r\n\r\n        x = self.model(x)\r\n        return x\r\n\r\n\r\n# ==================================================================================================\r\n\r\n\r\ndef representative_dataset():\r\n    for i in range(100):\r\n        data = np.random.rand(1, (i % 10 + 1) * 1000)\r\n        yield [data.astype(np.float32)]\r\n\r\n\r\n# ==================================================================================================\r\n\r\n\r\ndef export_tflite(model, save_path, optimize, fullint):\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n    if optimize:\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    if fullint:\r\n        converter.representative_dataset = representative_dataset\r\n\r\n    tflite_model = converter.convert()\r\n\r\n    with open(save_path, \"wb+\") as file:\r\n        file.write(tflite_model)\r\n\r\n\r\n# ==================================================================================================\r\n\r\n\r\ndef predict(interpreter, audio):\r\n    \"\"\"Feed an audio signal with shape [1, len_signal] into the network and get the predictions\"\"\"\r\n\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Enable dynamic shape inputs\r\n    interpreter.resize_tensor_input(input_details[0][\"index\"], audio.shape)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Feed audio\r\n    interpreter.set_tensor(input_details[0][\"index\"], audio)\r\n    interpreter.invoke()\r\n\r\n    output_data = interpreter.get_tensor(output_details[0][\"index\"])\r\n    return output_data\r\n\r\n\r\n# ==================================================================================================\r\n\r\n\r\ndef main():\r\n    if not os.path.isdir(exportdir):\r\n        os.makedirs(exportdir)\r\n\r\n    # Eager execution is required for pb and tflite exports\r\n    enable_eager_execution()\r\n\r\n    model_tl = MyModel()\r\n    model_tl.build(input_shape=(None, None))\r\n    model_tl.summary()\r\n\r\n    print(\"\\nExporting models ...\")\r\n    export_tflite(\r\n        model_tl, exportdir + \"model_full.tflite\", optimize=False, fullint=False\r\n    )\r\n    export_tflite(\r\n        model_tl, exportdir + \"model_quantized.tflite\", optimize=True, fullint=False\r\n    )\r\n    export_tflite(model_tl, exportdir + \"model_int.tflite\", optimize=True, fullint=True)\r\n\r\n    print(\"\\nmodel_full:\")\r\n    interpreter = tflite.Interpreter(model_path=exportdir + \"model_full.tflite\")\r\n    print(\"Input details:\", interpreter.get_input_details())\r\n    print(predict(interpreter, np.random.rand(1, 1234).astype(np.float32)))\r\n\r\n    print(\"\\nmodel_quantized:\")\r\n    interpreter = tflite.Interpreter(model_path=exportdir + \"model_quantized.tflite\")\r\n    print(\"Input details:\", interpreter.get_input_details())\r\n    print(predict(interpreter, np.random.rand(1, 1234).astype(np.float32)))\r\n\r\n    print(\"\\nmodel_int:\")\r\n    interpreter = tflite.Interpreter(model_path=exportdir + \"model_int.tflite\")\r\n    print(\"Input details:\", interpreter.get_input_details())\r\n    print(predict(interpreter, np.random.rand(1, 1234).astype(np.float32)))\r\n\r\n\r\n# ==================================================================================================\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n    print(\"\\nFINISHED\")\r\n\r\n```", "Ok @DanBmh ! I used tf.lite.interpreter from version 2.8 instead of tflite.interpreter from tflite_runtime. Attaching resolved [gist](https://colab.sandbox.google.com/gist/mohantym/3a378726bd039b780f5691bb631906e1/untitled328.ipynb) for reference. Thanks!", "Thanks, it is working for me with `tf.lite` instead of `tflite_runtime` and `v=2.8` as well. (I kept float32 input and output)\r\nDo you know when `tflite_runtime=2.8` will be released?\r\nMy target is a RaspberryPi and using the full tensorflow installation isn't really feasible.", "I also tried the export with the full model, which is working too, but the outputs are now incorrect. \r\nInstead of predicting some text the model now only predicts _blank_ labels (normally the most common label), so it returns an empty text...\r\nThere was no problem with the default quantization approach without the representative dataset (I tried different sizes between 20 and 2000 samples, resulting in slight changes of the softmax output, but no real changes of the argmax label).\r\nDo you have an idea about this?\r\nIs it common that _int8_ quantization can break the model?", "@DanBmh ! Regarding the Raspberry pi deployment and tflite_run_time 2.8 , You can check this [repo](https://github.com/PINTO0309/TensorflowLite-bin). Can you check the wave to spectrogram  part of document mentioned in these documents [1](https://www.tensorflow.org/tutorials/audio/simple_audio) ,[ 2](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio) for debugging the prediction part? ", "I think I could solve my problem with the full model. For mel-spectrogram and feature-normalization I'm adding very small values to the tensors to prevent _log_ and _div_ with zero, but with the _int8-quantization_ those \"zero-guards\" are just set to zero themselves, so I'm getting _nan_ values inbetween...\r\n\r\nDo you know the smallest value I can use? Currently I'm using `2**-5`, and `2**-6` was to small.\r\n\r\nThanks for the tip to the spectrogram debugging documents. You can find my complete debugging script here: https://gitlab.com/DANBER/Scribosermo/-/blob/cfc/extras/misc/complex_tflite_test.py\r\n\r\nRegarding the version problem, it seems that _2.8_ is only required for exporting.\r\n\r\nI also found that, presumably only in the case of the example script, using a larger number (>= 100) of representative examples decreases the performance of quantization. I think the effect could be smaller if there is a real model after the feature calculation. What do you think about this?", "@DanBmh ! Glad you sorted out the issue .  Try with \"1.001e-5\" as the minimum value to avoid zero division error.  You can batch the data in the representative dataset if you are worried about performance( which seems to be implemented already in gitlab link). Please move this to closed status if it worked. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "The smallest value working as zero-guard for `log` was `2 ** -5`, smaller values sometimes resulted in `-inf` results. The smallest value for `div` was `1/255`, smaller values sometimes resulted in `nan` results.\r\n\r\nBatching might work here, but I think it won't work for my real representative dataset, because the audio inputs have quite different lengths.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55349\">No</a>\n"]}, {"number": 55348, "title": "summarization  ", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\nAs per our\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\nwe only address code/doc bugs, performance issues, feature requests and\nbuild/installation issues on GitHub. tag:performance_template</em>\n\n**System information**\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n- TensorFlow installed from (source or binary):\n- TensorFlow version (use command below):\n- Python version:\n- Bazel version (if compiling from source):\n- GCC/Compiler version (if compiling from source):\n- CUDA/cuDNN version:\n- GPU model and memory:\n\nYou can collect some of this information using our environment capture\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\nYou can also obtain the TensorFlow version with:\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\n\n**Describe the current behavior**\n\n**Describe the expected behavior**\n\n**Standalone code to reproduce the issue**\nProvide a reproducible test case that is the bare minimum necessary to generate\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\n\n**Other info / logs** Include any logs or source code that would be helpful to\ndiagnose the problem. If including tracebacks, please include the full\ntraceback. Large logs and files should be attached.", "comments": []}, {"number": 55347, "title": "MirroredStrategy", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@FannySimoes  \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!"]}, {"number": 55346, "title": "Opening GUIs on host machine using the TensorFlow docker image", "body": "Hello everyone \r\n\r\nI'm using TensorFlow gpu docker image and it is working just fine on my python scripts. \r\nMy issue is trying to open some GUI applications like spyder-ide.\r\n\r\nThe _Dockerfile_ used to build the image is:\r\n```\r\nFROM tensorflow/tensorflow:latest-gpu\r\nRUN apt-get update\r\nRUN apt-get install python3-opencv libasound2 -y\r\nRUN pip install --upgrade pip\r\nRUN pip install spyder\r\nRUN pip install numpy\r\nRUN pip install pandas\r\nRUN pip install sklearn\r\nRUN pip install matplotlib\r\nWORKDIR /home\r\n```\r\n\r\nand the docker arguments used to start the image are\r\n```\r\nsudo docker run -it \\\r\n    --gpus all \\\r\n    --env=\"DISPLAY\" \\\r\n    --net=host \\\r\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\r\n    -v $PWD:/home imagename/imagename \\\r\n    spyder\r\n```\r\n\r\nBut I'm getting the following error...\r\n\r\n```\r\nroot@350X:/home# spyder\r\nCould not load the Qt platform plugin \"xcb\" in \"\" even though it was found.\r\nThis application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.\r\n\r\nAvailable platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.\r\n\r\nAborted (core dumped)\r\n```\r\n\r\nAny ideas how could I open GUIs on the host machine using the TensorFlow docker image ?\r\nI tried reinstalling the application and getting these plugins but had no success in opening GUIs.\r\n", "comments": ["@caliari-italo ,\r\nCan you please take a look at this link [1](https://github.com/google/mediapipe/issues/1373), [2](https://github.com/google/mediapipe/issues/1373) and [3](https://github.com/tensorflow/tensorflow/issues/16658) with the similar error.It helps.Thanks!", "Unfortunately any of these approaches could solve my problem. I managed to open Firefox app from this docker image without problems but spyder is giving me a hard time with\r\n\r\n```\r\nroot@350X:/home# spyder\r\nCould not load the Qt platform plugin \"xcb\" in \"\" even though it was found.\r\nThis application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.\r\n\r\nAvailable platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.\r\n\r\nAborted (core dumped)\r\n```", "The problem was solved following  `https://github.com/spyder-ide/spyder/issues/17542`"]}, {"number": 55345, "title": "How to convert Keras LSTM model into tf.nn.rnn_cell.LSTMCell model", "body": "I have trained a LSTM model on pytorch, and converted it into Keras lstm model with `set_weights` method. The performance looks almost the same. However, I need to convert the model into lstm model with older tensorflow version (V1.xx), by using `tf.nn.rnn_cell.LSTMCell` method. What confused me was that I did not find any method that can load lstm weights from numpy like arrays. The only method I find is loading saved checkpoints, but not suitable for me.\r\nAny suggestion for me? Thanks.", "comments": ["@Ironbrotherstyle,\r\n\r\nThis is not a bug or feature request, for any queries you may open this issue in tf discussion [forum](https://discuss.tensorflow.org/) as there is a larger community there. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55345\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55345\">No</a>\n"]}, {"number": 55343, "title": "Normalization and BatchNormalization layer does not rescale or normalize inputs, am I missing anything?", "body": "**System information**\r\n\r\ncolab with tf-2.8\r\n[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/8330439/old-tf_env.txt)\r\n\r\n---\r\n\r\n**Describe the current behavior**\r\n\r\nNormalization and Batch Normalization layer does not rescale or normalize inputs as excepted, the inputs barely not changing.\r\n\r\n---\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n## Tensorflow:\r\n\r\n###  Normalization layer\r\n```python\r\ntfdata = np.array([[1,2,3,4,5]],dtype=float)\r\nprint(\"tfdata:\" ,tfdata)\r\nprint(\"avg: \", np.average(tfdata))\r\nprint(\"var: \", np.var(tfdata))\r\n```\r\n>tfdata: [[1. 2. 3. 4. 5.]]\r\navg:  3.0\r\nvar:  2.0\r\n\r\n\r\n\r\n```python\r\nnormalization_layer = tf.keras.layers.Normalization(axis=-1)\r\nnormalized_data = normalization_layer(tfdata)\r\nprint(normalized_data.numpy())\r\nprint(\"normalized avg: \", np.average(normalized_data.numpy()))\r\nprint(\"normalized var: \", np.var(normalized_data.numpy()))\r\n```\r\n**output:** \r\n\r\n>[[1. 2. 3. 4. 5.]]\r\nnormalized avg:  3.0\r\nnormalized var:  2.0\r\n\r\n**This output is unchanged**\r\n\r\n\r\n### BatchNormalization layer\r\n\r\n```python\r\n\r\nbatch_normalization_layer = tf.keras.layers.BatchNormalization(axis=-1,scale=True,center=True)\r\nbatch_normalized_data = batch_normalization_layer(tfdata)\r\nprint(batch_normalized_data.numpy())\r\nprint(\"batch normalized avg: \", np.average(batch_normalized_data.numpy()))\r\nprint(\"batch normalized var: \", np.var(batch_normalized_data.numpy()))\r\n```\r\n**output:**\r\n>[[0.9995004 1.9990008 2.9985013 3.9980016 4.997502 ]]\r\nbatch normalized avg:  2.9985013\r\nbatch normalized var:  1.9980018\r\n\r\n---\r\n\r\n**This output still unchanged!**\r\n\r\n## Torch:\r\n\r\n### LayerNorm\r\n\r\n```python\r\ntdata = torch.tensor([1,2,3,4,5],dtype=torch.float32)\r\nnormalization_layer=torch.nn.LayerNorm(normalized_shape=[5])\r\nnormalized_data = normalization_layer(tdata).detach().numpy()\r\nprint(normalized_data)\r\nprint(\"normalized avg: \", np.average(normalized_data))\r\nprint(\"normalized var: \", np.var(normalized_data))\r\n```\r\n\r\n**output**\r\n\r\n>[-1.4142101  -0.70710504 -0.00000006  0.7071049   1.4142098 ]\r\nnormalized avg:  -4.7683717e-08\r\nnormalized var:  0.9999949\r\n\r\nnormalized av closed to 0, and normalized var close to 1, correct answer.\r\n\r\nand BatchNormalization ... all the same\r\n", "comments": ["@jayagami ,\r\nCan you please elaborate about your issue here.Also please confirm which output you are mentioning as the right one in both? It helps to debug the issue.Thanks!", "> @jayagami , Can you please elaborate about your issue here.Also please confirm which output you are mentioning as the right one in both? It helps to debug the issue.Thanks!\r\n\r\nThanks for replying.\r\n\r\nWell,  both Normalization and BatchNormalization layer in tf are not working.\r\n\r\naccording to BatchNormalization definition:\r\n\r\n![image](https://user-images.githubusercontent.com/40586590/159834449-b71459b6-b513-4fec-bf9e-fa34742744dc.png)\r\n\r\n\r\nIn the outputs, the mean should be close to 0, and the variance should be close to 1. (ignore the gamma, beta\u2026\u2026)\r\nHowever, as you can see, the outputs are unchanged in tensorflow.\r\n\r\n\r\n\r\nAs a comparison, Torch gives the expected answer:\r\n\r\n>normalized avg: -4.7683717e-08\r\nnormalized var: 0.9999949\r\n\r\nBy the way, Layer Normalization is working as excepted in tf 2.8.\r\n\r\nThis is a very basic layer, and I doubt I'm missing something.\r\n", "besides, this output is different with examples in  [Normalization docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization#args_1)\r\n\r\n```python\r\nadapt_data = np.array([[0., 7., 4.],\r\n                       [2., 9., 6.],\r\n                       [0., 7., 4.],\r\n                       [2., 9., 6.]], dtype='float32')\r\ninput_data = np.array([[0., 7., 4.]], dtype='float32')\r\nlayer = tf.keras.layers.Normalization(axis=-1)\r\nlayer.adapt(adapt_data)\r\nlayer(input_data)\r\n```\r\n\r\nthe outputs:\r\n\r\n><tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-1., -1., -1.]], dtype=float32)>\r\n\r\n", "I checked the code, could be a keras problem.\r\n\r\n\r\nThe call method is quite short, seems mean and variance are never calculated\r\n\r\nhttps://github.com/keras-team/keras/blob/v2.8.0/keras/layers/preprocessing/normalization.py#L300-L306\r\n\r\n```python\r\n  def call(self, inputs):\r\n    inputs = self._standardize_inputs(inputs)\r\n    # The base layer automatically casts floating-point inputs, but we\r\n    # explicitly cast here to also allow integer inputs to be passed\r\n    inputs = tf.cast(inputs, self.compute_dtype)\r\n    return ((inputs - self.mean) /\r\n            tf.maximum(tf.sqrt(self.variance), backend.epsilon()))\r\n```\r\n\r\nand the mean is initialized as 0, variance is initialized as 1, unless you manually set it.\r\n\r\nhttps://github.com/keras-team/keras/blob/v2.8.0/keras/layers/preprocessing/normalization.py#L244-L298\r\n\r\n", "I re-checked the documentation and found it right in front of me, surprised I hadn't noticed this paragraph before.\r\n\r\nI've used BatchNormalization with unprocessed data and got unexpected results, which is confusing me.\r\nI focus on the Normalization issue and ignore the differences in their implementation.\r\n\r\nI understand the need for this design, which is very different from torch. I should have paid more attention to these details.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55343\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55343\">No</a>\n"]}, {"number": 55342, "title": "fix typos", "body": "I hope it helps.", "comments": ["Can you try to fix multiple typos in the same file/directory please? As you see, the CI time is significant (and this is ~1/3rd of the total CI if we take into account the internal CI too). We're trying to minimize the hours_of_ci/delta_of_change metric", "Thank you for telling me. I'll try to find it.", "I'll look for a little more"]}, {"number": 55341, "title": "Error Cannot copy to a TensorFlowLite tensor", "body": "When i traid to process my model  got this error:\r\n`java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_conv2d_input:0) with 90000 bytes from a Java Buffer with 22500 bytes.`\r\n\r\nHere are the libraries i'm using:\r\n`implementation 'org.tensorflow:tensorflow-lite:2.8.0'`\r\n   ` implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'`\r\n    `implementation 'org.tensorflow:tensorflow-lite-metadata:0.3.1'`\r\n\r\n[Here ](https://i.stack.imgur.com/1njLj.png)you can see my model properties:\r\n\r\nI'm traing to config the output of this way, i'm not sure if is of this way..\r\n`val p = photoFile.data!!.extras!!.get(\"data\") as Bitmap`\r\n            `val tensorImage = TensorImage.fromBitmap(p)`\r\n            `val model = Modelo.newInstance(applicationContext)1`\r\n            `val imageProcessor: ImageProcessor = ImageProcessor.Builder()`\r\n                `.add(ResizeOp(150, 150, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR))`\r\n                `.add(TransformToGrayscaleOp())`\r\n                `.add(NormalizeOp(0f,255f))`\r\n                `.build()`\r\n            `imageProcessor.process(tensorImage)`\r\n", "comments": ["Hi @AndryCU ! Can you let us know the results after changing the resize factor to (300,300)(90000 bytes) instead of (150,150)(22500 bytes) in this line ?        \r\n\r\n`.add(ResizeOp(150, 150, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR))`\r\n\r\nAttaching relevant thread for reference. R [1](https://github.com/ultralytics/yolov5/issues/6912) ,[2 ](https://stackoverflow.com/questions/66958808/fatal-exception-inference-cannot-copy-to-a-tensorflowlite-tensor-from-a-java-b)", "@mohantym it's works but, why change that value if my model was trained with images of 150x150 in greyscale with pixels values between 0 and 1???? Can you explain me??? Also who can i set each pixel value from 0 to1?? I will really apreciet it", "@AndryCU ! The tensor buffer size is determined by datasize (float32: 4bytes) * flat size of the tensor shape (1 * height * width * 1). (4x22500 = 90000 bytes) . Can you allocate a buffer size of 90000 bytes in above method and let us know?\r\n\r\n`val imgData = ByteBuffer.allocateDirect(4 * 150 * 150 * 1 * 1);\r\n`\r\n", "@mohantym sorry, can you explain better, i don't know what to do sorry, if you preffer you can text me on telegram @andryssd, of course i will post evething here, but i want more interchange only if you can", "@AndryCU ! Sorry ,I was proposing these changes to have a buffer size of 90000 bytes automatically. Attaching relevant [thread ](https://www.tensorflow.org/lite/inference_with_metadata/lite_support)for reference. Thanks!\r\n\r\n```\r\nimport org.tensorflow.lite.DataType;\r\nimport org.tensorflow.lite.support.image.ImageProcessor;\r\nimport org.tensorflow.lite.support.image.TensorImage;\r\nimport org.tensorflow.lite.support.image.ops.ResizeOp;\r\nimport org.tensorflow.lite.support.common.ops.NormalizeOp;\r\n\r\n\r\n#val p = photoFile.data!!.extras!!.get(\"data\") as Bitmap\r\n#val tensorImage = TensorImage.fromBitmap(p)\r\n\r\nImageProcessor imageProcessor =\r\n    new ImageProcessor.Builder()\r\n       .add(new ResizeOp(150, 150, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR)))\r\n       .add(TransformToGrayscaleOp())\r\n       .add(NormalizeOp(0,255))\r\n       .build();\r\n\r\n// Create a TensorImage object. This creates the tensor of the corresponding\r\n// tensor type (Float32 in this case) that the TensorFlow Lite interpreter needs.\r\nTensorImage tensorImage = new TensorImage(DataType.FLOAT32); // setting Datatype float32 to allocate buffer size 90000 automatically\r\n\r\n// Analysis code for every frame\r\n// Preprocess the image\r\ntensorImage.load(bitmap);\r\ntensorImage = imageProcessor.process(tensorImage);\r\n```\r\n\r\n", "It's work perfectly. Thanks, i only have to said that you wrote some lines in java, but, android studio detects it and convert to kotlin automatically. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55341\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55341\">No</a>\n"]}, {"number": 55340, "title": "Autovectorization fail with tf.vectorized_map and range", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nColab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\nNightly\r\n- Python version:\r\nColab\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(10)\r\ndef outer_product(a):\r\n  for i in tf.range(10):\r\n    pass\r\n  return tf.tensordot(a, a, 0)\r\n\r\n@tf.function\r\ndef vectorized(a):\r\n  return tf.vectorized_map(outer_product, a)  \r\n\r\nbatch_size = 100\r\na = tf.ones((batch_size, 32, 32))\r\nc = vectorized(a)\r\nassert c.shape == (batch_size, 32, 32, 32, 32)\r\n```\r\n\r\n```\r\nOperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\n```\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Just to expose a few reformulations it seems that we have some side effect with `tf.vectorized_map` in an autovectorizzation region.\r\n\r\nThis is working correctly instead:\r\n```python\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(10)\r\ndef outer_product(a):\r\n  for i in range(10):\r\n    pass\r\n  return tf.tensordot(a, a, 0)\r\n\r\n@tf.function\r\ndef vectorized(a):\r\n  return tf.vectorized_map(outer_product, a)  \r\n\r\nbatch_size = 100\r\na = tf.ones((batch_size, 32, 32))\r\nc = vectorized(a)\r\nassert c.shape == (batch_size, 32, 32, 32, 32)\r\n```\r\n\r\nThis is failing\r\n```python\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(10)\r\ndef outer_product(a):\r\n  for i in range(tf.constant(10)):\r\n    pass\r\n  return tf.tensordot(a, a, 0)\r\n\r\n@tf.function\r\ndef vectorized(a):\r\n  return tf.vectorized_map(outer_product, a)  \r\n\r\nbatch_size = 100\r\na = tf.ones((batch_size, 32, 32))\r\nc = vectorized(a)\r\nassert c.shape == (batch_size, 32, 32, 32, 32)\r\n```\r\n\r\n```python\r\n    TypeError: 'Tensor' object cannot be interpreted as an integer\r\n```\r\n\r\nThis is failing:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(10)\r\ndef outer_product(a):\r\n  for i in tf.range(tf.constant(10)):\r\n    pass\r\n  return tf.tensordot(a, a, 0)\r\n\r\n@tf.function\r\ndef vectorized(a):\r\n  return tf.vectorized_map(outer_product, a)  \r\n\r\nbatch_size = 100\r\na = tf.ones((batch_size, 32, 32))\r\nc = vectorized(a)\r\nassert c.shape == (batch_size, 32, 32, 32, 32)\r\n```\r\n```python\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\n````\r\n\r\nMoving Autovectorizzation to execlude `tf.vectorized_map`:\r\n\r\nThis is working\r\n```python\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(10)\r\n@tf.function\r\ndef outer_product(a):\r\n  for i in tf.range(tf.constant(10)):\r\n    pass\r\n  return tf.tensordot(a, a, 0)\r\n\r\ndef vectorized(a):\r\n  return tf.vectorized_map(outer_product, a)  \r\n\r\nbatch_size = 100\r\na = tf.ones((batch_size, 32, 32))\r\nc = vectorized(a)\r\nassert c.shape == (batch_size, 32, 32, 32, 32)\r\n```\r\n\r\nThis is working:\r\n```python\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(10)\r\n@tf.function\r\ndef outer_product(a):\r\n  for i in range(tf.constant(10)):\r\n    pass\r\n  return tf.tensordot(a, a, 0)\r\n\r\ndef vectorized(a):\r\n  return tf.vectorized_map(outer_product, a)  \r\n\r\nbatch_size = 100\r\na = tf.ones((batch_size, 32, 32))\r\nc = vectorized(a)\r\nassert c.shape == (batch_size, 32, 32, 32, 32)\r\n```\r\n\r\nThis is working:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(10)\r\n@tf.function\r\ndef outer_product(a):\r\n  for i in range(10):\r\n    pass\r\n  return tf.tensordot(a, a, 0)\r\n\r\ndef vectorized(a):\r\n  return tf.vectorized_map(outer_product, a)  \r\n\r\nbatch_size = 100\r\na = tf.ones((batch_size, 32, 32))\r\nc = vectorized(a)\r\nassert c.shape == (batch_size, 32, 32, 32, 32)\r\n```\r\n\r\n/cc @mdanatg ", "P.s. just a note the `tf.vectorized_map` example was derived from official example in docstring:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/vectorized_map#examples", "Looking at the detailed logs, this raw error points at the culprit:\r\n\r\n```\r\nINFO:tensorflow:Caught error in user callable <function outer_factory.<locals>.inner_factory.<locals>.tf__vectorized at 0x7fe7ed8998c0>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/__autograph_generated_file8fgy2tkv.py\", line 12, in tf__vectorized\r\n    retval_ = ag__.converted_call(ag__.ld(tf).vectorized_map, (ag__.ld(outer_product), ag__.ld(a)), None, fscope)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 377, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\r\n    return f(*args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 545, in vectorized_map\r\n    fallback_to_while_loop=fallback_to_while_loop)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 202, in pfor\r\n    outputs = f()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 187, in f\r\n    parallel_iterations=parallel_iterations)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 281, in _pfor_impl\r\n    loop_fn_outputs = loop_fn(loop_var)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 525, in loop_fn\r\n    return fn(gathered_elems)\r\n  File \"<ipython-input-1-780f7c7a2a2e>\", line 4, in outer_product\r\n    for i in range(tf.constant(10)):\r\nTypeError: 'Tensor' object cannot be interpreted as an integer\r\n```\r\n\r\nThe failure happens in pure Python code, which means that `_pfor_impl` fails to call autograph. The solution should be relatively straightforward, there is an internal `tf_convert` call designed to do the right thing (i.e. apply autograph if enabled in the parent function, ensure the error message is accurate, etc.). I think that's the reason why the error message is incorrect (it says that \"AutoGraph did convert this function\", when it in fact didn't).\r\n\r\nA good example of how `tf_convert` is called can be found in the `tf.data` code: https://github.com/tensorflow/tensorflow/blob/0a8a781e597b18ead006d19b7d23d0a369e9ad73/tensorflow/python/data/ops/structured_function.py#L177\r\n", "Thanks for the hint /cc @wangpengmit", "@bhack Can we close this issue, since associated PR got merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55340\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55340\">No</a>\n"]}, {"number": 55339, "title": "[TF-TRT] Make TF-TRT respect TF32 ON/OFF TF Flag", "body": "@bixia1 for review.\r\n\r\nAllow TF-TRT to respect TensorFlow flag for disable/enable TF32", "comments": ["@bixia1 can you provide feedback why this PR seems to be failing some unittests inside Google"]}, {"number": 55338, "title": "Tensorflow installation in venv results in RuntimeError when importing the library", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Using pip\r\n- TensorFlow version: Latest (cannot determine version after installation)\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?: Using pip, inside venv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen trying to install Tensorflow using pip inside a virtual environment, installation completes, but importing the library results in a RuntimeError.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. `python3 -m venv venv`\r\n2. `. venv/bin/activate`\r\n3. `python3 -m pip install tensorflow`\r\n4. `python3 -c \"import tensorflow as tf; print(tf.__version__)\"`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFirst 3 steps work as expected, 4th step results in the following errors:\r\n```\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd.\r\n...\r\nImportError: SystemError: <built-in method __contains__ of dict object at 0x7f47294d6b80> returned a result with an error set\r\n```\r\nI have tried manually upgrading numpy, but that didn't resolve my issue.\r\n", "comments": ["Finally got a solution.\r\nNot sure what I did different, but here are the steps that worked:\r\n1. `python3 -m venv --system-site-packages ./venv`\r\n2. `source ./venv/bin/activate`\r\n3. `pip install --upgrade pip`\r\n4. `pip install --upgrade tensorflow`\r\n\r\nAfter that, the previous import step worked.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55338\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55338\">No</a>\n"]}, {"number": 55336, "title": "undefined references on tflite x86_64 cross compiled with NDK Android toolchain", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 21.04\r\n- TensorFlow version: 2.8.0\r\n- GCC/Compiler version: Clang 9.0.8, from Android NDK 21.3.6528147\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm able to build `libtensorflow-lite.a` for Android x86_64, but when I try to link the library I get\r\n```\r\nerror: undefined reference to 'xnn_setup_runtime'\r\n```\r\nIf I build the lib without XNN delegate support, I got an error on NNAPI. If also disable NNAPI, I got undefined reference errors for ryu related functions. It seems like that all the delegate symbols in the built static library are undefined.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nto build  `libtensorflow-lite.a`:\r\n```\r\nmkdir build\r\ncd build\r\ncmake -DCMAKE_TOOLCHAIN_FILE=~/Android/Sdk/ndk/21.3.6528147/build/cmake/android.toolchain.cmake -DANDROID_ABI=x86_64  ../tensorflow/lite\r\nmake -j12\r\n```\r\nlater linked using CMake on an Android project with `target_link_libraries(...)`.\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\noutput of `nm -C libtensorflow-lite.a | grep xnn`:\r\n```\r\n0000000000000000 T tflite::xnnpack::DequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int, double)\r\n0000000000000000 T tflite::xnnpack::DequantizeFloat16(unsigned short const*, float*, unsigned long)\r\n0000000000000000 T tflite::xnnpack::PerChannelDequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int const*, float const*, int)\r\nxnnpack_delegate.cc.o:\r\n                 U xnn_create_runtime_v2\r\n                 U xnn_create_subgraph\r\n                 U xnn_define_abs\r\n                 U xnn_define_add2\r\n                 U xnn_define_argmax_pooling_2d\r\n                 U xnn_define_average_pooling_2d\r\n                 U xnn_define_bankers_rounding\r\n                 U xnn_define_ceiling\r\n                 U xnn_define_channelwise_quantized_tensor_value\r\n                 U xnn_define_clamp\r\n                 U xnn_define_convert\r\n                 U xnn_define_convolution_2d\r\n                 U xnn_define_deconvolution_2d\r\n                 U xnn_define_depth_to_space\r\n                 U xnn_define_depthwise_convolution_2d\r\n                 U xnn_define_divide\r\n                 U xnn_define_elu\r\n                 U xnn_define_floor\r\n                 U xnn_define_fully_connected\r\n                 U xnn_define_global_average_pooling_2d\r\n                 U xnn_define_hardswish\r\n                 U xnn_define_leaky_relu\r\n                 U xnn_define_maximum2\r\n                 U xnn_define_max_pooling_2d\r\n                 U xnn_define_minimum2\r\n                 U xnn_define_multiply2\r\n                 U xnn_define_negate\r\n                 U xnn_define_prelu\r\n                 U xnn_define_quantized_tensor_value\r\n                 U xnn_define_sigmoid\r\n                 U xnn_define_softmax\r\n                 U xnn_define_square\r\n                 U xnn_define_squared_difference\r\n                 U xnn_define_square_root\r\n                 U xnn_define_static_constant_pad\r\n                 U xnn_define_static_reshape\r\n                 U xnn_define_static_resize_bilinear_2d\r\n                 U xnn_define_subtract\r\n                 U xnn_define_tensor_value\r\n                 U xnn_define_unpooling_2d\r\n                 U xnn_delete_runtime\r\n                 U xnn_delete_subgraph\r\n                 U xnn_initialize\r\n                 U xnn_invoke_runtime\r\n                 U xnn_setup_runtime\r\n0000000000000000 b guard variable for tflite::xnnpack::(anonymous namespace)::Delegate::Delegate(TfLiteXNNPackDelegateOptions const*)::s_logged\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphFree(TfLiteContext*, void*)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphInit(TfLiteContext*, char const*, unsigned long)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphInvoke(TfLiteContext*, TfLiteNode*)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphPrepare(TfLiteContext*, TfLiteNode*)\r\n0000000000000000 d tflite::xnnpack::(anonymous namespace)::kSubgraphRegistration\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReluNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, float, float, std::__ndk1::vector<unsigned int, std::__ndk1::allocator<unsigned int> > const&)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckPoolingParams(TfLiteContext*, TfLitePoolParams const*, int)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckMediaPipePoolParams(TfLiteContext*, TfLitePoolParams const*, int)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckTensorQInt8OrQUInt8Type(tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteTensor const&, int, int)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CalculateTransposeConvPaddings(TfLiteContext*, TfLitePadding, int, int, int, int, int, int, int, int, int, int, int, int*, int*, int*, int*, int*, int*)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckTensorFloat32OrQCInt8Type(tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteTensor const&, int, int, int)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckTensorFloat32OrQUInt8Type(tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteTensor const&, int, int)\r\n0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::VisitNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteRegistration*, TfLiteNode*, int, std::__ndk1::unordered_set<int, std::__ndk1::hash<int>, std::__ndk1::equal_to<int>, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<unsigned int, std::__ndk1::allocator<unsigned int> > const&)\r\n                 U tflite::xnnpack::DequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int, double)\r\n                 U tflite::xnnpack::DequantizeFloat16(unsigned short const*, float*, unsigned long)\r\n                 U tflite::xnnpack::PerChannelDequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int const*, float const*, int)\r\n0000000000000000 t bool std::__ndk1::__insertion_sort_incomplete<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)\r\n0000000000000000 t void std::__ndk1::__sort<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)\r\n0000000000000000 W void std::__ndk1::vector<xnn_external_value, std::__ndk1::allocator<xnn_external_value> >::__push_back_slow_path<xnn_external_value const&>(xnn_external_value const&)\r\n0000000000000000 t unsigned int std::__ndk1::__sort3<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)\r\n0000000000000000 t unsigned int std::__ndk1::__sort4<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)\r\n0000000000000000 t unsigned int std::__ndk1::__sort5<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, int*, int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)\r\n0000000000000000 b tflite::xnnpack::(anonymous namespace)::Delegate::Delegate(TfLiteXNNPackDelegateOptions const*)::s_logged\r\ntflite_with_xnnpack_optional.cc.o:\r\n```\r\n\r\n", "comments": ["Hi @mtamburrano ! Sorry for the late response. Could you rebuild  your Cmake project with below changes and let us know the difference?\r\n\r\n```\r\ncmake -DCMAKE_TOOLCHAIN_FILE=<NDK path>/build/cmake/android.toolchain.cmake \\\r\n  -DANDROID_ABI=arm64-v8a ../tensorflow_src/tensorflow/lite/examples/minimal <change to location containing your .cc file and cmakelist.txt file>  -DTFLITE_ENABLE_XNNPACK=ON\r\n\r\ncmake --build . -j\r\n```", "hi @mohantym \r\nI'm sorry but I'm not sure what are you trying to suggest. My project is an Android app, that uses some JNI cpp modules that I build through CMake, but I don't see the point to add `-DTFLITE_ENABLE_XNNPACK=ON` there because of course that CMakeList is not aware on that flag.\r\nInstead, if you wanted to make me to build tflite for `arm64-v8a` arch instead than for `x86_64` like I was doing previously, I tried but the outcome is the same, I got a lot of undefined symbols in the built `libtensorflow-lite.a` that then cause undefined reference when I try to link that library in my android project", "I managed to figure out what the issue is.\r\nI was supposing that after building all the third party dependencies, all the static libraries would have been combined in a single one (libetensorflow-lite.a).\r\nInstead there are a bunch of libraries that need to be linked in the `_deps/` folder. Is it not possible to produce a single fat static library with everything needed to link an executable, instead having to rely to all those different *.a files?\r\nI can produce it by merging all the libraries together with AR commands, but I wonder if this functionality is already supported somehow by the current CMakeLists", "@mtamburrano ! you can add a static library using [add_library (..)](https://www.youtube.com/watch?v=abuCXC3t6eQ)and use STATIC keyword along with desired libraries in it. Attaching relevant thread for reference. [1](https://stackoverflow.com/a/41495207/11530462), [2](https://www.youtube.com/watch?v=abuCXC3t6eQ) . Feel free to move this to closed status if it helped .Thanks!  ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55336\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55336\">No</a>\n"]}, {"number": 55333, "title": "[TF-TRT] Fix `quantization_ops_test` build error", "body": "@bixia1 for review\r\n\r\nThis PR fixes a dependency bug introduced by #53082", "comments": []}, {"number": 55332, "title": "[TF-TRT] Adding `TF_TRT_EXPERIMENTAL_FEATURES` environment variable controller", "body": "This PR adds the APIs to query if an experimental feature is activated and the test utility necessary to test these features.\r\nExample: `TF_TRT_EXPERIMENTAL_FEATURES=feature_name1,feature_name2`", "comments": ["@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@DEKHTIARJonathan there are still unaddressed comments.", "@bixia1 I was still working on it. You can approve the PR, everything is clear ;) \r\n@Nyrio we are good, you can cherrypick this PR in yours", "@DEKHTIARJonathan I'm not sure that `_experimental_feature_scope` should be in `tf_trt_integration_test_base.py`.\r\nI need it in `trt_convert_test.py`, so it would make sense to put it in a separate `test_utils.py` or similar file.", "@DEKHTIARJonathan would you please address the comments and add a PR description?", "> @DEKHTIARJonathan I'm not sure that `_experimental_feature_scope` should be in `tf_trt_integration_test_base.py`. I need it in `trt_convert_test.py`, so it would make sense to put it in a separate `test_utils.py` or similar file.\r\n\r\nDone\r\n\r\n@bixia1: all comments have been addressed. Please review"]}, {"number": 55327, "title": "I dont have TransformToGrayscaleOp class in android using tensorflowlite", "body": "I want to transform my image to grayscale in android but when i traid to use [TransformToGrayscaleOp](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/support/image/ops/TransformToGrayscaleOp?hl=en) class i got this error in the ide: _**Unresolved reference: TransformToGrayscaleOp**_\r\n\r\nHere's my code until now:\r\n\r\n        val imageProcessor: ImageProcessor = ImageProcessor.Builder()\r\n            .add(ResizeOp(150, 150, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR))\r\n            .add( TransformToGrayscaleOp()) //<----- dont find this class\r\n            .build()\r\n        imageProcessor.process(tensorImage)`\r\n\r\n**build.gradle**\r\n\r\n`implementation 'org.tensorflow:tensorflow-lite:2.8.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-support:0.1.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0'`\r\n\r\nWhy i dont have that class?", "comments": ["Solved. I update to a new version of tensorflow-lite-support 0.3.1"]}, {"number": 55326, "title": "[Accuracy] round issue on tf.nn.conv2d, it does not return the same results for a kernel depth size < 8 vs kernel depth size > 16", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04 ($uname -a)\r\n`Linux *** 5.0.0-37-generic #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n- TensorFlow installed from : pypi.org\r\n- TensorFlow version: tested on 2.4.1, 2.6.0, 2.6.2, 2.8.0\r\n  - tf.version.VERSION: \"2.6.2\"\r\n  - tf.version.GIT_VERSION: 'v2.6.1-9-gc2363d6d025'\r\n  - tf.version.COMPILER_VERSION: '7.3.1 20180303'\r\n- Python version: 3.6.9, 3.8.0, 3.9.11\r\n\r\n- CUDA/cuDNN version: no CUDA\r\n- GPU model and memory: no GPU\r\n\r\n**Describe the current behavior**\r\nComparing a 2d-convolution with kernel depth =16 with two 2d-convolution with kernel depth = 8 then concatenated would provide strictly the same results. However, it is not the case. Trace is:\r\n```\r\n>> test with kernel depth = 32 pass\r\n>> test with kernel depth =  8 pass\r\nTraceback (most recent call last):\r\n  File \"/***/test_conv2d/tf_conv2d_d8d8_vs_d16.py\", line 76, in <module>\r\n    _test(d)\r\n  File \"/***/test_conv2d/tf_conv2d_d8d8_vs_d16.py\", line 71, in _test\r\n    numpy.testing.assert_allclose(out_1, out_2)\r\n  File \"/***/.local/share/virtualenvs/python-3.9-venv/lib/python3.9/site-packages/numpy/testing/_private/utils.py\", line 1527, in assert_allclose\r\n    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\r\n  File \"/***/.local/share/virtualenvs/python-3.9-venv/lib/python3.9/site-packages/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0\r\n\r\nMismatched elements: 2044522 / 4194304 (48.7%)\r\nMax absolute difference: 1.7881393e-07\r\nMax relative difference: 0.10666173\r\n x: array([[[[ 0.283143,  0.117474,  0.101533, ...,  0.302509, -0.060525,\r\n          -0.123441],\r\n         [ 0.044446, -0.08746 ,  0.078083, ...,  0.173368,  0.143438,...\r\n y: array([[[[ 0.283143,  0.117474,  0.101533, ...,  0.302509, -0.060525,\r\n          -0.123441],\r\n         [ 0.044446, -0.08746 ,  0.078083, ...,  0.173368,  0.143438,...\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt appears that:\r\n- for a depth size of 8 vs 4-4, results are strictly the same\r\n- for a depth size of 16 vs 8-8, results are not strictly the same (round issue?)\r\n- for a depth size of 32 vs 16-16, results are strictly the same\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): ...\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\ntf_conv2d_d8d8_vs_d16.py\r\n```python\r\nimport numpy\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model_1conv2d(tf.Module):\r\n    def __init__(self, kernel):\r\n        super().__init__()\r\n        self.conv2d_weigths = tf.constant(kernel)\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec(shape=(1, 512, 512, 3), dtype=tf.float32, name='input')])\r\n    def f(self, input_1):\r\n        conv2d = tf.nn.conv2d(\r\n            input_1,\r\n            self.conv2d_weigths,\r\n            data_format=\"NHWC\",\r\n            padding=\"SAME\",\r\n            dilations=[1, 1],\r\n            strides=[1, 1, 1, 1],\r\n            name=\"conv2d\")\r\n        return conv2d\r\n\r\nclass Model_2conv2d(tf.Module):\r\n    def __init__(self, k1, k2):\r\n        super().__init__()\r\n        self.conv2d_weigths_1 = tf.constant(k1)\r\n        self.conv2d_weigths_2 = tf.constant(k2)\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec(shape=(1, 512, 512, 3), dtype=tf.float32, name='input_1_input')\r\n    ])\r\n    def f(self, input_1):\r\n        conv2d_1 = tf.nn.conv2d(\r\n            input_1,\r\n            self.conv2d_weigths_1,\r\n            data_format=\"NHWC\",\r\n            padding=\"SAME\",\r\n            dilations=[1, 1],\r\n            strides=[1, 1, 1, 1],\r\n            name=\"conv2d1\")\r\n        conv2d_2 = tf.nn.conv2d(\r\n            input_1,\r\n            self.conv2d_weigths_2,\r\n            data_format=\"NHWC\",\r\n            padding=\"SAME\",\r\n            dilations=[1, 1],\r\n            strides=[1, 1, 1, 1],\r\n            name=\"conv2d2\")\r\n        concat = tf.concat([\r\n            conv2d_1,\r\n            conv2d_2,],\r\n            axis=3,\r\n            name=\"output\")\r\n        return concat\r\n\r\n\r\ndef _test(depth):\r\n\r\n    kernel = (numpy.random.uniform(\r\n        low=-0.05,\r\n        high=0.05,\r\n        size=[3, 3, 3, depth])).astype(dtype=numpy.float16).astype(dtype=numpy.float32)\r\n\r\n    x = 2 * numpy.random.rand(1, 512, 512, 3).astype(dtype=numpy.float32)\r\n    input_1_feed = numpy.where(x > 1, x - 0.5, x - 1.5).astype(dtype=numpy.float32)\r\n\r\n    model_1 = Model_1conv2d(kernel)\r\n    model_2 = Model_2conv2d(kernel[:, :, :, :(depth // 2)], kernel[:, :, :, (depth // 2):])\r\n\r\n    out_1 = model_1.f(input_1_feed,).numpy()\r\n    out_2 = model_2.f(input_1_feed,).numpy()\r\n\r\n    numpy.testing.assert_allclose(out_1, out_2)\r\n\r\n\r\nif __name__ == '__main__':\r\n    for d in [32, 8, 16]:\r\n        _test(d)\r\n        print('>> test with kernel depth = %2s pass' % d)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@qmller ,\r\nCan you please take a look at this SO link [1](https://stackoverflow.com/questions/40205497/why-conv2d-in-tensorflow-gives-an-output-has-the-same-shape-as-input), [2](https://stackoverflow.com/questions/34619177/what-does-tf-nn-conv2d-do-in-tensorflow) and the [link](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) which delivers the required information.It helps.Thanks!", "Hi @tilakrayal,\r\nI took a look at the links you shared me. However, it does not answer to my request.\r\nLink SO 1 is talking about : why the output dimension does not change if I change the depth of my kernel filter in a conv2d.\r\nLink SO 2 is talking about :  conv2d implementation explanation\r\n\r\nAnd nothing in the help documentation explains why the results are very similar (but different) for 2 x conv2d-concatenated with a kernel filter depth of 8 and a conv2d with a kernel filter depth of 16 (my origin question).\r\n\r\nThanks by adavance for your support and explanation,\r\nQ.\r\n\r\n", "ping @tilakrayal ", "@chunduriv ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/c50b65c12977f1eabb7dd48ef387954a/55326.ipynb).", "We sometimes take different code paths depending on the size of the inputs, so it's not guaranteed you'll get bit-exact results.  For example, the order of additions may change, and the compiler may introduce fused multiply-add instructions, leading to slightly different numerical values.\r\n\r\nFor input values on the order of magnitude 2.0, and kernel values on the order of magnitude 0.05 (which seems to be the case here), the result of a 3x3x3 convolution will have an error per element distributed ~ N(0, 2.5e-8).  The errors you're seeing seem to be in line with this.  Changing the types to `float64`, the error should be ~ N(0, 4.7e-17).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55326\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55326\">No</a>\n"]}, {"number": 55325, "title": "AttributeError: 'arguments' object has no attribute 'posonlyargs'", "body": "summary: while running locally the efficient net tutorial, the logs showed and error and asked to report to the tf team.\r\n\"AttributeError: 'arguments' object has no attribute 'posonlyargs'\r\nWARNING:tensorflow:AutoGraph could not transform <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\"\r\n\r\n**System information**\r\n- Code is an official example of how to run efficientnet:(https://github.com/tensorflow/tpu/blob/master/models/official/detection/GETTING_STARTED.md) \r\n- OS Platform Linux Ubuntu 20.04, Lenovo T14, no gpu.\r\n- TensorFlow installed from binary.\r\n- TensorFlow version v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0.\r\n- Python version 3.8\r\n\r\nlog:\r\n\r\n\r\n`tpu/models/official/detection/main.py -model=retinanet --model_dir=/home/dannyb/Documents/Research/copy_paste/model_output --mode=train --eval_after_training=True --use_tpu=False \"--params_override={ train: { checkpoint: { path: gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07/model.ckpt-112603, prefix: resnet50/ }, train_file_pattern: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco }, eval: { val_json_file: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco/raw-data/annotations/image_info_test-dev2017.json, eval_file_pattern: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco } }\"\r\nConnected to pydev debugger (build 213.7172.26)\r\n2022-03-22 13:26:39.711187: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2022-03-22 13:26:39.711228: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dannybarash-t14-gen-1): /proc/driver/nvidia/version does not exist\r\nI0322 13:26:39.726564 140697653870976 main.py:114] Model Parameters: {'anchor': {'anchor_size': 4.0,\r\n            'aspect_ratios': [1.0, 2.0, 0.5],\r\n            'num_scales': 3},\r\n 'architecture': {'backbone': 'resnet',\r\n                  'max_level': 7,\r\n                  'min_level': 3,\r\n                  'multilevel_features': 'fpn',\r\n                  'num_classes': 91,\r\n                  'output_flat_fpn_features': False,\r\n                  'parser': 'retinanet_parser',\r\n                  'pre_parser': None,\r\n                  'space_to_depth_block_size': 1,\r\n                  'use_bfloat16': False},\r\n 'batch_norm_activation': {'activation': 'relu',\r\n                           'batch_norm_epsilon': 0.0001,\r\n                           'batch_norm_momentum': 0.997,\r\n                           'batch_norm_trainable': True,\r\n                           'use_sync_bn': False},\r\n 'dropblock': {'dropblock_keep_prob': None, 'dropblock_size': None},\r\n 'enable_summary': False,\r\n 'eval': {'eval_batch_size': 8,\r\n          'eval_dataset_type': 'tfrecord',\r\n          'eval_file_pattern': '/home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco',\r\n          'eval_samples': None,\r\n          'eval_timeout': None,\r\n          'min_eval_interval': 180,\r\n          'num_steps_per_eval': 1000,\r\n          'per_category_metrics': False,\r\n          'skip_eval_loss': False,\r\n          'suffix': '',\r\n          'type': 'box',\r\n          'use_json_file': True,\r\n          'val_json_file': '/home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco/raw-data/annotations/image_info_test-dev2017.json'},\r\n 'fpn': {'fpn_feat_dims': 256,\r\n         'use_batch_norm': True,\r\n         'use_separable_conv': False},\r\n 'isolate_session_state': False,\r\n 'model_dir': '/home/dannyb/Documents/Research/copy_paste/model_output',\r\n 'nasfpn': {'activation': None,\r\n            'block_fn': 'conv',\r\n            'fpn_feat_dims': 256,\r\n            'init_drop_connect_rate': None,\r\n            'num_repeats': 5,\r\n            'use_separable_conv': False,\r\n            'use_sum_for_combination': False},\r\n 'platform': {'eval_master': None,\r\n              'gcp_project': None,\r\n              'tpu': None,\r\n              'tpu_zone': None},\r\n 'postprocess': {'apply_nms': True,\r\n                 'apply_sigmoid': True,\r\n                 'max_total_size': 100,\r\n                 'nms_iou_threshold': 0.5,\r\n                 'nms_version': 'v1',\r\n                 'pre_nms_num_boxes': 5000,\r\n                 'score_threshold': 0.05,\r\n                 'use_batched_nms': False},\r\n 'predict': {'predict_batch_size': 8},\r\n 'resnet': {'init_drop_connect_rate': None, 'resnet_depth': 50},\r\n 'retinanet_head': {'anchors_per_location': None,\r\n                    'num_convs': 4,\r\n                    'num_filters': 256,\r\n                    'use_batch_norm': True,\r\n                    'use_separable_conv': False},\r\n 'retinanet_loss': {'box_loss_weight': 50,\r\n                    'focal_loss_alpha': 0.25,\r\n                    'focal_loss_gamma': 1.5,\r\n                    'huber_loss_delta': 0.1,\r\n                    'normalizer_momentum': 0.0},\r\n 'retinanet_parser': {'aug_policy': '',\r\n                      'aug_rand_hflip': True,\r\n                      'aug_scale_max': 1.0,\r\n                      'aug_scale_min': 1.0,\r\n                      'match_threshold': 0.5,\r\n                      'max_num_instances': 100,\r\n                      'output_size': [640, 640],\r\n                      'regenerate_source_id': False,\r\n                      'skip_crowd_during_training': True,\r\n                      'unmatched_threshold': 0.5},\r\n 'spinenet': {'init_drop_connect_rate': None,\r\n              'model_id': '49',\r\n              'use_native_resize_op': False},\r\n 'spinenet_mbconv': {'init_drop_connect_rate': None,\r\n                     'model_id': '49',\r\n                     'se_ratio': 0.2,\r\n                     'use_native_resize_op': False},\r\n 'tpu_job_name': None,\r\n 'train': {'checkpoint': {'path': 'gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07/model.ckpt-112603',\r\n                          'prefix': 'resnet50/',\r\n                          'skip_variables_regex': ''},\r\n           'frozen_variable_prefix': None,\r\n           'gradient_clip_norm': 0.0,\r\n           'input_partition_dims': None,\r\n           'iterations_per_loop': 100,\r\n           'l2_weight_decay': 0.0001,\r\n           'learning_rate': {'init_learning_rate': 0.08,\r\n                             'learning_rate_levels': [0.008, 0.0008],\r\n                             'learning_rate_steps': [15000, 20000],\r\n                             'type': 'step',\r\n                             'warmup_learning_rate': 0.0067,\r\n                             'warmup_steps': 500},\r\n           'num_cores_per_replica': None,\r\n           'num_shards': 8,\r\n           'optimizer': {'momentum': 0.9, 'type': 'momentum'},\r\n           'pre_parser_dataset': {'dataset_type': 'tfrecord',\r\n                                  'file_pattern': ''},\r\n           'regularization_variable_regex': '.*(kernel|weight):0$',\r\n           'space_to_depth_block_size': 1,\r\n           'total_steps': 22500,\r\n           'train_batch_size': 64,\r\n           'train_dataset_type': 'tfrecord',\r\n           'train_file_pattern': '/home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco',\r\n           'transpose_input': True},\r\n 'type': 'retinanet',\r\n 'use_tpu': False}\r\nINFO:tensorflow:gpu devices: []\r\nI0322 13:26:39.727839 140697653870976 tpu_executor.py:134] gpu devices: []\r\n2022-03-22 13:26:39.728716: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nW0322 13:26:39.730379 140697653870976 cross_device_ops.py:1386] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nI0322 13:26:39.737167 140697653870976 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nINFO:tensorflow:Number of devices: 1\r\nI0322 13:26:39.737861 140697653870976 tpu_executor.py:137] Number of devices: 1\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nI0322 13:26:39.827292 140697653870976 run_config.py:582] Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nI0322 13:26:39.827548 140697653870976 estimator_training.py:163] Not using Distribute Coordinator.\r\nINFO:tensorflow:Using config: {'_model_dir': '/home/dannyb/Documents/Research/copy_paste/model_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7ff6509af290>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\r\nI0322 13:26:39.831076 140697653870976 estimator.py:202] Using config: {'_model_dir': '/home/dannyb/Documents/Research/copy_paste/model_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7ff6509af290>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\r\nWARNING:tensorflow:From /home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1244: StrategyBase.configure (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nuse `update_config_proto` instead.\r\nW0322 13:26:39.871531 140697653870976 deprecation.py:343] From /home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1244: StrategyBase.configure (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nuse `update_config_proto` instead.\r\nWARNING:tensorflow:From /home/dannyb/Documents/Research/copy_paste/tpu/models/official/detection/dataloader/input_reader.py:48: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\r\nW0322 13:26:39.898388 140697653870976 deprecation.py:343] From /home/dannyb/Documents/Research/copy_paste/tpu/models/official/detection/dataloader/input_reader.py:48: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\r\nINFO:tensorflow:Converted call: <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=string>,)\r\n    kwargs: {}\r\nI0322 13:26:39.902364 140697653870976 ag_logging.py:136] Converted call: <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=string>,)\r\n    kwargs: {}\r\nINFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7ff6443d2e60>: default rule\r\nI0322 13:26:39.903000 140697653870976 ag_logging.py:136] Not allowed: <method-wrapper '__call__' of function object at 0x7ff6443d2e60>: default rule\r\nINFO:tensorflow:Not allowed: <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>: default rule\r\nI0322 13:26:39.903301 140697653870976 ag_logging.py:136] Not allowed: <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>: default rule\r\nINFO:tensorflow:<function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60> is not cached for subkey ConversionOptions[{}]\r\nI0322 13:26:39.903635 140697653870976 ag_logging.py:136] <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60> is not cached for subkey ConversionOptions[{}]\r\nINFO:tensorflow:Source code of <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>:\r\nlambda file_name: dataset_cls(file_name).prefetch(1),\r\nI0322 13:26:39.912487 140697653870976 ag_logging.py:136] Source code of <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>:\r\nlambda file_name: dataset_cls(file_name).prefetch(1),\r\nINFO:tensorflow:Error transforming entity <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>\r\nTraceback (most recent call last):\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 427, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 269, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 282, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 466, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 359, in transform_function\r\n    result = self.transform_ast(node, context)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 237, in transform_ast\r\n    node = self.initial_analysis(node, ctx)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 225, in initial_analysis\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 700, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 441, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 605, in visit_Lambda\r\n    node = self._visit_arg_annotations(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 546, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 551, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\nI0322 13:26:39.914238 140697653870976 ag_logging.py:136] Error transforming entity <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60>\r\nTraceback (most recent call last):\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 427, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 269, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 282, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 466, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 359, in transform_function\r\n    result = self.transform_ast(node, context)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 237, in transform_ast\r\n    node = self.initial_analysis(node, ctx)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 225, in initial_analysis\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 700, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 441, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 605, in visit_Lambda\r\n    node = self._visit_arg_annotations(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 546, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"/home/dannyb/miniconda/envs/copypaste/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 551, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\nWARNING:tensorflow:AutoGraph could not transform <function _create_pre_batch_dataset_fn.<locals>.get_dataset.<locals>.<lambda> at 0x7ff6443d2e60> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\n`", "comments": ["@dannybarash7 ,\r\nCan you please take a look at this link [1](https://github.com/tensorflow/tensorflow/issues/53879) and [2](https://github.com/tensorflow/tensorflow/issues/48239) with the similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55325\">No</a>\n"]}, {"number": 55324, "title": "Auto comment generation project - model training loop crash", "body": "Trying to train a model in an automatic comment generation project, the training loop throws an exception.\r\nThis error has occured with 3 combinations of python/tensorflow versions, which I note below as (1), (2) and (3), as well as on both a conda env in Powershell and in the WSL Ubuntu command line.\r\nIt also happens on 2 different machines, both with the official (https://github.com/tech-srl/code2seq) and unofficial (https://github.com/Kolkir/code2seq) versions of the project. \r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not sure if the project devs did, I assume not.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64, Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12 (1), 2.8.0 (2), 2.8.0 (3)\r\n- Python version: 3.6.5 (1), 3.9.7 (2), 3.8.10 (3)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: RTX 2080 SUPER 8GB, but N/A\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the training loop with `bash train.sh` both in Powershell (conda) and Ubuntu WSL command line :\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 407, in _flatten_module\r\n    leaves = nest.flatten_with_tuple_paths(\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 1698, in flatten_with_tuple_paths\r\n    flatten(structure, expand_composites=expand_composites)))\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 451, in flatten\r\n    return _pywrap_utils.Flatten(structure, expand_composites)\r\nTypeError: '<' not supported between instances of 'WhileBodyFuncGraph' and 'FuncGraph'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gbaulard\\Documents\\GitHub\\code2seq\\code2seq.py\", line 29, in <module>\r\n    model.train()\r\n  File \"C:\\Users\\gbaulard\\Documents\\GitHub\\code2seq\\modelrunner.py\", line 129, in train\r\n    gradients = tape.gradient(loss, self.model.trainable_variables)\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 171, in trainable_variables\r\n    return tuple(\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 449, in _flatten_module\r\n    for subvalue in subvalues:\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 449, in _flatten_module\r\n    for subvalue in subvalues:\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 449, in _flatten_module\r\n    for subvalue in subvalues:\r\n  File \"C:\\Users\\gbaulard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 410, in _flatten_module\r\n    six.raise_from(\r\n  File \"<string>\", line 3, in raise_from\r\nValueError: Error processing property '_dropout_mask_cache' of <ContextValueCache at 0x17309fd0df0>\r\n```\r\n\r\nCode snippet being triggered in module.py of tensorflow :\r\n\r\n```\r\ntry:\r\n      leaves = nest.flatten_with_tuple_paths(\r\n          prop, expand_composites=expand_composites)\r\n    except Exception as cause:  # pylint: disable=broad-except\r\n      six.raise_from(\r\n          ValueError(\r\n              \"Error processing property {!r} of {!r}\".format(key, prop)),\r\n          cause)\r\n```\r\n**Describe the expected behavior**\r\n\r\nThe traning loop should terminate without errors.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nBest I can do is give the project git:\r\n- https://github.com/tech-srl/code2seq (official version, tensorflow 1.12)\r\n- https://github.com/Kolkir/code2seq (unofficial version, tensorflow 2.8.0)", "comments": ["@gbaulard \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Here's a link with the minimal amount of files to launch the training : https://we.tl/t-lrwIEVS9Dw\r\nRun this in a terminal at the root with python 3.6+, tensorflow 2.1+ installed.\r\n`python -u code2seq.py --data data/java-small/java-small --test data/java-small/java-small.val.c2s --save_path models/java-small-model/model`", "@gbaulard \r\nCould you please  open this issue in TF discussion [forum](https://discuss.tensorflow.org/) as there is a larger community  to get you the right help.\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55324\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55324\">No</a>\n"]}, {"number": 55321, "title": "distributed training with tensorflow on 'x' gpu makes loss 1/x", "body": "I was trying to run a model on multiple gpu with `mirror strategy` of tensorflow.\r\n\r\nI used a custom loss function like this:\r\n```python3\r\ndef mae(y_true, y_pred):\r\n\t# y_true, y_pred shape  = (B, L)\r\n\tloss = tf.keras.metrics.mean_absolute_error(y_true, y_pred) \r\n\t# loss shape = (B,)\r\n\treturn loss\r\nclass custom_loss(tf.keras.losses.Loss):\r\n\tdef __init__(self, BATCH_SIZE = 1, **kwargs):\r\n\t\tsuper(custom_loss, self).__init__(**kwargs)\r\n\t\tself.BATCH_SIZE = BATCH_SIZE\r\n\r\n\tdef call(self, y_true, y_pred):\r\n\t\t# y_true, y_pred shape = (B, L, 1)\r\n\t\tloss = mae(tf.squeeze(y_true, [-1]), tf.squeeze(y_pred, [-1]))\r\n\t\tloss = tf.reduce_sum(loss) * (1. / self.BATCH_SIZE)\r\n\t\treturn loss\r\n\r\n\tdef get_config(self):\r\n\t\tconfig = super().get_config().copy()\r\n\t\tconfig.update({'BATCH_SIZE': self.BATCH_SIZE})\r\n\t\treturn config\r\n```\r\n\r\nwith mirror strategy I train the model like this:\r\n```python3\r\ndef get_compiled_model(args, BATCH_SIZE):\r\n\t# Make a simple 2-layer densely-connected neural network.\r\n\tmodel = MyCustomModel(input_shape=(args.L, 1))\r\n\tmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=args.learning_rate, beta_1=args.beta_1, beta_2=args.beta_2, epsilon=args.epsilon), loss = custom_loss(BATCH_SIZE))\r\n\treturn model\r\n\r\ndef run_training(args, steps, model = None):\r\n\t# Create a MirroredStrategy.\r\n\tstrategy = tf.distribute.MirroredStrategy()\r\n\tBATCH_SIZE_PER_REPLICA = args.batch_size\r\n\tBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\n\t# Open a strategy scope and create/restore the model\r\n\twith strategy.scope():\r\n\t\tif isinstance(model, type(None)):\r\n\t\t\tmodel = get_compiled_model(args, BATCH_SIZE)\r\n\t\ttrain_dataset, test_dataset, valid_dataset = get_dataset(args, BATCH_SIZE)\r\n\t\tcallbacks = [\r\n\t\t\ttf.keras.callbacks.ModelCheckpoint(\r\n\t\t\t\tfilepath=os.path.join(args.checkpoints_dir , steps + \"_epoch-{epoch:03d}_loss-{loss:.4f}\"), save_best_only = True\r\n\t\t\t)\r\n\t\t]\r\n\t\tmodel.fit(train_dataset, epochs=args.epochs, callbacks=callbacks, validation_data = valid_dataset, steps_per_epoch = (None if args.steps_per_epoch == -1 else args.steps_per_epoch), validation_steps = (None if args.steps_per_epoch == -1 else args.steps_per_epoch), verbose = 1)\r\n```\r\n\r\nBut if I run this on 4 GPU, my loss value becomes 1/4 times than the loss I get when run on single GPU. Does it fail to sum up the different losses from the different GPUs?", "comments": ["@st3inum \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!"]}, {"number": 55320, "title": "[TF-TRT] Remove dependency on einsum kernel", "body": "Previously, TF-TRT op converter for einsum used a helper function from the actual einsum kernel implementation in `tensorflow/core/kernels/linalg/einsum_op_impl.h` and had a dependency on einsum_op. This PR moves the helper function (`ParseEquation`) to `tensorflow/core/util/einsum_op_util.h` and remove such a dependency.\r\n\r\nThis change makes both the einsum kernel and TF-TRT more modular, which is helpful if we want to move either of them to a separate binary (dynamic kernel library for example).\r\n\r\nSummary of changes:\r\n* Rename `ParseEinsumEquation()` to `ValidateEinsumEquation()` in  `tensorflow/core/utils/einsum_op_util.h`\r\n* Rename `EinsumHelper::ParseEquation()` to  `ParseEinsumEquation()` and move from `tensorflow/core/kernels/linalg/einsum_kernel_impl.h` to `tensorflow/core/utils/einsum_op_util.h`.\r\n* Update TF-TRT to use new `ParseEinsumEquation()` in `tensorflow/core/utils/einsum_op_util.h` and remove dependency on `tensorflow/core/kernels/linalg/einsum_kernel_impl.h`.\r\n* Rename `EinsumHelper::DimensionType` to `EinsumDimensionType`.\r\n", "comments": ["@DEKHTIARJonathan", "@bixia1 for review\r\n", "@trevor-m I modified your PR description, please check.", "Thank you @bixia1 for the review! I have addressed your comments."]}, {"number": 55318, "title": "2", "body": "workflows", "comments": ["Please stop spamming with github actions or we might need to ban you from contributing to this repository."]}]