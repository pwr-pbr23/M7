[{"number": 18707, "title": "[tf.data] Allow `sample_from_datasets` to accept a tf.Dataset object \u2026", "body": "\u2026for `weights`.\r\n\r\nThis will be useful in developing a faster rejection resampler.\r\n\r\nTested:\r\nbazel test :interleave_dataset_op_test", "comments": []}, {"number": 18706, "title": "Wrong variance from initialisers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.3, 1.4, 1.5, 1.6, 1.7, 1.8\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See *Source Code* section\r\n\r\n\r\n### Describe the problem\r\nProbably the best known fact about weight initialisation is that initial weights should have small values, close to zero. Probably, a lot of people, especially those that read [1] by LeCun et al., also know that also some properties of the distribution from which the initial values are drawn matters. This has been discussed in great detail in the literature. Xavier Glorot argued in [2] that the variance of the (initial) weights should have zero mean and a variance of 1/(fan_in + fan_out) for sigmoid activation functions (focusing on tanh). He et al. [3] noted that  for ReLUs, the weights should have zero mean and a variance of 2/fan_in in order to correct for the variance that gets lost in the negative part. Finally, the entire theory of self-normalizing networks, introduced in [4] by Klambauer et al., and the parameters for alpha and gamma for SELU activation functions, is built upon the assumptions that the weights have mean 0 and variance 1/fan_in.\r\n\r\nThe [truncated normal distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution) is a commonly used distribution for initialising weights in a neural network. After all, it looks like a Gaussian, but does not allow outliers, which could cause saturation for certain activation function from the start. It also has a mean and variance that depend on the four parameters of the truncated normal distribution: mean, standard deviation, lower boundary and upper boundary. This distribution is implemented in `tf.truncated_normal` and allows to create a truncated normal distribution with arguments for its mean and standard deviation, i.e. the standard distribution of the Gaussian before truncating (I know that this is confusing #13686). Note that the boundaries are fixed to be at 2 standard deviations at each side of the mean.\r\n\r\nThis is all fine, until one wants to create a truncated normal distribution with a specific standard deviation, which happens to be the case for the initialisers. Although the well-studied initialisers, mentioned earlier, tell us exactly what the best variances are, the `<name>_normal_initializer` lead to wrong initialisations (for examples, see below). The problem can be brought back to the `VarianceScaling` initialiser, which allows to choose between a normal and uniform distribution. When using `distribution='normal'`, a truncated normal is used under the hood. Because there is no compensation for the truncation of the normal distribution (which would have had the correct standard deviation), the variance of the generated samples do not have the required variance. This has been resolved in Theano from the start (see issue theano/theano#6381) and I wanted to tackle this problem in Keras (see issue keras-team/keras#8048), where I was redirected here (keras-team/keras#9963).\r\n\r\nIt should be possible to map the solution implemented in keras-team/keras#9963 directly to the tensorflow code. I hope this gets fixed soon, because there are probably plenty of people using wrong initialisations due to this faulty implementation.\r\n\r\nPS: I am not eager to accept all these terms to issue a pull request for making this minor contribution directly. Thanks to anybody who would take this fix upon him/her.\r\n\r\nTL;DR; The default initialisers using truncated normal distributions are simply wrong. By truncating a normal distribution, variance gets lost and the entire theoretical foundations of the initialisers disappear. I believe that this issue can be fixed by accounting for the truncation in the `VarianceScaling` initialiser.\r\n\r\n### Source code / logs\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    fan_in, fan_out = 1000, 100\r\n\r\n    uniform_init = tf.variance_scaling_initializer(distribution='uniform')\r\n    normal_init = tf.variance_scaling_initializer(distribution='normal')\r\n\r\n    print(\"expected variance: {:f}\".format(1. / fan_in))\r\n    with tf.Session() as sess:\r\n         print(\" uniform variance: {:f}\".format(\r\n             np.var(sess.run(uniform_init((fan_in, fan_out))))))\r\n         print(\"  normal variance: {:f}\".format(\r\n             np.std(sess.run(normal_init((fan_in, fan_out))))))\r\n\r\n    uniform_init = tf.glorot_uniform_initializer()\r\n    normal_init = tf.glorot_normal_initializer()\r\n\r\n    print(\"expected variance: {:f}\".format(2. / (fan_in + fan_out)))\r\n    with tf.Session() as sess:\r\n         print(\" uniform variance: {:f}\".format(\r\n             np.var(sess.run(uniform_init((fan_in, fan_out))))))\r\n         print(\"  normal variance: {:f}\".format(\r\n             np.std(sess.run(normal_init((fan_in, fan_out))))))\r\n\r\n### References\r\n[1] LeCun, Yann, L\u00e9on Bottou, Genevieve B. Orr, and Klaus-Robert M\u00fcller. \"Efficient backprop.\" In Neural networks: Tricks of the trade, pp. 9-50. Springer, Berlin, Heidelberg, 1998.\r\n[2] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. 2010.\r\n[3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. 2015.\r\n[4] Klambauer, G\u00fcnter, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. \"Self-normalizing neural networks.\" Advances in Neural Information Processing Systems (2017): 972-981.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have updated my post. From what I was able to dig up, the bug should be present from version 1.3 onwards (or earlier). It seems to have been introduced in `tf.contrib` back then.", "I think this is right, the `stddev` for truncated normal initializers should be scaled by `1 / sqrt(1.3)` (about 0.9) to account for the truncation. The difference is initial values is small, but has the potential to be a breaking change.", "Please open a PR to implement the scaling (I think it should be a 1-line fix, with a new unit test added).", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18705, "title": "dynamic_rnn is much slower then manually using while_loop", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/\r\n- **GPU model and memory**: NVIDIA 1080 TI\r\n- **Exact command to reproduce**: (see below)\r\n\r\n``python benchmark_lstm.py -d 512 -b 60 -t 380 -n 512  -w 20 -i 40``\r\n\r\n### Describe the problem\r\nTensorflow `dynamic_rnn` is over twice as slow as `static_rnn` and using a simple application of `tf.while_loop`.\r\n\r\nI noticed while doing some profiling that `dynamic_rnn` was much slower than `static_rnn`. At first I assumed this was because of something inherently slow about using dynamic length, but then I also noticed naively using `tf.while_loop` with rnn cell was also much faster and performed comparably to `static_rnn`.\r\n\r\nI am not sure if there is some special cases `dynamic_rnn` needs to handle that causes it to be slower, or if there something not equivalent between my `while_loop` and the `dynamic_rnn` that I am missing. However a 2x performance change is a big deal and I wanted to report this just in case it points to a possible implementation improvement.\r\n\r\n### Source code / logs\r\nBenchmark used:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport argparse\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\n\r\n\r\ndef while_loop_rnn(rnn_cell, x):\r\n    initial_state = rnn_cell.zero_state(tf.shape(x)[0], tf.float32)\r\n    x_t = tf.transpose(x, [1, 0, 2])\r\n\r\n    def compute(i, cur_state, out):\r\n        output, cur_state = rnn_cell(x_t[i], cur_state)\r\n        return i+1, cur_state, out.write(i, output)\r\n\r\n    time = tf.shape(x_t)[0]\r\n\r\n    _, cur_state, out = tf.while_loop(\r\n        lambda a, b, c: a < time,\r\n        compute,\r\n        (0, initial_state, tf.TensorArray(tf.float32, time))\r\n    )\r\n    return tf.transpose(out.stack(), [1, 0, 2]), cur_state\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"-d\", \"--dim\", required=True, type=int)\r\n    parser.add_argument(\"-b\", \"--batch\", required=True, type=int)\r\n    parser.add_argument(\"-t\", \"--time\", required=True, type=int)\r\n    parser.add_argument(\"-n\", \"--hidden\", required=True, type=int)\r\n    parser.add_argument(\"-w\", \"--warm_up\", type=int, default=20)\r\n    parser.add_argument(\"-i\", \"--iterations\", type=int, default=20)\r\n    args = parser.parse_args()\r\n\r\n    cell = tf.nn.rnn_cell.LSTMCell(args.hidden)\r\n    x = tf.constant(np.random.normal(scale=2, size=(args.batch, args.time, args.dim)).astype(np.float32))\r\n\r\n    dynamic = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)[0]\r\n    static = tf.stack(tf.nn.static_rnn(cell, tf.unstack(x, args.time, axis=1), dtype=tf.float32)[0], axis=1)\r\n    while_loop = while_loop_rnn(cell, x)[0]\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(\"Checking equivalence...\")\r\n    dynamic_out, static_out, while_out = sess.run([dynamic, static, while_loop])\r\n\r\n    if not np.allclose(dynamic_out, static_out):\r\n        raise RuntimeError()\r\n    if not np.allclose(dynamic_out, while_out):\r\n        raise RuntimeError()\r\n\r\n    for name, op in zip([\"dynamic\", \"static\", \"while\"], [dynamic, static, while_loop]):\r\n        print(\"Testing %s\" % name)\r\n        print(\"Warming up...\")\r\n        for _ in range(args.warm_up):\r\n            sess.run(op)\r\n\r\n        for _ in tqdm(range(args.iterations), \"run\", ncols=80):\r\n            sess.run(op)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n", "comments": ["With TF 1.9 CPU version\r\nOn Dell XPS13 with i5-6200U\r\nRunning this benchmark via PyCharm, with the suggested parameters:\r\n-d 512 -b 60 -t 380 -n 512 -w 20 -i 40\r\nYields:\r\nDynamic: 3.15 seconds / iteration\r\nStatic: 2.77 s/it\r\nWhile-loop: 3.17 s/it\r\n<img width=\"835\" alt=\"dynamicvsstaticvswhile\" src=\"https://user-images.githubusercontent.com/8627404/43028600-06506b20-8c3e-11e8-842c-6b54619c7796.PNG\">\r\n", "Sounds like the numbers are in line now?\n\nOn Sat, Aug 4, 2018, 11:39 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @ebrevdo <https://github.com/ebrevdo>: It has been 14\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18705#issuecomment-410469245>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim48e3S1JbWRIGNFRDxHi7xI_llYhks5uNepNgaJpZM4Tcb9R>\n> .\n>\n", "In my case, yes. @chrisc36 , it could be interesting to see if a rerun with TF1.9 on your configuration gives a similar result, if you find it interesting.", "Nagging Assignee @ebrevdo: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing, please reopen if you see a regression", "@RanFeldesh Was your benchmarking run with GPU support? The problem I encountered is, bidirectioanl_dynamic_rnn is executed unreasonably slow even though setting tf.device(\"/device:GPU\") and allow_soft_placement=True. ", "@XilunWu No GPU", "Interesting, it is even slower than normal Python for loop.\r\nUsing tensorflow 1.10.0 installed with miniconda (python 3.6.6) on a Linux machine with GTX 1070 onboard.\r\n\r\nTested on a word+char level GRU-based classifier with `tf.nn.dynamic_rnn` and without using a simple for loop. There were two GRU cells (GRU(200), GRU(20)) for word level and char level accordingly.\r\nWith `tf.nn_dynamic_rnn` the epoch processing time takes **80** sec. Using simple for loop takes **50** sec!\r\n\r\nchar-level loop \r\n```python\r\n    def _apply_char_cell(self, inputs):\r\n        # inputs = [15 x [32 x 10]]\r\n        with variable_scope.variable_scope(\"char_recurrent_loop\"):\r\n            state = self.char_initial_state\r\n            _outputs = []\r\n            # 15 * [32 x 10]\r\n            for i, inp in enumerate(inputs):\r\n                # inp = [32 x 10]\r\n                if i > 0:\r\n                    variable_scope.get_variable_scope().reuse_variables()\r\n                output, state = self.char_cell(inp, state)\r\n                # output = [32 x 20]\r\n                _outputs.append(output)\r\n\r\n        # [15 x [32 x 20]] -> [32 x 15 x 20]\r\n        _output = tf.transpose(tf.stack(_outputs, axis=0), [1, 0, 2])\r\n        output = tf.reshape(_output, [self.batch_size, \r\n                                      self.char_seq_length*self.char_hidden_size])\r\n        return output, state\r\n```\r\n char-level `tf.nn.dynamic_rnn`\r\n```python\r\n    def _apply_char_cell(self, inputs):\r\n        # inputs = [15 x 32 x 20]\r\n        with variable_scope.variable_scope(\"char_recurrent_loop\"):\r\n            outputs, state = tf.nn.dynamic_rnn(self.char_cell,\r\n                                               inputs,\r\n                                               initial_state=self.char_initial_state,\r\n                                               time_major=True)\r\n        return outputs, state\r\n```\r\n\r\n\r\nMoreover, now the error loss is worse across the epochs with fixed seed!\r\n\r\nWith simple for loop: `Best valid loss so far: 93.98379198461771 epoch: 15`\r\n\r\nWith `tf.nn.dynamic_rnn`: `Best valid loss so far: 107.61339526996017 epoch: 21`\r\n\r\n"]}, {"number": 18704, "title": "Use Chromium's NoDesructor<T> to avoid dynamic ctors/dtors", "body": "", "comments": ["Overall looks good, but it seems like a lot of code for something small. Do you measure this to have significant performance implications?", "The cost of a single function call is small anyway. But here we do not require heap allocation and the pointer indirection. I have a benchmark where the new version of getting the empty string is 15% faster than the old one.", "But my question is \"how often does tensorflow get this empty string?\". As\nin, is there some end-to-end scenario where you see a difference?\n\nOn Mon, Apr 23, 2018 at 11:07 AM Sergey Abbakumov <notifications@github.com>\nwrote:\n\n> The cost of a single function call is small anyway. But here we do not\n> require heap allocation and the pointer indirection. I have a benchmark\n> where the new version of getting the empty string is 15% faster than the\n> old one.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18704#issuecomment-383668412>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxSKg2mi99G7aoRcnmxb5rWdS1k40ks5trhhZgaJpZM4TcOyl>\n> .\n>\n\n\n-- \n - Alex\n", "In this place no. But what I want is to avoid all\r\n```\r\nconst T& Get() {\r\n  static T t;\r\n  return t;\r\n}\r\n```\r\n\r\nwhich is all around the codebase (T can be a non trivially destructible) despite the fact that global constructors and destructors are banned.\r\nOne can change all that static objects to\r\n```\r\nconst T& Get() {\r\n  static T* t = new T;\r\n  return *t;\r\n}\r\n```\r\n\r\nbut I propose a better alternative and would like tensorflow to adopt this pattern.", "@m3bm3b do you have an opinion on this? I'm inclined to accept it but I fear without an explicit larger effort we'll just have two mildly incompatible ways of doing globals in our codebase.", "Perhaps I am missing something, but \r\nI'm struggling to see how using NoDestructor is better than writing:\r\n   T* Get() {\r\n      static T* t = new T;\r\n      return t;\r\n   }\r\n\r\nI see two issues:   1. readability and 2. resource usage.\r\n\r\n1. From the readability point of view, NoDestructor seems to be worse:\r\nThe example above is shorter and easier for\r\nreaders to understand than the examples of using NoDestructor given \r\nin the comments at the top of its header file.   This is especially so because NoDestructorreaders will\r\nhave to look up the unfamiliar NoDestructor and understand why it's being used.   \r\n(It took me quite a while to understand what its technical advantage \r\nmight be; see below.)\r\n\r\n2. The concrete advantage of NoDestructor mentioned in its description\r\nis that it avoids a call to malloc().   However, tthe advantage is not\r\nclear, because there is a trade off between space and time that the comments do not discuss.\r\nNoDestructor avoids the malloc() by allocating the memory unconditionally\r\nin the bss segment (and that memory will probably get faulted in by accesses\r\nto nearby variables).   To avoid the malloc() is already of uncertain value, \r\nbecause there are so few calls at stake.\r\nIt's even more dubious because TensorFlow is a complex library, and we would not expect every \r\nsuch static variable to be initialized in a given run.   Thus, some fraction of the memory\r\nallocated in bss by NoDestructor will be wasted, while the malloc() approach \r\nwould have saved that memory.  Given the small number of cycles involved,\r\nmy bet is that few users will care.  But I suspect that the few that do care will prefer\r\nthe dynamic allocation, because the memory is likely to be more precious than the cycles.\r\n\r\nSo unless there's another technical argument to be made,\r\nI think the main effect of NoDestructor would be to make the code more complicated,\r\nand I recommend against adopting it.\r\n", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The thing is that there's a lot of code doing just:\r\n```\r\nBar& Foo() {\r\n  static Bar bar;\r\n  return bar;\r\n}\r\n```\r\n\r\nwhich also has all the drawbacks you mentioned. But it also has dynamic initializers whereas NoDestructor doesn't.\r\n\r\nThere are two major solutions on this:\r\n1. Use NoDestructor everywhere.\r\n2. Convert every static object allocation to static T* t = new T;", "@sabbakumov I prefer the second option (converting every static to static T* t = new T)"]}, {"number": 18703, "title": "Add tf.float16 support to tf.contrib.nccl.all_sum", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: TitanX 12Gb x2\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen making a call to tf.contrib.nccl.all_sum I get the following output:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 393, in <module>\r\n    main(ARGS)\r\n  File \"trainer.py\", line 49, in main\r\n    thread_count=args.thread_count, use_nccl=args.use_nccl\r\n  File \"trainer.py\", line 191, in train\r\n    use_nccl=use_nccl)\r\n  File \"/home/thomas/Projects/Thomas/deepspeech2/deepspeech2/utils/tensorflow.py\", line 27, in tf_average_gradients\r\n    gradient_sum_list = nccl.all_sum(flat_gradients_list)\r\n  File \"/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 47, in all_sum\r\n    return _apply_all_reduce('sum', tensors)\r\n  File \"/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 228, in _apply_all_reduce\r\n    shared_name=shared_name))\r\n  File \"/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py\", line 59, in nccl_all_reduce\r\n    num_devices=num_devices, shared_name=shared_name, name=name)\r\n  File \"/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float32, float64, int32, int64\r\n```\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\ngradient_sum_list = nccl.all_sum(flat_gradients_list)\r\n```\r\n", "comments": ["fp16 support was enabled for nccl is 5c469e6bafb479ef110b2f02f070507a3711664d. This will be in TensorFlow 1.9, I think.", "Thank You!"]}, {"number": 18702, "title": "Fix tf.variable_scope unique name after entering root scope", "body": "This PR fixes a bug in creating unique scope name after entering and closing the root scope.\r\n\r\nThe following snippet exploits the bug\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope(None, \"Block\") as scope:\r\n    print(\"SCOPE:\" + scope.name)\r\nwith tf.variable_scope('') as scope:\r\n    print(\"SCOPE:\" + scope.name)\r\nwith tf.variable_scope(None, \"Block\") as scope:\r\n    print(\"SCOPE:\" + scope.name)\r\n```\r\n\r\nExecuting it produces\r\n```\r\n    SCOPE:Block\r\n    SCOPE:\r\n    SCOPE:Block\r\n```\r\nWhile the expected behavior should be\r\n```\r\n    SCOPE:Block\r\n    SCOPE:\r\n    SCOPE:Block_1\r\n```\r\nAs it turns out, closing the root scope ('') results in resetting the dict `self.variable_scopes_count` to `0`,\r\nsince `not ''` evaluates to `True`, which is unlikely the required behavior.", "comments": ["@lukaszkaiser I am honestly not sure what the \"correct\" behavior here is. I have a strong feeling that the current behavior is what we have, backwards compatibility will prevent us from changing this.", "@martinwicke : this is just another \"None vs empty string\" python bug. It's a small bug and I believe this correction could be good and not cause any incompatibility problems. But let's wait for the tests -- if they're passing, then we should accept it. If a lot of tests are failing, then I'm afraid we may need to keep the state as is, but let's see.", "@lukaszkaiser Passing the tests doesn't quite make me comfortable -- can you pull this CL in and merge it internally so we get more coverage of actual use?", "On it, will report back here.", "@lukaszkaiser any update?", "This looks fine! I'll need to update some internal tests. I'll try to do it next week, but due to ICLR it may only happen the week after. Martin: should we wait for that before merging this PR?", "Yes, let's wait until then, otherwise it'll block merges.\n\n>\n", "The internal part is now updated. Martin: should we merge? What's the process?"]}, {"number": 18701, "title": "Fix doc gen error", "body": "Mismatch after the fix in #17815", "comments": []}, {"number": 18700, "title": "Getting an error for tensors with partial shape information with tf.contrib.staging.StagingArea", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: TitanX 12Gb\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen using tf.contrib.staging.StagingArea to prefetch data to the GPU from a tf.data pipeline. I get the following output:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 387, in <module>\r\n    main(ARGS)\r\n  File \"trainer.py\", line 49, in main\r\n    thread_count=args.thread_count, use_nccl=args.use_nccl\r\n  File \"trainer.py\", line 161, in train\r\n    features = stage([features])\r\n  File \"trainer.py\", line 112, in stage\r\n    get_tensors = [tf.reshape(gt, t.get_shape()) for (gt,t) in zip(get_tensors, tensors)]\r\n  File \"trainer.py\", line 112, in <listcomp>\r\n    get_tensors = [tf.reshape(gt, t.get_shape()) for (gt,t) in zip(get_tensors, tensors)]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 528, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor: (1, ?)\r\n```\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\ndef stage(tensors):\r\n    \r\n    staging_area = tf.contrib.staging.StagingArea(\r\n        dtypes=[tensor.dtype for tensor in tensors],\r\n        shapes=[tensor.get_shape() for tensor in tensors]\r\n    )\r\n    put_op = staging_area.put(tensors)\r\n    get_tensors = staging_area.get()\r\n\r\n    get_tensors = [tf.reshape(gt, t.get_shape()) for (gt,t) in zip(get_tensors, tensors)]\r\n\r\n    return put_op, get_tensors\r\n\r\ndataset = tf.data.TFRecordDataset([source])\r\ndataset = dataset.map(parse, num_parallel_calls=thread_count)\r\ndataset = dataset.apply(tf.contrib.data.padded_batch_and_drop_remainder(\r\n    batch_size, ([None], [], [None])\r\n))\r\nif shuffle:\r\n    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(shuffle_buffer_size, 1))\r\ndataset.prefetch(2 * gpu_count)\r\niterator = dataset.make_initializable_iterator()\r\n\r\nfor gpu in range(gpu_count):\r\n    with tf.device('/device:CPU:0'):\r\n        features, timesteps, labels = iterator.get_next()\r\n        labels = tf.deserialize_many_sparse(labels, dtype=tf.int32)\r\n\r\n        device = '/device:GPU:{}'.format(gpu)\r\n        with tf.device(device):\r\n            scope_name = 'clone_{}'.format(gpu)\r\n            with tf.variable_scope(scope_name) as scope, tf.name_scope(scope_name):\r\n                features = stage([features])\r\n```\r\n", "comments": ["Does it work if you replace `tf.reshape(gt, t.get_shape())` with `tf.reshape(gt, tf.shape(t))`?", "@mrry dude you are freaking awesome! If you are ever in S. FL I owe you lots of :beers:. That worked but I'm getting this weird error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 404, in <module>\r\n    main(ARGS)\r\n  File \"trainer.py\", line 48, in main\r\n    thread_count=args.thread_count, use_nccl=args.use_nccl\r\n  File \"trainer.py\", line 182, in train\r\n    mixed_mode=mixed_mode, use_gpu=True)\r\n  File \"/home/thomas/projects/thomas/models/asr.py\", line 21, in asr_logits\r\n    data_format=data_format, is_training=is_training, mixed_mode=mixed_mode, padding='SAME'\r\n  File \"/home/thomas/projects/thomas/layers/convolution.py\", line 52, in convolution_layer\r\n    z = tf.nn.conv2d(x, W, strides, padding, data_format=data_format, dilations=dilations)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 956, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 513, in _apply_op_helper\r\n    raise err\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1104, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 235, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 214, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 532, in make_tensor_proto\r\n    append_fn(tensor_proto, proto_values)\r\n  File \"tensorflow/python/framework/fast_tensor_util.pyx\", line 108, in tensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 68, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got <tf.Tensor 'clone_0/clone_0/truediv:0' shape=(2, 1, 1, ?) dtype=float16>\r\n```\r\n\r\nThe conv2d op is the first op in the network.", "Actually I think this issue is on my end. I will update the issue shortly. Thanks again!", "@mrry Once again, thx a bunch! The issue was resolved.", "Thanks for confirming!"]}, {"number": 18699, "title": "Relax type of tf.constant.value to sequence", "body": "", "comments": ["I'd have to look closer, but what happens to strings?", "Indeed, this seems like it would break `tf.convert_to_tensor(\u201cfoo\u201d)` because `isinstance(\u201cfoo\u201d, collections.Sequence)` is true.", "@nathanielmanistaatgoogle do you want to try to fix this? ", "> @nathanielmanistaatgoogle do you want to try to fix this?\r\n\r\nShort answer: yes.\r\n\r\nLong answer: in the next three or four weeks maybe; definitely not this week or next.", "@tensorflowbutler: this should probably be assigned to me? It's certainly blocked waiting for a response from me...", "Yup. Thanks!", "@tensorflowbutler: this is still worth fixing, and I do intend to turn it around... at some point. Thank you for your continued patience.", "I'm cleaning the decks of my assigned reviews, and since this PR seems to have stalled, I'm going to remove myself as reviewer.", "Do you plan on addressing my review asking for a test?", "Yes, I plan on coming back to this as soon as I find time. It's been... wow, almost a year. Wow. Didn't think I'd stay that busy... :slightly_smiling_face:", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 18698, "title": "Fix code block rendering in several api definitions", "body": "This PR is to fix code block rendering format in several api definitions:\r\n- [tensorflow::ops::QuantizeV2](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/quantize-v2#summary);\r\n![image](https://user-images.githubusercontent.com/1680977/39005897-081f1dcc-4434-11e8-96a9-92338c945028.png)\r\n\r\n- [tensorflow::ops::Pad](https://www.tensorflow.org/versions/r1.5/api_docs/cc/class/tensorflow/ops/pad);\r\n![image](https://user-images.githubusercontent.com/1680977/39005877-f3e4f138-4433-11e8-8a33-38613767eb01.png)\r\n", "comments": []}, {"number": 18697, "title": "Tensorflow Lite demo app crashes when using inception-v3/Mobilenet_v1 (float) instead of quantised mobilenet", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Git cloned source\r\n- **TensorFlow version (use command below)**: NA\r\n- **Python version**: python 2.7\r\n- **Bazel version (if compiling from source)**: bazel release 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:  4.8.4\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: x86 8GB RAM\r\n- **Exact command to reproduce**:  bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config android  --cpu=x86_64 --fat_apk_cpu=x86_64\r\n\r\n### Describe the problem\r\n\r\nTensorflow Lite demo app crashes while launching the app, if I use Float inception model (inceptionv3_slim_2016.tflite or mobilenet_v1_1.0_224.tflite) instead of quantised mobilenet. I added the code changes from https://github.com/tensorflow/tensorflow/issues/14719 on top of the github source for moving quantised mobilenet to float inception/Mobilenet.\r\n\r\n### Source code / logs\r\n```\r\n--------- beginning of crash --------- \r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime: FATAL EXCEPTION: VideoBackground\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 7131\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to get input dimensions. 0-th input should have 602112 bytes, but found 4291248 bytes.\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:98)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:143)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.run(Interpreter.java:121)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.ImageClassifierFloatInception.runInference(ImageClassifierFloatInception.java:123)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:121)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.MainActivity.classifyFrame(MainActivity.java:496)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.MainActivity.access$600(MainActivity.java:54)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.MainActivity$2.run(MainActivity.java:459)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:790)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:164)\r\n04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n04-19 08:57:06.663  7131  7131 I chatty  : uid=10073(com.example.android.tflitecamerademo) identical 3 lines\r\n04-19 08:57:06.680  7131  7131 D ViewRootImpl[MainActivity]: updatePointerIcon called with position out of bounds\r\n04-19 08:57:09.482  5448  5591 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.MainActivity\r\n```\r\n-----------------------------------------------------------\r\n", "comments": ["@albin84b, the comments by  @pkurogerjs  on #14719 are correct. Your image bytebuffer is the worng size a float buffer should to feed in a 224x224 image should be `224*224*3*4 = 602112` bytes. Your buffer is a factor of 4 too big. Can you provide your full code changes as  PR so I can take a look at exactly what you are doing?\r\n", "Hi, \r\nI am trying to run tensorflow lite android demo app using inception v3, but it is crashing on the device.\r\nI am using android SDK to run and install the app ( refer:  https://www.tensorflow.org/mobile/tflite/demo_android).\r\nIs it supposed to work with inception V3 model or does it have some known issues ? Is it failing just for me or everyone ? \r\nThanks.\r\n\r\nFollowing is the error I am getting:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n                  Process: android.example.com.tflitecamerademo, PID: 4192\r\n                  java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2981)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3042)\r\n                      at android.app.ActivityThread.-wrap14(ActivityThread.java)\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1639)\r\n                      at android.os.Handler.dispatchMessage(Handler.java:102)\r\n                      at android.os.Looper.loop(Looper.java:154)\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6780)\r\n                      at java.lang.reflect.Method.invoke(Native Method)\r\n                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1496)\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1386)\r\n                   Caused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)\r\n                      at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:90)\r\n                      at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:87)\r\n                      at com.example.android.tflitecamerademo.ImageClassifierFloatInception.<init>(ImageClassifierFloatInception.java:46)\r\n                      at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:301)\r\n                      at android.app.Fragment.performActivityCreated(Fragment.java:2361)\r\n                      at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1014)\r\n                      at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1171)\r\n                      at android.app.BackStackRecord.run(BackStackRecord.java:815)\r\n                      at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:1582)\r\n                      at android.app.FragmentController.execPendingActions(FragmentController.java:372)\r\n                      at android.app.Activity.performStart(Activity.java:6964)\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2934)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3042)\u00a0\r\n                      at android.app.ActivityThread.-wrap14(ActivityThread.java)\u00a0\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1639)\u00a0\r\n                      at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\r\n                      at android.os.Looper.loop(Looper.java:154)\u00a0\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6780)\u00a0\r\n                      at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1496)\u00a0\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1386)\u00a0\r\nApplication terminated. ", "I have also noticed this crash when using Inception_v3 model after pulling the latest demo app changes. ", "@aselle, Its works for me after changing the buffer size as per the code changes given in #14719. thanks a lot for the help.", "Glad it worked. Thanks.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue as it works"]}, {"number": 18696, "title": "Unexpected behavior with Estimator warm start", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI've run into two issues when using a warm start with the `Estimator` API (please let me know if you'd like me to file these as separate issues):\r\n1. If checkpoints exist in the `model_dir` then the warm start is overridden when the session is created.\r\n2. The warm start is performed on every `train` call, which means warm starting can't be used in the context of `train_and_evaluate` (or in a manual loop, unless reinitializing the `Estimator`).\r\n\r\nI'd be willing (pending corporate CLA) to submit a patch if you can clarify the desired behavior. I would personally expect that:\r\n1. Checkpoint restore is skipped on session creation if a warm start was performed.\r\n2. The warm start settings would be cleared after the first `train` call. (However, if this is the desired behavior, looking longer term it seems like warm start settings shouldn't be a property of the `Estimator` instance.)\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 66 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "/cc @ispirmustafa any impact here with the new train behaviour?", "This is expected behavior. We define warm start as a starting point. If there is a checkpoint in model-dir, that means training is already started. If there is no checkpoint that means it's first training.", "What about the second point? I agree with @dlsmith that it seems more natural to only \"warm start\" on the first `train` call.", "warm-start says how you initialize your model. Same as defining initialization when you create a variable. It's part of the model definition. But used only if there isn't any checkpoint. Same as initialization. "]}, {"number": 18695, "title": "Merge pull request #1 from tensorflow/master", "body": "merge from google", "comments": ["I think this was opened by mistake."]}, {"number": 18694, "title": "Update docs for tf.unstack with respect to numpy compatibility", "body": "In #18692 an issue was raised over whether `tf.unstack` is compatible with `numpy.unstack `(specified in existing docs) or `numpy.split`.\r\n\r\nIt looks like there is no `numpy.unstack`. And for `numpy.split`, it is not compatible with `tf.unstack`.\r\n\r\nThe `tf.split` is indeed very close to `numpy.split`. However, the second arg `num_or_size_splits` in `tf.split` requires the number of the splits, while the second arg `indices_or_sections` in `numpy.split` requires the index of the splits. For that reason the `tf.split` is not compatible with `numpy.split` as well.\r\n\r\nAccording to the above this fix simply removes `The numpy equivalent` part in the docs of `tf.unstack`.\r\n\r\nThis fix fixes #18692.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18693, "title": "supporting vector parameters for mixture ", "body": "Hi,\r\nIn the example below:\r\n\r\n# Create a mixture of two Gaussians:\r\ntfd = tf.contrib.distributions\r\nmix = 0.3\r\nbimix_gauss = tfd.Mixture(\r\n  cat=tfd.Categorical(probs=[mix, 1.-mix]),\r\n  components=[\r\n    tfd.Normal(loc=-1., scale=0.1),\r\n    tfd.Normal(loc=+1., scale=0.5),\r\n])\r\n\r\n**_# Why cannot I apply it to vector parameters?_** \r\n\r\n    tfd.Normal(loc=[-1, 0]., scale=[0.1, 0.2),\r\n    tfd.Normal(loc=[+1., 2.], scale=[0.5, 3.]),\r\n\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi bashhwu. Thanks for reporting this issue!  Let me study this issue and report back indicating whether this is or is not expected behavior.", "Are you looking to get a mixture of two bivariate Normals, or two mixtures of scalar Normals?\r\n\r\nHere's how you can achieve a bimixture of bivariate spherical Normals:\r\n\r\n    bimix_bigauss = tfd.MixtureSameFamily(\r\n        mixture_distribution=tfd.Categorical(probs=[.3,1-.3]),\r\n        components_distribution=tfd.Independent(\r\n            tfd.Normal(loc=[[-1.,0],[1,2]], scale=[[0.1,0.2],[0.5,3.]]),\r\n            reinterpreted_batch_ndims=1))\r\n    print(bimix_bigauss)\r\n    # ==> tf.distributions.MixtureSameFamily(\"MixtureSameFamily/\", batch_shape=(), event_shape=(2,), dtype=float32)\r\n\r\nHere's how you can achieve two bimixtures of scalar Normals:\r\n\r\n    bimix_gauss = tfd.MixtureSameFamily(\r\n        mixture_distribution=tfd.Categorical(probs=[.3,1-.3]),\r\n        components_distribution=tfd.Normal(loc=[[-1.,0],[1,2]], scale=[[0.1,0.2],[0.5,3.]]))\r\n    print(bimix_gauss)\r\n    # ==> tf.distributions.MixtureSameFamily(\"MixtureSameFamily_1/\", batch_shape=(2,), event_shape=(), dtype=float32)\r\n\r\n\r\n\r\nAs for using tfd.Mixture, its possible there's a broadcasting bug. However in your case using MixtureSameFamily is probably more efficient and things do work there.", "As for using tfd.Mixture, it appears to not be broadcasting the mixture distribution. That said it can be made to work by tiling the mix probs, eg\r\n\r\n\r\n    mix = 0.3\r\n    bimix_gauss = tfd.Mixture(\r\n        cat=tfd.Categorical(probs=[[mix, 1.-mix],[mix,1.-mix]]),\r\n        components=[\r\n            tfd.Normal(loc=[-1.,0], scale=[0.1,0.2]),\r\n            tfd.Normal(loc=[+1.,2.], scale=[0.5,3]),\r\n    ])\r\n", "Nagging Assignee @jvdillon: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18692, "title": "Documentation mistake?", "body": "Hi,\r\n\r\nThe documentation [page](https://www.tensorflow.org/api_docs/python/tf/unstack) of `tf.unstack` says the equivalent is np.unstack. But np doesn't have an unstack method? (Should have been np.split ? ) \r\n\r\nCheers,", "comments": ["The tf.unstack is not truly compatible with numpy.split. There is an op of tf.split but this is also different with numpy.split in that the second arg `num_or_size_splits` vs `indices_or_sections` are also different. I created a PR #18694 to simply remove the mentioning of numpy equivalent for tf.unstack for now."]}, {"number": 18691, "title": "Periodic overhead when using tensorflow dataset for model training on GPU", "body": "As you can see it in the following code, I am trying to train a simple model on Tensorflow with a Tensorflow Dataset. The dataset is pretty huge (2000 exemples of each 300*2000 elements). I shuffle, repeat and batch the dataset in order to do a stochastic gradient descent for training my model.\r\n\r\nBut I can observe a period overhead of the optimisation step (it is sess.run(train) in my code).\r\n\r\nAs you can see it here, every 5 steps, it needs 3s instead of 0.5 to do the optimisation.\r\n\r\nStep 105 duration : 3.5233473777770996\r\n\r\nStep 106 duration : 0.5653283596038818\r\n\r\nStep 107 duration : 0.5391891002655029\r\n\r\nStep 108 duration : 0.5480048656463623\r\n\r\nStep 109 duration : 0.0415492057800293\r\n\r\nStep 110 duration : 3.032115936279297\r\n\r\nStep 111 duration : 0.5407207012176514\r\n\r\nStep 112 duration : 0.5276811122894287\r\n\r\nStep 113 duration : 0.5448746681213379\r\n\r\nStep 114 duration : 0.04253268241882324\r\n\r\nStep 115 duration : 3.1273345947265625\r\n\r\nMoreover my GPU is almost all the time at 0% utilisation with around 90% of the memory used.\r\n\r\nIt seems that this overhead arrived when the Iterator finish to see all the dataset.\r\n\r\n\r\nDo you have any idea how I can speed up my training ?\r\nIt is due to the fact that my model is really simple ? \r\n\r\nBest,\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code **:  named Test_TimeDataset.py\r\n- **OS Platform and Distribution **:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version **: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0 and cuDNN 6.0.21\r\n- **GPU model and memory**: GeForce 1080 with 11Go \r\n- **Exact command to reproduce**: python Test_TimeDataset.py\r\n\r\nI also reproduce it with \r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version **: 1.8 \r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0 and cuDNN 7.1.3.16\r\nOn the same machine (I have both versions on the same Ubuntu session).\r\n\r\nBut I am not sure that Tensorflow use the cuDNN library.\r\n\r\nYou can also found the code [here](https://github.com/nicaogr/Git_Issues/blob/master/Test_TimeDataset.py)\r\n\r\n### Source code\r\n``\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os, time, multiprocessing\r\nimport matplotlib.pyplot as plt\r\n\r\ndef _floats_feature(value):\r\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))\r\n\r\n\r\ndef parser(record):\r\n    num_features = 2000\r\n    size_group = 300\r\n    num_classes= 10\r\n    class_indice = 0\r\n    keys_to_features={\r\n                'X': tf.FixedLenFeature([size_group*num_features],tf.float32),\r\n                'label' : tf.FixedLenFeature([num_classes],tf.float32)}\r\n    parsed = tf.parse_single_example(record, keys_to_features)\r\n    \r\n    label = parsed['label']\r\n    label = tf.slice(label,[class_indice],[1])\r\n    label = tf.squeeze(label) # To get a vector one dimension\r\n    X = parsed['X']\r\n    X= tf.reshape(X, [size_group,num_features])\r\n    return X, label\r\n\r\n\r\ndef test_train_w_dataset(): \r\n    num_features = 2000\r\n    num_ex = 2000\r\n    size_group = 300\r\n    num_classes = 10\r\n    batch_size= 480\r\n    max_iters = 300\r\n    buffer_size = 10000\r\n    \r\n    # Creation of the Dataset \r\n    filename_tfrecords = 'tmp.tfrecords'\r\n    if not(os.path.isfile(filename_tfrecords)): # If the file doesn't exist we will create it\r\n        print(\"Start creating the Dataset\")\r\n        writer = tf.python_io.TFRecordWriter(filename_tfrecords)\r\n        \r\n        for i in range(num_ex):\r\n            if i % 1000 == 0: print(\"Step :\",i)\r\n            X = np.random.normal(size=(size_group,num_features))\r\n            vectors =  2*np.random.randint(0,2,(num_classes,1))-1\r\n            features=tf.train.Features(feature={\r\n                        'X': _floats_feature(X),\r\n                        'label' : _floats_feature(vectors)})\r\n            example = tf.train.Example(features=features)       \r\n            writer.write(example.SerializeToString())\r\n        writer.close()\r\n    else:\r\n        print(\"The dataset tfrecords already exist\")\r\n     \r\n    train_dataset = tf.data.TFRecordDataset(filename_tfrecords)\r\n    num_proc = multiprocessing.cpu_count()\r\n    train_dataset = train_dataset.map(parser,\r\n                                        num_parallel_calls=num_proc)\r\n    dataset_shuffle = train_dataset.shuffle(buffer_size=buffer_size,\r\n                                                 reshuffle_each_iteration=True) \r\n    dataset_shuffle = dataset_shuffle.batch(batch_size)\r\n    dataset_shuffle = dataset_shuffle.repeat() \r\n    dataset_shuffle = dataset_shuffle.prefetch(batch_size) \r\n    shuffle_iterator = dataset_shuffle.make_initializable_iterator()\r\n    X_, y_ = shuffle_iterator.get_next()\r\n\r\n    W=tf.Variable(tf.random_normal([num_features], stddev=1.),name=\"weights\")\r\n    W=tf.reshape(W,(1,1,num_features))\r\n    Prod=tf.reduce_sum(tf.multiply(W,X_),axis=2)\r\n    Max=tf.reduce_max(Prod,axis=1)\r\n    Tan= tf.reduce_sum(tf.multiply(tf.tanh(Max),y_))\r\n    loss= tf.add(Tan,tf.reduce_sum(tf.multiply(W,W)))\r\n\r\n    LR = 0.01\r\n    restarts = 1\r\n    optimizer = tf.train.GradientDescentOptimizer(LR) \r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    train = optimizer.minimize(loss)  \r\n    print(\"The graph is defined\")\r\n    sess = tf.Session(config=config)\r\n        \r\n    durationTab = []\r\n    \r\n    for essai in range(restarts+1):\r\n        t0 = time.time()\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        sess.run(shuffle_iterator.initializer)\r\n        t1 = time.time()\r\n        duration = t1 - t0\r\n        print('Duration of initialization : ',duration)\r\n        for step in range(max_iters):\r\n            t0 = time.time()\r\n            sess.run(train)\r\n            t1 = time.time()\r\n            duration = t1 - t0\r\n            print(\"Step \",str(step),' duration : ',duration)\r\n            durationTab += [duration]\r\n            \r\n\r\n    plt.plot(durationTab)\r\n    plt.ylabel('Duration')\r\n    plt.xlabel('Iteration')\r\n    plt.show()\r\n\r\nif __name__ == '__main__':\r\n    test_train_w_dataset()\r\n\r\n``", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nExact command to reproduce", "Can you fill the cpus on training?", "In this test case, the data is 5Go, that mean I can load all the dataset on the CPU Memory without any problem. The idea should be to use bigger dataset later. \r\n\r\nThe CPU is utilization is also cyclic and almost 200% of the 4 core are used but I am reaching the full utilization of the CPU. \r\n![image](https://user-images.githubusercontent.com/23408564/39178890-2b44bcda-47b3-11e8-9419-94b0432f41af.png)\r\n", "So seems to me that you cannot produce with a throughput of 4 cpus at the same time but just with 2 if you are at a mean of 200%. I've a problem to fill all the cores simultaneously also with a simpler example like https://github.com/tensorflow/tensorflow/issues/15694#issuecomment-383300152", "But as you see on the image from my last post. I can get at least 80% for each core of the CPU sometimes. The 200% is more the average behavior.  \r\nBut the problem is maybe the one you mentioned in [#15694](https://github.com/tensorflow/tensorflow/issues/15694#issuecomment-383300152).", "From your image seems to me that they are alternating cycles around 50% and 80% so it is not a continuous throughput. ", "Yes that's true.\r\nIt seems that what @mrry mentionned in [#15694 (comment)](https://github.com/tensorflow/tensorflow/issues/15694#issuecomment-354377649) can permit to avoid the overhead. If I add the cache comment between the batch and the repeat comment as follows : \r\n```\r\n   dataset_shuffle = dataset_shuffle.batch(batch_size)\r\n   dataset_shuffle = dataset_shuffle.cache() \r\n   dataset_shuffle = dataset_shuffle.repeat() \r\n```", "@nicaogr can you grab timeline results that show the difference between the 3s and the 0.5s? I agree that something is happening every 5 timesteps that's pretty slow. It's not clear to me why you think the problem comes from datasets--is there some evidence that you have to point the finger there, or is it just the intuition about the size of your training set?\r\n\r\nI wonder whether you're running into an unintended consequence of your dataset (1.2G elements, right?) being able to fit into memory. It might be that the datasets API is designed to work in circumstances that are disk-bound, and that it's not optimized for the in-memory case.", "@cy89 Could be not similar to https://github.com/tensorflow/tensorflow/issues/15694#issuecomment-383300152?", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18690, "title": "problems with word2vec_basic.py", "body": "### System information\r\n- **OS Platform and Distribution: Linux Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.5\r\n\r\n\r\n\r\n### the problem\r\nhappened in the 'loss' function\r\nDid anyone else meet this problem?\r\n \r\n### Source code / logs\r\n  loss = tf.reduce_mean(\r\n      tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\r\n                     num_sampled, vocabulary_size))\r\n\r\n\r\nTypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'.\r\n", "comments": ["It seems something wrong with the order of parameters.\r\nModify it like that:\r\n```\r\nloss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\r\nbiases=nce_biases,\r\ninputs=embed,\r\nlabels=train_labels,\r\nnum_sampled=num_sampled,\r\nnum_classes=vocabulary_size))\r\n```"]}, {"number": 18689, "title": "[Request] Pre-build support old CPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 9.4 (stretch)\r\n- **CPU model**: Intel(R) Xeon(R) CPU E7- 8837  @ 2.67GHz and any \"old\" CPU\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n### Describe the problem\r\nSince version 1.6 prebuilt binaries use AVX instructions, thus TF cannot be used on \"old\" CPUs. However, many of them are not that old. In fact, I was surprise that my server cannot run TF. I believe many people run into the same problem because those CPUs are popular in many labs at universities.\r\n\r\nThis is bad because it blocks many potential users/researchers from trying TF and starting using TF. User should not build TF themselves, because building by each user is daunting and resource-wasteful.\r\n\r\nSo if it is not technically impossible, please provide alternative prebuilt binaries that support old CPUs.", "comments": ["You can find unofficial builds from other repo. \r\n", "@PaulWang1905 I should have mentioned that I could not find unofficial prebuilt for neither 1.6 nor 1.7. Moreover, I think this is a legitimate need, thus it should be supported officially.", "I am facing the same issue. My recent laptop can run TF without issue but on my desktop computer at the lab, I can't since the CPU is quite old.\r\n\r\nBuilding it is a possibility but honestly, it's quite a complex task...\r\n\r\nIf you are aware of some repo that provides unofficial Linux builds for old CPU please tell us.", "Also please consider document this problem clearer, or make the error message more informative. Currently `import tensorflow` just exit the program, kills `python` and says `Illegal instruction`.", "which generation is your microarchitecture?  I build tensorflow, but not for CPU. I can build unofficial builds if you guys really need", "@PaulWang1905 My CPU is [Intel(R) Xeon(R) CPU E7- 8837](https://ark.intel.com/products/53576/Intel-Xeon-Processor-E7-8837-24M-Cache-2_66-GHz-6_40-GTs-Intel-QPI). Detailed information is below. Thanks a lot!\r\n\r\n> vendor_id       : GenuineIntel\r\n> cpu family      : 6\r\n> model           : 47\r\n> model name      : Intel(R) Xeon(R) CPU E7- 8837  @ 2.67GHz\r\n> stepping        : 2\r\n> microcode       : 0x36\r\n> cpu MHz         : 2659.832\r\n> cache size      : 24576 KB\r\n> physical id     : 0\r\n> siblings        : 8\r\n> core id         : 0\r\n> cpu cores       : 8\r\n> apicid          : 0\r\n> initial apicid  : 0\r\n> fpu             : yes\r\n> fpu_exception   : yes\r\n> cpuid level     : 11\r\n> wp              : yes\r\n> flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good noplxtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt aes lahf_lm kaiser tpr_shadow vnmi flexpriority ept vpid dthermida arat\r\n> bugs            : clflush_monitor\r\n> bogomips        : 5319.66\r\n> clflush size    : 64\r\n> cache_alignment : 64\r\n> address sizes   : 44 bits physical, 48 bits virtual", "Here is mine:\r\n\r\n```\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x19\r\ncpu MHz\t\t: 1652.866\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 3\r\ncpu cores\t: 4\r\napicid\t\t: 7\r\ninitial apicid\t: 7\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 6414.75\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\n```", "Unfortunately we are unable to provide pre-built binaries without AVX. Unofficial third-party builds may be your best option.", "Forcing usage of machines with Intel CPU's that contain the AVX instruction is a false move in my opinion. I have been testing other AI platforms and they have no such requirement, Also when one is primarily focusing on GPU acceleration for AI apps the usage of AVX instruction becomes a mute point. This move is making it problematic for many user groups not having access to the latest CPU's for example hobbyists and schools with older hardware. Please consider a AVX free build or me and others will be forced to ditch TensorFlow for another platform. ", "@makeandbreak \r\n\r\nI have some questions to better understand the situation and please anyone response not just makeandbreak:\r\n\r\n- Do you do training on the older CPUs? and If so what do you train and how long does it take?\r\n- What are you doing with TensorFlow on those systems?  (I have a guess I do not want to lead the answer)\r\n\r\nWe are looking into options.  AVX very much speeds up performance for CPU over SSE, this is also true for AMD processors.  The move was done to improve out of the box performance for CPU users in general as well as those doing inference, which is often done on CPU and gets a large boost from AVX.  \r\n\r\nThis section is not to win you over but you let you know the thought process at a high level.  I looked at the Intel CPU line.  Sandy bridge, which is where AVX started came out, was in 2011.  AWS makes it fairly hard to get non-AVX systems for anything but random web serving and that is still limited.  I suspected many users are using mac book pros and other laptops that are sandy bridge and newer.  AVX provides a significant boost and I wanted to get that to end users.\r\n\r\nFinally for this comment, the build matrix is very large because it needs to be supported, e.g. all unit tests need to pass and it should be performance tested.  We want to make a pragmatic decision and your feed back is very helpful.  We do not know what we do not know and I personally want to balance ensuring people can use TensorFlow, with TensorFlow being fast, and the testing workload.  \r\n\r\nThe MKL build from Intel might work but I think they still default to AVX for ops other than those handled by MKL.  We are looking at this option.  It has been almost a year but I did test MKL on AMD Ryzen and it was fine and can be tested again.  Painful to test as at the time and even still now there are not many or any cloud providers with AMD systems, I suspect some schools still have opterons laying around.  ", "Assigning to myself in the short-term until we have a final answer.  If someone has a public build for 1.8 feel free to share in the short-term.  The rub is do you do python 2, 3 both and do you also do mac and windows along with linux.  :-)  The matrix explodes fast.  ", "Why we cannot approach a solution like https://github.com/tensorflow/tensorflow/issues/12541?\r\nUpdate: I cannot find the original mention about intrinsics but there was already another very old issue when AVX was still not enabled.", "tfboyd\r\n\r\nMy main use case is learning the functionality provided in AI tool kits \r\n(TensorFlow being one of them) for specific problem solving scenarios.\r\n\r\nAs such the data and model sizes are always a subset of the real world cases.\r\n\r\nWhen I have selected a specific toolkit and created the initial data and model \r\nthe work will continue on a AVX enabled platforms with also magnificent GPU \r\nresources or we will use cloud computing solutions.\r\n\r\nI have not found execution times to be unreasonable for the data and models I use.\r\n\r\nRestricting the compiled (none AVX) binaries to Python 3 could be \r\nconsidered a reasonable limitation as 2 is in depreciation mode.\r\n\r\nWhen is comes to platforms I would assume Windows 10 64bit and Ubuntu 18.04 LTS 64bit\r\nwould cover most of the user base on none AVX platforms.\r\n\r\nJust to note my 'old' platform is: Intel Core i7 875K, 16 GB DDR3 RAM, NVIDIA GeForce \r\nGTX 1050 Ti 4 GB DDR5\r\n\r\nIt's base performance has not dissapointed me so far so updating it just for AVX is as \r\nI see it a waste of money. I would need a new CPU, motherboard and possible memory (DDR4)\r\n", "@tfboyd Thanks for looking into this.\r\n\r\nAbout TF working process, I code up the model on my laptop (AVX). Then test that everything is working well on the CPU server (No AVX, large HDD/RAM, many CPU). Finally run training on a GPU server (shared and scheduled for many experiments). \r\n\r\nThe alternative build TF on CPU does not need to be fast, just workable is enough.\r\n\r\nPardon me for not knowing much about the building process, but is it possible to change building flags and keep everything else the same?", "I'm using a several years old CPU without AVX support and currently forced to stay with TF `1.5` for this reason, so I'm not able to benefit from newer updates and bug fixes.\r\n\r\nI've got several options (except buying a whole new computer):\r\n\r\n1. Try to compile latest TF from source (I'm using windows) - I'm working on getting to it, but it seems quite challenging (installing updates would be a complicated procedure as well).\r\n2. Buy a compatible GPU and skip CPU-based training/prediction altogether (??!).\r\n\r\n**Question**: will `tensorflow-gpu` work on a processor **without** AVX? (and if it currently will, will non-AVX systems be guaranteed to be supported for the next few years?)", "**Windows** users: I've found an [unofficial wheel repository](https://github.com/fo40225/tensorflow-windows-wheel) that includes a comprehensive collection (seems to be actively maintained) of pre-compiled binaries for many system combinations, including `TF 1.8.0`/`Py3.6`/`Amd64`/`SSE2` (No AVX) for both CPU and GPU (I've installed the CPU only version and it seems to work fine so far).\r\n\r\nSeveral other repositories, including some MacOS and Linux builds:\r\nhttps://github.com/mind/wheels (not sure if maintained anymore)\r\nhttps://github.com/lakshayg/tensorflow-build\r\nhttps://github.com/yaroslavvb/tensorflow-community-wheels/issues (various community contributed builds but doesn't cover everything)\r\n", "https://github.com/fo40225/tensorflow-windows-wheel seems to work. I used \r\n\r\n    pip install tensorflow_gpu-1.8.0-cp36-cp36m-win_amd64.whl\r\n\r\nwhl-file local copy downloaded from\r\n\r\n    tensorflow-windows-wheel/1.8.0/py36/GPU/cuda91cudnn71sse2/\r\n", "This is really useful information, while words can seem easy at times I want to express that we are working through this for the right solution.  What I am hearing is that it is really important to have a build that works anywhere as there are very likely \"many\" (hard to quantify but many seems fair) users on 6+ year old hardware and having it just work is very important.    @tatianashp @martinwicke    I helped make this choice and I am having second thoughts for sure and I do not like leaving you without a solution.  I do not believe there are huge changes in versions, so I hope you will user a slightly older version while we sort everything out.\r\n\r\n", "Adding @tatianashp  as an owner for visibility.", "Thank you for looking into this. Are you guys aware of similar builds for Linux? I know I could compile it myself but that would spare me one day of work...", "@tfboyd I agree that we need a better solution. For now, the only choices for those with old CPUs are to use community supported build, build from source or use an older version of TensorFlow. \r\n\r\n@hadim Building from source on Linux is not as difficult as it might seem and well documented. Please give a try if nobody from the community volunteers a pre-build wheel. Let us know if you have any questions.", "I didn't say it was complicated, just that I would have to spend some time on it while a simple `pip install` would have been done in a minute :-)", "What if a user on linux cannot `sudo`? Can they compile `tf` locally?", "Userspace compilation should be possible.", "I was looking at a few other AI toolkits to see if they require AVX support from the CPU. I could not find this to be the case. For example https://www.microsoft.com/en-us/cognitive-toolkit/ (CNTK) latest version works fine on my system.", "I think that also pytorch has as [SIMD dispatcher](https://github.com/pytorch/pytorch/issues/4825). In the updated TF [Roadmap](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/roadmap.md#cpu-optimizations)  we have:\r\n> Dynamic loading of SIMD-optimized kernels\r\n\r\nThat was add on 22th Feb", "We also have \"MKL for Linux and Windows\" in the roadmap. The build will be SSE-based and will use MKL dynamic SIMD dispatch to leverage AVX or AVX2 when it's present. This is likely to happen before dynamic loading of SIMD-optimized kernels. ", "I would like to ask the same question @rotemdan asked above i.e.\r\n\"will tensorflow-gpu work on a processor without AVX?\"\r\nI'm running into the issue where I have a new GPU (gtx 1060) but an old CPU (6+ years old). I'm stuck at TF 1.5. Is there a way to get 1.7 or 1.8 to work?", "Yes, you should be able to install from source without trouble, assuming you have the dependencies installed (all called out in the installation guide). ", "\"will tensorflow-gpu work on a processor without AVX?\"\r\n\r\nI have tested TensorFlow 1.8 GPU on windows and it seems AVX code is in the base code. So in other words no it does not work. ", "FYI, here is a Docker image that can build TensorFlow https://github.com/hadim/docker-tensorflow-builder. It can help to compile TF on a wide range of configurations as long as you have Docker installed on it.", "Just adding my 2c to the thread... The laptop i use for the initial check of the code before running it on the cloud has ssd and not enough space to build tf. It's uncomplicated build, yes... But huge. ", "I built Tensorflow 1.8 on a GPU-less Ubuntu box without AVX support. Maybe this wheel is useful for some of you: [repo](https://github.com/maxhgerlach/tensorflow-1.8.0-ubuntu16.04-py27-no_avx-xeon_x5650)", "Thank you! I actually did the same a few days ago, with external usb drive. Bazel was not impressed... But, after a short fight, i managed to make a wheel. It's on my github repos. Thank you again for your offer. I still think tf should provide such no strings attached wheel. ", "If you're not going to support non-AVX, at least improve your checks or errors. \"Illegal instruction\" in response to \"import keras\" is not helpful.", "> I built Tensorflow 1.8 on a GPU-less Ubuntu box without AVX support. Maybe this wheel is useful for some of you: [repo](https://github.com/maxhgerlach/tensorflow-1.8.0-ubuntu16.04-py27-no_avx-xeon_x5650)\r\n\r\nThis worked great on my old xeon E5507. Thanks a bunch!!!!!", "Confirming that TF 1.8 CPU build without AVX from @maxhgerlach works on Intel Pentium G4400 (Skylake) without AVX.", "I built a Tensorflow 2.0 version running with old CPUs not compatible with AVX instructions nor SSE. See here on my Github : https://github.com/Anakeyn/Tensorflow2.0ForOldComputers. \r\nRem : this is a Linux version !!! I tested it under Windows 10 WSL Ubuntu 18.04 LTS on a Lenovo PR-G700 with Intel Celeron CPU 1000M 1,80 Ghz (2013 CPU) and directly under Linux Ubuntu 18.04.3 LTS on an HP Pavillon DV6 with Intel Core 2 Duo CPU T6600 2.2 GHz x 2  (2009 CPU) and it works.", "> I built a Tensorflow 2.0 version running with old CPUs not compatible with AVX instructions nor SSE. See here on my Github : https://github.com/Anakeyn/Tensorflow2.0ForOldComputers.\r\n> Rem : this is a Linux version !!! I tested it under Windows 10 WSL Ubuntu 18.04 LTS on a Lenovo PR-G700 with Intel Celeron CPU 1000M 1,80 Ghz (2013 CPU) and directly under Linux Ubuntu 18.04.3 LTS on an HP Pavillon DV6 with Intel Core 2 Duo CPU T6600 2.2 GHz x 2 (2009 CPU) and it works.\r\n\r\nThank you man! I was trying to update tensorflow with a source build, but accidentally broke my server while trying. So I was trying to find precompiled binaries and I am glad I found yours!", "> \r\n> \r\n> > I built a Tensorflow 2.0 version running with old CPUs not compatible with AVX instructions nor SSE. See here on my Github : https://github.com/Anakeyn/Tensorflow2.0ForOldComputers.\r\n> > Rem : this is a Linux version !!! I tested it under Windows 10 WSL Ubuntu 18.04 LTS on a Lenovo PR-G700 with Intel Celeron CPU 1000M 1,80 Ghz (2013 CPU) and directly under Linux Ubuntu 18.04.3 LTS on an HP Pavillon DV6 with Intel Core 2 Duo CPU T6600 2.2 GHz x 2 (2009 CPU) and it works.\r\n> \r\n> Thank you man! I was trying to update tensorflow with a source build, but accidentally broke my server while trying. So I was trying to find precompiled binaries and I am glad I found yours!\r\n\r\nYou're welcome! Thank you for your feedback!", "> I built a Tensorflow 2.0 version running with old CPUs not compatible with AVX instructions nor SSE. See here on my Github : https://github.com/Anakeyn/Tensorflow2.0ForOldComputers.\r\n> Rem : this is a Linux version !!! I tested it under Windows 10 WSL Ubuntu 18.04 LTS on a Lenovo PR-G700 with Intel Celeron CPU 1000M 1,80 Ghz (2013 CPU) and directly under Linux Ubuntu 18.04.3 LTS on an HP Pavillon DV6 with Intel Core 2 Duo CPU T6600 2.2 GHz x 2 (2009 CPU) and it works.\r\n\r\n@Anakeyn, I tried to install tensorflow following your instructions and it seems to be installed (\"import tensorflow\" gives no errors) but it seems like for some reason the tensorflow has no attributes installed at all. I ran multiple attributes (tf.test, tf.constant, tf.session) and all of them returned the error: \" module 'tensorfow' has no attribute ..\". \r\n\r\nDid you have to manually install anything else after that? Am I missing something?\r\nThanks!", "@anavc97 \r\nIt seems that you are using tensorflow 1.x syntax. Some modules were removed in version 2. x\r\nSee last documentation for tensorflow 2 : [https://www.tensorflow.org/api_docs/python/tf/version](https://www.tensorflow.org/api_docs/python/tf/version)\r\nCheck also how to migrate from tensorflow 1 to 2 [https://www.tensorflow.org/guide/migrate](https://www.tensorflow.org/guide/migrate)\r\nYou can also running tensorflow 1.x code with the following instructions :\r\n\r\n`import tensorflow.compat.v1 as tf`\r\n`tf.disable_v2_behavior()`\r\n\r\n\r\n", "> @anavc97\r\n> It seems that you are using tensorflow 1.x syntax. Some modules were removed in version 2. x\r\n> See last documentation for tensorflow 2 : https://www.tensorflow.org/api_docs/python/tf/version\r\n> Check also how to migrate from tensorflow 1 to 2 https://www.tensorflow.org/guide/migrate\r\n> You can also running tensorflow 1.x code with the following instructions :\r\n> \r\n> `import tensorflow.compat.v1 as tf`\r\n> `tf.disable_v2_behavior()`\r\n\r\n@Anakeyn \r\nThank you for your reply. It didn't recognize compat either, actually. \r\nI fixed it by moving the folders in the zip to the site-packages directory of python.\r\nUnfortunately I'm having trouble using the TF2.0 instalation with GPU. It's not recognizing my device, even though I installed the compatible CUDA and Cudnn versions (CUDA: 10.0, Cudnn: 7.4). I'm currently debugging this, but there's not much information except it returns \"False\" when I run\r\n\r\n`import tensorflow as tf`\r\n`print(tf.test.is_gpu_available())`\r\n`print(tf.test.is_built_with_cuda())`\r\n\r\nI don't suppose you have, by any chance, information that might help me? ", "@anavc97 \r\nThis version is not compatible with GPU !  I didn't build it with GPU options !  This is a version for VERY old computers :-)\r\nPlease take a look at this page to see how to build a version with GPU options : [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source)", "> @anavc97\r\n> This version is not compatible with GPU ! I didn't build it with GPU options ! This is a version for VERY old computers :-)\r\n> Please take a look at this page to see how to build a version with GPU options : https://www.tensorflow.org/install/source\r\n\r\n@Anakeyn thanks for the input! I thought any version 2 of TF was compatible with GPU, my mistake. I'll try to build TF from source :)", "@tatianashp @av8ramit @tfboyd  it seems that, as members of TensorFlow, you are trying to justify the AVX generalization and that there is a lack of evidence for rejecting the idea \"everybody that cares about good performance has AVXs ready CPUs\" when you do not have enough evidence to prove it either. Have you ever considered ALL THE OTHER users/practitioners that stumble upon this AVX exclusivity and did not leave a message here? (let alone try to build it from source). \r\n\r\nI will add my part then. I work at a university in Mexico and we have a 64 threads CPU IBM cluster, we use it for research and teaching in undergraduate and graduate courses. Only a few universities in Mexico have the resources to buy a physical server or to grant a budget for cloud computing, and so many countries in the world are in the same general position or worst. \r\n\r\nI do appreciate the effort and maintenance of the library, but your rhetoric about \"generalization\" is the one I strongly disagree with. (and just to be clear, I'm having problems since the server I'm using does not support AVX)", "Hello @IFFranciscoME. I would first like to apologize to you for the difficulties you have been having. We made this decision in 2018 with a lot of different factors in mind. I definitely did not make this decision singlehandedly, and I was just one of the first to respond to the issue. Obviously that decision was not perfect, and I'm sure there were users without AVX that had to turn to building from source.\r\n\r\nThat being said if you'd like to continue this discussion I'd like to invite you to the [Build Special Interest Group](https://github.com/tensorflow/build#community) where we discuss different TensorFlow build topics, configurations, and solutions to problems similar to this one. "]}, {"number": 18688, "title": "Estimator.predict() has Shape Issues?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 8.1\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.6\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n-**Exact command to reproduce**:\r\nJust execture run.py in this [repository](https://github.com/selcouthlyBlue/ShapeErrorReproduce)\r\n\r\n### Describe the problem\r\nI've already posted this [problem](https://stackoverflow.com/questions/49911525/estimator-predict-has-shape-issues?noredirect=1#comment86845047_49911525) in stackoverflow. Unfortunately, no one can figure it out there so I'm posting it here as well.\r\n\r\nI can train and evalaute a Tensorflow Estimator model without any problems. When I do prediction, this error arises:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 79 should be: 2\r\n\t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n```\r\n\r\n\r\nAll of the model functions I made use the same architecture:\r\n\r\n```\r\ndef _train_model_fn(features, labels, mode, params):\r\n    features, outputs = _get_fed_features_and_resulting_predictions(features, params['num_classes'])\r\n    predictions = {\"outputs\": outputs}\r\n\r\n    ... # loss initialization and whatnot\r\n\r\n    return _create_model_fn(mode,\r\n                                            predictions,\r\n                                            loss,\r\n                                            train_op)\r\n\r\ndef _predict_model_fn(features, mode, params):\r\n    features, outputs = _get_fed_features_and_resulting_predictions(features, params['num_classes'])\r\n    predictions = {\"outputs\": outputs}\r\n\r\n    return _create_model_fn(mode, predictions,\r\n                            export_outputs={\r\n                                \"outputs\": tf.estimator.export.PredictOutput(outputs)\r\n                            })\r\n\r\n\r\ndef _create_model_fn(mode, predictions, loss=None, train_op=None, export_outputs=None):\r\n    return tf.estimator.EstimatorSpec(mode=mode,\r\n                                      predictions=predictions,\r\n                                      loss=loss,\r\n                                      train_op=train_op,\r\n                                      export_outputs=export_outputs)\r\n```\r\n\r\nHere's the training code and the predict code:\r\n\r\n```\r\ndef train(features, labels, num_classes, params, checkpoint_dir,\r\n          batch_size=1, num_epochs=1, save_checkpoint_every_n_epochs=1):\r\n    num_steps_per_epoch = len(features) // batch_size\r\n    save_checkpoint_steps = save_checkpoint_every_n_epochs * num_steps_per_epoch\r\n    params['num_classes'] = num_classes\r\n    params['log_step_count_steps'] = num_steps_per_epoch\r\n    estimator = tf.estimator.Estimator(model_fn=_train_model_fn,\r\n                                       params=params,\r\n                                       model_dir=checkpoint_dir,\r\n                                       config=tf.estimator.RunConfig(\r\n                                           save_checkpoints_steps=save_checkpoint_steps,\r\n                                           log_step_count_steps=num_steps_per_epoch,\r\n                                           save_summary_steps=num_steps_per_epoch\r\n                                       ))\r\n    estimator.train(input_fn=_input_fn(features, labels, batch_size),\r\n                    steps=num_epochs * num_steps_per_epoch)\r\n\r\n\r\ndef predict(features, params, checkpoint_dir):\r\n    estimator = tf.estimator.Estimator(model_fn=_predict_model_fn,\r\n                                       params=params,\r\n                                       model_dir=checkpoint_dir)\r\n    predictions = estimator.predict(input_fn=_input_fn(features))\r\n    for i, p in enumerate(predictions):\r\n        print(i, p)\r\n```\r\n\r\nI also checked the shapes given every time the input passes a layer when training, and the same thing for predicting. They give the same shapes:\r\n\r\nTraining:\r\n\r\n```\r\nconv2d [1, 358, 358, 16]\r\nmax_pool2d [1, 179, 179, 16]\r\ncollapse_to_rnn_dims [1, 179, 2864]\r\nbirnn [1, 179, 64]\r\n```\r\n\r\nPrediction:\r\n\r\n```\r\nconv2d [1, 358, 358, 16]\r\nmax_pool2d [1, 179, 179, 16]\r\ncollapse_to_rnn_dims [1, 179, 2864]\r\nbirnn [1, 179, 64]\r\n```\r\n\r\nHere are the `SparseTensors` that are passed to `sparse_to_dense`:\r\n\r\nTraining:\r\n\r\n`SparseTensor(indices=Tensor(\"CTCBeamSearchDecoder:0\", shape=(?, 2), dtype=int64), values=Tensor(\"CTCBeamSearchDecoder:1\", shape=(?,), dtype=int64), dense_shape=Tensor(\"CTCBeamSearchDecoder:2\", shape=(2,), dtype=int64))`\r\n\r\nPrediction:\r\n\r\n`SparseTensor(indices=Tensor(\"CTCBeamSearchDecoder:0\", shape=(?, 2), dtype=int64), values=Tensor(\"CTCBeamSearchDecoder:1\", shape=(?,), dtype=int64), dense_shape=Tensor(\"CTCBeamSearchDecoder:2\", shape=(2,), dtype=int64))`\r\n\r\nWhich are all pretty much the same.\r\n\r\nAny reason why this is happening? Shouldn't the `_predict_model_fn` work given that it follows the same architecture as that of the other `model_fn`s?\r\n\r\nHere's the full stacktrace: (Note: I called the predict function after evaluation)\r\n\r\n```\r\nINFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000006041C05978>, '_model_dir': 'model-20180420-112636', '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000, '_task_id': 0, '_session_config': None, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_service': None, '_tf_random_seed': None, '_log_step_count_steps': 100, '_evaluation_master': '', '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_master': '', '_keep_checkpoint_max': 5, '_is_chief': True, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600}\r\nINFO:tensorflow:Calling model_fn.\r\n[1, 179, 64]\r\nINFO:tensorflow:Done calling model_fn.\r\nSparseTensor(indices=Tensor(\"CTCBeamSearchDecoder:0\", shape=(?, 2), dtype=int64), values=Tensor(\"CTCBeamSearchDecoder:1\", shape=(?,), dtype=int64), dense_shape=Tensor(\"CTCBeamSearchDecoder:2\", shape=(2,), dtype=int64))\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from model-20180420-112636\\model.ckpt-1\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 82 should be: 2\r\n\t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py\", line 56, in <module>\r\n    main()\r\n  File \"C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py\", line 52, in main\r\n    predict_with_model(checkpoint_dir)\r\n  File \"C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py\", line 46, in predict_with_model\r\n    predict(images, run_params, checkpoint_dir)\r\n  File \"C:\\Users\\asus.11\\Documents\\ShapeErrorReproduce\\experiment_ops.py\", line 173, in predict\r\n    for i, p in enumerate(predictions):\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 492, in predict\r\n    preds_evaluated = mon_sess.run(predictions)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 546, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1022, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1113, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\six.py\", line 693, in reraise\r\n    raise value\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1098, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1170, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 950, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 82 should be: 2\r\n\t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n\r\nCaused by op 'output', defined at:\r\n  File \"C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py\", line 56, in <module>\r\n    main()\r\n  File \"C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py\", line 52, in main\r\n    predict_with_model(checkpoint_dir)\r\n  File \"C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py\", line 46, in predict_with_model\r\n    predict(images, run_params, checkpoint_dir)\r\n  File \"C:\\Users\\asus.11\\Documents\\ShapeErrorReproduce\\experiment_ops.py\", line 173, in predict\r\n    for i, p in enumerate(predictions):\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 479, in predict\r\n    features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 793, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\Users\\asus.11\\Documents\\ShapeErrorReproduce\\experiment_ops.py\", line 129, in _predict_model_fn\r\n    features, outputs = _get_fed_features_and_resulting_predictions(features, params['num_classes'])\r\n  File \"C:\\Users\\asus.11\\Documents\\ShapeErrorReproduce\\experiment_ops.py\", line 124, in _get_fed_features_and_resulting_predictions\r\n    outputs = _get_decoded_outputs(features, num_classes)\r\n  File \"C:\\Users\\asus.11\\Documents\\ShapeErrorReproduce\\experiment_ops.py\", line 73, in _get_decoded_outputs\r\n    name=\"output\")\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 791, in sparse_to_dense\r\n    name=name)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\", line 2401, in _sparse_to_dense\r\n    name=name)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 82 should be: 2\r\n\t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n```\r\n\r\nApparently, the shape error also changes with every run.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Hi @selcouthlyBlue \r\nAre you using same _input_fn for train and predict? (this is to make sure that they are same)\r\nIt is difficult to find out problem from your description. Could you please provide us a small script which reproduce this issue?", "I'm using `numpy_input_fn` for both `input_fn`s. Here's the reproducible example: https://github.com/selcouthlyBlue/ShapeErrorReproduce\r\n\r\nJust execute run.py ", "I have finally figured out the error. The problem actually lies in the way I used `sparse_to_dense`. Apparently, the order I gave is wrong where the values came first before the shape:\r\n\r\n```\r\nreturn tf.sparse_to_dense(tf.to_int32(decoded[0].indices),\r\n                              tf.to_int32(decoded[0].values),\r\n                              tf.to_int32(decoded[0].dense_shape),\r\n                              name=\"output\")\r\n```\r\n\r\nThe order should be (shape comes first before values):\r\n\r\n```\r\nreturn tf.sparse_to_dense(tf.to_int32(decoded[0].indices),\r\n                              tf.to_int32(decoded[0].dense_shape),\r\n                              tf.to_int32(decoded[0].values),\r\n                              name=\"output\")\r\n```\r\n\r\nMy apologies for thinking it was a bug. It was just me :(", "thanks for letting us know."]}, {"number": 18687, "title": "Absence of 'tanh()' operation in the computation of attention vector", "body": "Tanh() operation is missed in the computation of attention vector (https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py), which is mentioned useful in the definition of Attetntion vector(https://www.tensorflow.org/tutorials/seq2seq).\r\n\r\nOS : Ubuntu 14.04.4 LTS\r\nTensorFlow installed from: (https://files.pythonhosted.org/packages/f7/52/4f78cc674775bd36a28223fd63d98260eb7130128298f7c70edbdcb34075/tensorflow-1.4.1-cp34-cp34m-manylinux1_x86_64.whl)   via pip \r\nTensorFlow version: tensorflow-gpu 1.4.1 \r\nCUDA/cuDNN version: 8.0\r\nGPU model and memory: Tesla K80\r\n\r\nThe problem is found when I compare the attention vector value, computed with Numpy and network's weights, to those retrieved from Attention Vector tensor. These two vectors are the same when I did not apply tanh(), meaning a tanh() opertion is missed with regards to the Attention vector's definition.", "comments": ["This is fixed on the master branch with 0586c572.\r\n\r\nThe new `attention_layer` argument allows the use of an arbitrary layer to produce the attention vector. For example, to replicate the equation (3) from [Background on the Attention Mechanism](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism):\r\n\r\n```python\r\ncell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell,\r\n    attention_mechanism,\r\n    initial_cell_state=initial_cell_state,\r\n    attention_layer=tf.layers.Dense(512, activation=tf.tanh, use_bias=False))\r\n```", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@guillaumekln  Thanks for your reply! I'm checking this latest version."]}, {"number": 18686, "title": "fix typo", "body": "fix typo\r\nunit8 -> uint8", "comments": []}, {"number": 18685, "title": "XLA CPU Fft no core framework and single thread", "body": "See https://github.com/tensorflow/tensorflow/issues/18588\r\n\r\nDoes two things:\r\n1. Fixes the XLA CPU implementation of the fast fourier transform to avoid reliance on the tf core framework.\r\n2. Allows AOT compilation of FFT to single-threaded environments.\r\n\r\nSee the linked issue for a longer discussion. I ran the [`randomized_tests.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/randomized_tests.cc) suite and it passed with `--tf_xla_test_use_jit=false --tf_xla_test_device=CPU:0`. None of the tests (neither my changes nor any other ops) would run with `--tf_xla_test_use_jit=true`; I don't know if that's a problem with my setup.\r\n\r\nHappy to make changes.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Ok, I figured out that I had to manually execute `bazel-bin/tensorflow/compiler/tests/fft_test_cpu --test_device=CPU --types=DT_HALF,DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64,DT_BOOL,DT_COMPLEX64`, and then it passes all tests.\r\n\r\nIf I just run `bazel test --config=opt //tensorflow/compiler/tests:fft_test` (which calls `--test_device=XLA_CPU`; see [build_defs.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/build_defs.bzl)) then it fails with the same error as the randomized tests error:\r\n```\r\n2018-04-21 01:14:32.248668: I tensorflow/compiler/xla/service/service.cc:159] XLA service 0x7f928643e070 executing computations on platform Host. Devices:\r\n2018-04-21 01:14:32.248701: I tensorflow/compiler/xla/service/service.cc:167]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-04-21 01:14:32.780468: F tensorflow/compiler/xla/service/cpu/cpu_executable.cc:69] Check failed: sym Symbol cluster_0__XlaCompiledKernel_true__XlaNumConstantArgs_0__XlaNumResourceArgs_0_.v31 not found.\r\nAbort trap: 6\r\n```\r\nI'm not sure if that means that I didn't build something I was supposed to build, or if there's some subtle problem with my changes, or something else.", "@sanjoy could you please continue the review after @nuchi's comments?", "Thanks! Out of curiosity, do you know of a simple explanation for the test failure I saw?", "> Thanks! Out of curiosity, do you know of a simple explanation for the test failure I saw?\r\n\r\nAre you able to reproduce the failure?  If so, can you please post the full log output with `--v=2 --logtostderr`?", "What's the full command, please?\r\n```\r\n$ bazel test --config=opt //tensorflow/compiler/tests:fft_test --v=2 --logtostderr\r\nERROR: Unrecognized option: --v=2\r\n```\r\n```\r\n$ bazel-bin/tensorflow/compiler/tests/fft_test_cpu --v=2 --logtostderr --test_device=XLA_CPU --types=DT_HALF,DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64,DT_BOOL,DT_COMPLEX64\r\noption --verbose must not have an argument\r\nUsage: fft_test.py [options] [test] [...]\r\n\r\nOptions:\r\n  -h, --help       Show this message\r\n  -v, --verbose    Verbose output\r\n...\r\n```\r\nand\r\n```\r\n$ bazel-bin/tensorflow/compiler/tests/fft_test_cpu -v --test_device=XLA_CPU --types=DT_HALF,DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64,DT_BOOL,DT_COMPLEX64\r\ntestContribSignalSTFT (__main__.FFTTest) ... 2018-04-26 19:13:32.977221: I tensorflow/compiler/xla/service/service.cc:159] XLA service 0x7f951bd49090 executing computations on platform Host. Devices:\r\n2018-04-26 19:13:32.977300: I tensorflow/compiler/xla/service/service.cc:167]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-04-26 19:13:35.492196: F tensorflow/compiler/xla/service/cpu/cpu_executable.cc:69] Check failed: sym Symbol cluster_0__XlaCompiledKernel_true__XlaNumConstantArgs_0__XlaNumResourceArgs_0_.v31 not found.\r\nAbort trap: 6\r\n```", "I can't reproduce this failure with your change.  Are you possibly based on an older git revision?  The test passed with your changes merged with git sha1 `1863070e234a4a1c34a70ff73896bc9eff29fc70`.", "From about a week ago. If you can't reproduce then it's probably fine.", "Does anything else need to happen on my end? I wasn't sure what `Ubuntu CC Expected \u2014 Waiting for status to be reported` meant and whether it is supposed to take a long time.", "I don't know what the Ubuntu CC bit is either.  @martinwicke ?", "If you're happy with this PR, set the \"awaiting testing\" and \"kokoro:force-run\" labels. That'll trigger the tests.", "Looks like the gpu test failed for \"Internal CI infrastructure error\". @sanjoy would you rerun the tests?", "Does it need to keep the \"await testing and then merge\" label to get merged? I noticed all the tests passed."]}, {"number": 18684, "title": "How to use float inception model instead of quantised mobile net.", "body": "OS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow installed from: Git cloned\r\nTensorFlow version: N/A\r\nBazel version: Build label: 0.11.1\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: x86 8GB RAM\r\nExact command to reproduce: Using Android studio as a existing project\r\nHave I written custom code: no\r\n\r\nHow to use float inception model instead of quantised mobile net.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "updated..", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18683, "title": "Add shape check for batch related Dataset ops", "body": "This fix adds several shape check for batch related Dataset ops: `BatchDataset`, `DenseToSparseBatchDataset`, `SlideDataset`, `PrefetchDataset`.\r\n\r\nNote: Most of the restrictions are scalar. However, `row_shape` in `DenseToSparseBatchDataset` should be a 1-D vector.", "comments": []}, {"number": 18682, "title": "Shape validation with random/shuffle related Dataset ops", "body": "This fix add shape validation with random/shuffle related Dataset ops: `ShuffleAndRepeatDataset`, `ShuffleDataset`,  `RandomDataset`, `RangeDataset`, `CacheDataset`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18681, "title": "When override the git tag, the commits in front of tag val should be 0.", "body": "For the git_version string, when overriding the git tag (when creating\r\nrelease builds before tag is made), the commits in front of tag val\r\nshould be 0. Its expected you are going to create the new git tag\r\nat the current commit.", "comments": []}, {"number": 18680, "title": "Validation in shape functions of Dataset ops", "body": "This fix does validations on several Dataset ops: `PrependFromQueueAndPaddedBatchDataset`, `FixedLengthRecordDataset`, `SqlDataset`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18679, "title": "Cherrypicks 1.8rc1 fix tf.GIT_VERSION always 'unknown' on windows cmake build (#16730)", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "clabot this is a cherrypick from #16730", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "Oops, I merged this. @yifeif I hope I didn't mess up the release?"]}, {"number": 18678, "title": "Add --test_output=errors as default", "body": "", "comments": ["lgtm"]}]