[{"number": 7692, "title": "Update doc string to indicate clip_by_value accepts tensors as min and max arguments too", "body": "`tf.clip_by_value`'s current documentation mentions that it accepts only scalars as min and max arguments, but it can accept tensors too. This PR fixes that, and resolves the confusion raised in #7225  ", "comments": ["Can one of the admins verify this patch?", "cc @yaroslavvb @vrv "]}, {"number": 7691, "title": "Update stat_summarizer_test.py", "body": "Optimizing and easy to understand the code, I have set \"sa = self.assertRegexpMatches\" and have called the variable sa later instead of \"self.assertRegexpMatches\" many times", "comments": ["Can one of the admins verify this patch?", "Anyone ?", "This style: 'self.assertRegexpMatches' is used throughout the whole repository where test.TestCase us used. Other assertions, e.g. 'assertEqual', 'assertFalse' etc... also follow this kind of explicit calling, let's keep it this way."]}, {"number": 7690, "title": "Docs about installing Tensorflow for Ubuntu don't always distinguish between python and python3", "body": "In this page [Installing TensorFlow on Ubuntu ](https://www.tensorflow.org/install/install_linux), there are some commands to install Python in Ubuntu. Tough Ubuntu distinguishes between python (2.7) and python3 (3.n) packages. So I think that the page should be updated to distinguish the two cases, as using the current procedure may result in the creation of a python (2.7) environment when a python3 was needed or preferred.\r\n\r\n## Update proposal\r\n### [virtualenv](https://www.tensorflow.org/install/install_linux#InstallingVirtualenv)\r\n\r\nAt the point 1:\r\n```\r\nsudo apt-get install python-pip python-dev python-virtualenv # for Python 2.7\r\nsudo apt-get install python3-pip python3-dev python3-virtualenv # for Python 3.n\r\n```\r\nAt the point 2:\r\n```\r\npython -m virtualenv --system-site-packages -p python targetDirectory # for Python 2.7\r\npython3 -m virtualenv --system-site-packages -p python3 targetDirectory # for Python 3.n\r\n```\r\n\r\n### [\"native\" pip](https://www.tensorflow.org/install/install_linux#InstallingNativePip)\r\n\r\nPrerequisites:\r\n```\r\nsudo apt-get install python-pip python-dev # for Python 2.7\r\nsudo apt-get install python3-pip python3-dev # for Python 3.n\r\n```\r\n", "comments": ["If you only have python3 installed on your system, `python` and `pip` will use `python3` and `pip3` respectively. To add to all of these, you can even install more versions of python, and access them using `python3.4` or `python3.5` or `python3.6`.\r\nAll of these possibilities are omitted from the docs for simplicity.\r\nIn most users cases, there is only one version of python installed, so the instructions will work.\r\nFor users with multiple python versions installed, we trust users to correctly access their desired python installation.", "I'm sorry but using python 2.7 or 3.n is a different issue than just choosing a point version, and accordingly the guide makes a distinction in most cases except the ones I mentioned.\r\n\r\nIf a user follows each step, including the [first](https://www.tensorflow.org/install/install_linux#InstallingVirtualenv), he/she will end up installing python (2.7). For the user to skip that line he/she should be aware both that that command is going to install python (2.7) and how to find the python3 versions of the listed packages. As the guide makes a distinction in many cases between python and python3, the user could wrongly consider that line valid for both. \r\n\r\nAlso, for python3, the command ```virtualenv``` cannot be invoked in that way on Ubuntu (tested on 16.04), but only using ```python3 -m virtualenv```. Installing ```python-virtualenv``` causes the installation of the version for python 2.7.", "I have not considered the invalidity of our virtualenv command for py3. You have a good point there. Id rather have a shorter installation guide, but we should have a working install path for all versions of python installed.", "I have checked this again, but for me having `python-virtualenv` command is sufficient.\r\nYou simply need to feed the `-p` flag, and thats it.\r\nI tested this on a clean docker image as follows:\r\n```\r\n$ docker run -it ubuntu:16.04 bash\r\n$ apt-get update\r\n$ apt-get install python3 python-virtualenv\r\n$ virtualenv -p python3 tf\r\n$ source tf/bin/activate\r\n$ python3 --version\r\nPython 3.5.2\r\n```\r\n\r\nAnyway, we are updating the documentation with more stuff.", "Good, it makes the docs more concise than my proposal.\n\nOn Wed, 8 Mar 2017, 23:17 gunan, <notifications@github.com> wrote:\n\n> I have checked this again, but for me having python-virtualenv command is\n> sufficient.\n> You simply need to feed the -p flag, and thats it.\n> I tested this on a clean docker image as follows:\n>\n> $ docker run -it ubuntu:16.04 bash\n> $ apt-get update\n> $ apt-get install python3 python-virtualenv\n> $ virtualenv -p python3 tf\n> $ source tf/bin/activate\n> $ python3 --version\n> Python 3.5.2\n>\n> Anyway, we are updating the documentation with more stuff.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7690#issuecomment-285187544>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABO3xqrbG_ckbO4MnVLJH8w-FV0jgNZaks5rjykIgaJpZM4MF7Bh>\n> .\n>\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow documentation. Thank you."]}, {"number": 7689, "title": "tf.nn.softmax errors out, when dim=<dim_size - 1> instead of -1", "body": "### Environment info\r\nOperating System:\r\nMacOS 10.12.1\r\n\r\nInstalled version of CUDA and cuDNN: \r\nlrwxr-xr-x  1 root  wheel     50 Sep 26 15:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x  1 root  wheel     47 Oct 24 21:11 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\n\r\nTensorflow version 0.12.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\ndef test_softmax():\r\n  with tf.variable_scope(\"test_coattention_layer\"):\r\n    doc1_placeholder = tf.placeholder(tf.float32, shape=(None, 4, 3))\r\n\r\n  init = tf.global_variables_initializer()\r\n\r\n  with tf.Session() as session:\r\n    session.run(init)\r\n    input = np.array(range(2 * 4 * 3), dtype=np.float32).reshape((2, 4, 3))\r\n\r\n    softmax1 = tf.nn.softmax(doc1_placeholder, dim=-1) # <======== THIS WORKS\r\n    softmax2 = tf.nn.softmax(doc1_placeholder, dim=2) # <======== THIS BREAKS\r\n\r\n    print(\"doc1 = \" + str(input))\r\n    softmax1_out = session.run(softmax1, feed_dict={doc1_placeholder: input})\r\n    softmax2_out = session.run(softmax2, feed_dict={doc1_placeholder: input})\r\n    print(\"softmax1_out = \" + str(softmax1_out))\r\n    print(\"softmax2_out = \" + str(softmax2_out))\r\n\r\n```\r\n\r\nError message:\r\n\r\nInvalidArgumentError (see above for traceback): Requires start <= limit when delta > 0: 3/2\r\n\t [[Node: range_1 = Range[Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](range_1/start, Sub, range_1/delta)]]\r\n", "comments": ["@annarev, could you take a quick look?\r\n", "Looks like this was fixed by this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/fb53109ad20b2e568246b05796296a047c271856\r\n\r\nIt doesn't seem to be in the r1.0 branch unfortunately, but should be in the nightly build.", "Closing the issue, as the fix seems to be in nightly build and should be in the next release.", "i have the same problem with you , have you worked out?", "@SherryyHou i haven't tested it since. try upgrading to the latest version."]}, {"number": 7688, "title": "[Java] [Feature] Load from SavedModel", "body": "Added `SavedModelBundle` class to represent a loaded saved model.\r\n\r\nCloses #7134\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@asimshankar as we discussed.", "Thanks for writing this! :+1: ", "Updated based on review comments.   ", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "@EronWright there are test failures: \r\n\r\nERROR: /workspace/tensorflow/workspace.bzl:422:3: in java_import_external rule //external:junit: \r\nTraceback (most recent call last):\r\n\tFile \"/workspace/tensorflow/workspace.bzl\", line 422\r\n\t\tjava_import_external(name = 'junit')\r\n\tFile \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/external/io_bazel_rules_closure/closure/private/java_import_external.bzl\", line 29, in _java_import_external\r\n\t\trepository_ctx.name\r\nObject of type 'ctx' has no field \"name\".\r\nERROR: Analysis of target '//tensorflow/java:SavedModelBundleTest' failed; build aborted.", "The error seems unrelated to this change, but possibly related to other recent changes.\r\n\r\n@jart, mind looking at the java_import_external error?", "Ah, nevermind @jart, ignore the above. It looks like this change needs updating.", "Tip for future pull requests: it's a little easier if the `Allow edits from maintainers` option is left enabled, because there are often minor edits that we have to make before merging and we can often do them ourselves :)", "@jhseu \"allow edits\" was enabled on this PR but I'm happy with any approach that gets the patch in. :)\r\n\r\nClosing in favor of #8148\r\n\r\n", "Weird, I tried pushing to that branch and it got rejected. Maybe it's because it was in cookieai rather than your personal repo?"]}, {"number": 7687, "title": "tensorflow support sgd with monment Optimizer??", "body": "tensorflow support sgd with monment???\r\n\r\nis it tf.train.MomentumOptimizer", "comments": ["Yes, `tf.train.MomentumOptimizer` is the one you're looking for. Please ask on StackOverflow with the tag tensorflow in the future. GitHub is for bugs and feature requests. \ud83d\ude03 "]}, {"number": 7686, "title": "Looking for a function to replace sklearn.mixture.GaussianMixture.predict_proba(X)", "body": "In sklearn, I can use `predict_proba(X)` if I want to Predict posterior probability of data per each component.\r\n\r\nHowever, I cannot find a similar function in 'tensorflow gmm_ops'\r\n\r\nDid anyone find that before?\r\n", "comments": ["Tip: Try asking on StackOverflow with the tag tensorflow instead (more people answer there). GitHub is mostly for bugs."]}, {"number": 7685, "title": "Image problems in Tensorflow documentation at tensorflow.org, Error 404", "body": "I'm not sure I should put the issue here. But when I clicked \"Issue tracker\" on tensorflow.org, it redirects me here.\r\n\r\nI realized that the directory structure for TF documentation has changed a lot for TF 1.0 on http://www.tensorflow.org. Now there are problems for web pages with images, like https://www.tensorflow.org/api_docs/python/tf/segment_max?hl=bn. A 404 error is reported while loading the images.", "comments": ["that happens to me, too. have you solved the problem?", "@hustlrr I can't solve it. Hope the maintainers fix this issue soon.", "it's an issue with how we now have python docs in api_docs/python/tf/whatever but they all refer to \"../../images/\" -- the proper directory is tensorflow.org/images rather than tensorflow.org/api_docs/images.  Not sure whether to fix in docstring or site generation scripts but we'll get it done one way or the other."]}, {"number": 7684, "title": "embedding variable in ptb_word_lm.py", "body": "In ptb_word_lm.py I see that for word2vec vectors we are doing:\r\n\r\nembedding = tf.get_variable(\r\n          \"embedding\", [vocab_size, size], dtype=data_type())\r\n      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\r\n\r\nbut where is the variable embedding created? Is it random or is it pretrained?", "comments": ["Hi @sharod \r\n\r\ntf.get_variable() [creates](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L124) the 'embedding' variable and [initializes it](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L335) using a random initializer during training time as [var scope for training](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L335) is set to 'None'. \r\n\r\nSo, during training the word embedding is initialized and further trained since ['trainable' parameter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L203) is set to True. While during, validation or testing, the trained embeddings are retrieved as [variable reuse](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L342) is set to True.\r\n\r\nAlso, please use Stackoverflow for these kind of questions."]}, {"number": 7683, "title": "Package not Reslove.", "body": "Hello,\r\n\r\nI am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:\r\n\r\nimport org.tensorflow.DataType;\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Session;\r\nimport org.tensorflow.Tensor;\r\nimport org.tensorflow.TensorFlow;\r\n\r\nCan any one help to find out where some thing is missing.", "comments": ["Hi, how are you trying to build the demo? Are you getting this while following the instructions in the examples/android/README.md?", "@andrewharp Do you know how to resolve the issue, I have the same problem. I had followed the steps in the README.md", "@kinDSa @patoalejor Can you please explain the exact steps you're using to reproduce this? It sounds like it's not finding the java API classes under tensorflow/java for some reason.", "@andrewharp\r\nI followed step given in  README.md file of TensorFlow Android Camera Demo.First I clone Tensorflow demo, Then Import android project at path tensorflow/tensorflow/examples/android  in android studio. After importing I face above error. There are two class TensorFlowInferenceInterface and RunStats under tensorflow/java package.", "I have the same problem, I followed the README.md, and compiled andoird demo with bazel. When trying to use Android Studio, the compilation filed with the same error.", "Is there anyway we could get an Android Studio release for the Image Classifier? For us folks that aren't able to get a Bazel build going? I've tried for about a week now trying to get this build to work (and I really mean for a week, full days and nights of research and hackjobs). I've had terrible luck with Bazel, and finding a work around to Bazel. This might be due to me being relatively unacknowledged of app development (as my only use, as of now, is to get an app deployed for a web project that I'm working on). I've found one resource, of something somewhat similar (https://github.com/miyosuda/TensorFlowAndroidDemo), but it appears to be outdated, as the changes in files: TensorFlowClassifier.java (iirc, now: TensorFlowImageClassifier), and TensorFlowImageListener.java (which to my understanding has been renamed to ClassifierActivity). I think maybe releasing a depository for Android Studio, that we're able to drop in libtensorflow_demo.so, .jar (from nightly), and our model (of course with input values changed in ClassifierActivity.java (2/23/2017)) might be helpful. Kindest regards, @andrewharp, as you appear to be the only person with updated pertinent documentation.\r\n\r\nThis comment here, is probably the most useful thing I've read, but I'm not sure what to do with it:\r\n\r\n\r\n> We definitely could use some centralized documentation about what lib is what. Essentially the libs are consecutive supersets of each other in the following order:\r\n> \r\n> libandroid_tensorflow_lib.lo:\r\n> core TensorFlow runtime + ops for linking into other libraries (you would still need all the TF headers to really use it, but perhaps it can be useful in some situations)\r\n> \r\n> libtensorflow_inference.so:\r\n> core TF runtime+ops with added JNI bindings that allow you to use TF for inference from Java. This + the associated jarfile are all you should need to integrate TF into your own Android app, unless you want custom ops or training.\r\n> \r\n> libtensorflow_demo.so:\r\n> contains everything in libtensorflow_inference.so + demo-specific native code; if you want to run the official demo without building anything native this can be dropped into the APK\r\n> \r\n> edit: note that the libs currently found at nightly-android (build #11) have been unfortunately built without NEON support, but the next set to be uploaded should be fixed.\r\n>", "@shuangwu @kinDSa @patoalejor \r\nIt's not finding the Java files under [tensorflow/java](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java). Looks like this was missed on the recent conversion of TensorFlowInferenceInterface to use the Java API. If you add `\"../../java/src/main\"` to java.srcDirs in [build.gradle](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/build.gradle#L41) that should do the trick. I'll send a PR to fix this.\r\n\r\n@htyspghtz We're working on full Gradle/cmake/makefile support that will let Android Studio build on Windows. Should be out in the next week or two :)", "@htyspghtz Note that the comment you pasted is a little outdated: currently you need _both_ libtensorflow_inference.so and libtensorflow_demo.so as native libs for tensorflow_demo.apk. This is because the TF library and JNI bindings have been removed from libtensorflow_demo.so (it only contains object tracking+YUV->RGB conversion code now).", "@andrewharp thanks for the help, it's now complaining about package java.nio.file, another missing entry in java.srcDirs?", "@shuangwu java.nio.file was added in Java 7 -- it sounds like your JDK might be too old or you're forcing compatibility with older APIs.\r\n\r\nedit: Correction, Android is based on Java 6; this means java.nio.file will not be found regardless of the JDK installed. The problem was it was incorrectly trying to build non-Android example files as part of the Java API.", "@andrewharp I have java 1.8.0_121 installed, so definitely not old, somehow it's not find it?", "@shuangwu Ok, I'm seeing this too now actually. It's coming from the LabelImage.java example, which is not part of the Java API itself.\r\n\r\nIf you replace java.srcDirs with this I think it will work:\r\n```\r\n            java {\r\n                srcDir '../../java/src/main/java'\r\n                exclude '**/examples/**'\r\n            }\r\n            java {\r\n                srcDir '../../contrib/android/java'\r\n            }\r\n            java {\r\n                srcDir 'src'\r\n            }\r\n```", "@andrewharp it is working, thanks."]}, {"number": 7682, "title": "tensorflow support SGD with nesterov momentum in tensorflow??", "body": "tensorflow support SGD with nesterov momentum in tensorflow??", "comments": ["this seems like a good question for stackoverflow", "@yaroslavvb ,,in tensorflow ,sgd with mommentum optimizer is tf.train.MonmentOptimizer?"]}, {"number": 7681, "title": "DOCS for offsets in extract_glimpse don't match Implementation", "body": "Docs state:\r\n>    offsets: A `Tensor` of type `float32`.\r\n      A 2-D integer tensor of shape `[batch_size, 2]` containing\r\n      the x, y locations of the center of each window.\r\n\r\nwhereas [the implementation states](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/attention_ops.cc#L92):\r\n>      // calling TensorFlow operates with (y,x) as indices.\r\n\r\nNotice x,y vs y,x\r\n\r\nHere is a rough demo of the issue:\r\n```python\r\narr = np.zeros((1,5,5,1), dtype=np.float32)\r\narr[0, 3,2] = 1\r\narr = tf.constant(arr)\r\nglim = tf.image.extract_glimpse(arr, (1,1), offsets=tf.constant([(2,3)], dtype=tf.float32)[:,::-1] + 1, normalized=False, centered=False)\r\nsess.run(glim)\r\n```\r\n```\r\n> array([[[[ 1.]]]], dtype=float32)\r\n```\r\n\r\nIt appears that the docs are wrong and that the comment is right.", "comments": ["yes, this looks like a bug in the docs.  I'll submit a fix to the docs.", "Is that the right place to fix or would it be better to take x y which is more normal and would simplify op code ad well. ", "Also not sure if there are any additional docs fixes outlined in https://github.com/tensorflow/tensorflow/issues/2134", "In the immediate term, fixing the docs seems good to do since any changes in the behavior will break backwards compatibility and so requires more stages to accomplish.", "Since the expected shape of the input is [batch_size, height, width, channels], then I think y,x makes some sense since it's the same order as values in the input shape.", "https://github.com/tensorflow/tensorflow/commit/f1dd0e32522bd183d422bdac75137ebba2375327"]}, {"number": 7680, "title": "CUDA_ERROR_OUT_OF_MEMORY with tf.contrib.learn (basic linear regression model)", "body": "### Environment info\r\nOperating System: Ubuntu 16.04,  GeForce GTX TITAN X\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nls /usr/local/cuda-8.0/lib64/libcud*\r\n/usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\n### If installed from binary pip package, provide:\r\n\r\n1) Installed with pip tensorflow-gpu\r\n2) ~ python -c \"import tensorflow; print(tensorflow.__version__)\r\n1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nUsing the provided tf.contrib.learn model in get_started/get_started documentation\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfeatures = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\r\nestimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\r\nx = np.array([1., 2., 3., 4.])\r\ny = np.array([0., -1., -2., -3.])\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4, num_epochs=1000)\r\nestimator.fit(input_fn=input_fn, steps=10)\r\n\r\n```\r\n\r\nThe Core model works fine.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\npython test-oom.py \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpew3U6z\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /home/gajop/projekti/ubuntu-ranking-dataset-creator/env/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.52GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 11.92G (12799180800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n```", "comments": ["related https://github.com/tensorflow/tensorflow/issues/7677", "Hi, I have the same issue:\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GT 740M\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 1.0325\r\npciBusID 0000:09:00.0\r\nTotal memory: 1.96GiB\r\nFree memory: 1.90GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 740M, pci bus id: 0000:09:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.96G (2100953088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n```", "Apparently there is something consuming your GPU memory (`Total memory: 11.92GiB\r\nFree memory: 11.52GiB`) making unavailable the amount TensorFlow is trying to allocate, \"`failed to allocate 11.92G`\". \r\n\r\nYou can set how much memory TensorFlow will have available for use with [tf.GPUOptions](https://www.tensorflow.org/api_docs/python/tf/GPUOptions)\r\n```\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  #define amount of GPU memory\r\n\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```\r\nFor instance if you have 12GB, ~4GB will be allocated for TensorFlow.\r\nYour other option is to use a NVidia SMI to see where your memory is going. ", "I'll try to explicitly limit memory usage when I'm back at the computer, but it still seems that allocating all the GPU memory is clearly a bug.\nAm I interpreting it wrong? ", "Today's a Google corporate holiday, so followup may be slow, but this looks like it could be a recently introduced bug.  By default, TF should be trying to allocate all of the free memory on the GPU, not the total memory.  Your error message indicates it's trying to allocate total memory.  As @Carmezim notes, there is a work-around.  ", "In **tensorflow\\contrib\\learn\\python\\learn\\estimators\\run_config.py** you can change the GPUOption.\r\nI changed in `def __init__`  **per_process_gpu_memory_fraction** to **0.7** .\r\n\r\n", "I have the same issue when trying to run the basic usage program from https://www.tensorflow.org/get_started/get_started using the version 1.0.1. TensorFlow tries to allocate the total memory:\r\n\r\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:` \r\n`name: GeForce GTX 960M`\r\n`major: 5 minor: 0 memoryClockRate (GHz) 1.176`\r\n`pciBusID 0000:01:00.0`\r\n`Total memory: 3.95GiB`\r\n`Free memory: 3.57GiB`\r\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0` \r\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y` \r\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)`\r\n`E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 3.95G (4240965632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY`\r\n\r\nAny news on fixing this?\r\n\r\nBy the way, the program also generates some warnings:\r\n\r\n`WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp8_ec6rtc`\r\n`WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.`\r\n`WARNING:tensorflow:From /home/trd/apps/miniconda/envs/mt/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.`\r\n`Instructions for updating:`\r\n`Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.`\r\n\r\nThe one from head.py about deprecation and removal after 2016-11-30 looks extremely weird to me.", "@trd714 Could you try out the suggestion by @hajome73, the one right above your comment?", "@gunan Yes, it works.\r\nBut I like the solution by @Carmezim (https://github.com/tensorflow/tensorflow/issues/7680#issuecomment-281084099) much more as it doesn't involve hard coding anything in the TensorFlow package.", "The solution by @Carmezim suppresses error messages, does not fix anything.\r\nIt is OK to use it in the warning messages described in that issue.\r\n\r\nIn your case,, you are seeing an actual error message.\r\nYou will get bad results so just using that solution over there will not help you for fixing the training process.", "@gunan I see, thank you.\r\n\r\nBut anyway the solution by @hajome73 doesn't seem to be ideal.\r\nI took a closer look at the output after applying the solution and it looks suspicious to me. See this gist: https://gist.github.com/trd714/551cd37e00a57a83ae40a36f195b470b.\r\nFirstly, I see two lines that say \"Creating TensorFlow device\" (27 and 32).\r\nSecondly, I get \"Out of range\" at line 33. After reading the line number 34 I assume that it's somehow related to the CPU as it states _device=\"/job:localhost/replica:0/task:0/cpu:0\". Are lines 33 and 34 an issue at all? If yes, are they related to the initial issue or should I create a new one?\r\n\r\nP.S. I had to wrap the last statement at https://www.tensorflow.org/get_started/get_started#basic_usage (estimator.evaluate(input_fn=input_fn)) in a print() function to get the output. \r\n", "In tf logging library, the error message lines that start with `I` means information, this line is just for information, nothing bad is happening there. `W` means warning, something unexpected happened but The overall process will still run, it may be nothing. These messages you should be aware of, but most of the time can be ignored. All your current logs are information or Warning, so it all looks OK to me.\r\n\r\n`E` means Error, which means something bad happened. Something that has to be fixed.\r\n`CUDA_ERROR_OUT_OF_MEMORY` was an error, so it had to be fixed.\r\nCurrent messages are all benign.", "@gunan Thank you for the information.\r\nBy the way, today I've updated to 1.1.0-rc1 and the example works out of the box.\r\n\r\n@gajop Do you experience the issue on 1.1.0-rc1?", "I had the same issue with TensorFlow 1.0.1 and boston.py from tutorials. Works now fine without errors with update 1.1.0 .\r\n", "I'm new to TF and I'd the same issue following the getting started  tf.contrib.learn tutorial. I'm using TF 1.0.0-rc2 with gpu support. On various forums people have pointed out to limit the GPU fraction memory, it worked; though it also worked only by starting a session without any GPU settings.\r\n```\r\nsess = tf.Session() \r\n```\r\nThis solves the issue and does not have to specifically provide the GPU memory fraction. I have tried with following:\r\n```\r\n#config = tf.ConfigProto()\r\n#config.gpu_options.per_process_gpu_memory_fraction = 0.3\r\n#config.allow_soft_placement = True\r\n#config.log_device_placement = True\r\n#sess = tf.Session(config=config)\r\n\r\n# Added this line and error was fixed\r\nsess = tf.Session()\r\n\r\n# Declare list of features. We only have one real-valued feature. There are many\r\n# other types of columns that are more complicated and useful.\r\nfeatures = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\r\n... \r\n```\r\nFrom the documentation on [GPU usage](https://www.tensorflow.org/tutorials/using_gpu):\r\n> By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process\r\n\r\nI can only speculate that starting a session limits the GPU memory allocation, though I have no idea why starting a Session solved the GPU memory issue. It would be great if someone can kindly explain.", "I had the same issue. Here is my solution.\r\n\r\nAdd the `config`  option when you define your estimator, for exzample: \r\n\r\n```python\r\nconfig = tf.contrib.learn.RunConfig(gpu_memory_fraction=0.3)\r\nestimator = tf.contrib.learn.LinearRegressor(feature_columns=features, config=config)\r\n```\r\n\r\nBut I still want to know how I can only use CPU. ", "Before running TF, if you set `export CUDA_VISIBLE_DEVISES=\"\"` That should make all of your GPUs invisible to TF. One you start, it should run things on your CPU.\r\nCould you try it out?", "@gunan I'm using MINGW32 bash on Windows and that didn't work for me. TensorFlow still use GPU to compute.", "MINGW32 on windows is not a tested/supported platform.\r\nTherefore, I cannot really comment on how that might work, or if even GPU access correctly works on your setup.\r\n\r\nI recommend heading to stackoverflow to reach out to community for help on your issue.\r\nAs the initial issue on ubuntu seems to be resolved/addressed, I will close this issue."]}, {"number": 7679, "title": "How to reproduce the 58x performance improving claimed by TF r1.0?", "body": "TF r1.0 claims that it can achieve 58x performance improving by 64 GPUs for Inception v3. Are there any guidelines or sample code to help us to reproduce the results?", "comments": ["I believe @tfboyd is working on reproduction instructions for AWS", "@yaroslavvb Are we talking about a repro on on EC2 P2 instances with K80s? The Google example mentioned during the TF Dev Summit was on K80s, albeit possibly via Google's internal RPC, rather than grpc.", "The example at the TF Dev Summit was gRPC on AWS p2.8xLarge systems.  We did that on purpose to ensure it was \"commodity\" hardware everyone can use.  Most of use took time off after the summit.  While not something I should promise, I will commit to updating this thread once a week until we have all of the code published.  The updates might be boring.  Feel free to ping me if I forget.  ", "I have never had to track something publicly.  I am sure I am doing it incorrectly but spending time trying to figure out the PR spin is time that could be spent on the work.  I am going to toss out a date of early April as when the code will be released.  The team has **not** committed to that date, but without a general date range it is impossible for me to event attempt set expectations with anyone reading this issue.  \r\n\r\nAs of today, we are on track for early April.  I have likely broken a million best practices with this comment.  I'll update again next Monday.  \r\n\r\n\r\n\r\n\r\n", "Late on the weekly update.  Not much to add beyond we are currently on schedule for early April.", "Does anyone make sure that the 7.3x & 58x performance data claimed on TF Dev Summit are based on AWS with K80? Google has servers with 8GPUs connected by PCIE, so for the 8 GPU get 7.3x performance improving, it is one node with 8 GPUs, or 8 GPUs with multiple nodes? And what batchsize do they use?\r\n\r\n ", "@tfboyd any update on the progress? Thx!", "I have been negligent in updating.  Testing, documentation, and code clean up is going well.  We are putting in a strong effort to release in early April.  I believe people internally think I have lost my mind in attempting to manage expectations externally.  It may be my cold medication speaking, but I want to be clear what I care about most is getting this code out so the users of TensorFlow can train their models faster and be more efficiently.  I am working get to some documentation created as well, as having example code is not always useful without some explanation.   To answer @AlvinChen13 's question.  For the K80 the numbers I used an AWS p2.8xlarge and recently on GCE with 8 K80s GPUs.  Both setups are one server with 8 GPUs.  Keep in mind these numbers may still be improving and this is not an official post, this is very informal although public.  \r\n\r\n- Inceptionv3 K80s had a speed up of 7.5x using synthetic data and 7.34x using real data from 1 GPU to 8 GPUs\r\n- InceptionV3 P100s has a speed up of 7.4x (7.37) using synthetic data and 7.2x using real data from 1 GPU to 8 GPUs.\r\n\r\nI used a batch-size of 32 per GPU to match some of the other bechmarks.\r\n\r\nThe 58x from 1 GPU to 64 GPUs was 8 server each with 8 K80 GPUs using AWS p2.8xLarge instances with the standard Enhanced Networking.  Again 32 was the batch-size per GPU.  We used Ubuntu 16.04 as well as Amazon Linux to see if there was a difference.  There is no real difference as long as you install the Enhanced Networking on Ubuntu 16.04.  I wrote up some [installation instructions](https://github.com/tfboyd/tf-tools/tree/master/install) for both operating systems and a poorly written [blog post](http://mediocrethoughts.ghost.io/2017/03/04/installing-tensorflow-on-aws/) on my single post lame blog.  :-)\r\n\r\nI also ran ResNet-50, ResNet-152, AlexNet (batch 128 and 512) and we will be running VGG-16.  I believe that covers the most popular image-classification models, and the techniques should apply to other models.   ", "@tfboyd Great! Thanks for informing the progress. Looking forward to testing it in my training case.", " I have a question that makes this more of a forum than a GitHub issue but if all of you play along who is going to stop us.  :-) \r\n\r\n**Question:**  Image-classification models are interesting and people like to benchmark them.  Where would you like to see the performance team focus after these common models?  Sure many of the techniques will apply to all types of models but I suspect not all.  Thoughts? \r\n", "Hi @tfboyd, I'm not sure I have completely understand your question. But for me, I would like to know the computation time in forward, backward, forward&backward, under different distributed training settings of workers/ps.", "Hi @tfboyd,  I often see that image classification models use synchronous data parallelism that at the end of the day has the same effect of using large batch sizes.  I guess a comparison between asynchronous and synchronous updates in terms of final obtainable accuracy and time saved in synchronization could be interesting. ", "Thanks @pedropgusmao, that is interesting to us as well.  \r\n\r\nQuick update.  We are moving into the documentation phase.  We are a little behind my made up goal of early April but progress is good.  I expect the code will be released in a few weeks.  Things rarely go as planned.  It is on me for not pushing harder in late March.  ", "@tfboyd any new updates? Thanks!", "80% likely next week.  We are branching the benchmark code today and starting the final round of \"runs\".  The end is near.  ", "Great lesson learned that I already knew.  Managing expectations is not fun.\r\n\r\n**Update:**  The draft documents are complete and working through the tech writer process.  While over sharing, I hope to have everything staged late this week and push the \"publish\" button on Tuesday.  Personally, I am excited to focus on other areas of performance and when TF 1.1 comes out (it is either out now or very near) I highly suggest moving to it or build from head.  There were some great changes pushed into core as a result of our performance benchmark effort.  I hope to write something small up about that early next week as well.  \r\n\r\nThank you for all the feedback, the end is finally near.  ", "The [benchmarks](https://www.tensorflow.org/performance/benchmarks) have been published along with the code.  Because all of you can do math, I want to address something in this thread.  InceptionV3 with batch size 32 no longer scales at x58.  What happened is that we got faster across the board but our gains for 1 GPU were higher than at 64 GPUs and that lowered the scaling factor a little.  Our images/sec, which we did not share, at the conference was ~1,496 and our current number is 1608 images/sec for batch size 32 per GPU.  \r\n\r\nI apologize for the delay and look forward to focusing on time to accuracy, async training with accuracy, and other interesting problems.  If I missed something let me know.  I personally ran all of the tests and made every attempt to not grab the \"best number\".  I run each test 5 times and averaged the results.  I suspect I have run 5-10K tests in the past few months (I should sit down and make a more accurate guess) and spent some absurd amount of money.  Thank you for being patient.  In the future, I will do what I can to avoid these types of delays.  I am closing this issue but you can still comment, I think?\r\n\r\nI also learned a lot about scaling on Google Cloud and AWS.  If anyone wants to share amusing stories or has questions let me know.  I documented my environment, which I doubt was ideal but they mostly worked well for what I was doing. \r\n\r\n**Final item**.  While, I say in the document that you need to use code post TF 1.1.  If you want to run the scripts with TF 1.1, you will get slightly worse results, but you can do it by commending out this line: `  config.gpu_options.force_gpu_compatible = FLAGS.force_gpu_compatible`.  And if you just want to jump to the [scripts](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)        \r\n\r\nEDIT:  slightly wrong link to master vs. root.  fixed\r\n\r\n"]}, {"number": 7678, "title": "Error message improvement", "body": "See issue https://github.com/tensorflow/tensorflow/issues/7675", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 7677, "title": "\"Getting started\" first tf.contrib.learn sample fails", "body": "The first tf.contrib.learn sample at\r\nhttps://www.tensorflow.org/get_started/get_started\r\nfails to run.\r\n\r\nThe  code is right under\r\n\"Basic usage\r\nNotice how much simpler the linear regression program becomes with tf.contrib.learn:\"\r\n\r\nOutput:\r\n\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\John\\AppData\\Local\\Temp\\tmpahtnt89p\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From C:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\n2017-02-19 11:53:57.997760: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-19 11:53:58.007657: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-19 11:53:58.008177: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-19 11:53:58.008726: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-19 11:53:58.009199: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-19 11:53:58.009591: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-19 11:53:58.010795: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-19 11:53:58.011384: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From C:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n\r\n\r\n------------------\r\n(program exited with code: 0)\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: Windows 10\r\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\r\n\r\nInstalled version of CUDA and cuDNN: \r\n???\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl (to fix the BestSplits\" error)\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.0.0-rc2\r\n\r\n\r\n", "comments": ["I get very similar behavior under ubuntu 16.10 with v1.0.0.\r\nAs a result, many of the tutorials fail - I don't think I've gotten any that rely on contrib.learn to work.\r\n", "Are you sure it actually failed? Those all look like warnings.", "I just redid this experiment.  I copied the code from the page mentioned above, i.e.\r\n\r\n> cat line2.py\r\n\r\n```\r\nimport tensorflow as tf\r\n# NumPy is often used to load, manipulate and preprocess data.\r\nimport numpy as np\r\n\r\n# Declare list of features. We only have one real-valued feature. There are many\r\n# other types of columns that are more complicated and useful.\r\nfeatures = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\r\n\r\n# An estimator is the front end to invoke training (fitting) and evaluation\r\n# (inference). There are many predefined types like linear regression,\r\n# logistic regression, linear classification, logistic classification, and\r\n# many neural network classifiers and regressors. The following code\r\n# provides an estimator that does linear regression.\r\nestimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\r\n\r\n# TensorFlow provides many helper methods to read and set up data sets.\r\n# Here we use `numpy_input_fn`. We have to tell the function how many batches\r\n# of data (num_epochs) we want and how big each batch should be.\r\nx = np.array([1., 2., 3., 4.])\r\ny = np.array([0., -1., -2., -3.])\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4,\r\n                                              num_epochs=1000)\r\n\r\n# We can invoke 1000 training steps by invoking the `fit` method and passing the\r\n# training data set.\r\nestimator.fit(input_fn=input_fn, steps=1000)\r\n\r\n# Here we evaluate how well our model did. In a real example, we would want\r\n# to use a separate validation and testing data set to avoid overfitting.\r\nestimator.evaluate(input_fn=input_fn)\r\n```\r\n\r\nand ran it using:\r\n\r\n> python line2.py\r\n\r\nand got warning like above, i.e.\r\n\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpnvvddpq0\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /home/estes/.virtualenv/tf/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /home/estes/.virtualenv/tf/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n\r\nOn the getting started page it says:\r\n\r\nWhen run, it produces\r\n\r\n    {'global_step': 1000, 'loss': 1.9650059e-11}\r\n\r\nThat output doesn't exist anywhere within the output generated.  Thus, I assume it failed.\r\nI just started learning TF today, so could be missing something.\r\n\r\nI used the pip-based install for a non-GPU config with python3; in a virtual environment which has been 'activated'.  There were no complexities during the install process as described on the getting started page.\r\n\r\n", "It's a warning.\r\nThere's a typo in the sample code.\r\n\r\noriginal\r\nestimator.evaluate(input_fn=input_fn)\r\n\r\nfix:\r\nprint(estimator.evaluate(input_fn=input_fn))", "Any idea where we can find this documentation md in the repo to fix it? I cannot match the structure of the tensorflow.org website and the g3doc directory here.", "I looked around for the markdown to update to fix this as well, but it doesn't appear to be on GitHub.", "I get the same problem under  win10", "get the same problem under win 10; but if executing line by line under interactive python, can get the result with the final line : \r\n\r\n.......\r\nINFO:tensorflow:Starting evaluation at 2017-02-28-21:32:38\r\nINFO:tensorflow:Restoring parameters from C:\\Users\\geldqb\\AppData\\Local\\Temp\\tmp772n4s7z\\model.ckpt-1000\r\nINFO:tensorflow:Finished evaluation at 2017-02-28-21:32:39\r\nINFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 4.51237e-09\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n{'loss': 4.5123736e-09, 'global_step': 1000}", "@skimmy12 Please fix this typo in the tutorial, and also the \"warnings\" are so confusing to newbies like me!  I couldn't even figure out the relationship between the warning and the actual fix! :) Someone to fix that as well? Thanks", "You can find the document right here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/get_started.md", "Warning can be disabled with this:\r\n\r\ntf.logging.set_verbosity(tf.logging.ERROR)", "I got an error when running this example code, it said CUDA_ERROR_OUT_OF_MEMORY\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpglsJ8i\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7085\r\npciBusID 0000:03:00.0\r\nTotal memory: 5.92GiB\r\nFree memory: 5.67GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:03:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 5.92G (6357843968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:03:00.0)\r\nW tensorflow/core/framework/op_kernel.cc:993] Out of range: RandomShuffleQueue '_1_enqueue_input/random_shuffle_queue' is closed and has insufficient elements (requested 4, current size 0)\r\n\t [[Node: random_shuffle_queue_DequeueUpTo = QueueDequeueUpToV2[component_types=[DT_INT64, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](enqueue_input/random_shuffle_queue, random_shuffle_queue_DequeueUpTo/n)]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Out of range: RandomShuffleQueue '_1_enqueue_input/random_shuffle_queue' is closed and has insufficient elements (requested 4, current size 0)\r\n\t [[Node: random_shuffle_queue_DequeueUpTo = QueueDequeueUpToV2[component_types=[DT_INT64, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](enqueue_input/random_shuffle_queue, random_shuffle_queue_DequeueUpTo/n)]]\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n{'loss': 4.6558966e-07, 'global_step': 1000}", "@QQRider Your error message is different.\r\nYours is actually a failure due to your code.\r\n`CUDA_ERROR_OUT_OF_MEMORY` means, well, you are out of GPU memory, as it suggests.\r\nYou are allocating 5.92 GBs of GPU memory, and it fails as there is not enough memory on your GPU.\r\nPlease reach out to stackoverflow if you need more help with that.", "@gunan \r\n@QQRider has the same issue as I described at https://github.com/tensorflow/tensorflow/issues/7680#issuecomment-290251024. TensorFlow tries to allocate the total GPU memory, not the free one.", "This is just a different issue. I want to keep the conversation about the initial issue, which is the CPU instruction set warnings. No errors are reported on the initial message.\r\n\r\nLooks like initial problem reported by @avantol already has some suggestions to resolve. I will close this issue.", "@QQRider just add this line to the example early at the top:\r\nconfig = tf.contrib.learn.RunConfig(gpu_memory_fraction=0.05)\r\nestimator = tf.contrib.learn.LinearRegressor(feature_columns=features, config=config)\r\nThe percentage depends on your card memory... my card had only a small fraction free so I used this small number and the example worked fine (also added the print statement as it was suggested).\r\n@gunan For new users of the framework I suggest strongly to update the docs (if the info is missing) for both issues.", "I have the same issue on OSX. Warnings I get:\r\n\r\n_WARNING:tensorflow:Using temporary folder as model directory: /var/folders/y6/h36z2y61059c6_9bqpb2f_9r0000gn/T/tmpfmi8rrr8\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /Users/siavash/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /Users/siavash/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32._", "Hi @avantol @headdab @aselle @skimmy12 @microtony \r\nI am trying to test the SketchModelling code,but stuck with error. I have tried adding\r\nsess.run(tf.local_variables_initializer())\r\nsess.run(tf.global_variables_initializer()) but did not solve the error. Please suggest\r\n\r\n(SKetchModelling) C:\\GenAssi\\SketchModeling\\Network\\code\\MonsterNet>python main.py --test --data_dir C:\\GenAssi\\SketchModeling\\zhaoliang_lun_trainingdata\\Chair\\ --train_dir C:\\GenAssi\\SketchModeling\\Checkpoint\\Chair\\ --test_dir C:\\GenAssi\\SketchModeling\\chair_results\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nstart running...\r\nLoading testing data...\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nBuilding network...\r\nTesting...\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Not found: Can not get size for: C:\\GenAssi\\SketchModeling\\zhaoliang_lun_trainingdata\\Chair\\sketch/03001627/264322794651490ec0d3c02f7e255b2b/sketch-S-0.png : The system cannot find the file specified.\r\n\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Not found: Can not get size for: C:\\GenAssi\\SketchModeling\\zhaoliang_lun_trainingdata\\Chair\\sketch/03001627/264322794651490ec0d3c02f7e255b2b/sketch-F-0.png : The system cannot find the file specified.\r\n\r\nTraceback (most recent call last):\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\nreturn fn(*args)\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\r\nstatus, run_metadata)\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\contextlib.py\", line 66, in exit\r\nnext(self.gen)\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\npywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_1_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n[[Node: batch = QueueDequeueManyV2[component_types=[DT_STRING, DT_FLOAT, DT_FLOAT, DT_BOOL, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"main.py\", line 153, in\r\ntf.app.run()\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\nFile \"main.py\", line 142, in main\r\nmonnet.test(sess, views, num_test_shapes)\r\nFile \"C:\\GenAssi\\SketchModeling\\Network\\code\\MonsterNet\\monnet.py\", line 471, in test\r\nnames,results,errors,images = sess.run([self.names, self.results, self.errors, self.pngs])\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\nrun_metadata_ptr)\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\nfeed_dict_string, options, run_metadata)\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\ntarget_list, options, run_metadata)\r\nFile \"C:\\Users\\dhornala.bharadwaj\\AppData\\Local\\Continuum\\anaconda3\\envs\\SKetchModelling\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\nraise type(e)(node_def, op, message)\r\n**tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_1_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n[[Node: batch = QueueDequeueManyV2[component_types=[DT_STRING, DT_FLOAT, DT_FLOAT, DT_BOOL, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]**"]}, {"number": 7676, "title": "Add .shape property to Variable object", "body": "also adjust docstrings for consistency with Tensor.shape/Tensor.get_shape ", "comments": ["Can one of the admins verify this patch?", "If this acceptable, I'd suggest adding a test of the new property, at the very least.", "I'll add tests if the API change is accepted. The motivation for change is that Variables are sometimes used as drop-in replacement for Tensor objects and we had users confused to why \"a.shape\" fails for some a", "Jenkins, test this please."]}, {"number": 7675, "title": "Error message could be improved", "body": "If your try and initialize a `dynamic_rnn` without specifying an `initial_state` **OR** `dtype`, you get this enigmatic error message:\r\n\r\n`If no initial_state is provided, dtype must be`\r\n\r\nI don't think that message is very clear. Something better would be along the lines of, \r\n\r\n`If no initial_state is provided, a dtype must be specified`\r\n\r\nThis error is thrown at \r\ntensorflow/tensorflow/python/ops/rnn.py, line 518", "comments": ["@rpyzant, would you be willing to submit a quick pull request for this?\r\n", "Done, see PR https://github.com/tensorflow/tensorflow/pull/7678", "Thanks so much!"]}, {"number": 7674, "title": "Fix bug #7647 in image retraining example retrain.py so that the --pr\u2026", "body": "Fix bug #7647 in image retraining example retrain.py so that the --print_misclassified_test_images flag now works properly in Python 3.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7673, "title": " Make learn/mnist.py work again", "body": "`contrib.layers.convolution` doesn't exist, so it looks like `convolution2d` has to be used instead.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins Test this please"]}, {"number": 7672, "title": "Don't try to read CUDA driver version from /proc on Windows", "body": "Fix for #7599", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 7671, "title": "make training compatible with python3", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7670, "title": "Fix tfdbg in windows cmake build", "body": "    Add tfdbg to cmake build\r\n    \r\n    Guard grpc:// debug URL scheme implementation out from open-source.\r\n    The service side of the grpc:// debugging is not available in\r\n    open-source anyway, due to the lack of an open-source Python gRPC\r\n    iibrary genrule. See b/23796275.\r\n    \r\n    Added C++ core/debug library, Python modules and unit tests of tfdbg.\r\n    chaser", "comments": ["Fixes issue https://github.com/tensorflow/tensorflow/issues/7615", "Tested with experimental builds (Jenkins login required to view logs):\r\nWindows cmake CPU build:\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-windows-cmake/16/\r\n\r\nWindows cmake GPU build:\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-windows-cmake-gpu/6/", "cc @gunan @yifeif ", "OK. I'll close this PR and do it from internal."]}, {"number": 7669, "title": "The tutorial \"Logging and Monitoring Basics with tf.contrib.learn\" has error.", "body": "When I used the code snippet in the section \"Customizing the Evaluation Metrics with MetricSpec\" of the tutorial [Logging and Monitoring Basics with tf.contrib.learn](https://www.tensorflow.org/get_started/monitors). the code snippet is \r\n\r\n```python\r\nvalidation_metrics = {\r\n    \"accuracy\":\r\n        tf.contrib.learn.metric_spec.MetricSpec(\r\n            metric_fn=tf.contrib.metrics.streaming_accuracy,\r\n            prediction_key=tf.contrib.learn.prediction_key.PredictionKey.\r\n            CLASSES),\r\n    \"precision\":\r\n        tf.contrib.learn.metric_spec.MetricSpec(\r\n            metric_fn=tf.contrib.metrics.streaming_precision,\r\n            prediction_key=tf.contrib.learn.prediction_key.PredictionKey.\r\n            CLASSES),\r\n    \"recall\":\r\n        tf.contrib.learn.metric_spec.MetricSpec(\r\n            metric_fn=tf.contrib.metrics.streaming_recall,\r\n            prediction_key=tf.contrib.learn.prediction_key.PredictionKey.\r\n            CLASSES)\r\n}\r\n```\r\n\r\nMy tensorflow version is r1.0 . When I run my program, it print the following error:\r\n\r\n```shell\r\n$ python iris.py \r\nTraceback (most recent call last):\r\n  File \"iris.py\", line 72, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"iris.py\", line 24, in main\r\n    \"accuracy\": tf.contrib.learn.metric_spec.MetricSpec(\r\nAttributeError: 'module' object has no attribute 'metric_spec'\r\n```\r\n\r\nI found that the class `tf.contrib.learn.metric_spec.MetricSpec` has been renamed to [`tf.contrib.learn.MetricSpec`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/MetricSpec). \r\n\r\nThe class `tf.contrib.learn.prediction_key.PredictionKey` also has been renamed to [`tf.contrib.learn.PredictionKey`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/PredictionKey).", "comments": ["@lienhua34 yes it's correct. The interface has been sealed recently. Welcome to submit a pull request! @martinwicke Does the team have any plan to rewrite Monitor tutorial by Hooks?", "Sanders, didn't you already do that?", "No, I did update this tutorial back in December, but haven't yet switched to use `SessionRunHook`, as I was waiting on an equivalent canned hook for `ValidationMonitor`. That's not yet available, correct? \r\n\r\nIn the meantime, for an example of applying a `SessionRunHook` to an `Estimator`, you can refer to the tf.layers tutorial (https://www.tensorflow.org/tutorials/layers), which covers how to configure a `LoggingTensorHook`.", "@ispirmustafa: FYI regarding our discussion today about\n\"SingleWorkerEvaluationHook\"\n", "I'm also following this tutorial and having problems with it. I'm using the latest 1.0.1 release.\r\n- The tutorial at https://www.tensorflow.org/get_started/monitors has outdated code snippet which generates errors.\r\n- The tutorial code at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/monitors/iris_monitors.py does not generates errors and generates some checkpoint files in the log_dir. But Tensorboard doesn't display any data (screenshot attached)\r\n![screenshot from 2017-04-12 15-44-57](https://cloud.githubusercontent.com/assets/8304443/24982605/0483d97c-1f97-11e7-8b49-08f365c3c8e9.png)\r\n\r\nIs there any working example for these monitors `CaptureVariable`, `PrintTensor`, `ValidationMonitor`?", "I have the same problem,and it does not work after changing tf.contrib.learn.metric_spec.MetricSpec/tf.contrib.learn.prediction_key.PredictionKey  to tf.contrib.learn.MetricSpec/tf.contrib.learn.PredictionKey.Any one could help?", "@terrytangyuan @sandersk \r\nAre these monitors CaptureVariable, PrintTensor, ValidationMonitor already deprecated by LoggingTensorHook?", "Yes. All Monitors are deprecated. Not all of them have a direct equivalent, but there should be hooks for the main use cases. Except ValidationMonitor, as of today.", "I wanted to learn, TF and started off with tf.contrib.learn from https://www.tensorflow.org/get_started/tflearn.  However I got stuck in the same problem with ValidationMonitors. I understand that they are depreciated now. I don't have the head to go through the new \"hooks\" tutorial. for visualizing through tensor board yet. Is there a simple tutorial using iris dataset as a continuation from ~/get_started/fflearn ?", "when I run the \" iris_monitors.py\"\r\nerrors as follows:\r\n\r\n    tf.contrib.learn.prediction_key.PredictionKey.CLASSES),\r\nAttributeError: module 'tensorflow.contrib.learn' has no attribute 'prediction_key'", "@ispirmustafa FYI\r\n\r\nWe should be fixing this as part of our tutorials rewrite for core estimators. ", "is there any update regarding ValidationMonitor as hook? The documentation seems to not be updated", "I am in the same boat as \"agniszczotka\". \r\nI have successfully used a SummarySaverHook to write some stats to file and display them on tensorboard, but i am wondering how i can evaluate the accuracy improvement through training. Should i put an estimator.evaluate with different \"step\" parameters to evaluate the accuracy in different moments/checkpoints?\r\nIn specific, i am trying to replicate this: https://www.tensorflow.org/versions/r1.3/get_started/monitors#evaluating_every_n_steps\r\n", "AxenGitHub I managed to run validation through training by using experiment\r\nsee a doc here: https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment\r\n\r\n ```\r\n\r\n experiment = Experiment(estimator=estimator, train_input_fn=training_input_fn,\r\n                                eval_input_fn=eval_input_fn, eval_steps=None, min_eval_frequency=1)\r\nexperiment.train_and_evaluate()\r\n\r\n```\r\n\r\nI am not sure how effective is it yet, but it did a job. \r\ncould you please share your solution for implementing validation monitor with the hooks. I made a question at stackoverflow https://stackoverflow.com/questions/45417502/validation-during-training-of-estimator?noredirect=1#comment77798445_45417502", "@agniszczotka Thanks for your help. When I implement your suggestion, I get the following error:\r\n`File \".../anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 253, in train\r\n    if (config.environment != run_config.Environment.LOCAL and`\r\n`AttributeError: 'RunConfig' object has no attribute 'environment'`\r\nAny idea on how to get around it?", "@maximedb I fixed that in https://github.com/tensorflow/tensorflow/pull/11385. It would be great if someone could take a look at that one.", "It was resolved by adding the following lines (see [here](https://stackoverflow.com/questions/45952149/tensorflow-estimator-periodic-evaluation-on-eval-dataset/))\r\n```\r\nos.environ['TF_CONFIG'] = json.dumps({'environment': 'local'})\r\nconfig = tf.contrib.learn.RunConfig()\r\nestimator = tf.estimator.Estimator(..., config = config)\r\n```\r\n", "How can I use early_stopping in environment? ", "@Moymix you can implement early stopping by using the `continuous_eval_predicate_fn`, available in [tf.contrib.learn.Experiment.continuous_eval_on_train_data](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment). For instance, let's take a batch size of 10 and early stop count of 15.  Modifying the example at [TF Layers tutorial](https://www.tensorflow.org/tutorials/layers) for a bigger dataset, the code would look like this:\r\n\r\n```python\r\nBATCH_SIZE  = 10\r\nEARLY_STOP_COUNT = 15\r\n\r\n# Model function\r\ndef model_fn(features, labels, mode):\r\n  # ...\r\n  eval_metric_ops = { \"accuracy\"  : accuracy}\r\n  return tf.estimator.EstimatorSpec(\r\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n\r\n# Early stopping function\r\naccuracy_reg = np.zeros(EARLY_STOP_COUNT)\r\ndef early_stopping(eval_results):\r\n  # None argument for the first evaluation\r\n  if not eval_results: \r\n    return True\r\n  \r\n  accuracy_reg[0 : EARLY_STOP_COUNT - 1] = accuracy_reg[1 : EARLY_STOP_COUNT]\r\n  accuracy_reg[EARLY_STOP_COUNT - 1] = eval_results[\"accuracy\"]\r\n  counts = 0\r\n  for i in range(0, EARLY_STOP_COUNT - 1):\r\n    if accuracy_reg[i + 1] <= accuracy_reg[i]:\r\n      counts += 1\r\n  if counts == EARLY_STOP_COUNT - 1:\r\n    print(\"\\nEarly stopping: %s \\n\" % accuracy_reg)\r\n    return False\r\n    \r\n  return True\r\n\r\n# Main function\r\ndef main(unused_argv):\r\n  #...\r\n  estimator = tf.estimator.Estimator(\r\n  #...\r\n  # Train the model \r\n  train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"data\": train_data},\r\n    y=train_labels,\r\n    batch_size=BATCH_SIZE,\r\n    num_epochs=None, # Continue until training steps are finished\r\n    shuffle=True\r\n    )\r\n  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"data\": validate_data},\r\n    y=validate_labels,\r\n    batch_size=BATCH_SIZE,\r\n    num_epochs=1, \r\n    shuffle=False\r\n    )\r\n  experiment = tf.contrib.learn.Experiment(\r\n    estimator=estimator,\r\n    train_input_fn=train_input_fn,\r\n    eval_input_fn=eval_input_fn,\r\n    train_steps=80000,\r\n    eval_steps=None, # evaluate runs until input is exhausted\r\n    eval_delay_secs=180, \r\n    train_steps_per_iteration=1000\r\n    )\r\n  experiment.continuous_train_and_eval(\r\n    continuous_eval_predicate_fn=early_stopping)  \r\n  \r\n  # ...\r\n```\r\nHowever, have in mind that `continuous_eval_predicate_fn` is an experimental function, so it could change at any moment.", "@xiejw could you PTAL re: new 1.4 utilities.", "Take a look at this example:\r\nhttps://stackoverflow.com/questions/46326848/early-stopping-with-experiment-tensorflow\r\n```\r\ndef experiment_fn(run_config):\r\n    estimator = tf.estimator.Estimator(...)\r\n\r\n    train_monitors = tf.contrib.learn.monitors.ValidationMonitor(\r\n            early_stopping_metric = \"loss\",\r\n    )\r\n\r\n    return learn.Experiment(\r\n        estimator = estimator,\r\n        train_input_fn = train_input_fn,\r\n        eval_input_fn = eval_input_fn,\r\n        train_monitors = [train_monitors])\r\n\r\nex = learn_runner.run(\r\n        experiment_fn = experiment_fn,\r\n)\r\n```", "@agniszczotka @alyaxey Using Experiment works and enables me to run validation along with training. However, I've found that the batch size is probably encoded as a constant instead of a symbolic tensor for the input node even though it is coded as a reshape node with variable batch size (i.e, tf.reshape(features[\"x\"], [-1, ...]). As a result, in the Android code, I have to allocate an array of similar size as the batch size to store the output (i.e, fetch()). \r\n\r\n<img width=\"711\" alt=\"screen shot 2018-01-20 at 10 15 16 pm\" src=\"https://user-images.githubusercontent.com/21777/35191494-b266e372-fe30-11e7-96eb-cc5cb7fbb2a5.png\">", "Any updates on this?", "@alyaxey, \r\n\r\n> train_monitors = tf.contrib.learn.monitors.ValidationMonitor(\r\n            early_stopping_metric = \"loss\",\r\n    )\r\n\r\nUnfortunately, in TensorFlow 1.5.0 ValidationMonitor is not available ...\r\n\"2016-12-05, Monitors are deprecated. Please use tf.train.SessionRunHook.\"", "@lelugom \r\nThank you for sharing your solution. I have implemented similar one for Dee-n-Weed model (tf.contrib.learn.DNNLinearCombinedClassifier) using tf.contrib.learn.Experiment . Model compiles well and runs on GCP Datalab instance. However, I noticed that if I run Google ML engine training on the same model, the training stalls after the first check point (does not produce any more check points while time goes). On the other hand, if I run training on GCP instance using `python -m trainer.task`  with parameters, the model converge as it should. What could be the reason for that difference between ML engine training and ordinary training??", "I've created a `ValidationHook` based on the existing `LoggingTensorHook`.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass ValidationHook(tf.train.SessionRunHook):\r\n    def __init__(self, model_fn, params, input_fn, checkpoint_dir,\r\n                 every_n_secs=None, every_n_steps=None):\r\n        self._iter_count = 0\r\n        self._estimator = tf.estimator.Estimator(\r\n            model_fn=model_fn,\r\n            params=params,\r\n            model_dir=checkpoint_dir\r\n        )\r\n        self._input_fn = input_fn\r\n        self._timer = tf.train.SecondOrStepTimer(every_n_secs, every_n_steps)\r\n        self._should_trigger = False\r\n\r\n    def begin(self):\r\n        self._timer.reset()\r\n        self._iter_count = 0\r\n\r\n    def before_run(self, run_context):\r\n        self._should_trigger = self._timer.should_trigger_for_step(self._iter_count)\r\n\r\n    def after_run(self, run_context, run_values):\r\n        if self._should_trigger:\r\n            self._estimator.evaluate(\r\n                self._input_fn\r\n            )\r\n            self._timer.update_last_triggered_step(self._iter_count)\r\n        self._iter_count += 1\r\n```\r\n\r\nYou can attach it as a hook whenever you run `Estimator.train()`.", "Is this still an issue ?", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=7669\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=7669\">No</a>\n"]}, {"number": 7668, "title": "Building from source - PYTHONPATH not respected", "body": "So I'm trying to build tensorflow from source, main reason is that I do not have root access and the `GLIBC` version was incompatible with the binaries. Additionally, I can not install packages on the python3.\r\n\r\nSteps so far:\r\n1. Build `gcc-4.9.1` from source - SUCCESS\r\n2. Build `bazel-0.4.4` from source - SUCCESS\r\n3. Install all CUDA stuff - SUCCESS\r\n4. Install extra packages in a separate directory (protobuf, nose, argparse, numpy, six etc..) - SUCCESS\r\n5. Build `tensorflow` with the `bazel` binary - FAIL\r\n\r\nOS:\r\n```\r\nLSB Version:\t:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\r\nDistributor ID:\tCentOS\r\nDescription:\tCentOS release 6.5 (Final)\r\nRelease:\t6.5\r\nCodename:\tFinal\r\n```\r\nThe new `gcc` is installed in `/share/apps/barber/system/` together with the other library dependencies `gcc` needs (gmp, mpfr, mpc, elf). \r\n\r\nThe `bazel` binary is also copied into `/share/apps/barber/system/bin`\r\n\r\nCUDA is installed under `/share/apps/barber/cuda` and CuDNN under `/share/apps/barber/cudnn`. \r\n\r\nThe python I'm using is not the default one and lives in `/share/apps/python-3.6.0-shared/bin/python3`. The alternative directory for my own packages is `/share/apps/barber/system/lib/python3.6/site-packages/` (it contains protobuf, argparse, nose etc...).\r\n\r\nGiven all this my environment has the following modified deifnitions:\r\n```\r\nexport BARBER_PATH=/share/apps/barber\r\nexport LD_LIBRARY_PATH=${BARBER_PATH}/system/lib/:${BARBER_PATH}/system/lib64/:/opt/gridengine/lib/linux-x64:/opt/openmpi/lib/\r\nexport PATH=${BARBER_PATH}/system/bin/:$PATH\r\nexport CC=${BARBER_PATH}/system/bin/gcc\r\nexport CXX=${BARBER_PATH}/system/bin/g++\r\nexport CUDA_ROOT=${BARBER_PATH}/cuda\r\nexport CUDA_HOME=${CUDA_ROOT}\r\nexport CUDNN_PATH=${BARBER_PATH}/cudnn\r\nexport CPATH=${CUDNN_PATH}/include:$CPATH\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${CUDA_ROOT}/lib64/:${CUDA_ROOT}/nvvm/lib64/:${CUDA_ROOT}/extras/CUPTI/lib64:${CUDNN_PATH}/lib64/\r\nexport PYTHONPATH=${BARBER_PATH}/system/lib/python3.6/site-packages/\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/share/apps/python-3.6.0-shared/lib/\r\nalias python=/share/apps/python-3.6.0-shared/bin/python3\r\nalias pip=/share/apps/python-3.6.0-shared/bin/pip\r\n```\r\n\r\nGetting back to the tensorflow build, I'm selecting corretly the python to use and the gcc to use when using CUDA. The `./configure` completes fine and works (I think I only had to change something to `_async`). However, when I try to run \r\n```\r\n bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\nI get the following error:\r\n```\r\nWARNING: Output base '/home/abotev/.cache/bazel/_bazel_abotev/9abd1d3abe11b8f0417e465a29633fc7' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nINFO: Found 1 target...\r\nERROR: /home/abotev/.cache/bazel/_bazel_abotev/9abd1d3abe11b8f0417e465a29633fc7/external/farmhash_archive/BUILD.bazel:12:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/abotev/.cache/bazel/_bazel_abotev/9abd1d3abe11b8f0417e465a29633fc7/execroot/tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/share/apps/barber/system/lib/:/share/apps/barber/system/lib64/:/opt/gridengine/lib/linux-x64:/opt/openmpi/lib/:/share/apps/barber/cuda//lib64/:/share/apps/barber/cuda//nvvm/lib64/:/share/apps/barber/cuda//extras/CUPTI/lib64:/share/apps/barber/cudnn_5_1/lib64/:/share/apps/barber/arrayfire-3/lib/:/share/apps/python-3.6.0-shared/lib/ \\\r\n    PATH=/share/apps/java/bin/:/share/apps/barber/system/bin/:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/bio/ncbi/bin:/opt/bio/mpiblast/bin:/opt/bio/EMBOSS/bin:/opt/bio/clustalw/bin:/opt/bio/tcoffee/bin:/opt/bio/hmmer/bin:/opt/bio/phylip/exe:/opt/bio/mrbayes:/opt/bio/fasta:/opt/bio/glimmer/bin:/opt/bio/glimmer/scripts:/opt/bio/gromacs/bin:/opt/bio/gmap/bin:/opt/bio/tigr/bin:/opt/bio/autodocksuite/bin:/opt/bio/wgs/bin:/opt/eclipse:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/rocks/bin:/opt/rocks/sbin:/opt/gridengine/bin/linux-x64:/home/abotev/bin \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.d '-frandom-seed=bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o' -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/farmhash_archive/src/farmhash.cc -o bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTraceback (most recent call last):\r\n  File \"external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\", line 41, in <module>\r\n    from argparse import ArgumentParser\r\nImportError: No module named argparse\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 16.723s, Critical Path: 0.88s\r\n```\r\nThi suggests that the `bazel` build for some reason is ignoring my `$PYTHONPATH` and can not find argparse. If I run my python argparse is imported with no problems. \r\n\r\nNow, I really am not that much concerned with why this is happening, but more of how could I can bypass it to build `tensorflow`? \r\n\r\nRelated issues: \r\nhttps://github.com/tensorflow/tensorflow/issues/190 - most related. However, the solution there is only for the case of uncompatible gcc, no resolution for the import error. \r\nhttp://stackoverflow.com/questions/15093444/importerror-no-module-named-argparse - I can't install system packages\r\nhttps://github.com/rg3/youtube-dl/issues/4483 - does not help me for tensorflow\r\nhttps://github.com/tensorflow/tensorflow/issues/2860 - does not resolve the issue, but seems pretty similar \r\nhttps://github.com/tensorflow/tensorflow/issues/2021 - this shows this might be a `bazel` issue", "comments": ["Can you try with `bazel build --action_env PYTHONPATH ...`", "Thanks for the fast reply. Unfortunately for me it did not work (same error reported)", "as a datapoint, I'm able to `pip install` packages on Linux and `bazel test` sees them (`action_env` is only needed on MacOS for me). Note that Python can throw `ImportError:` for reasons unrelated to Python path, for instance, when it loads a binary extension in the process of import and this binary extension can't find something. Sometimes you can diagnose this by `strace`. IE, you can rule out `PYTHONPATH` being the problem by doing something like `strace -Ttf -e -yy trace=open ...` and checking if it actually looks inside `PYTHONPATH` dirs", "Ah unfortunately I'm not too familiar with `bazel` so I'm not sure how to run test on install python packages, just to verify whether it sees them. Could you elaborate what I need to do for that testing?\r\n\r\n", "Actually never mind, given that very similar issue came up in the past, it seems more likely this is bazel related", "Hmm, do you have any pointers who I might need to ask/flag this to? I don't mind if needed to edit `bazel` source code and rebuilding it (e.g. if needed I can hard code the `$PYTHONPATH`). However, I have no idea where to look.", "Here: https://bazel.build/versions/master/docs/be/python.html\r\n\r\nThis indicates that there exist a flag `imports`. However, I did not find anything related in the tensorflow installation to modify it. ", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 7667, "title": "[GO] Make utility for ops public", "body": "Ops in util_test cannot be used because these are _test.go files. Since the usage\r\nof these ops utils are writen in API docs, it is better to make it public.", "comments": ["Can one of the admins verify this patch?", "Thanks for the change @Lewuathe, but I'd actually prefer to _not_ export these functions.\r\n\r\nUsers of the API should use the ['op' package](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op) to construct the graphs. The functions in `util_test.go` are there only for unittests of the `tensorflow` package to avoid any circular dependencies (the 'op' package depends on the 'tensorflow' package and I didn't want the reverse dependency just for unittests).\r\n\r\nLet me know if I misunderstood something.\r\n\r\nThanks!", "@asimshankar Thanks for comment. I understand.\r\nBut [example of partial run](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#example-PartialRun) in godoc uses these util test APIs. It was confusing to me because interface is different. So how about rewriting this example instead of make `util_test.go` public? ", "Ah, I can see how that is confusing. Feel free to suggest rewrites to the example, otherwise I can take a stab at it later  (hard to balance between keeping the example focused and not having to depend on the 'op' package :).", "But I'll go ahead and close this Pull Request for now, since it seems we've agreed to try something else.", "@asimshankar Thanks. I'll update example for partial run later."]}, {"number": 7666, "title": "Fix obsoleted tf.train.SummaryWriter() in the document page of Embedding Visualization", "body": "The original document use an incorrect function to instantiate the summary file writer.  ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I signed it!", "See https://patch-diff.githubusercontent.com/raw/tensorflow/tensorflow/pull/7666.patch for which commit you signed with, make sure it matches the one you signed the CLA with.", "@vrv I am unsure how to specify the commit I sighed with in the Google CLA website.\r\n\r\n I went to https://cla.developers.google.com/ and signed in my Google account. However, I signed in with a different Google account in the first attempt.", "Closing out because it's already fixed in master. Thanks for the pull request! In the future, you'll need to make sure that you signed the CLA with the same e-mail address you committed with."]}, {"number": 7665, "title": "Dockerfile manual build central repo URL is Incorrect", "body": "Dockerfile and Dockerfile.gpu have URLs that do not point to valid repo\r\n\r\nCurrent URL in Dockerfile and Dockerfile.gpu\r\n\r\nhttp://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl\r\n\r\nhttp://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.0.0-cp27-none-linux_x86_64.whl\r\n\r\nChanging the version number to 1.0.0 fixes the problem. \r\n\r\nWorking URL: http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl\r\n\r\nNot sure if the intent was to leave it at 0.0.0. If so, comments instructing the user to edit the URL to the current version would be helpful.", "comments": ["@gunan, @yifeif, could you take a look?", "@caisq assume this was intentional?", "@yifeif That is correct - these incorrect URLs are intentional. The strings like 0.0.0 are replaced by our docker build script. For example, see: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/parameterized_docker_build.sh#L215", "I'm closing this issue now. Feel free to comment or re-open the issue if there are remaining questions."]}, {"number": 7664, "title": "tf.nn has no attribute rnn_cell in version 1.0.1", "body": "\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n`<ipython-input-66-e322aca5289e> in makeGRUCells()\r\n      7 \r\n      8         def makeGRUCells():\r\n----> 9             base_cell = tf.nn.rnn_cell.GRUCell(num_units=RNN_HIDDEN_SIZE,)\r\n     10             layered_cell = tf.nn.rnn_cell.MultiRNNCell([base_cell] * NUM_LAYERS,state_is_tuple=False)\r\n     11             attn_cell =tf.contrib.rnn.AttentionCellWrapper(cell=layered_cell,attn_length=ATTN_LENGTH,state_is_tuple=False)\r\n\r\nAttributeError: 'module' object has no attribute 'rnn_cell'`\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["It was moved to contrib. Take a look [here](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)", "The change is not mentioned in release notes, conversion script doesn't notice it too.", "@gaika, sorry about that. Unfortunately it appears a few things slipped through. I'll update the conversion script. @yifeif, can you update the release notes, and @ebrevdo are there any rnn things that need to be added to the release notes?", "##TODO\r\n\r\n- [ ] fix converter script (aselle)\r\n- [x] fix release notes (yifeif)\r\n- [x] document anything else that is needed on release notes (ebrevdo)", "@xiatian122 Why did you close it? A lot of people will run into this issue when migrating to 1.0.", "Andrew, it does indeed seem that the move of many of the ops and cells to\ntf.contrib.rnn was not mentioned in release.md.  mea culpa; i was hoping to\nget them all back into core by 1.0.  Unfortunately looks like it won't\nhappen until TF 1.0.\n\nSo in the meantime, we should update the release notes to say that\ntf.nn.rnn_cell.* and most functions in tf.nn.rnn.* (with the exception of\ndynamic_rnn and raw_rnn) are temporarily in tf.contrib.rnn.  They will move\nback into core (presumably) for TF 1.1.\n\nOn Sun, Feb 19, 2017 at 6:46 PM, Joe Petviashvili <notifications@github.com>\nwrote:\n\n> Why did you close it? A lot of people will run into this issue when\n> migrating to 1.0.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7664#issuecomment-280975282>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5Z7eP4hB-Xl5x41Fwr4osdjdmMLks5reP6LgaJpZM4MFasF>\n> .\n>\n", "i also ran into this issue. I realized `tf.nn.rnn_cell.DropoutWrapper` is moved to `Module: tf.contrib.rnn.core_rnn_cell`. What is this? Why isn't all this included in the upgrade script??\r\n\r\nI think i need to migrate back to 0.12 for now...", "I also encountered such problems in Tensorflow 1.0:\r\nlstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\r\nAttributeError: 'module' object has no attribute 'rnn_cell'\r\nIt can not find 'rnn_cell'. When I use tf.contrib.rnn.BasicLSTMCell, it worked. So i suggest you change the  nn.rnn_cell to contrib.rnn. ", "@yifeif, could you update the release notes?", "Release note was updated in #7745 to indicate that RNN is temporarily moved to contrib. But it also says it will be back in core in r1.1. We should update that part to 1.2, @ebrevdo?  Also cc @av8ramit for release.", "Yes; we should definitely update that to document the  move back in TF 1.2;\nnot 1.1.\n\nOn Thu, Apr 13, 2017 at 10:17 AM, Yifei Feng <notifications@github.com>\nwrote:\n\n> Release note was updated in #7745\n> <https://github.com/tensorflow/tensorflow/pull/7745> to indicate that RNN\n> is temporarily moved to contrib. But it also says it will be back in core\n> in r1.1. We should update that part to 1.2, @ebrevdo\n> <https://github.com/ebrevdo>? Also cc @av8ramit\n> <https://github.com/av8ramit> for release.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7664#issuecomment-293965008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3sUUEoVMa-NPKBs5iwpQ2hcTBeWks5rvliagaJpZM4MFasF>\n> .\n>\n", "I am using tensorflow-1.1.0rc2, when I use the code for:\r\nscell = tf.nn.rnn_cell.GRUCell(embedding_size)\r\n\r\nI get the following error:\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'\r\n\r\nwhat should I use instead  of  `tf.nn.rnn_cell.GRUCell` for 1.1.0rc2 version?", "tf.contrib.rnn.GRUCell\n\nOn Apr 19, 2017 6:58 PM, \"davtalab\" <notifications@github.com> wrote:\n\n> I am using tensorflow-1.1.0rc2, when I use the code for:\n> scell = tf.nn.rnn_cell.GRUCell(embedding_size)\n>\n> I get the following error:\n> AttributeError: module 'tensorflow.python.ops.nn' has no attribute\n> 'rnn_cell'\n>\n> what should I use instead of tf.nn.rnn_cell.GRUCell for 1.1.0rc2 version?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7664#issuecomment-295541892>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8DzDevSgw98DrX4Xrix4CJyanaCks5rxrvHgaJpZM4MFasF>\n> .\n>\n", "Added a PR to fix the release notes to indicate TF 1.2 rather than TF 1.1.\r\n\r\nLooks like the only remaining thing to fix this now is to update the conversion script?", "FYI now the release note should be updated in 1.0, 1.1 and master branches. Thanks @nealwu!", "One more question here. I saved a RNN model in TF 0.12, how can I restore it in TF1.0?", "i got this error what i do \r\n\r\n  File \"C:\\Users\\win-7\\Downloads\\Compressed\\tensorflow_chatbot-master\\seq2seq_model.py\", line 98, in __init__\r\n    single_cell = tf.nn.rnn_cell.GRUCell(size)\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'\r\n\r\n\r\nand the error causing code is\r\n\r\n# Create the internal multi-layer cell for our RNN.\r\n    single_cell = tf.nn.rnn_cell.GRUCell(size)                                   <<--- line 98\r\n    if use_lstm:\r\n      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)                  \r\n    cell = single_cell\r\n    if num_layers > 1:\r\n      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)", "Hi @athuldevin, as mentioned above, `tf.nn.rnn_cell` has moved to `tf.contrib.rnn`.\r\n\r\nWhat you should actually do to solve the problem however is to pull your repository / grab the latest version of the code at https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py. Your current version is out of date, and there have been a few other bugs fixed since then.", "The RNN definition for TF 0.12 is different from 1.0: tf.nn. * -> tf.contrib.rnn. *\r\n\r\nThe model is not compatible, in other words, once we trained a model on TF 0.12, it is hard to restore it on TF 1.0.\r\n\r\nI wrote an adapter, using which you can transfer TF 0.12 RNN model into TF 1.0.\r\n\r\nSample code ({k} indicates the k_th RNN layer):\r\n\r\nreplace_vars = {\r\n'rnn/rnn/MultiRNNCell/Cell{k}/BasicLSTMCell/Linear/Matrix': 'rnn/rnn/multi_rnn_cell/cell_{k}/basic_lstm_cell/weights',\r\n'rnn/rnn/MultiRNNCell/Cell{k}/BasicLSTMCell/Linear/Bias': 'rnn/rnn/multi_rnn_cell/cell_{k}/basic_lstm_cell/biases'}\r\nsaver.restore(sess, \"old.ckpt\")\r\nnames_to_vars = {v.op.name: v for v in tf.all_variables()}\r\nfor key in replace_vars.keys():\r\n    bias_var = names_to_vars[key]\r\n    names_to_vars[replace_vars[key]] = bias_var\r\n    del names_to_vars[key]\r\nsaver = tf.train.Saver(var_list=names_to_vars)\r\nsaver.save(sess, 'new.ckpt')\r\n\r\nYou may refer to this repo for a complete solution:\r\n\r\nhttps://github.com/physicso/TensorFlow_RNN_Adaptor", "FYI we'll try to bring the tf.nn.rnn_cell namespace back in TF 1.2.", "@ebrevdo Great news, thanks!", "upgraded to 1.2.0rc0 \r\nwas rnn NOT moved back to core?\r\nstill getting:\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'", "I have a PR in place undergoing internal testing, should go out next week\nin the nighties and then be in the final 1.2 release.\n\nOn May 21, 2017 9:25 AM, \"LZ\" <notifications@github.com> wrote:\n\n> upgraded to 1.2.0rc0\n> was rnn NOT moved back to core?\n> still getting:\n> AttributeError: module 'tensorflow.python.ops.nn' has no attribute\n> 'rnn_cell'\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7664#issuecomment-302946986>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim73dA2nmYdGLQq0ET72BrqpOpv1Qks5r8GVmgaJpZM4MFasF>\n> .\n>\n", "And it'll be tf.nn.rnn_cell as before.\n\nOn May 21, 2017 1:34 PM, \"Eugene Brevdo\" <ebrevdo@google.com> wrote:\n\n> I have a PR in place undergoing internal testing, should go out next week\n> in the nighties and then be in the final 1.2 release.\n>\n> On May 21, 2017 9:25 AM, \"LZ\" <notifications@github.com> wrote:\n>\n>> upgraded to 1.2.0rc0\n>> was rnn NOT moved back to core?\n>> still getting:\n>> AttributeError: module 'tensorflow.python.ops.nn' has no attribute\n>> 'rnn_cell'\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/7664#issuecomment-302946986>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim73dA2nmYdGLQq0ET72BrqpOpv1Qks5r8GVmgaJpZM4MFasF>\n>> .\n>>\n>\n", "Is there a wheel for 1.2-rc1 gpu for windows? Can't seem to find it.", "Hi,\r\nI'm trying to move my LSTM network from TF v0.12 to v1.1, but its learning capabilities are very different:\r\nin v0.12 I had:\r\n\r\ncell = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, state_is_tuple=True)\r\ncell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\r\ncell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\r\n\r\nin my TFv1.1 code I have now:\r\n\r\ncell = []\r\nfor _ in range(num_layers):\r\n    cell.append(tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, state_is_tuple=True) )\r\n\r\nmulticell = tf.contrib.rnn.MultiRNNCell([cell[i] for i in range(num_layers)])\r\nmulticell = tf.contrib.rnn.DropoutWrapper(multicell, output_keep_prob=dropout)\r\n\r\nAnd these are not equivalent. The 0.12 version was learning much better than the one in 1.1. What is the reason? Did I move my code to 1.1 correctly?\r\n", "This works: \r\ncell = tf.contrib.rnn.BasicRNNCell(num_units = rnn_size) in current Tensorflow version", "Use tf.nn.rnn_cell.MultiRNNCell.\n\nOn Jun 26, 2017 1:25 AM, \"Julian Niedermeier\" <notifications@github.com>\nwrote:\n\n> There seems to be a discrepancy between tensorflow-gpu v1.2.0 from pip\n> and tensorflow v1.2.0 from pip.\n> I run the following within a model_fn for Estimator:\n>\n> import tensorflow.contrib.rnn as rnn\n> def single_cell():\n>         return rnn.BasicLSTMCell(cell_size)\n> cell = rnn.core_rnn_cell.MultiRNNCell(\n>             [single_cell() for _ in range(layers)])\n>\n> It throws the following error for tensorflow-gpu but works for tensorflow:\n>\n>   File \"/home/user/miniconda3/envs/pvalearning-gpu/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\n>     return func(*args, **kwargs)\n>   File \"/home/user/miniconda3/envs/pvalearning-gpu/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 455, in fit\n>     loss = self._train_model(input_fn=input_fn, hooks=hooks)\n>   File \"/home/user/miniconda3/envs/pvalearning-gpu/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 955, in _train_model\n>     model_fn_ops = self._get_train_ops(features, labels)\n>   File \"/home/user/miniconda3/envs/pvalearning-gpu/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1162, in _get_train_ops\n>     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\n>   File \"/home/user/miniconda3/envs/pvalearning-gpu/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1133, in _call_model_fn\n>     model_fn_results = self._model_fn(features, labels, **kwargs)\n>   File \"/home/user/pva/learning/packages/model/my_model.py\", line 67, in __model__\n>     cell = rnn.core_rnn_cell.MultiRNNCell(\n> AttributeError: module 'tensorflow.contrib.rnn' has no attribute 'core_rnn_cell'\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7664#issuecomment-310994898>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1ZqMv07KGoo8CpW9DKfJlPDI56iks5sH2r8gaJpZM4MFasF>\n> .\n>\n", "my tensorflow version is 1.2.1, and I find these three things are the same thing.\r\n- tf.nn.rnn_cell.*\r\n- tf.contrib.rnn.*\r\n- tensorflow.python.ops.rnn_cell_impl.*\r\n\r\nProof it in ipython:\r\n```python\r\nIn [78]: tf.nn.rnn_cell.LSTMCell\r\nOut[78]: tensorflow.python.ops.rnn_cell_impl.LSTMCell\r\n\r\nIn [79]: tf.contrib.rnn.LSTMCell\r\nOut[79]: tensorflow.python.ops.rnn_cell_impl.LSTMCell\r\n```", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "[`tf.nn.rnn_cell`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell) is available as of TensorFlow 1.2, so I think we can close this issue."]}, {"number": 7663, "title": "Issue with Ubuntu 16.04 GPU install: Bus error (core dumped)", "body": "Operating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0, cuDNN 5.1\r\n\r\njiexun@jiexun-XPS-15-9560:~$ ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root root   556000 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root root   415432 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Feb 19 16:10 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       18 Feb 19 16:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 root root 42762752 Feb 19 16:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n\r\nInstalled GPU enabled TensorFlow with:\r\npip install tensorflow-gpu\r\n\r\nError message importing tensorflow:\r\nPython 3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nBus error (core dumped)\r\n\r\n#### Any idea why the above 'Bus error' is happening? Thanks a lot!\r\n\r\n\r\n----\r\nI also tried installing from source, but got this error when creating the pip package:\r\n, after running `bazel build --config opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`:\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\r\nERROR: /home/jiexun/tensorflow/tensorflow/python/BUILD:2279:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 27 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/local_linux-py3-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudnn.so.5: file not recognized: File truncated\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1332.429s, Critical Path: 853.44s\r\n", "comments": ["@zheng-xq, @jart, @gunan do either of you have insight? It looks like one warning sign is that -Wno-self-assign is not being found which looks like the warning is a gcc warning that doesn't exist on clang. But that is only a warning.", "In the off-chance you're in Chris Manning's Natural Language Processing with Deep Learning class (CS 224N), I found that deleting the VM you're currently using and restarting TensorFlow (and all dependencies) installation with a fresh VM is a quick solution. Definitely doesn't address the underlying issue, though!", "Is it possible your `LD_LIBRARY_PATH` is not setup correctly?", "@gabbifish haha I'm just trying to install tensorflow gpu from scratch on my computer!\r\n\r\n@gunan I set it as \r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\r\nexport CUDA_HOME=/usr/local/cuda", "This looks very similar to #6744\r\nAnd the build error message points to an issue with your libcudnn.so file.\r\nCould you retry installing libcudnn?", "I will try and let you know, thanks a lot for the help!", "Closing due to lack of activity.  Please reopen if necessary.", "thanks for the help, I reinstalled everything and it works now!"]}]