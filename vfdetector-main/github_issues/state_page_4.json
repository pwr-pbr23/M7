[{"number": 55515, "title": "Custom Gradient for Sparse Weight Tensors", "body": "I am trying to create a custom layer which computes W\\*x + b where W is a sparse tensor. It is important that I don't ever form the dense version of W because it would be too large to store in memory. It is my understanding that the computation of W\\*x, using `tf.sparse.sparse_dense_matmul(W, x),` does not have a supported gradient. Is such a gradient expected to be supported anytime soon? It seems that many others would want this functionality as well.\r\n\r\nTo make this work, I am trying to implement a custom gradient using the following code:\r\n\r\n    @tf.custom_gradient\r\n          def sparse_weight_multiply(self, w):\r\n    \t     # compute the product sparse_W * inputs, where sparse_W is a sparse tensor formed from the entries in w\r\n  \r\n              self.sparse_W = tf.sparse.SparseTensor(self.indices, w, self.shape)\r\n              w_inputs = tf.sparse.sparse_dense_matmul(self.sparse_W, self.inputs)\r\n  \r\n              # define gradient for this function\r\n              def sparse_weight_grad(upstream_grad):\r\n                  '''\r\n                  upstream_grad is the gradient computed thus far in the computational graph. \r\n                  The output of this function will be the gradient of the function sparse_weight_multiply \r\n                  times upstream_grad, due to the product rule in differentiation. \r\n  \r\n                  '''\r\n                  # check the shape of upstream_grad\r\n                  print(\"Shape of upstream grad: {}\".format(upstream_grad.shape))\r\n                  print(\"Shape of inputs: {}\".format(inputs.shape))\r\n                  print(\"Shape of weight: {}\".format(self.shape))\r\n  \r\n                  # map entries of input to corresponding locations in the gradient of weights*inputs\r\n                  n_out   = upstream_grad.shape[0]\r\n                  num_RHS = upstream_grad.shape[1]\r\n  \r\n                  indices_i = range(0,self.num_connections)\r\n                  indices_j = self.indices[:, 0]\r\n                  indices_k = self.indices[:, 1]\r\n                  J_indices = tf.cast(tf.transpose(tf.concat([[indices_i],[indices_j]], 0)), tf.int64)\r\n  \r\n                  grad_weights = []\r\n  \r\n                  for l in range(num_RHS):\r\n                      input_permuted = np.array(self.inputs)[:,l][indices_k]\r\n                      sparse_J = tf.sparse.SparseTensor(J_indices, input_permuted, (self.num_connections, n_out))\r\n                      grad_weights.append(tf.sparse.sparse_dense_matmul(sparse_J, tf.reshape(upstream_grad[:,l], (n_out, 1)) ))\r\n  \r\n                  grad_weights = tf.transpose(tf.squeeze(tf.convert_to_tensor(grad_weights)))\r\n                  return grad_weights\r\n  \r\n              return w_inputs, sparse_weight_grad\r\n\r\nHowever, I get the error:\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: var and grad do not have the same shape[9632] [9632,638] [Op:ResourceApplyAdam]`\r\nI believe this is because my input, `w`, is a tensor of shape [9632]. However, I want to compute the gradient of `W*x` for each input `x` to the layer, of which I have 638 in my training set. Thus, the gradient I return has shape [9632,638], corresponding to a gradient with shape [9632] for each input. This matches the shape of the upstream gradient I am given, which has shape (3836, 638). I definitely want to pass a gradient for each input, but how do I tell tensorflow that that is what I am doing?", "comments": ["@jezvonek \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55513, "title": "RFE tensorflow-aarch64==2.6.0 build ?", "body": "**System information**\r\n TensorFlow version (you are using):  2.6.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nBrainchip Akida AKD1000 SNN neuromorphic MetaTF SDK support 2.6.0 on x86_64. They claim support for aarch64, but when creating a virtualenv it fails on aarch64 due to lacking tensorflow-aarc64==2.6.0 build.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNA\r\n\r\n**Who will benefit with this feature?**\r\n\r\nCustomer of Brainchip Akida who run on Arm64 platforms.\r\n\r\n**Any Other info.**\r\n\r\nhttps://doc.brainchipinc.com/installation.html\r\n\r\n\r\n", "comments": ["@torehl, Can you try the instructions mentioned [here](https://www.tensorflow.org/lite/guide/build_arm#cross-compilation_for_arm_with_bazel). While building you fetch Git branch 2.6 and build the Tensorflow. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55510, "title": "TypeError: __call__() missing 1 required positional argument: 'step'", "body": "there is bug in tensorflow 2.6,\r\nLearningRateSchedule is not correctly recognize by isinstance\r\ntensorflow/python/keras/optimizer_v2/optimizer_v2.py line 1014 and 807,\r\n```\r\n    # value is tf.keras.optimizers.schedules.PiecewiseConstantDecay\r\n    print(isinstance(value, learning_rate_schedule.LearningRateSchedule))  # return false , not desired\r\n    import tensorflow\r\n    print(isinstance(value, tensorflow.keras.optimizers.schedules.LearningRateSchedule)) # return true ,can slove the bug below\r\n```\r\nCan any one create a pull request?\r\n```\r\nne/training_generator_v1.py\", line 574, in fit\r\n    return fit_generator(\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_generator_v1.py\", line 256, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 1093, in train_on_batch\r\n    self._make_train_function()\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 2028, in _make_train_function\r\n    updates = self.optimizer.get_updates(\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 784, in get_updates\r\n    return [self.apply_gradients(grads_and_vars)]\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 671, in apply_gradients\r\n    apply_state = self._prepare(var_list)\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 957, in _prepare\r\n    self._prepare_local(var_device, var_dtype, apply_state)\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/gradient_descent.py\", line 125, in _prepare_local\r\n    super(SGD, self)._prepare_local(var_device, var_dtype, apply_state)\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 963, in _prepare_local\r\n    lr_t = array_ops.identity(self._decayed_lr(var_dtype))\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 1017, in _decayed_lr\r\n    lr_t = self._get_hyper(\"learning_rate\", var_dtype)\r\n  File \"/home/fujiaqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 814, in _get_hyper\r\n    value = value()\r\nTypeError: __call__() missing 1 required positional argument: 'step'\r\n\r\n```\r\ntest code\r\n```\r\n  boundaries = [len(x_train)/batch_size*70, len(x_train)/batch_size*150]\r\n  values = [0.1, 0.01, 0.001]\r\n  learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\r\n      \r\n  optimizer1 = gradient_descent_v2.SGD(learning_rate=learning_rate_fn,momentum=0.9)\r\n  model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer1, metrics=[\"accuracy\"])\r\n  model.fit()\r\n```", "comments": ["@JiaqingFu \r\nIn order to expedite the trouble-shooting process here,could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose)?\r\nThanks!", "@sushreebarsa  how\uff1fI have get all information about the bug.this is just a easy small bug and can be fixed very quickly!", "@JiaqingFu Thank you for the response !\r\nCould you please provide us the link to the source code for the bug  so that we can raise a PR?Thanks!\r\n\r\n> tensorflow/python/keras/optimizer_v2/optimizer_v2.py line 1017 and 814,", "https://github.com/tensorflow/tensorflow/blob/v2.6.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py   \r\n![image](https://user-images.githubusercontent.com/12004134/162149217-62511c8c-8968-4a67-9a70-324e9e93471d.png)\r\n![image](https://user-images.githubusercontent.com/12004134/162149330-b3d44c19-38b7-46bf-8c80-b95026e5afe6.png)\r\n"]}, {"number": 55505, "title": "Build TF wheel in another project which depends on TF", "body": "**System information**\r\n- OS Platform and Distribution: (Linux Ubuntu 20.04)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.8.0\r\n- Python version: 3.8.0\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): gcc version 9.4.0\r\n- CUDA/cuDNN version: CUDA 11.2/ cuDNN 8\r\n- GPU model and memory: V100\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have a bazel project which depends on `org_tensorflow`.  My project's `.bazelrc` is copied from TF project\r\nIn my project, I can successfully build TF `build_pip_package` target by:\r\n```bash\r\nbazel build --config=native_arch_linux --config=cuda @org_tensorflow//tensorflow/tools/pip_package:build_pip_package\r\n```\r\nHowever, the following error occurs when generating the wheel\r\n```\r\n./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\nWed Apr 6 10:41:47 CST 2022 : === Preparing sources in dir: /tmp/tmp.C0MooMsKIF\r\nCould not find bazel-bin.  Did you run from the root of the build tree?\r\n```\r\nThe prompt is very intuitive. I also checked `./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package`\r\nThe script assumes that TF is not an external dependency.\r\n In this situation. How can I generate the TF wheel?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```bash\r\nbazel build --config=native_arch_linux --config=cuda @org_tensorflow//tensorflow/tools/pip_package:build_pip_package\r\n```\r\n```bash\r\n./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```", "comments": ["Hello @372046933 ,\r\n\r\nIt seems there is no symlink for bazel-bin being created.The idea is to create the **`bazel-bin`** symlink is that the TensorFlow **`build_pip_package`** script expects to exist.\r\n\r\nCan you try manually creating it and copying all files from **`users/XXXXXXXXXXXXXXXXXX/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe`** into **`\\tmp\\tensorflow\\bazel-bin`**\r\n\r\nIf that doesn't work, can you please try building on a different directory, not **`C:\\tmp`**. Most of times **`tmp`** is mismanaged.\r\n\r\nIf possible, move your workspace to a filesystem that supports symlinks and re-run **`bazel build`** there.Thanks!\r\n\r\n\r\n", "@tilakrayal  Thanks. The reason for no `bazel-bin` is that I am building TF as an external dependency. By the way, I'm building on Linux. `C:\\tmp` does not exist.", "The key output of `bazel build --config=native_arch_linux --config=cuda @org_tensorflow//tensorflow/tools/pip_package:build_pip_package`:\r\n```\r\nINFO: Found 1 target...\r\nTarget @org_tensorflow//tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 211.970s, Critical Path: 98.34s\r\nINFO: 1 process: 1 internal.\r\nINFO: Build completed successfully, 1 total action\r\n```", "@372046933 ,\r\nCould you please confirm if the installation issue is resolved? Thanks!", "@tilakrayal No, it's not resolved. Cannot execute \r\n```\r\n./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```", "@372046933,\r\nBazel wasn't able to create the symlinks for bazel-bin, try workaround `bazel build --symlink_prefix=/ `when you run Bazel. This will tell Bazel to not create the symlink so it will hopefully work. Thanks!", "@gadagashwini  Added `--symlink_prefix=/` to `bazel build`. But nothing changed. My build command is\r\n```bash\r\nbazel build --config=native_arch_linux --config=cuda  --symlink_prefix=/ @org_tensorflow//tensorflow/tools/pip_package:build_pip_package\r\n```\r\nBy the way, did you notice that I am building TF as an external dependency. i.e. `@org_tensorflow//tensorflow/tools/pip_package:build_pip_package`  not `//tensorflow/tools/pip_package:build_pip_package`", "@372046933,\r\nCan you try the workaround mention on similar thread [#21461](https://github.com/tensorflow/tensorflow/issues/21461#issuecomment-430449910).Let us know if that helps. Thanks!", "@gadagashwini If I'm not mistaken, https://github.com/tensorflow/tensorflow/issues/21461#issuecomment-430449910 configures a local installed TF. Which is installed by pip. In my scenario, I was building TF wheel. Since TF is not installed, `cc_tf_configure` will fail.", "@372046933, Once you build the Tensorflow wheel. you need to install the wheel package with pip. \r\n\r\n```\r\nbazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package\r\n\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\npip install /tmp/tensorflow_pkg/tensorflow-version-tags.whl\r\n```", "@gadagashwini . Yes, here is what I executed:\r\n```\r\n./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\nThe following error occurred \r\n```\r\nWed Apr 6 10:41:47 CST 2022 : === Preparing sources in dir: /tmp/tmp.C0MooMsKIF\r\nCould not find bazel-bin.  Did you run from the root of the build tree?\r\n```"]}, {"number": 55503, "title": "2.7.0: memory leak in TFLite's tflite::Interpreter::Invoke()", "body": "Seeing a memory leak in tflite::Interpreter::Invoke().\r\nThe leak is observed while running our software, wrapping TFLite 2.7.0 (built from source at that tag), on iOS 15.4 (EDIT: confirmed still leaking with 2.7.1 and 15.4.1), and using CoreML delegate.\r\n\r\nThe following leaks seem to occur roughly with every call to Invoke:\r\n\r\n80 bytes chunk with the following stack:\r\n```\r\nclass_createInstance\t\t\r\n__CFAllocateObject\t\t\r\n__NSSetI_new\t\t\r\n-[NSSet initWithArray:range:copyItems:]\t\t\r\n+[NSSet setWithArray:]\t\t\r\n-[MLDictionaryFeatureProvider featureNames]\t\t\r\n0x11e19b664\t\t\r\n0x11e14b44c\t\t\r\ntflite::Subgraph::Invoke()\t\t\r\ntflite::Interpreter::Invoke()\t\r\n```\r\n\r\n48 bytes chunk with the following stack:\r\n```\r\nclass_createInstance\t\t\r\n__CFAllocateObject\t\t\r\n__NSSetI_new\t\t\r\n-[NSSet initWithArray:range:copyItems:]\t\t\r\n+[NSSet setWithArray:]\t\t\r\n-[MLDictionaryFeatureProvider featureNames]\t\t\r\n0x11e19b664\t\t\r\n0x11e14b44c\t\t\r\ntflite::Subgraph::Invoke()\t\t\r\ntflite::Interpreter::Invoke()\r\n```\r\n\r\n16 bytes chunk with the following stack:\r\n```\r\n_CFCreateArrayStorage\t\t\r\n-[NSDictionary allKeys]\t\t\r\n-[MLDictionaryFeatureProvider featureNames]\t\t\r\n0x11e19b664\t\t\r\n0x11e14b44c\t\t\r\ntflite::Subgraph::Invoke()\t\t\r\ntflite::Interpreter::Invoke()\t\r\n```\r\n\r\n32 bytes chunk with the following stack \r\n```\r\n_objc_rootAllocWithZone\t\t\r\nobjc_alloc_init\t\t\r\n-[NSTaggedPointerString UTF8String]\t\t\r\n0x11e19afd0\t\t\r\n-[MLNeuralNetworkEngine verifyInputs:error:]\t\t\r\n-[MLNeuralNetworkEngine evaluateInputs:options:error:]\t\t\r\n__62-[MLNeuralNetworkEngine predictionFromFeatures:options:error:]_block_invoke\t\t\r\n0x10342e7bc\t\t\r\n_dispatch_lane_barrier_sync_invoke_and_complete\t\t\r\n-[MLNeuralNetworkEngine predictionFromFeatures:options:error:]\t\t\r\n0x11e19b714\t\t\r\n0x11e14b44c\t\t\r\ntflite::Subgraph::Invoke()\t\t\r\ntflite::Interpreter::Invoke()\t\r\n```\r\n\r\nLast time I went hunting for memory leaks, we were using 2.5.0, and there had been no leak there.", "comments": ["Hi @w3sip ! Can you please share the steps or lite model to reproduce this issue? Did you get a chance to check in TF 2.8 or nightly version too? Thanks!", "- Unfortunately can't share the model, but it's a yolo v3 model.\r\n- The leak is obvious, when running an inference under Instruments. It also seems to persist even after destroying the interpreter with TfLiteInterpreterDelete)\r\n- Haven't checked in 2.8 (we'd have to upgrade CI to build it, hard dependency on Bazel makes things complex)\r\n- did check in 2.6 (seems like the leak was already present there)", "Hi @sachinprasadhs ! Could you please look at this issue?", "One more note: the leak is gone if CoreML delegate is disabled.", "This seems relevant:\r\nhttps://developer.apple.com/forums/thread/692425\r\n\r\n", "Looks like this had been addressed: https://github.com/tensorflow/tensorflow/commit/9f2e9d1e58f85a2b603baa1e682c0987fa49b203\r\n\r\nAny chance this could be cherry-picked into a 2.7 update? In the meantime, we'll patch it locally and retest.", "Thanks for the issue report, I have created a PR for cherrypicking into r2.8 update.", "Hi, I see that you want this fix to be included in 2.7 version, generally cherrypicks will be done if it is security fix or a large community is effected with the bug. \r\nSince the fix is available in 2.9 and it's release is around the corner. You can use Tf-nightly for the immediate fix and then use the Tensorflow 2.9 when it is released. You can close this issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55498, "title": "[TF:TRT] Improve dynamic shape INT8 calibration API", "body": "This PR moves the calibration function argument to the build function.\r\n\r\nIn dynamic shape mode we need to provide profile information before we can calibrate the TensorRT engine. The `build()` method of `TrtGraphConverterV2` can be used to provide the shape information. Previously, the calibration function had to be passed to `convert()`, where a reference was stored. But calibration actually happens in build, after the shape information is collected. This motivates to move the `calibration_fn` arg to `build()`, where it is actually used.", "comments": ["Tagging @bixia1 and @DEKHTIARJonathan for review.", "This PR builds on #55166.", "@tfeher Can you please resolve conflicts? Thank you!"]}, {"number": 55497, "title": "TF loads/initializes DevicePlugins multiple times if there are symlinked paths in sys.path.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS but others also are affected\r\n- TensorFlow installed from (source or binary): all >= 2.6\r\n- TensorFlow version (use command below): v2.6.1-9-gc2363d6d025 2.6.2\r\n- Python version: >= 3.7\r\n\r\n**Describe the current behavior**\r\nWhen TF starts up, the ```__init__.py``` checks ```sys.path``` for paths to site-packages, that will then be used for loading PluggableDevices. If the identical file is accessible through a symlink from a different path, i.e.:\r\n\r\n- ```.../lib/python3.8/site-packages/tensorflow-plugins/libmydevice.so```\r\n- ```.../lib64/python3.8/site-packages/tensorflow-plugins/libmydevice.so```\r\n\r\nwith ```lib64``` being a symlink to ```lib``` (as it is the case for VENVs) then TensorFlow loads the library twice and dies with\r\n```\r\nstream_executor::MultiPlatformManager::RegisterPlatform( std::move(cplatform)) status: Internal: platform is already registered with name: \"NAME\"\r\n```\r\n\r\nIn https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_experimental.cc#L724 a map with std::string is used, which cannot distinguish the symlink and therefore will call the initialization methods within the PluggableDevice library a second time.\r\n\r\n**Describe the expected behavior**\r\nThe library does not get initialized twice.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\nI think the easiest solution would be to change ```TF_LoadPluggableDeviceLibrary``` to use a set on the library handle instead of map, i.e.:\r\n\r\n```cpp\r\nTF_Library* TF_LoadPluggableDeviceLibrary(const char* library_filename,\r\n                                          TF_Status* status) {\r\n#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)\r\n  status->status = tensorflow::errors::Unimplemented(\r\n      \"PluggableDevice plugin functionality is not supported on mobile\");\r\n  return nullptr;\r\n#else\r\n  TF_Library* lib_handle = new TF_Library;\r\n  static tensorflow::mutex mu(tensorflow::LINKER_INITIALIZED);\r\n  static std::unordered_set<void*>* loaded_libs =\r\n      new std::unordered_set<void*>();\r\n  tensorflow::Env* env = tensorflow::Env::Default();\r\n  {\r\n    tensorflow::mutex_lock lock(mu);\r\n    status->status =\r\n      env->LoadDynamicLibrary(library_filename, &lib_handle->lib_handle);\r\n    if (status->status.ok()) {\r\n      if (loaded_libs.emplace(lib_handle->lib_handle).second) {\r\n        TF_CHECK_OK(tensorflow::RegisterPluggableDevicePlugin(lib_handle->lib_handle));\r\n      } else {\r\n        dlclose(lib_handle->lib_handle);\r\n      }\r\n    } else {\r\n      delete lib_handle;\r\n      return nullptr;\r\n    }\r\n    return lib_handle;\r\n  }\r\n#endif\r\n}\r\n```\r\n\r\n```LoadDynamicLibrary``` uses ```dlopen``` and will return ALWAYS the same handle independent of symlinks, see: https://man7.org/linux/man-pages/man3/dlopen.3.html. The call to ```dlclose``` is to decrease the reference count in case the lib got opened multiple times.\r\n\r\n**Standalone code to reproduce the issue**\r\nWe encountered this error when using a VENV with rh-python38 package, because it puts ```lib``` and ```lib64``` into ```sys.path```. But can also be triggered by forging the PYTHONPATH env var:\r\n\r\n```bash\r\npip3 install tensorflow\r\n# install any PluggableDevice library you like\r\nln -s ~/.local/lib ~/.local/lib64\r\nPYTHONPATH=~/.local/lib/python3.8/site-packages:~/.local/lib64/python3.8/site-packages python3 -c \"import tensorflow\"\r\n```\r\n", "comments": []}, {"number": 55495, "title": "TfLiteGpuDelegateV2Create() causes segmentation fault with tflite-gpu2.8 on native side in Android", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Android 10, API 29\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Galaxy S9\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Qualcomm Adreno 630\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n1. On Native side.\r\n`auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);`\r\nresults in following error with tensorflow-lite 2.8\r\n`A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x18 in tid 23129 (com.example.gpu), pid 23129 (com.example.gpu)`\r\n\r\n2. In Java.\r\nGPU delegation works fine with tensorflow-lite 2.8, following code works well.\r\n`GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();\r\ndelegate = new GpuDelegate(delegateOptions);\r\noptions.addDelegate(delegate);`\r\n\r\n**Describe the expected behavior**\r\nNo segmentation fault in 1.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Not at the moment\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nI downloaded the tflite-2.8 and tflite-gpu-2.8 from the maven repository(https://search.maven.org/) and added them to a new C++ Android project.\r\nAnd in the default native-lib.cpp added the above code, which throws the error.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n", "comments": ["Hi @lakshya-IIT4gp ! Did you get a chance to check this [thread ](https://stackoverflow.com/a/65992488/11530462)on using Bazel and Cmaklist.txt for using tflite in react native environment? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym, that thread is unfortunately irrelevant. I'm already able to use GPU delegate TF-2.3. The issue is it give seg fault with TF-2.8", "Hi @sachinprasadhs ! Could you please look at this issue?", "Can you attach the model?"]}, {"number": 55493, "title": " Adding option `--save_outputs_in_file` for tflite benchmark", "body": "- Exporting output tensors to a file in raw data format.\r\n- Supposed to be helpful for automating on-device(e.g. android-arm64 target) runtime validation of tflite models.", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55493/checks?check_run_id=5830309192).", "@zotanika  Can you please resolve conflicts? Thank you!", "> @zotanika Can you please resolve conflicts? Thank you!\r\n\r\nThank you for the note. Conflicts have been fixed, and waiting for completion of CI check."]}, {"number": 55491, "title": "[TF-TRT] Cast Converter Re-Engineered", "body": "This PR changes the way TF-TRT deal with `Cast` nodes. TensorRT engineers advised us to treat `Cast` as an Identity node and let TensorRT decides on which compute precision to use according to `precision_mode=...`.\r\n\r\nThis behavior can be deactivated using `TF_TRT_EXPERIMENTAL_FEATURES=reject_fp32_fp16_cast` if needed to work around any unforeseen issue\r\n\r\nThis PR also adds `converter.summary()` to TF-TRT test files in order to ease test debugability", "comments": ["@bixia1 for review\r\nCC: @tfeher \r\n", "I have two general comment on this PR:\r\n(1) the existing support is cast from fp16 to fp32, the feature name added here is called reject_fp32_fp16_cast. Shouldn't it be reject_fp16_fp32_cast?\r\n(2) the existing code only supports cast from fp16 to fp32, the change here make us support fp32 to fp16 as well. Is this an intensionally change? If yes, please add test and add this to the PR description.", "Hi @DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thank you!", "@DEKHTIARJonathan Any update on this PR? Please. Thank you!"]}, {"number": 55484, "title": "Adding a parameter to teh tf.image.extract_patches function to change the value of added border pixels due to the padding", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.8\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nUsing tf.image.extract_patches with \"same\" padding leads to zeros on the image border. Zero is a default value and cannot be changed by the user. This means that the mask data for image segmentation should always have zero as the background class, otherwise it does mess up after patching. It is best to add this default value as a parameter so the user can change it if needed.\r\n\r\n**Will this change the current api? How?**\r\nAdding a parameter to teh tf.image.extract_patches function to change the value of added border pixels due to the padding\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using this function for patching data for image segmentation\r\n\r\n**Any Other info.**\r\n", "comments": ["@mehran66 ,\r\nCan you please elaborate about your feature. Also,I request you to please specify the Use Cases for this feature. Thanks!", "The tf.image.extract_patches function is used to patchify large images into smaller images. When Padding is set \"same\", some extra pixels are generated in the border of some images with value of zero. If someone use this tool to patchify images and masks (in semantic segmentation), then zero pixels are added to the masks. If the zero is specific class in the mask, these added zero pixels on the boder can result in inacurate training. Adding a parameter for the default value of padding into the function can solve this issue."]}, {"number": 55480, "title": "Avoid unnecessary instantiations in DispatchToVectorized", "body": "- This avoids unnecessary template instantiations when vectorizing with 16-bit and larger data types, significantly reducing compile times for some kernels.\r\n- It should also help with the Windows compilation issue discussed here:\r\n  https://github.com/tensorflow/tensorflow/issues/54276#issuecomment-1032186006\r\n- No functional change.\r\n\r\ncc @nluehr", "comments": ["@reedwm Can you please review this PR ? Thank you!", "@benbarsdell  Can you please address Ubuntu Sanity errors? Thank you!", "Rebased, which seems to have fixed the CI (I think it was broken due to https://github.com/tensorflow/tensorflow/issues/55494, but I guess the CI doesn't automatically pick up that fix?)."]}, {"number": 55476, "title": "TFLite performance difference between python and c++", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Linux buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 2.8.0\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n\r\n**Describe the current behavior**\r\n\r\nPython performance of tflite is much better than C++.\r\nWhen number of threads is set to -1, Not getting best performance in C++.\r\nManual setting the number of threads to max is giving improvement in C++ API performance and still its less than python.\r\nAs per this issue(https://github.com/tensorflow/tensorflow/issues/46272) It is mentioned, number of threads in c++ are automatically set to -1 and all threads will be used, But its not happening and there is performance difference.\r\nPerformance is not modified proportionately based on the threads. Suppose when threads are set to 2, we are not getting 2x performance than threads as 1\r\n\r\n**Describe the expected behavior**\r\nMatch the performance of python with C++.\r\nGive an API or directly automate the setting the threads without manual change.\r\nWhat is the backend used for python and C++ ? Are they same ?\r\nCan we expect the performance proportionately based on the threads. Suppose when threads are set to 2, can we expect 2x performance than 1 thread ?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nPython TFlite--\r\n\r\n`` python3 tfliteversionprofile_latest_singleiteration.py ``\r\n\r\n2.8.0\r\nTime elapsed during the process:%d ms 99.971158\r\n\r\n`` python3 tfliteversionprofile_latest_singleiteration_multicores.py ``\r\n\r\n.2.8.0\r\nTime elapsed during the process:%d ms 85.076159\r\nThere is clear change in performance when number of threads are set to max. \r\nMy cpu has 6 cores and 2 threads per core, so set to 12.\r\n\r\nC++ TFlite-- \r\n\r\nBut when c++ API is used there is huge impact in performance , Using example label image.\r\n\r\nwhen -1 is set as number of threads.\r\n`` bazel-4.2.1 build -c opt //tensorflow/lite/examples/label_image:label_image ``\r\n\r\nwhen -1 (number of threads) is set in label_image.h present in tensorflow/lite/examples/label_image, \r\n\r\n`` bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model detect.tflite --labels labelmap.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp `` \r\n\r\nINFO: Loaded model detect.tflite\r\nINFO: resolved reporter\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: invoked\r\nINFO: average time: 323.143 ms\r\nINFO: 0.00389769: 3 car\r\nINFO: 0.0038741: 2 bicycle\r\n\r\n\r\n \r\nwhen 12 is set as number of threads.\r\n`` bazel-4.2.1 build -c opt //tensorflow/lite/examples/label_image:label_image ``\r\n\r\nwhen 12 (number of threads) is set in label_image.h present in tensorflow/lite/examples/label_image, \r\n\r\n`` bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model detect.tflite --labels labelmap.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp ``\r\n\r\nINFO: Loaded model detect.tflite\r\nINFO: resolved reporter\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: invoked\r\nINFO: average time: 141.746 ms\r\nINFO: 0.00389769: 3 car\r\nINFO: 0.0038741: 2 bicycle\r\n\r\n\r\nAttached the model, scripts and labels file.,\r\n[labelmap.txt](\r\n[New folder.zip](https://github.com/tensorflow/tensorflow/files/8405110/New.folder.zip)\r\nhttps://github.com/tensorflow/tensorflow/files/8405102/labelmap.txt)\r\n[labelmap.txt](https://github.com/tensorflow/tensorflow/files/8405103/labelmap.txt)\r\n\r\n", "comments": ["> **Describe the expected behavior** Match the performance of python with C++. Give an API or directly automate the setting\r\n> the threads without manual change. What is the backend used for python and C++ ? Are they same ? Can we expect the \r\n> performance proportionately based on the threads. \r\n> Suppose when threads are set to 2, can we expect 2x performance than 1 thread ?\r\n\r\n1. Python binding actually uses the C++ implementation of Tensorflow Lite. Considering the Python runtime overhead itself, I would expect, under the same running environment, the Python performance should be no better than that of the C++ one.\r\n2. Linear scaling w/ threads isn't guaranteed at all in practice due to various factors. For example, if some TFLite op in the graph isn't parallelized, then running with multiple threads on the op doesn't help at all. In addition, the multithreading performance could suffer if there's resource contention on CPU cores.\r\n\r\nBtw, you could use the [TFLite benchmark tool](https://www.tensorflow.org/lite/performance/measurement) to measure the performance of your model. In practice, the overall performance could be further impacted by other components of your inference binary, including the data pre-processing, data post-processing etc.\r\n\r\n\r\n\r\n", "But based on the attached files, Its clearly having difference between python and C++", "I have run the benchmark tool for tensorflow 2.8.0 and still got less performance than python.\r\n![Capture2](https://user-images.githubusercontent.com/37923435/162717140-c805cdaf-ebd8-43cf-a16d-4aa1d4a7d902.PNG)\r\n![Capture1](https://user-images.githubusercontent.com/37923435/162717148-881f69cd-4cfb-4cba-8562-5de0c6679e83.PNG)\r\n\r\n", "> I have run the benchmark tool for tensorflow 2.8.0 and still got less performance than python. ![Capture2](https://user-images.githubusercontent.com/37923435/162717140-c805cdaf-ebd8-43cf-a16d-4aa1d4a7d902.PNG) ![Capture1](https://user-images.githubusercontent.com/37923435/162717148-881f69cd-4cfb-4cba-8562-5de0c6679e83.PNG)\r\n\r\nCould you also paste the performance of the corresponding Python program here? In addition, what are the performances when num_threads is set to 1, 2, 4 respectively? Thx!\r\n\r\nBtw, which hardware platform are you running the performance test?", "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Linux buster\r\n- TensorFlow installed from (source or binary): 2.8.0\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n\r\n`cat /sys/devices/system/cpu/smt/active`\r\n      1\r\n      Hyper threading is enabled.\r\n\r\n` sudo dmidecode -t processor | grep Count`\r\n\tCore Count: 6\r\n\tThread Count: 12\r\n\r\nFor C++, I profiled using benchmark tool and also with label example, with loop count set to 100 and warm-up runs zero and threads ranging from -1 to 12 and  >12 .\r\n\r\n`bazel-4.2.1 build -c opt //tensorflow/lite/examples/label_image:label_image`\r\n`bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model detect.tflite --labels labelmap.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp`\r\n\r\n### C++ (2.8.0)\r\n-1 -- 306.917 ms  \r\n0  -- 122.151 ms (Randomly getting cores )\r\n1  -- 293.365 ms  \r\n2  -- 155.565 ms \r\n3  -- 105.54 ms  \r\n4  -- 82.3551 ms\r\n5  -- 98.8991 ms\r\n6  -- 64.5327 ms \r\n7  -- 103.385 ms \r\n8  -- 98.1083 ms  \r\n9  --  87.2203 ms \r\n10 --  76.0722 ms \r\n11 --  63.1864 ms \r\n12 --  71.3735 ms\r\n13 -- 131.074 ms  (Randomly getting cores when greater than threads given)\r\n14 -- 133.658 ms \r\n15 -- 134.247 ms\r\n\r\n### Python Performance for threads (2.8.0) (Ran for 100 runs)\r\n\r\nTFLite 2.8.0 Python\r\n\r\n-1 -- unable to set\r\n0 -- unable to set\r\n1 --  67.28704053 ms\r\n2 --  37.27101533 ms\r\n3 --  24.89689852 ms\r\n4 --  19.27222278 ms \r\n5 --  16.82612416 ms\r\n6 --  14.75332775 ms \r\n7 --  26.53925619 ms\r\n8 --  22.71062904 ms\r\n9 --  20.09229901 ms\r\n10 -- 17.41771392 ms\r\n11 -- 14.67389033 ms\r\n12 -- 14.79799008 ms\r\n13 --  81.4597831 ms (Randomly pick threads)\r\n14 --   76.92194408 ms \r\n15 --  76.67167669 ms\r\n\r\nThese are the observations when all are closed in my pc except profiler.\r\nI have the following queries\r\n\r\n\r\n1) **As per this issue(https://github.com/tensorflow/tensorflow/issues/46272) It is mentioned when the number of threads in c++ is set to -1 all threads will be used, But it's not happening and there is a clear performance difference.**\r\n\r\n2) **What is meant by threads set as zero, what is ideal behavior in c++?** \r\n\r\n3) **Why the performance in C++ is random, and in my case, there is a drop in performance after 6 threads (My pc has 6 cores) and an increase happened when it reaches almost 11 or 12 threads. Is this behavior expected? There is no proportional increase in performance w.r.t to threads set .**\r\n\r\n  **This is similar even in python until 6 there is an increase, But there is a drop in performance, and eventually, performance increases when it reaches the max number of threads which is 12. What is the reason behind it?**\r\n\r\n4) **Can we expect a proportional increase in performance based on the threads set?**\r\n\r\nlscpu info file and scripts for python benchmarking and model are attached in this thread.\r\n\r\n\r\n", "Any update", "> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Linux buster\r\n> * TensorFlow installed from (source or binary): 2.8.0\r\n> * TensorFlow version (use command below): 2.8.0\r\n> * Python version: 3.7.3\r\n> * Bazel version (if compiling from source): 4.2.1\r\n> * GCC/Compiler version (if compiling from source): 8.3.0\r\n> \r\n> `cat /sys/devices/system/cpu/smt/active` 1 Hyper threading is enabled.\r\n> \r\n> ` sudo dmidecode -t processor | grep Count` Core Count: 6 Thread Count: 12\r\n> \r\n> For C++, I profiled using benchmark tool and also with label example, with loop count set to 100 and warm-up runs zero and threads ranging from -1 to 12 and >12 .\r\n> \r\n> `bazel-4.2.1 build -c opt //tensorflow/lite/examples/label_image:label_image` `bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model detect.tflite --labels labelmap.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp`\r\n> \r\n> ### C++ (2.8.0)\r\n> -1 -- 306.917 ms 0 -- 122.151 ms (Randomly getting cores ) 1 -- 293.365 ms 2 -- 155.565 ms 3 -- 105.54 ms 4 -- 82.3551 ms 5 -- 98.8991 ms 6 -- 64.5327 ms 7 -- 103.385 ms 8 -- 98.1083 ms 9 -- 87.2203 ms 10 -- 76.0722 ms 11 -- 63.1864 ms 12 -- 71.3735 ms 13 -- 131.074 ms (Randomly getting cores when greater than threads given) 14 -- 133.658 ms 15 -- 134.247 ms\r\n> \r\n> ### Python Performance for threads (2.8.0) (Ran for 100 runs)\r\n> TFLite 2.8.0 Python\r\n> \r\n> -1 -- unable to set 0 -- unable to set 1 -- 67.28704053 ms 2 -- 37.27101533 ms 3 -- 24.89689852 ms 4 -- 19.27222278 ms 5 -- 16.82612416 ms 6 -- 14.75332775 ms 7 -- 26.53925619 ms 8 -- 22.71062904 ms 9 -- 20.09229901 ms 10 -- 17.41771392 ms 11 -- 14.67389033 ms 12 -- 14.79799008 ms 13 -- 81.4597831 ms (Randomly pick threads) 14 -- 76.92194408 ms 15 -- 76.67167669 ms\r\n> \r\n> These are the observations when all are closed in my pc except profiler. I have the following queries\r\n> \r\n> 1. **As per this issue([use of interpreter-->SetNumThreads. Do we need to invoke setNumThreads always to improve performance\u00a0#46272](https://github.com/tensorflow/tensorflow/issues/46272)) It is mentioned when the number of threads in c++ is set to -1 all threads will be used, But it's not happening and there is a clear performance difference.**\r\n\r\nWhen it's set to -1, in general, the actual number of threads used is 1 because it's the safest choice on mobile phones in practice so that we could avoid resource contention with other activities on the phone.\r\n\r\n> 2. **What is meant by threads set as zero, what is ideal behavior in c++?**\r\n\r\nWhen it's set to 0, it should mean to disable multithreading as noted [here](https://github.com/tensorflow/tensorflow/blob/7a99ce11636fb7e0c974d4d9b8b5dbff7f259c87/tensorflow/lite/interpreter.h#L541-L559).\r\n\r\n> 3. **Why the performance in C++ is random, and in my case, there is a drop in performance after 6 threads (My pc has 6 cores) and an increase happened when it reaches almost 11 or 12 threads. Is this behavior expected? There is no proportional increase in performance w.r.t to threads set .**\r\n> \r\n> **This is similar even in python until 6 there is an increase, But there is a drop in performance, and eventually, performance increases when it reaches the max number of threads which is 12. What is the reason behind it?**\r\n> \r\n\r\nI think this is largely because of the over-subscription on the underlying cores (i.e. severe computing resource contention) by the computing inference threads. As a result, performance decrease is expected.\r\n\r\n> 4. **Can we expect a proportional increase in performance based on the threads set?**\r\n\r\nI hope my [earlier comment](https://github.com/tensorflow/tensorflow/issues/55476#issuecomment-1089729503) could clarify this.\r\n\r\n> \r\n> lscpu info file and scripts for python benchmarking and model are attached in this thread.\r\n\r\nLastly, regarding the intriguing performance difference reported here, after looking at the additional details you've provided, I'm now thinking whether it could be caused by the compiler or not as there's a performance-difference issue before that's solved by upgrading the compiler to gcc 9.0+ (you used gcc 8.3 here in this case?).  \r\n\r\nMy guess is based on the assumption that you compiled the C++ lib/binary from source while using the Python library from the official TF 2.8.0 release.\r\n\r\nTo help validate the guess, could you try the following (from being easy to being complex):\r\n* Download the pre-built [latest linux_x86-64 benchmark binary](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model) to benchmark the detect model and check whether there are performance differences? More options for the benchmark tool could be found [here](https://www.tensorflow.org/lite/performance/measurement#download_or_build_the_binary).\r\n* As your model is a quantized model (uint8), could you try run with the corresponding non-quantized fp32 model? Just wondering the performance difference also occurs with the fp32 model.\r\n* Also build the TFLite python library from source using the same building env? Or use the TF dev/nightly docker (which includes newer compilers) to build the lib/binary?\r\n\r\nAgain, many thanks for reporting the issue and the detailed description earlier!\r\n\r\n\r\n"]}, {"number": 55474, "title": "Using tf.data.Dataset.list_files prints \"unshardable source dataset\" warning", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: 3.8.10\r\n\r\n**Describe the current behavior**\r\n\r\nUsing distributed strategy and `tf.data.Dataset.list_files` prints a `unshardable source dataset` warning.\r\n\r\n**Describe the expected behavior**\r\nAPI should detect that the source dataset is files and can be sharded. \r\n\r\nReference: https://www.tensorflow.org/tutorials/distribute/input\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nstrategy = tf.distribute.MirroredStrategy()\r\nds = tf.data.Dataset.list_files(\".*\", shuffle=False).map(lambda x: (tf.strings.length(x), tf.strings.length(x)))\r\nwith strategy.scope():\r\n  dummy_model = tf.keras.Sequential()\r\n  dummy_model.add(tf.keras.layers.Dense(1, input_shape=(1,)))\r\n  dummy_model.compile(loss=\"mse\")\r\ndummy_model.fit(ds.batch(4))\r\n```\r\n```\r\n2022-04-03 16:22:04.058728: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\r\nop: \"TensorSliceDataset\"\r\ninput: \"Placeholder/_0\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_STRING\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_cardinality\"\r\n  value {\r\n    i: 1\r\n  }\r\n}\r\nattr {\r\n  key: \"is_files\"\r\n  value {\r\n    b: false\r\n  }\r\n}\r\nattr {\r\n  key: \"metadata\"\r\n  value {\r\n    s: \"\\n\\026TensorSliceDataset:350\"\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\nexperimental_type {\r\n  type_id: TFT_PRODUCT\r\n  args {\r\n    type_id: TFT_DATASET\r\n    args {\r\n      type_id: TFT_PRODUCT\r\n      args {\r\n        type_id: TFT_TENSOR\r\n        args {\r\n          type_id: TFT_STRING\r\n        }\r\n      }\r\n    }\r\n  }\r\n  args {\r\n    type_id: TFT_DATASET\r\n    args {\r\n      type_id: TFT_PRODUCT\r\n      args {\r\n        type_id: TFT_TENSOR\r\n        args {\r\n          type_id: TFT_STRING\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```", "comments": ["Hello @dmho418 ,\r\n\r\nExample mentioned in https://www.tensorflow.org/tutorials/distribute/input#sharding demonstrates how to set the sharding policy:\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensors(([1.],[1.])).repeat(64).batch(16)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.FILE\r\ndataset = dataset.with_options(options)\r\n```\r\n\r\nThis will work as long as the dataset starts with list of files. If the dataset doesn't start with list of files, you will face an error along the lines of `Found an unshardable source dataset: name: \"foo\"`.\r\nAlso please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/45157#issuecomment-788376526) from the similar issue by google developer.Thanks!", "Hi @tilakrayal\r\n\r\nFrom https://www.tensorflow.org/tutorials/distribute/input#sharding doesn't that mean that the default policy AUTO would  shard by FILE automatically if a file-based dataset is detected?\r\n\r\nIn https://github.com/tensorflow/tensorflow/issues/45157#issuecomment-788376526 I think it's a different issue. There the user manually iterating the dataset and passing tensors to `train_on_batch`, so it makes sense that Tensorflow doesn't know about the dataset. ", "When the `AutoShardPolicy` is set, you can choose multiple options to perform shard from [here](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy). \r\nWhen the `AutoShardPolicy` is set to `Auto`, it tries to apply `FILE` based shard, when it fails to do so it throws warning and proceeds to apply `DATA` shard.\r\nBelow is the code reference to show how each case is handled.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/data/auto_shard.cc#L763-L786"]}, {"number": 55471, "title": "difference Between the output of conv1d with same input and weights for tf2.3 and tf2.6", "body": "Hello, I had met a problem just like the title. The inputs and weights  are same, use difference tf's version 2.3 and 2.6, I got difference outputs. Is this caused by precision\r\n![clipboard_image_1648902447965](https://user-images.githubusercontent.com/20295041/161383301-524ba7bc-e9ef-4ac3-a5aa-a69566420557.png)\r\n![clipboard_image_1648902397799](https://user-images.githubusercontent.com/20295041/161383307-36ef8a79-fd5a-446f-bf6c-874107116a7a.png)\r\n?", "comments": ["@zhazl ,\r\nIn order to expedite the trouble-shooting process, I request you, please provide a complete code you are using.Thanks!,", "![image](https://user-images.githubusercontent.com/20295041/161932162-e873634b-49a0-4971-bc75-e8b874d704f5.png)\r\nThanks for response. The code I used as follows. Inputs x  and weights are all same, but I got diffenrent results.", "> @zhazl , In order to expedite the trouble-shooting process, I request you, please provide a complete code you are using.Thanks!,\r\n\r\nI wonder the method to realize Conv1d between the version of tf about how to process value in type of float32.  Is tf2.6 internally implemented based on the double type?", "@zhazl ,\r\nI request you can you please provide the code in reproducible format or provide the colab gist.It helps to debug the issue.Thanks!", "@zhazl You are right. **The results are slightly different due to different degrees of precision.** The are no changes in implementation in Conv1D in [ Tensorflow 2.3 ](https://github.com/tensorflow/tensorflow/blob/7462dcaae1e8cfe1dfd0c62dd6083f9749a9d827/tensorflow/python/keras/layers/convolutional.py#L373) and [Tensorflow 2.6](https://github.com/tensorflow/tensorflow/blob/f06f10ddd92230a42992f75a2e13a301c20de8d4/tensorflow/python/keras/layers/convolutional.py#L384) ", "@gowthamkpr I wonder the detail of the process leads to the degree's difference. Is there any detailed code ?", "@zhazl Sorry for digressing from our conversation. Tried with the latest versions of tensorflow aka (2.7 and 2.8) and this issue doesn't persist. Look at my gists here for tensorflow [2.7](https://colab.research.google.com/gist/gowthamkpr/164005c68a52c075b5d704a1ef2362f9/untitled323.ipynb) and [2.8](https://colab.research.google.com/gist/gowthamkpr/28a2ff3c0394c999ac7bba70f1e2e990/untitled.ipynb).\r\n\r\nPlease try latest version of tensorflow and check if your issue still persists. Thanks!\r\n\r\n", "@zhazl ,\r\nAs suggested above request you to please try in latest version of tensorflow 2.8 and check if your issue still persists. Thanks!", "@tilakrayal @gowthamkpr Ok, thanks for your solution. It's ok. Thanks again!", "@zhazl ,\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status"]}, {"number": 55466, "title": "undefined symbol when linking tf code with custom op", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AmazonLinux2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): installed through pip\r\n- TensorFlow version: 2.8\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Nvidia V100\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHi,\r\n\r\nI'm writing a custom op and I'd like to reuse an existing tf function `tensorflow::functor::DenseUpdate<>` for updating `tf.Variable`. I am able to compile my custom op successfully but I get the following error at runtime when I try to load my op in python:\r\n\r\n```\r\nundefined symbol: _ZN10tensorflow7functor11DenseUpdateIN5Eigen9GpuDeviceEdLNS_15DenseUpdateTypeE2EEclERKS3_NS2_9TensorMapINS2_6TensorIdLi1ELi1ElEELi16ENS2_11MakePointerEEENS8_INS9_IKdLi1ELi1ElEELi16ESB_EE\r\n```\r\n\r\nI am able to find this symbol in `_pywrap_tensorflow_internal.so` library which gets loaded when I import tensorflow.\r\n```\r\n$ nm -gD /home/ec2-user/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so | grep _ZN10tensorflow7functor11DenseUpdateIN5Eigen9GpuDevice\r\n_ZN10tensorflow7functor11DenseUpdateIN5Eigen9GpuDeviceEdLNS_15DenseUpdateTypeE2EEclERKS3_NS2_9TensorMapINS2_6TensorIdLi1ELi1ElEELi16ENS2_11MakePointerEEENS8_INS9_IKdLi1ELi1ElEELi16ESB_EE\r\n```\r\n\r\nCan someone help point out what could be an issue here?\r\n\r\nThanks\r\n\r\n", "comments": ["I believe this issue is somewhat related to https://github.com/tensorflow/tensorflow/issues/25513. I don't see any solution there as well.", "@pranavladkat, \r\nHi Thanks for reporting this issue.\r\nCould you tell us the GCC version that you are using. Thanks!", "Hi, thanks for the response. I'm using GCC 9.3.0.\r\n\r\nThanks", "@pranavladkat, \r\nHi Thanks for the information. \r\nLooks like issue with Tensorflow build. Can you try the workaround mention here [#48064](https://github.com/tensorflow/tensorflow/issues/48064#issuecomment-811554566). Delete `/root/.cache/bazel` to force it to rebuild from scratch. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55461, "title": "Possibly platform related tflite conversion issue", "body": "### 1. System information\r\n\r\n#### (1) With issues in:\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS Monterey 12.2.1 (m1)`\r\n- TensorFlow installation (pip package or built from source):  following https://developer.apple.com/metal/tensorflow-plugin/\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): \r\n```\r\ntensorflow-deps==2.8.0\r\ntensorflow-graphics==2021.12.3\r\ntensorflow-macos==2.8.0\r\ntensorflow-metal==0.4.0  \r\n```\r\n\r\n#### (2) Without issues in:\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Arch Linux x86_64 (5.16.16-arch1-1)`\r\n- TensorFlow installation (pip package or built from source):  `pip`\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): `2.8.0`\r\n\r\n### 2. Code\r\nWritten to match [this example](https://www.tensorflow.org/graphics/api_docs/python/tfg/math/optimizer/levenberg_marquardt/minimize#examplesl) as much as possible.\r\n\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow_graphics.math.optimizer import levenberg_marquardt\r\n\r\n\r\ndef convert_to_tflite(\r\n    model_path: str,\r\n) -> bytes:\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n        tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.\r\n    ]\r\n\r\n    tflite_model = converter.convert()\r\n    assert isinstance(tflite_model, bytes)\r\n\r\n    return tflite_model\r\n\r\n\r\ndef tflite_inference(\r\n    inputs: list[tf.Tensor],\r\n    tflite_model: bytes,\r\n) -> list:\r\n    # create interpreter\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n    # set inputs\r\n    input_details = interpreter.get_input_details()\r\n    for input_tensor, input_placeholder in zip(inputs, input_details):\r\n        interpreter.set_tensor(input_placeholder[\"index\"], input_tensor)\r\n\r\n    # invoke interpreter\r\n    interpreter.invoke()\r\n\r\n    # return outputs\r\n    output_details = interpreter.get_output_details()\r\n    return [interpreter.get_tensor(output[\"index\"]) for output in output_details]\r\n\r\n\r\ndef create_lmo_model():\r\n    def f1(x, y):\r\n        return x + y\r\n\r\n    def f2(x, y):\r\n        return x * y\r\n\r\n    class LMO(tf.keras.layers.Layer):\r\n        def call(self, inputs):\r\n            _, (r1, r2) = levenberg_marquardt.minimize(\r\n                residuals=(f1, f2),\r\n                variables=inputs,\r\n                max_iterations=10,\r\n            )\r\n            return [r1, r2]\r\n\r\n    input_x = tf.keras.Input(\r\n        shape=(1, 2),\r\n    )\r\n    input_y = tf.keras.Input(\r\n        shape=(3, 1),\r\n    )\r\n    output = LMO()([input_x, input_y])\r\n\r\n    return tf.keras.Model(\r\n        inputs=[input_x, input_y],\r\n        outputs=output,\r\n    )\r\n\r\n\r\ndef main():\r\n    tf.random.set_seed(5)\r\n    x = tf.random.uniform((1, 1, 2))\r\n    y = tf.random.uniform((1, 3, 1))\r\n    inputs = [x, y]\r\n\r\n    model = create_lmo_model()\r\n    print(model(inputs))\r\n\r\n    tf.saved_model.save(\r\n        model,\r\n        \"lmo_model\",\r\n    )\r\n\r\n    tflite_model = convert_to_tflite(\"lmo_model\")\r\n    print(\r\n        tflite_inference(\r\n            tflite_model=tflite_model,\r\n            inputs=inputs,\r\n        )\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### 3. Failure after conversion\r\nOn platform 1 tflite interpreter fails to invoke with error:\r\n```bash\r\nRuntimeError: Input matrix is not invertible.\r\n\t (while executing 'MatrixTriangularSolve' via Eager)Node number 15 (TfLiteFlexDelegate) failed to invoke.Node number 104 (IF) failed to invoke.Node number 10 (WHILE) failed to invoke.\r\n```\r\nOn platform 2 everything works as expected.", "comments": ["Hi @sachinprasadhs ! Could you look at this issue? I modified the original code run in colab environment  . The issue is surfacing in 2nd inference (not in 1st inference). Attaching gist in [2.7](https://colab.sandbox.google.com/gist/mohantym/9a726f539a17b8b28d20d1377422f13e/git_55461.ipynb#scrollTo=Zxg4aODy_cjj), [2.8](https://colab.sandbox.google.com/gist/mohantym/301325009d7dce8b62dd6de966ecce7d/git_55461.ipynb#scrollTo=lxQY9XhqwTcb) and [nightly](https://colab.sandbox.google.com/gist/mohantym/8bdfdc3e8368fd4d448a0c2aa56768c3/git_55461.ipynb#scrollTo=Zxg4aODy_cjj). Thanks!"]}, {"number": 55455, "title": "GRU performance severely degraded inside tf.function with Apple m1 chip", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS Monterey 12.3, Metal device set to: Apple M1 Pro\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version:\r\ntensorflow-deps           2.7.0                \r\ntensorflow-macos          2.8.0              \r\ntensorflow-metal          0.4.0 \r\n- Python version:\r\n3.9.12\r\n- GPU model and memory:\r\n Apple M1 Pro\r\n\r\n**Describe the current behavior**\r\n\r\nI run this simple code with a GRU layer with a `tf.function` decorator:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\na = tf.random.truncated_normal([4, 4, 4])\r\nlayer = tf.keras.layers.GRU(4) \r\n\r\n@tf.function\r\ndef f(a):\r\n    return layer(a)\r\n\r\nstart = time()\r\nfor _ in range(1000):\r\n    with tf.GradientTape() as tape:\r\n        b = f(a)\r\nprint(str(time() - start), \"seconds\")\r\n```\r\nits much slower (~5-10x times) than running in the eager mode. However, this bug only shows up for recurrent layers. When using Dense, the `tf.function` mode is faster than the eager mode as expected. The issue also disappeared outside `tf.GradientTape()`.\r\nI only encountered this problem in my Apple Macbook Pro with M1 chip. I tried it on a linux machine and it's ok.\r\n\r\n**Describe the expected behavior**\r\n`tf.function` should be faster (at least not several times slower) than the eager mode.\r\n\r\n**Standalone code to reproduce the issue**\r\nIt cannot be reproduced on a linux machine, so no Colab notebook is available.\r\n\r\n**Other info / logs** \r\n\r\nFYI the code above runs with the warning message as follows:\r\n\r\n> 2022-03-31 14:40:34.462151: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\r\n2022-03-31 14:40:34.463604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\r\n2022-03-31 14:40:34.480917: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.\r\n2022-03-31 14:40:34.487243: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] tfg_optimizer{} failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.\r\n\twhen importing GraphDef to MLIR module in GrapplerHook\r\n2022-03-31 14:40:34.488903: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.\r\n2022-03-31 14:40:34.494395: W tensorflow/core/common_runtime/process_function_library_runtime.cc:932] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.\r\n2022-03-31 14:40:34.508855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.", "comments": ["@David-Mao I tried to reproduce the issue on colab using 2.8.0 , tf-nightly and didn't face the error reported.Could you please have a look at the gist [here](https://colab.research.google.com/gist/sushreebarsa/77091b03d1dcac212375cdf89b2b90d0/55455.ipynb) and confirm the same?Thanks!", "> \r\n\r\n@sushreebarsa As I said in the problem description, this problem is reproducible on Apple m1 chip machine (like a recent macbook Pro).  I think Colab uses Linux machine, which works fine. ", "@David-Mao, \r\nThanks for reporting this issue. \r\nI tried on macOS Monterey v12.3. \r\n\r\n**@tf.function**\r\n```\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\na = tf.random.truncated_normal([4, 4, 4])\r\nlayer = tf.keras.layers.GRU(4) \r\n\r\n@tf.function\r\ndef f(a):\r\n    return layer(a)\r\n\r\nstart = time()\r\nfor _ in range(1000):\r\n    with tf.GradientTape() as tape:\r\n        b = f(a)\r\nprint(str(time() - start), \"seconds\")\r\n```\r\n**Output**\r\n`16.70040798187256 seconds`\r\n\r\n**Eager execution**\r\n```\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\na = tf.random.truncated_normal([4, 4, 4])\r\nlayer = tf.keras.layers.GRU(4) \r\n\r\ndef f(a):\r\n    return layer(a)\r\n\r\nstart = time()\r\nfor _ in range(1000):\r\n    with tf.GradientTape() as tape:\r\n        b = f(a)\r\nprint(str(time() - start), \"seconds\")\r\n```\r\n\r\n**Output**\r\n\r\n`17.127399921417236 seconds`", "@gadagashwini On my computer they took 2.0382091999053955 seconds vs 11.459597826004028 seconds respectively. In fact, for such a small code, if it took you 17 seconds, then there is a performance bug anyway, as it's way too slower than normal. \r\n\r\nPlease notice that in the gist [above](https://colab.research.google.com/gist/sushreebarsa/77091b03d1dcac212375cdf89b2b90d0/55455.ipynb) (given by @sushreebarsa ) it took 2.979365348815918 seconds on a colab machine. I think if there weren't any bug it shouldn't take 17 seconds on any modern machine, no matter on which OS.\r\n", "@David-Mao, \r\nTime 16 and 17 seconds is because i executed code on terminal directly. \r\nWhen I run .py file it took\r\n**@tf.function**\r\n1.9312949180603027 seconds\r\n\r\n**Eager execution** \r\n4.352757930755615 seconds ", "> \r\n\r\nSorry I didn't know that. Are you running on a M1 chip machine? I encounter this problem on the M1 arm chip Mac.", "@David-Mao,\r\nYes I am running on Mac M1 chip. \r\nPlease do check with tf-nightly version. Let us know if you observe same behaviour. Thanks!", "> @David-Mao, Yes I am running on Mac M1 chip. Please do check with tf-nightly version. Let us know if you observe same behaviour. Thanks!\r\n\r\n@gadagashwini  Is there a nightly tensorflow for M1 chip? I installed my tensorflow from https://pypi.org/project/tensorflow-macos/, which only has 2.8.0 as the newest version. I checked https://pypi.org/project/tf-nightly/, but that's not for M1 chip. Could you please tell me what's the correct channel to install nightly version for m1 chip? Thank you so much.", "@David-Mao, Now the latest stable Tensorflow version is 2.8.0. Once the new release done we can expect Tf 2.9. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55452, "title": "TensorRT conversion fails with 5D input data", "body": "Hi, I'm having an issue with tensorrt conversion of model which uses 3D convolutions and processes 5D input.\r\n\r\nCode to reproduce error (I cut the model to minimal example):\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Input,Conv3D,BatchNormalization,Activation\r\n\r\nprint(\"TensorFlow version: \",tf.version.VERSION)\r\n\r\nsrc_dir='model_bn'\r\ndst_dir='model_bn_conv'\r\nbatch=4\r\ninput_shape=(24,160,160,3)\r\n\r\ndef input_fn():\r\n    yield [np.zeros(((batch,)+input_shape),dtype='float32')]\r\n\r\nmodel=Sequential()\r\nmodel.add(Input(shape=input_shape))\r\nmodel.add(Conv3D(24,3))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\n\r\nmodel.save(src_dir)\r\n\r\nparams=tf.experimental.tensorrt.ConversionParams(precision_mode='FP32')\r\nconverter=tf.experimental.tensorrt.Converter(input_saved_model_dir=src_dir,conversion_params=params)\r\n\r\nconverter.convert()\r\nprint('Converted successfully!')\r\n\r\nprint('Running build()...')\r\nconverter.build(input_fn=input_fn)\r\nprint('Build completed')\r\nconverter.save(dst_dir)\r\n```\r\n\r\nThe output:\r\n```\r\nTensorFlow version:  2.8.0\r\n2022-03-31 11:44:04.711186: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-03-31 11:44:05.311164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13608 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\n2022-03-31 11:44:05.990618: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2022-03-31 11:44:06.422604: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-03-31 11:44:06.422784: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-03-31 11:44:06.458670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13608 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5\r\n2022-03-31 11:44:06.485282: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1191] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 30 nodes (20), 41 edges (31), time = 1.05ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.01ms.\r\n\r\n2022-03-31 11:44:06.529996: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-03-31 11:44:06.530104: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-03-31 11:44:06.539721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13608 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5\r\n2022-03-31 11:44:06.551640: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:192] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.\r\n2022-03-31 11:44:06.551665: I tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:211] [TF-TRT] not using explicit QDQ mode\r\n2022-03-31 11:44:06.552184: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:954]\r\n\r\n################################################################################\r\nTensorRT unsupported/non-converted OP Report:\r\n        - NoOp -> 2x\r\n        - Identity -> 1x\r\n        - Placeholder -> 1x\r\n--------------------------------------------------------------------------------\r\n        - Total nonconverted OPs: 4\r\n        - Total nonconverted OP Types: 3\r\nFor more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\r\n################################################################################\r\n\r\n2022-03-31 11:44:06.552276: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1281] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 1 TRT Engines with  at least minimum_segment_size=3 nodes.\r\n2022-03-31 11:44:06.552292: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:795] Number of TensorRT candidate segments: 1\r\n2022-03-31 11:44:06.552678: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 0 consisting of 11 nodes by TRTEngineOp_0_0.\r\n2022-03-31 11:44:06.556775: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1191] Optimization results for grappler item: tf_graph\r\n  model_pruner: Graph size after: 22 nodes (-8), 33 edges (-8), time = 0.273ms.\r\n  debug_stripper: debug_stripper did nothing. time = 0.008ms.\r\n  layout: Graph size after: 26 nodes (4), 37 edges (4), time = 1.157ms.\r\n  dependency_optimizer: Graph size after: 16 nodes (-10), 15 edges (-22), time = 0.228ms.\r\n  constant_folding: Graph size after: 16 nodes (0), 15 edges (0), time = 0.564ms.\r\n  common_subgraph_elimination: Graph size after: 13 nodes (-3), 15 edges (0), time = 0.124ms.\r\n  TensorRTOptimizer: Graph size after: 3 nodes (-10), 2 edges (-13), time = 1.148ms.\r\n  constant_folding: Graph size after: 3 nodes (0), 2 edges (0), time = 0.294ms.\r\nOptimization results for grappler item: TRTEngineOp_0_0_native_segment\r\n  model_pruner: Graph size after: 13 nodes (0), 15 edges (0), time = 0.083ms.\r\n  debug_stripper: debug_stripper did nothing. time = 0.006ms.\r\n  layout: Graph size after: 13 nodes (0), 15 edges (0), time = 0.4ms.\r\n  dependency_optimizer: Graph size after: 12 nodes (-1), 13 edges (-2), time = 0.116ms.\r\n  constant_folding: Graph size after: 12 nodes (0), 13 edges (0), time = 0.366ms.\r\n  common_subgraph_elimination: Graph size after: 12 nodes (0), 13 edges (0), time = 0.093ms.\r\n  TensorRTOptimizer: Graph size after: 12 nodes (0), 13 edges (0), time = 0.013ms.\r\n  constant_folding: Graph size after: 12 nodes (0), 13 edges (0), time = 0.346ms.\r\n\r\nConverted successfully!\r\nRunning build()...\r\n2022-03-31 11:44:06.640977: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:446] TRTEngineOp not using explicit QDQ\r\n2022-03-31 11:44:06.643431: I tensorflow/compiler/tf2tensorrt/common/utils.cc:100] Linked TensorRT version: 8.2.3\r\n2022-03-31 11:44:06.643602: I tensorflow/compiler/tf2tensorrt/common/utils.cc:102] Loaded TensorRT version: 8.2.3\r\n2022-03-31 11:44:07.618558: F ./tensorflow/compiler/tf2tensorrt/utils/trt_tensor_proxy.h:158] 'trt_tensor_' Must be non NULL\r\nAborted\r\n```\r\n\r\nOther observations:\r\n- Removing activation layer or swapping it with BatchNorm makes script to execute without errors, but doesn't produce valid trt engine. It writes the following warning: \"TF-TRT Warning: Engine creation for TRTEngineOp_0_0 failed. The native segment will be used instead. Reason: INVALID_ARGUMENT: Rank of perm for transpose does not match with that of the input.\"\r\n- Removing Conv3D layer does not change the result.\r\n- Trying to tweak conversion parameters did not help.\r\n- Using 4D input data and 2D convolutions seem to work fine.\r\n\r\n**System information**\r\n(Using NVidia NGC container for Tensorflow, version 22.03)\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.8.0**\r\n- Python version: **3.8.10**\r\n- CUDA/cuDNN version: **11.6/8.3.3**\r\n- GPU model and memory: **NVidia Tesla T4 16Gb**\r\n", "comments": ["@sachinprasadhs I was able to reproduce this issue on colab using TF v2.8.0 & tf-nightly ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/8a0c18310a75b69758529ee9a8a327ab/55452.ipynb#scrollTo=fZq3m4XP3WxG).Thanks!"]}, {"number": 55448, "title": "X86_64 - TFLite different inference results depending on whether it was compiled with XNNpack delegate or not", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **_Yes_**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **_4.19.0-18-amd64 1 SMP Debian 4.19.208-1 (2021-09-29) x86_64 GNU/Linux_**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **_source_**\r\n- TensorFlow version (use command below): **_r2.5, r2.8_**\r\n- Python version: **_3.7.3_**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): **gcc 8.3.0**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nRunning inference in CPP using tensorflow lite library compiled from source using CMake with default settings with XNNPACK support produce wrong results:\r\n```[0, 0, 1, 0, ]```\r\n If tflite is sompiled with disabled XNNPACK support (`cmake -D TFLITE_ENABLE_XNNPACK=OFF ..`) - model works as expected.\r\nIf inference is running from python with XNNPACK support (based on console output - it prints `INFO: Created TensorFlow Lite XNNPACK delegate for CPU.`) - it is provide expected results.\r\n```[1, 0, 0, 0, ]```\r\n+ if we run inference from python with tensorflow==2.5.0 it doesn't show string above in regards of XNNPACK but produce valid results as well.\r\n\r\n**Describe the expected behavior**\r\nTFlite compiled with XNNPACK in cpp produce the same result as library without XNNPACK\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/kruglov-dmitry/tensorflow_lite_from_cpp_python\r\n\r\nRepository contains:\r\n- tflite model and test image to run inference\r\n- cpp source, makefile and build instructions\r\n- python source", "comments": ["Currently, we use XNNPACK for TFLite floating-point ops. Comparing with the TFLite that doesn't use XNNPACK, we expect very similar accuracy but won't expect bitwise identical results.\r\n\r\nAs for your case, what're the input values of the calculation that produces the final integer vector? Do they differ a lot between TFLite w/ XNNPACK and TFLite w/o XNNPACK?", "Sorry for delay with answer.\r\n\r\n> Do they differ a lot between TFLite w/ XNNPACK and TFLite w/o XNNPACK?\r\n\r\nw/o XNNPACK:\r\n```\r\n7.89871e+36,\r\n-3.18431e+38,\r\n3.14411e+37,\r\n2.27216e+38,\r\n```\r\n\r\nw/ XNNPACK\r\n```\r\n-2.71105e+36,\r\n-inf,\r\n3.25951e+37,\r\n2.47283e+38,\r\n```\r\n\r\nsame exact model in python:\r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n[[ 1799.3785    582.42206 -1973.1395  -1173.8969 ]]\r\n```\r\n\r\nModel were produced by loading weights of keras model, pop last layer and converting using the same snippet as in repo above.\r\n\r\nModel without last layer - [google drive](https://drive.google.com/file/d/1Wk2MlEM_fPTre1AXaqPPplUY5bPYjM1L/view?usp=sharing) ~ 15 mb\r\n"]}, {"number": 55446, "title": "Checkpoint variable names are assigned \"variables/0/...\" etc if keras model has more than two nested layers/models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0 (gpu)\r\n- Python version: 3.10.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.5/8.3.1\r\n- GPU model and memory: 1080 Ti 12GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen a keras model has more than two nested layers/models the names of saved variables of inner layers get assigned incremental values like `variables/{0,1,2...}/.ATTRIBUTES/VARIABLE_VALUE` instead of `inner/layer/kernel/.ATTRIBUTES/VARIABLE_VALUE` for example. This does not happen in nested `tf.Module`s.\r\n\r\n**Describe the expected behavior**\r\n\r\nTo be honest I would expect both tensorflow and keras to respect the actual names of variables in `.name` attribute when saving to a checkpoint, like with TF1.x scopes; rather than this mess of `module_name/module_name/kernel/.ATTRIBUTES/VARIABLE_VALUE`. But if we have to accept the `.ATTRIBUTES/VARIABLE_VALUE` business, the variable names should still follow the nested module names rather than `variables/0/` etc.\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\nThe issue is caused when the `TrackableReference` chain to a variable is shorter through `variables/{index}` (2 jumps) than it is with `inner/layer/kernel` (3 jumps or more) during the breadth first traversal implemented here: https://github.com/tensorflow/tensorflow/blob/38976aedc4e9d9149bfe75f485be0dd833e9eec7/tensorflow/python/training/tracking/graph_view.py#L246\r\n\r\n**Solution**\r\n\r\nAdd\r\n```\r\nelif any([path_element.name in [\"variables\", \"trainable_variables\", \"non_trainable_variables\"] for path_element in node_paths[dependency]]):\r\n    node_paths[dependency] = node_paths[current_trackable] + (base.TrackableReference(name, dependency),)\r\n```\r\nafter https://github.com/tensorflow/tensorflow/blob/38976aedc4e9d9149bfe75f485be0dd833e9eec7/tensorflow/python/training/tracking/graph_view.py#L260\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass inner_block(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super().__init__(name=\"inner\")\r\n        self.layer = tf.keras.layers.Dense(2, name=\"inner_dense\") # kernel of this will save as \"variables/0/.ATTRIBUTES/VARIABLE_VALUE\"\r\n        self.var = tf.Variable([1.0, 2.0], name=\"inner_var\") # this will save as expected \"inner_m/var/.ATTRIBUTES/VARIABLE_VALUE\"\r\n\r\n    def call(self, x):\r\n        return self.layer(x) + self.var\r\n\r\nclass model(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__(name=\"outer\")\r\n        self.inner_m = inner_block()\r\n        self.dense = tf.keras.layers.Dense(2, name=\"outer_dense\") # kernel of this will save as expected \"dense/kernel/.ATTRIBUTES/VARIABLE_VALUE\"\r\n\r\n    def call(self, x):\r\n        x = self.inner_m(x)\r\n        x = self.dense(x)\r\n        return x\r\n\r\nm = model()\r\nm.build((None, 3))\r\nm(tf.random.normal((1, 3)))\r\n\r\nm.save(\"model\")  # or tf.saved_model.save(m, \"model\")\r\ntf.train.list_variables(\r\n    \"model/variables/variables\"\r\n)  # this lists 'variables/0/...' 'variables/1/...' along with the expected names\r\n```", "comments": ["@canbakiskan I tried to replicate the issue on colab using TF v2.8.0(gpu) and faced a warning message.Could you please find the [gist](https://colab.research.google.com/gist/sushreebarsa/3d2cc435f86e6e4a598c0e01b554c89e/55446.ipynb)  and confirm the same?\r\nThanks!", "Ok, when I used `save_traces=False` to turn those off in `m.save()` it does save with the proper names. I guess that was the issue. Is it saved like that intentionally when `save_traces=True`?", "@chunduriv I was able to replicate this issue onn colab using TF v2.8.0, tf-nightly,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/2402f679bf715cdfad81b49a90c6bef5/gist55446.ipynb#scrollTo=lOjG2SGMpT3E) for reference.Thanks! "]}, {"number": 55442, "title": "Pylint phantom checks", "body": "Some check that we have enabled in `.pylintrc` are silently not executed anymore.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fa6895a1564f14f488c5019039e3f0addef49f32/tensorflow/tools/ci_build/pylintrc#L48\r\n`python -m pylint --list-msgs | egrep \"indexing-exception|old-raise-syntax|W0311|W0312|C0330|C0301|C0326|W0611|W0622\"`\r\n\r\nWe have only 4/9 explicitly enabled checks that are still valid: \r\n```\r\n:line-too-long (C0301): *Line too long (%s/%s)*\r\n:bad-indentation (W0311): *Bad indentation. Found %s %s, expected %s*\r\n:unused-import (W0611): *Unused %s*\r\n:redefined-builtin (W0622): *Redefining built-in %r*\r\n```\r\nIt could require `black` to support some of these check again. \r\nPlease check https://github.com/tensorflow/tensorflow/pull/55396#issuecomment-1083685885\r\n\r\n/cc @mihaimaruseac @angerson @mdanatg", "comments": ["/cc @haifeng-jin as I suppose he has mirrored the same in Keras 20 days ago https://github.com/keras-team/keras/pull/16264"]}, {"number": 55441, "title": "[Saved Model] Loading Saved Model & GraphDef mostly doesn't respect device placement", "body": "In the context of TF-TRT, we need to be able to load and assign SavedModel and GraphDef to a specific `tf.device`. And as shown in this reproducer, virtually all calls lead to a loss/disrespect of the device information, and defaults back on `GPU:0` when actually executed.\r\n\r\nProblem: TF-TRT needs internally to access graphdef, and manipulate it. Unfortunately for now, we can't find a way to assign any graphdef to a GPU != 0.\r\n\r\nPlease see the following reproducer case:\r\n\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras import backend as K\r\nfrom tensorflow.python.framework import ops\r\n\r\ntf.get_logger().setLevel('WARNING')\r\n\r\n\r\ndef extract_devices_from_graphdef(graphdef):\r\n    all_nodes = [n for n in graphdef.node]\r\n    all_devices = list(set([n.device for n in all_nodes]))\r\n    return all_devices\r\n\r\n\r\ndef create_model():\r\n  \"\"\"Define a simple sequential model\"\"\"\r\n  model = tf.keras.models.Sequential([\r\n    keras.layers.Dense(512, activation='relu', input_shape=(784,)),\r\n    keras.layers.Dropout(0.2),\r\n    keras.layers.Dense(10)\r\n  ])\r\n  model.compile(optimizer='adam',\r\n                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                metrics=[tf.metrics.SparseCategoricalAccuracy()])\r\n  return model\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    try:\r\n        gpus = tf.config.list_physical_devices('GPU')\r\n    except AttributeError:\r\n        gpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\n    if not gpus:\r\n        raise RuntimeError(\"No GPUs has been found.\")\r\n\r\n    print('Found the following GPUs:')\r\n    for gpu in gpus:\r\n        print(f\"\\t- {gpu}\")\r\n\r\n\r\n    # Create a basic model instance\r\n    model = create_model()\r\n    model.save('./saved_model/my_model')\r\n\r\n    # Case 1 - Working\r\n    with tf.device(\"gpu:1\"):\r\n        print(\"\\n=================== CASE 1: `tf.saved_model.load` ===================\")\r\n        from tensorflow.python.saved_model import load as load_module\r\n        from tensorflow.python.saved_model.load import load as load_fn\r\n        print(\"TF2 API:      \", id(tf.saved_model.load))\r\n        # >>> TF2 API:       139634355265248\r\n        print(\"Direct Access:\", id(load_fn))\r\n        # >>> Direct Access: 139634355265248\r\n        print(\"Module Access:\", id(load_module.load))\r\n        # >>> Module Access: 139634355265248\r\n\r\n        model_loaded = tf.saved_model.load(export_dir='./saved_model/my_model')\r\n        print(\"Loaded Model:\", model_loaded.variables[0].device)\r\n        # >>> '/job:localhost/replica:0/task:0/device:GPU:1'\r\n\r\n        from tensorflow.python.saved_model import signature_constants\r\n        func = model_loaded.signatures[\r\n            signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\r\n        ]\r\n\r\n        print(\"Loaded Func Found Devices:\", extract_devices_from_graphdef(func.graph.as_graph_def()))\r\n        # >>> Loaded Func Found Devices: {''}\r\n\r\n        from tensorflow.python.framework import convert_to_constants\r\n        frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)\r\n        print(\"Frozen Func Found Devices:\", extract_devices_from_graphdef(frozen_func.graph.as_graph_def()))\r\n\r\n        print(\"\\n=================== CASE 2: `loader.load` ===================\")\r\n        from tensorflow.python.saved_model import loader\r\n        from tensorflow.python.saved_model import tag_constants\r\n        from tensorflow.python.client import session\r\n        from tensorflow.python.saved_model import signature_constants\r\n\r\n        with session.Session() as sess:\r\n            input_meta_graph_def = loader.load(\r\n                sess, [tag_constants.SERVING], './saved_model/my_model'\r\n            )\r\n            # input_signature_def = input_meta_graph_def.signature_def[\r\n            #     signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\r\n            # ]\r\n\r\n            print(\"Found Devices:\", extract_devices_from_graphdef(sess.graph.as_graph_def()))\r\n            # >>> Found Devices: ['', '/device:CPU:0']\r\n\r\n        print(\"\\n=================== CASE 3: `importer.import_graph_def` ===================\")\r\n        from tensorflow.python.framework import importer\r\n\r\n        print(\"Direct Access:\", id(tf.graph_util.import_graph_def))\r\n        # >>> Direct Access: 139634355265248\r\n        print(\"Module Access:\", id(importer.import_graph_def))\r\n        # >>> Module Access: 139634355265248\r\n        \r\n        with ops.Graph().as_default() as graph:\r\n            importer.import_graph_def(input_meta_graph_def.graph_def, name=\"\")\r\n            print(\"Found Devices:\", extract_devices_from_graphdef(graph.as_graph_def()))\r\n            # >>> Found Devices: ['', '/device:CPU:0']\r\n```\r\n\r\n\r\nOutput:\r\n\r\n```bash\r\nFound the following GPUs:\r\n\t- PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\r\n\t- PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\r\n\t- \r\n=================== CASE 1: `tf.saved_model.load` ===================\r\nTF2 API:       140219455831776\r\nDirect Access: 140219455831776\r\nModule Access: 140219455831776\r\nLoaded Model: /job:localhost/replica:0/task:0/device:GPU:1\r\nLoaded Func Found Devices: ['']\r\nFrozen Func Found Devices: ['']\r\n\r\n=================== CASE 2: `loader.load` ===================\r\nWARNING:tensorflow:From whatever.py:85: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nFound Devices: ['', '/device:CPU:0']\r\n\r\n=================== CASE 3: `importer.import_graph_def` ===================\r\nDirect Access: 140219510558784\r\nModule Access: 140219510558784\r\nFound Devices: ['', '/device:CPU:0']\r\n```\r\n\r\n**Question: How can we load and manipulate a graphdef on a specific GPU ?**", "comments": ["CC: @meena-at-work @bixia1 @tfeher @reedwm @nluehr ", "It looks like creating a new graph and enter its scope clears the device placement. Adding the `tf.device` placement after creating the new graph fixes it. Replacing the last four lines of your sample with the following causes GPU:1 to be used\r\n\r\n```python\r\n    with ops.Graph().as_default() as graph, tf.device('/GPU:1'):\r\n      importer.import_graph_def(input_meta_graph_def.graph_def, name=\"\")\r\n      print(\"Found Devices:\", extract_devices_from_graphdef(graph.as_graph_def()))\r\n      # >>> Found Devices: ['/device:GPU:1', '/device:CPU:0']\r\n```\r\n\r\nDoes this solve the issue? If not I can delegate to someone else, since I'm not familiar with SavedModel.", "Oh that's interesting.\r\n@reedwm is there a way to \"query what's the current device scope\" ? Because obviously the device scope is usually set by the user, we don't have visibility on that.\r\n\r\nIn any case, it sounds like a bug, can you bring it to the attention of the right people ? Opening a graph scope should not clear a device scope ...\r\n\r\n", "A somewhat hacky way to get the device is to create an empty tensor and immediately query its device: \r\n\r\n```\r\ndevice = tf.constant([], dtype=tf.float32).device\r\n```\r\n\r\n@rohan100jain do you know of a better way?\r\n\r\nI don't think the fact opening a graph scope clears the device scope is a bug, since each graph has a different device scope, although I admit this behavior is unintuitive. In any case, since this only affects the TF1 API (graphs), I doubt this behavior would ever get fixed."]}, {"number": 55436, "title": "TF 2.8.0 NNApi createInterpreter fails on Android API 32 devices", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes (sample project linked below)\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nAndroid API 32 - Pixel 6\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nAny device running Android API 32 (such as Pixel 6)\r\n\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n\r\n- TensorFlow version (use command below):\r\n2.8.0\r\n\r\n- GPU model and memory:\r\nMali-G78 MP20, N/A (Shared memory)\r\n\r\n**Describe the current behavior**\r\n\r\nInstrumented test in sample project succeeds on API <= 31, fails on API == 32\r\n\r\n**Describe the expected behavior**\r\n\r\nInstrumented test in sample project succeeds on all Android API levels\r\n\r\n- Do you want to contribute a PR? (yes/no): No, I don't know the fix.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n`./gradlew connectedAndroidTest`\r\n\r\nhttps://github.com/elevenfive/TensorFlowApplication\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nNNApi test threw exception\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_OP_FAILED at line 4483 while completing NNAPI compilation.\r\n    \r\n    Node number 7 (TfLiteNnapiDelegate) failed to prepare.\r\n    Restored original execution plan after delegate application failure.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:93)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)\r\n        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:44)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:226)\r\n```", "comments": ["Hi @elevenfive ! Sorry for the late response. The above app is running fine in an emulator with Pixel 5 and API 32 with latest android studio version.  Can you confirm your android studio version too?                                                  \r\n\r\n![API32](https://user-images.githubusercontent.com/86464649/161043609-728d4e00-7831-4003-a667-651a9640e870.png)", "Hi, the code is in the instrumented test. You can run that in studio or in the command line with `./gradlew connectedAndroidTest`. \r\n\r\nif you comment out the call to `options.setUseNnapiCpu(true);` it will fail with the noted exception on API 32 devices only.\r\n\r\nThanks!", "@elevenfive! I could run the instrumental test after commenting out **options.setUseNnapiCpu(true)** in Pixel XL with API 32 in latest Android  studio version( BumbleBee) without any exception. I would like to hear from @sachinprasadhs too.  Thanks!\r\n\r\n[NNAPI_55436](https://user-images.githubusercontent.com/86464649/161215324-bfe09f09-7f55-43f1-b530-2a4c313f8aad.png)", "> @elevenfive! I could run the instrumental test after commenting out **options.setUseNnapiCpu(true)** in Pixel XL with API 32 in latest Android studio version( BumbleBee) without any exception. I would like to hear from @sachinprasadhs too. Thanks!\r\n> \r\n> [NNAPI_55436](https://user-images.githubusercontent.com/86464649/161215324-bfe09f09-7f55-43f1-b530-2a4c313f8aad.png)\r\n\r\nAre you sure you ran the test there and not the \"app\" configuration?  You should use that little green play button next to the test method name.  You should see the unit test results as pass/fail at the bottom."]}, {"number": 55435, "title": "Loss names are not equal to the outputs dictionary keys of a model", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.18363\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n**Current behavior**\r\n\r\nThe names for the single losses of a multi output model correspond to the output \r\nlayer names instead to the output names provided by the keys in the outputs dictionary.\r\nI am not sure if this is the desired behavior.\r\n\r\n**Expected behavior**\r\n\r\nI would expect that the names of the losses are the same as the keys in the dictionary \r\npassed to the outputs argument when creating the model.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Dense, Input\r\n    from tensorflow.keras.models import Model\r\n    \r\n    first_input = Input(1)\r\n    x = Dense(100)(first_input)\r\n    x = Dense(1)(x)\r\n    \r\n    second_input = Input(1)\r\n    x2 = Dense(100)(second_input)\r\n    x2 = Dense(1)(x2)\r\n    \r\n    model = Model(\r\n        inputs={\"first_input\": first_input, \"second_input\": second_input},\r\n        outputs={\"first_output\": x, \"second_output\": x2},\r\n    )\r\n    \r\n    first_input_data = np.random.random(100)\r\n    second_input_data = np.random.random(100)\r\n    first_output_data = np.random.random(100)\r\n    second_output_data = np.random.random(100)\r\n    \r\n    model.compile(loss=\"mse\")\r\n    \r\n    model.fit(\r\n        x={\"first_input\": first_input_data, \"second_input\": second_input_data},\r\n        y={\"first_output\": first_output_data, \"second_output\": second_output_data},\r\n        batch_size=2,\r\n        epochs=2,\r\n    )\r\n\r\n\r\n\r\n**Other info / logs**\r\n\r\nThe loss names are visible in the following logging, when running the code example:\r\n\r\n\r\n    Epoch 1/2\r\n    50/50 [==============================] - 0s 673us/step - loss: 0.2440 - dense_1_loss: 0.1308 - dense_3_loss: 0.1131\r\n    Epoch 2/2\r\n    50/50 [==============================] - 0s 592us/step - loss: 0.1934 - dense_1_loss: 0.0973 - dense_3_loss: 0.0961\r\n\r\n\r\nThe problem is that I cannot see which loss belongs to which single output. \r\n\r\nOf course, my real use case is much more complicated, so that I cannot just name the output layers correctly.", "comments": ["Give a name to the output layers:\r\nhttps://stackoverflow.com/questions/53504813/is-there-a-way-of-renaming-the-metrics-and-losses-of-a-keras-model", "@szleb Could you please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/55435#issuecomment-1083572061) above and let us know if it helps?\r\nThanks!", "In my case I have nested models with multiple outputs which I use to build larger models. Giving the correct names to the output layers of the inner model does not solve the issue because they are not visible any more. The output layers of the outer model have the name of the inner model not of the inner outptu layers.", "Here is another code example closer to my use case. I hope that makes things a bit clearer\r\n\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Dense, Input, concatenate, multiply, subtract\r\n    from tensorflow.keras.models import Model\r\n    \r\n    first_input = Input(1, name=\"first_input\")\r\n    x = Dense(100)(first_input)\r\n    x = Dense(1, name=\"model_1_output\")(x)\r\n    \r\n    model_1 = Model(\r\n        name=\"model_1\",\r\n        inputs={\"first_input\": first_input},\r\n        outputs={\"model_1_output\": x},\r\n    )\r\n    \r\n    second_input = Input(1, name=\"second_input\")\r\n    third_input = Input(1, name=\"third_input\")\r\n    subtraction = subtract([second_input, third_input], name=\"subtraction\")\r\n    multiplication = multiply([second_input, third_input], name=\"multiplication\")\r\n    \r\n    model_2 = Model(\r\n        name=\"model_2\",\r\n        inputs={\"second_input\": second_input, \"third_input\": third_input},\r\n        outputs={\"subtraction\": subtraction, \"multiplication\": multiplication},\r\n    )\r\n    \r\n    input_1 = Input(1, name=\"input_1\")\r\n    input_2 = Input(1, name=\"input_2\")\r\n    layer_1 = model_1(input_1)\r\n    layer_2 = model_2({\"second_input\": input_2, \"third_input\": layer_1[\"model_1_output\"]})\r\n    \r\n    my_model = Model(\r\n        name=\"my_model\",\r\n        inputs={\"first_input\": input_1, \"second_input\": input_2},\r\n        outputs={\r\n            \"model_1_output\": layer_1[\"model_1_output\"],\r\n            \"multiplication\": layer_2[\"multiplication\"],\r\n            \"subtraction\": layer_2[\"subtraction\"],\r\n        },\r\n    )\r\n    \r\n    first_input_data = np.random.random(100)\r\n    second_input_data = np.random.random(100)\r\n    first_output_data = np.random.random(100)\r\n    second_output_data = np.random.random(100)\r\n    third_output_data = np.random.random(100)\r\n    \r\n    my_model.compile(loss=\"mse\")\r\n    \r\n    my_model.fit(\r\n        x={\"first_input\": first_input_data, \"second_input\": second_input_data},\r\n        y={\r\n            \"model_1_output\": first_output_data,\r\n            \"subtraction\": third_output_data,\r\n            \"multiplication\": second_output_data,\r\n        },\r\n        batch_size=2,\r\n        epochs=2,\r\n    )\r\n\r\nThis produces the following log:\r\n\r\n    Epoch 1/2\r\n    50/50 [==============================] - 0s 694us/step - loss: 0.7059 - model_1_loss: 0.1758 - model_2_loss: 0.2769 - model_2_1_loss: 0.2531\r\n    Epoch 2/2\r\n    50/50 [==============================] - 0s 653us/step - loss: 0.6777 - model_1_loss: 0.1460 - model_2_loss: 0.2485 - model_2_1_loss: 0.2832\r\n\r\nHere I do not know any more which loss belongs to the multiplication layer and which to the subtraction layer.\r\n\r\nI realised that in the code above `my_model.output_names =  ['model_1', 'model_2', 'model_2_1']`, but the keys of `my_model.output` and `my_model.output_shape` are the custom names `['model_1_output', 'multiplication', 'subtraction']`. I also figured out that when the model is created the outputs are ordered alphabetically with respect to the keys in the outputs dictionary (by tf.nest.flatten). So I am able to map the loss names to my output names, e.g. in a customized callback. In the given example this means that model_2_loss refers to the multiplication layer and model_2_1_loss to the subtraction layer. (Correct me if I am wrong.)", "You need to use model outputs as a list not a dict:\r\n\r\nhttps://github.com/keras-team/keras/blob/813e6911bd2edd78f85b0c245af6432bdd75cfc7/keras/engine/compile_utils.py#L659-L668", "Seems not to solve the problem in this case:\r\n\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Dense, Input, concatenate, multiply, subtract\r\n    from tensorflow.keras.models import Model\r\n    \r\n    first_input = Input(1, name=\"first_input\")\r\n    x = Dense(100)(first_input)\r\n    x = Dense(1, name=\"model_1_output\")(x)\r\n    \r\n    model_1 = Model(\r\n        name=\"model_1\",\r\n        inputs={\"first_input\": first_input},\r\n        outputs=x,\r\n    )\r\n    \r\n    second_input = Input(1, name=\"second_input\")\r\n    third_input = Input(1, name=\"third_input\")\r\n    subtraction = subtract([second_input, third_input], name=\"subtraction\")\r\n    multiplication = multiply([second_input, third_input], name=\"multiplication\")\r\n    \r\n    model_2 = Model(\r\n        name=\"model_2\",\r\n        inputs={\"second_input\": second_input, \"third_input\": third_input},\r\n        outputs=[subtraction, multiplication],\r\n    )\r\n    \r\n    input_1 = Input(1, name=\"input_1\")\r\n    input_2 = Input(1, name=\"input_2\")\r\n    layer_1 = model_1(input_1)\r\n    layer_2 = model_2({\"second_input\": input_2, \"third_input\": layer_1})\r\n    \r\n    my_model = Model(\r\n        name=\"my_model\",\r\n        inputs={\"first_input\": input_1, \"second_input\": input_2},\r\n        outputs=[\r\n            layer_1,\r\n            layer_2[1],\r\n            layer_2[0],\r\n        ],\r\n    )\r\n    \r\n    first_input_data = np.random.random(100)\r\n    second_input_data = np.random.random(100)\r\n    first_output_data = np.random.random(100)\r\n    second_output_data = np.random.random(100)\r\n    third_output_data = np.random.random(100)\r\n    \r\n    my_model.compile(loss=\"mse\")\r\n    \r\n    my_model.fit(\r\n        x={\"first_input\": first_input_data, \"second_input\": second_input_data},\r\n        y=[\r\n            first_output_data,\r\n            third_output_data,\r\n            second_output_data,\r\n        ],\r\n        batch_size=2,\r\n        epochs=2,\r\n    )\r\n\r\nBut still the ouptut looks like this:\r\n\r\n\tEpoch 1/2\r\n\t50/50 [==============================] - 0s 673us/step - loss: 0.5893 - model_1_loss: 0.1533 - model_2_loss: 0.1916 - model_2_1_loss: 0.2445\r\n\tEpoch 2/2\r\n\t50/50 [==============================] - 0s 612us/step - loss: 0.5682 - model_1_loss: 0.1377 - model_2_loss: 0.1807 - model_2_1_loss: 0.2498\r\n\r\nThe problem is that the inner model has two outputs and is used as a layer which covers the names of its output layers. Besides that I actually do use dictionaries on purpose no to depend on any order when defining the models and to get dictionaries when predicting data, so that the outputs can be identified. ", "I don't know if you can minimize your example a bit but the output names are\r\n\r\n`print(my_model.output_names)`\r\n`['model_1', 'model_2', 'model_2_1']`\r\n", "Yes exactly. The ouptut_names are the names of the output layers and not the outputs-dictionary keys (when using dicts). Since the last layer is the model called `model_2` having two outputs, the output_names of the model are `model_2` and `model_2_1`.\r\nWhatever the name of `model_2` is, I will never be able to identify the two outputs by their loss name. In the example with dictionary you also get\r\n\r\n    print(my_model.output_names)\r\n    ['model_1', 'model_2', 'model_2_1']\r\n\r\nbut\r\n\r\n    print(my_model.output)\r\n    {'model_1_output': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'model_1')>, \r\n     'multiplication': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'model_2')>, \r\n     'subtraction': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'model_2')>}\r\n\r\nWhy it does not use the dicitionary keys for the `output_names` when provided? I think that is the point of the problem.\r\n\r\nThe long code example was just to show that it is not always possible to name the output layers correctly. The issue with the `output_names` can already be seen in the very first example of this issue.", "Ok so this seems similar to https://github.com/tensorflow/tensorflow/issues/34114", "/cc @qlzh727 was involved in the original thread.", "changing the method _set_output_names in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/functional.py to something like the following would solve my problem:\r\n\r\n    if type(self.output) is dict:\r\n        self.output_names = list(self.output.keys())\r\n        self.output_names.sort()\r\n    else:\r\n        <<current code>>\r\n", "Keras now has its own repo so a ticket or a PR need to be submited to Keras directly:\nhttps://github.com/keras-team/keras\n\nAbout this change I have some doubt related to the previous @qlzh727 evaluation:\n\nhttps://github.com/tensorflow/tensorflow/issues/34114#issuecomment-588538295", "Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55432, "title": "MobileNetV3 reloading weights from version 2.8 gives error but not 2.7", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.7 and 2.8\r\n- Python version: 3\r\n\r\n**current behavior** is it give an error when loading weights in V2.8 but not in 2.7. tested models include \r\nResNetV2 = works!\r\nMobileNetV3Large = ERROR\r\nMobileNetV3Small = ERROR\r\ninceptionnetV3 = works!\r\n\r\nit give the following error for MobileNetV3Large \r\n`ValueError: Layer count mismatch when loading weights from file. Model expected 110 layers, found 111 saved layers.`\r\n\r\n**expected behavior** is to load the weights \r\n\r\n- Do you want to contribute a PR? (yes/no): no, \r\n\r\n**Standalone code to reproduce the issue**\r\nhere is a [Colab notebook](https://colab.research.google.com/drive/1RdRdW2GtLl_XrRkRgA78R170r9hZh-of?usp=sharing) that shows the issue, it downloads my weight file and everything but just in case [here is a link](https://www.dropbox.com/s/9pkhkvij53paz5m/0022_cp.hdf5?dl=0) to my weights.\r\n\r\n**Other info / logs** \r\nwhen I originally created and trained my model and when rebuilding my model I get the following warning\r\n`WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.`\r\n\r\nbecause I use image size of [96, 96, 3] and not [224, 224] as the warning mentioned above. the model worked great so I didn't worry about it, maybe this is related to the issue? \r\n\r\n\r\nThanks for your hard work and I hope this helps other community members! ", "comments": ["@chunduriv ,\r\nI was able to reproduce the issue in tf v2.8 and [nightly](https://colab.research.google.com/gist/tilakrayal/cb4fb69f9096a4a2260acae5fd5fb9e7/nigtlytf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb), whereas in [v2.7](https://colab.research.google.com/gist/tilakrayal/182544e20eeb1d103f8d2cea85bdd781/tf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb) i was able to execute without facing any issue.Please find the gist.", "I just noticed this in 2.8 when load model without head it prints \r\n`Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5`\r\nwhereas 2.7 prints\r\n`Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top.h5`\r\n\r\nnote 2.8 name load `_v2` while 2.7 does not. can I force it to download from the 2.7 version? I can test if this solves the issue! \r\n", "@PhillipMaire,\r\n\r\n>note 2.8 name load _v2 while 2.7 does not. \r\n\r\nYes\r\n\r\n>can I force it to download from the 2.7 version? I can test if this solves the issue!\r\n\r\nIf you can force it to `weights_mobilenet_v3_large_224_1.0_float_no_top.h5`, it is working. Thanks!\r\n"]}, {"number": 55429, "title": "Removing the static tpu initializer for JAX altogether, and returning Status when it is not successful.", "body": "Removing the static initializer for JAX tpu use. We only call libtpu initializer when there is a need for it. Here is the third PR dependent on the previous two PRs: [PR1](https://github.com/tensorflow/tensorflow/pull/55413) and [PR2](https://github.com/tensorflow/tensorflow/pull/55415)", "comments": ["@skye Could you take a look at this PR as well? It is dependent on the previous PRs, but I just wanted to get the idea of how it will end up looking like.", "@sshahrokhi This PR is in draft, any update on this? Please. Thank you!", "> @sshahrokhi This PR is in draft, any update on this? Please. Thank you!\r\n\r\nI am waiting for [this PR ](https://github.com/tensorflow/tensorflow/pull/55415) to get in before modifying this one as a real PR. This one is dependent on that PR. ", "@michaelbanfield  could you please review these changes? I have applied what Skye had requested, so I would appreciate it if you could take a look at the code."]}, {"number": 55427, "title": "Add dct_method argument to tf.image.adjust_jpeg_quality", "body": "Partially fixes issue #55138.\r\nIn particular, this pull request adds an argument `dct_method`, which allows the user to select between accuracy and speed of the DCT method used for decompression.", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55427/checks?check_run_id=5739608314).", "@rohan100jain  Can you please review this PR ? Thank you!"]}, {"number": 55419, "title": "RuntimeError: Encountered unresolved custom op: StridedSlice.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 21.10\r\n- TensorFlow installation: pip\r\n- TensorFlow library: 2.8.0\r\n\r\n### 2. Code\r\n        original = img\r\n        height = self.input_details[0]['shape'][1]\r\n        width = self.input_details[0]['shape'][2]\r\n        img = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\r\n        img = np.expand_dims(img, axis=0)\r\n\r\n        # Normalize input data\r\n        input_mean = 127.5\r\n        input_std = 127.5\r\n        input_data = np.uint8((np.float32(img) - input_mean) / input_std)\r\n        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\r\n\r\n        self.interpreter.invoke()\r\n\r\nCode Im using to convert model:\r\n\r\n          converter = tf.lite.TFLiteConverter.from_saved_model('./saved_model')\r\n          converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n          converter.allow_custom_ops=True\r\n          converter.experimental_new_converter =True\r\n          tflite_model = converter.convert()\r\n\r\n### 3. Failure after conversion\r\nModel converting finished with no error, but the converted model couldn't run through the interpreter.\r\n\r\n### 4. Error traceback\r\n       Traceback (most recent call last):\r\n        File \"/home/whoisltd/detect/serve_model.py\", line 32, in <module>\r\n          print(predict(image))\r\n        File \"/home/whoisltd/detect/serve_model.py\", line 21, in predict\r\n          result = model.predict(img)\r\n        File \"/home/whoisltd/detect/src/merged_model.py\", line 88, in predict\r\n          self.detect_text(cropped_image)\r\n        File \"/home/whoisltd/detect/src/merged_model.py\", line 49, in detect_text\r\n          detection_boxes, detection_classes, _ = self.text_detection_model.predict(image)\r\n        File \"/home/whoisltd/detect/src/detector/detector.py\", line 46, in predict\r\n          self.interpreter.invoke()\r\n        File \"/home/whoisltd/detect/det/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py\", line 916, in invoke\r\n          self._interpreter.Invoke()\r\n      RuntimeError: Encountered unresolved custom op: StridedSlice.\r\n      See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 11 (StridedSlice) failed to prepare.Node number 0 (WHILE) failed to invoke.\r\n      double free or corruption (!prev)\r\n      Aborted (core dumped)\r\n", "comments": ["Hi @whoisltd ! Could you please share a complete stand alone code to replicate this issue?\r\n", "> Hi @whoisltd ! Could you please share a complete stand alone code to replicate this issue?\r\n\r\nwhat do you mean?", "@whoisltd ! The conversion process looks fine to me.I was skeptical about the inference part actually  and needed either a sample model or the lite model for investigation. Below line in the stack trace  might give some hint about the issue.\r\n\r\n`  double free or corruption (!prev)`\r\n\r\nAttaching relevant thread for reference. [1](https://stackoverflow.com/questions/50342405/error-in-python-double-free-or-corruption-prev-0x00005634ba7d3e80) , [2](https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-498542543). Thanks!\r\n", "@mohantym \r\nI was fix `double free or corruption (!prev)` follow [this](https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-380319691) comment. \r\nBut I still have runtime error:\r\n```\r\nRuntimeError: Encountered unresolved custom op: StridedSlice.\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 11 (StridedSlice) failed to prepare.Node number 0 (WHILE) failed to invoke.\r\n```\r\n[Here](https://drive.google.com/drive/folders/1Cu6fuA38QdVGI_RFmSRrLnI25JleOAz6?usp=sharing) is my checkpoint, saved model", "Hi @sachinprasadhs ! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/2d5f401614bd971ef46d86f55cfcdecd/github_55419.ipynb#scrollTo=uHGXHgtNsGPN) ,[2.8](https://colab.sandbox.google.com/gist/mohantym/77e671c5950af663a844cda4cbf697b5/github_55419.ipynb#scrollTo=Kl6t5SPZpXXS) and [nightly](https://colab.sandbox.google.com/gist/mohantym/2d5dbe212edd4f1a4ad640368092a2dc/github_55419.ipynb#scrollTo=uHGXHgtNsGPN)"]}, {"number": 55418, "title": "[INTEL MKL] Fix oneDNN Ubuntu partials and Dockerfiles and remove support for Ubuntu 16.04", "body": "Signed-off-by: Abolfazl Shahbazi <abolfazl.shahbazi@intel.com>", "comments": ["To build Ubuntu 18.04 and 20.04 containers from `Intel` try this:\r\n\r\n```\r\ncd <PATH_TO_THE_SOURCE>/tensorflow/tools/dockerfiles\r\n\r\ndocker build --no-cache -f tools.Dockerfile -t tf-tools .\r\n\r\nalias asm_dockerfiles=\"docker run ${DOCKER_RUN_ENVS} --rm -u $(id -u):$(id -g) -v $(pwd):/tf tf-tools python3 assembler.py \"\r\n\r\nasm_dockerfiles --release dockerfiles --construct_dockerfiles\r\n\r\nalias asm_images=\"docker run ${DOCKER_RUN_ENVS} --rm -v $(pwd):/tf -v /var/run/docker.sock:/var/run/docker.sock tf-tools python3 assembler.py \"\r\n```\r\nand then build Ubuntu 18.04 and 20.04 base and Jupyter images for example, this way:\r\n\r\n\r\n```\r\n# For UBUNTU images\r\ndeclare -a OS_VERSIONS=( \"ubuntu-18.04\" \"ubuntu-20.04\" )\r\nfor OS_VERSION in ${OS_VERSIONS[@]}; do\r\n  PY_MINOR_VER=8\r\n  PYTHON=python3.${PY_MINOR_VER}\r\n  PYTHON_TAG=py3${PY_MINOR_VER}\r\n  TF_VERSION=2.8.0\r\n  TF_PACKAGE=intel-tensorflow\r\n  HOROVOD_VERSION=v0.24.2\r\n  asm_images \\\r\n      ${PARTIALS_BUILD_ARGS} \\\r\n      --release onednn \\\r\n      --repository intel-optimized-tensorflow \\\r\n      --arg BAZEL_VERSION=4.2.1 \\\r\n      --arg TF_BRANCH=v${TF_VERSION} \\\r\n      --arg TF_PACKAGE=${TF_PACKAGE} \\\r\n      --arg HOROVOD_VERSION=${HOROVOD_VERSION} \\\r\n      --arg TF_PACKAGE_VERSION=${TF_VERSION} \\\r\n      --arg PYTHON=${PYTHON} \\\r\n      --arg _TAG_PREFIX=${PYTHON_TAG}-${TF_VERSION} \\\r\n      --build_images \\\r\n      --only_tags_matching ${PYTHON_TAG}-${TF_VERSION}-${OS_VERSION}'$'\r\n\r\n  asm_images \\\r\n      ${PARTIALS_BUILD_ARGS} \\\r\n      --release onednn \\\r\n      --repository intel-optimized-tensorflow \\\r\n      --arg BAZEL_VERSION=4.2.1 \\\r\n      --arg TF_BRANCH=v${TF_VERSION} \\\r\n      --arg TF_PACKAGE=${TF_PACKAGE} \\\r\n      --arg HOROVOD_VERSION=${HOROVOD_VERSION} \\\r\n      --arg TF_PACKAGE_VERSION=${TF_VERSION} \\\r\n      --arg PYTHON=${PYTHON} \\\r\n      --arg _TAG_PREFIX=${PYTHON_TAG}-${TF_VERSION} \\\r\n      --build_images \\\r\n      --only_tags_matching ${PYTHON_TAG}-${TF_VERSION}-${OS_VERSION}-jupyter'$'\r\ndone\r\n```\r\n\r\nOnce the build is done you have the following images ready for testing:\r\n\r\n```\r\n$ docker images --format \"{{.Repository}}:{{.Tag}}\" | grep py38 | sort\r\nintel-optimized-tensorflow:py38-2.8.0-ubuntu-18.04\r\nintel-optimized-tensorflow:py38-2.8.0-ubuntu-18.04-jupyter\r\nintel-optimized-tensorflow:py38-2.8.0-ubuntu-20.04\r\nintel-optimized-tensorflow:py38-2.8.0-ubuntu-20.04-jupyter\r\n```\r\n", "@penpornk Can you please review this PR ? Thank you!"]}]