[{"number": 43053, "title": "C++ canonicalize_function_inputs WIP", "body": "Descended from #42903, differing only in aaeebeb. Commit contains in-progress code for C++ `canonicalize_function_inputs`. All significant logic is present, but remains to be debugged. Not ready for merge, and currently on stand-by. Points to look into: \r\n- C++ maps/unordered_maps are used for `args_to_indices`, `args_indices_to_default_values`, and `args_indices_to_values`. Their types are ints, std::strings, and py::objects. Possible that the lookups used to match arguments aren't working as intended, due to conversions between py::objects representing argument names and std::strings.\r\n- Methods used to modify the kwargs dictionary. Would consider using C API functions directly.\r\n- Casting style is vague (C-style: (py::object) object_name, etc.). Should be replaced by more specific casting methods, either C++ style or the methods for casting or implicitly/explicitly converting between py::object subclasses introduced by PyBind, which handle reference counting in particular ways\r\n\r\n@sun51 and @kkimdev have more information.", "comments": ["@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33  This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!", "@jonathanchu33 This PR is in draft, any update on this? Please. Thanks!"]}, {"number": 43035, "title": "Generate sparse tensor in tflite", "body": "I'm trying to understand the `SparsityParameters` in tflite schema. But, AFAIK, there is only one example that contains this parameters(`tensorflow/lite/testdata/sparse_tensor.json`).\r\n\r\nSo, I've got trouble generating a network which has tensors whose contents include `SparsityParameters`.\r\n\r\nAs `tensorflow/core/util/sparse/README.md` said, it sounds like, even if I try to generate a sparse tensor, it will generate just two dense tensors with a shape.\r\n> Sparse Tensors are stored as two dense tensors and a shape\r\n\r\nAnd actually, when I write a code like below and convert it to tflite file,\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nlhs_ = tf.compat.v1.placeholder(dtype=tf.float32, shape=(2), name=\"Hole\")\r\nrhs_ = tf.compat.v1.placeholder(dtype=tf.float32, shape=(2), name=\"Hole\")\r\n\r\nst1 = tf.compat.v1.sparse.SparseTensor(\r\n    indices=[[0, 0], [1, 2]], values=lhs_, dense_shape=[3, 4])\r\nst2 = tf.compat.v1.sparse.SparseTensor(\r\n    indices=[[0, 0], [1, 2]], values=rhs_, dense_shape=[3, 4])\r\n\r\n# op_ = tf.compat.v1.sparse_add(st1, st2)\r\n# op_ = tf.compat.v1.matmul(st1, st2)\r\n```\r\nIt doesn't generate a sparse tensor.\r\n\r\n**How can I generate the network that contains sparse tensors?**\r\n\r\np.s. I can manually force it to be generated with json file. But it doesn't seem natural.\r\n\r\n<details>\r\n\r\n```json\r\n{\r\n  version: 3,\r\n  operator_codes: [\r\n    {\r\n      builtin_code: \"FULLY_CONNECTED\"\r\n    }\r\n  ],\r\n  subgraphs: [\r\n    {\r\n      tensors: [\r\n        {\r\n          shape: [\r\n            1,\r\n            8\r\n          ],\r\n          buffer: 2,\r\n          name: \"input\",\r\n          quantization: {\r\n          }\r\n        },\r\n        {\r\n          shape: [\r\n            1,\r\n            4\r\n          ],\r\n          buffer: 3,\r\n          name: \"output\",\r\n          quantization: {\r\n          }\r\n        },\r\n        {\r\n          shape: [\r\n            4,\r\n            8\r\n          ],\r\n          buffer: 1,\r\n          name: \"weight\",\r\n          quantization: {\r\n          },\r\n          sparsity: {\r\n            traversal_order: [\r\n              0,\r\n              1\r\n            ],\r\n            dim_metadata: [\r\n              {\r\n                dense_size: 4\r\n              },\r\n              {\r\n                format: \"SPARSE_CSR\",\r\n                array_segments_type: \"Uint16Vector\",\r\n                array_segments: {\r\n                  values: [\r\n                    0,\r\n                    1,\r\n                    3,\r\n                    6,\r\n                    10\r\n                  ]\r\n                },\r\n                array_indices_type: \"Uint16Vector\",\r\n                array_indices: {\r\n                  values: [\r\n                    1,\r\n                    1,\r\n                    4,\r\n                    0,\r\n                    4,\r\n                    5,\r\n                    0,\r\n                    2,\r\n                    6,\r\n                    7\r\n                  ]\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ],\r\n      inputs: [\r\n        0\r\n      ],\r\n      outputs: [\r\n        1\r\n      ],\r\n      operators: [\r\n        {\r\n          inputs: [\r\n            0,\r\n            2,\r\n            -1\r\n          ],\r\n          outputs: [\r\n            1\r\n          ],\r\n          builtin_options_type: \"FullyConnectedOptions\",\r\n          builtin_options: {\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  description: \"TOCO Converted.\",\r\n  buffers: [\r\n    {\r\n    },\r\n    {\r\n      data: [\r\n        0,\r\n        0,\r\n        128,\r\n        63,\r\n        0,\r\n        0,\r\n        128,\r\n        63,\r\n        0,\r\n        0,\r\n        0,\r\n        64,\r\n        0,\r\n        0,\r\n        128,\r\n        63,\r\n        0,\r\n        0,\r\n        0,\r\n        64,\r\n        0,\r\n        0,\r\n        64,\r\n        64,\r\n        0,\r\n        0,\r\n        128,\r\n        63,\r\n        0,\r\n        0,\r\n        0,\r\n        64,\r\n        0,\r\n        0,\r\n        0,\r\n        64,\r\n        0,\r\n        0,\r\n        128,\r\n        63\r\n      ]\r\n    },\r\n    {\r\n    },\r\n    {\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n</details>\r\n\r\n", "comments": ["Have you seen the reply in https://stackoverflow.com/questions/63719424/understanding-of-sparsityparameters-in-tensorflow-lite-schema?", "@bhack Yes. Actually, the author of that question is me:) After posting that question, I've understood the TACO format and thanks to the answer to that question, I also noticed that tensorflow and lite format are different.\r\n\r\n> How can I generate the network that contains sparse tensors?\r\n\r\nBut then, doesn't the tflite converter ever create a sparse tensor in TACO format? Maybe `tensorflow/lite/tools/optimize/sparsity/format_converter.cc` is a library that does what I think but it's just be used to verify the file(`tensorflow/lite/testdata/sparse_tensor.json`) that is manually generated not to generate a file that contains tensors that has `SparsityParameters`."]}, {"number": 43025, "title": "Using Intermediate (non-Input) Layer as input to secondary network", "body": "**System information**\r\n- TensorFlow version: 2.3.0 \r\n- Are you willing to contribute it: No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, a user can define a network to only run partially through a network using something like below:\r\n```\r\n# Load Pre-Trained Model\r\nmodel = ...\r\n# Using I/O of previous model, build model with an additional (or replacement) output \r\nfirst_half = tf.keras.models.Model(model.inputs, model.get_layer(\"some_intermediate_layer_n\").output)\r\n```\r\nHowever, if you were to build a final model to run through the remainder of the network, from `some_intermediate_layer_n` onward, it falls over. It would be nice to be able to make a similar call, to use the base models outputs, but starting from an intermediate input step as below. You could use the output from the network above as the input for the remainder of the network in this example. \r\n```\r\n second_half = tf.keras.models.Model(model.get_layer(\"some_intermediate_layer_n_plus_1\").input, model.outputs)\r\n```\r\n\r\n The exact warning is `WARNING:tensorflow:Functional inputs must come from tf.keras.Input (thus holding past layer metadata), they cannot be the output of a previous non-Input layer.` This is quickly followed by a `Graph Disconnected` error.\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nNot to my knowledge. Should be able to call in a similar fashion above, where the new input is the intermediate layer you wish to start from.\r\n\r\n**Who will benefit with this feature?**\r\nAny network architecture where an intermediate representation is important. Architecture's reliant on pairs of networks, or encoding networks would benefit at the inference stage.  \r\n\r\n**Any Other info.**\r\nI'm assuming it's quite likely this has been requested before but I couldn't find it. If so, it would also be nice to have an explanation as to why the current limitation to inputs coming from `tf.keras.Input (thus holding past layer metadata)` is important. I understand this from a training perspective, but when inferring, I would've thought this could be overlooked.\r\n\r\nI of course understand you could build two networks with the latter building on from the prior but this way you can wrap it all up into one when training, or if using a pre-trained network, you can split it out as and when needed. \r\n\r\nThank you.", "comments": ["Do you have a very, very minima but runnable example or colab to reproduce this?", "I'll pull something together with some sample weights tomorrow.", "Please see the primitive example below;\r\n```\r\nimport tensorflow as tf\r\ndef def_enc_dec_pair():\r\n    input_ = tf.keras.layers.Input(shape=[None, None, 3])\r\n    x_e = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\")(input_)\r\n    x_e = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\")(x_e)\r\n    x_e = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\")(x_e)\r\n    x_e = tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\", name=\"encoded_layer\")(x_e)\r\n\r\n    x_d = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\", name=\"second_section\")(x_e)\r\n    x_d = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\")(x_d)\r\n    x_d = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\")(x_d)\r\n    x_d = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\")(x_d)\r\n\r\n    model = tf.keras.Model(inputs=[input_], outputs=[x_d])\r\n    model.compile(optimizer=tf.optimizers.Adam(), loss=[tf.keras.losses.MSE])\r\n\r\n    return model\r\n\r\nbase_model = def_enc_dec_pair()\r\n# base_model.summary()\r\n\r\nfirst_half = tf.keras.Model(base_model.inputs, base_model.get_layer(\"encoded_layer\").output)\r\n# first_half.summary()\r\n\r\nsecond_half = tf.keras.Model(base_model.get_layer(\"second_section\").input, base_model.output)\r\nsecond_half.summary()\r\n```\r\n\r\nNote that for usability, it would be beneficial to call the input to `second_half` as one of the following options in this example\r\n`base_model.get_layer(\"encoded_layer\").output`\r\n`base_model.get_layer(\"second_section\").input`\r\n`first_half.output`\r\n\r\nI hope this clarifies exactly what i'm talking about. `base_model` and `first_half` build fine but `second_half` fails for the previously mentioned reason. ", "So it seems to me that your 2nd use case it is like: https://stackoverflow.com/questions/52800025/keras-give-input-to-intermediate-layer-and-get-final-output", "That looks like what i'm after yes. Using against this trivial example you'd build as such\r\n\r\n```\r\n...\r\nsecond_input_shape = base_model.get_layer(\"encoded_layer\").output.shape\r\nsecond_input_shape = second_input_shape[1:]  # Remove Batch from front.\r\nsecond_input = tf.keras.layers.Input(shape=second_input_shape)\r\nx = second_input\r\nfor layer in base_model.layers[5:]:  # encoded_layer idx + 1\r\n    x = layer(x)\r\n\r\nsecond_half = tf.keras.Model(second_input, x)\r\nsecond_half.summary()\r\n```\r\nFor this simplistic approach, the above is okay but it could quickly get out of hand as the original stackoverflow answer points out. In my actual code i'm having to do something slightly more convoluted to stitch the remaining layers together. \r\n\r\nConsidering the initial hurdle is simply to create an Input Layer with dimensions to match the layer we want, is it something that could still be of added benefit? \r\n\r\nMy suggestion would be instead of throwing the warning, an attempt could be made to simply create an `Input` from the requested starting point (whether that's the output shape of a layer or the input shape of the next as they'll match up ofc), and then look at all the connections from the base network that are required to go from this point in the original model to the requested output.", "So do you mean that you want to automate this?", "Basically yes ", "I don't know the recent internals design limit to automate this in the Keras API\r\n/cc @tanzhenyu do you have an opinion about this?", "Just to state for the record the above solution becomes a nightmare when you're using anything residual or with multiple paths, which is why is why it would be so useful to base it off of the knowledge you already have about all the connections from the original `base_model`", "This is where subclassed model works better than functional models:\r\n```\r\nclass Encoder(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Encoder, self).__init__()\r\n    self.encoder_layers = []\r\n    self.encoder_layers.append(tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\"))\r\n    self.encoder_layers.append(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\"))\r\n    self.encoder_layers.append(tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\"))\r\n    self.encoder_layers.append(tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\", name=\"encoded_layer\"))\r\n\r\n  def call(self, inputs):\r\n    output = inputs\r\n    for layer in self.encoder_layers:\r\n      output = layer(output)\r\n    return output\r\n\r\nclass Decoder(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Decoder, self).__init__()\r\n    self.decoder_layers = []\r\n    self.decoder_layers.append(tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\", name=\"second_section\"))\r\n    self.decoder_layers.append(tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\"))\r\n    self.decoder_layers.append(tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\"))\r\n    self.decoder_layers.append(tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\"))\r\n  \r\n  def call(self, inputs):\r\n    output = inputs\r\n    for layer in self.decoder_layers:\r\n      output = layer(output)\r\n    return output\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.encoder = Encoder()\r\n    self.decoder = Decoder()\r\n\r\n  def call(self, inputs):\r\n    return self.decoder(self.encoder(inputs))\r\n```\r\n\r\nMaking things modular will also help you even in residual path case, no?", "@phillips96 Of course If you don't need to be constrained inside the functional API perimeter as @tanzhenyu's suggested you can handle this with model subclass.", "Thanks @tanzhenyu, so my actual application uses subclassed models in a similar fashion to this.\n\nI just found it curious that the api is capable of building a sub-model going from the input to an intermediate layer using that api call, but you can't go from the intermediate layer to the final output in quite the same fashion.\n\nHappy to close this if it's not something that seems overall beneficial to the repository \ud83d\ude04 ", "> Thanks @tanzhenyu, so my actual application uses subclassed models in a similar fashion to this.\r\n> \r\n> I just found it curious that the api is capable of building a sub-model going from the input to an intermediate layer using that api call, but you can't go from the intermediate layer to the final output in quite the same fashion.\r\n> \r\n> Happy to close this if it's not something that seems overall beneficial to the repository \ud83d\ude04\r\n\r\nNo, I agree with you, this seems desirable, however currently blocked by the same reason mentioned in the stackflow thread. Contribution welcome!", "@tanzhenyu Yes this was the real question. Glad to know that there is any hard design constrain to implement this.\r\n\r\n@phillips96 I suppose that a PR is welcome if you want to implement this behaviour.", "@bhack sure thing.\n\nThis will be my first PR here so I'll sign up to the contributors forms and do some initial scoping for this. \n\n"]}, {"number": 42999, "title": "tflite(micro) pointer-arithmetic overflow left unhandled in TfLiteStatus SimpleMemoryAllocator::EnsureHeadSize", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (SHA-1): 44dbace063796ea99c5b3d27a3a5810048d5096c\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe current implementation the check for an already-sufficient Head area in `SimpleMemoryAllocator::EnsureHeadSize` will fail due to a pointer-arithmetic overflow if the arena is allocated close to highest address and a large head size is requested resulting.\r\n\r\n```\r\n  if (aligned_result + size < head_) {\r\n    // Size is below the current head size, just return.\r\n    return kTfLiteOk;\r\n  }\r\n```\r\n\r\nThe overflowed pointer aligned_result + size will be below head_ causing a false kTfliteOk return rather than failing.\r\nThe Bug showed compiling tflite(u) test  for RISCV 32-bit using gcc-9 with default settings using the [RISCV GNU toolchain](https://github.com/riscv/riscv-gnu-toolchain) and executing on a semi-hosting simulator (whisper).    The test fails as in target environment a small arena is allocated on the stack very high in memory with a size large enough to overflow aligned_result + size.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe overflow case has to be handled.   The fix in our local fork is\r\n\r\n ```\r\n ptrdiff_t aligned_head_head_diff = head_-aligned_result;\r\n if (aligned_head_head_diff >=0 && size < static_cast<size_t>(aligned_head_head_diff)) {\r\n    // Size is below the current head size, just return.\r\n    return kTfLiteOk;\r\n  }\r\n```\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nAdd to recording_simple_memory_allocator_test\r\n\r\n```\r\nTF_LITE_MICRO_TEST(DemonstrateOverflow) {\r\n  constexpr size_t arena_size = 1024;\r\n  uint8_t *arena = reinterpret_cast<uint8_t *>(-arena_size);\r\n  tflite::RecordingSimpleMemoryAllocator allocator(micro_test::reporter, arena,\r\n                                                   arena_size);\r\n\r\n  TF_LITE_MICRO_EXPECT_EQ(\r\n      kTfLiteError, allocator.EnsureHeadSize(/*size=*/2048, /*alignment=*/1));\r\n}\r\n```\r\n\r\n\r\n\r\n", "comments": ["Thanks for the bug report.\r\n\r\nFor future reference, if you make a [TensorFlow Lite for Microcontrollers Issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md) from the different available templates, it will get routed to the micro team much faster.\r\n\r\n@nkreeger what do you think of the proposed fix?"]}, {"number": 42980, "title": "Loading a TF1 Protocol buffer does not work in TF versions 2.2.0 and above", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 4.19.112+** and **Windows 10**\r\n- TensorFlow version (use command below): **v2.2.0-rc4-8-g2b96f3662b 2.2.0** and **v2.3.0-0-gb36436b087 2.3.0**\r\n- Python version: **3.6.9**\r\n- CUDA/cuDNN version: **CUDA disabled**\r\n- GPU model and memory: **GPU disabled**\r\n\r\n**Describe the current behavior**\r\n\r\nModel trained with Tensorflow 1 and saved with the [tf.saved_model.simple_save](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/simple_save) can't be loaded with Tensorflow 2.2.0 and above using [tf.saved_model.load](https://www.tensorflow.org/api_docs/python/tf/saved_model/load). The same code works perfectly well with Tensorflow 2.1.1. \r\n\r\nThe error I get in TF 2.2.0 and above:\r\n\r\n```\r\n<ipython-input-3-fa86b40288e8> in <module>()\r\n----> 1 model_loaded = tf.saved_model.load('tensorflow_model/')\r\n      2 model_loaded = model_loaded.signatures['serving_default']\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags, options)\r\n    601     ValueError: If `tags` don't match a MetaGraph in the SavedModel.\r\n    602   \"\"\"\r\n--> 603   return load_internal(export_dir, tags, options)\r\n    604 \r\n    605 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls)\r\n    647   else:\r\n    648     with ops.init_scope():\r\n--> 649       root = load_v1_in_v2.load(export_dir, tags)\r\n    650       root.graph_debug_info = debug_info\r\n    651   return root\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(export_dir, tags)\r\n    261   \"\"\"Load a v1-style SavedModel as an object.\"\"\"\r\n    262   loader = _EagerSavedModelLoader(export_dir)\r\n--> 263   return loader.load(tags=tags)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(self, tags)\r\n    207     wrapped = wrap_function.wrap_function(\r\n    208         functools.partial(self.load_graph, load_graph_returns, meta_graph_def),\r\n--> 209         signature=[])\r\n    210     saver, = load_graph_returns\r\n    211     restore_from_saver = self._extract_saver_restore(wrapped, saver)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)\r\n    626           signature=signature,\r\n    627           add_control_dependencies=False,\r\n--> 628           collections={}),\r\n    629       variable_holder=holder,\r\n    630       signature=signature)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)\r\n     85 \r\n     86   def __call__(self, *args, **kwargs):\r\n---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n     88 \r\n     89   def call_with_variable_creator_scope(self, fn):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)\r\n     91     def wrapped(*args, **kwargs):\r\n     92       with variable_scope.variable_creator_scope(self.variable_creator_scope):\r\n---> 93         return fn(*args, **kwargs)\r\n     94 \r\n     95     return wrapped\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load_graph(self, returns, meta_graph_def)\r\n     88     # pylint: disable=protected-access\r\n     89     saver, _ = tf_saver._import_meta_graph_with_return_elements(\r\n---> 90         meta_graph_def)\r\n     91     # pylint: enable=protected-access\r\n     92     returns[0] = saver\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\r\n   1484           import_scope=import_scope,\r\n   1485           return_elements=return_elements,\r\n-> 1486           **kwargs))\r\n   1487 \r\n   1488   saver = _create_saver_from_imported_meta_graph(meta_graph_def, import_scope,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)\r\n    797         input_map=input_map,\r\n    798         producer_op_list=producer_op_list,\r\n--> 799         return_elements=return_elements)\r\n    800 \r\n    801     # TensorFlow versions before 1.9 (not inclusive) exported SavedModels\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)\r\n    403       return_elements=return_elements,\r\n    404       name=name,\r\n--> 405       producer_op_list=producer_op_list)\r\n    406 \r\n    407 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n    499       except errors.InvalidArgumentError as e:\r\n    500         # Convert to ValueError for backwards compatibility.\r\n--> 501         raise ValueError(str(e))\r\n    502 \r\n    503     # Create _DefinedFunctions for any imported functions.\r\n\r\nValueError: Node 'loss/gradients/model/batch_normalization_3/FusedBatchNormV3_1_grad/FusedBatchNormGradV3' has an _output_shapes attribute inconsistent with the GraphDef for output #3: Dimension 0 in both shapes must be equal, but are 0 and 64. Shapes are [0] and [64].\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nModels should be loaded regardless of TF2 version. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[Colab notebook to reproduce issue](https://colab.research.google.com/drive/18IWxx-eppX2Sjoo2h1hGe0TzCHNiykF1?usp=sharing)\r\n\r\n**Other info / logs** \r\n\r\n\r\n\r\nAlthough this should be enough information to reproduce the issue there are even more details in the [Stackoverflow question](https://stackoverflow.com/questions/63656778/how-to-load-a-trained-tf1-protobuf-model-into-tf2) I created.\r\n\r\n\r\n**Please provide a workaround in the latest Tensorflow version if possible as I have to use this model in an environment where I am constrained to the latest Tensorflow version.**\r\n\r\n\r\n", "comments": ["I have tried in colab with TF 2.1.1 and i am not seeing any issue. I could reproduce the issue with TF 2.3, nightly versions(`2.4.0-dev20200906`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/26b939ddd8c497c148479478fca3e23e/untitled312.ipynb). Thanks!", "@VSZM \r\n\r\nI believe [this](https://github.com/tensorflow/tensorflow/commit/84f2ec1d60b5bb14a59ccef8f8fa7eb5a1096e8f) commit broke backwards compatibility with the op in question.  I've passed the bug along to the relevant maintainers, though I don't have a estimate on when they might be able to take a look.\r\n\r\nIn the meantime, you mention you mostly just need a workaround.  My reading of the code is that the outputs in question are actually just reserved placeholders that aren't really in use.  As such, I tried modifying your model directly to make the output size compatible, and it seems to load.  I will reach out to you privately to send you the fixed model.\r\n\r\nIf you'd like to make the modification yourself, I modified the fourth output (`3` when zero indexing) of the failing node (`\"loss/gradients/model/batch_normalization_3/FusedBatchNormV3_1_grad/FusedBatchNormGradV3\"`) to change the shape dimension size from `64` to `0` (the exact path on the node is `attr[\"_output_shapes'][3].shape.dim.size`).\r\n\r\nIf this doesn't work, we'll unfortunately have to wait for a proper fix.", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/40f53ec0075665274660838b8886f444/42980.ipynb). Thanks!"]}, {"number": 42973, "title": "tf.nn.max_pool2d: No support for tf.Tensor input for ksize", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.19041\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 11.0/7.6.5\r\n- GPU model and memory: RTX 2070 8GB\r\n\r\n**Describe the current behavior**\r\nThe ksize arguement of tf.nn.max_pool2d supports only native Python int but not tf.Tensor. This is not a problem when using eager mode, but very disadvantageous when calculating the ksize as a Tensor in TF Graph mode.\r\n\r\n`TypeError: Expected int for argument 'ksize' not <tf.Tensor 'random_uniform:0' shape=() dtype=int32>.`\r\n\r\n**Describe the expected behavior**\r\nTensor support for tf.nn.max_pool2d function parameters, espacially ksize.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n@tf.function\r\ndef issue():\r\n    inp = tf.convert_to_tensor(np.arange(75).reshape((1, 5, 5, 3)))\r\n    ksize = tf.random.uniform([], 1, 3, dtype=tf.int32)\r\n    return tf.nn.max_pool2d(inp, [ksize], 1, \"SAME\")\r\n\r\nif __name__ == \"__main__\":\r\n    issue()\r\n```\r\n\r\n**Other info / logs**\r\n[traceback.txt](https://github.com/tensorflow/tensorflow/files/5177269/traceback.txt)\r\n\r\nThank you in advance!", "comments": ["Check https://stackoverflow.com/questions/43574076/tensorflow-maxpool-with-dynamic-ksize\r\nAnd https://github.com/tensorflow/tensorflow/issues/4746\r\n", "Specifying shape in ksize may help u,\r\n\r\na = 1        # a can be '1','2' or '4' \r\nksize = tf.random.uniform([a], 1, 3, dtype=tf.int32)\r\n\r\nor \r\n\r\nksize = tf.random.uniform(shape = [],minval = 1,maxval = 3, dtype=tf.int32)\r\n\r\n", "Thank you @bhack! There seems to be a solution for this issue provided by PR #11875. Use `gen_nn_ops.max_pool_v2` instead of `gen_nn_ops.max_pool`, which is used by default when calling `tf.nn.max_pool2d`. Change the code above to:\r\n```\r\nfrom tensorflow.python.ops import gen_nn_ops\r\nreturn gen_nn_ops.max_pool_v2(inp, [1, ksize, ksize, 1], [1, 1, 1, 1], \"SAME\")\r\n```\r\nPR #14983 tries to integrate the utilization of `gen_nn_ops.max_pool_v2` as standard behavior but was reverted. @yongtang @rmlarsen, are you interested to push PR #14983 into `tensorflow:master` to completely solve this issue, #1967 and, #4746?", "Was able to reproduce your issue in Tensorflow 2.5 version, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/f267af22e1879854802a153e7d5123ba/42973.ipynb). Thanks!", "> ```\r\n> from tensorflow.python.ops import gen_nn_ops\r\n> return gen_nn_ops.max_pool_v2(inp, [1, ksize, ksize, 1], [1, 1, 1, 1], \"SAME\")\r\n> ```\r\nthis is not working for me\r\n\r\n\r\n", "Was able to reproduce your issue in Tensorflow 2.8 version, please find the gist [here](https://colab.research.google.com/gist/chunduriv/7c118d3199933746d64d8ec9b03f08f6/42973.ipynb). Thanks!", "/cc @rmlarsen @yongtang "]}, {"number": 42972, "title": "Did not find a parser for CONV_2D with latest repo download", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Running on Raspberry Raspian ( latest version )\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2.3.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Raspian\r\n\r\n**Describe the problem**\r\nWith latest sources, I get Did not find a parser for CONV_2D when I try to load a converted model that contains\r\n  model = models.Sequential([\r\n   layers.Conv2D(32, (3, 3), activation='relu', input_shape=(height, width,1)),\r\n   layers.Conv2D(64, (3, 3), activation='relu',padding='same'),\r\n   layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\r\n   layers.Conv2D(128, (3, 3), activation='relu',padding='same'),\r\n   layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\r\n   layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n   layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\r\n   layers.Flatten(),\r\n   layers.Dense(128, activation='linear'),\r\n   layers.Dense(num_classes, activation='softmax')\r\n ])\r\n\r\nI got rid of all layer operations that aren't supported by tf micro.  I got a RESHAPE version error, so I updated from the tensorflow source repo and rebuilt the tf micro library for rpi.  Now I get \"Did not find a parser for CONV_2D\" when I run\r\n\r\nbool PositionML::setup()\r\n  {\r\n    // Not sure why they did it this way, but all examples do it this way\r\n    static tflite::MicroErrorReporter micro_error_reporter;\r\n    mErrorReporter = &micro_error_reporter;\r\n\r\n    mModel = tflite::GetModel(model_kara_cnn_tflite);\r\n    if (mModel->version() != TFLITE_SCHEMA_VERSION) {\r\n      TF_LITE_REPORT_ERROR(mErrorReporter, \"Model provided is schema version %d not equal to supported version %d.\", mModel->version(), TFLITE_SCHEMA_VERSION);\r\n      return false;\r\n    }\r\n\r\n    constexpr int tensor_arena_size = 16 * 1024;\r\n    uint8_t tensor_arena[tensor_arena_size];\r\n    tflite::MicroMutableOpResolver<4> resolver;\r\n    resolver.AddConv2D();\r\n    resolver.AddFullyConnected();\r\n    resolver.AddMaxPool2D();\r\n    resolver.AddSoftmax();\r\n\r\n    tflite::MicroInterpreter interpreter(mModel, resolver, tensor_arena, tensor_arena_size, mErrorReporter);\r\n    TfLiteStatus allocate_status = interpreter.AllocateTensors();\r\n    if (allocate_status != kTfLiteOk) {\r\n      TF_LITE_REPORT_ERROR(mErrorReporter, \"AllocateTensors() failed\");\r\n      return false;\r\n    }\r\n\r\n    std::cout << \"Setup complete\" << std::endl;\r\n    return true;\r\n  }\r\n\r\n\r\n", "comments": ["@nkreeger Could you take a look at this issue?", "Has there been any progress on this?", "@RolfSchlup sorry for the lag - long holiday weekend. \r\n\r\nCan you post the exact error that is happening? I see the original model source - but it's unclear what is going on. Any chance you can try and run the model visualization tool and post the generated HTML file here:\r\n\r\nFrom root tensorflow checkout:\r\n\r\nbazel run tensorflow/lite/tools:visualize /path/to/my-model.tflite /tmp/my-model.tflite.html\r\n\r\nWe use this tool to understand what is in the flatbuffer itself.", "@nkreeger This is the error that I get \r\n\"Did not find a parser for CONV_2D\"\r\n\r\nI wanted to run the bazel command, but now I found that on CentOS 7 ( the machine where the full tensorflow is running), the gcc version does not support c++14.  So, I'm currently updating it to gcc 8.2.0.   On the pi where tensorflow lite micro is running, the gcc version is already at 8.3.0\r\n", "@nkreeger I just updated gcc to 8.2.0 and reran the command.  I also got the latest from tensorflow master branch.  Got rid of the unknown option -std=c++14 error.  Now I get\r\n\r\n\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=183\r\nINFO: Reading rc options for 'run' from /home/rolf/tensorflow-master/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'run' from /home/rolf/tensorflow-master/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/rolf/tensorflow-master/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/rolf/tensorflow-master/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /home/rolf/tensorflow-master/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/rolf/tensorflow-master/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Analyzed target //tensorflow/lite/tools:visualize (30 packages loaded, 191 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/rolf/tensorflow-master/tensorflow/lite/python/BUILD:10:1: Generating flatbuffer files for <source file tensorflow/lite/schema/schema.fbs>: failed (Exit 1): flatc failed: error executing command\r\n  (cd /home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/flatbuffers/flatc --python -o bazel-out/k8-opt/bin/tensorflow/lite/python/schema_py_srcs_no_include_all -I ./ -I bazel-out/k8-opt/bin -I bazel-out/k8-opt/bin --no-includes --no-union-value-namespacing --gen-object-api tensorflow/lite/schema/schema.fbs)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/host/bin/external/flatbuffers/flatc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by bazel-out/host/bin/external/flatbuffers/flatc)\r\nbazel-out/host/bin/external/flatbuffers/flatc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by bazel-out/host/bin/external/flatbuffers/flatc)\r\nbazel-out/host/bin/external/flatbuffers/flatc: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/host/bin/external/flatbuffers/flatc)\r\nTarget //tensorflow/lite/tools:visualize failed to build\r\nINFO: Elapsed time: 46.606s, Critical Path: 12.87s\r\nINFO: 32 processes: 32 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n", "@nkreeger I have bazel release 3.10 installed", "@nkreeger What is the minimum version of gcc? I'm going to try again with 8.30\r\n", "@nkreeger \r\nOk.  I rebuilt tensorflow from scratch using sources.  Now I have Tensorflow 2.4.0 installed.  Since I'm running this inside of a CentOS8 VM 64 bit, I disabled all CUDA support\r\n\r\nThe following is the result of running the bazel visualize script\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=171\r\nINFO: Reading rc options for 'run' from /home/rolf/tensorflow-master/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'run' from /home/rolf/tensorflow-master/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'run' from /home/rolf/tensorflow-master/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib64/python3.6/site-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/rolf/tensorflow-master/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/rolf/tensorflow-master/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/rolf/tensorflow-master/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/rolf/tensorflow-master/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/rolf/tensorflow-master/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Analyzed target //tensorflow/lite/tools:visualize (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools:visualize up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/visualize\r\nINFO: Elapsed time: 1.528s, Critical Path: 0.02s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Build completed successfully, 1 total action\r\nTraceback (most recent call last):\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 517, in <module>\r\n    main(sys.argv)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 513, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 429, in CreateHtmlFile\r\n    data = CreateDictFromFlatbuffer(file_data)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 414, in CreateDictFromFlatbuffer\r\n    model_obj = schema_fb.Model.GetRootAsModel(buffer_data, 0)\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n\r\n", "Found a solution to the above from another issue solution.  Now I have instead \r\nAttributeError: module 'flatbuffers' has no attribute 'encode'", "@nkreeger \r\nTraceback (most recent call last):\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 517, in <module>\r\n    main(sys.argv)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 513, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 429, in CreateHtmlFile\r\n    data = CreateDictFromFlatbuffer(file_data)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 414, in CreateDictFromFlatbuffer\r\n    model_obj = schema_fb.Model.GetRootAsModel(buffer_data, 0)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/python/schema_py_generated.py\", line 9948, in GetRootAsModel\r\n    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\r\nAttributeError: module 'flatbuffers' has no attribute 'encode'\r\n", "@nkreeger The problem seems to be that flatbuffers' attribute has been deprecated.  schema_py_generated.py is a generated file so it should be a matter of modifying the generation script.  Which file needs to be modified?", "@nkreeger \r\nThe generated schema_py_generated.py file needs \r\n```from flatbuffers import encode as flatbuffers.encode```\r\nor something similar", "Has anyone looked at this for me.  I tried running\r\nbazel run tensorflow/lite/tools:visualize /path/to/my-model.tflite /tmp/my-model.tflite.htm\r\nas suggested, but I get\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 517, in <module>\r\n    main(sys.argv)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 513, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 429, in CreateHtmlFile\r\n    data = CreateDictFromFlatbuffer(file_data)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 414, in CreateDictFromFlatbuffer\r\n    model_obj = schema_fb.Model.GetRootAsModel(buffer_data, 0)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/4da7831c14df8ef17ee1e6d70dca59a7/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/python/schema_py_generated.py\", line 9948, in GetRootAsModel\r\n    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\r\nAttributeError: module 'flatbuffers' has no attribute 'encode'\r\n\r\n", "@nkreeger \r\nI just updated the entire tensorflow library and rebuild it from source\r\nNow running\r\nbazel run tensorflow/lite/tools:visualize /path/to/my-model.tflite /tmp/my-model.tflite.html\r\nI get\r\n```\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 517, in <module>\r\n    main(sys.argv)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 513, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 429, in CreateHtmlFile\r\n    data = CreateDictFromFlatbuffer(file_data)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 414, in CreateDictFromFlatbuffer\r\n    model_obj = schema_fb.Model.GetRootAsModel(buffer_data, 0)\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n", "I just updated the entire tensorflow library and rebuild it from source\r\nRunning\r\nbazel run tensorflow/lite/tools:visualize /path/to/my-model.tflite /tmp/my-model.tflite.html\r\nI still get\r\n```Traceback (most recent call last):\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 517, in <module>\r\n    main(sys.argv)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 513, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 429, in CreateHtmlFile\r\n    data = CreateDictFromFlatbuffer(file_data)\r\n  File \"/home/rolf/.cache/bazel/_bazel_rolf/71df654fc5fbedc3c54b5bf44b8a24d2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 414, in CreateDictFromFlatbuffer\r\n    model_obj = schema_fb.Model.GetRootAsModel(buffer_data, 0)\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n"]}, {"number": 42967, "title": "Extremely slow eigendecomposition compared to numpy/scipy.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI am using eigendecomposition in Tensorflow and find that it is extremely slow. This is on a mac, so these are CPU computations. But I've also done this on a linux box with a GPU and found the same thing. Here's the code to show Tensorflow's speed vs numpy and scipy:\r\n\r\n```\r\nimport numpy as np\r\nimport scipy as sp\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\nA = np.random.randn(400, 400)\r\nA_tf = tf.constant(A)\r\n\r\ncur = time()\r\nd, v = sp.linalg.eig(A)\r\nprint(f'sp: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = np.linalg.eig(A)\r\nprint(f'np: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = tf.linalg.eig(A_tf)\r\nprint(f'tf: {time() - cur:4.2f} s')\r\n```\r\nThis gives the following output:\r\n```\r\nsp: 0.09 s\r\nnp: 0.08 s\r\ntf: 5.04 s\r\n```\r\nAny ideas of what's up here?\r\n\r\n", "comments": ["Ref. Stackoverflow https://stackoverflow.com/questions/63746426/tensorflow-eigenvalue-decomposition-is-extremely-slow", "I posted both. Wasn't sure which venue was the correct one.", "Internally it is just a call to the eigen library: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/linalg/eig_op_impl.h#L77\r\n\r\n/cc @randl", "Also as we are using  `Eigen::ComplexEigenSolver` this is generally classified as a quite slow solution: [Check this table](https://eigen.tuxfamily.org/dox/group__TopicLinearAlgebraDecompositions.html).  \r\n> As it is interactive their convergence speed depends on how well the eigenvalues are separated.\r\n\r\nSo probably `np.random.randn(400, 400)` it is not one of the best input case for this API call in Eigen.", "Depending on your query, I used TensorFlow 1.5 on ubuntu 18.04.\r\nI used TensorfFlow.linalg.eigh instead. it was fast that two others.\r\n```\r\nimport numpy as np\r\nimport scipy as sp\r\nimport tensorflow.compat.v2 as tf\r\nfrom time import time\r\n\r\nA = np.random.randn(400, 400)\r\nA_tf = tf.constant(A)\r\n\r\ncur = time()\r\nd, v = sp.linalg.eig(A)\r\nprint(f'sp: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = np.linalg.eig(A)\r\nprint(f'np: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = tf.linalg.eigh(A_tf)    # in tensofow 1.5\r\nprint(f'tf: {time() - cur:4.2f} s')\r\n\r\n\r\n```\r\n\r\nThe result.\r\n![result](https://user-images.githubusercontent.com/29865600/92306034-2e50f300-ef84-11ea-8fd4-cc89e9ff6000.png)\r\n", "@bhack Any reason to not use a faster solver? Or is there a hack to switch to a different one? My use case is an optimization that includes a sum of a random NxN matrix plus a parameterized low-rank component.", "@bm777 tf.linalg.eigh is only for symmetric matrices. It didn't give you an error because it ignores the upper triangle of the input.", "@bhack Things are a bit more equal when using a complex matrix as input to the numpy and scipy solvers, but there is still a very significant difference which renders my optimization unworkable.\r\n```\r\nimport numpy as np\r\nimport scipy as sp\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\nA = np.random.randn(400, 400).astype(complex)\r\nA_tf = tf.constant(A)\r\n\r\ncur = time()\r\nd, v = sp.linalg.eig(A)\r\nprint(f'sp: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = np.linalg.eig(A)\r\nprint(f'np: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = tf.linalg.eig(A_tf)\r\nprint(f'tf: {time() - cur:4.2f} s')\r\n```\r\nWith output:\r\n```\r\nsp: 0.22 s\r\nnp: 0.17 s\r\ntf: 4.53 s\r\n```", "Can you try to code and benchmark a minimal c++ example for your use case with [Spectra](https://spirit-docs.readthedocs.io/en/latest/core/thirdparty/spectra/README.html)?", "@seanslice Also you will probably find similar `np.linalg.eig` performance compiling Eigen with `EIGEN_USE_LAPACKE`. See https://eigen.tuxfamily.org/dox-devel/TopicUsingBlasLapack.html", "@bhack I need all the eigenvalue and vectors, so Spectra (which returns a subset of the values/vectors) isn't the right solution. I'm varying matrix sizes between 50 and 1000, which is well within the range of relatively rapid eigendecomposition on modern hardware.\r\n\r\nMy use case is below. The loss function is a bit of a mess of linear algebra operations, but the limiting step is the call to eig() during the forward pass.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom time import time\r\n\r\n# meta-parameters\r\nN = 100\r\nn = 40\r\nepochs = 1001\r\nprint_interval = 200\r\nbeta = 0.05\r\next_penalty = 1e5\r\nnp.random.seed(0)\r\n\r\n# fixed values\r\nJ0 = np.random.randn(N, N) / np.sqrt(N)\r\nw = np.random.randn(N) / np.sqrt(N)\r\n\r\neig_time = [0.0]\r\n# loss function; store time performing eig()\r\ndef loss_f(U, V):\r\n    U = U / tf.linalg.norm(U, axis=-2, keepdims=True)\r\n    V = V / tf.linalg.norm(V, axis=-1, keepdims=True)\r\n    J = J0 + U @ V\r\n    pre_eig = time()\r\n    d, r = tf.linalg.eig(J)\r\n    eig_time[0] += time() - pre_eig\r\n    L = tf.linalg.inv(tf.linalg.adjoint(r) @ r)\r\n    g = d - 1\r\n    g_real = tf.math.real(g)\r\n    tmp = -g[:, None] - tf.math.conj(g)\r\n    G1 = tmp / (tmp * tmp + 1e-20)\r\n    G2 = (g[:, None] * tf.math.conj(g)) * G1\r\n    wR = tf.linalg.matvec(r, tf.cast(w, tf.complex128), transpose_a=True)\r\n    return tf.math.real(tf.reduce_sum((r @ (L * G1)) * tf.math.conj(r)) / N + beta * tf.reduce_sum(wR * tf.linalg.matvec(L * G2, tf.math.conj(wR)))) \\\r\n           + ext_penalty * tf.reduce_sum(tf.where(g_real < 0, tf.zeros_like(g_real), g_real)**2)\r\n\r\n# weights to be optimized\r\nU = tf.Variable(np.random.randn(N, n) / np.sqrt(N))\r\nV = tf.Variable(np.random.randn(n, N) / np.sqrt(N))\r\n\r\n# convenience fx for printing losses & execution time during optimization\r\ndef stager():\r\n    loss = loss_f(U, V)\r\n    if stager.count % print_interval == 0:\r\n        print(f'iter {stager.count:4d}, time {time() - stager.time:5.2f}s, eig_time {eig_time[0]:5.2f}s, loss {loss:7.5f}')\r\n        stager.time = time()\r\n        eig_time[0] = 0.0\r\n    stager.count += 1\r\n    return loss\r\nstager.count = 0\r\nstager.time = time()\r\n\r\n# optimization routine\r\nopt = tf.keras.optimizers.Adam(learning_rate=0.01)\r\nfor _ in range(epochs):\r\n    opt.minimize(stager, [U, V])\r\n```\r\nHere's sample output for a few steps of optimization for a matrix of 100x100 (`N=100`, `n=40`, `epochs=1001`, `print_interval=200`):\r\n```\r\niter    0, time  0.09s, eig_time  0.09s, loss 4656.80738\r\niter  200, time 17.48s, eig_time 14.64s, loss 2.00800\r\niter  400, time 17.69s, eig_time 14.82s, loss 1.72453\r\niter  600, time 17.67s, eig_time 14.81s, loss 1.57498\r\niter  800, time 17.59s, eig_time 14.76s, loss 1.47659\r\niter 1000, time 17.76s, eig_time 14.90s, loss 1.40378\r\n```\r\nAnd here's for a matrix of 400x400 (`N=400`, `n=160`, `epochs=21`, `print_interval=4`):\r\n```\r\niter    0, time  4.52s, eig_time  4.45s, loss 11191.68619\r\niter    4, time 20.34s, eig_time 19.52s, loss 12.15866\r\niter    8, time 20.34s, eig_time 19.50s, loss 4.54211\r\niter   12, time 19.64s, eig_time 18.80s, loss 3.22969\r\niter   16, time 19.61s, eig_time 18.78s, loss 2.80901\r\niter   20, time 19.05s, eig_time 18.24s, loss 2.61946\r\n```", "If you need all the values and something more like `np.linalg.eig` you can call LAPACK `?gees` but you need to recompile Tensorflow from source and modify a little bit the bazel flags for Eigen to enable `EIGEN_USE_LAPACKE`.  \r\n\r\nIf you are adventurous you can copy some build tips from: \r\nhttps://gitlab.com/arm-hpc/packages/-/wikis/packages/tensorflow?version_id=5c32ca4f0132af89f79db6303437d6e66526364b#alter-eigens-defines-includes-and-depencencies\r\n\r\nAfter you have patched the build rules you could build TF from source as usual:\r\nhttps://www.tensorflow.org/install/source", "P.s. In the meantime waiting for other maintainers comments if they want to suggest you an alternative formulation of your loss.", "Thanks a lot for your input. I really appreciate it. I haven't tried to compiling from source. I admit to being a bit daunted by the prospect, but I'll dive in and see how it goes. Thanks again!", "@seanslice As building Tensorflow it is quite a resource intensive and long process I suggest you to directly prototype standalone  c++ code like:\r\n\r\n```\r\n#include <iostream>\r\n#include <Eigen/Eigenvalues>\r\n#include <Eigen/Dense>\r\n#include <chrono>\r\n\r\nusing namespace std;\r\nusing namespace Eigen;\r\n\r\nint main( int argc, const char* argv[] )\r\n{\r\n\tauto start = chrono::steady_clock::now();\r\n\tMatrixXcf A = MatrixXcf::Random(400,400); \r\n\tComplexEigenSolver<MatrixXcf> ces;\r\n\tces.compute(A);\r\n\tauto end = chrono::steady_clock::now();\r\n        auto diff = end - start;\r\n        cout << chrono::duration <double, milli> (diff).count() << \" ms\" << endl;\r\n}\r\n```\r\n\r\n`g++  -I /usr/include/eigen3 eig_bench.cc -o eig_bench -llapack -llapacke -lblas  -D EIGEN_USE_LAPACKE=1 -lm`", "It took me awhile to get openblas and lapack properly installed and discoverable by the compiler. In addition to the C++ benchmark code you wrote, I made two little python scripts. `eig_test_np.py`:\r\n```\r\nimport numpy as np\r\nfrom time import time\r\n\r\nJ = np.random.randn(400, 400)\r\ncur = time()\r\nd, v = np.linalg.eig(J)\r\nprint(f'{1000 * (time() - cur):.2f} ms')\r\n```\r\nAnd `eig_test_tf.py`:\r\n```\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\nJ = tf.random.normal((400, 400), dtype=tf.float64)\r\ncur = time()\r\nd, v = tf.linalg.eig(J)\r\nprint(f'{1000 * (time() - cur):.2f} ms')\r\n```\r\nIt's pretty clear from the output that numpy is not using `ComplexEigenSolver`:\r\n```\r\n$ ./eig_bench\r\n2626.33 ms\r\n$ python eig_test_tf.py\r\n4817.74 ms\r\n$ python eig_test_np.py\r\n106.44 ms\r\n```", "I suppose you need to test with something like:\n```\na\u00a0=\u00a0np.random.randn(400,400)+np.random.randn(400,400)*1j\nw,\u00a0v\u00a0=\u00a0np.linalg.eig(a)\n```\n\nAs you can see from numpy doc:\nhttps://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html#numpy-linalg-eig\n> This is implemented using the\u00a0_geev\u00a0LAPACK routines\n\nThat with the c++ compiler flag in Eigen will use `\u200b?gees` as you can see in the table:\nhttps://eigen.tuxfamily.org/dox-devel/TopicUsingBlasLapack.html", "This is ultimately due to single threaded Eigen implementations for eig op which could be linked to multithreaded MKL but actually not in tf bazel setup. There are many similar problems complaining the speed as  https://github.com/tensorflow/tensorflow/issues/7128, https://github.com/tensorflow/tensorflow/issues/13222, etc., and this problem can only be fully addressed and solved by https://github.com/tensorflow/tensorflow/issues/34924. Namely, by supporting MKL linkage for eigen when compiling tensorflow, but as described in the above issue, I am not fully clear how to make such setup work.\r\n\r\nFor now there is a workaround using `tf.py_function` though, which can utilize ``eig`` from numpy or scipy in forward pass (which is ultimately provided by multithreded MKL or openblas) and enjoy automatic differentiation at the same time. See a full demo below (the gradient code part is directly copied from tf codebase):\r\n\r\n```python\r\ndef _SafeReciprocal(x, epsilon=1E-20):\r\n      return x / (x * x + epsilon)\r\n\r\n@tf.custom_gradient\r\ndef my_eig(x):\r\n    e, v = np.linalg.eig(x)\r\n    def grad(grad_e, grad_v):\r\n        vt = tf.linalg.adjoint(v)\r\n        f = tf.linalg.set_diag(\r\n          _SafeReciprocal(\r\n              tf.expand_dims(e, -2) - tf.expand_dims(e, -1)),\r\n          tf.zeros_like(e))\r\n        f = tf.math.conj(f)\r\n        vgv = tf.matmul(vt, grad_v)\r\n        mid = tf.linalg.diag(grad_e)\r\n        diag_grad_part = tf.linalg.diag(tf.linalg.diag_part(\r\n          tf.cast(tf.math.real(vgv), vgv.dtype)))\r\n        mid += f * (vgv - tf.matmul(tf.matmul(vt, v), diag_grad_part))\r\n        grad_a = tf.linalg.solve(vt, tf.matmul(mid, vt))\r\n\r\n        return tf.cast(grad_a, x.dtype)\r\n    return (e, v), grad\r\n    \r\na = tf.random.normal([400,400])\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(a)\r\n    y = tf.py_function(func=my_eig, inp=[a], Tout=[tf.complex64, tf.complex64])\r\n    print(y)\r\n    l =  y[1][2,2]**2/tf.linalg.norm(y[1][:,2])\r\nprint(tape.gradient(l, a))\r\n```\r\n\r\nSuch an approach provide speed similar to numpy eig and compatible with tf's AD infrastructure. The 400*400 case with gradient calculation requires time around 0.5s while a pure ``tf.eig`` foward pass with back propagation needs 8.5s.\r\n\r\nOf course, this is not a perfect solution as ``py_function`` has many limitations such as it cannot be serialized, but I guess it is ok for most research cases when one just plays some small things with python.", "I don't know If the table is still updated but I don't think also enabling MKL flag and linking It Will have an effect on that call:\n\nhttps://eigen.tuxfamily.org/dox/TopicUsingBlasLapack.html\n\nI think the main difference with that flag in the c++ standalone and numpy is lapack `?gees` vs `?geev`.\n\nProbably the standalone example could freely use multithread but I've not checked Tensorflow flags for the Eigen library.\n\n[?gees doc](https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-c/top/lapack-routines/lapack-least-squares-and-eigenvalue-problem-routines/lapack-least-squares-and-eigenvalue-problem-driver-routines/nonsymmetric-eigenvalue-problems-lapack-driver-routines/gees.html)\n\n[?geev doc](https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-c/top/lapack-routines/lapack-least-squares-and-eigenvalue-problem-routines/lapack-least-squares-and-eigenvalue-problem-driver-routines/nonsymmetric-eigenvalue-problems-lapack-driver-routines/geev.html)", "@refraction-ray This solution solved the problem. I achieved about a 20x speed up on my MacBook for `N=500`. With `tf.linalg.eig`, 25 iterations of ADAM took ~300 s. With `py_function` wrapping `np.linalg.eig`, 25 iterations took ~14 s.\r\n\r\nI thought there might be a solution like this, but I couldn't find any help online about how to use custom gradients with `py_function`.\r\n\r\nThank you tremendously for your help here!!", "I was able to reproduce the issue with TF version 2.3, nightly version(`2.4.0-dev20200906`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0d4767d0179310577ef82560db0e6f01/untitled310.ipynb#scrollTo=aJMXFa70L9OS).Thanks!", "Also, `tf.linalg.eigh` uses way too much memory, this could be the reason why it's slow (when it fits into the GPU RAM, at least).\r\n\r\n```\r\n    def test_eigh_oom(self):\r\n        cov = Uniform(0, 1, dtype=tf.float64).sample((5, 100000, 4, 4))\r\n        cov += tf.transpose(cov, perm=[0, 1, 3, 2])\r\n\r\n        s, u, v = tf.linalg.svd(cov, full_matrices=True)\r\n        e, v = tf.linalg.eigh(cov)\r\n```\r\n\r\nSVD works, eigh gives OOM.", "I just want to add that `tf.linalg.svd` without the `full_matrices=True` parameter also results in OOM if the input is large."]}, {"number": 42946, "title": "tf.nn.ctc_greedy_decoder: self define the output node's name", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15\r\n- Are you willing to contribute it (Yes/No): Maybe\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere's no \"name\" argument in the current \"tf.nn.ctc_greedy_decoder\", in both tensorflow 1.15 and tensorflow 2. \r\nBut it does has one in \"tensorflow.python.ops.gen_ctc_ops.ctc_greedy_decoder\", which the \"tf.nn.ctc_greedy_decoder\" inherits from.  \r\nSo why not add a \"name\" argument, so users can define the name of output node's name of  \"tf.nn.ctc_greedy_decoder\" by themselves, and locate it easily? \r\n\r\n**Will this change the current api? How?**\r\nOnly need to make a little change to \"tf.nn.ctc_greedy_decoder\"\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who uses this function. \r\n\r\n**Any Other info.**\r\nActually, I also wonder what's the default output node's name of tf.nn.ctc_greedy_decoder? ", "comments": []}, {"number": 42934, "title": "How to save tf.data.Dataset object?", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): If possible\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nHi, I am using `tf.data.Dataset` to prepare a streaming dataset which is used to train a tf.kears model. To use a python pipeline package like [Luigi](https://luigi.readthedocs.io/en/stable/index.html#) or [kedro](https://kedro.readthedocs.io/en/stable/) it is required to pickle the object. According to quantumblacklabs/kedro#91 the problem is that the `tf.data.Dataset` object ...\r\n\r\n> can't be deep copied because somewhere deep inside it contains a `threading.RLock` object (which is quite unusual).\r\n\r\n> The reference to pickle is somewhat misleading and comes up due to how deepcopy is implemented. See here for more detail: https://stackoverflow.com/questions/22388854/relationship-between-pickle-and-deepcopy/22618214\r\n> \r\n> The best fix here would be to talk to the developers of keras about having adam optimizers [and tf.data.Dataset] implement the __getstate__ and __setstate__ functions so that they can be copied (and also pickled).\r\n\r\nWould it be possible to add the feature which allows picklable `tf.data.Dataset` objects?\r\n\r\nIssue #38483 is similar to this one but got closed because the idiomatic solution seems to be `tfrecords` and the implemented `tf.data.experimental.save` method. However, as far as I know, these approaches will save the complete dataset to disk which can be a huge waste of disk resources and the time it takes to save the complete dataset. \r\n\r\nIs there a way to save just the created `tf.data.Dataset` **object** to disk or are there any plans to implement this? And what would have to be done to achieve this?\r\n\r\nAre there alternative suggestions to `tf.data.Dataset` when we want to use reproducible pipelines? \r\n\r\n- tfx\r\n- tf.transform\r\n- Apache beam pipelines (seem to be picklable)\r\n\r\nI would welcome some more information about the differences between `tf.data.Dataset` and tfx releated packages (`tf.transform and apache beam). When should we use which?\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who wants to create reproducible pipelines with packages like `kedro` in combination with `tf.data.Dataset`.", "comments": ["Hello! I found these guides useful: \r\n1 - [TFRecord and tf.train.Example](https://www.tensorflow.org/api_docs/python/tf/train/Example)\r\n2 - [tf.data.experimental.TFRecordWriter](https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter)", " @fjp ! Beside TFRecord , I also found an article on [kafka](https://github.com/tensorflow/tensorflow/issues/42934#issuecomment-803314776) for streaming data."]}, {"number": 42903, "title": "C++ FunctionSpec() - all methods besides canonicalize_function_inputs", "body": "This should come strictly after #42695. In fact, this branch is a descendant of the second-to-last commit of #42695 (55ea345)\r\n\r\nThis PR contains C++ versions of all methods of the `FunctionSpec()` class except for `canonicalize_function_inputs` and the staticmethod `from_function_and_signature`); specifically,\r\n1. `FunctionSpec()` constructor, `signature_summary()`, `convert_variables_to_tensors()`, `convert_annotated_args_to_tensors()`\r\n2. 3 utility functions: `JoinVector` and `JoinPyDict` - similar to Python's string.join() method, and `PyObjectToString`\u00a0- equivalent of Python's `str()`\r\n\r\nThe C++ code is complete and tested, but not actually called in `function.py` in this PR. However, they pass `function_test` if called in `function.py` as such:\r\n- `self._cpp_functionspec = _concrete_function.FunctionSpec(fullargspec, is_method, input_signature, bool(is_pure), experimental_follow_type_hints, name)` initialized in `FunctionSpec().__init__()`\r\n- `def signature_summary(default_values = False): return self._cpp_functionspec.signature_summary(default_values)` and similarly for `convert_variables_to_tensors()` and `convert_annotated_args_to_tensors()`\r\n\r\nNo dependencies would need to be added to the BUILD to call the C++ from Python.\r\n\r\nThe outstanding minor todos / failing tests for this PR would be:\r\n- `def_function_test` potentially fails due to the C++ FunctionSpec class not being pickle-able (if a C++ instance is initialized in the Python code as in above). This feature can be added by following https://pybind11.readthedocs.io/en/stable/advanced/classes.html#pickling-support\r\n- Potential internal test failures due to global imports, like in #42695. These fixes have not been carried over from #42695 (2355df1).\r\n\r\n@sun51 and @kkimdev have more information.", "comments": ["Updated original description to reflect status of PR"]}, {"number": 42878, "title": "TensorArray issues in XLA", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3 (exists in 1.5 as well)\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI know it was documented in xla official doc that xla doesn't support TensorArray with `dynamic_size=True`, however, I still ran into issues even w/ `dynamic_size=False`(it is default).\r\n\r\n**Describe the expected behavior**\r\nexpect the tensorarray related stuffs are supported.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\n\r\n@tf.function(experimental_compile=True)\r\ndef test_tensor_array_scatter_gather():\r\n    dtype = \"float32\"\r\n    t = tf.constant(np.array([[1.0], [2.0], [3.0]]).astype(dtype))\r\n    scatter_indices = tf.constant([2, 1, 0])\r\n    gather_indices = tf.constant([1, 2])\r\n    ta1 = tf.TensorArray(dtype=dtype, size=3, infer_shape=True)\r\n    ta2 = ta1.scatter(scatter_indices, t)\r\n    t1 = ta2.gather(gather_indices)\r\n    return t1\r\n\r\ntest_tensor_array_scatter_gather()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@yongwww \r\n\r\nI have tried in colab with TF versions 2.3, nightly versions(`2.4.0-dev20200831`) and was able to reproduce the issue.If I change\r\n@tf.function(experimental_compile=True) to @tf.function i am not seeing any issue.Looks like it does not utilize XLA.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/599f185c99881fc8f9a30e3ccd82df3b/untitled293.ipynb).Thanks!", "@ravikyram  yeah, I think `@tf.function` doesn't use XLA either. ", "I have several other tensorarray test cases, will run into similar issues with all of them, I can upload some of them if needed.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210603, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/8e05169dde8daaeabb3d1c1256cf8be8/42878.ipynb). Thanks!"]}, {"number": 42876, "title": "control flow support issue in TensorFlow XLA", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary (pip install tensorflow)\r\n- TensorFlow version (use command below): 2.3 (issue exists in 1.5 as well)\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nSeems control flow is supported in xla, but I ran into issues with some test cases\r\n\r\n**Describe the expected behavior**\r\nthe graph should be supported by xla\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import control_flow_ops\r\ntf.compat.v1.enable_eager_execution()\r\n@tf.function(experimental_compile=True)\r\ndef test_switch():\r\n    data_np = np.random.uniform(0, 5, size=(2, 4, 5, 1)).astype('float32')\r\n    data = tf.constant(data_np)\r\n    split = tf.split(data, 2, axis=0)\r\n    flag = False\r\n    output_false, output_true = control_flow_ops.switch(split[1], flag)\r\n    return output_false\r\n\r\ntest_switch()\r\n```\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe following is the error message I got when tried to run with xla.\r\n```\r\nSwitch: unsupported op: No registered 'Switch' OpKernel for XLA_CPU_JIT devices compatible with node {{node Switch}}\r\n\tStacktrace:\r\n\t\tNode: __inference_test_switch_12, function: \r\n\t\tNode: Switch, function: __inference_test_switch_12\r\n [Op:__inference_test_switch_12]\r\n```", "comments": ["I am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7042a073fd9ca20e10c881938a7c587e/untitled403.ipynb)", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210603, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/da738e8701d9febde7ec5671c1369796/42876.ipynb). Thanks!"]}, {"number": 42875, "title": "Tensorflow model dump in c++ during inference", "body": "Tensorflow 1.15/2.x\r\n\r\nFor debugging process, we hit nan score issue during c++ inference (saved model). i would like to know which intermediate node having issue.  I try to search the official documentation and google, there is no way to dump all tensors in the graph in c++ side during inference.\r\n\r\n**Will this change the current api? How?**\r\nNo, I imagine we can provide an option in the run options, to serialize all tensors after inference into string and put it into RunMetadata.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone?  :)\r\n\r\n**Any Other info.**\r\nIf we already have some ways to dump the all graph tensors in c++ side during inference, please let me know. Very appreciate it!\r\n", "comments": []}, {"number": 42826, "title": "tflite delagte to nn accelerator", "body": "I'm looking to create or contribute to NN delegator which is applicable to NPU, is there any open-source project for delegator which can be (re)used with tflite? I'm not so much interested in GPU, Hexagon DSP etc...", "comments": ["Did you get a chance to see [TensorFlow Lite NNAPI delegate](https://www.tensorflow.org/lite/performance/nnapi) for NPU?\r\n", "yes, somehow - but NNAPI seems to be not enough since HW I used has more capabilities, and seems there some problems with Ops versioning not reflected with NNAPI ...", "@multiverse-tf for custom delegates.\r\n@peter197321 which NPU is this? Or is it your own custom hardware you want to target?", "NNAPI specification has subset of TFLite operators and uses its versioning scheme for operators. If there is a mismatch on operators versioning then the operator is refused to delegate to NNAPI.\r\nWhy the operator (e.g. RELU) is refused to be delegated to NNAPI.", "@multiverse-tf for custom delegates. :) multiverse-tf doesn\u2019t have any public repositories yet.", "> NNAPI specification has subset of TFLite operators and uses its versioning scheme for operators. If there is a mismatch on operators versioning then the operator is refused to delegate to NNAPI.\r\n> Why the operator (e.g. RELU) is refused to be delegated to NNAPI.\r\n\r\nBecause operator version changes mean some change in behavior, and each delegate chooses which version they support. Delegating a newer version might provide an output different from CPU inference.\r\n\r\nCould you specify which ops have this issue, and the version in the model?\r\n(or share the model, even if untrained, for me to inspect myself)", "Here are some ops in different versions: BuiltinOperator_RELU, BuiltinOperator_RELU6, BuiltinOperator_LOGISTIC, BuiltinOperator_FULLY_CONNECTED, BuiltinOperator_MUL, BuiltinOperator_DEQUANTIZE, BuiltinOperator_LOG_SOFTMAX, ...", "One of the problems is that if the operator version does not match then the operator is refused to be delegated over NNAPI and the part of the graph is computed on CPU instead of acceleration.", "@peter197321 That is expected, since the delegate may not perform the same operation as the newest version of the op. Can you specify which version of these ops is present in the model?", "The TensorFlow Lite builtin operators support a limited number of TensorFlow operators, not every model is convertible, and due to NN-API limitation (versioning problems) so is there some full sample delegate for NPU?", "Not that I know of. @multiverse-tf can point to documentation for you to write your own delegate.", "Is there a way to write your own delegate the most efficient way to utilize the Hardware NPU (so NN-API might not be a good fit)?"]}, {"number": 42757, "title": "tensorflowlite v2.3 + XNNPACK run into error {ModifyGraphWithDelegate is disallowed}", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian buster 32-bit\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: RPI2\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: v2.3.0\r\n-   **Python version**: N/A\r\n-   **Bazel version (if compiling from source)**: 3.1.0\r\n-   **GCC/Compiler version (if compiling from source)**:  7.5\r\n-   **CUDA/cuDNN version**: N/A\r\n-   **GPU model and memory**: N/A\r\n-   **Exact command to reproduce**:  N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI built tensorflow `v2.3.0` with XNNPack enabled `--define tflite_with_xnnpack=true` However, when I try to inference with `coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.tflite`, the starter model from [example page](https://www.tensorflow.org/lite/models/object_detection/overview), i see this error \r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nERROR: ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n```\r\n\r\nThe XNNPACK thus was not effective during inference.\r\n\r\nIs this error expected? Is so if there a CPU model that can be used with XNNPACK?  (Right now it seems there is a dilemma that the object_dection optimization detailed [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) is not yet available in tensorflow v2.3.0 but XNNPACK only works with v2.3.0, making `coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.tflite` seeming the only CPU object detection model available at this point.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["cc @multiverse-tf"]}, {"number": 42722, "title": "non_max_suppression outputs each ROI's assignment to non-max supressed bbox", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): \r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.image.non_max_suppresion` returns outputs containing only indices of non-max suppressed bboxes, the requested feature is to also return the assignment indices of deleted ROIs to its' NMS bbox, the usage would be like:\r\n```\r\nselected_indices, assign_nms_indices = tf.image.non_max_suppression(boxes, scores, max_output_size)\r\nassert tf.shape(assign_nms_indices)[0] == tf.shape(boxes)[0]\r\n```\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nWhos want to manipulate the scores within ROIs deleted by NMS and the NMSed bbox.\r\n\r\n**Any Other info.**\r\nApologize for my broken English", "comments": []}, {"number": 42709, "title": "Use of GPU delegate increases inference runtime in Android's Native environment (C++) but not using Java API", "body": "**System information**\r\n\r\n- Have I written custom code: I did\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device: Samsung Galaxy S10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to run a simple UNet model that runs on Android's Native environment and uses the GPU delegate.\r\nI'm testing the same model on `Float32` / `Uint8` fully-quantized (same model, separate `.tflite` files) and also with / without GPU delegate on both cases (so 4 different types of inference in total).\r\n\r\n- When testing with / without the GPU delegate - inference runtimes are the same.\r\n- `ModifyGraphWithDelegate()` returns `kTfLiteOk`, but GPU delegate is not applied, and returns an error:\r\n`Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`\r\n- Model (see below) shouldn't have any dynamic sized tensors.\r\n- When checked the model input Dims `(B, H, W, C)`, I got an unexpected result.\r\n\r\nTensorflow model was built with `(1, 128, 128, 12)`, but with Android and Tensorflow-Lite I get `(4, 1, 128, 128)`.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow model built with `(1, 128, 128, 12)`, should result in Tensorflow-Lite's `(1, 128, 128, 12)` model, and GPU delegate should be applied and reduce inference time on `Float32` type inference.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSample C++ code which runs in Android Studio's NDK (bare code, excluding status and sanity checks):\r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> model;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\nmodel = tflite::FlatBufferModel::BuildFromFile(MODEL_FILENAME_HERE.c_str());\r\nmodel -> error_reporter();\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\nTfLiteGpuDelegateOptionsV2 gpu_opts = {\r\n        .is_precision_loss_allowed = -1,\r\n        .inference_preference =\r\n                TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,\r\n        .inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION,\r\n        .inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n        .inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n        .experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE,\r\n        .max_delegated_partitions = 1,\r\n};\r\ngpu_opts.inference_preference =\r\n        TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;\r\ngpu_opts.inference_priority1 =\r\n        conf.allow_fp16 ? TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY\r\n                : TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;\r\nauto * gpu_delegte = TfLiteGpuDelegateV2Create(&gpu_opts);\r\nif (USE_GPU_DELEGATE_FLAG) {\r\n        interpreter->ModifyGraphWithDelegate(gpu_delegte);\r\n}\r\ninterpreter->AllocateTensors();\r\nint input_tensor_index = interpreter -> inputs()[0];\r\nTfLiteTensor * input_tensor = interpreter -> tensor(input_tensor_index);\r\n\r\n// checkpoint - after here there's more code to set input, invoke interpreter, get inference output, etc..\r\n\r\n...\r\n```\r\n\r\nAt `checkpoint`, get input dims by logging them or any other form for example:\r\n```\r\nint d1 = input_tensor -> dims[0];\r\nint d2 = input_tensor -> dims[1];\r\nint d3 = input_tensor -> dims[2];\r\nint d4 = input_tensor -> dims[3];\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nSimple UNet `.tflite` model files [can be found here](https://gofile.io/d/PVJiBY).\r\n\r\nWhen logging with:\r\n```\r\n\"Input dims are %dx%dx%dx%d\",\r\n                input_tensor -> dims[0],\r\n                input_tensor -> dims[1],\r\n                input_tensor -> dims[2],\r\n                input_tensor -> dims[3]\r\n```\r\nLog shows `Input dims are 4x1x128x128`.\r\n\r\nRunning inference without GPU delegate shows nothing special in log, but running with GPU shows:\r\n`Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`\r\n", "comments": ["I trimmed down the model to a single-operator (`Conv2D`) model ([model file here](https://gofile.io/d/R87ROS)).\r\nThe `Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors` error doesn't occur on the single-operator model.\r\nThe GPU delegate loads successfully, though I didn't use any 'dynamic' operators in the full Simple UNet model - the full model only has `Conv2D`, `MaxPooling2D` and `UpSampling2D`.\r\nAren't they all static?", "I replaced `UpSampling2D` with `Resize` and now it loads the GPU delegate successfully ([model files here](https://gofile.io/d/jw7rSe)), but runtimes are exceptionally long with the GPU delegate running inference times 2x-6x longer (default GPU delegate config) compared to no-delegate inference on F32 (non-quantized) models.", "I don't see any OpenCL / OpenGL errors as described in https://github.com/tensorflow/tensorflow/issues/41769 when running with default GPU delegate options.\r\nWhen adding the `TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY` option:\r\n```\r\n...\r\ngpu_opts.experimental_flags |= TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_QUANT;\r\ngpu_opts.experimental_flags |= TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY;\r\n...\r\n```\r\nand running a quantized model I get the following error:\r\n`E/libEGL: call to OpenGL ES API with no current context (logged once per thread)`", "The error message you quoted is coming from `subgraph.cc`, you can do a quick grep to find out.  Anyway, that means some of the tensor dimensions are not fixed, e.g. the batch dimension or the height / width dimensions.  Those are not supported by the GPU and all tensor dimensions need to be specified without any unknowns.", "> The error message you quoted is coming from `subgraph.cc`, you can do a quick grep to find out. Anyway, that means some of the tensor dimensions are not fixed, e.g. the batch dimension or the height / width dimensions. Those are not supported by the GPU and all tensor dimensions need to be specified without any unknowns.\r\n\r\nThanks @impjdi ,\r\n\r\nPlease note that replacing `UpSampling2D` solved it (I've included the model files [here](https://github.com/tensorflow/tensorflow/issues/42709#issuecomment-683413400)).\r\n\r\nThe issue is that running the same model with the Java API and the C++ API doesn't show the same behavior:\r\nUsing the GPU delegate via the **C++ API** (in native) on a `Float32` model **increases** the inference runtime, while doing so with **Java API** (same model file, same input) **decreases** inference runtime.", "The extra runtime of the GPU delegate in the C++ API comes from here:\r\n`interpreter -> ModifyGraphWithDelegate(gpu_delegte)`\r\n\r\nThere's no extra runtime when using the Java API.\r\nOn the Java API, inference runtimes get significantly shorter (compared to CPU) when using the GPU delegate (on `Float32`).\r\n\r\nJava API is only a wrapper for the C++ API ([here](https://github.com/tensorflow/tensorflow/blob/da2c5d264781f587294c9866d778d22288b3c9cd/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L573) and [here](https://github.com/tensorflow/tensorflow/blob/23d482eaa2efe2bb38de7eb4f89539be9e3aa32a/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java#L336)).\r\nSo what's missing?\r\n\r\nHere's basically how I use the C++ API:\r\n\r\n```\r\n// setup interpreter\r\nstd::unique_ptr<tflite::FlatBufferModel> model;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\nmodel = tflite::FlatBufferModel::BuildFromFile(MODEL_NAME);\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\ninterpreter -> UseNNAPI(false);\r\ninterpreter -> SetAllowFp16PrecisionForFp32(false);\r\ninterpreter -> SetNumThreads(NUMBER_OF_THREADS);\r\n\r\n// setup and create GPU delegate\r\nTfLiteGpuDelegateOptionsV2 gpu_opts = {\r\n        .is_precision_loss_allowed = -1,\r\n        .inference_preference =\r\n                TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED,\r\n        .inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION,\r\n        .inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n        .inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n        .experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_QUANT,\r\n        .max_delegated_partitions = 1,\r\n};\r\nauto * gpu_delegte = TfLiteGpuDelegateV2Create(&gpu_opts);\r\n\r\n// apply GPU delegate\r\ninterpreter -> ModifyGraphWithDelegate(gpu_delegte);\r\n\r\n// allocate tensors, invoke interpreter, etc...\r\n```\r\n\r\nAm I not using the GPU delegate correctly?\r\n\r\nCalling `ModifyGraphWithDelegate` can take 10x longer than calling `interpreter -> Invoke()`.\r\nFor example (using the models I shared) on a 0.2Mp input:\r\n- **Without the GPU delegate**: calling `interpreter -> Invoke()` takes ~2500 [ms].\r\n- **With the GPU delegate**: calling `interpreter -> Invoke()` takes ~650 [ms], but calling `ModifyGraphWithDelegate` takes ~7000 [ms].", "Yeah, OpenCL does an exhaustive tuning for the fastest possible inference when you specify `TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED`.  If faster initialization is of your concern, use `TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER`; inference may not be as optimal but may give you a reasonable init time.", "> Yeah, OpenCL does an exhaustive tuning for the fastest possible inference when you specify `TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED`. If faster initialization is of your concern, use `TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER`; inference may not be as optimal but may give you a reasonable init time.\r\n\r\nThanks @impjdi ,\r\n\r\nI've tried playing around with these flags - this still doesn't explain how the Java API performs much faster comparing to the C++ API, when Java is just a wrapper for the same C++ code (which I assume initializes the GPU delegate with default settings).\r\n\r\n### Example test\r\n\r\n- Model (see [here](https://github.com/tensorflow/tensorflow/issues/42709#issuecomment-683413400)) is `Float32` datatype\r\n- Input (for this test) is a 0.2 Mp image (any image will do, though)\r\n- I'm running an inference in 2 Android apps - one has Java API and the other has C++ API.\r\nThe test includes providing the same input, to the same model, in all test cases.\r\n\r\n### Results\r\n\r\n- 0.2 Mp input / `Float32` model / Java API:\r\nTotal inference runtime - with no delegate: **4057 [ms]**\r\nTotal inference runtime* - with GPU delegate: **541 [ms]**\r\n\r\n- 0.2 Mp input / `Float32` model / C++ API / `TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER` flag:\r\nTotal inference runtime - with no delegate: **2439 [ms]**\r\nTotal inference runtime - with GPU delegate: **3608 [ms]**\r\nNet inference runtime** - with GPU delegate: **843 [ms]**\r\n\r\n- 0.2 Mp input / `Float32` model / C++ API / `TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED` flag:\r\nTotal inference runtime - with no delegate: **2420 [ms]**\r\nTotal inference runtime - with GPU delegate: **7035 [ms]**\r\nNet inference runtime** - with GPU delegate: **680 [ms]**\r\n\r\n\\* _Total inference runtime_ with Java API includes the `ModifyGraphWithDelegate` call runtime\r\n\\*\\* _Net inference runtime_ with C++ API refers to the runtime without `ModifyGraphWithDelegate` runtime\r\n\r\n### Discussion\r\n\r\n- In the case without selecting any delegate, is the inference runtime lower with the C++ API since it uses NNAPI delegate by default?\r\n- When running the inference via Java API, runtime is significantly improved when selecting the GPU delegate, compared to no delegate (more than 7x faster!)\r\n- The runtime when using Java API includes the call to `ModifyGraphWithDelegate`, and is still significantly faster.\r\n- When running the (same) inference via C++ API, runtime isn't improved, and is in fact worse, when selecting the GPU delegate, compared to no delegate (1.5x - 3x slower!)\r\n- Measuring the net inference runtime separately reveals that `ModifyGraphWithDelegate` takes the lion share of the runtime.\r\n- Changing the `inference_preference` flags in the C++ API did affect `ModifyGraphWithDelegate`, but inference runtime was still worse than Java API.\r\n\r\nHow is it possible that in Java API the call to `ModifyGraphWithDelegate` barely takes up any runtime?", "Note that `ModifyGraphWithDelegate` is supposed to be called only once at initialization time and `Invoke` multiple times.  So, you can't really say \"total inference runtime\".   Anyway...\r\n\r\nThere is a possibility that when using Java, you're using OpenGL, and thus you see a much shorter initialization.  Can you check which backend (OpenGL vs OpenCL) is being triggered when using Java's GPU delegate?", "> Note that `ModifyGraphWithDelegate` is supposed to be called only once at initialization time and `Invoke` multiple times. So, you can't really say \"total inference runtime\". Anyway...\r\n\r\n\r\nOn the C++ API app, I manually call `ModifyGraphWithDelegate` only once.\r\nOn the Java API app, the Java wrapper is doing the same - and `ModifyGraphWithDelegate` is called only once.\r\n\r\nFor convenience, let's define \"total inference runtime\" as the time passed between (A) the first call made to any Tensorflow-Lite library function, and (B) the return from the call to the interpreter's `Invoke`.\r\nBy that definition  - the total inference runtime with the Java API is significantly shorter than the C++ API, and the question is, why? (in other words, what calls to Tensorflow-Lite library functions should be made to achieve the same runtime in C++?)\r\n\r\n** All the following inferences were performed with the same input, same model file, etc.\r\n\r\nRunning inference with GPU delegate via **C++ API**, log contains the following:\r\n```\r\n...\r\n15:18:50.182  I/OpenGLRenderer: Davey! duration=984ms; Flags=1, IntendedVsync=5318679868849, Vsync=5318679868849, OldestInputEvent=9223372036854775807, NewestInputEvent=0, HandleInputStart=5318685995521, AnimationStart=5318686085313, PerformTraversalsStart=5318686104167, DrawStart=5319563188854, SyncQueued=5319595133490, SyncStart=5319595886562, IssueDrawCommandsStart=5319595981667, SwapBuffers=5319660922500, FrameCompleted=5319664853489, DequeueBufferDuration=135000, QueueBufferDuration=3184000, \r\n15:18:50.223  I/OpenGLRenderer: Davey! duration=987ms; Flags=0, IntendedVsync=5318717434628, Vsync=5319684101256, OldestInputEvent=9223372036854775807, NewestInputEvent=0, HandleInputStart=5319691200364, AnimationStart=5319691309010, PerformTraversalsStart=5319692114219, DrawStart=5319697963906, SyncQueued=5319700952031, SyncStart=5319702011094, IssueDrawCommandsStart=5319702115885, SwapBuffers=5319704500677, FrameCompleted=5319705990573, DequeueBufferDuration=336000, QueueBufferDuration=881000, \r\n15:19:27.618  I/OpenGLRenderer: Davey! duration=1193ms; Flags=2, IntendedVsync=5355908480170, Vsync=5355908480170, OldestInputEvent=0, NewestInputEvent=0, HandleInputStart=5355908480170, AnimationStart=5355908480170, PerformTraversalsStart=5355908480170, DrawStart=5355908480170, SyncQueued=5355916279882, SyncStart=5355916281496, IssueDrawCommandsStart=5355916887746, SwapBuffers=5356711030350, FrameCompleted=5357101705559, DequeueBufferDuration=344382000, QueueBufferDuration=390205000, \r\n>>> Called Here <<<\r\n15:19:32.689  I/tflite: Initialized OpenCL-based API.\r\n15:19:33.682  I/OpenGLRenderer: Davey! duration=7391ms; Flags=0, IntendedVsync=5355773403908, Vsync=5363056736950, OldestInputEvent=9223372036854775807, NewestInputEvent=0, HandleInputStart=5363072122171, AnimationStart=5363072310817, PerformTraversalsStart=5363073168473, DrawStart=5363115731181, SyncQueued=5363122091129, SyncStart=5363123004619, IssueDrawCommandsStart=5363157759879, SwapBuffers=5363163700556, FrameCompleted=5363165528265, DequeueBufferDuration=976000, QueueBufferDuration=729000, \r\n```\r\n\r\n- NOTE: GPU delegate options are default options, with `experimental_flags` set to `TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_QUANT` (though it's not used - model is `Float32`).\r\n\r\n\r\nRunning inference with GPU delegate via **Java API**, log contains the following:\r\n```\r\n...\r\n>>> Called Here <<<\r\n15:24:28.558  I/tflite: Initialized OpenCL-based API.\r\n15:24:29.962  I/OpenGLRenderer: Davey! duration=1862ms; Flags=0, IntendedVsync=5657581696841, Vsync=5659381696769, OldestInputEvent=9223372036854775807, NewestInputEvent=0, HandleInputStart=5659383255964, AnimationStart=5659383328516, PerformTraversalsStart=5659384314922, DrawStart=5659417704141, SyncQueued=5659423333829, SyncStart=5659424487579, IssueDrawCommandsStart=5659434430547, SwapBuffers=5659441704349, FrameCompleted=5659445346954, DequeueBufferDuration=697000, QueueBufferDuration=2316000, \r\n```\r\n\r\n> There is a possibility that when using Java, you're using OpenGL, and thus you see a much shorter initialization. Can you check which backend (OpenGL vs OpenCL) is being triggered when using Java's GPU delegate?\r\n\r\n1. I tried to change delegate options on C++ API to include `TFLITE_GPU_EXPERIMENTAL_FLAGS_CL_ONLY` or `TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY` but output remain the same - no other OpenCL / OpenGL messages on log other than those seen above\r\n2. HW supports `OpenGL ES 3.2` and `OpenCL 2.0`\r\n3. From your comment here: https://github.com/tensorflow/tensorflow/issues/32840#issuecomment-560536940, I guess that in both cases OpenCL is used.\r\n4. Note `OpenGL duration`s: `7391ms` for the C++ and `1862ms` for the Java.\r\n5. Setting `TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER` does help (duration is `3871ms`), but still Java duration is considerably lower, every time.\r\n\r\nBasically I'm just trying to run an inference via the C++ API, in the same way the Java API does.\r\nAll settings are just basic default options - what could possibly explain the higher performance the Java API have?", "How read the output data? \r\nwhen I call\r\nTfLiteTensor* predict_tensor = Interpreter->tensor(Interpreter->outputs()[1]);\r\noutput = predict_tensor->data.f;\r\nthe output data is Abnormal\u3002", "@orangesomethingorange Sorry for the slow reply here; have missed this update of yours.  I can't think of anything that would make Java execution faster than the C++ build.  Maybe something related to compilation flags?  Are you doing `bazel build --config android_arm64 -c opt` ?\r\n\r\n@ShifengJin `[0]` instead of `[1]` maybe?"]}, {"number": 42694, "title": "Translataion bug in tf.math.reduce_variance and tf.keras.backend.var", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra-10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not checked\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nModel built using tensorflow and later converter to tflite gives different results for same input. See code provided to reproduce the error. Issue [#42346](https://github.com/tensorflow/tensorflow/issues/42346), I created earlier, might have the same error.\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nCode to reproduce error with output\r\n```\r\nimport tensorflow as tf\r\nimport tflite\r\nimport numpy as np\r\nimport imutils\r\n\r\ntf.compat.v1.enable_eager_execution()\r\n\r\nclass MyFunc(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32, name='imdata')])\r\n    def detect(self, imdata):\r\n        # both tf.keras.backend.var and tf.math.reduce_variance operations create the same error scores\r\n        #tensor_var = tf.keras.backend.var(imdata)\r\n        tensor_var = tf.math.reduce_variance(imdata)\r\n        return tensor_var\r\ntf_func = MyFunc()\r\n\r\nconc_func = tf_func.detect.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([conc_func])\r\nfpath = 'model.tflite'\r\nwith tf.io.gfile.GFile(fpath, 'wb') as f:\r\n    f.write(converter.convert())\r\nloaded_model = tf.lite.Interpreter(fpath)\r\n\r\ndef tflite_inference(tmp_model, tmp_img_data):\r\n    input_details = tmp_model.get_input_details()\r\n    output_details = tmp_model.get_output_details()\r\n    input_data = np.array(tmp_img_data).astype(input_details[0]['dtype'])\r\n    w_img, h_img, ch_img = tmp_img_data.shape\r\n    tmp_model.resize_tensor_input(input_details[0][\"index\"], [w_img, h_img, 3])\r\n    tmp_model.allocate_tensors()\r\n    tmp_model.set_tensor(input_details[0]['index'], tmp_img_data)\r\n    tmp_model.invoke()\r\n    result = tmp_model.get_tensor(output_details[0]['index'])\r\n    return result\r\n\r\n\r\nimages = ['https://images.unsplash.com/photo-1470020337050-543c4e581988',\r\n          'https://images.unsplash.com/photo-1516811108838-030371f93644',\r\n          'https://zipbooks.com/wp-content/uploads/2017/05/royalty-free-images-free-of-charge.jpeg',\r\n       ]\r\n\r\nprint(\"tensorflow vs tflite inference scores on images\")\r\nfor img_url in images:\r\n    tmp_imdata = imutils.url_to_image(img_url).astype(np.float32)\r\n    print(f'\\t tensorflow: {tf_func.detect(tmp_imdata).numpy():.4f}', end='\\t')\r\n    print(f'tflite: {tflite_inference(loaded_model, tmp_imdata):.4f}')\r\n\r\n    \r\nprint(\"\\n\\n tensorflow vs tflite inference scores on random arrays\")\r\nshape = (2000, 2000, 3)\r\nnum_range = 255\r\narrays = [np.random.randint(num_range, size=shape).astype(np.float32), \r\n          np.random.randint(num_range, size=shape).astype(np.float32),\r\n          np.random.randint(num_range, size=shape).astype(np.float32)\r\n         ]\r\nprint('\\t tensorflow \\t tflite \\t   Equal?')\r\nfor arr in arrays:\r\n    tf_score = tf_func.detect(arr).numpy()\r\n    tflite_score = tflite_inference(loaded_model, arr)\r\n    eq = tf_score == tflite_score\r\n    print(f'\\t {tf_score:.4f} \\t {tflite_score:.4f} \\t   {eq}')\r\n```\r\n\r\n**The output from the converter invocation** : \r\n```\r\ntensorflow vs tflite inference scores on images\r\n\t tensorflow: 4722.1460\ttflite: 4706.3296\r\n\t tensorflow: 5759.6831\ttflite: 4486.9917\r\n\t tensorflow: 1643.2089\ttflite: 1642.7393\r\n\r\n\r\n tensorflow vs tflite inference scores on random arrays\r\n\t tensorflow \t tflite \t   Equal?\r\n\t 5417.6357 \t 5351.8447 \t   False\r\n\t 5418.3472 \t 5352.1660 \t   False\r\n\t 5419.2515 \t 5353.3892 \t   False\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef** : Unzip file [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/5133550/model.tflite.zip) .\r\n\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy: Wrong results\r\n- Producing correct results, but the model is slower than expected (model generated from old converter): No\r\n\r\n\r\n**Any other info / logs**\r\nSee above for source code and logs.\r\n", "comments": ["I have tried in colab with TF versions 2.3, nightly versions (`2.4.0-dev20200826`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5b490846517f4d553c747ddc00cd83d7/untitled272.ipynb).Thanks!", "There are three ops in the execution plan for the (tflite) model attached: MEAN, SQUARED_DIFFERENCE, MEAN. These come from the TF implementation of [reduce_variance](https://www.tensorflow.org/api_docs/python/tf/math/reduce_variance).\r\n\r\nI tried playing around with @ravikyram 's gist, and looks like the accuracy issue might stem from TFLite's reduction ops. I modified the script to use reduce_sum instead of reduce_variance:\r\n\r\n```\r\nclass MyFunc(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32, name='imdata')])\r\n    def detect(self, imdata):\r\n        tensor_var = tf.math.reduce_sum(imdata)\r\n        return tensor_var\r\ntf_func = MyFunc()\r\n```\r\n\r\nIn this case, for the random inputs generated at the end of the script, TF & TFLite outputs match upto a certain dim size (200, 200, 3):\r\n\r\n```\r\nprint(\"\\n\\n tensorflow vs tflite inference scores on random arrays\")\r\nshape = (200, 200, 3)\r\nnum_range = 255\r\narrays = [np.random.randint(num_range, size=shape).astype(np.float32), \r\n          np.random.randint(num_range, size=shape).astype(np.float32),\r\n          np.random.randint(num_range, size=shape).astype(np.float32)\r\n         ]\r\nprint('\\t tensorflow \\t tflite \\t   Equal?')\r\nfor arr in arrays:\r\n    tf_score = tf_func.detect(arr).numpy()\r\n    tflite_score = tflite_inference(loaded_model, arr)\r\n    eq = tf_score == tflite_score\r\n    print(f'\\t {tf_score:.4f} \\t {tflite_score:.4f} \\t   {eq}')\r\n``` \r\n\r\nHowever, for larger float32 inputs, there is a difference between what TF & TFLite provide.\r\nThe dip in accuracy gradually grows from (say) 230x230x3 to 500x500x3, and becomes much larger for the dimensions tried by @naolenikhil (2000x2000x3).\r\n\r\nreduce_min & reduce_max don't have this issue, so there might be some problem with the accumulators used by TFLite kernels?", "@talumbau Would you have an idea of what might be the issue here? (Refer comment above)", "I could replicate this issue in [2.8 version](https://colab.sandbox.google.com/gist/mohantym/7f529af5b3f5ad51a578856e84d8b434/gtihub_42694.ipynb) too . "]}, {"number": 42688, "title": "individual neuron pruning (similar to tfkerassurgeon)", "body": "Just wanted to bring up that existing pruning packages (tfkerassurgeon and keras-surgeon https://github.com/Raukk/tf-keras-surgeon) are no longer supported or compatible with Tensorflow, so it'd be great if Tensorflow could implement their own simple individual neuron pruning capability. I'm aware of the existing Tensorflow APIs that focus on automated low-magnitude pruning(https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide), but being able to simply delete specific neurons/channels from layers or delete specific layers would be useful, and aid in the study of neural networks. For example, something like:\r\n```\r\n# delete layer_1 from a model\r\nmodel = delete_layer(model, layer_1)\r\n# delete channels 0, 4 and 67 from layer_2 in model\r\nmodel = delete_channels(model, layer_2, [0,4,67])\r\n```\r\nThis allows researchers and scientists much more flexibility in deciding how exactly they want to prune their model, and foster development of new pruning techniques", "comments": ["Any update on this?"]}, {"number": 42681, "title": "ImageDataGenerator should have a verbosity argument", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen calling [`tensorflow.keras.preprocessing.image.ImageDataGenerator.flow_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory), currently a message like `Found 1437 images belonging to 2 classes.` is printed.\r\n\r\nI suggest adding an optional keyword argument `verbose` with default setting `verbose=1` to this function, such that `verbose=0` suppresses this printed message. See e.g. the `verbose` keywords of [`tf.keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), or of the [`EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) or [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callbacks.\r\n\r\n**Will this change the current api? How?**\r\nThis will add an optional keyword argument.\r\n\r\n**Who will benefit with this feature?**\r\n\r\n* Everyone who needs to call this function repeatedly in order to loop over different `batch_size` values during hyperparameter tuning. In this case, the repeated message is uninteresting and distracts from the interesting output and should therefore be omittable.\r\n* Everyone who upgraded from standalone Keras (which did not show this message, e.g. version 2.2.4), who wants to get the same output as before.\r\n", "comments": ["Btw., standalone Keras (at least 2.2.4) did not print this message. I have now added this to the issue description.", "I am a new contributor to tensorflow, and changed and added feature. But i can't push branch. shows error like this\r\n\r\n`remote:` Permission to tensorflow/tensorflow.git denied to anuragcp.", "@anuragcp, you have to push it to your own fork, then create a pull request.", "Okey thanks. Let me check.", "I now found the relevant code location: It's [in directory_iterator.py](https://github.com/keras-team/keras-preprocessing/blob/f69929ff67518d89274ae4790d9517dc4f1e5ec1/keras_preprocessing/image/directory_iterator.py#L144) of the project keras_preprocessing!\r\n\r\nHere is a similar issue: https://github.com/keras-team/keras-preprocessing/issues/304 .\r\n", "Hi @jondo , is this what you are looking for to implement, I don't know how correct this https://www.tensorflow.org/api_docs/python/tf/compat/v1/logging is as it is still in version 1 and there seem to be no ports to 2 and the only reference to it I found was for `tf.autograph.set_verbosity`. I hope this helps.\r\nAn simple high-level call I found was in `keras.callbacks`\r\n\r\n`\r\nif self.verbose > 0:\r\nprint('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\r\n` \r\n\r\nin `tf./python./keras./callbacks` using `progbar`\r\n\r\n\r\n\r\n", "According to the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0), current Tensorflow 2.3.0 introduced a new Keras dataset generation utility (emphasis added):\r\n\r\n> [`image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) is a utility based on `tf.data.Dataset`, meant to replace the **legacy `ImageDataGenerator`**. It takes you from a structured directory of images to a labeled dataset, in one function call. Note that it doesn't perform image data augmentation (which is meant to be done using preprocessing layers).\r\n\r\nThis means that I'll simply comment out the annoying `print` command locally for now."]}, {"number": 42680, "title": "adapt() method of a CategoryEncoding layer returns the wrong shape when fitted to Integer list not starting at 0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0-dev20200825\r\n- Python version: 3.7.8\r\n\r\n**Describe the current behavior**\r\nWhen calling \"adapt()\" method on a CategoryEncoding layer with either:\r\n- a dataset feature with integers not starting at 0 (eg. month identifiers from 1 to 12)\r\n- a np.array with a list of possible integers not starting at 0\r\nthe resulting shape of the CategoryEncoding layer seems to be determined by the max integer, and not the cardinality of integers in the list / data.\r\neg, we get a shape of 13 if we adapt the layer to an integer list ranging from 1 to 12, or even from 3 to 12 for example\r\n\r\n**Describe the expected behavior**\r\nExpect the adapt method of a CategoryEncoding layer to output an object with a shape equal to the number of unique values in the integer-coded feature\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmonth_input_layer = tf.keras.layers.Input(shape=(1,), name=\"month\", dtype=\"int64\")\r\nmonth_encoded_layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(output_mode=\"binary\")\r\ndense_layer = tf.keras.layers.Dense(1,activation=\"relu\")\r\n\r\nmonth_encoded_layer.adapt(np.arange(1,13))\r\noutput = dense_layer(month_encoded_layer(month_input_layer))\r\n\r\nmodel = tf.keras.models.Model(month_input_layer, output)\r\n\r\nmodel.summary()\r\n```\r\n\r\n**Other info / logs**:\r\n```\r\n\r\n2020-10-05 10:12:27.032788: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-05 10:12:27.103511: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1dbad9023f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-05 10:12:27.112275: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n(.venv) PS C:\\Users\\SamuelCheptou\\CSN Energy\\March\u00e9 - Paris Restricted - General\\5# Ressources & Donn\u00e9es\\CDC\\Analyses CDC forecasting\\scripts python SC>  cd 'c:\\Users\\SamuelCheptou\\CSN Energy\\March\u00e9 - Paris Restricted - General\\5# Ressources & Donn\u00e9es\\CDC\\Analyses CDC forecasting\\scripts python SC'; & 'c:\\Users\\SamuelCheptou\\local_CDC\\.venv\\Scripts\\python.exe' 'c:\\Users\\SamuelCheptou\\.vscode\\extensions\\ms-python.python-2020.9.112786\\pythonFiles\\lib\\python\\debugpy\\launcher' '57146' '--' 'c:\\Users\\SamuelCheptou\\CSN Energy\\March\u00e9 - Paris Restricted \r\n- General\\5# Ressources & Donn\u00e9es\\CDC\\Analyses CDC forecasting\\scripts python SC\\Github_bug.py' \r\n2020-10-05 10:31:37.812387: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-05 10:31:37.833441: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x241a1d351f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-05 10:31:37.861026: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nmonth (InputLayer)           [(None, 1)]               0\r\n_________________________________________________________________\r\ncategory_encoding (CategoryE (None, 13)                1\r\n_________________________________________________________________\r\ndense (Dense)                (None, 1)                 14\r\n=================================================================\r\nTotal params: 15\r\nTrainable params: 14\r\nNon-trainable params: 1\r\n_________________________________________________________________\r\n\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/78ee390cd9917f7a73037611f844d214/42680.ipynb). Thanks!", "Hello, any update on this issue ? thanks!", "@scd75 \r\nI ran the code on nightly '2.5.0-dev20210114', please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/c88dd3d964b7712420ce4e64acc26191/untitled499.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "can somebody take a look at this issue ?\r\nIt has now been reproduced 2 times, but no solution was given so far.. Thanks!", "@scd75 \r\nPlease try on latest version [nightly[ and let us know if you still face the issue.", "Yes I just tried and still face the issue on the last nightly. The shape of the \"category_encoding\" layer should be (None, 12) instead of (None, 13)", "You could verify in TF 2.5 but:\r\n\r\n@scd75  So you need   specify the `num_tokens` arg in `tf.keras.layers.experimental.preprocessing.CategoryEncoding`\r\n\r\nOR\r\n\r\n> For integer inputs where the total number of tokens is not known, see tf.keras.layers.experimental.preprocessing.IntegerLookup", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "TF 2.5 raises `ValueError: num_tokens must be set to use this layer. If the number of tokens is not known beforehand, use the IntegerLookup layer instead.` \r\nSwitching to `IntegerLookup layer` results in same reported behavior. See [gist](https://colab.research.google.com/gist/ymodak/fdb09c83f0b45fa48ecce393faadf2f2/untitled10.ipynb)"]}, {"number": 42660, "title": "from_dlpack unable to process arrays with column-major strides", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.3.0\r\n- Python version:\r\n3.6.9/3.7\r\n- CUDA/cuDNN version:\r\n10.1/7\r\n- GPU model and memory:\r\nK80/V100-32GB\r\n\r\n\r\n**Describe the current behavior**\r\nTrying to pass dlpack capsules to TensorFlow that come from arrays with column-major strides, like those leveraged by CuDF, raises an `InvalidArgumentError`. It's possible to work around this by transposing the array first, passing it to TensorFlow via dlpack, then transposing back, but obviously this is less than ideal.\r\n\r\n**Describe the expected behavior**\r\nThese arrays should be read properly without needing to transpose first.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf, cupy as cp\r\n\r\n# initialize tf\r\nx = tf.random.uniform((1,))\r\n\r\ntf.experimental.dlpack.from_dlpack(cp.asfortranarray(cp.ones((5, 2))).toDlpack())\r\n```\r\n\r\nOutput should be\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-cb770ae77ab3> in <module>()\r\n      1 import tensorflow as tf, cupy as cp\r\n      2 x = tf.random.uniform((5,))\r\n----> 3 tf.experimental.dlpack.from_dlpack(cp.asfortranarray(cp.ones((5, 2))).toDlpack())\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/dlpack/dlpack.py in from_dlpack(dlcapsule)\r\n     64     A Tensorflow eager tensor\r\n     65   \"\"\"\r\n---> 66   return pywrap_tfe.TFE_FromDlpackCapsule(dlcapsule, context.context()._handle)\r\n\r\nInvalidArgumentError: Invalid strides array from DLPack\r\n```\r\nSee notebook [here](https://colab.research.google.com/drive/1ALUBTckq2ycXwQDIBNUzifYWJl9UJLN1?usp=sharing)\r\n\r\n---------------------------\r\n@miguelusque for viz\r\n", "comments": ["I don't think this can be fixed easily -- `tensorflow::Tensor` does not have an independent layout that can be set.  We can, however, do an in-memory transpose, but it won't be free.", "If that can simplify the API to avoid having to do checks on the shape and layout of the incoming array, that seems like it might be worth it even if the performance is less than optimal", "CC @VoVAllen \r\n\r\nI'm slightly worried that this might break the (implicit) promise that transferring tensors via dlpack is \"free\", but I'm happy to be corrected.", "Hmm that's an interesting and valid point. Would a warning be a potential middle ground?", "> Would a warning be a potential middle ground?\r\n\r\nWDYT about adding a `allow_noncanonical_layouts` boolean argument to `tf.experimental.dlpack.from_dlpack`?  If this arg is set then we do a transpose, otherwise we don't.  That way it is obvious at the call site if a transpose may happen.", "I don't think it's a good idea to make it happen in the `from_dlpack` function. I would prefer it to be an option in the `to_dlpack` in cupy or other library, such as `force_canonical_layouts`. Since transpose seems always needed, I would prefer it happening at the source framework side. \r\n\r\nProject DGL also met similar problem, that the tensor might be non-alignment on cpu, which would cause error in tensorflow. Our solution is add an argument to force the alignment at DGL side. I think the case here is similar. ", "Hey, cuDF maintainer here and we produce a column major dlpack object since it makes the most sense given a columnar DataFrame.\r\n\r\n> I'm slightly worried that this might break the (implicit) promise that transferring tensors via dlpack is \"free\", but I'm happy to be corrected.\r\n\r\nRegardless of whether the transpose happens at the source or destination this promise is broken. I guess you could argue that doing it at the source is the user opting in to breaking that promise more explicitly, but most other DLPack adopters (CuPy, PyTorch, MXNet, etc.) all either support a column major memory layout or implicitly handle the transpose.\r\n\r\n> I don't think it's a good idea to make it happen in the `from_dlpack` function. I would prefer it to be an option in the `to_dlpack` in cupy or other library, such as `force_canonical_layouts`. Since transpose seems always needed, I would prefer it happening at the source framework side.\r\n\r\nI'm against this, because not every producer has a transpose implementation or wants to add a transpose implementation. I.E. cuDF will produce a column major DLPack object via copy since our columns are individual allocations normally, and we don't want to transpose the dataframe since that's extremely expensive and implementing an array transpose is out of scope for a dataframe library. This will then force users to hop through something like CuPy to do the array transpose, which is non-obvious for non-power users.", "Hi,\r\n\r\nCould transpose be avoided? I'm wondering what's your downstream usage. What would be the API exposed to user in cuDF? Could user/downstream task accept a transposed tensor as a starting point? (i.e. `to_tf_tensor(transpose=True)` by default). \r\n\r\n", "Also there's possibility to avoid transpose with copy if it's handling at the source framework, but there's no way to avoid this at tensorflow side. Source framework may have the cache of the transposed tensor/make it a transposed tensor at creation, which improves the efficiency.\r\n\r\nAnd I don't think there's existing op to make DLPack tensor row-major. May need time to implement so if needed", "> Could transpose be avoided? I'm wondering what's your downstream usage. What would be the API exposed to user in cuDF? Could user/downstream task accept a transposed tensor as a starting point? (i.e. `to_tf_tensor(transpose=True)` by default).\r\n\r\ncuDF exposes an api of `df.to_dlpack()` which returns a `PyCapsule` wrapping a `DLTensor`. It similarly exposes an api of `cudf.from_dlpack(...)` which takes in a `PyCapsule` wrapping a `DLTensor` and returns either a Column or a DataFrame depending on if the tensor is 1d or 2d.\r\n\r\ncuDF is a column major dataframe with separate allocations for each columns. To produce a DLPack object we copy each column into a new dense column major array. Given we're a DataFrame library, our stance has been that we produce the dense column major array in the most efficient way possible for us and then allow the downstream array/tensor framework (CuPy, PyTorch, MXNet, Tensorflow, etc.) handle the transpose if needed as they're more well equipped to do so than we are in cuDF.", "Hi!\r\n\r\nI was wondering if there is any progress on this issue.\r\n\r\nThanks!\r\n\r\nRegards,\r\nMiguel", "Hi @alecgunny  Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "@sushreebarsa I'm actually no longer a part of the team on whose behalf I filed this issue. @miguelusque it looks like the [corresponding workaround](https://github.com/NVIDIA/NVTabular/blob/a04cbeed73a4ec7ad840471f2cd8db1ea6110ff1/nvtabular/loader/tensorflow.py#L334) is still being used in NVTabular, so I would recommend getting in touch with @EvenOldridge about whether someone from that team is still interested in taking a look at this.", "@EvenOldridge, can you please respond to the above comment?\r\n\r\n", "@sushreebarsa Chiming in from @EvenOldridge's team: I'm able to reproduce this issue with Tensorflow 2.5.0 and CuPy 9.2.0, so I believe this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42660\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42660\">No</a>\n", "I think this issue is still valid"]}, {"number": 42649, "title": "[tflite] Conv2DTranspose output difference between mobile CPU and mobile GPU ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Windows 10, Android\r\n- Mobile device: Galaxy Fold, Pixel 3\r\n- TensorFlow installed from binary\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- TensorFlow Lite version  ['org.tensorflow:tensorflow-lite:0.0.0-nightly'](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-39e7d8c00954e920b98e7636f0ac30b2R46) and 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: Adreno 640\r\n\r\n**Describe the current behavior**\r\nIn a specific situation, `tf.keras.layers.Conv2DTranspose` [executed on mobile CPU and on mobile GPU](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-31e60fe4136762a27be59770ef684d01R71) returns different outputs -the mean difference between the output from CPU and output from GPU is  ~0.1 - 0.7 but the difference should be ~1e-07.\r\n([my metric](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-31e60fe4136762a27be59770ef684d01R117) - mean difference => abs(cpu_output - gpu_output) => and it shoould be close to zero)\r\n\r\nSpecific situation:\r\n* tf.keras.layers.Conv2DTranspose\r\n* Input shape has odd width (NWHC) e.x. (1, 9, 6, 45)\r\n* padding='same', kernel_size=(5, 3), strides=(2, 1),\r\n\r\n[model](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-98d852ec33b2563b33b6c75585566a06R5)\r\n\r\n[incorrect model - model_01](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-98d852ec33b2563b33b6c75585566a06R52)\r\n\r\n**Describe the expected behavior**\r\nThe difference between the two outputs should be close to zero.\r\n\r\n**Standalone code to reproduce the issue**\r\n[https://github.com/koodzi/tflite-comparing-cpu-gpu-out](https://github.com/koodzi/tflite-comparing-cpu-gpu-out)\r\n\r\n**Other info/logs** Include any logs or source code that would be helpful to\r\n![diff](https://user-images.githubusercontent.com/18346395/91163679-42683b00-e6ce-11ea-8d6f-bb3128fe3a00.png)\r\n", "comments": ["@koodzi It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version of TF 2.6 and let us know if the issue still persists? Thanks!", "@kumariko the problem still persists \r\nI have updated the TensorFlow and TFLite to 2.6 and created new models, but there is still difference between CPU and GPU outputs on Pixel3 and Samsung Fold1. \r\n\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 42637, "title": "Running the Smart Replies Model in TFLite Python", "body": "I'm experimenting with the [Smart Reply][1] Lite model in Python. I used Bazel to compile TFLite with the custom ops  needed to run this model (normalize.cc, predict.cc, extract_features.cc) with the help of this [fantastic tutorial][2] and now I'm trying to run inference. \r\n\r\nHere's the code I use:\r\n```python\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    tflite_interpreter = tf.lite.Interpreter(model_path='smartreply.tflite')\r\n    \r\n    tflite_interpreter.allocate_tensors()\r\n    input_details = tflite_interpreter.get_input_details()\r\n    output_details = tflite_interpreter.get_output_details()\r\n    \r\n    # print(input_details)\r\n    # print(output_details)\r\n    \r\n    tflite_interpreter.set_tensor(input_details[0]['index'], 'Where are you?')\r\n    # Run inference\r\n    tflite_interpreter.invoke()\r\n    # Get prediction results\r\n    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0])\r\n    print(\"Prediction results shape:\", tflite_model_predictions)\r\n```\r\n\r\nIn doing so I got the following error:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 12, in <module>\r\n    tflite_interpreter.set_tensor(input_details[0]['index'], 'Where are you?')\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 175, in set_tensor\r\n    self._interpreter.SetTensor(tensor_index, value)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 136, in SetTensor\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_SetTensor(self, i, value)\r\nValueError: numpy array had 56 bytes but expected 0 bytes.\r\n```\r\nI tried resizing the tensor with this line of code before calling `tflite_interpreter.allocate_tensors()`:\r\n```python\r\ntflite_interpreter.resize_tensor_input(0, [56])\r\n```\r\nBut that raised `ValueError: Cannot set tensor: Dimension mismatch`. Similarly, if I try to change tensor shape to [1,56] it still fails with the same error. \r\n\r\nMy understanding is, the string is converted to a numpy array(based on the model description, the input type is int32 - 4 bytes per character).\r\nWhat changes do I need to make to this input method to run this model?\r\n\r\n\r\n  [1]: https://www.tensorflow.org/lite/models/smart_reply/overview\r\n  [2]: https://medium.com/@bsramasubramanian/running-a-tensorflow-lite-model-in-python-with-custom-ops-9b2b46efd355", "comments": ["I noticed a similar issue with the [BERT_qa](https://www.tensorflow.org/lite/models/bert_qa/overview) model where the two inputs and output have a fixed shape of (1,384) and type int32.\r\nINPUTS:\r\n1. Passage about a topic\r\n2. Question on topic\r\n\r\nOUTPUT:\r\n1. Predicted Answer\r\n\r\n```\r\n[{'name': 'input_ids', 'index': 5422, 'shape': array([  1, 384], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}, {'name': 'input_mask', 'index': 5423, 'shape': array([  1, 384], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}, {'name': 'segment_ids', 'index': 5424, 'shape': array([  1, 384], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}]\r\n\r\n[{'name': 'end_logits', 'index': 5421, 'shape': array([  1, 384], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'start_logits', 'index': 5425, 'shape': array([  1, 384], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n```", "Hi, I'd love to hear if there's been any progress on this front?", "@videetparekh Were you able to get it to work in python?", "No, sorry. Received no support from the Google team. ", "@videetparekh  How did you implement this feature in js/python? \r\nI saw an unofficial port, but didn't work for me. Building from scratch isn't going to be very efficient.", "Yeah! I could replicate this issue in the [2.8 ](https://colab.sandbox.google.com/gist/mohantym/dc06b7051441919b4a677a61a485145b/tensorflow-ranking.ipynb#scrollTo=2Wr1rC6nqWKf)version . Thanks!"]}, {"number": 42621, "title": "CoordConv", "body": "**System information**\r\n- TensorFlow version 2.3.0 and everything latest and greatest:\r\n- Are you willing to contribute it (Yes/No): Depends, there are already a few written by developers that might be a solution for this request.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBasically a gradient matrices before the convolutional operation (Conv1D, Conv2D, Conv3D) that gives more information about convolutional operation place in the image.\r\n\r\n**Will this change the current api? How?**\r\nI don't think so. CoordConv can be added as a separate layer before the convolutional operation.\r\n\r\n**Who will benefit with this feature?**\r\nIn general, all the segmentation or detection task in which objects transition in the image is important\r\n\r\n**Any Other info.**\r\nArticle: [https://arxiv.org/abs/1807.03247](https://arxiv.org/abs/1807.03247) and original implementation by authors [https://github.com/uber-research/CoordConv](https://github.com/uber-research/CoordConv)\r\nVideo explanation: [https://www.youtube.com/watch?v=8yFQc6elePA](https://www.youtube.com/watch?v=8yFQc6elePA)\r\nPossible implementation: [https://github.com/mvoelk/keras_layers](https://github.com/mvoelk/keras_layers)\r\nSimilar feature issue/mention : #32222\r\n", "comments": ["Hi,\r\nRegarding CoordConv implementation, I would at the service for contribution, even in C++ side :)"]}, {"number": 42616, "title": "Memory leak when using MultiWorkerMirroredStrategy for distributed training", "body": "**System information**\r\n\r\n- Have I written custom code: YES\r\n- OS Platform and Distribution: CentOS 7.3\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.3.0\r\n- Python version:3.7.7\r\n- CPU ONLY\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I use `MultiWorkerMirroredStrategy` for distributed training, as the number of training epochs increases, memory usage of tensorflow is also increasing, until beyond the memory limitation.\r\n\r\nBut the memory usage of stand-alone(not distributed) training is always stable.\r\n\r\nBecause I use cpu only for distributed training, I can't get any memory infomation from tensorboard using profiler.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nNote that I don't know how to use `MultiWorkerMirroredStrategy` in `colab`, so I just give the reproduce steps here, and it's very easy.\r\n\r\n1. Training Code (worker.py)\r\n\r\n```python\r\nimport os\r\nimport json\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom absl import app, flags\r\nimport numpy as np\r\n\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_string(\"logs\", \"logs\", \"logs dir\")\r\nflags.DEFINE_integer(\"index\", 0, \"worker index\")\r\n\r\nclass ThreeLayerMLP(keras.Model):\r\n    def __init__(self, name=None):\r\n        super().__init__(name=name)\r\n        self.dense_1 = layers.Dense(32, activation='relu', name='dense_1')\r\n        self.dense_2 = layers.Dense(16, activation='relu', name='dense_2')\r\n        self.pred_layer = layers.Dense(\r\n            1,\r\n            activation='sigmoid',\r\n            name='predictions',\r\n        )\r\n\r\n    def call(self, inputs, training=None):\r\n        print(inputs.shape)\r\n        x = self.dense_1(inputs)\r\n        x = self.dense_2(x)\r\n        return self.pred_layer(x)\r\n\r\n\r\ndef prepare_data():\r\n    np.random.seed(0)\r\n    x_train, y_train = (\r\n        np.random.random((6000000, 31)),\r\n        np.random.randint(2, size=(6000000, 1)),\r\n    )\r\n\r\n    x_val, y_val = (\r\n        np.random.random((10000, 31)),\r\n        np.random.randint(2, size=(10000, 1)),\r\n    )\r\n\r\n    return ((x_train, y_train), (x_val, y_val))\r\n\r\n\r\ndef main(argv):\r\n    del argv  # Unused args\r\n    tf_config = {\r\n        \"cluster\": {\r\n            \"worker\": [\"ip1:12345\", \"ip2:12345\"],\r\n        },\r\n        \"task\": {\r\n            \"index\": FLAGS.index,\r\n            \"type\": \"worker\"\r\n        }\r\n    }\r\n    os.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\r\n    print(json.loads(os.environ[\"TF_CONFIG\"]))\r\n    # distributed strategy\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    BATCH_SIZE_PER_REPLICA = 128\r\n    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n    print('Number of devices: %d' % strategy.num_replicas_in_sync)\r\n\r\n    with strategy.scope():\r\n        model = ThreeLayerMLP(name='3_layer_mlp')\r\n        model.compile(\r\n            loss=tf.keras.losses.BinaryCrossentropy(),\r\n            optimizer=keras.optimizers.RMSprop(),\r\n            metrics=[\"AUC\"],\r\n        )\r\n\r\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n        log_dir=FLAGS.logs,\r\n        histogram_freq=10,\r\n        update_freq=100,\r\n    )\r\n\r\n    ((x_train, y_train), (x_val, y_val)) = prepare_data()\r\n\r\n    model.fit(\r\n        x_train,\r\n        y_train,\r\n        epochs=100,\r\n        batch_size=BATCH_SIZE,\r\n        validation_data=(x_val, y_val),\r\n        callbacks=[tensorboard_callback],\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(main)\r\n```\r\n\r\n2. Distributed training: change the `ip1` and `ip2` to your machine's ip in the codes above, and execute the command below seperately:\r\n\r\n```shell\r\npython worker.py --index=0\r\npython worker.py --index=1\r\n```\r\n\r\n3. The memory change curve of distributed training in my machine is shown as below\uff1a\r\n![image](https://user-images.githubusercontent.com/15494997/91021608-94ee1c80-e626-11ea-9adc-c775b3ff575a.png)\r\n\r\n4. The memory usage of stand-alone training is only 3-4G.", "comments": ["Hi @AlexanderJLiu, do you observe the same behavior when you remove the callbacks? \r\nI did not realize that with CPU only configurations you can't use the memory page of the Profiler. Since you didn't use the profiler, what did you use instead to plot the usage? \r\nIn the meantime, I will try and reproduce this behavior this on my end. ", "@nikitamaia Thanks for your quick response.\r\n\r\n> do you observe the same behavior when you remove the callbacks?\r\n\r\nYes, when I remove the callbacks, the same behavior occurs.\r\n\r\n> I did not realize that with CPU only configurations you can't use the memory page of the Profiler. Since you didn't use the profiler.\r\n\r\nI've used the profiler in another training by setting `profile_batch='100,1000',` in tensorboard callback.\r\n\r\nAnd the memory tool of tensorboard is shown as below: (Other tool like overview_page works well)\r\n\r\n![image](https://user-images.githubusercontent.com/15494997/91115748-78e98a00-e6bd-11ea-8a3d-10076c67e517.png)\r\n\r\nAccording to the comment [here](https://github.com/tensorflow/tensorflow/issues/42123#issuecomment-675047711), may be the memory tool is designed for device (i.e. GPU or TPU) not on the host (CPU). But I'm not pretty sure.\r\n\r\n> what did you use instead to plot the usage?\r\n\r\nI use `hadoop yarn` for scheduling tensorflow distributed training, so the memory usage is plotted by yarn web.\r\n\r\n> In the meantime, I will try and reproduce this behavior this on my end.\r\n\r\nThanks again, I have been troubled by this problem for a long time.\r\n", "Can you report if you see the memory leak when using a single machine with `MultiWorkerMirroredStrategy`? Despite the name, `MultiWorkerMirroredStrategy` can be used on a single machine without additional setup. \r\nAdditionally, can you provide the logs for the training with two machines?", "> Can you report if you see the memory leak when using a single machine with MultiWorkerMirroredStrategy?\r\n\r\nThere is no memory leak using  a single machine with MultiWorkerMirroredStrategy just like stand-alone training without strategy. Their memory usage is around **2.6 G**.\r\n\r\n---\r\nIn addition the memory usage diagram I've provide is not very suitable, because it also contain the memory usage of jvm(yarn container). \r\nBut the distributed training memory  is indeed growing as the steps continue, every worker will reach to **6.3G** and more.\r\n\r\nI could understand that distributed training may take up more memory, but not so much, I mean 2-3 times of stand-alone training.\r\n\r\n---\r\n\r\nBelow are logs of two machines:\r\n\r\n1. chief worker:\r\n\r\n```shell\r\n{'cluster': {'worker': ['host1:12345', 'host2:12345']}, 'task': {'index': 0, 'type': 'worker'}}\r\n2020-08-27 15:38:20.297571: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-27 15:38:20.375920: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2194920000 Hz\r\n2020-08-27 15:38:20.390435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdde155bc60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-27 15:38:20.390500: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-27 15:38:20.513077: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> host1:12345, 1 -> host2:12345}\r\n2020-08-27 15:38:20.528659: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://host1:12345\r\nINFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0']\r\nI0827 15:38:20.540649 140590888851264 collective_all_reduce_strategy.py:329] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0']\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0',)\r\nI0827 15:38:20.543771 140590888851264 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:0',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\nI0827 15:38:20.544113 140590888851264 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\nNumber of devices: 2\r\n2020-08-27 15:39:16.963784: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'\r\nI0827 15:39:19.306446 140590888851264 distribute_coordinator.py:783] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0827 15:39:19.306725 140590888851264 distribute_coordinator.py:832] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nW0827 15:39:19.306816 140590888851264 distribute_coordinator.py:836] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0',)\r\nI0827 15:39:19.310764 140590888851264 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:0',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\nI0827 15:39:19.310969 140590888851264 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0',)\r\nI0827 15:39:19.313967 140590888851264 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:0',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\nI0827 15:39:19.314168 140590888851264 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-08-27 15:39:21.066740: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:521] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\r\nop: \"FlatMapDataset\"\r\ninput: \"PrefetchDataset/_8\"\r\nattr {\r\n  key: \"Targuments\"\r\n  value {\r\n    list {\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"f\"\r\n  value {\r\n    func {\r\n      name: \"__inference_Dataset_flat_map_slice_batch_indices_84\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_types\"\r\n  value {\r\n    list {\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\r\nEpoch 1/100\r\nWARNING:tensorflow:From /data2/xiaohu/python37-tensorflow-2.3.0/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nW0827 15:42:17.483410 140422423435072 deprecation.py:323] From /data2/xiaohu/python37-tensorflow-2.3.0/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:17.520840 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\n(None, 31)\r\nINFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.048505 140422423435072 cross_device_ops.py:1079] Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.305560 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.309205 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.315551 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.318860 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.323117 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.327072 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.330104 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:19.333960 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:20.492240 140422423435072 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\n(None, 31)\r\nINFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:42:20.632939 140422423435072 cross_device_ops.py:1079] Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\n  408/23438 [..............................] - ETA: 58s - loss: 0.6936 - auc: 0.5005\r\n```\r\n\r\n2. worker\r\n\r\n```shell\r\n{'cluster': {'worker': ['host1:12345', 'host2:12345']}, 'task': {'index': 1, 'type': 'worker'}}\r\n2020-08-27 15:45:00.512494: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-27 15:45:00.522673: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2194960000 Hz\r\n2020-08-27 15:45:00.536417: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8c6a65eca0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-27 15:45:00.536485: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-27 15:45:00.591712: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> host1:12345, 1 -> host2:12345}\r\n2020-08-27 15:45:00.605558: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://host2:12345\r\nINFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:XLA_CPU:0']\r\nI0827 15:45:00.608686 140240986761024 collective_all_reduce_strategy.py:329] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:XLA_CPU:0']\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\nI0827 15:45:00.611357 140240986761024 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:1',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nI0827 15:45:00.611701 140240986761024 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nNumber of devices: 2\r\n2020-08-27 15:45:17.748821: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, environment = None, rpc_layer = 'grpc'\r\nI0827 15:45:20.202999 140240986761024 distribute_coordinator.py:783] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, environment = None, rpc_layer = 'grpc'\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0827 15:45:20.203293 140240986761024 distribute_coordinator.py:832] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nW0827 15:45:20.203418 140240986761024 distribute_coordinator.py:836] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\nI0827 15:45:20.205906 140240986761024 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:1',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nI0827 15:45:20.206124 140240986761024 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\r\nI0827 15:45:20.208848 140240986761024 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:1',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nI0827 15:45:20.209068 140240986761024 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['host1:12345', 'host2:12345']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\n2020-08-27 15:45:22.340415: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:521] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\r\nop: \"FlatMapDataset\"\r\ninput: \"PrefetchDataset/_8\"\r\nattr {\r\n  key: \"Targuments\"\r\n  value {\r\n    list {\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"f\"\r\n  value {\r\n    func {\r\n      name: \"__inference_Dataset_flat_map_slice_batch_indices_78\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_types\"\r\n  value {\r\n    list {\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\r\nEpoch 1/100\r\nWARNING:tensorflow:From /data2/xiaohu/test/python37-tensorflow-2.3.0/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nW0827 15:45:22.731915 140240986761024 deprecation.py:323] From /data2/xiaohu/test/python37-tensorflow-2.3.0/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:22.753906 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\n(None, 31)\r\nINFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.100841 140240986761024 cross_device_ops.py:1079] Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.330459 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.332968 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.336559 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.338489 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.340856 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.343185 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.345053 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:23.349698 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:24.438810 140240986761024 cross_device_ops.py:1085] Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\n(None, 31)\r\nINFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\nI0827 15:45:24.590398 140240986761024 cross_device_ops.py:1079] Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, communication_hint = AUTO, num_packs = 1\r\n 1028/23438 [>.............................] - ETA: 56s - loss: 0.6937 - auc: 0.4986\r\n```\r\n\r\n\r\n", "I was able to reproduce the memory increase. However when I changed the numpy arrays to a `tf.data.dataset` the memory was pretty stable. Under the hood, `model.fit` should convert your numpy arrays to a `tf.data.dataset`, but there is possibly some problem there. Can you try the same and convert your data (both training and validation) ahead of time so you are passing in a `tf.data.dataset` to model.fit?", "> Can you try the same and convert your data (both training and validation) ahead of time so you are passing in a tf.data.dataset to model.fit?\r\n\r\nYes, I converted the numpy array to tf.data.Dataset before training using the code below, but the memory usage was still slowly growing. With almost the same number of training steps, the memory reaches about **4G** at the end of training, which is lower than before. Is there something different with your test code?\r\n\r\n```python\r\ndef prepare_data():\r\n    np.random.seed(0)\r\n    x_train, y_train = (\r\n        np.random.random((6000000, 31)),\r\n        np.random.randint(2, size=(6000000, 1)),\r\n    )\r\n\r\n    x_val, y_val = (\r\n        np.random.random((10000, 31)),\r\n        np.random.randint(2, size=(10000, 1)),\r\n    )\r\n\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(256).repeat()\r\n\r\n    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\n    val_dataset = val_dataset.batch(256).repeat()\r\n\r\n    return train_dataset, val_dataset\r\n  \r\n# Training\r\ntrain_dataset, val_dataset = prepare_data()\r\n\r\nmodel.fit(\r\n    train_dataset,\r\n    epochs=100,\r\n    steps_per_epoch=20000,\r\n    validation_data=val_dataset,\r\n    validation_steps=100,\r\n)\r\n```\r\n\r\nAnd in my real training job, I did use `tf.data.Dataset` API for reading data from files, the memory was growing as what I've said. PS: numpy inputs are just used to test and illustrate the distributed problem(May be related to the input pipeline). Here are codes I've used for reading data from HDFS files:\r\n\r\n```python\r\ndef make_dataset(input_pattern, shuffle_size, batch_size):\r\n    # For vectorized map\r\n    def labeler(record):\r\n        fields = tf.io.decode_csv(\r\n            record,\r\n            record_defaults=['0'] * 64,\r\n            field_delim='\\t',\r\n        )\r\n        data = tf.strings.to_number(fields[1:64], out_type=tf.int32)\r\n        label = tf.strings.to_number(fields[:1], out_type=tf.int32)\r\n\r\n        data = tf.transpose(data)\r\n        label = tf.transpose(label)\r\n\r\n        return data, label\r\n\r\n    filenames = tf.data.Dataset.list_files(input_pattern)\r\n    dataset = filenames.interleave(\r\n        lambda filename: tf.data.TextLineDataset(filename),\r\n        cycle_length=tf.data.experimental.AUTOTUNE,\r\n        num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n    )\r\n    dataset = dataset.repeat().shuffle(shuffle_size).batch(batch_size)\r\n    dataset = dataset.map(\r\n        lambda ex: labeler(ex),\r\n        num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n    )\r\n\r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n    return dataset\r\n\r\n# Training\r\ntrain_dataset = make_dataset(\r\n    os.path.join(FLAGS.input, \"train\", \"*\"),\r\n    SHUFFLE_SIZE,\r\n    BATCH_SIZE,\r\n)\r\nvalidation_dataset = make_dataset(\r\n    os.path.join(FLAGS.input, \"validation\", \"*\"),\r\n    SHUFFLE_SIZE,\r\n    BATCH_SIZE,\r\n)\r\n\r\nmodel.fit(train_dataset,\r\n          epochs=1,\r\n          steps_per_epoch=10,\r\n          validation_data=validation_dataset,\r\n          validation_steps=10,\r\n          callbacks=tensorboard_callback)\r\n```\r\n\r\nAt the beginning, I also suspected that it was caused by data input, but there is no memory leak in stand-alone local training with the same data pipeline. And in distributed training, `tf.data.experimental.AutoShardPolicy` will use `FILE` policy, which means every worker will only be responsible for its own files same as the local training, may not the key problem causing the memory leak.\r\n\r\nThe main difference between distributed training and local training is parameters synchronization, when using MultiWorkerMirroredStrategy strategy, it will select communication method automatically, will the `CollectiveOps` be added to the tensorflow graph gradually or will the memory leak be releated to it?\r\n\r\nThose are just my personal opinion, I'm looking forward to your view.\r\n\r\nThanks~", "You're right, changing to a `tf.data.dataset` did not solve the leak. However, when I ran the code with the `tf.data.Dataset`, I was actually running in tf-nightly so I'm wondering if it was the switch to nightly that solved the leak for me. I used the following dataset function, and did not pass in `steps_per_epoch` or `validation_steps` in my `model.fit`\r\n\r\n```\r\ndef prepare_data(BATCH_SIZE):\r\n    np.random.seed(0)\r\n    x_train, y_train = (\r\n        np.random.random((6000000, 31)),\r\n        np.random.randint(2, size=(6000000, 1)),\r\n    )\r\n\r\n\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n    train_dataset = train_dataset.batch(BATCH_SIZE)\r\n    x_val, y_val = (\r\n        np.random.random((10000, 31)),\r\n        np.random.randint(2, size=(10000, 1)),\r\n    )\r\n\r\n    validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\n    validation_dataset = validation_dataset.batch(128)\r\n    return train_dataset, validation_dataset\r\n```\r\n\r\n\r\nI've just kicked off another training job in nightly using the `prepare_data()` function you've provided with the `.repeat()` `.shuffle()` and passing in the `steps_per_epoch` and `validation_steps` args to `model.fit`. I will let you know what the results look like, but so far they seem pretty stable", "Okay, with tf-nightly, I only see a slight increase in memory usage over the 100 epochs. I monitored every minute and memory trended upwards from ~8.3G to ~8.9G over the 100 epochs. So still an increase but significantly lower than what I was seeing in 2.3.", "I've tried `tf-nightly-cpu==2.4.0-dev20200828` with the codes above, exactly as you said. And according to my real training job, I can see a pretty stable memory usage as the figure shown below.\r\n\r\n![image](https://user-images.githubusercontent.com/15494997/92226692-b3ca9980-eed7-11ea-8016-ac2feb7d1852.png)\r\n\r\nAlthough the memory is still growing as the training continues, the growth can be tolerated.\r\n\r\n**I will continue to focus on this problem, if there are any new changes, I will sync it here.**\r\n\r\nThanks very much for your help.", "Awesome, very glad to hear that moving to nightly works for now. We are currently working hard on improving MWMS and to move it out of experimental. I will close this issue since upgrading to nightly is sufficient for now. But please do feel free to reopen this thread should you encounter any follow up issues with the memory growth or file a new issue if you run into a different bug. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42616\">No</a>\n", "my situation is similar to yours but I can not solve the problem by using tf-nightly. \r\n\r\nenvironments (docker, 2.5.0.dev20201110, 10 workers):  \r\n[https://hub.docker.com/r/tensorflow/tensorflow/tags](url)\r\n`docker pull tensorflow/tensorflow:nightly` \r\n\r\nI used your codes to test and add a memory callback:\r\n`class MemoryCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, batch, logs=None):\r\n        print(\" - Memory: {:.04f} GB \".format(psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024))`\r\n\r\nthe memory increase like: \r\n![image](https://user-images.githubusercontent.com/30276826/98908442-6849f280-24fb-11eb-99ed-d8b59bbbf340.png)\r\n\r\n\r\n", "having similar issues on the google Cloud ML engine. with tensorflow-2.3 ", "update: the issue still exists in tensorflow 2.4. verified with the latest google ML engine runtime 2.4. \r\nThings did improve a little bit comparing to 2.3.  With the same data and model, in 2.3 with 40 high-mem-2 machines, it takes 1 hour for some of the workers to exhaust memory. And with 2.4, it takes 2 hours. ", "I also see this on 2.3 and 2.4.  It looks worse on 2.4 for me.\r\n\r\nA lot of the marginal memory growth comes from eval steps for me.\r\n\r\nSee this for a long run with a reasonable eval frequency.  Note that at each eval, the memory peaks a bit and does not return to the original amount of memory.\r\n![image](https://user-images.githubusercontent.com/1923997/111208126-9c0b5680-8587-11eb-8cf2-2ce2bd27d546.png)\r\n\r\n\r\nHere is a training run with eval being done after every single step.  You can see it has grown much, much quicker.\r\n![image](https://user-images.githubusercontent.com/1923997/111208260-c230f680-8587-11eb-8568-247bb54daf69.png)\r\n", "Is the memory problem still there if you remove `validation_data`?", "much slower growth if any at all after a similar ~30 mins\r\n![image](https://user-images.githubusercontent.com/1923997/111229803-cf5bde80-85a3-11eb-9921-99bc0562b1e5.png)\r\n", "seeing a less drastic growth from `MirroredStrategy` but still the bumps from eval\r\n\r\nso likely two separate problems\r\n\r\n![image](https://user-images.githubusercontent.com/1923997/111248990-37bcb700-85c8-11eb-8302-74b85e684833.png)\r\n", "Any updates on this issue?", "Hello RuhuaJiang@, we don't have an update on this yet. Did you happen to test with other strategies such as MirroredStrategy? Also, would ParameterServerStrategy be something that you can consider?", "Im also having the same problem with a custom training loop and a keras model backend. The bumps occur for me in between training steps (no eval so far), possibly when the dataset is exhausted as i do not train for epochs. The magnitude of the bumps correlates with the batch_size. I would suggest its a bug within tf.data.\r\n\r\nI load my dataset like this\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices(dict(df))\r\ndataset = dataset.repeat().shuffle(buffer_size=shuf_buf_size, seed=seed)\r\ndataset = dataset.map(customized_parse_func,\r\n                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\r\n                .batch(batch_size)\\\r\n                .prefetch(tf.data.experimental.AUTOTUNE)\r\n```\r\nand im using\r\n\r\n```\r\nself.strategy = tf.distribute.MirroredStrategy()\r\ntrain_dist_dataset = self.strategy.experimental_distribute_dataset(train_dataset)\r\n```\r\nNo difference between experimental.AUTOTUNE and tf.data.AUTOTUNE. Manually setting the params delays or accelerates the OOM.\r\n\r\nI tested 2.2, 2.3, 2.4 and i also would say 2.4 looks worse for me.\r\n\r\nI tried to solve it for a week now, going through lots of posts and trying different things like \r\n`tf.keras.backend.clear_session()` and `gc.collect()` calls, running it with tcmalloc, reducing the shuffle buffer, etc.. I also made sure i did not create things twice.", "Hi @KuenstlicheIntelligenz , would you mind testing it without distribution strategy, just to make sure the problem comes from tf.data? cc @w-xinyi @jsimsa ", "This might be another instance of https://github.com/tensorflow/tensorflow/issues/44176#issuecomment-783768033.", "@yuefengz \r\nTried it without strategy and could not reproduce the memory leak with the bumps.", "Update:\r\n- Still occurs in TF 2.5\r\n- Occurs with any strategy i tested (also default (tf.distribute.get_strategy()) and OneDevice)\r\n- Occurs with and without strategy.experimental_distribute_dataset(train_dataset)\r\n- Does not occur when dropping any strategy related code and training on a single device only.", "cc @crccw "]}, {"number": 42614, "title": "Request for Prebuilt C++ Library libtensorflow_cc.so (probably from CI artifacts)", "body": "**System information**\r\n- TensorFlow v2.3.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIt has been lot of demand and lot of time consumption in building the TensorFlow C++ Library (monolithic), which could be saved if TensorFlow provides official `libtensorflow_cc` (C++ lib) as well along with `libtensorflow.so` (C lib) as there are lots of functionality which one would want from C++ lib and due to the fact that it is not available anywhere, one has to built it himself for this, which I believe Tensorflow official CI is already building (cache) so if we have CI package available for download and use for all various OS, this could reduce the deployment efforts for anyone to great extend. \r\n\r\nI have built the lib manually for various systems by following steps provided below as I wanted to have single lib + headers to deploy with other projects.\r\n\r\n```bash\r\nbazel build -c opt --config=monolithic //tensorflow:libtensorflow_cc.so //tensorflow:install_headers\r\n```\r\n\r\nIn addition to that, if TensorFlow can provide complete package which includes `libtensorflow_cc.so`, `libtensorflow_lite.so`, `libtensorflow.so`, headers and `tools` along with Bazel BUILDs (i.e. package) then one could speed up his own CI compilation time and lot more as all he has to do is to fetch package and link the binaries against it. \r\n\r\nPackage for each OS (i.e. `linux, windows, macOS`)\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAlmost everyone who wants to use C++ APIs for deployment\r\n\r\n**Any Other info.**\r\nnone", "comments": ["C++ libraries have no stable ABIs. That means any client code that uses `libtensorflow_cc.so` will need to build with the exact same toolchain (a particular version of C++ compiler and libstdc++/libc++) as those were used to build the pre-built C++ libraries.\r\n\r\nSo the pre-built libraries may not be so useful as you expect if the TF team only provides them built with a particular toolchain. ", "> C++ libraries have no stable ABIs. That means any client code that uses libtensorflow_cc.so will need to build with the exact same toolchain (a particular version of C++ compiler and libstdc++/libc++) as those were used to build the pre-built C++ libraries.\r\n> So the pre-built libraries may not be so useful as you expect if the TF team only provides them built with a particular toolchain.\r\n\r\nI don't know exactly these ABI issues but what I know is that for most dev environments will have x86_64 platform with either GCC or CLANG as toolchain (versions would be based on the latest OS support package - i.e. in the case of Ubuntu 20.04 this will be 8.3 and 10.0 respectively) so I believe this is still very useful and important for Developers. I have already tried generating `monolithic` packages for `libtensorflow_cc.so` with headers (with GCC and CLANG) manually and works fine so far with my DEV environment efficient and portable way. If Team can integrate similar to CI that would be a great help to the community in my opinion.\r\n\r\nAnother reason why I do want a pre-compiled TF Library is that building TF on the fly (with Bazel/CMake as dependencies) is going to really time-consuming even for a very simple `hello-tf.cpp` project. This is not OK for small project CI Pipeline as well and requires easy and quick build which can only be achieved if we have a precompiled TF binary. - *libtorch already provides these*\r\n\r\n**Alternative**: Provide OS Package (i.e. for ubuntu/Debian - *.deb, for RedHat - *.rpm, etc.) into Official/Unofficial OS Package repository like OpenCV does this, so that every single dev environment can avail these pre-compiled packages out-of-the-box and easily develop projects without worrying about setting up TF environment for C++ which is a big head-ache now.", "> > C++ libraries have no stable ABIs. That means any client code that uses libtensorflow_cc.so will need to build with the exact same toolchain (a particular version of C++ compiler and libstdc++/libc++) as those were used to build the pre-built C++ libraries.\r\n> > So the pre-built libraries may not be so useful as you expect if the TF team only provides them built with a particular toolchain.\r\n> \r\n> I don't know exactly these ABI issues but what I know is that for most dev environments will have x86_64 platform with either GCC or CLANG as toolchain (versions would be based on the latest OS support package - i.e. in the case of Ubuntu 20.04 this will be 8.3 and 10.0 respectively) so I believe this is still very useful and important for Developers. I have already tried generating `monolithic` packages for `libtensorflow_cc.so` with headers (with GCC and CLANG) manually and works fine so far with my DEV environment efficient and portable way. If Team can integrate similar to CI that would be a great help to the community in my opinion.\r\n> \r\n\r\nThe plan is to have `libtensorlow_cc` releases the same way we currently have `libtensorflow` releases in Q4 this year. Sorry for the delay. ", "@jinay1991 Could you please let us know if the issue still persists ? Thanks!", "> @jinay1991 Could you please let us know if the issue still persists ? Thanks!\r\n\r\n@sushreebarsa No, I don't think that this is resolved (i.e. Issue still persists). I see there are [nightly build from README.md](https://github.com/tensorflow/tensorflow/blob/master/README.md) available for `libtensorflow.so` but this is a `C` library. This issue is specifically requesting for `libtensorflow_cc.so` `C++` library to ease the use of TensorFlow for various projects. ", "Hi @jinay1991 ! Have you checked these threads yet?,[link1](https://github.com/karthickai/tflite), [link2](https://reposhub.com/python/deep-learning/FloopCZ-tensorflow_cc.html),[link3](https://iq.opengenus.org/build-tensorflow-cpp-library/).  Thanks!", "> Hi @jinay1991 ! Have you checked these threads yet?,[link1](https://github.com/karthickai/tflite), [link2](https://reposhub.com/python/deep-learning/FloopCZ-tensorflow_cc.html),[link3](https://iq.opengenus.org/build-tensorflow-cpp-library/). Thanks!\r\n\r\nYes, I have seen many such places where many developers have automated this, [I did as well](https://github.com/jinay1991/spleeter/releases/tag/v2.3) but this does not really solve the problem of migrating to newer version of TensorFlow with ease, and not to forget it takes a lot of time to build the library in monolithic configuration. Hence I am requesting exactly same from the TensorFlow Official Packaging CI/CD, as this possibly builds`libtensorflow_cc` for their official release (`PyPI`), but just doesn't include in release as monolithic library.\r\n\r\nAnother downside of all these developer work is that no-one is really providing/maintaining the most recent version of the `libtensorflow_cc.so` always and that basically blocks many developers like me who has to do the **hard-work** again. Wouldn't it be nice that you can download and build your TensorFlow application with C++ just the way you can do for Python (i.e. download package and go)?\r\n\r\nNote that, all of the links you have provided only tells how to build the Library, but no-one really provides the binary for various OS and Platforms. :)"]}, {"number": 42606, "title": "[Bug] apply_gradients core dump when variable shape is [0] on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: GeForce RTX 2080\r\n\r\n**Describe the current behavior**\r\nVariable with empty shape **core dumps** on GPU but works fine on CPU.\r\n\r\n**Describe the expected behavior**\r\nWork on both CPU / GPU or fail gracefully.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\na = tf.Variable(tf.random.normal([3]))\r\nb = tf.Variable(tf.random.normal([0]))\r\nAdam = tf.optimizers.Adam\r\noptimizer = Adam()\r\nwith tf.GradientTape() as t:\r\n    c = tf.concat([a, b], axis=0)\r\ngrad = t.gradient(c, b)\r\noptimizer.apply_gradients(zip([grad], [b]))\r\n```\r\ncore dumped with: `F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)`\r\n", "comments": ["@mavenlin  Yes This bug exists in Tensorflow 2.3.0 and the session is crashing even on [colab](https://colab.research.google.com/gist/gowthamkpr/b6271cf33375aa4f025215baf62d631d/untitled310.ipynb) but has been fixed in tf-nightly. Please find the gist [here](https://colab.research.google.com/gist/gowthamkpr/35ddba3e3edd6ad54cc651e5d0a8cea0/untitled.ipynb)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42606\">No</a>\n", "The gist provided on colab works because there's no GPU device loaded.\r\nSeems like the `tf-nightly-gpu` does not properly load GPU on colab.\r\nI tested on my local machine with `tf-nightly` and it still causes core dump.\r\n\r\ntested with \r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nprint(tf.config.list_physical_devices())\r\na = tf.Variable(tf.random.normal([3]))\r\nb = tf.Variable(tf.random.normal([0]))\r\nAdam = tf.optimizers.Adam\r\noptimizer = Adam()\r\nwith tf.GradientTape() as t:\r\n    c = tf.concat([a, b], axis=0)\r\ngrad = t.gradient(c, b)\r\noptimizer.apply_gradients(zip([grad], [b]))\r\n```\r\n\r\nGet \r\n```\r\n2.4.0-dev20200908\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>\r\n```", "I have a similar issue in 2.4.1. I have custom layers bundled as a dict. Works fine on CPU but crashes on GPU.\r\n\r\n./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)\r\n\r\nAny workaround?", "It looks there is a missing early exit here: https://github.com/tensorflow/tensorflow/blob/9f594b848ac12c98518a544198fc1c0571f68ce1/tensorflow/core/kernels/training_ops_gpu.cu.cc#L812\r\n\r\n@sgpyc can you PTAL?", "If the variable shape is [0], the optimizer should be an no op and early exit should work.\r\nHaving an empty gradient seems strange to me, not sure whether it's an issue that should be caught earlier.", "I don't remember what was the exact scenario that I encounter this bug since it's been a long time.\r\nSeems to be some dynamically shaped variable whose shape is determined dynamically, in my case it is the newly allocated weights, which could be zero if no capacity increase is necessary.", "Was able to replicate the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c560a9c361ffd130be44b438db1675ec/untitled264.ipynb) ..Thanks !"]}, {"number": 42595, "title": "Support for batch to single element conversion", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current implementation of batch data requires a batch output, however, I made a model that converts batch data into a single element. So the first dimensions need not be the same. \r\n\r\n**Will this change the current api? How?**\r\nThe current implementation gives me an error \"the first dimensions do not match\". That might change\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who is making STGCNNs\r\n\r\n**Any Other info.**\r\nThough convoluted, here's the model summary for reference:\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param      Connected to                     \r\n\r\ninput_1 (InputLayer)            [(150, 640, 3)]      0                                            \r\n__________________________________________________________________________________________________\r\ntf_op_layer_ExpandDims (TensorF [(1, 150, 640, 3)]   0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Transpose (TensorFl [(1, 640, 150, 3)]   0           tf_op_layer_ExpandDims[0][0]     \r\n__________________________________________________________________________________________________\r\nglu (GLU)                       (1, 640, 148, 2)     40          tf_op_layer_Transpose[0][0]      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Transpose_1 (Tensor [(1, 148, 640, 2)]   0           glu[0][0]                        \r\n__________________________________________________________________________________________________\r\ninput_3 (InputLayer)            [(150, 640, 640)]    0                                            \r\n__________________________________________________________________________________________________\r\ninput_2 (InputLayer)            [(150, 640, 640, 1)] 0                                            \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Squeeze (TensorFlow [(148, 640, 2)]      0           tf_op_layer_Transpose_1[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice_1 (Te [(148, 640, 640)]    0           input_3[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice (Tens [(148, 640, 640, 1)] 0           input_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ngraph_conv (GraphConv)          (148, 640, 3)        9           tf_op_layer_Squeeze[0][0]        \r\n                                                                 tf_op_layer_strided_slice_1[0][0]\r\n                                                                 tf_op_layer_strided_slice[0][0]  \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Transpose_2 (Tensor [(640, 148, 3)]      0           graph_conv[0][0]                 \r\n__________________________________________________________________________________________________\r\ntf_op_layer_ExpandDims_1 (Tenso [(1, 640, 148, 3)]   0           tf_op_layer_Transpose_2[0][0]    \r\n__________________________________________________________________________________________________\r\nglu_1 (GLU)                     (1, 640, 146, 2)     40          tf_op_layer_ExpandDims_1[0][0]   \r\n__________________________________________________________________________________________________\r\nglu_2 (GLU)                     (1, 640, 144, 2)     28          glu_1[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Transpose_3 (Tensor [(1, 144, 640, 2)]   0           glu_2[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Squeeze_1 (TensorFl [(144, 640, 2)]      0           tf_op_layer_Transpose_3[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice_3 (Te [(144, 640, 640)]    0           input_3[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_strided_slice_2 (Te [(144, 640, 640, 1)] 0           input_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ngraph_conv_1 (GraphConv)        (144, 640, 3)        9           tf_op_layer_Squeeze_1[0][0]      \r\n                                                                 tf_op_layer_strided_slice_3[0][0]\r\n                                                                 tf_op_layer_strided_slice_2[0][0]\r\n__________________________________________________________________________________________________\r\ntf_op_layer_Transpose_4 (Tensor [(640, 144, 3)]      0           graph_conv_1[0][0]               \r\n__________________________________________________________________________________________________\r\ntf_op_layer_ExpandDims_2 (Tenso [(1, 640, 144, 3)]   0           tf_op_layer_Transpose_4[0][0]    \r\n__________________________________________________________________________________________________\r\nglu_3 (GLU)                     (1, 640, 142, 2)     40          tf_op_layer_ExpandDims_2[0][0]   \r\n__________________________________________________________________________________________________\r\nglu_4 (GLU)                     (1, 640, 1, 3)       1710        glu_3[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Transpose_5 (Tensor [(1, 1, 640, 3)]     0           glu_4[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Squeeze_2 (TensorFl [(640, 3)]           0           tf_op_layer_Transpose_5[0][0]    \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (640, 3)             12          tf_op_layer_Squeeze_2[0][0]      \r\n\r\nTotal params: 1,888\r\nTrainable params: 1,888\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________", "comments": ["@spandananupam Can you please share little more details on the feature you are requesting and what are the possible use-cases that the new feature will support? Thanks! ", "> @spandananupam Can you please share little more details on the feature you are requesting and what are the possible use-cases that the new feature will support? Thanks! \n\nYes sure. @jvishnuvardhan \n\nThe model which I am making compresses a batch of graphs to a single graph. So the first dimensions do not match. This throws an error, because tensorflow expects a model that converts batch to batch.\n\nThe use case: for stuff like spatial temporal graph neural networks, where M graphs are compressed to predict the t+1th graph"]}]