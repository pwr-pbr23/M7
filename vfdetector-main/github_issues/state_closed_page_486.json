[{"number": 39207, "title": "Distributed TensorFlow scaling efficiency with CUDA_VISIBLE_DEVICES", "body": "Hello! I'm seeing a strange dip in performance when I run distributed training on a subset of available GPUs using CUDA_VISIBLE_DEVICES and TensorFlow 1.x. I included a case using TensorFlow Docker images and the tf_cnn_benchmarks. Is this expected behavior?\r\n\r\nThank you for your help!\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No (TF CNN benchmarks + official TF docker images)**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.3 within container (CentOS 7.3 outside, Singularity 3.5)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): **official TF Docker container: tensorflow/tensorflow:1.15.2-gpu-py3.sif**\r\n- TensorFlow version (use command below): **1.15.2**\r\n- Python version: **3.6.9** \r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: **10**\r\n- GPU model and memory: **GTX 1080Ti - 11GB x8**\r\n\r\n**Describe the current behavior**\r\n\r\nOn a node with 8 GPUs, distributed scaling efficiency on 4 GPUs is significantly (~15%) worse when setting `CUDA_VISIBLE_DEVICE=<subset of nodes>` compared to keeping CUDA_VISIBLE_DEVICES unset or setting it to all physical GPUs, as seen in the graph below (using the scripts in the next section):\r\n\r\n![CUDA scaling with different CUDA_VISIBLE_DEVICES](https://user-images.githubusercontent.com/814638/81127747-341ec500-8f0d-11ea-814c-f44a60c3fa6f.png)\r\n\r\nIn these runs, I set `CUDA_VISIBLE_DEVICES=0,\u2026,(n-1)` in the dotted case (the TensorFlow default order) and `CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7` in the solid case. According to `nvidia-smi`, the same GPUs are working in both cases.\r\n\r\nThis pattern is also present for different NVIDIA hardware (seen in 11GB GTX 1080Ti, 11GB RTX 2080Ti, and 32GB V100 nodes), different GPU interconnects (seen in PCIe, NVLink, and DMA-only), different all-reduce strategies (seen in NCCL and xring), and different versions of TensorFlow (seen in 1.15.2, 1.14.0, and 1.13.2). I also get these worse performance results for any subset of nodes (e.g. `CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7` for `--num_gpus=2`), not just the neat case above.\r\n\r\n**Describe the expected behavior**\r\n\r\nI'd expect that distributed training on 4 nodes with `CUDA_VISIBLE_DEVICES=0,1,2,3` would have similar performance to the same run with `CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSetting up the environment:\r\n```bash\r\n$ singularity pull docker://tensorflow/tensorflow:1.15.2-gpu-py3\r\n$ git clone tensorflow/benchmarks\r\n$ cd benchmarks && git checkout cnn_tf_v1.15_compatible\r\n```\r\n\r\n1 GPU:\r\n```bash\r\n$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \\\r\n    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n      --num_gpus 1 --tf_random_seed 4321 --device gpu \\\r\n      --variable_update replicated --all_reduce_spec nccl \\\r\n      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \\\r\n      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64\r\n\r\n#=> average total images/sec: ~220\r\n\r\n$ CUDA_VISIBLE_DEVICES=0 \\\r\n    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \\\r\n    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n      --num_gpus 1 --tf_random_seed 4321 --device gpu \\\r\n      --variable_update replicated --all_reduce_spec nccl \\\r\n      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \\\r\n      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64\r\n\r\n#=> average total images/sec: ~220\r\n```\r\n\r\n4 GPUs:\r\n```bash\r\n$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \\\r\n    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n      --num_gpus 4 --tf_random_seed 4321 --device gpu \\\r\n      --variable_update replicated --all_reduce_spec nccl \\\r\n      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \\\r\n      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64\r\n\r\n#=> average total images/sec: ~840 (~95% scaling efficiency)\r\n\r\n$ CUDA_VISIBLE_DEVICES=0,1,2,3 \\   # or any other subset of 4-7 GPUs\r\n    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \\\r\n    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n      --num_gpus 4 --tf_random_seed 4321 --device gpu \\\r\n      --variable_update replicated --all_reduce_spec nccl \\\r\n      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \\\r\n      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64\r\n\r\n#=> average total images/sec: ~700 (~80% scaling efficiency)\r\n```\r\n\r\n8 GPUs:\r\n```bash\r\n$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \\\r\n    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n      --num_gpus 8 --tf_random_seed 4321 --device gpu \\\r\n      --variable_update replicated --all_reduce_spec nccl \\\r\n      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \\\r\n      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64\r\n\r\n#=> average total images/sec: ~1580 (~90% scaling efficiency)\r\n```\r\n\r\n**Other info / logs**", "comments": ["1.15.2 is a past release that only receives security fixes. That means even if we could spot the bug (assume there is one) and fix it, it won't be back ported to 1.15, not to mention 1.13/1.14.\r\n\r\nI will suggest you to use latest tf-nightly and see if that is still an issue.", "Thank you for your quick response!\r\n\r\nI re-ran the tests using tf-nightly, and I found a similar pattern:\r\n\r\n![tf-nightly_cuda_scaling](https://user-images.githubusercontent.com/814638/81201422-40e5fc00-8f93-11ea-846a-b5a33869fc78.png)\r\n\r\nThe run scripts are the same except for the container/branch:\r\n\r\n```bash\r\n$ singularity pull docker://tensorflow/tensorflow:nightly-gpu-py3\r\n$ git clone tensorflow/benchmarks\r\n$ cd benchmarks && git checkout cnn_tf_v2.1_compatible  # there isn't a 2.2 branch, but I don't think it should matter for this\r\n```\r\n\r\nRunning with the tf 2.1.0 container produces the same pattern as well.", "To get rid of the extra abstraction layer, I ran this test again without Singularity. It looks like this issue is still present.\r\n\r\nCreating a clean environment:\r\n```bash\r\n(base)  $ conda create -n tf2.1 python=3.6 tensorflow-gpu==2.1.0 && conda activate tf2.1\r\n(tf2.1) $ git clone tensorflow/benchmarks\r\n(tf2.1) $ cd benchmarks && git checkout cnn_tf_v2.1_compatible\r\n```\r\n\r\nRunning the benchmark (using xring this time, which makes the discrepancy more dramatic):\r\n```bash\r\n(tf2.1) $ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n    python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --num_gpus 4 --tf_random_seed 4321 --device gpu --variable_update replicated --all_reduce_spec xring --local_parameter_device cpu --nodistortions --gradient_repacking 1 --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64\r\n#=> average total images/sec: ~800 (~93% scaling efficiency)\r\n\r\n(tf2.1) $ CUDA_VISIBLE_DEVICES=0,1,2,3 \\   # or any other subset of 4-7 GPUs\r\n    python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --num_gpus 4 --tf_random_seed 4321 --device gpu --variable_update replicated --all_reduce_spec xring --local_parameter_device cpu --nodistortions --gradient_repacking 1 --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64\r\n#=> average total images/sec: ~600 (~70% scaling efficiency)\r\n```\r\n\r\n![tf_scaling_no_container](https://user-images.githubusercontent.com/814638/81334445-0d2ed300-9074-11ea-95ff-8e22db942752.png)\r\n\r\nThis was run on Centos 7 using CUDA 10.1 and TensorFlow 2.1.0 on a node with 8x 11GB GTX 1080Ti's.", "@leesharma could you please provide more information about your GPU nodes and how they interconnect together (using nvlink etc.)\r\nnvidia-smi topo -m\r\n\r\nWe have seen some 4GPU cases has lower performance due to the fact that there are no high-speed nvlink among all these GPUs and gradient allreduce needs to go through slower PCI-e.", "I tested three different IC configurations. The results above are for running with the same configuration on the same hardware\u2014`CUDA_VISIBLE_DEVICES` is the only thing that's changing.\r\n\r\nThe graphs above are from nccl/xring all-reduce with PCIe:\r\n\r\n```bash\r\n# 1080Ti system\r\n$ nvidia-smi topo -m\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    mlx5_0  CPU Affinity\r\nGPU0     X      PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    0-15,32-47\r\nGPU1    PIX      X      PIX     PIX     NODE    NODE    NODE    NODE    NODE    0-15,32-47\r\nGPU2    PIX     PIX      X      PIX     NODE    NODE    NODE    NODE    NODE    0-15,32-47\r\nGPU3    PIX     PIX     PIX      X      NODE    NODE    NODE    NODE    NODE    0-15,32-47\r\nGPU4    NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     0-15,32-47\r\nGPU5    NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     0-15,32-47\r\nGPU6    NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     0-15,32-47\r\nGPU7    NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     0-15,32-47\r\nmlx5_0  NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\nI also saw the same trend on a V100 node with NVLink:\r\n```bash\r\n# V100 system\r\n$ nvidia-smi topo -m\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    mlx5_0  mlx5_1  CPU Affinity\r\nGPU0     X      NV1     NV1     NV2     NV2     SYS     SYS     SYS     NODE    SYS     0-15,32-47\r\nGPU1    NV1      X      NV2     NV1     SYS     NV2     SYS     SYS     NODE    SYS     0-15,32-47\r\nGPU2    NV1     NV2      X      NV2     SYS     SYS     NV1     SYS     PIX     SYS     0-15,32-47\r\nGPU3    NV2     NV1     NV2      X      SYS     SYS     SYS     NV1     PIX     SYS     0-15,32-47\r\nGPU4    NV2     SYS     SYS     SYS      X      NV1     NV1     NV2     SYS     PIX     16-31,48-63\r\nGPU5    SYS     NV2     SYS     SYS     NV1      X      NV2     NV1     SYS     PIX     16-31,48-63\r\nGPU6    SYS     SYS     NV1     SYS     NV1     NV2      X      NV2     SYS     NODE    16-31,48-63\r\nGPU7    SYS     SYS     SYS     NV1     NV2     NV1     NV2      X      SYS     NODE    16-31,48-63\r\nmlx5_0  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS      X      SYS\r\nmlx5_1  SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\nTo rule out IC issues, I also tried training with nccl forcing direct memory access for the all-reduce (`NCCL_P2P_DISABLE=1`) on both systems and saw the same pattern in both cases.", "Looks like for the two systems you provided, for example, for 4GPUs (0,1,2,3), NCCL should be able to find optimal cross-GPU connection to achieve maximum allreduce throughput. The issue may not be related to allreduce.\r\n\r\nIt would be great if you could get some performance profile (for example, using https://www.tensorflow.org/guide/profiler#collect_performance_data).\r\n\r\n@reedwm any thoughts from the tf_cnn_benchmarks.py command line options that may be related to the issues?\r\n\r\n\r\n\r\n", "I general, I recommend using the [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official) instead of tf_cnn_benchmarks, as tf_cnn_benchmarks is unmaintained. In this particular case, I can reproduce using both tf_cnn_benchmarks and the Official Models on the latest nightly, so either works in debugging the issue.\r\n\r\nThe tf_cnn_benchmarks command line flags are fine. `--local_parameter_device cpu` has no impact since nccl is used, but in general it's better to have the local parameter device be the GPU(s).\r\n\r\nWith tf_cnn_benchmarks, I don't notice any difference in the profile traces (obtained with `--trace_file`). There is a lot of variance when tracing, so if the performance difference is in the trace, I cannot tell. Sometimes the 4-visible devices trace is faster, and sometimes the 8-visible trace is faster. Despite this, the actual 8-visible run is consistently about 15% faster, as reported in the original post. I also checked the post-optimized graphs with `--partitioned_graph_file_prefix`, and they are identical regardless of CUDA_VISIBLE_DEVICES. I am stumped.\r\n\r\n@nluehr, any ideas why training would be slower if you restricted the visible GPUs with CUDA_VISIBLE_DEVICES?", "As a minor update, I tried running this on some AMD GPUs, and it looks like the same effect is not present\u2014the performance is nearly identical regardless of HIP_VISIBLE_DEVICES.", "@leesharma, thanks for reporting this issue. It appears that the GPU kernel launch latency increases when CUDA_VISIBLE_DEVICES is set to more than 1 but fewer than all GPUs. I have filed a bug against the NVIDIA CUDA driver.", "@nluehr Thanks for the update! Does this affect containers as well?", "If the CUDA_VISIBLE_DEVICES environment variable is used within the container, it will trigger the regression.\r\nIf, instead, a subset of GPUs are made available within the container (e.g., with `docker run --gpus 4`), then I don't see a regression.", "@leesharma The issue should be resolved in the 450.51 beta driver which is now available [here](https://www.nvidia.com/Download/Find.aspx?lang=en-us).", "I have confirmed this issue does not occur on 450.51, so I'm closing this issue."]}, {"number": 39206, "title": "Calling custom op changes data type", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.15.0-rc1-16779-g605ebe703f 2.1.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 10.2.89 / 7.6.5\r\n- GPU model and memory: GTX 1070 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have a custom op defined as follows\r\n```\r\nREGISTER_OP(\"Mean2D\")\r\n    .Attr(\"T: {float, double}\")\r\n    .Attr(get_data_format())\r\n    .Attr(\"alpha: {float, double} \")\r\n    .Input(\"img: T\")\r\n    .Input(\"kernel: T\")\r\n    .Output(\"out: T\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n        // ... snip ...\r\n    });\r\n```\r\n\r\nWhen I call this op like this\r\n```\r\n# Code  \r\ndisp[1] = ops.mean2d(img=disp[1], kernel=self.gaussian(self.params[\"blur_sigma\"]), alpha=float(self.params[\"blur_threshold\"]))\r\n```\r\n\r\nI get this error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\", line 685, in build\r\n    self.call(x, **kwargs)\r\n  File \"/media/RAID/Projects/PhD/HVSnet/model/structure.py\", line 372, in call\r\n    disp[1] = ops.mean2d(img=disp[1], kernel=self.gaussian(self.params[\"blur_sigma\"]), alpha=alpha)\r\n  File \"<string>\", line 68, in mean2d\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 692, in _apply_op_helper\r\n    attr_value.type = _MakeType(value, attr_def)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 180, in _MakeType\r\n    _SatisfiesTypeConstraint(i, attr_def, param_name=attr_def.name)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 57, in _SatisfiesTypeConstraint\r\n    raise TypeError(\r\nTypeError: Value passed to parameter 'alpha' has DataType int8 not in list of allowed values: float32, float64\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect that parameter `alpha` would maintain its `float` data type \r\n\r\n", "comments": ["This is definitely strange behavior. Lets try and find a minimal reproduction so that we can figure out what the problem is. One idea would be to try running this op by itself without it being part of a Keras model - feed in some random values of img and alpha that mimics the failure case.", "The following two files reproduce the error. Compile `tf_op.cpp` to `tf_op.so` and put it next to `tf_op.py`\r\n\r\ntf_op.py\r\n```\r\n#! /usr/bin/env python3\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf_op_module = tf.load_op_library('./tf_op.so')\r\n\r\nx = tf.random.normal(shape=(1, 1, 20, 20))\r\nx = tf_op_module.mean2d(img=x, alpha=6.0)\r\n```\r\n\r\ntf_op.cpp\r\n```\r\n#include <tensorflow/core/framework/op.h>\r\n#include <tensorflow/core/framework/op_kernel.h>\r\n#include <tensorflow/core/framework/shape_inference.h>\r\n#include <tensorflow/core/framework/tensor_types.h>\r\n#include <tensorflow/core/platform/cpu_info.h>\r\n#include <tensorflow/core/platform/env.h>\r\n#include <tensorflow/core/platform/logging.h>\r\n\r\nREGISTER_OP(\"Mean2D\")\r\n    .Attr(\"T: {float, double}\")\r\n    .Attr(\"alpha: {float, double} \")\r\n    .Input(\"img: T\")\r\n    .Output(\"out: T\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n        // Output has the same shape as d0\r\n        c->set_output(0, c->input(0));\r\n\r\n        return tensorflow::Status::OK();\r\n    });\r\n\r\ntemplate <typename Device, typename T>\r\nclass Mean2DOp : public tensorflow::OpKernel {\r\npublic:\r\n    explicit Mean2DOp(tensorflow::OpKernelConstruction* context) : OpKernel(context) {\r\n        // No version of GetAttr is defined for type double\r\n        float f_alpha;\r\n        OP_REQUIRES(context,\r\n                    context->GetAttr(\"alpha\", &f_alpha).ok() && f_alpha > 0.0f,\r\n                    tensorflow::errors::InvalidArgument(\"alpha must be positive\"));\r\n        alpha = f_alpha;\r\n    }\r\n\r\n    void Compute(tensorflow::OpKernelContext* context) override {\r\n\r\n        // Check that the shape of each of the inputs is valid\r\n        OP_REQUIRES(context,\r\n                    context->input(0).shape().dims() == 4,\r\n                    tensorflow::errors::InvalidArgument(\"img must be a 4D tensor\"));\r\n\r\n        // Extract information from our input tensors\r\n        tensorflow::Tensor img_tensor = context->input(0);\r\n\r\n        // Create an output tensor\r\n        tensorflow::Tensor* output_tensor = nullptr;\r\n        OP_REQUIRES_OK(context, context->allocate_output(0, img_tensor.shape(), &output_tensor));\r\n\r\n        auto img_tensor_flat = img_tensor.flat<T>();\r\n        auto output_flat = output_tensor->flat<T>();\r\n\r\n        const int N = img_tensor_flat.size();\r\n        for (int i = 0; i < N; i++) {\r\n          output_flat(i) = img_tensor_flat(i);\r\n        }\r\n    }\r\n\r\nprivate:\r\n    T alpha;\r\n};\r\n\r\n// Register a version for all the combinations of float/double and int32/int64\r\nREGISTER_KERNEL_BUILDER(\r\n    Name(\"Mean2D\").Device(tensorflow::DEVICE_CPU).TypeConstraint<float>(\"T\"),\r\n    Mean2DOp<Eigen::ThreadPoolDevice, float>)\r\nREGISTER_KERNEL_BUILDER(\r\n    Name(\"Mean2D\").Device(tensorflow::DEVICE_CPU).TypeConstraint<double>(\"T\"),\r\n    Mean2DOp<Eigen::ThreadPoolDevice, double>)\r\n```\r\n\r\nI get the following traceback\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 39, in mean2d\r\ntensorflow.python.eager.core._FallbackException: Expecting a DType.dtype for attr alpha, got float\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./tf_op.py\", line 8, in <module>\r\n    x = tf_op_module.mean2d(img=x, alpha=6.0)\r\n  File \"<string>\", line 45, in mean2d\r\n  File \"<string>\", line 86, in mean2d_eager_fallback\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'alpha' of int8 is not in the list of allowed values: float, double\r\n\t; NodeDef: {{node Mean2D}}; Op<name=Mean2D; signature=img:T -> out:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=alpha:type,allowed=[DT_FLOAT, DT_DOUBLE]> [Op:Mean2D]\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39206\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39206\">No</a>\n", "Also, is there a reason why `GetAttr` isn't overloaded for `double`?", "Was curious how you resolved the problem? Usually when we have a list of potential values for Attrs its for the types that we'd like to have kernels for e.g. what you do with the \"T\" attr there. For the \"alpha\" attribute, it should either be a float or a double. If you want to support both, you can do \r\n\r\nREGISTER_OP(\"Mean2D\")\r\n    .Attr(\"T: {float, double}\")\r\n    .Attr(\"alpha_type: {float, double}\")\r\n    .Attr(\"alpha: alpha_type\")\r\n    .Input(\"img: T\")\r\n    .Output(\"out: T\")\r\n\r\nand then you can add that as a type constraint in your kernel registration. You'll then have to templatize your kernel etc. \r\n\r\nThere is a still a problem that if you had done\r\n\r\nREGISTER_OP(\"Mean2D\")\r\n    .Attr(\"T: {float, double}\")\r\n    .Attr(\"alpha: double\")\r\n    .Input(\"img: T\")\r\n    .Output(\"out: T\")\r\n\r\nyou would run into issues with the kernel because GetAttr isn't overloaded for double as you pointed out. Would you be willing do a PR for this? It simply involves adding a double to GetNodeAttr here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/node_def_util.h#L192", "I tried this (I also tried just using `T` rather than introducing an `alpha_type`)\r\n> REGISTER_OP(\"Mean2D\")\r\n> .Attr(\"T: {float, double}\")\r\n> .Attr(\"alpha_type: {float, double}\")\r\n> .Attr(\"alpha: alpha_type\")\r\n> .Input(\"img: T\")\r\n> .Output(\"out: T\")\r\n\r\nand I get this error when running the python script. I just assumed that you couldn't have polymorphic attributes (the documentation only says that your inputs and outputs can be polymorphic)\r\n```\r\nTraceback (most recent call last):\r\n  File \"./tf_op.py\", line 5, in <module>\r\n    tf_op_module = tf.load_op_library('./tf_op.so')\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Trouble parsing type string at 'alpha_type ' from Attr(\"alpha: alpha_type \") for Op Mean2D\r\n```\r\n\r\nEDIT: I just noticed I had a space at the end of `alpha_type` in `.Attr(\"alpha: alpha_type \")`, fixed this but still the same error.\r\n\r\nEDIT: The workaround I ended up using was `.Attr(\"alpha: numbertype\")`. It works, but I find it less than ideal.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39206\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39206\">No</a>\n"]}, {"number": 39205, "title": "Tensorflow writes events file in TMPDIR with unbounded size", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.15.0-rc1-11276-gc9f7f636eb 2.1.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 2.1.1\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: GTX 1080Ti 11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have a custom keras model and I am using the fit api to train. I am NOT using the tensorboard callback. During a training an events file in generated in my `/tmp` directory that slowly grows in size until all remaining space on the hard drive is consumed, causing training to crash as it is not able to write to this file any longer.\r\n\r\n```\r\n-rw-r--r-- 1 bidski bidski 28G May  6 10:00 /tmp/tmp_50ztys5/events.out.tfevents.1587776979.bidski-alien.30810.513.v2\r\n```\r\n\r\nI once tried deleting the file once it got too large, but this resulted in the training process hanging. Why is this file even created?\r\n\r\n**Describe the expected behavior**\r\nI would expect a tempfile to \r\n  1. not consume the entire hard drive\r\n  1. not cause training to crash when writing to the file fails\r\n\r\n", "comments": ["I believe this is because of [StatsAggregator](https://www.tensorflow.org/api_docs/python/tf/data/experimental/StatsAggregator). I completely forgot that I had this in my code and without looking into the code for `StatsAggregator` I had no idea that it created an events file in `TMPDIR`. \r\n\r\nI propose that `StatsAggregator` be modified so that a log directory can be provided for `StatsAggregator` to write to, or to have a `SummaryWriter` object passed to it.", "@Bidski \r\n\r\nWill it be possible to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\n\r\nHere is a simple snippet of code which I suspect causes the problem. \r\n\r\n```\r\ndef create_dataset(path, args):\r\n    # Create a stats aggregator for the dataset\r\n    aggregator = tf.data.experimental.StatsAggregator()\r\n\r\n    # Load in the dataset\r\n    dataset = tf.data.TFRecordDataset(path)\r\n\r\n    # Convert dataset to training format\r\n    dataset = dataset.map(parse_entry)\r\n\r\n    # Take a random crop\r\n    dataset = dataset.map(random_crop)\r\n\r\n    # Fix up element shapes\r\n    dataset = dataset.map(fixup_shape)\r\n\r\n    # Shuffle the elements\r\n    if args.shuffle_size > 1:\r\n        dataset = dataset.shuffle(args.shuffle_size, reshuffle_each_iteration=False)\r\n\r\n    # Set up batch size\r\n    dataset = dataset.batch(batch_size=args.batch_size, drop_remainder=False)\r\n\r\n    # Aggregate stats\r\n    dataset = dataset.apply(tf.data.experimental.latency_stats(\"total_bytes\"))\r\n    dataset = dataset.apply(tf.data.experimental.bytes_produced_stats(\"bytes_produced\"))\r\n    options = tf.data.Options()\r\n    options.experimental_stats.aggregator = aggregator\r\n    options.experimental_stats.latency_all_edges = True\r\n    dataset = dataset.with_options(options)\r\n\r\n    return dataset\r\n```\r\n\r\nIt loads a TFRecord file, does some simple transformations to the data (via the map calls) and initialises stats aggregation on the dataset. This dataset is then fed to a keras model via the fit API.\r\n\r\nWith the dataset stats aggregation an events file is created in `/tmp` and this file slowly (I assume the rate of growth is dependent on on the number of the elements in the dataset and the size of the elements in the dataset) grows in size. \r\n\r\nRemoving the dataset stats aggregation results in no event files being generated (tensorboard callback is not being used and there are no summary writers anywhere in my code).\r\n", "@Bidski Is this still an issue? Can you please share a simple standalone code to reproduce the issue? Thanks!", "I don't think this is a coding issue. Any software library that writes files to disk has potential to fill in the partition and then crash. It is the responsibility of the user of the library to make sure this does not happen.\r\n\r\nIt is a different issue though if the unbounded write is not a documented behavior or is unexpected or could be triggered by small user controlled inputs. Please let us if that is the case", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39205\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39205\">No</a>\n"]}, {"number": 39204, "title": "TF-TRT Conv2d op conversion dynamic shape mode", "body": "This PR improves the Conv2d op converter to handle explicit batch and dynamic shape input data.\r\nTagging @bixia1 for review.", "comments": ["@tfeher Can you please resolve conflicts? Thanks!"]}, {"number": 39203, "title": "Ubtunu:14.04 Docker Build Terminates with fatal error: pybind11/pybind11.h: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n### OS Platform and Distribution \r\n- [x] Ubuntu 14.04 arm32\r\n### Mobile device?\r\n- [ ] Nope \r\n### TensorFlow installed from (source or binary):\r\n- [ ] Source\r\n### TensorFlow version:\r\n- [ ] 2.1.0 \r\n### Python version:\r\n- [ ] python3.4\r\n### Installed using\r\n- [ ] Grabbed it from Git\r\n### Bazel Version?\r\n### GCC/Compiler version (if compiling from source):\r\nn/a\r\n### CUDA/cuDNN version:\r\nn/a\r\n### GPU model and memory:\r\nn/a\r\n\r\n### Describe the problem\r\nTrying to use a Docker container to build a clean tflite build from the scripts specified in:\r\n```\r\n/tensorflow/tensorflow/lite/tools/pip_package/\r\n```\r\n\r\n### Provide the exact sequence of commands / steps that you executed before running into the problem\r\n```\r\n$ tensorflow/lite/tools/make/download_dependencies.sh\r\n$ tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n```\r\n### Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is what I'm getting right after running the second command\r\n```bash\r\n+++ dirname tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n++ cd tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python3\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ grep '_VERSION = ' /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ cut -d= -f2\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.1.0\r\n+ export PACKAGE_VERSION=2.1.0\r\n+ PACKAGE_VERSION=2.1.0\r\n+ BUILD_DIR=/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ rm -rf /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ mkdir -p /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ cp -r /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup.py /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ cp /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ echo '__version__ = '\\''2.1.0'\\'''\r\n++ git -C /tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\n+ echo '__git_version__ = '\\''0.6.0-85035-g8e29dc7'\\'''\r\n+ cd /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ [[ -n '' ]]\r\n+ python3 setup.py bdist bdist_wheel\r\nrunning bdist\r\nrunning bdist_dumb\r\nrunning build\r\nrunning build_py\r\nrunning build_ext\r\nmake: Entering directory `/tensorflow'\r\nmake: Nothing to be done for `all'.\r\nmake: Leaving directory `/tensorflow'\r\nbuilding 'tflite_runtime._pywrap_tensorflow_interpreter_wrapper' extension\r\ncreating build\r\ncreating build/temp.linux-armv7l-3.4\r\ncreating build/temp.linux-armv7l-3.4/interpreter_wrapper\r\narm-linux-gnueabihf-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -I/tensorflow/tensorflow/lite/tools/pip_package/../../../.. -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package -I/usr/lib/python3/dist-packages/numpy/core/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/absl -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/local/include/python3.4 -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/include/python3.4m -I/usr/include/python3.4m -c interpreter_wrapper/interpreter_wrapper.cc -o build/temp.linux-armv7l-3.4/interpreter_wrapper/interpreter_wrapper.o --std=c++11\r\ncc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ [enabled by default]\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'std::string tflite::interpreter_wrapper::InterpreterWrapper::TensorName(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:317:56: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (!interpreter_ || i >= interpreter_->tensors_size() || i < 0) {\r\n                                                        ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorType(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:327:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorSize(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:345:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorSizeSignature(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:360:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorSparsityParameters(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:380:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorQuantization(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:391:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorQuantizationParameters(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:398:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::SetTensor(int, PyObject*)':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:432:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::NodeInputs(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:56:37: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->nodes_size() || i < 0) {         \\\r\n                                     ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:511:3: note: in expansion of macro 'TFLITE_PY_NODES_BOUNDS_CHECK'\r\n   TFLITE_PY_NODES_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::NodeOutputs(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:56:37: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->nodes_size() || i < 0) {         \\\r\n                                     ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:521:3: note: in expansion of macro 'TFLITE_PY_NODES_BOUNDS_CHECK'\r\n   TFLITE_PY_NODES_BOUNDS_CHECK(i);\r\n   ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In member function 'std::string tflite::interpreter_wrapper::InterpreterWrapper::NodeName(int) const':\r\ninterpreter_wrapper/interpreter_wrapper.cc:530:54: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (!interpreter_ || i >= interpreter_->nodes_size() || i < 0) {\r\n                                                      ^\r\ninterpreter_wrapper/interpreter_wrapper.cc: In function 'PyObject* tflite::interpreter_wrapper::{anonymous}::CheckGetTensorArgs(tflite::Interpreter*, int, TfLiteTensor**, int*)':\r\ninterpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (i >= interpreter_->tensors_size() || i < 0) {                         \\\r\n                                       ^\r\ninterpreter_wrapper/interpreter_wrapper.cc:556:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'\r\n   TFLITE_PY_TENSOR_BOUNDS_CHECK(tensor_index);\r\n   ^\r\nIn file included from /usr/lib/python3/dist-packages/numpy/core/include/numpy/ufuncobject.h:327:0,\r\n                 from /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper/numpy.h:52,\r\n                 from interpreter_wrapper/interpreter_wrapper.cc:36:\r\n/usr/lib/python3/dist-packages/numpy/core/include/numpy/__ufunc_api.h: At global scope:\r\n/usr/lib/python3/dist-packages/numpy/core/include/numpy/__ufunc_api.h:241:1: warning: 'int _import_umath()' defined but not used [-Wunused-function]\r\n _import_umath(void)\r\n ^\r\narm-linux-gnueabihf-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -I/tensorflow/tensorflow/lite/tools/pip_package/../../../.. -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package -I/usr/lib/python3/dist-packages/numpy/core/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/absl -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/local/include/python3.4 -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/include/python3.4m -I/usr/include/python3.4m -c interpreter_wrapper/interpreter_wrapper_pybind11.cc -o build/temp.linux-armv7l-3.4/interpreter_wrapper/interpreter_wrapper_pybind11.o --std=c++11\r\ncc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ [enabled by default]\r\ninterpreter_wrapper/interpreter_wrapper_pybind11.cc:16:31: fatal error: pybind11/pybind11.h: No such file or directory\r\n #include \"pybind11/pybind11.h\"\r\n                        \r\n```\r\n\r\n\r\n", "comments": ["#38904 ", "the strange part is that I have modified the setup.py file to point at the location directly, nevertheless, it still doesn't compile...is there a workaround in the meantime?\r\n", "This is probably Tagged Incorrectly, but I'm going to close the issue, because applying the above fix (still a pull request rn) solved it. \r\nNext time i'll read more thoroughly the other open issues!\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39203\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39203\">No</a>\n"]}, {"number": 39202, "title": "Update RELEASE.md", "body": "", "comments": []}, {"number": 39201, "title": "Update version numbers for TensorFlow 2.2.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 2\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.2.0-rc4\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.2.0rc4\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 39200, "title": "[INTEL MKL] Fix for UT failure in graph_runner_test", "body": "Fix for UT Failure: //tensorflow/core/common_runtime:graph_runner_test", "comments": []}, {"number": 39199, "title": "Breaking changes related to infinite generators missing in TF 2.2 release notes", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly\r\n\r\nI have spent the better part of a working day on tracking down several infinite loops using TF 2.2.0. In summary, the following two pieces of code run fine in TF 2.1.0, and run in infinite loops in TF 2.2.0:\r\n\r\n## Issue 1\r\n\r\n```\r\n\"\"\"Bug 1.\"\"\"\r\nimport numpy as np\r\nfrom tensorflow.keras.applications import vgg16\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nN = 9\r\ndata = ImageDataGenerator().flow(np.empty((N, 224, 224, 3)), np.empty((N, 1000)))\r\n\r\nmodel = vgg16.VGG16()\r\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\r\nmodel.fit(\r\n    x=data,\r\n    epochs=2,\r\n    verbose=2,  # 1\r\n    # steps_per_epoch=N,\r\n)\r\n\r\n# https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly\r\n# \"When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument.\"\r\n# This is a breaking change that should be part of the release notes.\r\n```\r\nThis code auto-selects `steps_per_epoch=1` (\"Train for 1 steps\") in TF 2.1.0, which is correct (default `batch_size` is 32, `len(train_flow)` is 1). In TF 2.2.0, it's an infinite loop that is hard to debug. After setting `verbose=1`, one can see that something is in fact happening, with the number of steps per epoch growing without limits; from there, looking at the docs, one notices that `steps_per_epoch` is required (\"When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument.\"). But it's hard to recognize that breaking change from the release notes. (It's even hard to know that the `ImageDataGenerator().flow()` does indeed repeat infinitely, since it does have a length.)\r\n\r\n## Issues 2 and 3\r\n\r\n```\r\n\"\"\"Bug 2.\"\"\"\r\nimport numpy as np\r\nfrom tensorflow.keras.applications import vgg16\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nN = 9\r\ndata = ImageDataGenerator().flow(np.empty((N, 224, 224, 3)), np.empty((N, 1000)))\r\n\r\nmodel = vgg16.VGG16()\r\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\r\nmodel.fit(\r\n    x=data,\r\n    epochs=2,\r\n    verbose=1,\r\n    # steps_per_epoch=N,\r\n    validation_data=data,\r\n    ## validation_steps=N,\r\n)\r\n\r\n# https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly\r\n# \"Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict, generator [...]\"\r\n# This is a breaking change that should be part of the release notes.\r\n```\r\nSimilarly, this code auto-selects `steps_per_epoch=1` and `validation_steps=1` (\"Train for 1 steps, validate for 1 steps\") in TF 2.1.0, which is correct (default `batch_size` is 32, `len(val_flow)` is 1). In TF 2.2.0, it's again an infinite loop that is hard to debug, even if one sets `verbose=1` **and** `steps_per_epoch=N` (since there is no verbosity on the validation part). Again, looking at the docs, one notices that `validation_steps` is required (\"In the case of an infinitely repeated dataset, it will run into an infinite loop.\"). But, again, it's hard to recognize that breaking change from the release notes. (Again, it's even hard to know that the `ImageDataGenerator().flow()` does indeed repeat infinitely, since it does have a length.)\r\n\r\nFinally, the nightly docs say that \"`validation_data` does not support all the data types that are supported in `x`, eg, dict, generator [...]\". As you can see above, using a `ImageDataGenerator` still works if you adhere to the other requirements. If use of any generator is disencouraged from now on, this is a breaking change that needs to go into the release notes in my opinion.", "comments": ["I am able to replicate this issue even on gpu, please find the gist of [nightly](https://colab.sandbox.google.com/gist/Saduf2019/02c85eb6a8a1dca5aa731be6a4512f81/nightly_gpu.ipynb) and [2.1](https://colab.sandbox.google.com/gist/Saduf2019/d228e57a1c59c9bd04e45dd7c36489d5/gpu.ipynb).", "@bersbersbers I ran your codes in `TF2.2` and `tf-nightly` and both are not any infinite loop now. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d181ea48127195b8d037fdeb112b13a4/nightly_gpu.ipynb). \r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This is similiar to #39277 and has been fixed in [b53ed4d](https://github.com/tensorflow/tensorflow/commit/b53ed4d560aaeb7a92185f4fbf2562e5e274456a). The fix is available in nightly and the next TF version release will have it. \r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39199\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39199\">No</a>\n"]}, {"number": 39198, "title": "Error while reading resource variable _AnonymousVar46 from Container: localhost.", "body": "I have this code: (all variables are initialized)\r\n`def train(models, X_train, noise_plot, dir_result=\"D:\\gan\\result\", epochs=10000, batch_size=100):\r\n        combined, discriminator, generator = models\r\n        nlatent_dim = noise_plot.shape[1]\r\n        half_batch  = int(batch_size / 2)\r\n        history = []\r\n        for epoch in range(epochs):\r\n\r\n            # ---------------------\r\n            #  Train Discriminator\r\n            # ---------------------\r\n\r\n            # Select a random half batch of images\r\n            idx = np.random.randint(0, X_train.shape[0], half_batch)\r\n            imgs = X_train[idx]\r\n            noise = get_noise(half_batch, nlatent_dim)\r\n\r\n            # Generate a half batch of new images\r\n            gen_imgs = generator.predict(noise)\r\n\r\n            \r\n            # Train the discriminator q: better to mix them together?\r\n            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\r\n            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\r\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n\r\n\r\n            # ---------------------\r\n            #  Train Generator\r\n            # ---------------------\r\n\r\n            noise = get_noise(batch_size, nlatent_dim)\r\n\r\n            # The generator wants the discriminator to label the generated samples\r\n            # as valid (ones)\r\n            valid_y = (np.array([1] * batch_size)).reshape(batch_size,1)\r\n            \r\n            # Train the generator\r\n            g_loss = combined.train_on_batch(noise, valid_y)\r\n\r\n            history.append({\"D\":d_loss[0],\"G\":g_loss})\r\n            \r\n            if epoch % 100 == 0:\r\n                # Plot the progress\r\n                print (\"Epoch {:05.0f} [D loss: {:4.3f}, acc.: {:05.1f}%] [G loss: {:4.3f}]\".format(\r\n                    epoch, d_loss[0], 100*d_loss[1], g_loss))\r\n            if epoch % int(epochs/100) == 0:\r\n                plot_generated_images(noise_plot,\r\n                                      path_save=dir_result+\"/image_{:05.0f}.png\".format(epoch),\r\n                                      titleadd=\"Epoch {}\".format(epoch))\r\n            if epoch % 1000 == 0:\r\n                plot_generated_images(noise_plot,\r\n                                      titleadd=\"Epoch {}\".format(epoch))\r\n                        \r\n        return(history)\r\nhistory = train(models=_models, X_train=X_train, noise_plot=noise, dir_result=dir_result,epochs=10000, batch_size=100)`\r\n- OS: Windows 10\r\n\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7\r\n\r\n\r\nAnd I have some problems:\r\n`FailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-13-7e6cdc16ab87> in <module>\r\n     72 _models = combined, discriminator, generator\r\n     73 \r\n---> 74 history = train(models=_models, X_train=X_train, noise_plot=noise, dir_result=dir_result,epochs=10000, batch_size=100)\r\n     75 end_time = time.time()\r\n     76 print(\"-\"*10)\r\n\r\n<ipython-input-13-7e6cdc16ab87> in train(models, X_train, noise_plot, dir_result, epochs, batch_size)\r\n     27 \r\n     28             # Train the discriminator q: better to mix them together?\r\n---> 29             d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\r\n     30             d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\r\n     31             d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1512             ins = x + y + sample_weights\r\n   1513         self._make_train_function()\r\n-> 1514         outputs = self.train_function(ins)\r\n   1515 \r\n   1516         if reset_metrics:\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py in __call__(self, inputs)\r\n   3725         value = math_ops.cast(value, tensor.dtype)\r\n   3726       converted_inputs.append(value)\r\n-> 3727     outputs = self._graph_fn(*converted_inputs)\r\n   3728 \r\n   3729     # EagerTensor.numpy() will often make a copy to ensure memory safety.\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1549       TypeError: For invalid positional/keyword argument combinations.\r\n   1550     \"\"\"\r\n-> 1551     return self._call_impl(args, kwargs)\r\n   1552 \r\n   1553   def _call_impl(self, args, kwargs, cancellation_manager=None):\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n   1589       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n   1590           list(kwargs.keys()), list(self._arg_keywords)))\r\n-> 1591     return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n   1592 \r\n   1593   def _filtered_call(self, args, kwargs):\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\nC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError:  Error while reading resource variable _AnonymousVar46 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar46/class tensorflow::Var does not exist.\r\n\t [[node mul_46/ReadVariableOp (defined at C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_2593]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph`\r\n![image](https://user-images.githubusercontent.com/20381972/81101401-bf508880-8f16-11ea-80fc-773a8a6f0390.png)\r\n![image](https://user-images.githubusercontent.com/20381972/81101421-caa3b400-8f16-11ea-979b-1af07cd77b96.png)\r\n", "comments": ["@EkaterinaAntipova \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version?\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39198\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39198\">No</a>\n"]}, {"number": 39197, "title": "Very slow recording of TFRecordWriter with tf.data.Dataset.shard()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0-dev20200412\r\n- Python version: 3.6.10\r\n\r\n**Describe the current behavior**\r\nI'm trying to convert my dataset to multiple *.tfrecord files. My initial idea was to compile a `tf.data.Dataset` with all data of a certain class, and then split it into multiple datasets using `tf.data.Dataset.shard(...)`, which produced terrible recording to disk speed. \r\n\r\nAs a workaround i split and compiled the data first into multiple `tf.data.Dataset` and recorded them as it is, which produced much better results.\r\n\r\nCode to reproduce: https://gist.github.com/theonekeyg/68b3df515ce872d70611f92a857895a1\r\nThe exact data that i used can be obtained from this Kaggle competition: https://www.kaggle.com/c/alaska2-image-steganalysis/data\r\n\r\nTo be more specific, with `tf.data.Dataset.shard(...)` approach, to process 3000 samples it took ~35 minutes, and ~1.2 minutes with second approach.\r\n\r\nIs that an expected behavior, that `ShardDataset` takes so long to compile/write to .tfrecord?", "comments": ["@theonekeyg,\r\nDue to the size of the dataset, I am unable to reproduce the issue. Could you please provide a sample dataset of a smaller size, so that we can look into this. Thanks!", "Hi, sure. One thing i found in particular while i was making a slice of a dataset, is that the performance issue isn't that clear or completely doesn't appear on small datasets. I tried to keep the slice as small as i could but so it still could clearly produce the performance issue, i also put the scripts to reproduce the issue to .zip archive: https://drive.google.com/file/d/1PkZjEJzUn3ioXIoNWgXDtSU9zMmsQAkk/view?usp=sharing\r\n\r\nHopefully you can see as i see, and also, according to my experience, the larger the dataset, the clearer the issue, so if the issue won't appear, i recommend trying it on bigger dataset, or i could upload larger sample of ALASKA2 dataset.", "I'm having the same issue, and noticed what you told is true. If I'm writing a TFrecord with ~16000 images in it, I get 2 it/s and writing 4040 images gives me 6 it/s.", "Was able to reproduce the issue with TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/85f404dbae4ec1679cf756e8199b94bb/39197.ipynb). Thanks!", "Why not use the method suggested by the [official documentation](https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter)? This will work if the dataset can fit into memory.", "It's not about what approach to use, although i tested the method from official documentation, it produces about the same speed as my second approach. I am curious if that's a known issue, that ShardDataset produces really bad speed at writing to .tfrecord?", "From [official documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard):\r\n\r\n\"Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. ...\"\r\n\r\nThis is because `shard` will evaluate the entire upstream input pipeline filtering out `(num_shards - 1) / num_shards` of the data. In your example, you are running the input pipeline inside of a for loop, which means that you will preprocess the entire input pipeline `num_shards` many times.\r\n\r\nYou could use `window` instead of `shard` to divide up the dataset into chunks and save each chunk separately along the following lines:\r\n\r\n```\r\ndataset = ...\r\ndataset = dataset.window(ELEMENTS_PER_FILE)\r\ndataset = dataset.enumerate()\r\n\r\ndef write_data(i, dataset):\r\n  out_path = os.path.join(output_folder, f'{folder}{i+1}.tfrecord')\r\n  dump_dataset(dataset, out_path)\r\n  return out_path\r\n\r\ndataset = dataset.map(write_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n```\r\n\r\nIf you get rid of using `from_generator` in your `dump_dataset` method, you would be able to do the writing in parallel as well (which is additional inefficiency of your program -- writing a unnecessarily serialized on Python GIL).", "It's clear to me now, thanks for the thoughtful explanation :) @jsimsa", "Hello,\r\n@jsimsa do you have a working example?\r\n\r\nThank you", "In my case I follow the following [blog](https://towardsdatascience.com/how-to-build-efficient-audio-data-pipelines-with-tensorflow-2-0-b3133474c3c1) and use [this code](https://gist.githubusercontent.com/dschwertfeger/0c787c0af781c28720c28f1bebc9a126/raw/37dc7a5be0cc79ecb9809898a8e460133b936f76/convert.py).\r\n\r\nI have 300000 audio files(each being a 10s 48KHz wav file) approximately. It will take approximately 30hour to convert all the wav files to tfrecord. Am I doing something wrong? or it is normal? \r\n\r\nNote: I've seen the disk usage and CPU usage. It shows that it is using only one core.\r\n\r\n\r\n\r\n**UPD:** **I solved the issue with `multiprocessing`, with it I reduced my convertion time 32x less. The only thing I did is, divide the task among the cores.**"]}, {"number": 39196, "title": "Got small output value error between .h5 model and .pb model", "body": "I tried both on `tf-gpu1.4+keras2.1.3` and on `tf-gpu1.12+keras2.2.4` and the problem always happens.\r\n\r\nThe problem is:  **After I converted the keras.application.ResNet50() model into freeze graph model in .pb format, I feed in the same picture into the converted .pb model but the output value changes just a little.**\r\n\r\nBelow is the codes, which prints the first 10 element of the ResNet output vector , and also freeze the graph to output pb model file:\r\n```\r\nfrom tensorflow.python.framework.graph_util_impl import convert_variables_to_constants\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.resnet50 import preprocess_input, ResNet50\r\nimport keras.backend as K\r\nK.set_learning_phase(0)\r\n\r\nimg = image.load_img('images/34rews.jpg', target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx_input = preprocess_input(x)\r\n\r\nnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\r\nsess = K.get_session()\r\npreds = sess.run(net_model.get_output_at(0), feed_dict={net_model.get_input_at(0): x_input})\r\nprint('before convert to pb :', np.array(preds).squeeze()[:10])\r\n\r\noutput_name0 = net_model.get_output_at(0).op.name  # 'global_average_pooling2d_1/Mean'\r\nconstant_graph = convert_variables_to_constants(sess, sess.graph_def, [output_name0])\r\n\r\nwith tf.gfile.GFile('saved_model_constant.pb', 'wb') as f:\r\n    f.write(constant_graph.SerializeToString())\r\n```\r\n\r\nand the print log is : \r\n```\r\nbefore convert to pb : [**0.99536467** 0.31807986 2.0998483  0.9077819  0.10606026 0.93215793\r\n 0.04187933 0.10000334 1.1727284  1.0535308 ]\r\n```\r\n\r\nThen we predict the same image through the pb file generated by above codes:\r\n```\r\ndef test_constant(pb_dir, img_path='images/34rews.jpg'):\r\n    img = image.load_img(img_path, target_size=(224, 224))\r\n    x = image.img_to_array(img)\r\n    x = np.expand_dims(x, axis=0)\r\n    x = preprocess_input(x)\r\n\r\n    from tensorflow.python.platform import gfile\r\n    with tf.Session() as sess:\r\n        with gfile.FastGFile(pb_dir, 'rb') as f:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(f.read())\r\n\r\n        result = tf.import_graph_def(graph_def, return_elements=[\"global_average_pooling2d_1/Mean:0\"], name='')\r\n        preds = sess.run(result, feed_dict={sess.graph.get_tensor_by_name('input_1:0'):x})\r\n        print('using pb file:', np.array(preds).squeeze()[:10])\r\n```\r\n\r\nThe output printing log is:\r\n```\r\nusing pb file: [**0.99536514** 0.3180797  2.0998483  0.90778273 0.10606024 0.9321572\r\n 0.04187941 0.10000295 1.1727289  1.0535315 ]\r\n```\r\n\r\nI can clearly find the extreme small value error of the predict vector, between the original keras model and the pb model after using the freeze graph method.\r\ne.g. The first element value of the resnet output vector using original keras model is **0.99536467**, but the output one is **0.99536514** using the converted pb file. \r\nI wonder why there is such a small value error? It may not cause big accuracy error but it is really strange!", "comments": ["\u6211\u4e5f\u60f3\u5c06\u6a21\u578b\u51bb\u7ed3\u5e76\u8f6c\u79fb\u81f3\u5176\u5b83\u8bed\u8a00\u4f7f\u7528\uff0c\u4f46\u662f\u6211\u65e0\u6cd5\u627e\u5230\u6700\u540e\u7684\u8f93\u51fa\u5c42\u540d\u5b57\uff0c\u697c\u4e3b\u53ef\u5426\u8d34\u4e00\u4e0b\u51bb\u7ed3\u4ee3\u7801\u6216\u8005\u544a\u77e5\u4e00\u4e0b\u8f93\u51fa\u5c42\u540d\u5b57", "@Whatsetsthisend tensorflow\u7684\u8bdd\u53ef\u901a\u8fc7\u8fd9\u4e2a[notebook](https://github.com/Tony607/keras-tf-pb/blob/master/keras_freeze_tf_pb.ipynb)\u91cc\u5b9e\u4f8b\u91cc\u7684\u65b9\u6cd5\u6253\u5370\u51fa\u6765graph_def\u7684node\u7684name\uff0c\u7136\u540e\u81ea\u5df1\u6dfb\u52a0:0\u5373\u53ef\u3002Keras\u901a\u8fc7model.summary()\u5373\u53ef\u67e5\u770b\u3002", "I removed the `K.set_learning_phase(0)` and the problem is solved. Maybe it could be better to let Keras to handle with the K.learning_phase() by iteself.\r\nFor my experiment, use the value of Keras.model.predict() as the correct outcome:\r\n```\r\nwith K.set_learning_phase(0) and without convert_variables_to_constants:  value is the same.\r\nwithout K.set_learning_phase(0) and without convert_variables_to_constants:  value is the same.\r\nwithout K.set_learning_phase(0) and with convert_variables_to_constants:  value is the same.\r\nwith K.set_learning_phase(0) and with convert_variables_to_constants:  value is changed!\r\n```", "@youyuge34 \r\nAs the issue is resolved please confirm if we may move this to closed status ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39196\">No</a>\n"]}, {"number": 39195, "title": "Possible bug with rejection sampling", "body": "This is about the [official TensorFlow tutorial on input pipelines](https://www.tensorflow.org/guide/data). In the [section](https://www.tensorflow.org/guide/data#rejection_resampling) on rejection sampling, the very last cell, namely\r\n```\r\nfor features, labels in balanced_ds.take(10):\r\n    print(labels.numpy())\r\n```\r\nproduces a set of labels which are all zeros, indicating that the dataset hasn't been balanced.\r\n\r\n**System information**\r\n- Windows 10\r\n- Python 3.7.4 under conda 4.8.3\r\n- tensorflow-datasets 1.3.0\r\n- tensorflow-estimator 2.0.1\r\n- tensorflow-gpu 2.1.0 \r\n- tensorflow-gpu-estimator 2.1.0\r\n- gast 0.2.2        ", "comments": ["@laghaout \r\nI have tried in colab in TF version 2.1,2.2.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c77f99292e6e5b0c15dadb6527f548e5/untitled51.ipynb).Is this the expected behavior?.Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ravikyram What you're getting is indeed the expected behaviour, but what I get is a 10-by-10 matrix of all zeros.", "@laghaout \r\n\r\nThis is not a bug in Tensorflow as we see the results as expected in the colab. Can you try again in virtual environment and see if the problem is still persists.Thanks!", "Automatically closing this out since I understand it is not a bug in Tensorflow, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39195\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39195\">No</a>\n"]}, {"number": 39194, "title": "Disable OwnedMultiDeviceIterator on TPU Pod.", "body": "PiperOrigin-RevId: 309309347\r\nChange-Id: Ie8acfc4da7a349e9eeda530baaf6eb330b02cf63", "comments": []}, {"number": 39193, "title": "Increase Apache Spark version to 2.4.5 to handle GitHub Security Alert", "body": "Handles CVE-2019-10099, CVE-2018-17190, CVE-2018-11770.\r\n\r\nTo be cherrypicked on r1.15, r2.0, r2.1 and r2.2 branches\r\n\r\nPiperOrigin-RevId: 309955549\r\nChange-Id: I5ee68fdd3270534066487be67232c1abc687f968", "comments": []}, {"number": 39192, "title": "Increase Apache Spark version to 2.4.5 to handle GitHub Security Alert", "body": "Handles CVE-2019-10099, CVE-2018-17190, CVE-2018-11770.\r\n\r\nTo be cherrypicked on r1.15, r2.0, r2.1 and r2.2 branches\r\n\r\nPiperOrigin-RevId: 309955549\r\nChange-Id: I5ee68fdd3270534066487be67232c1abc687f968", "comments": []}, {"number": 39191, "title": "Increase Apache Spark version to 2.4.5 to handle GitHub Security Alert", "body": "Handles CVE-2019-10099, CVE-2018-17190, CVE-2018-11770.\r\n\r\nTo be cherrypicked on r1.15, r2.0, r2.1 and r2.2 branches\r\n\r\nPiperOrigin-RevId: 309955549\r\nChange-Id: I5ee68fdd3270534066487be67232c1abc687f968", "comments": []}, {"number": 39190, "title": "Increase Apache Spark version to 2.4.5 to handle GitHub Security Alert", "body": "Handles CVE-2019-10099, CVE-2018-17190, CVE-2018-11770.\r\n\r\nTo be cherrypicked on r1.15, r2.0, r2.1 and r2.2 branches\r\n\r\nPiperOrigin-RevId: 309955549\r\nChange-Id: I5ee68fdd3270534066487be67232c1abc687f968", "comments": []}, {"number": 39189, "title": "ValueError: Please initialize `TimeDistributed` layer with a `Layer` instance. You passed: <mrcnn.model1.BatchNorm object at 0x7f0b39c36588>", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 19.10):\r\n- TensorFlow installed from (binary):\r\n- Libraries: \r\n     - Keras==2.1.6\r\n     - tensorflow==1.14.0\r\n     - tensorflow-model-optimization==0.3.0\r\n\r\n\r\n**My code snippet:**\r\n\r\n```\r\nfrom mrcnn import model as modellib\r\n\r\ncustom={        \r\n    'DepthwiseConv2D': keras.applications.mobilenet.DepthwiseConv2D,\r\n    'PyramidROIAlign':modellib.PyramidROIAlign,\r\n    'DetectionLayer':modellib.DetectionLayer,\r\n    'ProposalLayer': modellib.ProposalLayer,\r\n    'BatchNorm':modellib.BatchNorm,\r\n    'relu6':relu6,\r\n    'tf':tf,\r\n}\r\n\r\ntflite_converter = tf.lite.TFLiteConverter.from_keras_model_file(model_path, \r\n                                                    custom_objects=custom)\r\n```\r\n\r\n**Output Error:**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-0cdc241e349b> in <module>\r\n     15 \r\n     16 tflite_converter = tf.lite.TFLiteConverter.from_keras_model_file(model_path, \r\n---> 17                                                     custom_objects=custom)\r\n     18 \r\n     19 tflite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in from_keras_model_file(cls, model_file, input_arrays, input_shapes, output_arrays, custom_objects)\r\n    745     _keras.backend.clear_session()\r\n    746     _keras.backend.set_learning_phase(False)\r\n--> 747     keras_model = _keras.models.load_model(model_file, custom_objects)\r\n    748     sess = _keras.backend.get_session()\r\n    749 \r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    144       h5py is not None and (\r\n    145           isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\r\n--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n    147 \r\n    148   if isinstance(filepath, six.string_types):\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\r\n    210     model_config = json.loads(model_config.decode('utf-8'))\r\n    211     model = model_config_lib.model_from_config(model_config,\r\n--> 212                                                custom_objects=custom_objects)\r\n    213 \r\n    214     # set weights\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)\r\n     53                     '`Sequential.from_config(config)`?')\r\n     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n---> 55   return deserialize(config, custom_objects=custom_objects)\r\n     56 \r\n     57 \r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n     87       module_objects=globs,\r\n     88       custom_objects=custom_objects,\r\n---> 89       printable_module_name='layer')\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    190             custom_objects=dict(\r\n    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 192                 list(custom_objects.items())))\r\n    193       with CustomObjectScope(custom_objects):\r\n    194         return cls.from_config(cls_config)\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in from_config(cls, config, custom_objects)\r\n   1119     # First, we create all layers and enqueue nodes to be processed\r\n   1120     for layer_data in config['layers']:\r\n-> 1121       process_layer(layer_data)\r\n   1122     # Then we process nodes in order of layer depth.\r\n   1123     # Nodes that cannot yet be processed (if the inbound node\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in process_layer(layer_data)\r\n   1103       from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\r\n   1104 \r\n-> 1105       layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n   1106       created_layers[layer_name] = layer\r\n   1107 \r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n     87       module_objects=globs,\r\n     88       custom_objects=custom_objects,\r\n---> 89       printable_module_name='layer')\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    190             custom_objects=dict(\r\n    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 192                 list(custom_objects.items())))\r\n    193       with CustomObjectScope(custom_objects):\r\n    194         return cls.from_config(cls_config)\r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py in from_config(cls, config, custom_objects)\r\n     84     layer = deserialize_layer(\r\n     85         config.pop('layer'), custom_objects=custom_objects)\r\n---> 86     return cls(layer, **config)\r\n     87 \r\n     88 \r\n\r\n~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py in __init__(self, layer, **kwargs)\r\n    149       raise ValueError(\r\n    150           'Please initialize `TimeDistributed` layer with a '\r\n--> 151           '`Layer` instance. You passed: {input}'.format(input=layer))\r\n    152     super(TimeDistributed, self).__init__(layer, **kwargs)\r\n    153     self.supports_masking = True\r\n\r\nValueError: Please initialize `TimeDistributed` layer with a `Layer` instance. You passed: <mrcnn.model1.BatchNorm object at 0x7f0b39c36588>\r\n\r\n```\r\n\r\n\r\n**Failure details:**\r\nIt' not even loading model from keras file. It's a custom build MRCNN model so there is some kind of issue with layers, but I am not sure.\r\n\r\n\r\n**Error source:**\r\nError generated by this [file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/wrappers.py). Kindly if any one knows the solution post it here.", "comments": ["@Craftsman381,\r\nOn running the given code, I'm facing an error stating `ModuleNotFoundError: No module named 'mrcnn'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e1cdd4d58a94b28700cba8e812ecf922/39189.ipynb).\r\n\r\nLooks like you are importing user defined modules in the code. In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!"]}, {"number": 39188, "title": "Add Python 3.8 to classifiers and remove old ones", "body": "According to\r\nhttps://github.com/tensorflow/tensorflow/issues/33374#issuecomment-595341808,\r\nnightly and TF 2.2 support Python 3.8.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39188) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39188) for more info**.\n\n<!-- cla_yes -->", "OVerriding CLA check as this is a cherry-pick"]}, {"number": 39187, "title": "Fix a flaky test when pretty print function keyword arguments in Python 3.5", "body": "I observed the following error with Python 3.5 on `//tensorflow/python/eager:function_test`:\r\n\r\n```\r\nFAIL: testPrettyPrintedSignature (__main__.FunctionTest)\r\ntestPrettyPrintedSignature (__main__.FunctionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/bazel-buildfarm/default/operations/83398799-fc19-4863-a3e0-99fb3531f4ad/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py\", line 3625, in testPrettyPrintedSignature\r\n    c3_summary + '\\n' + c3_details)\r\nAssertionError: Regex didn't match: \"func\\\\(x, kangaroo=None, octopus=7\\\\)\\n  Args:\\\\n    x: {'a': <1>, 'b': \\\\[<2>, <3>\\\\]}\\\\n      <1>: int32 Tensor, shape=\\\\(\\\\)\\\\n      <2>: RaggedTensorSpec\\\\(.*\\\\)\\\\n      <3>: RaggedTensorSpec\\\\(.*\\\\)\\\\n  Returns:\\\\n    {'a': <1>, 'b': \\\\[<2>, <3>\\\\]}\\\\n      <1>: int32 Tensor, shape=\\\\(\\\\)\\\\n      <2>: RaggedTensorSpec\\\\(.*\\\\)\\\\n      <3>: RaggedTensorSpec\\\\(.*\\\\)\" not found in \"func(x, kangaroo=None, octopus=7)\\n  Args:\\n    x: {'b': [<2>, <3>], 'a': <1>}\\n      <1>: int32 Tensor, shape=()\\n      <2>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)\\n      <3>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)\\n  Returns:\\n    {'b': [<2>, <3>], 'a': <1>}\\n      <1>: int32 Tensor, shape=()\\n      <2>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)\\n      <3>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)\"\r\n```\r\n\r\nIt appears that when enumerating the `kwargs` dictionary, the order is only deterministic for Python >=3.6 (so the pre-submit didn't catch it). See https://mail.python.org/pipermail/python-dev/2017-December/151263.html\r\n\r\ncc @edloper ", "comments": ["why cant i donwload my python in my socialfish. i was trying for 4 days \r\n", "Sorry, wrong fix. Turns out it is a nested regular arg, not kwargs. Still digging...", "@edloper seems it is fixed in https://github.com/tensorflow/tensorflow/commit/14e27e3b9ba44e0acd3cde5a4f63067ece780417. I will close this one."]}, {"number": 39186, "title": "tf.data.experimental.make_csv_dataset modifies mutable variables passed to it", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow installed from (source or binary): docker\r\n- TensorFlow version (use command below): TF 2.1.0\r\n\r\n**Describe the current behavior**\r\n\r\ntf.data.experimental.make_csv_dataset modifies passed variables in place. So if you call\r\n`tf.data.experimental.make_csv_dataset(file_pattern, batch_size, select_columns=columns_to_use)`\r\nthe variable `columns_to_use` is changed. It's in line 463 https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/data/experimental/ops/readers.py#L463 but it may happen to other variables passed. Specifically the list sent to `select_columns=` is replaced by a list of the indices of those columns in the file to read.\r\n\r\n**Describe the expected behavior**\r\n\r\nA function should never modify the mutable objects passed to it. This is only ever appropriate for methods of a class.", "comments": ["@grofte \r\nPlease provide with simple stand alone code for us to replicate the issue faced.", "```python\r\nimport tensorflow as tf\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'a': [2, 3, 5], 'b': [3, 4, 6], 'c': [4, 5, 7]})\r\ndf.to_csv('tf.csv', index=False)\r\ncolumns_to_use = ['b', 'c']\r\nprint(columns_to_use)\r\nds = tf.data.experimental.make_csv_dataset('tf.csv', \r\n                                            batch_size=1, \r\n                                            select_columns=columns_to_use)\r\nprint(columns_to_use)\r\n```\r\n\r\n['b', 'c']  \r\n[1, 2]", "I am able to replicate this issue please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/e52ece5e44c25385b36a7e6288063ece/untitled166.ipynb)", "@rachellim could you please take a look? thanks", "thanks for catching this @grofte ! fix incoming.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39186\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39186\">No</a>\n"]}, {"number": 39185, "title": "Failed to find bogomips on s390x architecture", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): s390x Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):  v2.2.0-rc4-0-g70087ab4f4 2.2.0-rc4\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): Build label: 2.0.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen running test cases on s390x architecture following Warning message is logged:\r\n\r\n`W tensorflow/core/platform/profile_utils/cpu_utils.cc:106] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency`\r\n\r\n**Describe the expected behavior**\r\nCPU frequency on s390x architecture should be correctly detected.\r\n\r\n**Standalone code to reproduce the issue**\r\nRun the following test case:\r\n```\r\nroot@3cefe6d659f6:/home/tensorflow# python tensorflow/python/data/experimental/kernel_tests/optimization/shuffle_and_repeat_fusion_test.py\r\nRunning tests under Python 3.6.9: /usr/bin/python\r\n[ RUN      ] ShuffleAndRepeatFusionTest.testShuffleAndRepeatFusion\r\n2020-05-05 13:11:16.071244: W tensorflow/core/platform/profile_utils/cpu_utils.cc:106] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nIt seems like `tensorflow/core/platform/profile_utils/cpu_utils.cc`  is not parsing `bogomips` line correctly on s390x: \r\n```\r\nroot@3cefe6d659f6:/home# cat /proc/cpuinfo | grep bogomips\r\nbogomips per cpu: 3033.00\r\n```\r\nThis patch should fix it:\r\n```\r\ndiff --git a/tensorflow/core/platform/profile_utils/cpu_utils.cc b/tensorflow/core/platform/profile_utils/cpu_utils.cc\r\nindex 587c97875a..b22123a804 100644\r\n--- a/tensorflow/core/platform/profile_utils/cpu_utils.cc\r\n+++ b/tensorflow/core/platform/profile_utils/cpu_utils.cc\r\n@@ -88,6 +88,8 @@ static ICpuUtilsHelper* cpu_utils_helper_instance_ = nullptr;\r\n      defined(__ppc__) && (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__))\r\n     retval = sscanf(line.c_str(), \"clock              : %lfMHz\", &cpu_freq);\r\n     freq_factor = 1.0;\r\n+#elif defined(__s390x__)\r\n+    retval = sscanf(line.c_str(), \"bogomips per cpu: %lf\", &cpu_freq);\r\n #else\r\n     retval = sscanf(line.c_str(), \"bogomips : %lf\", &cpu_freq);\r\n #endif\r\n```", "comments": ["@rposts Thank you very much for reporting the issue and suggesting the fix! \r\n1. Would you like to open a pull request to fix this? I can review it quickly. (Please mention me in the PR so I can see it right away.)\r\n2. Or I can make the change now and mention your username in the commit log, but your name might not appear in the contributors list in the release note (see the end of the [2.2.0-rc4 release note](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc4) for example). (I think the list is auto-generated from commit authors.) \r\n\r\nLet me know which way you prefer.\r\n\r\n", "Sure - I can PR. I will get the process started on my end. Thanks.", "Closing - Fixed in https://github.com/tensorflow/tensorflow/pull/39311", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39185\">No</a>\n"]}, {"number": 39184, "title": "InvalidArgumentError: PartialTensorShape: Incompatible shapes during merge: [1,2] vs. [2,2]", "body": "**System information**  \r\n-OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n-Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n-TensorFlow installed from (source or binary): NVIDIA (https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes)\r\n-TensorFlow version (use command below): 2.1\r\n-Python version: 3.6.9\r\n\r\n\r\nHello, I am running my NN model and it runs perfectly some epochs and suddenly it breaks showing up the next issues (sometimes one and sometimes the other one).\r\n\r\n_**InvalidArgumentError**:  PartialTensorShape: Incompatible shapes during merge: [1,2] vs. [2,2]\r\n\t [[node TensorArrayV2Stack/TensorListStack (defined at /workspace/code/model_NN.py:268) ]] [Op:__forward_p2d_fluoro_prediction_117955]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node TensorArrayV2Stack/TensorListStack:\r\n while (defined at /workspace/code/model_NN.py:261)\r\n\r\nFunction call stack:\r\np2d_fluoro_prediction_\r\n\r\n_**InvalidArgumentError:** 2 root error(s) found.\r\n  (0) Invalid argument:  Tried to set a tensor with incompatible shape at a list index. Item element shape: [2,2] list shape: [1,2]\r\n\t [[{{node while/body/_1/TensorArrayV2Write/TensorListSetItem}}]]\r\n  (1) Invalid argument:  Tried to set a tensor with incompatible shape at a list index. Item element shape: [2,2] list shape: [1,2]\r\n\t [[{{node while/body/_1/TensorArrayV2Write/TensorListSetItem}}]]\r\n\t [[while/loop_body_control/_24/_13]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__forward_p2d_fluoro_prediction_1262956]\r\n\r\nFunction call stack:\r\np2d_fluoro_prediction -> p2d_fluoro_prediction_\r\n\r\n\r\nFor me, this is a strange behavior because each time it breaks after a different epoch.  I am running all my dataset in each epoch so it is not a problem of the data. I have try adding this line tf.config.threading.set_inter_op_parallelism_threads(1) like this, but adain it does not work:\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n              tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\ntf.keras.backend.set_floatx('float32')\r\n\r\ncontext._context = None\r\ncontext._create_context()\r\n\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n```\r\n\r\n\r\nThe function which cause the problem is the following one and it works well.\r\n\r\n```\r\n@tf.function    \r\n    def p2d_fluoro_prediction(self,heat_map):\r\n        points = tf.TensorArray(tf.int64, size=self.nb_points,dynamic_size=True)\r\n        for i in tf.range(self.nb_points):\r\n            hm = heat_map[:,:,i]\r\n            heat_max = tf.reduce_max(hm)\r\n            point = tf.where(hm[:,:]==heat_max)\r\n            point = tf.reverse(point, [1])\r\n            points = points.write(i,point)          \r\n        points = points.stack()    \r\n        return tf.cast(tf.squeeze(points,axis = 1),dtype ='float32')\r\n```\r\n\r\nI hope you can help me with this issue. Because I do not if it is a problem of my program or is an internal problem in tensorflow. Thank you in advance.\r\n\r\n\r\n", "comments": ["@patriciacs1994,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the supporting files used in the code. Thanks!", "Hello, the problem is fixed, it was a small bug in the code. Thank you for your help. ", "@patriciacs1994 What was the small bug in your code? I am running into a similar error currently and cannot for the life of me figure out what the issue is... working with tf.data and using generators with dummy data works, but as soon as I copy data from a loaded CSV as np arrays it fails with this partial shape error.", "> @patriciacs1994 What was the small bug in your code? I am running into a similar error currently and cannot for the life of me figure out what the issue is... working with tf.data and using generators with dummy data works, but as soon as I copy data from a loaded CSV as np arrays it fails with this partial shape error.\r\n\r\nHi, my error was into the function, cause sometimes it tooks 2 points instead of one, so I had to put a conditional there, to avoid that problem. So, I recommend you to check your data and your functions.\r\nI hope that helps, good luck!", "Thanks for the quick answer! Will check my code again.\r\n\r\nEDIT: BTW, it only happens when I use `tf.vectorized_map` "]}, {"number": 39183, "title": "TF-TRT squeeze op conversion with empty squeeze_dims attribute", "body": "This PR improves TF-TRT squeeze op conversion in explicit batch mode.\r\n\r\nIn explicit batch mode with static input shapes, we treat empty squeeze_dims equivalent to squeeze all the size 1 dimensions. This is in accordance with TF's squeeze op definition.\r\n\r\nHandling empty squeeze_dims correctly is only possible in explicit batch mode with static (known) input shapes, since we need to know all the input dimensions during conversion time.\r\n\r\nIn implicit batch mode it is not possible to handle empty squeeze_dims: while all the non-batch dimensions must be known in implicit batch mode, the batch size is treated as unknown, and therefore we cannot decide whether we need to squeeze that during conversion time.", "comments": ["Tagging @bixia1 for review."]}, {"number": 39182, "title": "Can not concat RaggedTensor in custom keras layer.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\n2.1.0\r\n- TensorFlow version (use command below):\r\nv2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version:\r\n3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10.1\r\n- GPU model and memory:\r\nGTX2080 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nraise exception\r\n**Describe the expected behavior**\r\nnot raise exception\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nimport tensorflow.keras.layers as layers\r\n\r\nMAX_LEN=20 \r\n\r\nlookuptable = tf.lookup.StaticVocabularyTable(tf.lookup.TextFileInitializer(\"vocab.txt\", tf.string, 0, tf.int64, 1, delimiter=\" \"), num_oov_buckets=1)\r\n\r\ninput_encoding_string = layers.Input(dtype=tf.string,shape=1)\r\n\r\ndef custom_tokenizer(input_tensor_string, width=MAX_LEN):\r\n    ragged_tensor = tf.strings.split(input_tensor_string)\r\n    words_index = tf.ragged.map_flat_values(lookuptable.lookup,ragged_tensor)\r\n    rt = words_index[-width:]  # Truncate rows to have at most `width` items\r\n    pad_row_lengths = width - rt.row_lengths()\r\n    pad_values = tf.zeros([(width * rt.nrows()) - tf.size(rt, tf.int64)], rt.dtype)\r\n    padding = tf.RaggedTensor.from_row_lengths(pad_values, pad_row_lengths)\r\n    return tf.concat([padding, rt], axis=1).to_tensor()\r\n\r\ndef custom_tokenizer_shape(shapes):\r\n    return (shapes[0], MAX_LEN)\r\n\r\n\r\nprocessed_input = layers.Lambda(custom_tokenizer, output_shape=custom_tokenizer_shape)(input_encoding_string)\r\ntm = Model(input_encoding_string, processed_input)\r\n```\r\nthe vocab.txt is just a word index map like :\r\n```\r\na 1\r\nb 2\r\nc 3\r\nd 4\r\n```\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    542     try:\r\n--> 543       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    544     except TypeError:\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in <listcomp>(.0)\r\n    542     try:\r\n--> 543       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    544     except TypeError:\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/compat.py in as_bytes(bytes_or_text, encoding)\r\n     86     raise TypeError('Expected binary or unicode string, got %r' %\r\n---> 87                     (bytes_or_text,))\r\n     88\r\n\r\nTypeError: Expected binary or unicode string, got tf.RaggedTensor(values=Tensor(\"lambda_24_1/zeros:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"lambda_24_1/RaggedFromRowLengths/concat:0\", shape=(None,), dtype=int64))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    411               preferred_dtype=default_dtype,\r\n--> 412               as_ref=input_arg.is_ref)\r\n    413           if input_arg.number_attr and len(\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1381             preferred_dtype=preferred_dtype,\r\n-> 1382             ctx=ctx))\r\n   1383   return ret\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1313     if ret is None:\r\n-> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1315\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    316   _ = as_ref\r\n--> 317   return constant(v, dtype=dtype, name=name)\r\n    318\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    257   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 258                         allow_broadcast=True)\r\n    259\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 296           allow_broadcast=allow_broadcast))\r\n    297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    546                       \"Contents: %s. Consider casting elements to a \"\r\n--> 547                       \"supported type.\" % (type(values), values))\r\n    548     tensor_proto.string_val.extend(str_values)\r\n\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(\"lambda_24_1/zeros:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"lambda_24_1/RaggedFromRowLengths/concat:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in concat(values, axis, name)\r\n   1516       return identity(values[0], name=name)\r\n-> 1517   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n   1518\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py in concat_v2(values, axis, name)\r\n   1125   _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n-> 1126         \"ConcatV2\", values=values, axis=axis, name=name)\r\n   1127   _result = _outputs[:]\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    439             else:\r\n--> 440               raise TypeError(\"%s that don't all match.\" % prefix)\r\n    441           else:\r\n\r\nTypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [<NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-74-900126414798> in <module>\r\n----> 1 l(input_encoding_string)\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    771                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    772                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    775                     # circular dependencies.\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs, mask, training)\r\n    844     with backprop.GradientTape(watch_accessed_variables=True) as tape,\\\r\n    845         variable_scope.variable_creator_scope(_variable_creator):\r\n--> 846       result = self.function(inputs, **kwargs)\r\n    847     self._check_variables(created_variables, tape.watched_variables())\r\n    848     return result\r\n\r\n<ipython-input-63-7e27594b3fb4> in custom_tokenizer(input_tensor_string, width)\r\n     10     tf.print('pad',padding.dtype)\r\n     11     tf.print('pad',rt.dtype)\r\n---> 12     return tf.concat([padding, rt], axis=1).to_tensor()\r\n     13\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n    183       # TypeError, when given unexpected types.  So we need to catch both.\r\n--> 184       result = dispatch(wrapper, *args, **kwargs)\r\n    185       if result is not OpDispatcher.NOT_SUPPORTED:\r\n    186         return result\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in dispatch(op, *args, **kwargs)\r\n     99   \"\"\"\r\n    100   for dispatcher in getattr(op, DISPATCH_ATTR):\r\n--> 101     result = dispatcher.handle(args, kwargs)\r\n    102     if result is not OpDispatcher.NOT_SUPPORTED:\r\n    103       return result\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_dispatch.py in handle(self, args, kwargs)\r\n    251   def handle(self, args, kwargs):\r\n    252     if self.is_supported(args, kwargs):\r\n--> 253       return self._ragged_op(*args, **kwargs)\r\n    254     else:\r\n    255       return self.NOT_SUPPORTED\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_concat_ops.py in concat(values, axis, name)\r\n     68     values = [values]\r\n     69   with ops.name_scope(name, 'RaggedConcat', values):\r\n---> 70     return _ragged_stack_concat_helper(values, axis, stack_values=False)\r\n     71\r\n     72\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_concat_ops.py in _ragged_stack_concat_helper(rt_inputs, axis, stack_values)\r\n    159       ndims = rt.shape.ndims\r\n    160     else:\r\n--> 161       rt.shape.assert_has_rank(ndims)\r\n    162\r\n    163   out_ndims = ndims if (ndims is None or not stack_values) else ndims + 1\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py in assert_has_rank(self, rank)\r\n    988     \"\"\"\r\n    989     if self.rank not in (None, rank):\r\n--> 990       raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\r\n    991\r\n    992   def with_rank(self, rank):\r\n\r\nValueError: Shape (None, None, None) must have rank 2\r\n\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39182\">No</a>\n", "My issue is caused by a shape mismatch. Sorry for the false alarm."]}, {"number": 39181, "title": "Added TANH op and test to micro", "body": "resubmit of https://github.com/tensorflow/tensorflow/pull/39055\r\n\r\n", "comments": []}, {"number": 39180, "title": "TensorFlow build is failing on Bazel CI due to Eigen update", "body": "https://buildkite.com/bazel/tensorflow/builds/4927\r\nThe build is failing on Linux and macOS with\r\n```\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX512.h:432:16: error: could not convert 'Eigen::internal::pfirst<__vector(2) long long int>(_mm_min_epi32(res.Eigen::internal::eigen_packet_wrapper<__vector(2) long long int, 0>::operator __vector(2) long long int&(), _mm_shuffle_epi32(res.Eigen::internal::eigen_packet_wrapper<__vector(2) long long int, 0>::operator __vector(2) long long int&(), ((((0 << 6) | (0 << 4)) | (0 << 2)) | 1))))' from 'Eigen::internal::unpacket_traits<__vector(2) long long int>::type {aka __vector(2) long long int}' to 'Eigen::QInt32'\r\n   return pfirst(\r\n          ~~~~~~^\r\n       _mm_min_epi32(res, _mm_shuffle_epi32(res, _MM_SHUFFLE(0, 0, 0, 1))));\r\n```\r\n\r\nIt's weird we don't see the failure on TF CI.\r\n\r\nA bisect shows 510f0f9a6c79d5e2f29c52911a3819fbdb61a6e0 is the culprit\r\n\r\n/cc @gunan", "comments": ["I am puzzled, as all TF builds look healthy...\r\n@mihaimaruseac have we seen this error?\r\n@rmlarsen any ideas?", "I've seen a related issue where specific integer intrinsics were MIA from the standard header on some versions of GCC. I'll try to work around it in Eigen.", "I haven't seen it all on internal CI.", "@meteorcloudy how do I repro this build failure?", "I can try to update the old FixedPoint code in TF to use the eigen_packet_wrapper, but I'm not sure if that will fix it.", "I was able to repro in TF with -mavx2.", "I have a fix in cl/310035740.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39180\">No</a>\n", " Hi @rmlarsen, could you show the fix solution?"]}, {"number": 39179, "title": "tensorflow/lite/kernels/gather.cc:80 0 <= axis && axis < NumDimensions(input). tflite for android", "body": "I have converted a [mask-rcnn model](https://github.com/matterport/Mask_RCNN/releases/tag/v2.0) to deploy it on android. I was able to load the keras weight, freeze the model and convert it to .tflite model using **tflite 1.13 toco** using [this script](https://gist.github.com/bmabir17/754a6e0450ec4fd5e25e462af949cde6). This model contains ResizeNearestNeighbor, Stack, and TensorFlowShape tf_ops. So i had to use \r\n`converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]`\r\nBecause of this i have already add the tflite build with select-tf-ops like the following\r\n```java\r\ndependencies {\r\n    implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly'){ changing = true }\r\n    implementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'){ changing = true }\r\n    implementation('org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'){ changing = true }\r\n    implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly'){ changing = true }\r\n}\r\n```\r\n\r\n\r\nNow i am trying to use this converted model for inference in android.\r\n```java\r\n    protected Classifier(Activity activity, Device device, int numThreads) throws IOException {\r\n        tfliteModel = FileUtil.loadMappedFile(activity, getModelPath());\r\n        switch (device) {\r\n            case GPU:\r\n                // TODO: Create a GPU delegate instance and add it to the interpreter options\r\n\r\n                break;\r\n            case CPU:\r\n                break;\r\n        }\r\n        tfliteOptions.setNumThreads(numThreads);\r\n        tflite = new Interpreter(tfliteModel, tfliteOptions);\r\n\r\n        // Loads labels out from the label file.\r\n        labels = FileUtil.loadLabels(activity, getLabelPath());\r\n        // Todo change output as multiple output for maskrcnn\r\n        // Reads type and shape of input and output tensors, respectively.\r\n        int imageTensorIndex = 0;\r\n        int[] imageShape = tflite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}\r\n        imageSizeY = imageShape[1];\r\n        imageSizeX = imageShape[2];\r\n        DataType imageDataType = tflite.getInputTensor(imageTensorIndex).dataType();\r\n        int probabilityTensorIndex = 0;\r\n        int[] probabilityShape =\r\n                tflite.getOutputTensor(probabilityTensorIndex).shape(); // {1, NUM_CLASSES}\r\n        DataType probabilityDataType = tflite.getOutputTensor(probabilityTensorIndex).dataType();\r\n        // Creates the input tensor.\r\n        inputImageBuffer = new TensorImage(imageDataType);\r\n\r\n        // Creates the output tensor and its processor.\r\n        outputProbabilityBuffer = TensorBuffer.createFixedSize(probabilityShape, probabilityDataType);\r\n\r\n        // Creates the post processor for the output probability.\r\n        probabilityProcessor = new TensorProcessor.Builder().add(getPostprocessNormalizeOp()).build();\r\n\r\n        LOGGER.d(\"Created a Tensorflow Lite Image Classifier.\");\r\n    }\r\n    public List<Recognition> InferImage(final Bitmap bitmap) {\r\n        int sensorOrientation=0;\r\n        inputImageBuffer = loadImage(bitmap, sensorOrientation);\r\n        tflite.run(inputImageBuffer.getBuffer(), outputProbabilityBuffer.getBuffer().rewind());\r\n\r\n        // Gets the map of label and probability\r\n        Map<String, Float> labeledProbability =\r\n                new TensorLabel(labels, probabilityProcessor.process(outputProbabilityBuffer))\r\n                        .getMapWithFloatValue();\r\n        // Gets top-k results.\r\n        return getTopKProbability(labeledProbability);\r\n    }\r\n```\r\n[Full Source Code](https://gist.github.com/bmabir17/4ba3df476b5e27c38a4979e0e627f275)\r\n### But the line with tflite.run throws the following error\r\n`tensorflow/lite/kernels/gather.cc:80 0 <= axis && axis < NumDimensions(input).`\r\n\r\nDetailed Logcat\r\n```\r\n2020-05-05 16:46:05.199 8663-8755/co.chowagiken.tflite_android E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: co.chowagiken.tflite_android, PID: 8663\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/gather.cc:80 0 <= axis && axis < NumDimensions(input) was not true.\r\n    Node number 222 (GATHER) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:275)\r\n        at co.chowagiken.tflite_android.Classifier.InferImage(Classifier.java:196)\r\n        at co.chowagiken.tflite_android.ClassifierActivity$1.run(ClassifierActivity.java:93)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:201)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n\r\n```\r\n\r\n#### Converted Model\r\n[mask_rcnn_coco_1024_quantize.tflite](https://drive.google.com/file/d/1K12f-NCQy52XmMqvnqMsgEQQA7PTTgRq/view?usp=sharing)\r\n", "comments": ["BTW, i have also tried to run inference on the converted model using the tf_lite python api. But it throws segmentation error on `interpreter.invoke()` and python script crashes. I have tried to find a reason why, most of the issues mention that `tf.lite.OpsSet.SELECT_TF_OPS` is still not supported on python interpreter.\r\n[Detailed Issue discussed on stackoverflow](https://stackoverflow.com/questions/61571375/convert-mask-rcnn-model-for-android-with-tflite)", "@ymodak can you please help to understand why this error is error is being thrown?", "`ResizeNearestNeighbor, Stack, and TensorFlowShape` should all be supported in the latest TF 2.2 release. Can you give that a try? That should avoid the need to use the TF ops.", "@jdduke I have tried TF 2.2 But the problem persists.\r\nIs this because of image size mismatch from camera which takes image of 640x360 ?\r\nWhile my model takes input of 1024? I tried to capture image of 1024*1024. But android does not seems to support that. I am kinda new on android and was trying to run my converted model on a classification example.\r\n```2020-06-03 21:58:37.991 13579-13579/co.chowagiken.tflite_android I/tensorflow: CameraConnectionFragment: Desired size: 256x256, min size: 320x320\r\n2020-06-03 21:58:37.991 13579-13579/co.chowagiken.tflite_android I/tensorflow: CameraConnectionFragment: Valid preview sizes: [4032x3024, 4000x3000, 4032x2268, 4032x2016, 3840x2160, 2880x2156, 2688x1512, 2592x1940, 2592x1458, 2592x1296, 1920x1440, 1920x1080, 1600x1200, 1280x960, 1280x720, 1280x640, 800x600, 720x480, 640x480, 640x360]\r\n2020-06-03 21:58:37.991 13579-13579/co.chowagiken.tflite_android I/tensorflow: CameraConnectionFragment: Rejected preview sizes: [352x288, 320x240, 176x144]\r\n2020-06-03 21:58:37.991 13579-13579/co.chowagiken.tflite_android I/tensorflow: CameraConnectionFragment: Chosen size: 640x360\r\n```\r\n\r\n[CameraActivity.java](https://gist.github.com/bmabir17/4ba3df476b5e27c38a4979e0e627f275#file-cameraactivity-java)\r\n", "I was going through the converted model and found that tflite converted model has some difference compared with the original model.pb(saved before conversion to tflite).\r\nIt seems the range op conversion has some problem. And  this seems to be the input for the expand dims.\r\n_Tensorflow Model(original)_ [Model Link](https://drive.google.com/file/d/1DLzCOb9p8c_s86qNW_7QIsyikfRjkhK9/view?usp=sharing)\r\n![tensorflow_range](https://user-images.githubusercontent.com/5344072/85119479-a3454400-b243-11ea-946f-ad6ece115221.png)\r\n\r\n\r\n_Tflite Model(converted)_ [Model Link](https://drive.google.com/file/d/1Naomfft5yA7nbto_8aD72S2bC-GCW6rM/view?usp=sharing)\r\n![tflite_range](https://user-images.githubusercontent.com/5344072/85119483-a50f0780-b243-11ea-835d-9fa2c77deba9.png)\r\n\r\nCan anyone help why is this happening? ", "@jdduke \r\n> `ResizeNearestNeighbor, Stack, and TensorFlowShape` should all be supported in the latest TF 2.2 release. Can you give that a try? That should avoid the need to use the TF ops.\r\n\r\nThis is [Notebook](https://colab.research.google.com/drive/1JYDYX6X8pSYjsWcTQt87l2Xt0Lzj7JRW?usp=sharing) contains all the different ways i tried to convert the model using tensorflow 2.2 . Only one of the conversion process is successful, and it still requires `tf_ops` to convert successfully. \r\n", "@bmabir17 It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!", "Closing this bug due to staleness. There was some discussion on [this issue](https://github.com/tensorflow/tensorflow/issues/40297#issuecomment-648988845) about converting the matterport model to TFLite, and looks like the user succeeded. @bmabir17 If this issue still persists (sorry about tghe extremely delayed response), then feel free to reopen. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39179\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39179\">No</a>\n"]}, {"number": 39178, "title": "Something wrong with \"predict_on_batch\"", "body": "<em>\r\nPerformance with predict_on_batch when using not determined input size is not good enough on tf.2.2.0. In my experiments with TensorFlow version 2.1 the prediction speed is for flexible input the same as fixed input, but for tf.2.2 prediction, speed is dramatically lower. Moreover, about 10 days ago, when I used nightly build the performance was good.\r\n</em>\r\n\r\n**System information**\r\n- OS Platform: Linux Ubuntu 20.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.2 nightly. v1.12.1-31119-gce72f093cb 2.2.0-dev20200504\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: GTX1080 TI, 11Gb\r\n\r\n**Describe the current behavior**\r\n**for v2.1.0-rc2-17-ge5bf8de 2.1.0**\r\n\r\n- **_Fixed input_**\r\n\r\n```\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\n\r\nnet = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)\r\nfor n in tqdm(range(1000)):\r\n    a = net.predict_on_batch(np.random.randn(1, 224, 224, 3))\r\n\r\noutput:\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:13<00:00, 74.74it/s]\r\n```\r\n\r\n- **_Flexible input_**\r\n\r\n```\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nnet = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)\r\nfor n in tqdm(range(1000)):\r\n    a = net.predict_on_batch(np.random.randn(1, np.random.randint(112, 224), 224, 3))\r\n    \r\noutput:\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:14<00:00, 68.15it/s] \r\n```\r\n\r\n**for tf.2.2 nightly build (v1.12.1-31119-gce72f093cb 2.2.0-dev20200504)**\r\n\r\n- **_Fixed input_**\r\n\r\n```\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nnet = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)\r\nfor n in tqdm(range(1000)):\r\n    a = net.predict_on_batch(np.random.randn(1, 224, 224, 3))\r\n    \r\noutput:\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:26<00:00, 38.24it/s]\r\n```\r\n\r\n- **_Flexible input_**\r\n\r\n```\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nnet = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)\r\nfor n in tqdm(range(1000)):\r\n    a = net.predict_on_batch(np.random.randn(1, np.random.randint(112, 224), 224, 3))\r\n\r\noutput:\r\n  1%|          | 6/1000 [00:47<2:07:54,  7.72s/it]\r\n```\r\n\r\nupd:\r\nfor current stable version: v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2 \r\nresult is same:\r\n\r\n- **_Flexible input_**\r\n\r\n```\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nnet = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)\r\nfor n in tqdm(range(1000)):\r\n    a = net.predict_on_batch(np.random.randn(1, np.random.randint(112, 224), 224, 3))\r\n\r\noutput:\r\n  0%|          | 4/1000 [00:31<2:12:39,  7.99s/it]\r\n```", "comments": ["@17p9h9\r\nI ran the code shared, please find the [gist for 2.1](https://colab.sandbox.google.com/gist/Saduf2019/340f4bbe70afbcaf60f6969fcf452a3b/2.ipynb) and for [nightly](https://colab.sandbox.google.com/gist/Saduf2019/e98c3ee313bb1429a5ccd5ec9aa0023d/untitled163.ipynb).\r\nPlease confirm if this replicates your issue.", "> @\r\n> I ran the code shared, please find the [gist for 2.1](https://colab.sandbox.google.com/gist/Saduf2019/340f4bbe70afbcaf60f6969fcf452a3b/2.ipynb) and for [nightly](https://colab.sandbox.google.com/gist/Saduf2019/e98c3ee313bb1429a5ccd5ec9aa0023d/untitled163.ipynb).\r\n> Please confirm if this replicates your issue.\r\n\r\nYes, I have similar behavior.", "@Saduf2019 FYI the same problem with \"predict_generator\" method. ", "Thanks for the issue. This is fixed with tf-nightly '2.3.0-dev20200601' version. \r\nSee attached [gist](https://colab.research.google.com/gist/ymodak/838fdc95c837eb247a421fc1781ec5bb/untitled163.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}]