[{"number": 27525, "title": "tf.function-decorated function ", "body": "**System information**\r\n- I have written several custom layers for this code. However, they are tested and are not the cause of this bug.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDebian Stable (up to date)\r\n- TensorFlow installed from (source or binary): downloaded the latest alpha of tf2 through pip\r\n- TensorFlow version (use command below): That command does not work for tf2\r\n- Python version: 3.6.8\r\n\r\n\r\n\r\nI have an error in my code (located : [here](https://github.com/alexsludds/6888_final_project)). If you look at the main.py file you see that I am trying to change a variable and train a model repeatedly, extracting the testing accuracy each time. If you run this file for a minute, you see that on the second step of this first for loop I get an error:\r\n\r\nValueError: tf.function-decorated function tried to create variables on non-first call.\r\n\r\nTo me this means that train is trying to create a new tf variable, but I don't understand why, the model should be different and the everything is getting called again. \r\n\r\nIs there anything I can do to get around this by flushing the graph, or is this a bug in the alpha version of tf2?\r\n\r\nThank you", "comments": ["I did some debugging. The error occurs at \r\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\r\nin my train.py file.\r\nI don't know why this is occuring. I don't think this function should try to create any new variables.", "Are you using a different optimizer on subsequent calls to the tf.function than the one you used on the first call? TF optimizers can create variables.", "Hi alextp,\r\nIn the code I posted every time I generate a new model I generate a new instance of the Adam optimizer. I changed my code and moved the generation of the optimizer out of the functions and just have it get called once. However, I still get the same error as before.\r\nAny ideas?", "tf.function was designed so you need to make a new tf.function object every\ntime you generate a new model\n\nOn Mon, Apr 8, 2019 at 3:00 PM Alex Sludds <notifications@github.com> wrote:\n\n> Hi alextp,\n> In the code I posted every time I generate a new model I generate a new\n> instance of the Adam optimizer. I changed my code and moved the generation\n> of the optimizer out of the functions and just have it get called once.\n> However, I still get the same error as before.\n> Any ideas?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27525#issuecomment-481023661>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcBmtmr1XUccnAoLKBPxB7wBCdeBks5ve7wXgaJpZM4cd7JA>\n> .\n>\n\n\n-- \n - Alex\n", "Oh I see. If I have a function, such as train in my code, how do I create many a new train object on each iteration of a for-loop?", "If f is a python function, tf_f = tf.function(f) will create a new tf\nfunction from f.\n\nOn Mon, Apr 8, 2019 at 3:21 PM Alex Sludds <notifications@github.com> wrote:\n\n> Oh I see. If I have a function, such as train in my code, how do I create\n> many a new train object on each iteration of a for-loop?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27525#issuecomment-481028913>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYBoX6dUeBsOc0SFZWfvhNVBLZp8ks5ve8DbgaJpZM4cd7JA>\n> .\n>\n\n\n-- \n - Alex\n", "Oh awesome! That worked!\r\nThanks for the help.\r\nA problem I will probably have soon I might as well ask, is there a way for me to clear the graph in tf2 to reduce the RAM used?", "Currently it's not easy, we're working on doing this right.\n\nOn Mon, Apr 8, 2019 at 3:45 PM Alex Sludds <notifications@github.com> wrote:\n\n> Oh awesome! That worked!\n> Thanks for the help.\n> A problem I will probably have soon I might as well ask, is there a way\n> for me to clear the graph in tf2 to reduce the RAM used?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27525#issuecomment-481034350>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxT4INQG54ddlhMS7FWCgRSI0NoTuks5ve8Z5gaJpZM4cd7JA>\n> .\n>\n\n\n-- \n - Alex\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27525\">No</a>\n", "Is it the reason that tf 2.0 GPU utilization is lower than tf 1.* ?? since we cannot use apply_gradients with tf.function decorator ?", "@Chunpai GPU utilization should be the same for equivalent code, and apply_gradients should work with tf.function.\r\n\r\nPlease open a separate issue if you're having problems.", "getting this issue in 1.14 using skopt to do hyperparameter optimization, the graph runs great but hangs during later searches\r\n"]}, {"number": 27524, "title": "Clean-up unused functors", "body": "`ShuffleAndReverse`, `InflatePadAndShuffle` and `TransformDepth` were included in the very first public commit of tensorflow.\r\nThey are used at the beginning, but as far as I can see they are not used anywhere in the codebase for a long time.\r\nIs there any reason keeping them around?", "comments": ["@smit-hinsu can i merge this PR internally ?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "I'm interested in pursuing this but the mentioned build failure with clang cannot be reproduced by me.", "Rebased.", "Can one of the admins verify this patch?", "@smit-hinsu I did a rebase. Will you have a chance to review it again?"]}, {"number": 27523, "title": "`tf.train.NanTensorHook` does not prevent `tf.train.NanLossDuringTrainingError` when using custom Estimator.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): BINARY\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to use `tf.train.NanTensorHook` to prevent my training script from halting with an Error. (Specifically this is to do Hyperparameter optimization on cloudml, but I observe this bug on my local machine as well). My instantiation looks like:\r\n\r\n```\r\ndef model_fn():\r\n  loss = ...\r\n  train_op = ...\r\n\r\n  training_hooks=[]\r\n  # Report training failed if loss becomes `Nan`.\r\n  training_hooks.append(tf.train.NanTensorHook(loss, fail_on_nan_loss=False))\r\n\r\n  return tf.estimator.EstimatorSpec(\r\n    mode=mode,\r\n    loss=loss,\r\n    train_op=train_op,\r\n    predictions=predict_output,\r\n    eval_metric_ops=eval_metric_ops,\r\n    training_hooks=training_hooks,\r\n  )\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect that this should log a warning and halt training.\r\n\r\n**Code to reproduce the issue**\r\nWill update when I have written a minimal example.\r\n\r\n**Other info / logs**\r\nError thrown:\r\n\r\n> ERROR:tensorflow:Model diverged with loss = NaN.\r\n> Traceback (most recent call last):\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n>     \"__main__\", mod_spec)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/runpy.py\", line 85, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train_basic_model.py\", line 140, in <module>\r\n>     main()\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train_basic_model.py\", line 132, in main\r\n>     eval_parse_fns=parse_fns,\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train.py\", line 166, in run_train_and_evaluate\r\n>     log_step_count=args.log_step_count,\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train.py\", line 81, in train_and_evaluate\r\n>     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 471, in train_and_evaluate\r\n>     return executor.run()\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 610, in run\r\n>     return self.run_local()\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 711, in run_local\r\n>     saving_listeners=saving_listeners)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n>     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n>     return self._train_model_default(input_fn, hooks, saving_listeners)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1241, in _train_model_default\r\n>     saving_listeners)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1471, in _train_with_estimator_spec\r\n>     _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n>     run_metadata=run_metadata)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1156, in run\r\n>     run_metadata=run_metadata)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n>     raise six.reraise(*original_exc_info)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n>     raise value\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1240, in run\r\n>     return self._sess.run(*args, **kwargs)\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1320, in run\r\n>     run_metadata=run_metadata))\r\n>   File \"/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 753, in after_run\r\n>     raise NanLossDuringTrainingError", "comments": ["Thank you for reaching out to us. Can you please provide minimum reproducible code snippet to help us proceed further and verify what is going wrong", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27522, "title": "[Intel MKL] Graph Transform tool fold_old_batchnorms biasadd node bug", "body": "tensorflow/tensorflow/tools/graph_transforms/fold_old_batch_norms.cc has a bug.\r\n\r\nBiasadd node copy needs to copy the attribute dataformat. Else there will be a load bug after batchnorm folding.. i.e. The data_format attribute of NCHW is not copied from conv2d node to Biasadd node without which the shape for convolution with the mismatched order is improperly checked.", "comments": ["If you can think of a test to add, please add it."]}, {"number": 27521, "title": "Bug in tf.parallel_stack", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master branch from 9e0b9b9ff2b01e07c2a71ce0fbbae93a4bba86f9 and onward\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.23.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\ntf.parallel_stack is producing incorrect output.\r\n**Describe the expected behavior**\r\ntf.parallel_stack has to have same output as tf.stack or tf.concat\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThe below reproducer code/test passes with any commit that precedes 9e0b9b9ff2b01e07c2a71ce0fbbae93a4bba86f9\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  images = np.random.random_sample((100, 24, 24, 3))\r\n  parallel_stacked_images = tf.parallel_stack(images)\r\n  stacked_images = tf.stack(images)\r\n  concatenated_images = tf.concat(images, 0)\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n  np_parallel_stacked_images, np_stacked_images, np_concatenated_images = sess.run(\r\n    [parallel_stacked_images, stacked_images, concatenated_images])\r\n\r\nif np.array_equal(np_parallel_stacked_images, np_stacked_images):\r\n  print (\"Pass: outputs of tf.parallel_stack and tf.stack are equal\")\r\nelse:\r\n  print (\"Fail: outputs of tf.parallel_stack and tf.stack are not equal\")\r\n\r\nif np.array_equal(np_parallel_stacked_images, np_concatenated_images):\r\n  print (\"Pass: outputs of tf.parallel_stack and tf.concat are equal\")\r\nelse:\r\n  print (\"Fail: outputs of tf.parallel_stack and tf.concat are not equal\")\r\n```\r\n\r\n**Other info / logs**\r\nWe have debugged this issue and found that this commit 9e0b9b9ff2b01e07c2a71ce0fbbae93a4bba86f9 actually causes the bug. That commit changes the set of `Edge`s in the graph from std::set (ordered) to gtl::FlatSet (unordered). TF has a graph optimizer pass for parallel_stack (or parallel_concat) that replaces ParallelConcat node with several _ParallelConcatUpdate nodes. But it seems that it assumes all in_edges are ordered which is not the case with gtl::FlatSet. Therefore, there is a problem in updating the output buffer that is using the incremental index (the `loc` attribute) which does not correspond to the right input_edge. In particular, I am talking about this code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/parallel_concat_optimizer.cc#L83\r\n\r\n", "comments": ["Thanks you so much for root causing this! We should probably move that graph rewrite to Grappler.", "@rmlarsen We have a simple fix for this until it is moved to Grappler. Will submit a PR for that. It will be great if you can take a look at it. ", "Sounds great!", "@rmlarsen Here is the PR #27554 . Thanks!", "This was resolved already with the PR https://github.com/tensorflow/tensorflow/pull/27554 as it was already merged. \r\n\r\n```\r\n# Output\r\nPass: outputs of tf.parallel_stack and tf.stack are equal\r\nPass: outputs of tf.parallel_stack and tf.concat are equal\r\n```\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/5caa1efff76de239a36fa68ced8a10d0/untitled.ipynb) is a gist for reference. Thanks!\r\n\r\nClosing this issue as this was already resolved."]}, {"number": 27520, "title": "segmentation fault using tensorflow debugger", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nWhen i type run in tfdbg i get a segmentation fault.\r\n\r\n**System information**\r\n- Have I written custom code?\r\nyes i have.\r\n- OS Platform and Distribution:\r\nkali linux rolling 2019 (see tf_env)\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\nnot compiled from source\r\n- CUDA/cuDNN version:\r\ni don't know what CUDA is but i don't have Nvidia GPU i think but radeon amd?\r\ni think tensorflow runs on my cpu?\r\n- GPU model and memory:\r\n lspci -vnn | grep VGA -A 12\r\n01:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Thames [Radeon HD 7500M/7600M Series] [1002:6840] (prog-if 00 [VGA controller])\r\n\tSubsystem: Toshiba America Info Systems Radeon HD 7670M [1179:fb22]\r\n\tFlags: bus master, fast devsel, latency 0, IRQ 29\r\n\tMemory at b0000000 (64-bit, prefetchable) [size=256M]\r\n\tMemory at c0000000 (64-bit, non-prefetchable) [size=128K]\r\n\tI/O ports at 3000 [size=256]\r\n\tExpansion ROM at 000c0000 [disabled] [size=128K]\r\n\tCapabilities: [50] Power Management version 3\r\n\tCapabilities: [58] Express Legacy Endpoint, MSI 00\r\n\tCapabilities: [a0] MSI: Enable+ Count=1/1 Maskable- 64bit+\r\n\tCapabilities: [100] Vendor Specific Information: ID=0001 Rev=1 Len=010 <?>\r\n\tCapabilities: [150] Advanced Error Reporting\r\n\tKernel driver in use: radeon\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\noutput of tf_env_collect.sh:\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3045026/tf_env.txt)\r\n\r\n\r\nYou can also obtain the TensorFlow version \r\nwith\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\ni installed tensorflow in pycharm using pip (my projects uses the system interpreter)\r\ntf.VERSION:\r\n1.13.1\r\ntf.GIT_VERSION:\r\nb'v1.13.0-rc2-5-g6612da8'\r\n\r\n**Describe the current behavior**\r\nI first go to a terminal and type the following:\r\ntensorboard --logdir=/root/files/Tensorboard/logs/ --debugger_port=6064\r\nthen in another terminal i type:\r\npython3 NeuralNetwork.py --debug\r\ni then see the run-start CLI like this.\r\n![Schermafdruk van 2019-04-04 20-48-05](https://user-images.githubusercontent.com/41961612/55580465-e5f02e80-571a-11e9-9d56-e0ebe3fea6b5.png)\r\n\r\nthen i type run and i get an segmentation fault.\r\n![Schermafdruk van 2019-04-04 20-50-34](https://user-images.githubusercontent.com/41961612/55580547-13d57300-571b-11e9-9b43-4dc0c2733fc5.png)\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nthe expected behavior i think is that the run command would not crash the debugger.\r\n\r\n**Code to reproduce the issue**\r\n[Example.txt](https://github.com/tensorflow/tensorflow/files/3044988/Example.txt)\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@hugo-boser This is a stale issue. Just checking whether it is still an issue for you? Can you please check recent TF/TB versions and let us know whether the issue was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27520\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27520\">No</a>\n"]}, {"number": 27519, "title": "TF 2.0 'Tensor' object has no attribute 'numpy' while using .numpy() although eager execution enabled by default", "body": "Although Eager_execution is enabled by default in TF 2.0, I am getting errors while using .numpy()\r\n\r\nPlease note that i am not using the code in compatibility mode to TF 1.0.\r\n\r\n  expt = [[[  0,   0,   0],\r\n            [  4,  71, 141],\r\n            [  0,   0,  0]],\r\n\r\n           [[ 83,  25,  85],\r\n            [ 90, 190, 143],\r\n            [  4, 141,  49]],\r\n\r\n           [[  0,   0,   0],\r\n            [  4,  71,  49],\r\n            [  0,   0,   0]]]\r\nexpt = tf.convert_to_tensor(expt)\r\n\r\nexpected_values = expt.numpy()\r\n\r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n\r\n\r\nCPU TEST VERSION OF TENSORFLOW 2.0. ", "comments": ["@Mainak431  Did you solved your problem?", "Yes.  ", "what was your solution?", ".Numpy is only supported in eager mode.  If you are in graph mode,  it will not be supported.  To check,  if you are in eager mode.  Do,  tf.eagerly().  It returns true or false.  In graph mode,  you have to use eval in a session to get the value of the tensor in numpy array.  ", "@Mainak431 \r\nmodule 'tensorflow' has no attribute 'eagerly'\r\nmodule 'tensorflow.compat.v1' has no attribute 'eagerly'", "I'm using the package tensorflow-gpu==2.0.0-alpha0\r\n.numpy() on one of my tensors yield 'Tensor' object has no attribute 'numpy'\r\n\r\ntf.eagerly() gives module 'tensorflow' has no attribute 'eagerly'\r\n", "tf.executing_eagerly() instead of tf.eagerly() worked for me to check if I'm in eager mode\r\n", "Has anyone found a solution for this?", "same issue here ...  i invoke a keras model in eager mode and i get a Tensor, not an EagerTensor, which causes issues with OpenAI Gym", "Run this tf.enable_eager_execution() and then when you try tf.executing_eagerly() it should give True. After this you can use something.numpy() to view the values.\r\n", "@AkashNagaraj I just filed an issue where the code is executing eagerly (and should, since it's TF 2.0), but I'm having a problem of \"missing numpy\". Would you care to take a look? https://github.com/tensorflow/tensorflow/issues/32842\r\n\r\nThanks!", "I had the same issue. Turned out that I was trying to use .numpy() inside a @tf.function. As far as I understand tf.function is not executed eagerly for performance purposes. If I remove the @tf.function decorator .numpy() works.", "> I had the same issue. Turned out that I was trying to use .numpy() inside a @tf.function. As far as I understand tf.function is not executed eagerly for performance purposes. If I remove the @tf.function decorator .numpy() works.\r\n\r\nThis works for me, @tf.function turned the whole function into graph mode", "Why isn't there a single solution mentioned in this thread =.=", "My function does not have a decorator but .numpy() still fails as described by previous posters. Has anyone found an solution to this?", "I've found that my issue clears up after inserting tf.compat.v1.enable_eager_execution() at the top of my script (very similar to what previous posters have said, but this works for TF 2.0)... ", "I'm having the same issue and none of the aforementioned solutions worked for me. \r\n\r\nI'm using TF 2.0, my function does not have a decorator, tf.eagerly() returns True and I still get the same AttributeError: 'Tensor' object has no attribute 'numpy'.", "Same problem. \r\n\r\nI made sure I was executing eagerly and do not have a decorator on my custom loss function. I also tried Michael's solution two comment up which didn't work. \r\n\r\nI get the error: AttributeError: 'Tensor' object has no attribute 'numpy'", "I noticed that this error only appears when I try to convert tensors to numpy during a model fit. My best guess is that it seems to be shape issue.\r\n\r\nFor example, the following tensor\r\n\r\n<class 'tensorflow.python.framework.ops.EagerTensor'>\r\n\r\ntf.Tensor([[1 3]  [0 4]], shape=(2, 2), dtype=int64)\r\n\r\nis convertible using .numpy(). However, when trying to implement a custom metric for a classification problem, both y_true.numpy() and y_pred.numpy() raise\r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'.\r\n\r\nHere is one example of both y's:\r\n\r\ny_true:\r\nprint(y_true): Tensor(\"dense_target:0\", shape=(None, None, None), dtype=float32)\r\nprint(type(y_true)): <class 'tensorflow.python.framework.ops.Tensor'>\r\n\r\ny_pred:\r\nprint(y_pred): Tensor(\"dense/Identity:0\", shape=(None, None, 6), dtype=float32)\r\nprint(type(pred)): <class 'tensorflow.python.framework.ops.Tensor'>", "@renatomello I am having the same problem as you are. I have opened a new issue: #35393.", "@renatomello the problem still persists when trying to implement a custom metric. Did you find a workaround?", "> \r\n> \r\n> @renatomello the problem still persists when trying to implement a custom metric. Did you find a workaround?\r\n\r\nNo, I did not. I'm trying to see if there's a TF/Keras backend function that does something similar that I can work with. Otherwise, I'll just have to create one myself.", "I encountered this area when using the regex functions within data preprocessing. Using python logic requires the use of `tf.py_function` as mentioned in [docs](https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic) and this [StackOverflow thread](https://stackoverflow.com/questions/56122670/how-to-get-string-value-out-of-tf-tensor-which-dtype-is-string) (Note `tf.py_func()` is now `tf.py_function()`) \r\n\r\nOnce I changed my code from \r\n`data = fnames.map(process_path)`\r\nto\r\n`data = fnames.map(lambda x: tf.py_function(process_path, [x], [tf.string]))`\r\nthe code executed correctly.", "I ran into this issue while trying out the `TextVectorization` layer in TF2.1 with a very custom `split`. What fixed it for me was to pass in `dynamic=True` to the `TextVectorization `construction.", "\r\n\r\n\r\n\r\n> I noticed that this error only appears when I try to convert tensors to numpy during a model fit. My best guess is that it seems to be shape issue.\r\n> \r\n> For example, the following tensor\r\n> \r\n> <class 'tensorflow.python.framework.ops.EagerTensor'>\r\n> \r\n> tf.Tensor([[1 3] [0 4]], shape=(2, 2), dtype=int64)\r\n> \r\n> is convertible using .numpy(). However, when trying to implement a custom metric for a classification problem, both y_true.numpy() and y_pred.numpy() raise\r\n> \r\n> AttributeError: 'Tensor' object has no attribute 'numpy'.\r\n> \r\n> Here is one example of both y's:\r\n> \r\n> y_true:\r\n> print(y_true): Tensor(\"dense_target:0\", shape=(None, None, None), dtype=float32)\r\n> print(type(y_true)): <class 'tensorflow.python.framework.ops.Tensor'>\r\n> \r\n> y_pred:\r\n> print(y_pred): Tensor(\"dense/Identity:0\", shape=(None, None, 6), dtype=float32)\r\n> print(type(pred)): <class 'tensorflow.python.framework.ops.Tensor'>\r\n\r\nDon't know the reason, but I solved such  issue  by adding \r\n`experimental_run_tf_function=False` \r\nin compile function of my model. ", "> > I noticed that this error only appears when I try to convert tensors to numpy during a model fit. My best guess is that it seems to be shape issue.\r\n> > For example, the following tensor\r\n> > <class 'tensorflow.python.framework.ops.EagerTensor'>\r\n> > tf.Tensor([[1 3] [0 4]], shape=(2, 2), dtype=int64)\r\n> > is convertible using .numpy(). However, when trying to implement a custom metric for a classification problem, both y_true.numpy() and y_pred.numpy() raise\r\n> > AttributeError: 'Tensor' object has no attribute 'numpy'.\r\n> > Here is one example of both y's:\r\n> > y_true:\r\n> > print(y_true): Tensor(\"dense_target:0\", shape=(None, None, None), dtype=float32)\r\n> > print(type(y_true)): <class 'tensorflow.python.framework.ops.Tensor'>\r\n> > y_pred:\r\n> > print(y_pred): Tensor(\"dense/Identity:0\", shape=(None, None, 6), dtype=float32)\r\n> > print(type(pred)): <class 'tensorflow.python.framework.ops.Tensor'>\r\n> \r\n> Don't know the reason, but I solved such issue by adding\r\n> `experimental_run_tf_function=False`\r\n> in compile function of my model.\r\n\r\nIt does not seem to make a difference in my case", "> I ran into this issue while trying out the `TextVectorization` layer in TF2.1 with a very custom `split`. What fixed it for me was to pass in `dynamic=True` to the `TextVectorization `construction.\r\n\r\nHow did you do that exactly?", "first, you should register Session as follow:\r\nsess=tf.Session()\r\nthen..use :\r\nexpected_values = expt.eval(session=sess)\r\nthe result should be:\r\nprint(expected_values)\r\nOut[41]: \r\narray([[[  0,   0,   0],\r\n        [  4,  71, 141],\r\n        [  0,   0,   0]]])\r\nthank", "> > I ran into this issue while trying out the `TextVectorization` layer in TF2.1 with a very custom `split`. What fixed it for me was to pass in `dynamic=True` to the `TextVectorization `construction.\r\n> \r\n> How did you do that exactly?\r\n\r\nLike this. I'm still learning ML/TF in general, so this may have been the wrong thing to do. It's all black magic until it 'just works'.\r\n```\r\nvectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\r\n    standardize=tf_custom_standardize,\r\n    split=tf_custom_split,\r\n    max_tokens=len(vocab)+1,\r\n    output_mode='int',\r\n    dynamic=True\r\n)\r\n```\r\n\r\n", "This problem still persists for me. I am not using a the @tf.function annotation and executing eagerly. The workaround I am using now is np.array(yourtensor.to_list()) ", "> This problem still persists for me. I am not using a the @tf.function annotation and executing eagerly. The workaround I am using now is np.array(yourtensor.to_list())\r\n\r\nThis is what I get\r\n `AttributeError: 'Tensor' object has no attribute 'to_list'`\r\n", "According to the codes in tensorflow, when the tensor object is in eager mode tensor(tensorflow.python.framework.ops.EagerTensor), we could call tesnsorObj.numpy()\r\nthis method is implemented in tensorflow/python/framework/ops.py\r\nBut if the tensor object is a general tensor, e.g. belonging to tensorflow.python.framework.ops.Tensor, then it'll have no numpy() method, the implementation codes is also in tensorflow/python/framework/ops.py\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py\r\n\r\nIt means all tensor object running in @tf.function couldn't call tensorObj.numpy()\r\nThe eager running mode adds the flexibility to TF2.0 and also adds complexity to it.", "> According to the codes in tensorflow, when the tensor object is in eager mode tensor(tensorflow.python.framework.ops.EagerTensor), we could call tesnsorObj.numpy()\r\n> this method is implemented in tensorflow/python/framework/ops.py\r\n> But if the tensor object is a general tensor, e.g. belonging to tensorflow.python.framework.ops.Tensor, then it'll have no numpy() method, the implementation codes is also in tensorflow/python/framework/ops.py\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py\r\n> \r\n> It means all tensor object running in @tf.function couldn't call tensorObj.numpy()\r\n> The eager running mode adds the flexibility to TF2.0 and also adds complexity to it.\r\n\r\nThanks for the detailed explanation.\r\n\r\nHowever, this issue is not solved in the stable 2.1.0 version. It was pointed out to me in the #38038 that it is solved in the tf-nightly 2.2.0 version.", "@renatomello I'm a complete newbie on TF. So, I'm not sure if the following will help to solve your issue.\r\nI solve the error `AttributeError: 'Tensor' object has no attribute 'numpy'`, by use of [experimental_run_functions_eagerly(True)](https://www.tensorflow.org/api_docs/python/tf/config/experimental_run_functions_eagerly).\r\nI use TF ver. 2.1 (For the detailed version information see the below console output). I allow to explain what I'm doing with this code:\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\ncounter=0\r\n\r\nclass MyDense(layers.Dense):\r\n  def __init__(self, units, activation,input_shape):\r\n    super().__init__(units=units, activation=activation,input_shape=input_shape)\r\n\r\n  def call(self, inputs):\r\n      global counter\r\n      \r\n      counter += 1\r\n      print('\\n{}. inputs.numpy()='.format(counter))\r\n      if hasattr(inputs,'numpy'):\r\n          print('{}'.format(inputs.numpy()))\r\n      else:\r\n          print('not available.')\r\n          \r\n      return super().call(inputs)\r\n\r\ndef test(run_eagerly):\r\n    print('\\n*** *** *** test(run_eagerly={})'.format(run_eagerly))\r\n    tf.config.experimental_run_functions_eagerly(run_eagerly)\r\n    \r\n    dim0=256\r\n    dim1=24\r\n    train = np.arange(dim0*dim1).reshape(dim0,dim1)\r\n    label = np.ones(dim0)\r\n    \r\n    model = tf.keras.Sequential()\r\n    model.add(MyDense(10,activation='softmax',input_shape=(dim1,)))\r\n    \r\n    model.compile(optimizer=tf.keras.optimizers.SGD(),\r\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy())\r\n    \r\n    model.fit(train,\r\n              label,\r\n              batch_size=dim0,\r\n              epochs=1)\r\n\r\nprint(\"Python version\")\r\nprint (sys.version)\r\nprint(\"TensorFlow version\")\r\nprint(tf.__version__)\r\nprint('\\n\\n')\r\n\r\ntest(False)\r\ntest(True)\r\n```\r\n\r\nI execute the above code in the editor Spyder\u2019s console (Spyder Version 4.0.1). The corresponding **console output** is (just parts of the output):\r\n\r\n```\r\nPython version\r\n3.7.6 (default, Jan  8 2020, 19:59:22) \r\n[GCC 7.3.0]\r\nTensorFlow version\r\n2.1.0\r\n\r\n*** *** *** test(run_eagerly=False)\r\n\r\n1. inputs.numpy()=\r\nnot available.\r\nTrain on 256 samples\r\n\r\n2. inputs.numpy()=\r\nnot available.\r\n\r\n3. inputs.numpy()=\r\nnot available.\r\n256/256 [==============================] - 0s 899us/sample - loss: 6344.8682\r\n\r\n*** *** *** test(run_eagerly=True)\r\n\r\n4. inputs.numpy()=\r\nnot available.\r\nTrain on 256 samples\r\n\r\n5. inputs.numpy()=\r\n[[2328. 2329. 2330. ... 2349. 2350. 2351.]\r\n [5280. 5281. 5282. ... 5301. 5302. 5303.]\r\n [2208. 2209. 2210. ... 2229. 2230. 2231.]\r\n ...\r\n [5160. 5161. 5162. ... 5181. 5182. 5183.]\r\n [ 840.  841.  842. ...  861.  862.  863.]\r\n [6048. 6049. 6050. ... 6069. 6070. 6071.]]\r\n256/256 [==============================] - 0s 132us/sample - loss: 16.1181\r\n```\r\nNow, see in the console output below `test(run_eagerly=False)`: All three MyDense.call() calls state, that input.numpy() is not available (Side note: When I understand it correctly, the first two MyDense.call() calls are just for building the model. So, it\u2019s meaningful, that they don\u2019t have input.numpy() available.). So, this is meaningful.\r\nSee in the console output below `test(run_eagerly=True)`: In the fifth MyDense.call() is input.numpy() available. So, this is also meaningful. And it solves the error.", "> I had the same issue. Turned out that I was trying to use .numpy() inside a @tf.function. As far as I understand tf.function is not executed eagerly for performance purposes. If I remove the @tf.function decorator .numpy() works.\r\n\r\nIt works for me. ", "I've tried all solutions mentioned in this thread, and none of them worked for me. I'm on 2.2.0-rc2.\r\n\r\nIt doesn't seem to be an issue with eager execution in my case. The error comes from these lines when I called `keras.backend.get_value()`:\r\n```\r\n  if context.executing_eagerly() or isinstance(x, ops.EagerTensor):\r\n    return x.numpy()\r\n```", "Had the same issue. It turns out that by default, eagerly execution is disabled when running the fit function on a model. To counter this, you can run : `model.run_eagerly = True` before running model.fit.\r\nThis worked like a charm for me on Tensorflow 2.2", "model.compile(..., run_eagerly=True) worked for me when creating a custom metric", "https://stackoverflow.com/questions/63062304/extracting-numpy-value-from-tensorflow-object-during-transformation/63064174?noredirect=1#comment111652837_63064174", "I had similar problem while working with Keras. \r\nI don't know if this will help other , but following helped me:\r\nI was using:\r\nmodel = Sequential() before and changed that to \r\nmodel = tf.keras.Sequential() \r\nAnd it worked for me. ", "@Mainak431 would you mind reopening the issue?", "> I had similar problem while working with Keras.\r\n> I don't know if this will help other , but following helped me:\r\n> I was using:\r\n> model = Sequential() before and changed that to\r\n> model = tf.keras.Sequential()\r\n> And it worked for me.\r\n\r\nThat was actually the solution for me, I reused some old code which imported directly from keras, not tensorflow.keras layers etc. changing the import statements resolved the attribute error.", "@Blubbaa Same here. It was only thing that worked for me.", "If I want to check the value in tensor but I have to disable eager mode because of `tf.placeholder` compatibility, how should I do? I've tried to add `tf.enable_eager_execution` between the layer without `tf.placeholder` only to get this error `ValueError: tf.enable_eager_execution must be called at program startup.`", "Following code worked:\r\n\r\n    def parse_str(str_tensor):\r\n        raw_string = str_tensor.numpy().decode(\"utf-8\") \r\n    \r\n        # play with raw string\r\n        raw_string = 'AAA'+raw_string     \r\n        return raw_string\r\n    \r\n\r\nCall parse function:\r\n\r\n\r\n    def tf_pre_processing(row):\r\n      return tf.py_function(parse_str, [row['context']], [tf.string])\r\n    \r\n    \r\n    train = t.map(tf_pre_processing).batch(1).take(1)\r\n    \r\n    list(train)\r\n\r\n", "@Mainak431 The original code was working as expected. [Here](https://colab.research.google.com/gist/jvishnuvardhan/9085fdddb86208357ba785c426c1633d/untitled45.ipynb) is the gist. \r\n\r\n@tu1258 You don't need to disable eager execution. Just import it as `tf.compat.v1.placeholder` instead of `tf.placeholder`. If it still doesn't work, then please share a standalone code to reproduce the issue.\r\n\r\nPlease let me know if there are any more questions. Thanks! ", "> I had the same issue. Turned out that I was trying to use .numpy() inside a @tf.function. As far as I understand tf.function is not executed eagerly for performance purposes. If I remove the @tf.function decorator .numpy() works.\r\n\r\nI think following option in Tensorflow2.x can help\r\n[tf.config.run_functions_eagerly](config.run_functions_eagerly)\r\n\r\nThe place you are calling tf.function, you can try to execute it eagerly and then disable eager execution for tf.function.", "I'm new to tensorflow but following this tutorial (on Jupyter Notebook) https://www.tensorflow.org/tutorials/quickstart/beginner?hl=en will give you the error \r\n\r\n```\r\npredictions = model(x_train[:1]).numpy()\r\nTensor object has no attribute 'numpy'\r\n```", "@louisnot I cannot reproduce the error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/0bcf017989e93c82a94855775b4ff23e/beginner.ipynb). Thanks!\r\n\r\nIf you are facing the error, can you please share a gist? Thanks!", "I did not face any problem using the gist. ", "I am closing this issue as this was resolved in `tf-nightly`. Please feel free to reopen if I the issue persists. Thanks!", "@bhupendrathore Can you please open a new issue with a simple standalone code to reproduce the error? Thanks!", "I get the same problem,; when I fix it in your solution with `pip install tf-nightly` , the errors occurs:\r\n```\r\nERROR: Could not find a version that satisfies the requirement tf-nightly (from versions: none)\r\nERROR: No matching distribution found for tf-nightly\r\n```\r\n@jvishnuvardhan ", "@liangzelang i see you opened another new issue. we will resolve it there. Thanks", "fuck tf", "Add `tf.config.run_functions_eagerly(True)` before run other code.\r\n\r\nsee: https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly", "> Add `tf.config.run_functions_eagerly(True)` before run other code.\r\n> \r\n> see: https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly\r\n\r\n@OnlyBelter thanks a million! You save my day.", "I have tried all of the above methods, but they didn't work for me.\r\n**I solved this problem by simply restarting my Jupyter notebook.** \r\nIt turned out that activating eager mode or trying different TensorFlow commands messed up my environment.", "tf.config.run_functions_eagerly(True)\r\nI fixed my problem by explicitly specify eager execution.\r\nmy tf version is 2.3.0", "What fixed the problem for me was changing:\r\n\r\nimport keras\r\nto\r\nfrom tensorflow import keras\r\n\r\nDon't know why but import keras changed tf.executing_eagerly() to false again.", "@renatomello I encountered it when run this command `model.tabnet.aggregate_feature_selection_mask.numpy()`. In google colab, it worked well. However, on my local, it threw this error.\r\nI ensured that the eager model is enabled. `print(tf.executing_eagerly())` returns True. \r\n\r\nPlease help me. Thank you a lot.\r\n[notebook](https://colab.research.google.com/drive/1T8P5DrwBBZpx-FjWrAxXNhZNfsco8y-t?usp=sharing)", "> @renatomello I encountered it when run this command `model.tabnet.aggregate_feature_selection_mask.numpy()`. In google colab, it worked well. However, on my local, it threw this error.\r\n> I ensured that the eager model is enabled. `print(tf.executing_eagerly())` returns True.\r\n> \r\n> Please help me. Thank you a lot.\r\n> [notebook](https://colab.research.google.com/drive/1T8P5DrwBBZpx-FjWrAxXNhZNfsco8y-t?usp=sharing)\r\n\r\nWhat worked for me at the time was updating my TF to the nightly version, where the problem was already fixed.", "What fixed it for me:\r\nUse either `tf.config.run_functions_eagerly(True)`, or compile the model with the flag `run_eagerly=True`.\r\nThen, add `if tf.executing_eagerly():` before calling the `.numpy()` function.\r\n\r\nTF version: 2.5.0, I use *tensorflow.keras*.", "If you go the tf.compat.v1.enable_eager_execution() route don't forget to restart your Kernel in Spyder.", "> I have tried all of the above methods, but they didn't work for me. **I solved this problem by simply restarting my Jupyter notebook.** It turned out that activating eager mode or trying different TensorFlow commands messed up my environment.\r\n\r\nHoly... I just can't believe!! It worked with Spyder too. Thank you a lot! :D", "simply give up tensorflow and hardly every error disappears. pytorch yes!"]}, {"number": 27518, "title": "STOP USING ELIPSES!", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: All\r\n- Doc Link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/data\r\n\r\n\r\nEvery time higher level API like Slim says it will present an example for how to load data it starts with this or it's moral equivalent:\r\n\r\n```\r\n# Load the data\r\nmy_encoded_data = ...\r\ndata_decoder = MyDataDecoder()\r\n# Decode the inputs and labels:\r\ndecoded_input, decoded_labels = data_decoder.Decode(data, ['input', 'labels'])\r\n# Decode just the inputs:\r\ndecoded_input = data_decoder.Decode(data, ['input'])\r\n# Check which items a data decoder knows how to decode:\r\nfor item in data_decoder.list_items():\r\n  print(item)\r\n```\r\n\r\nI am sorely tempted to lay some biting and just dripping sarcasm down here, but I know all the deeply experienced TF programmers and creators probably honestly think this kind of thing is enlightening. For some reason.\r\n\r\nIf I knew how to load data I would not be grepping the Internet and finding stuff like this.\r\n\r\nWhat goes in the elipsis?", "comments": []}, {"number": 27517, "title": "ML Kit for Android fails to load valid TFLite model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BLU Vivo XI\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: ('v1.13.1-0-g6612da8951', '1.13.1') \r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.24.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: NVIDIA GeForce GTX 1050, ~5 GB\r\n\r\n**Describe the current behavior**\r\nI am currently attempting to use Firebase ML Kit to run a Tensorflow-Lite model that was converted to the .tflite format from Keras. I have been following [this](https://firebase.google.com/docs/ml-kit/android/use-custom-models) tutorial to host a custom model locally. \r\nEvery attempt thus far has ended with the same error: \"ByteBuffer is not a valid flatbuffer model\" (full stack trace shown below).\r\nThe .tflite model has been created correctly; running the Interpreter (from the Python Tensorflow module) to allocate tensors and get the input and output details returned the proper information without error. \r\nFurthermore, the iOS version of the app (using the same model, stored locally and loaded with Firebase ML Kit) is working properly. It used a custom build of Tensorflow (following [this](https://firebase.google.com/docs/ml-kit/ios/use-custom-tflite) tutorial) that started by forking  Tensorflow 12.0.0 and then cherry-picked future commits to include the additional ops required. I did not write any custom ops myself; all of the required ops had already been added to Tensorflow at one time or another.\r\nFirst, I attempted to use the same build by following [this](https://firebase.google.com/docs/ml-kit/android/use-custom-tflite) tutorial with Bazel version 0.18.0, which failed. I also attempted [this](https://heartbeat.fritz.ai/compiling-a-tensorflow-lite-build-with-custom-operations-cf6330ee30e2) tutorial to import TFLite as a module rather than publish it to my local maven repository. This had the same result.\r\nThen, under the assumption that the latest release of Tensorflow would have all the necessary ops anyway, I built Tensorflow 1.13.1 using Bazel version 0.24.0. This failed with the same error.\r\n\r\nHere are my questions, as succinctly as possible:\r\n1. Is ML Kit currently unequipped to handle the most recent versions of Tensorflow-Lite? (If so, why would the iOS version still work?)\r\n2. Is Tensorflow-Lite for Android currently not working for Tensorflow 1.13.1? I'm no expert in machine learning, so this may be nonsensical, but are there operations/features created for Tensorflow that Tensorflow-Lite is unable to handle? (And again, if this was the case, why would the iOS version still work?)\r\n3. Are the tutorials for ML Kit missing any essential information? \r\n4. Has anyone else been able to get a custom model converted to .tflite and have it work with ML Kit?\r\n\r\n**Other info / logs**\r\nE/ModelResourceManager: Error preloading model resource\r\n    com.google.firebase.ml.common.FirebaseMLException: Local model load failed: \r\n        at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:129)\r\n        at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104)\r\n        at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56)\r\n        at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7)\r\n        at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24)\r\n        at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29)\r\n        at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2)\r\n        at android.os.Handler.handleCallback(Handler.java:790)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:69)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:175)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:163)\r\n        at com.google.android.gms.internal.firebase_ml.zzpe.zzc(Unknown Source:224)\r\n        at com.google.android.gms.internal.firebase_ml.zzpf.zzd(Unknown Source:0)\r\n        at com.google.android.gms.internal.firebase_ml.zzpe.zzb(Unknown Source:150)\r\n        at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:118)\r\n        at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104)\u00a0\r\n        at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56)\u00a0\r\n        at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7)\u00a0\r\n        at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24)\u00a0\r\n        at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29)\u00a0\r\n        at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2)\u00a0\r\n        at android.os.Handler.handleCallback(Handler.java:790)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\u00a0\r\n        at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6)\u00a0\r\n        at android.os.Looper.loop(Looper.java:164)\u00a0\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n", "comments": ["Issue was fixed using these gradle dependencies together:\r\nimplementation 'com.google.firebase:firebase-ml-model-interpreter:18.0.0'\r\nimplementation 'org.tensorflow:tensorflow-lite:1.13.1@aar'", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27517\">No</a>\n"]}, {"number": 27516, "title": "Placeholder removed from tf 2.0", "body": "tensorflow placeholder is not found on tf 2.0. I need to create a Tensor with shape = None. But As the placeholder is gone . I am not finding ways to do it the 2.0 way. Please help. ", "comments": ["Placeholder is present at tf.compat.v1.placeholder . And can only be executed in eager mode off. \r\n\r\n#   tf.compat.v1.disable_eager_execution()\r\n\r\nIf we dont deactivate eager mode, we get this error -\r\n\r\ntf.placeholder() is not compatible with eager execution.\r\n\r\nGot this. ", "https://www.tensorflow.org/guide/migrate#low-level_variables_operator_execution"]}, {"number": 27515, "title": "Fix configure.py to properly compare \"X.Y.Z\" with \"X.Y\"", "body": "This is also needed to address #26553, as the current `devel` Docker\nimages fail during `./configure`.\n\nRight now, \"0.24\" is treated as lower than \"*.*.*\" because of the odd comparison method that adds digits to each existing section. This change converts \"0.24\" to \"0.24.0\" to fix that.\n\nPiperOrigin-RevId: 241950768", "comments": ["Thank you @angersson!"]}, {"number": 27514, "title": "Fix typo in generated temporary placeholder name", "body": "While working on some code to feed the contents of a SavedModel through the Graph Transform Tool, I used the new SavedModel loader to load the object detection model from [here](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz) and noticed two issues:\r\na. The graph had several hundred additional, unused Placeholder ops after converting to a GraphDef proto\r\nb. The names of these ops contained a misspelled word.\r\n\r\nThis PR corrects problem (b). Correcting problem (a) will require some more thought. One of the possible ways around problem (a) is to detect the unused ops by name and remove them prior to passing the graph through the Graph Transform Tool. But that solution would break if someone were to correct the spelling mistake later on. Hence correcting it now.", "comments": ["@frreiss can you please sign CLA", "My corporate CLA (with IBM) is on file, or at least the tool at https://cla.developers.google.com/clas says it is. Are the automated tools having trouble finding my CLA?"]}, {"number": 27513, "title": "Fix doc in Makefile of lite", "body": "", "comments": []}, {"number": 27512, "title": "High RAM usage after loading inference model into GPU", "body": "OS - Ubuntu 16.04\r\nCUDA Version 10.0.130\r\nTensorflow version 1.13.1\r\nYoloV2 - https://pjreddie.com/darknet/yolov2/\r\n\r\nTrained a YOLOv2 architecture on the custom images and after freezing the graph, Model weight (.pb) file size is 268MB, Once we load this into gpu for inference it is consuming 7.93GB. I know tensorflow allocates the buffers for the output data at each stage at the beginning. Please, can somebody explain why there is so much memory usage by tensorflow.\r\n\r\nBtw, if we run the same model with darknet framework it is taking 2.9GB of RAM.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27512\">No</a>\n", "Please, can somebody share or throw some insight into Tensorflow memory allocations while inference.", "Apologies for the delay in response. Have a look on this [link](https://www.tensorflow.org/guide/using_gpu) and let us know if that helps. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27511, "title": "[TF==2.0.0-alpha0] Memory leak with tf.keras.models.load_model", "body": "**System information**\r\n- Have I written custom code? Yes\r\n- OS Platform and Distribution  (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Docker_image tensorflow/tensorflow:2.0.0a0-py3-jupyter\r\n- TensorFlow version : 2.0.0a0-py3-jupyter \r\n- Python version:  3.5\r\n- Using only CPU\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have a memory leak when I load several keras model with `load_model` function previously save with `model.save`.\r\nHere is what I obtain when i load 20 times a model (the same model for the example).\r\nThe output is obtain with the memory profiler library. *https://pypi.org/project/memory-profiler/* (see code below)\r\n\r\nFirst Iteration:\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    14    229.5 MiB    229.5 MiB   @profile\r\n    15                             def load_model_keras(model_dir):\r\n    16    232.5 MiB      2.9 MiB       K.clear_session()\r\n    17    252.9 MiB     20.4 MiB       model = load_model(model_dir)\r\n    18    252.9 MiB      0.0 MiB       return model\r\n```\r\n\r\nAfter 20 iterations:\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    14    539.9 MiB    539.9 MiB   @profile\r\n    15                             def load_model_keras(model_dir):\r\n    16    539.9 MiB      0.0 MiB       K.clear_session()\r\n    17    566.4 MiB     26.5 MiB       model = load_model(model_dir)\r\n    18    566.4 MiB      0.0 MiB       return model\r\n```\r\n**Describe the expected behavior**\r\n\r\nI do not have this problem with tensorflow==1.13.1\r\nHere is what I obtain with this version and the behavior I expect to have.\r\n\r\n\r\nFirst Iterations :\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    14    210.7 MiB    210.7 MiB   @profile\r\n    15                             def load_model_keras(model_dir):\r\n    16    214.2 MiB      3.4 MiB       K.clear_session()\r\n    17    239.8 MiB     25.6 MiB       model = load_model(model_dir)\r\n    18    239.8 MiB      0.0 MiB       return model\r\n```\r\n\r\nAfter 20 iteration :\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    14    257.9 MiB    257.9 MiB   @profile\r\n    15                             def load_model_keras(model_dir):\r\n    16    257.9 MiB      0.0 MiB       K.clear_session()\r\n    17    259.0 MiB      1.1 MiB       model = load_model(model_dir)\r\n    18    259.0 MiB      0.0 MiB       return model\r\n```\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nimport tensorflow.keras.backend as K\r\nfrom memory_profiler import profile\r\nmodel_dir = \"MODEL_PATH.h5\"\r\n\r\n@profile\r\ndef load_model_keras(model_dir):\r\n    K.clear_session()\r\n    model = load_model(model_dir)\r\n    return model\r\n\r\nfor i in range(100):\r\n    print(i)\r\n    load_model_keras(model_dir)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI also tried to put within the function : \r\n* model=None\r\n* del model\r\n* gc.collect() (import gc)\r\n\r\nwith no effect", "comments": ["@bguillouet Can you please provide the colab python link so that we can reproduce the bug as .h5 is not available currently here", "Hi, \r\n\r\nYou can reproduce the bug with any \".h5\" model. \r\n\r\nHere is an example with a Resnet50 model from keras. Here is the code on colab https://colab.research.google.com/drive/1Uafi1hSqaHaEXpYyi8LFVXdK2gTQQVEs\r\n\r\n```\r\nimport os\r\nwd = os.getcwd()\r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.applications import ResNet50\r\nimport tensorflow.keras.backend as K\r\nfrom memory_profiler import profile\r\n\r\nmodel = ResNet50(weights='imagenet')\r\nmodel_dir = wd+\"/model.h5\"\r\nmodel.save(model_dir)\r\n\r\n\r\n@profile\r\ndef load_model_keras(model_dir):\r\n    K.clear_session()\r\n    model = load_model(model_dir)\r\n    return model\r\n\r\nfor i in range(20):\r\n    print(i)\r\n    load_model_keras(model_dir)\r\n ```\r\n\r\nI didn't get the memory_profiler output on colab but here are the output on my laptop. \r\n\r\n**tensorflow 1.13.1**\r\n\r\n*First iteration*\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    22    480.1 MiB    480.1 MiB   @profile\r\n    23                             def load_model_keras(model_dir):\r\n    24    399.8 MiB      0.0 MiB       K.clear_session()\r\n    25    750.4 MiB    350.6 MiB       model = load_model(model_dir)\r\n    26    750.4 MiB      0.0 MiB       return model\r\n```\r\n*20 iterations*\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    22    830.9 MiB    830.9 MiB   @profile\r\n    23                             def load_model_keras(model_dir):\r\n    24    830.9 MiB      0.0 MiB       K.clear_session()\r\n    25    840.1 MiB      9.2 MiB       model = load_model(model_dir)\r\n    26    840.1 MiB      0.0 MiB       return model\r\n````\r\n\r\n**tensorflow 2.0.0-aplha**\r\n\r\n*First iteration*\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    22    535.1 MiB    535.1 MiB   @profile\r\n    23                             def load_model_keras(model_dir):\r\n    24    535.1 MiB      0.1 MiB       K.clear_session()\r\n    25    684.0 MiB    148.9 MiB       model = load_model(model_dir)\r\n    26    684.0 MiB      0.0 MiB       return model\r\n```\r\n*20 iterations*\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    22   3350.5 MiB   3350.5 MiB   @profile\r\n    23                             def load_model_keras(model_dir):\r\n    24   3350.5 MiB      0.0 MiB       K.clear_session()\r\n    25   3502.3 MiB    151.7 MiB       model = load_model(model_dir)\r\n    26   3502.3 MiB      0.0 MiB       return model\r\n```\r\n\r\n", "Thanks for reporting this. I believe the issue is with `K.clear_session()`, which clears the default graph. In 1.x, Keras uses the default tf graph, but in 2.0 Keras manages its own graph. So the clear session call isn't actually clearing the keras graph and models are accumulating there and preventing memory from being freed. It should be a simple fix to make clear_session clears the right graph in 2.0.", "https://github.com/tensorflow/tensorflow/commit/5956c7e9c44e23cd1a006df872ae468201fdb600#diff-e329ed6b8d30dca9a441689005047035 should fix the issue. I tested on the latest `tf-nightly-2.0-preview` and observed that memory is no longer increasing.\r\n\r\nThis also means that our keras tests for 2.0 haven't been properly independent, since https://github.com/tensorflow/tensorflow/blob/2fe0e5265cb74926df25552f7533b404ecaf2c3d/tensorflow/python/keras/keras_parameterized.py#L39 wasn't actually clearing the graph in 2.0. So we might also pick up some testing robustness for free as well. All in all, this has been a tremendously useful bug report. Thanks so much!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27511\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27511\">No</a>\n", "I have TensorFlow version 2.0.0 and Keras 2.3.1, but still having the same problem."]}, {"number": 27510, "title": "Add HDF5 to tf.data.Dataset", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13 (Though I would like in on tf 2.0)\r\n- Are you willing to contribute it (Yes/No): No. Sorry, I don't know how this works... I am simply not qualified to contribute.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI am currently using HDF5 files (.h5 or .hdf5) to store my data, which is a data type frequently used in scientific research. (See https://github.com/tensorflow/tensorflow/issues/2089 for a similar but different request which makes the case for a HDF5 interface nicely). It is very convenient and widely used. MATLAB for example uses HDF5 files for large files. Indeed, it is much more convenient to use than Tensorflow's TFRecord format. \r\n\r\nHowever, in Tensorflow, there is no native support for HDF5 files in the tf.data.Dataset API, which is supposed to be the new API for all data loading. Currently, I am using tf.py_funtion to load my data for the simple reason that tf Dataset is always in graph mode and hence cannot give out the values of the files that I want it to read.\r\n\r\nMoreover, I have found that reading an HDF5 file in this way **DRAMATICALLY** slows down data I/O for unknown reasons. When I used the tf.keras.utils.Sequence API to read HDF5 files without the supposed optimizations that tensorflow is making, an operation that previously took hours now took just a few seconds. (However, I suspect that using tf.defun somehow got tangled up with this. I am not sure why but when I removed some lines, the code sped up, but was still much slower than even a single threaded for loop) \r\n\r\nTherefore, I would like to propose creating a new API in tf Dataset for HDF5 files. It could be called HDF5Dataset, similar to TFRecordDataset or CSVDataset. \r\n\r\nMoreover, this would allow Tensorflow to make I/O optimizations for reading using the C++ API for HDF5 instead of h5py, which has many limitations and factors that newbie users might not be familiar with. \r\n\r\nFor example, most builds of h5py cannot do multiprocessing. Also, most people do not know how to chunk their data slices, though this can make a 5-fold difference in read/write speed.\r\n\r\nI believe that adding this API would make Tensorflow much friendlier to scientific calculation.\r\n\r\n**Will this change the current api? How?**\r\nThis would add a new Dataset type in tf.data.Dataset, or a new method/function for making a dataset from an HDF5 file of an arbitrary format. This may require some low-level integration with the HDF5 format.\r\n\r\n**Who will benefit with this feature?**\r\nPeople in medical imaging, video datasets, astronomy, or any other type of very large dataset, which is often stored in HDF5. See their [website](https://www.hdfgroup.org/solutions/hdf5/) for information on its utility. Also people who don't want to go through the difficult process of making TFRecord files.\r\n\r\n**Any Other info.**\r\nPerhaps integrating aspects of h5py will make the process easier. \r\n", "comments": ["@veritas9872 Given the usage of HDF5, I think this issue naturally fits the scope of tensorflow-io and will likely be implemented in tensorflow-io first. The tensorflow-io has a focus on different file format, I/O, and streaming.\r\n\r\nThe repo of tensorflow-io is:\r\nhttps://github.com/tensorflow/io \r\n\r\nThe google group is:\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!forum/io\r\n\r\nI have opened another issue with content copied in https://github.com/tensorflow/io/issues/174 , we could continue the discussion there.\r\n\r\n\r\n"]}, {"number": 27509, "title": "*help* changing gradients  value through Masking the gradients with a matrix whose shape same with var but non-compatible", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: tf 1.8\r\n- Doc Link:https://tensorflow.google.cn/versions/r1.8/api_docs/python/tf/train/AdamOptimizer?hl=zh-cn#compute_gradients\r\n\r\n> This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\r\n\r\n**Describe the documentation issue**\r\nI have a corresponding mask matrix which is obtained with a trained model, and using the code as following. \r\n`grad = tf.multiply(grad.values, tf.convert_to_tensor(tf.abs(gradient_mask[name])), name='%s_pruning_mask'%name)`   it is report a error of compatible because this gradient has  a shape(17374, 512) and matrix shape(17353, 512). In Truth, I have to found the gradient shape is correspond other tensors but found nothing . the mask matrix is target embeddings weights shape. So what's wrong with this code or documents, why? please and Thank you.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Apologies for the delay in response. If you think it's a bug, please provide a minimal code snippet to reproduce the issue.\r\nIf you think we need to update our documents; please can you elaborate on what changes are you looking to be made. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27508, "title": "Missing device_attributes.pb.h when make tensorflow to .a file", "body": "I want make tensorflow to a static link file so I can use it to run the pretrained model with the cpp API.After I run the **build_all_linux.sh**, I move the **tensorflow** and **third_party** file and the **libtensorflow-core.a** and **libprotobuf.a** and **libnsync.a** to a work path.\r\nAnd when I compile the cpp code, it returns an error which said \r\n\r\n> fatal error: tensorflow/core/framework/device_attributes.pb.h: No such file or directory\r\n\r\nAnd it occurs in the session.h. I find there is only device_attributes.proto in the path,but I dont know why because I just followed the ways of many blogs online.Is there any thing that I missed?\r\n\r\nAny help is appreciate!Thx!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Now,I found all the xxx.pb.h file in this path \r\n\r\n> tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/framework\r\n\r\nBut why? As the example said, you need to `#include \"tensorflow/core/public/session.h\"` and the session.h file `include \"tensorflow/core/framework/device_attributes.pb.h\"`. So,the generate file from protoc should be in the path which is right to the session.h,right?\r\n\r\nNeed I check all the files in the gen?", "It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "hi, @SefaZeng , I got the same error, have you solved the problem?", "> hi, @SefaZeng , I got the same error, have you solved the problem?\r\n\r\n\u4f60\u597d\u50cf\u662f\u4e2a\u4e2d\u56fd\u4eba\u6211\u5c31\u6253\u4e2d\u6587\u4e86.. \u6211\u5176\u5b9e\u8bb0\u4e0d\u592a\u6e05\u8fd9\u91cc\u662f\u548b\u56de\u4e8b \u597d\u50cf\u662fproto\u751f\u6210\u7684\u4e00\u4e9b\u6587\u4ef6\u5728\u6211\u5199\u7684\u90a3\u4e2a\u8def\u5f84\u4e0b\u9762 \u5728cmakelist\u91cc\u9762\u628a\u8fd9\u4e9b\u8def\u5f84\u52a0\u8fdb\u53bb\u597d\u50cf\u5c31\u53ef\u4ee5"]}, {"number": 27507, "title": "Weird behavior of tf.Variable.assign in a tf.Dataset.map callback function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip3.7\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: Running the CPU version\r\n- GPU model and memory: NA\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n\r\nI was trying using a Variable to track a seed number in Dataset. Because I want my model to resume training with the same training order as before break if I can checkpoint the variable.\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nA minimal code is like:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nseed = tf.Variable(0, dtype=tf.int64, trainable=False,\r\n    use_resource=True)\r\ndef eject(x):\r\n    seed.assign(x) \r\n    return {'seed': seed, 'x': x}\r\n\r\ndataset = tf.data.Dataset.range(10000)\r\ndataset = dataset.map(eject, num_parallel_calls=1)\r\n\r\niterator = dataset.make_initializable_iterator()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n    next_element = iterator.get_next()\r\n\r\n    for i in range(10):\r\n        fetches = sess.run(next_element)\r\n        print(fetches)\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected print should be 0-9 for both `x` and `seed`. \r\n\r\n**Describe the current behavior**\r\n\r\nBut the output of `seed` does not stays the same and changes over different executions. The output is like:\r\n\r\n```\r\n{'seed': 0, 'x': 0}\r\n{'seed': 1, 'x': 1}\r\n{'seed': 2, 'x': 2}\r\n{'seed': 2, 'x': 3}\r\n{'seed': 4, 'x': 4}\r\n{'seed': 4, 'x': 5}\r\n{'seed': 6, 'x': 6}\r\n{'seed': 7, 'x': 7}\r\n{'seed': 7, 'x': 8}\r\n{'seed': 8, 'x': 9}\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I could reproduce the issue with TF1.13.1 and TF1.12. Thanks!", "@prclibo We need to initialize the seed also. Pleas check [here](https://www.tensorflow.org/api_docs/python/tf/Variable) for more details about tf.Variable.\r\n\r\nPlease check the code below\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nseed = tf.Variable(0, dtype=tf.int64, trainable=False,\r\n    use_resource=True)\r\ndef eject(x):\r\n    seed.assign(x) \r\n    return {'seed': seed, 'x': x}\r\n\r\ndataset = tf.data.Dataset.range(10000)\r\ndataset = dataset.map(eject, num_parallel_calls=1)\r\n\r\niterator = dataset.make_initializable_iterator()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(seed.initializer)\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n    next_element = iterator.get_next()\r\n\r\n    for i in range(10):\r\n        fetches = sess.run(next_element)\r\n        print(fetches)\r\n```\r\nPlease let me know what you think? Thanks!", "@jvishnuvardhan Thanks for the remind.\r\n\r\nThough below is not related to this question itself. But I found that `seed` is included in `tf.global_variables()`, so is it necessary to separately initialize `seed` besides `tf.global_variables_initializer()`?\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nseed = tf.Variable(0, dtype=tf.int64, trainable=False,\r\n    use_resource=True, name='ssss')\r\ndef eject(x):\r\n    seed.assign(x) \r\n    return {'seed': seed, 'x': x}\r\n\r\ndataset = tf.data.Dataset.range(10000)\r\ndataset = dataset.map(eject, num_parallel_calls=1)\r\n\r\niterator = dataset.make_initializable_iterator()\r\n\r\nprint(tf.global_variables())\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n    next_element = iterator.get_next()\r\n\r\n    for i in range(10):\r\n        fetches = sess.run(next_element)\r\n        print(fetches)\r\n```\r\n\r\nThis will output `global_variables [<tf.Variable 'ssss:0' shape=() dtype=int64>]`", "@prclibo Looks like this was an issue which was resolved in tf-nightly. Could you try running it in Google colab with !pip install tf-nightly. I don't see any issue with tf-nightly. Please let me know how it progress and close the issue if it was resolved by tf-nightly. Thanks!", "@jvishnuvardhan @muddham \r\nSee https://colab.research.google.com/drive/1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF\r\n\r\nThe issue did not happen in my very limited tests.\r\n\r\nAlso could you please take a look at #28043 . It seems a similar problem but not fixed in tf-nightly.", "@prclibo I will close this issue as it was resolved in tf-nightly. We will resolve the other issue there. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27507\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27507\">No</a>\n"]}, {"number": 27506, "title": "Lite: Compare Op bug fix", "body": "", "comments": []}, {"number": 27505, "title": "source build won't accept GPU with compute capability lower than 3.5", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce GTX 970\r\n\r\n**Describe the problem**\r\n\r\nThe set of CUDA compute capabilities (CCC from now on) is set as {3.0, 5.2} during configuration. File `.tf_configure.bazelrc` contains `build --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.0,5.2\"` (full file: [tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/3043058/tf_configure.bazelrc.txt)). After transfering the resulting pip package to a PC with CCC 3.0, tensorflow will reject a GPU with CCC lower than 3.5.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. `./configure` (resulting in [tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/3043058/tf_configure.bazelrc.txt))\r\n2. `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n3. `./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n4. move `/tmp/tensorflow_pkg/tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl` to other PC with CCC 3.0\r\n\r\n", "comments": ["I built the pip package directly on the target system with CCC 3.0 only (`TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\"`) but the pip package still requires CCC 3.5.\r\n\r\nIs it actually possible to build TF with a CCC of less than 3.5? If no, this should be stated in the configure script before a user continues with the compilation.", "I am able to use CCC 3.0 when building version 1.12.0 from source.", "Unfortunately you cannot compile TF with CCC 3.0. TF 1.6 and above require CCC 3.5 or higher. We have mentioned it under [hardware requirements](https://www.tensorflow.org/install/gpu#hardware_requirements) section on TF website.", "Does this also apply to source builds? E.g. I can build TF 1.12 with CCC 3.0 from source.\r\nCan you state in the documentation the minimum CCC version for binary and source builds per TF version?", "It does not apply to source builds, however we test tf against cc 3.5 and above. For ccc below 3.5 it may work but requires building from sources as you did. The minimum ccc version is mentioned in ```configure.py```  script. According to ```configure.py``` https://github.com/tensorflow/tensorflow/blob/8dc2d0eedac7760deb65254a8ef89878743299d7/configure.py#L1313 there are other limitations to tf build with ccc less than 3.5 Please take a look if haven't already. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27505\">No</a>\n", "@ymodak The version check you linked refers to XLA support. It is not a hint that is shown for general builds with CCC < 3.5.\r\nIt's still not clear, what the minimum CCC version for source builds for different TF versions is.", "@ymodak This issue is not solved. In https://github.com/tensorflow/tensorflow/issues/25329 the same issue is reported (i.e. CCC 3.0 works with TF 1.12 but fails with TF 1.13). If you closed this issue because of duplication, could you at least state in the other issue what the official state for CCC version support in source builds is?", "@gunan Can you PTAL? Thanks!", "@chsigg to confirm.\r\nIt is actually documented in multiple places that we officially support cuda compute capabilities >= 3.5.\r\nIn configure script, it is documented here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L999\r\nAnd in the website it is documented here:\r\nhttps://www.tensorflow.org/install/gpu#hardware_requirements\r\n\r\nSo we did push the minimum official support to >=3.5 and it is documented in the website.", "Correct, we no longer support compute capability < 3.5, even when building from source."]}, {"number": 27504, "title": "Fix build with latest flatbuffers", "body": "This change make be possible to build gpu delegate on Linux.\r\nFor raspberry pi, this may not work with good performance since  this patch use libegl1-mesa, libgles2-mesa not VideoCore. This change does not provide changes for bazel BUILD since this is not for vc. However, if someone implement EGL3 for raspberry pi, this will work well.\r\n\r\nUpdate https://github.com/tensorflow/tensorflow/issues/27320", "comments": ["To compiling, [Makefile](https://gist.github.com/ddc0fc9dc53f8a02f5f4fa953a29fe2d) is required.", "@mattn  Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "> Changes to egl_context.cc are not required, right? I don't see any cstring or vector usages.\r\n\r\nOkay, I'll update this PR in later.\r\n\r\n> I cannot approve #undef Status. That would break Google. I suggest that you exclude X11 locally when you build your stuff.\r\n\r\nAny change to add hack something to undefine macro?\r\n\r\n> Changes to egl_context.h, elementwise.cc, Makefile, download_dependencies.sh look good to me.\r\n\r\nThank you.", "@impjdi For about changing of egl_context.cc, calling `strstr` should be `std::strstr` ?", "@impjdi removed changes of undef Status.", "@mattn Can you please check build failures? Thanks!", "@mattn Could you please check build failures and resolve the conflicts? Thanks!", "Sorry, at least, I can't build this on raspberry pi since I removed \"#define Status\".  If you point the URL to the CI, I can fix it.", "@mattn Could you please resolve the conflicts? Thanks!", "Sorry, `$(OBJDIR)%.o: %.cc %.cpp` did not work since the source of a target must be separated.", "FYI, This is [SSD app using go-tflite](https://github.com/mattn/go-tflite/tree/master/_example/ssd). 7 May 2019\r\n\r\nhttps://twitter.com/mattn_jp/status/1125429350495473664\r\n\r\nAnd this is same app with this patch (including optimizations merged into master branch recently) 25 Jun 2019\r\n\r\nhttps://twitter.com/mattn_jp/statuses/1143533459375214592", "@mattn\r\n\r\nI ran into this problem of Status conflicting with X11 int when trying to use TFLite GPU on desktop.  It turns out EGL/egl.h pulls in EGL/eglplatform.h which pulls in X11/Xlib.h & X11/Xutil.h.  To bypass that, I built things with with\r\n\r\n-DMESA_EGL_NO_X11_HEADERS\r\n\r\nOf course, this may be different on RPi, but I would suggest looking tat the EGL include headers and make things work to exclude X11 includes.", ">-DMESA_EGL_NO_X11_HEADERS\r\n\r\nOh, Thanks. I didn't know it.", "You don't like to add `-DDMESA_EGL_NO_X11_HEADERS` in Makefile. Right?", "@mattn \r\n\r\n> You don't like to add `-DMESA_EGL_NO_X11_HEADERS` in Makefile. Right?\r\n\r\nHm, I am not a direct owner of TFLite (I'm only an owner of the GPU backend) and I don't know the answer to that :(  AFAIK I think the TF team is planning on deprecating the Makefile.  I'll loop in @jdduke but he's out for the week.  You might get an answer to your question next week.", "I don't have strong opinion for deprecating Makefile, but deprecating Makefile may lose many tensorflow lite novices, or hobby users.", "@mattn Could you please resolve the conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Well, I can't use the button."]}, {"number": 27503, "title": "can't save keras model built by functional api", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): n/a\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): na\r\n- GCC/Compiler version (if compiling from source): na\r\n- CUDA/cuDNN version: na\r\n- GPU model and memory: na\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n> lib/python3.6/site-packages/tensorflow/python/util/serialization.py\", line 69, in get_json_type\r\n>     raise TypeError('Not JSON Serializable:', obj)\r\n> TypeError: ('Not JSON Serializable:', b\"\\n\\x04Mean\\x12\\x04Mean\\x1a'embedding_1/embedding_lookup/Identity_2\\x1a\\x16Mean/reduction_indices*\\x07\\n\\x01T\\x12\\x020\\x01*\\n\\n\\x04Tidx\\x12\\x020\\x03*\\x0f\\n\\tkeep_dims\\x12\\x02(\\x01\")\r\n\r\n\r\n**Describe the expected behavior**\r\nmodel.save(path) works with a keras model\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\n\r\nimport copy\r\n import tensorflow as tf\r\n import pandas as pd\r\n from tensorflow.keras.layers import Input, Embedding, concatenate, Dense, Flatten\r\n from tensorflow import feature_column\r\n from tensorflow.python.keras.engine import training_utils\r\n\r\n\r\n def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n   dataframe = dataframe.copy()\r\n   labels = dataframe.pop('target')\r\n   d = dict(dataframe)\r\n\r\n   ds = tf.data.Dataset.from_tensor_slices((d, labels))\r\n   if shuffle:\r\n     ds = ds.shuffle(buffer_size=len(dataframe))\r\n   ds = ds.batch(batch_size)\r\n   return ds\r\n\r\n\r\n def get_model():\r\n     first_input = Input(shape = (1,), name='first_input')\r\n     second_input = Input(shape = (1,), name='second_input' )\r\n     embedding_layer = Embedding(input_dim=10, output_dim=3, input_length=1)\r\n\r\n     first_input_encoded = embedding_layer(first_input)\r\n     first_input_encoded = tf.keras.layers.Reshape((3,))(first_input_encoded)\r\n\r\n\r\n     selected = embedding_layer(second_input)\r\n     item_average = tf.reduce_mean(selected, axis=1, keepdims=True)\r\n     second_input_encoded = tf.keras.layers.Reshape((3,))(item_average)\r\n\r\n\r\n     o = concatenate([first_input_encoded, second_input_encoded])\r\n     o = Dense(1)(o)\r\n\r\n     inputs = [first_input, second_input]\r\n     model = tf.keras.models.Model(inputs=inputs, outputs=o)\r\n     return model\r\n\r\n\r\n df = pd.DataFrame(\r\n     [[3,[4,2,3],5, 'boy', 0],\r\n     [2,[6,1,2],7, 'girl', 1]], columns=['first_input', 'second_input', 'child_month_young', 'child_gender_young', 'target'])\r\n\r\n train_ds = df_to_dataset(df)\r\n val_ds = df_to_dataset(df)\r\n\r\n model = get_model()\r\n\r\n model.compile(optimizer='adam',\r\n               loss='binary_crossentropy',\r\n               metrics=['accuracy'])\r\n\r\n\r\n model.fit(\r\n     train_ds,\r\n     validation_data=val_ds,\r\n     epochs=1\r\n )\r\n\r\n print(model.summary())\r\n\r\n # tf.keras.models.save_model(model, 'test.h5')\r\n model.save('test.h5')\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["**I have same question as you, could you tell me how did you solve it?**\r\n---\r\n```\r\nTypeError: ('Not JSON Serializable:', b'\\n.model_1/ExpandDims/model/ExpandDims/ExpandDims\\x12\\nExpandDims\\x1a%embedding/embedding_lookup/Identity_2\\x1a\\x0eExpandDims/dim*\\n\\n\\x04Tdim\\x12\\x020\\x03*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n```", "> ## **I have same question as you, could you tell me how did you solve it?**\r\n> ```\r\n> TypeError: ('Not JSON Serializable:', b'\\n.model_1/ExpandDims/model/ExpandDims/ExpandDims\\x12\\nExpandDims\\x1a%embedding/embedding_lookup/Identity_2\\x1a\\x0eExpandDims/dim*\\n\\n\\x04Tdim\\x12\\x020\\x03*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n> ```\r\n\r\n@ShaneTian I have the same trouble as you, have you solved it?", "> > ## **I have same question as you, could you tell me how did you solve it?**\r\n> > ```\r\n> > TypeError: ('Not JSON Serializable:', b'\\n.model_1/ExpandDims/model/ExpandDims/ExpandDims\\x12\\nExpandDims\\x1a%embedding/embedding_lookup/Identity_2\\x1a\\x0eExpandDims/dim*\\n\\n\\x04Tdim\\x12\\x020\\x03*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n> > ```\r\n> \r\n> @ShaneTian I have the same trouble as you, have you solved it?\r\n\r\nDon't use 'expanddims' in Keras model, it's not compatible @ricosr ", "> > > ## **I have same question as you, could you tell me how did you solve it?**\r\n> > > ```\r\n> > > TypeError: ('Not JSON Serializable:', b'\\n.model_1/ExpandDims/model/ExpandDims/ExpandDims\\x12\\nExpandDims\\x1a%embedding/embedding_lookup/Identity_2\\x1a\\x0eExpandDims/dim*\\n\\n\\x04Tdim\\x12\\x020\\x03*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n> > > ```\r\n> > \r\n> > \r\n> > @ShaneTian I have the same trouble as you, have you solved it?\r\n> \r\n> Don't use 'expanddims' in Keras model, it's not compatible @ricosr\r\n\r\n@ShaneTian Thank you so much! Actually my problem is a little different from yours, I used tf.reshape in my model, but both of them are similar compatible troubles. Now I change tf.reshape by tf.keras.layers.Reshape, which can work well.", "I have also had this problem drive me mad when trying to use json to load my Keras model. I used a different approach that is slightly more primitive. Instead of using the save_model method I stored each set of weights using the model.layers and layer.get_weights() commands after training the model, storing them using cPickle. Then I reinitialised each layer by assigning the weights when adding a new layer to a completely new model in a different script:\r\nmodel = Sequential()\r\nmodel.add(Dense(M, weights=[Ww, Wb], input_shape=[M])). \r\nwhere Wb is the bias vector and Ww is the weight matrix. \r\n \r\nHope this helps! "]}, {"number": 27502, "title": "A result calculated inside of the tf.while_loop is wrong and not raising any error/alert when it occurred by GPU OOM.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 with tensorflow on Spyder IDE\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): Not applicable\r\n- GCC/Compiler version (if compiling from source): Not applicable\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.5.0\r\n- GPU model and memory: Titan RTX w/ 24.0GB of mem.\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen we are dealing with huge data that are going to be processed by any tf operations, tf.while_loop does not raise any error/flag/alert when it reaches to the GPU memory limit and thus getting the wrong result. I think you should add some assertion so that people can learn what's the problem.\r\n\r\nHere's the output\r\n`WARNING:tensorflow:From C:\\Users\\SHS\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.`\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nshapes?  (?, **512**, 16, 16) (?, **512**, 16, 16)\r\nvalues?  [[0. 0.]\r\n [0. 0.]] [[0. 0.]\r\n [0. 0.]] **>> Shouldn't be zeros. Have to be some numbers because I assigned random number and multiplied them.**\r\niterations?  50\r\nResult? [[ -8.234    0.3125]\r\n [ -7.92   -10.53  ]]\r\n\r\nrunfile('C:/Users/SHS/FPGA Project files/Practices/test_for_fl16.py', wdir='C:/Users/SHS/FPGA Project files/Practices')\r\nshapes?  (?, **64**, 16, 16) (?, **64**, 16, 16)\r\nvalues?  [[-16.64  42.38]\r\n [ 37.9   34.2 ]] [[  3.514 -10.414]\r\n [ 20.88   24.62 ]] >> **when I reduce the data size it gives me the right values.**\r\niterations?  50\r\nResult? [[-3.031   -0.01172]\r\n [-5.043    2.027  ]]\r\n\r\n**Describe the expected behavior**\r\nOutput printed should not be zeros, because I assigned some random number.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n    \r\nsess = tf.InteractiveSession()\r\n\r\na = tf.random.uniform([512, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)\r\nb = tf.random.uniform([512, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)\r\nc = tf.random.uniform([50, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)\r\nd = tf.random.uniform([50, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)\r\n\r\ni = 0\r\nreal = tf.TensorArray(dtype=tf.float16, size=50, name='REAL')\r\nimag = tf.TensorArray(dtype=tf.float16, size=50, name='IMAG')\r\nloop_cond = lambda i, *_: i < 50\r\ndef loop_body(i, r_array, i_array):\r\n  r_array = r_array.write(i, tf.subtract(tf.multiply(a,c[i]), tf.multiply(b,d[i])))\r\n  i_array = i_array.write(i, tf.add(tf.multiply(a,d[i]), tf.multiply(b,c[i])))\r\n  i += 1\r\n  return i, r_array, i_array\r\ni, real, imag = tf.while_loop(loop_cond, loop_body, loop_vars=[i, real, imag])\r\n\r\nreal = real.stack()\r\nimag = imag.stack()\r\nreal = tf.reduce_sum(real, axis=2)\r\nimag = tf.reduce_sum(imag, axis=2)\r\nprint('shapes? ', real.shape, imag.shape)\r\nprint('values? ', sess.run(real[0,0, :2, :2]), imag.eval()[0,0, :2, :2])\r\nprint('iterations? ', i.eval())\r\n\r\ni = 0\r\ntt = tf.add(tf.multiply(a,c[i]), tf.multiply(b,d[i]))\r\nprint('Result?', tt.eval()[0,0,:2,:2])\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Also, I want to ask this question. Is there any way we can avoid OOM(out-of-memory) when we are using tf.while_loop? We have a bunch of data/tensors that needs to be processed and tf.while_loop/tf.map_fn is our only hope to parallelize the code and get some performance. Right now, I am manually adjusting \"parallel_iterations=\" parameter but it's painful because we have multiples of these kinds of loops in our codes.", "@Han-sok,\r\n\r\nI've checked the issue in latest stable version `2.6.0` and as you mentioned, it raises `ResourceExhaustedError: failed to allocate memory [Op:AddV2]` when tf.while_loop is causing OOM. Please take a look at this [gist here](https://colab.research.google.com/gist/sanatmpa1/23f2cbdefde27ce1bf56ca1c27aef345/27502.ipynb). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27502\">No</a>\n"]}, {"number": 27501, "title": "Bug Fix and added Test case file for Prelu.", "body": "Fixed some defects in the file and test case file as well.", "comments": ["@miaout17 , I have updated the code with another commit which adds the commutative add feature to the existing implementation. Would be great if you can have a look and provide your valuable feedback.\r\n\r\nRegards\r\nAmit", "@miaout17 , thanks for your quick response, highly appreciate the same , I have updated the code as per your suggestion, kindly have a look.\r\n\r\nRegards\r\nAmit", "@miaout17 , can you pls review the PR.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 could you please resolve the conflicts? Thanks!", "> @amitsrivastava78 could you please resolve the conflicts? Thanks!\r\n\r\n@gbaned , thanks for the update, i have resolved the merge conflicts.\r\n\r\nRegards\r\nAmit", "@miaout17 , thanks for approving the PR, there was one sanity check failure which i have resolved, can you please re-approve the PR again.\r\n\r\nRegards\r\nAmit", "@miaout17 , could you please re-approve the PR. Sorry for the trouble.\r\n\r\nRegards\r\nAmit", "@miaout17, can you please, re-approve the PR.\r\n\r\nRegards \r\nAmit ", "@miaout17, can you please, re-approve the PR. Sorry for the trouble.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts and keep us posted? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27499, "title": "optimizer_v2.apply_gradients() - surprising behavior, not explicitly documented", "body": "**System information**\r\n- TensorFlow version: 2.0.0-aplha0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Optimizer#apply_gradients\r\n\r\nI was implementing a DD Policy Gradient for reinforcement learning, and had a bug where my agent would minimize the reward, instead of maximizing it. It turned out optimizer.apply_gradients() follows negative of the gradients I give to it. This is a surprising behavior when using this function separately, outside of .minimize() context.\r\n\r\nDocumentation does not mention anywhere that negative gradient is followed by this method.\r\n\r\nhad to use a unit test to clarify the behavior:\r\n\r\n```\r\n    opt = tf.optimizers.Adam()\r\n    x = tf.Variable([1], dtype=tf.float32)\r\n    dx = tf.ones([1], dtype=tf.float32)\r\n\r\n    opt.apply_gradients( [(dx, x)] )\r\n    assert x.numpy()[0] > 1\r\n```\r\nThere is also no flag to invert this behavior and follow positive gradient. I have to pass negative gradient of the expected reward to follow it in positive direction.\r\n\r\n\r\n```\r\n    def train_actor(self, sars):\r\n\r\n        obs1, actions, rewards, obs2 = sars\r\n        with tf.GradientTape() as tape:\r\n            would_do_actions = self.actor(obs1)\r\n            score = tf.reduce_mean( self.critic( observations=obs1, actions=would_do_actions ) )\r\n            inverted = - score\r\n\r\n        # tf optimizer follows negative of the provided gradients.\r\n        # For this reason we provide negative gradient of the score -\r\n        # it will result in positive gradient being followed.\r\n        grads = tape.gradient( inverted, self.actor.trainable_weights )\r\n        self.optimizer.apply_gradients( zip(grads, self.actor.trainable_weights) )\r\n```", "comments": ["@ikamensh Thanks for bringing this to our notice. We will take a look and resolve it. Thanks!", "@ikamensh This is intended behavior and not 2.0 specific. In general all optimizers follow the algorithm from papers to minimize objective. If the goal is to maximize reward, then the best approach (that I think) is to negate the objective manually.\r\nFor example, see this ddpg implementation:\r\nhttps://github.com/openai/spinningup/blob/master/spinup/algos/ddpg/ddpg.py#L157 ", "In reinforcement learning reward is always maximized. Furthermore, Tensorflow can be seen as a broader framework than only machine learning, then it makes sense that \"apply gradients\" would at least mention that it will follow the negative of the provided gradient. Being specific does not hurt.", "Closing this since there is no intermediate plan for improving this. In general I think it's fine because we mostly use neg log prob so it is actually minimizing objective function", "@tanzhenyu as of July 2019, this still has not been documented. In my opinion the documentation for `apply_gradients` should mention that it follows the negative gradient. \r\n\r\nIndependent of the debate whether implementation is good or bad, a better documentation could never hurt."]}, {"number": 27498, "title": "Where is the BahdanauAttention?", "body": "tf 1.0: tensorflow.contrib.seq2seq.BahdanauAttention\r\nWhere is the BahdanauAttention with tf 2.0???", "comments": ["Starting TF 2.0 contrib won't be a part of TF. However  [tensorflow/addons](https://github.com/tensorflow/addons). You can check [tfa.seq2seq](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/README.md) to know more about your specific use case. Also you may a send PR to addons as well to add additional features."]}, {"number": 27497, "title": "`tf.reduce_sum` with multiple negative axes and `tf.RaggedTensor` bugged", "body": "## System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below):\r\n- Python version: ('v1.13.1-0-g6612da8951', '1.13.1')\r\n- GPU model and memory: quadro k620 2gb\r\n\r\n## Current behaviour\r\nMultiple axes passed to `tf.reduce_sum` with first argument being a ragged tensor results in incorrect behaviour.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx_values = tf.random.normal(shape=(100, 5, 6))\r\nx_row_lengths = tf.constant([20, 30, 50], dtype=tf.int64)\r\nx_ragged = tf.RaggedTensor.from_row_lengths(x_values, x_row_lengths)\r\nprint(x_ragged.shape)\r\n# [3, ?, 5, 6]\r\n\r\n# wrong shape\r\nprint(tf.reduce_sum(x_ragged, axis=(-2, -3)).shape)\r\n# [50, 6]\r\n\r\n# positive axes work\r\nprint(tf.reduce_sum(x_ragged, axis=(1, 2)).shape)\r\n# [3, 6]\r\n\r\n# separate reductions work\r\nprint(tf.reduce_sum(tf.reduce_sum(x_ragged, axis=-3), axis=-2).shape)\r\n# [3, 6]\r\n```\r\n\r\n## Expected behaviour\r\nSame result as corresponding positive indices/separate reductions.\r\n", "comments": ["Created a PR #27699 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27497\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27497\">No</a>\n"]}, {"number": 27496, "title": "Can not set 'dynamic' property on custom layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab environment\r\n- TensorFlow version (use command below): 2.0 alpha\r\n\r\nI am trying to write a custom layer in TF 2.0 and having trouble making it run eagerly.\r\n\r\nFor example if I open a documentation page: https://www.tensorflow.org/alpha/guide/eager\r\n\r\nAnd try to use the following piece of code from that page to create custom layer:\r\n```python\r\nclass MySimpleLayer(tf.keras.layers.Layer):\r\n  def __init__(self, output_units):\r\n    super(MySimpleLayer, self).__init__()\r\n    self.output_units = output_units\r\n    self.dynamic = True\r\n\r\n  def build(self, input_shape):\r\n    # The build method gets called the first time your layer is used.\r\n    # Creating variables on build() allows you to make their shape depend\r\n    # on the input shape and hence removes the need for the user to specify\r\n    # full shapes. It is possible to create variables during __init__() if\r\n    # you already know their full shapes.\r\n    self.kernel = self.add_variable(\r\n      \"kernel\", [input_shape[-1], self.output_units])\r\n\r\n  def call(self, input):\r\n    # Override call() instead of __call__ so we can perform some bookkeeping.\r\n    return tf.matmul(input, self.kernel)\r\n```\r\n\r\nAnd then try to create an instance, for example like this:\r\n\r\n```python\r\nMySimpleLayer(10)\r\n```\r\nI get the following error message:\r\n\r\n```python\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-26-1fe9d7429f3d> in <module>()\r\n----> 1 MySimpleLayer(10)\r\n\r\n<ipython-input-25-e48804825d2c> in __init__(self, output_units)\r\n      3     super(MySimpleLayer, self).__init__()\r\n      4     self.output_units = output_units\r\n----> 5     self.dynamic = True\r\n      6 \r\n      7   def build(self, input_shape):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __setattr__(self, name, value)\r\n   1778         # Exclude @property.setters from tracking\r\n   1779         hasattr(self.__class__, name)):\r\n-> 1780       super(Layer, self).__setattr__(name, value)\r\n   1781       return\r\n   1782 \r\n\r\nAttributeError: can't set attribute\r\n```\r\nThis error appears in Colab as well as on my machine (Windows 8).\r\n\r\nAm I doing something wrong here?\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "How is this not a bug if code from documentation does not work (in the environment provided by Google)?", "Thanks for trying TF 2.0 alpha. I was able to reproduce your behavior in TF 2.0 alpha however it executed successfully using TF 1.13.1 (after enabling eager mode)", "@fchollet can you triage this?\r\n\r\nI think dynamic is now a property, which is why this error happens, but I don't remember the right way to set it.", "Having the same problem, what is the correct way to set this property? The alternative I tried is passing it as an argument to the superclass init, but that raises a NotImplementedError\r\n\r\n```\r\nclass EVL_out (tf.keras.layers.Layer):\r\n    \r\n    def __init__ (self):\r\n        super (EVL_out, self).__init__ (dynamic=True)\r\n        self.out_units = 2\r\n\r\n    def build (self, input_shape):\r\n    ...\r\n```\r\n\r\n", "@ilia-nikiforov,\r\nWhat exactly is your problem? Initially I thought that setting this property is needed to run layers eagerly, but after digging some more I found out that it is not needed (in my case my custom layer was not running eagerly, but the source of the problem turned out to be completely different). In fact, at the moment I am not sure what does setting 'dynamic' property does at all.", "This is fixed with latest tf-nightly 2.0 build version 2.0.0-dev20190808\r\nNow we can expect new improved error message;\r\n```python\r\nAttributeError: Can't set the attribute \"dynamic\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name.\r\n```\r\nChanging the name of attribute helps fixing this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27496\">No</a>\n", "I'm using tensorflow 2.3 and still get this error. I'm using py_function in my layers and would like to set `dynamic = True` as it is documented here: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer](url)\r\n\r\n**Update:**\r\nThe error is ok i had to implement the `compute_output_shape` method in my custom layer.\r\n\r\nExample to reproduce the error:\r\n\r\n```\r\nclass Custom(keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        \r\n    def build(self, input_shape):\r\n        self._outputshape = [None,1]\r\n        super().build(input_shape)\r\n    \r\n    def py_function(self,inputs):\r\n        return inputs.numpy().sum(axis=0)\r\n    \r\n    def call(self, inputs):\r\n        x = tf.py_function(func=self.py_function,inp=[inputs],Tout=tf.float32)\r\n        x.set_shape(self._outputshape)\r\n        return x\r\n\r\nx_input = keras.layers.Input(shape = (16,), name = 'x_input')\r\nx = Custom(dynamic=True)(x_input)\r\ncustom_model = keras.Model(x_input, x, name=\"custom_model\")\r\ncustom_model.summary()\r\n\r\n```\r\n\r\noutputs:\r\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-52-4ed37a9049c5> in <module>\r\n      1 x_input = keras.layers.Input(shape = (16,), name = 'x_input')\r\n----> 2 x = Custom(dynamic=True)(x_input)\r\n      3 custom_model = keras.Model(x_input, x, name=\"custom_model\")\r\n      4 custom_model.summary()\r\n\r\nc:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n    924     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    925       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 926                                                 input_list)\r\n    927 \r\n    928     # Maintains info about the `Layer.call` stack.\r\n\r\nc:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1130           # TODO(fchollet): consider py_func as an alternative, which\r\n   1131           # would enable us to run the underlying graph if needed.\r\n-> 1132           outputs = self._symbolic_call(inputs)\r\n   1133 \r\n   1134         if outputs is None:\r\n\r\nc:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _symbolic_call(self, inputs)\r\n   2660   def _symbolic_call(self, inputs):\r\n   2661     input_shapes = nest.map_structure(lambda x: x.shape, inputs)\r\n-> 2662     output_shapes = self.compute_output_shape(input_shapes)\r\n   2663     # Convert to TensorShape so that nest.map_structure will not map into\r\n   2664     # individual dim of the shape.\r\n\r\nc:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in compute_output_shape(self, input_shape)\r\n    739                   self.__class__.__name__), e)\r\n    740       return nest.map_structure(lambda t: t.shape, outputs)\r\n--> 741     raise NotImplementedError\r\n    742 \r\n    743   @doc_controls.for_subclass_implementers\r\n\r\nNotImplementedError: \r\n```", "I think this means your dynamic=True layer must implement\ncompute_output_shape.\n\nOn Tue, Aug 4, 2020 at 4:02 AM oholimoli <notifications@github.com> wrote:\n\n> I'm using tensorflow 2.3 and still get this error. I'm using py_function\n> in my layers and would like to set dynamic = True as it is documented\n> here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer\n> <http://url>\n>\n> Example to reproduce the error:\n>\n> class Custom(keras.layers.Layer):\n>     def __init__(self, **kwargs):\n>         super().__init__(**kwargs)\n>\n>     def build(self, input_shape):\n>         self._outputshape = [None,1]\n>         super().build(input_shape)\n>\n>     def py_function(self,inputs):\n>         return inputs.numpy().sum(axis=0)\n>\n>     def call(self, inputs):\n>         x = tf.py_function(func=self.py_function,inp=[inputs],Tout=tf.float32)\n>         x.set_shape(self._outputshape)\n>         return x\n>\n> x_input = keras.layers.Input(shape = (16,), name = 'x_input')\n> x = Custom(dynamic=True)(x_input)\n> custom_model = keras.Model(x_input, x, name=\"custom_model\")\n> custom_model.summary()\n>\n>\n> outputs:\n>\n> ---------------------------------------------------------------------------\n> NotImplementedError                       Traceback (most recent call last)\n> <ipython-input-52-4ed37a9049c5> in <module>\n>       1 x_input = keras.layers.Input(shape = (16,), name = 'x_input')\n> ----> 2 x = Custom(dynamic=True)(x_input)\n>       3 custom_model = keras.Model(x_input, x, name=\"custom_model\")\n>       4 custom_model.summary()\n>\n> c:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\n>     924     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\n>     925       return self._functional_construction_call(inputs, args, kwargs,\n> --> 926                                                 input_list)\n>     927\n>     928     # Maintains info about the `Layer.call` stack.\n>\n> c:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\n>    1130           # TODO(fchollet): consider py_func as an alternative, which\n>    1131           # would enable us to run the underlying graph if needed.\n> -> 1132           outputs = self._symbolic_call(inputs)\n>    1133\n>    1134         if outputs is None:\n>\n> c:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _symbolic_call(self, inputs)\n>    2660   def _symbolic_call(self, inputs):\n>    2661     input_shapes = nest.map_structure(lambda x: x.shape, inputs)\n> -> 2662     output_shapes = self.compute_output_shape(input_shapes)\n>    2663     # Convert to TensorShape so that nest.map_structure will not map into\n>    2664     # individual dim of the shape.\n>\n> c:\\users\\hofo\\envs\\py-smart-bin-sensing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in compute_output_shape(self, input_shape)\n>     739                   self.__class__.__name__), e)\n>     740       return nest.map_structure(lambda t: t.shape, outputs)\n> --> 741     raise NotImplementedError\n>     742\n>     743   @doc_controls.for_subclass_implementers\n>\n> NotImplementedError:\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27496#issuecomment-668529878>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKXVK3Q23RREK3RRY3R67TELANCNFSM4HDQOTLQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 27495, "title": "to build with '-g' option", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2eebcc63a6cd3aad483bf7c1cb25df2b8780ef67\r\n- Python version: Python 3.5.2\r\n- Installed using virtualenv? pip? conda?:  virtualenv\r\n- Bazel version (if compiling from source): bazel-0.24.0-installer-linux-x86_64.sh\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\nMy purpose is to build tensorflow c library (libtensorflow_framework.so and libtensorflow.so) to be stepped into within gdb via source code.\r\n\r\nI tried two methods:\r\n1) add '-g -O0' option during ./configure\r\n![image](https://user-images.githubusercontent.com/10448440/55536831-5bdda100-56ed-11e9-9cb5-7d586981b292.png)\r\n\r\nand then run the below commands to build the c libraries:\r\nbazel test -c opt //tensorflow/tools/lib_package:libtensorflow_test\r\nbazel build -c opt //tensorflow/tools/lib_package:libtensorflow\r\n\r\nbut, gdb shows that it is unable to read the symbols as below:\r\n0x00007fffedea2400  0x00007ffff41f73ef  Yes (*)     /usr/local/lib/libtensorflow.so\r\n0x00007fffe9245bc0  0x00007fffea040ea0  Yes (*)     /usr/local/lib/libtensorflow_framework.so\r\n\r\n2) i tried dbg version with:\r\nbazel test -c dbg //tensorflow/tools/lib_package:libtensorflow_test\r\n\r\nand it never stops, with:\r\nINFO: Build option --compilation_mode has changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow/tools/lib_package:libtensorflow_test (0 packages loaded, 15440 targets configured).\r\nINFO: Found 1 test target...\r\n[7,070 / 7,073] Linking tensorflow/libtensorflow.so; 14876s local\r\n^C\r\nBazel caught interrupt signal; shutting down.\r\n\r\nand the build result is weird:\r\n$ file bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so          \r\nbazel-out/k8-dbg/bin/tensorflow/libtensorflow.so: data\r\n\r\n\r\nbtw, my build commands come from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md", "comments": ["I'm not sure what `Yes()` is. It looks wrong.\r\n\r\n1. Make sure you do the pip install if you want to use `/usr/local/lib` as above. Otherwise the bazel build will not be used unless you are running with a bazel environment.\r\n2. You can use `nm bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so` to find out what symbols are defined.\r\n3. Use `--linkopt=-Wl,--strip-never` to never strip symbols in your build.\r\n4. You can load symbols separately, by calling the linker command yourself and passing `-Wl,-Map=output.map`. You can load the mapfile from gdb.\r\n\r\nI think it's probably that gdb is getting confused with the symbols, so `Yes()` is probably some other function.", "> I'm not sure what `Yes()` is. It looks wrong.\r\n> \r\n> 1. Make sure you do the pip install if you want to use `/usr/local/lib` as above. Otherwise the bazel build will not be used unless you are running with a bazel environment.\r\n\r\nI think it is not a problem, because \"bazel test/build -c opt *\" successfully build libtensorflow.so and libtensorflow_framework.so.  I then link them into /usr/local/lib, and my application based on tf c lib works, the only issue is that i'm unable to step into the tf source code within gdb.\r\n\r\n$ ll /usr/local/lib/libtensorflow*\r\nlrwxrwxrwx 1 root   root          94 4\u6708   4 18:51 /usr/local/lib/libtensorflow_framework.so -> /work/ml/tensorflow_work/tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow_framework.so*\r\nlrwxrwxrwx 1 root   root          84 4\u6708   4 18:51 /usr/local/lib/libtensorflow.so -> /work/ml/tensorflow_work/tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow.so*\r\n\r\n\r\n> 2. You can use `nm bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so` to find out what symbols are defined.\r\n\r\n$ nm bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so\r\nnm: bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so: File format not recognized\r\n\r\nit is expected because:\r\n$ file bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so\r\nbazel-out/k8-dbg/bin/tensorflow/libtensorflow.so: **data**\r\n\r\nAs for opt version,\r\n$ file bazel-out/k8-opt/bin/tensorflow/libtensorflow.so\r\nbazel-out/k8-opt/bin/tensorflow/libtensorflow.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, BuildID[sha1]=7bf0c37714b11d6ff260961f62facdac858f0966, **not stripped**\r\n\r\n$ nm bazel-out/k8-opt/bin/tensorflow/libtensorflow.so | tail    (lots of output, so i use tail)\r\n....\r\n_ZZZZZN10tensorflow12_GLOBAL__N_123GrpcWorkerServiceThread23CompleteInstanceHandlerEPNS_4CallIS1_NS_4grpc13WorkerService12AsyncServiceENS_23CompleteInstanceRequestENS_24CompleteInstanceResponseEEEENKUlvE_clEvENKUlRKNS_6StatusEE0_clESD_ENKUliPKcE_clEiSG_E17vmodule_activated\r\n00000000088fd780 b _ZZZZZN10tensorflow16XlaDeviceContext21CopyDeviceTensorToCPUEPKNS_6TensorEN4absl11string_viewEPNS_6DeviceEPS1_St8functionIFvRKNS_6StatusEEEENKUlSA_E1_clESA_ENKUlvE_clEvENKUliPKcE_clEiSI_E17vmodule_activated\r\n\r\n> 3. Use `--linkopt=-Wl,--strip-never` to never strip symbols in your build.\r\n\r\nFrom above, the lib symbols are not stripped.  Anyway, i can try this option, but where can I add it? (I'm not familiar with bazel)\r\n\r\n> 4. You can load symbols separately, by calling the linker command yourself and passing `-Wl,-Map=output.map`. You can load the mapfile from gdb.\r\n\r\ncould you tell me how to pass `-Wl,-Map=output.map` to the linker command? thanks.\r\n\r\ni just searched 'gdb mapfile' but could not find a web page to show how to load a mapfile from gdb, could you also please give me a help for the exact command? thanks a lot. ( symbol-file?)\r\n\r\n> \r\n> I think it's probably that gdb is getting confused with the symbols, so `Yes()` is probably some other function.\r\n\r\nyes, for 'opt' version, i guess gdb is getting confused with the symbols, and the mapfile might help.\r\n\r\nAs for 'dbg' option, i changed nothing else, and the build process never stops for hours, i guess there might be an issue within tf.\r\n", "If you only want to step into a handful of C++ sources, try adding `-g` to the ``--per_file_copt`` in your bazel build command, for example:\r\n\r\n```\r\nbazel build -c opt --config=opt --per_file_copt=//tensorflow/core/distributed_runtime/rpc/.*\\.cc@-g //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nSee [here](https://docs.bazel.build/versions/master/user-manual.html#flag--per_file_copt) for details.", "I think your link to `/usr/local/lib` may not work. Not sure. We may be setting the `rpath` to look for the libraries in a different place. Can you try [this](https://github.com/bazelbuild/bazel/issues/6145)? Just create a shell script to run gdb and run the test under that script.", "> If you only want to step into a handful of C++ sources, try adding `-g` to the `--per_file_copt` in your bazel build command, for example:\r\n> \r\n> ```\r\n> bazel build -c opt --config=opt --per_file_copt=//tensorflow/core/distributed_runtime/rpc/.*\\.cc@-g //tensorflow/tools/pip_package:build_pip_package\r\n> ```\r\n> See [here](https://docs.bazel.build/versions/master/user-manual.html#flag--per_file_copt) for details.\r\n\r\nthanks, i don't know where are the source files for the build, so I just choose //tensorflow/.*, as following:\r\nbazel test -c opt --per_file_copt=//tensorflow/.*\\.cc\\.c@-g,-O0,--strip=never //tensorflow/tools/lib_package:libtensorflow_test\r\nbazel test -c dgb --per_file_copt=//tensorflow/.*\\.cc\\.c@-g,-O0,--strip=never //tensorflow/tools/lib_package:libtensorflow_test\r\n\r\nthe option '--strip=never' is from the documentation link you just mentioned:\r\nCaveat: If some files are selectively compiled with debug symbols the symbols might be stripped during linking. This can be prevented by setting --strip=never\r\n\r\nbut there is a build error:\r\ngcc: error: unrecognized command line option '--strip-never'\r\n\r\nI also tried with:\r\nbazel test -c opt --per_file_copt=//tensorflow/.*\\.cc\\.c@-g,-O0,--linkopt=-Wl,--strip-never //tensorflow/tools/lib_package:libtensorflow_test\r\nand still the same issue.\r\n\r\nI'm currently trying with below commands from a clean source tree (rm -rf *; git checkout .) and build tree (rm -rf ~/.cache/):\r\nbazel test -c opt --per_file_copt=//tensorflow/.*\\.cc\\.c@-g,-O0 //tensorflow/tools/lib_package:libtensorflow_test\r\nbazel build -c opt --per_file_copt=//tensorflow/.*\\.cc\\.c@-g,-O0 //tensorflow/tools/lib_package:libtensorflow\r\n\r\nhours need to get it finished. will report back after build finished.\r\n", " \r\n> I'm currently trying with below commands from a clean source tree (rm -rf _; git checkout .) and build tree (rm -rf ~/.cache/): bazel test -c opt --per_file_copt=//tensorflow/._.cc.c@-g,-O0 //tensorflow/tools/lib_package:libtensorflow_test\r\n> bazel build -c opt --per_file_copt=//tensorflow/.*.cc.c@-g,-O0 //tensorflow/tools/lib_package:libtensorflow\r\n> \r\n> hours need to get it finished. will report back after build finished.\r\n\r\nwith this new command, gdb can read symbols from libtensorflow.so, but still unable to read from libtensorflow_framework.so, see below.  And it still unable to step into tf source code when I press 's' within gdb at the TF_* functions, such as TF_SessionRun, TF_GetCode, TF_GraphImportGraphDef\r\n \r\n0x00007fffedea2400  0x00007ffff41f792f  Yes         /usr/local/lib/libtensorflow.so\r\n0x00007fffe9245bc0  0x00007fffea040ea0  Yes (*)     /usr/local/lib/libtensorflow_framework.so\r\n(*): Shared library is missing debugging information.\r\n", "> I think your link to `/usr/local/lib` may not work. Not sure. We may be setting the `rpath` to look for the libraries in a different place. Can you try [this](https://github.com/bazelbuild/bazel/issues/6145)? Just create a shell script to run gdb and run the test under that script.\r\n\r\nthanks, but my application is a stand-alone binary out of the tf source tree, and the below command failed to run:\r\nbazel run --run_under=gdb /path_to_my_binary\r\n\r\nERROR: not a valid absolute pattern (absolute target patterns must start with exactly two slashes):", "Perhaps that's because these are compiled from the C sources which are not matched with the `*.cc` regexp. \r\n\r\nMaybe it works with ``--per_file_copt=//tensorflow/.*\\.cc@-g,-O0,//tensorflow/.*\\.c@-g,-O0``.", "> Perhaps that's because these are compiled from the C sources which are not matched with the `*.cc` regexp.\r\n> \r\n> Maybe it works with `--per_file_copt=//tensorflow/.*\\.cc@-g,-O0,//tensorflow/.*\\.c@-g,-O0`.\r\n\r\nbut my command has considered .c file:\r\n![image](https://user-images.githubusercontent.com/10448440/55774626-7edecb00-5ac8-11e9-9175-2ed7698c46fe.png)\r\n", "I tried as #27744 with:\r\nbazel build  --cxxopt='-g' //tensorflow/tools/lib_package:libtensorflow_test\r\n\r\nand it stops at the below line for hours:\r\n[7,070 / 7,073] Linking tensorflow/libtensorflow.so; 10322s local\r\n\r\nand, libtensorflow.so is there, but as data format.\r\n$ file /work/ml/tensorflow_work/tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow.so\r\n/work/ml/tensorflow_work/tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow.so: data\r\n\r\nIt is the same when i tried with -c dbg\r\n", "i tried again with '-g' option, it takes ~6 hours to finish ...\r\n\r\n`INFO: Elapsed time: 24108.686s, Critical Path: 21192.22s`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27495\">No</a>\n"]}]