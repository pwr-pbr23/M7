[{"number": 11848, "title": "Can't import graph containing batch_sequences_with_states", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: v1.3.0.0rc0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nAs I'm working with sequences I make extensive use of `tf.contrib.training.batch_seq_with_states`. I need to be able to load my graph afterwards so I write out a meta graph file containing a graph definition using `Saver.save`. Upon loading using `tf.train.import_meta_graph` I get an error hinting the `batch_seq_with_states` operation isn't saved in the graph:\r\n\r\n```\r\n(Pdb) tf.train.import_meta_graph('model.ckpt-1.meta')\r\n*** KeyError: \"The name 'input/batch_seq_with_states/InputQueueingStateSaver/' refers to an Operation not in the graph.\"\r\n```\r\n\r\nUsing Tensorboard I can inspect `input/batch_seq_with_states/InputQueueingStateSaver` just fine from the same files.\r\n\r\nI will for now try to work around this by writing out a separate graph that relies on placeholders for data loading instead of `batch_seq_with_states` and then load the weights separate. ", "comments": ["I don't see any batch_seq_with_states, only contrib.training.batch_sequences_with_states, which is documented [here](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.training/splitting_sequence_inputs_into_minibatches_with_state_saving)", "~~So it is but the name used is without the leading module I believe. The documentation says nothing about it not being serializable.~~\r\n\r\nYou're right, I got confused. The `name` I supply to `batch_sequences_with_states` is `batch_seq_with_states`:\r\n\r\n```\r\n                batch = self._batch_sequences_with_states(\r\n                        input_sequences= input_sequences,\r\n                        input_key      = context['key'],\r\n                        input_context  = context,\r\n                        initial_states = initial_states,\r\n                        input_length   = context['num_frames'],\r\n                        num_unroll     = self.time_steps,\r\n                        batch_size     = self.batch_size,\r\n                        num_threads    = 2,\r\n                        capacity       = self.batch_size * 4,\r\n                        name           = 'batch_seq_with_states',\r\n                        make_keys_unique = True,\r\n                        allow_small_batch = True # Required otherwise blocks\r\n                        )\r\n```\r\n\r\n\r\nThe issue looks very similar to #11888", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Is this still a problem?", "Yes, it is, but it's a niche problem with some niche Tensorflow functionality so it shouldn't affect too many people. I've moved on."]}, {"number": 11847, "title": "Bug: Op type not registered 'BlockLSTM' in binary", "body": "### Describe the problem\r\nI want to load and run a single tensorflow model within another C++ project. To do this, I defined the `tensorflow_all` library in [`tensorflow/BUILD`] which should include all the necessary dependencies. (Details below)\r\n\r\nWhen loading the tensorflow model via the C++ API ([LoadSavedModel]), the following runtime error occurs:\r\n\r\n[LoadSavedModel]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.h#L50\r\n\r\n```\r\n2017-07-28 13:36:41.875493: I tensorflow/cc/saved_model/loader.cc:284] Loading SavedModel: fail. Took 43550 microseconds.\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  Not found: Op type not registered 'BlockLSTM' in binary running on myMachine. \r\n  Make sure the Op and Kernel are registered in the binary running in this process.\r\n```\r\n\r\nI found that the BlockLSTM Op is registered in [`tensorflow/contrib/rnn/ops/lstm_ops.cc`].\r\nHowever, I could not find a way to include the operation BlockLSTM in my C++ library `tensorflow_all`.\r\n\r\n[`tensorflow/contrib/rnn/ops/lstm_ops.cc`]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/ops/lstm_ops.cc#L179\r\n\r\nBear with me if this is actually not a bug (or even a feature request). I am still getting started with tensorflow. \r\n\r\nSince my implementation is part of another larger project, it is currently difficult for me to give an easy to reproduce example. However, if this is necessary, I will do it as soon as I find the time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\n\tYes, a colleague of mine trained a tensorflow graph which uses the BlockLSTM Op from `tensorflow/contrib/rnn/` using the python API. \r\n\r\n\tAs mentioned before, I added code to the [`tensorflow/BUILD`] to create a library `tensorflow_all`. This library is used to load the graph externally.\r\n\r\n\t[`tensorflow/BUILD`]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD\r\n\r\n\t```bazel\r\n\tcc_binary(\r\n\t\tname = \"libtensorflow_all.so\",\r\n\t\tlinkshared = 1,\r\n\t\tlinkopts = [\"-Wl,--version-script=tensorflow/tf_version_script.lds\"],\r\n\t\tdeps = [\r\n\t\t\t\"//tensorflow/cc:cc_ops\",\r\n\t\t\t\"//tensorflow/cc:client_session\",\r\n\t\t\t\"//tensorflow/cc/saved_model:loader\",\r\n\t\t\t\"//tensorflow/cc/saved_model:tag_constants\",\r\n\t\t\t\"//tensorflow/core:all_kernels\",\r\n\t\t\t\"//tensorflow/core:framework_internal\",\r\n\t\t\t\"//tensorflow/core:tensorflow\",\r\n\t\t\t\"//tensorflow/contrib:contrib_kernels\",\r\n\t\t\t\"//tensorflow/contrib:contrib_ops_op_lib\",\r\n\t\t],\r\n\t)\r\n\t```\r\n\tI was hoping that `BlockLSTM` was included in `\"//tensorflow/contrib:contrib_ops_op_lib\"`, but that appears not to be the case.\r\n\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version**: Master branch (commit bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b)\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: GeForce GTX 980, 4GB \r\n- **Exact command to reproduce**: Sorry. Not that easy to reproduce.\r\n", "comments": ["/CC @josh11b Can you comment? I know nothing about the C++ API, but it does seem strange how the [`lstm_ops_kernels`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/BUILD#L342) rule includes GEMM kernels but not LSTM kernels. Is this a bug?", "@reedwm @josh11b @ShapingView I experience the same problem. When launching `bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server` I get a following error:\r\n```\r\n...failed: Not found: Op type not registered 'LSTMBlockCell' in binary running on b7328d0093c7. Make sure the Op and Kernel are registered in the binary running in this process\r\n```\r\nWhen I rebuild tensorflow and serving modifying `lstm_ops_kernels`, like this:\r\n```\r\ntf_kernel_library(\r\n    name = \"lstm_ops_kernels\",\r\n    srcs = [\r\n        \"kernels/blas_gemm.cc\",\r\n        \"kernels/blas_gemm.h\",\r\n        \"kernels/lstm_ops.cc\",\r\n        \"kernels/lstm_ops.h\",\r\n        \"ops/lstm_ops.cc\",\r\n    ],\r\n    gpu_srcs = [\r\n        \"kernels/blas_gemm.h\",\r\n        \"kernels/lstm_ops_gpu.cu.cc\",\r\n        \"kernels/lstm_ops.h\",\r\n    ],\r\n    prefix = \"kernels/lstm_ops\",\r\n    deps = [\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core/kernels:eigen_helpers\",\r\n        \"//third_party/eigen3\",\r\n    ],\r\n)\r\n```\r\n\r\nI get a different error (in addition to the previous one), when building my client:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:359:1: Label '@org_tensorflow//tensorflow/contrib/rnn:kernels/lstm_ops_gpu.cu.cc' is duplicated in the 'srcs' attribute of rule 'lstm_ops_kernels_gpu'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:359:1: Label '@org_tensorflow//tensorflow/contrib/rnn:kernels/lstm_ops.h' is duplicated in the 'srcs' attribute of rule 'lstm_ops_kernels_gpu'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:359:1: Label '@org_tensorflow//tensorflow/contrib/rnn:kernels/lstm_ops.cc' is duplicated in the 'srcs' attribute of rule 'lstm_ops_kernels'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:214:1: Target '@org_tensorflow//tensorflow/contrib/rnn:gen_lstm_ops_py_wrappers_cc' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:lstm_ops_pygenrule'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:214:1: Target '@org_tensorflow//tensorflow/contrib/rnn:ops/gen_lstm_ops.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:lstm_ops'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:gru_ops' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:lstm_ops' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/_gru_ops.so' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/_lstm_ops.so' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:__init__.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/core_rnn_cell.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/fused_rnn_cell.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/gru_ops.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/lstm_ops.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/rnn.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/rnn/BUILD:27:1: Target '@org_tensorflow//tensorflow/contrib/rnn:python/ops/rnn_cell.py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib/rnn:rnn_py'.\r\nERROR: /root/.cache/bazel/_bazel_root/01a289b7faaf5ec651fb0e4e35f862a1/external/org_tensorflow/tensorflow/contrib/BUILD:8:1: Target '@org_tensorflow//tensorflow/contrib/rnn:rnn_py' contains an error and its package is in error and referenced by '@org_tensorflow//tensorflow/contrib:contrib_py'.\r\n```\r\n\r\nI will be very thankful if there is any quick hack or a fix that can be applied to fix this, because it prevents me from deploying a production servable. \r\nThanks in advance!", "Any progress here?", "It works for me with the following changes (also for gru_ops_kernels).\r\n\r\n```\r\n--- a/tensorflow/contrib/rnn/BUILD\r\n+++ b/tensorflow/contrib/rnn/BUILD\r\n@@ -326,6 +326,8 @@ tf_kernel_library(\r\n     srcs = [\r\n         \"kernels/blas_gemm.cc\",\r\n         \"kernels/blas_gemm.h\",\r\n+        \"kernels/gru_ops.h\",\r\n+        \"ops/gru_ops.cc\",\r\n     ],\r\n     gpu_srcs = [\r\n         \"kernels/blas_gemm.h\",\r\n@@ -344,6 +346,8 @@ tf_kernel_library(\r\n     srcs = [\r\n         \"kernels/blas_gemm.cc\",\r\n         \"kernels/blas_gemm.h\",\r\n+        \"kernels/lstm_ops.h\",\r\n+        \"ops/lstm_ops.cc\",\r\n     ],\r\n     gpu_srcs = [\r\n         \"kernels/blas_gemm.h\",\r\n```\r\n\r\nAnd adding ```\"//tensorflow/contrib/rnn:gru_ops_kernels\"``` and/or ```\"//tensorflow/contrib/rnn:lstm_ops_kernels\"``` to the deps of the respective binary.\r\n\r\n(using r1.3)", "@pks Cool, I think it's a bug and you can send a PR if you want to.", "Sure -- but that just solved a problem that I had with tools/graph_transforms/transform_graph, which resulted in the exact same error message. Maybe someone could confirm that it also solves the problem with tensorflow_serving or the initial problem.", "Hi there! I don't know if it could help but I have a `ValueError: No op named BlockLSTM in defined operations.` if I try to freeze model with `LSTMBlockFusedCell`.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\ninp = [tf.placeholder(tf.float32, [2, 4], 'input')]\r\nlstm_cell = tf.contrib.rnn.LSTMBlockFusedCell(10)\r\noutputs, _ = lstm_cell(inp, dtype=tf.float32)\r\n\r\nsess.run(tf.global_variables_initializer())\r\ninputData = np.random.standard_normal(inp[0].shape).astype(np.float32)\r\noutputData = sess.run(outputs[0], feed_dict={inp[0]: inputData})\r\n\r\nsaver = tf.train.Saver()\r\nsaver.save(sess, 'graph_with_lstm.ckpt')\r\ntf.train.write_graph(sess.graph.as_graph_def(), '', 'graph_with_lstm.pb')\r\n```\r\nand\r\n```shell\r\npython freeze_graph.py \\\r\n  --input_graph graph_with_lstm.pb \\\r\n  --input_checkpoint graph_with_lstm.ckpt \\\r\n  --output_graph frozen_graph.pb \\\r\n  --output_node_names lstm_block_wrapper/BlockLSTM\r\n```", "@dkurt You could try adding\r\n```\r\nfrom tensorflow.contrib.rnn import *\r\n```\r\nto freeze_graph.py.", "@pks your changes worked for my problem!\r\nI can load the model now. \r\n\r\nThanks!", "@ShapingView There's a cleaner patch in #12566 (if you're using tf > 1.0).", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing since it seems this issue is resolved.", "@pks  hi, i encounter the error: `Op type not registered 'BlockLSTM' in binary`, when i load a graph.pb with LSTMBlockFusedCell. I am using android c++ api in a android project, the linked `libtensorflow-core.a` comes from `tensorflow/contrib/makefile/build_all_android.sh`. I want to know how to modify things under `tensorflow/contrib/makefile` to make the lib comprises contrib op(especially LSTMBlockFusedCell). Can you help me?", "@songmeixu Have a look at this #12566", "@pks hi, I used your method to add operation \"BlockLSTM\" and compile successfully, but loading the model in c++ doesn't seem to work, have you ever been in this situation? Look forward to your reply!", "@songmeixu ", "@yanggeng1995 , the needed changes are: https://github.com/songmeixu/tensorflow/commit/c8341fb2b2f410f3fbd31b2e02463ed4499d54bc,\r\nhttps://github.com/songmeixu/tensorflow/commit/ab800e2ccc71191ec7b2057b5d51fd4e44841791\r\n\r\nand for anyone run into this error when compile an android TF c++ lib, more needed changes are:\r\nhttps://github.com/songmeixu/tensorflow/commit/e85c30b3550d2b8b84ffe72165f56f41b72c9a20"]}, {"number": 11845, "title": "Add resampler to release notes.", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11844, "title": "tf.contrib.rnn.static_rnn not found", "body": "I am using tensorflow version 0.12.1 and trying to implement simple RNN using static_rnn.\r\n\r\n    lstm_layer=tf.contrib.rnn.LayerNormBasicLSTMCell(n_hidden,forget_bias=1)\r\n    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n\r\nThe IDE is not able to resolve `static_rnn`.\r\n\r\nIn tensorflow documentation `tf.contrib.rnn.static_rnn` is listed [here](https://www.tensorflow.org/api_guides/python/contrib.rnn).But on clicking on it it redirects to 404 page.Is there a change in `static_rnn` in tensorflow?If yes then whats the new alternative/implementation?", "comments": ["You seem to be using an older version of tensorflow. 'contrib' module was introduced after TF1.0, I guess. You can either upgrade to a newer version of TF or use the `tf.nn` module which has what you need.\r\n\r\nAny reason why you didn't fill the form?", "Thanks for the response.My mistake.Pycharm and console show different version of tensorflow.I've resolved it.I was pretty sure it was version problem but pycharm indicated otherwise."]}, {"number": 11843, "title": "bazel build error: no such package '@protobuf//src/google/protobuf'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.x\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: sudo bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\n\r\nWhen I use bazel to build, I met the follow error. \r\nPS: I had installed python3-protobuf using pip3.\r\n\r\n> ERROR: no such package '@protobuf//src/google/protobuf': Could not find handler for bind rule //external:protobuf\r\n", "comments": ["Please fill the form", "Hi, @snnn  I had fill the form. Thanks!", "**Personally**, I don't think it's a bug of tensorflow. 1.2 is a release version. Recently, I build it everyday, for many times. I never encountered this error. If you are seeking help, please close this issue and go to Stack Overflow.", "I can not reproduce on Ubuntu 14.04. I installed python3-protobuf with pip3 in a virtualenv, than used the corresponding python3 in the virtualenv when installing TensorFlow. It built fine.\r\n\r\nCan you post the input/output when you run `./configure`? Also, try without sudo after doing a `bazel clean`, there is no need to use sudo when building.", "Hi, @reedwm  I re-build using bazel after doing a `bazel clean`, it works now. (I don't know why :P) Thanks a lot!  Also thanks @snnn !", "I get the exact same problem for TensorFlow Serving (current git head, after no problems of that sort for months). I've tried bazel 0.5.1 , 0.5.3 , bazel clean, manually removing the bazel caches and I get the exact same issue on both x86_64 and PowerPC versions under Linux.  Building TensorFlow itself (1.3.0rc0) is fine in the same environment.  What I would *really* like to learn though is not so much the specifics of this problem but how one even goes about figuring out what exactly irks bazel about these rules (aka something like \"explain to me what you are trying to do, don't just say 'failed'\".\r\n\r\nUPDATE: While I still would like to get a better idea how debug such issues, the underlying proximate cause was that  I used \"git pull --recurse-submodules\" but not  \"git submodule update --recursive\" .", "I had the same issue (Ubuntu 16.04.2 LTS) after git clone (--recurse-submodules) and bazel build:\r\n\r\nERROR: no such package '@protobuf//src/google/protobuf': Could not find handler for bind rule //external:protobuf.\r\n\r\nIt can be fixed in the same way by \"bazel clean\"", "I had the same error, and `bazel clean` helped me too. Can anyone tell why `bazel` is so unreliable?"]}, {"number": 11842, "title": "Fix windows bazel build", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Testing Bazel build at http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/33/console", "Jenkins, test this please", "Thanks for fixing this,\r\nI sent a fix to boringssl https://boringssl-review.googlesource.com/c/18544/", "d09304f introduced a dependency on\r\n@boringssl//:crypto from grpc, this change makes boringssl build on Windows with Bazel", "@tensorflow-jenkins test this please", "Since only failure is Linux related, merging."]}, {"number": 11841, "title": "arg_max  error", "body": "my code is\r\n\r\ny=tf.Variable(tf.random_normal(shape=[2,5,3,10,12 ,5 ]))\r\na=tf.arg_max(y,0)\r\na.shape\r\nsess=tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(a)\r\n\r\n\r\nerror is :\r\nInvalidArgumentError (see above for traceback): ArgOp : Unhandled input dimensions: 6\r\n\t [[Node: ArgMax_7 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_9/read, ArgMax_7/dimension)]]\r\n", "comments": ["`tf.arg_max` only handles tensors with a 5 or less dimensions. See [argmax_op.cc](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/core/kernels/argmax_op.cc#L91).\r\n\r\nIf you want to run `tf.arg_max` on higher dimension tensors, explain why and reopen this bug.", "I have a tensor with dimensions `(batch, time, width, height, channels, logits)` to predict video with a softmax over possible pixel values. This is a typical use case in video prediction. This logit tensor goes into `tfp.distributions.Categorical`. However, calling mode on this distribution fails because it uses `tf.argmax()`. It's possible to reshape and implement the mode manually, but slow with such big tensors.", "Thanks!", "Still seeing the same issue with tf.1.15\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/argmax_op.cc#L91", "TF 1.15 doesn't have the fix unfortunately. TF 2.1 is the first release with the fix."]}, {"number": 11840, "title": "Using a `tf.Tensor` as a Python `bool` is not allowed", "body": "```\r\nimport tensorflow as tf\r\n\r\na = tf.constant([1,2,3])\r\nb = tf.equal(a, 0)\r\n\r\nwith tf.Session() as ss:\r\n    print(ss.run(a,b))\r\n```\r\nWhen I run the code above, I encounter the following errors:\r\n\r\n> TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n\r\nBut when I change to the following code, it doesn't raise a exception\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.constant([1,2,3])\r\nb = tf.equal(a, 0)\r\n\r\nwith tf.Session() as ss:\r\n    print(ss.run(a))\r\n    print(ss.run(b))\r\n```\r\nwith the output of \r\n\r\n> [1 2 3]\r\n[False False False]\r\n\r\nIt seems to be a bug, or it's designed to be so?", "comments": ["Not a bug, `b` is treated as the `feed_dict` argument, you probably meant to do `ss.run([a,b])`", "Oh, yes, what a careless mistake! Thank you very much.", "\r\n    def iou(y_pred, y_true):\r\n        y_pred = tf.cast((y_pred > 0), dtype=tf.float32)\r\n        i = tf.reduce_sum(y_true * y_pred)\r\n        u = tf.reduce_sum(y_true + y_pred)\r\n        return (i / u).item()if u != 0 else u.item()\r\n    \r\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',iou])\r\n\r\nI encounter similar error when I run this code. The error messages are as follow.\r\n---------------------------------------------------------------------------\r\nOperatorNotAllowedInGraphError            Traceback (most recent call last)\r\n<ipython-input-197-a5eb92cb206c> in <module>\r\n----> 1 model = unet()\r\n\r\n<ipython-input-191-57144cd2aad3> in unet(pretrained_weights, input_size)\r\n     51         return (i / u).item()if u != 0 else u.item()\r\n     52 \r\n---> 53     model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',iou])\r\n     54 \r\n     55     model.summary()\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    437           targets=self._targets,\r\n    438           skip_target_masks=self._prepare_skip_target_masks(),\r\n--> 439           masks=self._prepare_output_masks())\r\n    440 \r\n    441       # Prepare sample weight modes. List with the same length as model outputs.\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\r\n   2002           metric_results.extend(\r\n   2003               self._handle_per_output_metrics(self._per_output_metrics[i],\r\n-> 2004                                               target, output, output_mask))\r\n   2005         if return_weighted_and_unweighted_metrics or return_weighted_metrics:\r\n   2006           metric_results.extend(\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)\r\n   1953       with K.name_scope(metric_name):\r\n   1954         metric_result = training_utils.call_metric_function(\r\n-> 1955             metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n   1956         metric_results.append(metric_result)\r\n   1957     return metric_results\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)\r\n   1153 \r\n   1154   if y_pred is not None:\r\n-> 1155     return metric_fn(y_true, y_pred, sample_weight=weights)\r\n   1156   # `Mean` metric only takes a single value.\r\n   1157   return metric_fn(y_true, sample_weight=weights)\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in __call__(self, *args, **kwargs)\r\n    194     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\r\n    195     return distributed_training_utils.call_replica_local_fn(\r\n--> 196         replica_local_fn, *args, **kwargs)\r\n    197 \r\n    198   @property\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)\r\n   1133     with strategy.scope():\r\n   1134       return strategy.extended.call_for_each_replica(fn, args, kwargs)\r\n-> 1135   return fn(*args, **kwargs)\r\n   1136 \r\n   1137 \r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in replica_local_fn(*args, **kwargs)\r\n    177     def replica_local_fn(*args, **kwargs):\r\n    178       \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\r\n--> 179       update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n    180       with ops.control_dependencies([update_op]):\r\n    181         result_t = self.result()  # pylint: disable=not-callable\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n     74 \r\n     75     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n---> 76       update_op = update_state_fn(*args, **kwargs)\r\n     77     if update_op is not None:  # update_op will be None in eager execution.\r\n     78       metric_obj.add_update(update_op)\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in update_state(self, y_true, y_pred, sample_weight)\r\n    585         y_pred, y_true)\r\n    586 \r\n--> 587     matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n    588     return super(MeanMetricWrapper, self).update_state(\r\n    589         matches, sample_weight=sample_weight)\r\n\r\n<ipython-input-191-57144cd2aad3> in iou(y_pred, y_true)\r\n     49         i = tf.reduce_sum(y_true * y_pred)\r\n     50         u = tf.reduce_sum(y_true + y_pred)\r\n---> 51         return (i / u).item()if u != 0 else u.item()\r\n     52 \r\n     53     model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',iou])\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in __bool__(self)\r\n    755       `TypeError`.\r\n    756     \"\"\"\r\n--> 757     self._disallow_bool_casting()\r\n    758 \r\n    759   def __nonzero__(self):\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in _disallow_bool_casting(self)\r\n    524     else:\r\n    525       # Default: V1-style Graph execution.\r\n--> 526       self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n    527 \r\n    528   def _disallow_iteration(self):\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in _disallow_in_graph_mode(self, task)\r\n    513     raise errors.OperatorNotAllowedInGraphError(\r\n    514         \"{} is not allowed in Graph execution. Use Eager execution or decorate\"\r\n--> 515         \" this function with @tf.function.\".format(task))\r\n    516 \r\n    517   def _disallow_bool_casting(self):\r\n\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\n### How can I correct my code so that I can run it successfully?"]}, {"number": 11839, "title": "tf.train.ExponentialMovingAverage.variables_to_restore", "body": "When I view the API doc, [tf.train.ExponentialMovingAverage.variables_to_restore](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/ExponentialMovingAverage#variables_to_restore). It said `If a variable has a moving average, use the moving average variable name as the restore name; otherwise, use the variable name.`, but I test it is wrong.\r\nLook the code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(tf.constant(1.0),name='a')\r\nb = tf.Variable(tf.constant(3.0),name='b')\r\nc = tf.Variable(tf.constant(5.0),name='c')\r\nema = tf.train.ExponentialMovingAverage(decay=0.9999)\r\nema.apply([a,b])\r\n\r\nprint(tf.get_default_graph().get_all_collection_keys())\r\nprint(tf.get_collection('moving_average_variables'))\r\nprint(tf.global_variables())\r\nvariables_to_restore = ema.variables_to_restore()\r\nprint(variables_to_restore)\r\n```\r\nAnd the output is:\r\n```\r\n['trainable_variables', 'moving_average_variables', 'cond_context', 'variables']\r\n[<tf.Variable 'a:0' shape=() dtype=float32_ref>, <tf.Variable 'b:0' shape=() dtype=float32_ref>]\r\n[<tf.Variable 'a:0' shape=() dtype=float32_ref>, <tf.Variable 'b:0' shape=() dtype=float32_ref>, <tf.Variable 'c:0' shape=() dtype=float32_ref>, <tf.Variable 'a/ExponentialMovingAverage:0' shape=() dtype=float32_ref>, <tf.Variable 'b/ExponentialMovingAverage:0' shape=() dtype=float32_ref>]\r\n{'a/ExponentialMovingAverage': <tf.Variable 'a:0' shape=() dtype=float32_ref>, 'c/ExponentialMovingAverage': <tf.Variable 'c:0' shape=() dtype=float32_ref>, 'b/ExponentialMovingAverage': <tf.Variable 'b:0' shape=() dtype=float32_ref>}\r\n\r\n```\r\nVariable c don't has a moving average, but in `variables_to_restore`, it use key=shadow_name either.\r\nWho can tell me what's wrong?", "comments": ["This behavior is confusing but it is not a bug. When calling `variables_to_restore`, you are supposed to indicate what variables have moving averages through its `moving_avg_variables` parameter. `ExponentialMovingAverage` doesn't actually use its own stored averages to determine what variables have moving averages, but instead relies on `moving_avg_variables`.  `moving_avg_variables` defaults to `variables.moving_average_variables() + variables.trainable_variables()`, which is why Variable `c` is included.\r\n\r\nI believe the reason `variables_to_restore` acts this way is that it is typically called during evaluation, while the moving averages were set during training. During evaluation, no moving averages are set, so `variables_to_restore` cannot rely on the  `ExponentialMovingAverage`'s stored moving averages, which is why it uses `moving_avg_variables` instead.\r\n\r\n@sguada, I think we should change the description to something like \"If a variable is in `moving_avg_variables`, use its moving average variable name as the restore name; otherwise, use the variable name.\" What do you think?", "I think it maybe a bug.\r\nLook at the code:\r\n\r\n```\r\nx = tf.Variable(12.,name='x')\r\ny = tf.Variable(34.,name='y')\r\nz = tf.Variable(0, trainable=False)\r\nema = tf.train.ExponentialMovingAverage(.0)\r\nema.apply([x])\r\nprint(ema.variables_to_restore())\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, './model.ckpt')\r\n\r\n```\r\nFirst, I save the variables to check point.\r\n\r\n```\r\nx = tf.Variable(12.,name='x')\r\ny = tf.Variable(34.,name='y')\r\nz = tf.Variable(0, trainable=False)\r\nema = tf.train.ExponentialMovingAverage(.0)\r\nema.apply([x])\r\nprint(ema.variables_to_restore())\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    saver = tf.train.Saver(ema.variables_to_restore())\r\n    saver.restore(sess, './model.ckpt')\r\n```\r\n\r\nThen, I want to restore variables from the check point. I want to load x with x_shadow, load y with y, and load z with z. But it will raise a error. Because `ema.variables_to_restore()` will map `y_shadow_name` to y, but there is no key of `y_shadow_name` in the check point, and z is untrainable variable, so it can be loaded correctly.\r\n\r\nSo I think the purpose of  `variables_to_restore()` is to load moving average variables with shadow, such as x, but don't want to load non-moving average variables with shadow, such as y.\r\n\r\n@reedwm \r\n\r\n", "As the docs and @reedwm  mentioned by default `ema.variables_to_restore()` uses `tf.moving_average_variables() + tf.trainable_variables()`.\r\n\r\nBut you can get the behavior that want by doing\r\n\r\n```\r\nx = tf.Variable(12.,name='x')\r\ny = tf.Variable(34.,name='y')\r\nz = tf.Variable(0, trainable=False)\r\nema = tf.train.ExponentialMovingAverage(.0)\r\nema.apply([x])\r\nvariables_to_restore = ema.variables_to_restore(tf.moving_average_variables())\r\nprint(variables_to_restore)\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    saver = tf.train.Saver(variables_to_restore)\r\n    saver.restore(sess, './model.ckpt')\r\n```", "It is wrong if you use `saver = tf.train.Saver(variables_to_restore)`, because the shadow variables aren't saved in check point. And the `variables_to_restore` map shadow name to moving average variable(just mean in check point, `key=shadow_name, value=moving average variable`), so when you `restore(variables_to_restore)`, the moving average variables will be load by themselves, not their shadows. But our purpose is to load moving average variables  with their shadows.\r\n\r\n@sguada ", "I'm sorry but I don't understand your comment. \r\n\r\nThe code I suggested is for the restoring phase, for saving one would use `tf.train.Saver()` which would save `x`, `y`, `z` and `x/ExponentialMovingAverage`\r\n\r\nIf you print `variables_to_restore` I would expect that you get the following:\r\n```\r\n{'x/ExponentialMovingAverage': <tf.Variable 'x:0' shape=() dtype=float32_ref>, \r\n'y': <tf.Variable 'y:0' shape=() dtype=float32_ref>,\r\n'z': <tf.Variable 'z:0' shape=() dtype=float32_ref>}\r\n```\r\n\r\nWhat means that the variable `x` would restore its value from `x/EMA` instead of from `x` itself, while `y` and `z` would restore their values from themselves.\r\n\r\n\r\n", "If I can get `'y': <tf.Variable 'y:0' shape=() dtype=float32_ref>`, I won't ask this question.\r\nLook at  my first commit, you will also get `'y/ExponentialMovingAverage': <tf.Variable 'y:0' shape=() dtype=float32_ref>`\r\n\r\n", "If you do what I mentioned then you would get it, because now we are telling ema that we only want to use `x.ExponentialMovingAverage` but not the others by passing `tf.moving_average_variables()`\r\n\r\n```\r\nx = tf.Variable(12.,name='x')\r\ny = tf.Variable(34.,name='y')\r\nz = tf.Variable(0, trainable=False)\r\nema = tf.train.ExponentialMovingAverage(.0)\r\nema.apply([x])\r\nvariables_to_restore = ema.variables_to_restore(tf.moving_average_variables())\r\nprint(variables_to_restore)\r\n```\r\n```\r\n{u'y': <tf.Variable 'y:0' shape=() dtype=float32_ref>, u'Variable': <tf.Variable 'Variable:0' shape=() dtype=int32_ref>, u'x/ExponentialMovingAverage': <tf.Variable 'x:0' shape=() dtype=float32_ref>}\r\n```"]}, {"number": 11838, "title": "Merge rc1 into master", "body": "", "comments": []}, {"number": 11837, "title": "Branch 163409348", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 11836, "title": "'module' object has no attribute 'BasicLSTMCell'", "body": "Hello guys,\r\n\r\nI'm adopting a project of version 1.1.0 to version 1.2.1. Looks like some APIs have changed. I'm new to tensorflow, could someone tell what's the new APIs for those old ones below:\r\n\r\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl as rnn_cell\r\n\r\ncore_rnn_cell_impl doesn't exist, but I need BasicLSTMCell is in rnn_cell.\r\n\r\nThank you very much.\r\n\r\nEric", "comments": ["Please check here - https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11835, "title": "Ensure that TensorBoard is still available when pip installed.", "body": "Context:\r\n\r\nAfter r1.2, TensorBoard moved out of the TensorFlow repository, into its own\r\nrepository and its own pip package (presently tensorflow-tensorboard, will later\r\nswitch to just tensorboard). The new pip package specifies the `tensorboard`\r\ncommand, so I removed it from the list of console scripts forTensorFlow. I also\r\nadded tensorflow-tensorboard as a pip dependency.\r\n\r\nHowever, it turns out that the pip order of operations is:\r\n- install pip dependencies (thus getting tensorflow-tensorboard and the new\r\n  tensorboard command)\r\n- remove deprecated console scripts (thus erroneously removing the new pointer\r\n  to tensorboard)\r\n\r\nTo fix this, I returned the `tensorboard` console script to tensorflow's\r\nsetup.py, except it now references the tensorboard package rather than the\r\ntensorflow package. Thus, the console script declaration in tensorflow and\r\ntensorboard are identical. We can be confident that the tensorboard package is\r\navailable, because it is specified by the pip dependency.\r\n\r\nTest Plan:\r\n\r\n- Create a clean virtualenv.\r\n- pip install tensorflow < 1.3.\r\n- verify that the tensorboard command works properly\r\n- pip install tensorflow 1.3 using a pip package generated with this change\r\n- verify that the tensorboard command still works\r\n\r\nThis is the fix for master. I sent a separate PR to move this fix into r1.3", "comments": ["(Same comment as on #11384.)", "@tensorflow-jenkins test this please"]}, {"number": 11834, "title": "Ensure that TensorBoard is still available when pip installed. (r1.3)", "body": "Context:\r\n\r\nAfter r1.2, TensorBoard moved out of the TensorFlow repository, into its own\r\nrepository and its own pip package (presently tensorflow-tensorboard, will later\r\nswitch to just tensorboard). The new pip package specifies the `tensorboard`\r\ncommand, so I removed it from the list of console scripts forTensorFlow. I also\r\nadded tensorflow-tensorboard as a pip dependency.\r\n\r\nHowever, it turns out that the pip order of operations is:\r\n- install pip dependencies (thus getting tensorflow-tensorboard and the new\r\n  tensorboard command)\r\n- remove deprecated console scripts (thus erroneously removing the new pointer\r\n  to tensorboard)\r\n\r\nTo fix this, I returned the `tensorboard` console script to tensorflow's\r\nsetup.py, except it now references the tensorboard package rather than the\r\ntensorflow package. Thus, the console script declaration in tensorflow and\r\ntensorboard are identical. We can be confident that the tensorboard package is\r\navailable, because it is specified by the pip dependency.\r\n\r\nTest Plan:\r\n\r\n- Create a clean virtualenv.\r\n- pip install tensorflow < 1.3.\r\n- verify that the tensorboard command works properly\r\n- pip install tensorflow 1.3 using a pip package generated with this change\r\n- verify that the tensorboard command still works\r\n\r\nThis is the fix for r1.3. I will also send this same commit in a PR against master.", "comments": ["LGTM, but I'd wait for a Real TensorFlow Reviewer.", "You have to update unfortunately.", "Amit for 1.3 merge"]}, {"number": 11833, "title": "External leveldb link changed", "body": "table_format.txt was renamed to table_format.md", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11832, "title": "remove duplicated code", "body": "remove duplicated code, and merge two implements.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11831, "title": "No mention of how to use custom RunConfig with Estimator in the Estimator tutorial", "body": "On the tutorial of creating estimators using tf.contrib.learn there doesn't seem to be any mention of how to create your own RunConfig object in order to specify the configurations for an Estimator run. The configuration in particular I wanted to find was on how to write summaries after custom sized steps. I eventually found it in the RunConfig description, but I think it would be worthwhile to mention it in the tutorial.\r\n\r\nLink to the tutorial:\r\nhttps://www.tensorflow.org/extend/estimators\r\n\r\nLink to the RunConfig description:\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig\r\n\r\nI was wondering if the tutorial could be updated to show how to create your own RunConfig object and use it with the Estimator. \r\n", "comments": ["The API also does not specify that the `config` parameter to [`tf.contrib.learn.Estimator`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Estimator) should be a [`RunConfig`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig). Often a tutorial does not contain all the details of a class, but we should at least specify this in the API.\r\n\r\n@dr4b, @sandersk, thoughts?", "Related to this issue, I wonder if we should not try to update the tutorial to use the core Estimator API?\r\n\r\nIn general, are tutorials considered as contributions to Tensorflow?", "Hi all,\r\n\r\nAgree it would be worthwhile to mention RunConfig in the tutorial, and link to appropriate resources.  \r\n\r\nAlso agree that it would be good to update this tutorial to use core Estimator rather than tf.contrib.learn. I'll grab this issue and get to these changes as soon as I can.\r\n\r\nThanks,\r\nSanders", "Unassigning myself from this one, as others have taken over this tutorial.", "@sandersk can you re-assign to whomever has taken over the tutorial?  (Otherwise the butler will just randomly auto-assign, as it did to me :)", "@wolffg, can you get this issue assigned to the appropriate person?", "The TF page on check points tells you how to do run config for an estimator:\r\n[TensorFlow checkpoints](https://www.tensorflow.org/get_started/checkpoints#checkpointing_frequency)\r\n\r\nJust about to try it.", "The TensorFlow checkpoints link leads to a 404- https://www.tensorflow.org/get_started/checkpoints#checkpointing_frequency\r\n\r\nA redirect, rather than removing, would be nice. I still can't find any doc on how to use the RunConfig with an Estimator.", "@natalieharris \r\nI can't see anything wrong with the link I provided, or yours for that matter.", "Yes, this has now been fixed", "Nagging Assignee @wolffg: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yhu9 is this still an issue?", "Closing this, feel free to open a new one if any issues come up.", "@wt-huang The original issue stated:\r\n\r\n> The configuration in particular I wanted to find was on how to write summaries after custom sized steps.\r\n\r\nAnd indeed, in the new link [Checkpointing frequency](https://www.tensorflow.org/get_started/checkpoints#checkpointing_frequency), there is mention of the `save_checkpoints_secs` parameter, but not `save_summary_steps`.\r\n\r\nIt might be confusing because it is different from the parameter `every_n_iter` of `LoggingTensorHook`. So it would be nice to make a link from both these pages to `RunConfig`:\r\n\r\n- Guide: [Creating Custom Estimators](https://www.tensorflow.org/guide/custom_estimators)\r\n- Tutorial: [Build a Convolutional Neural Network using Estimators](https://www.tensorflow.org/tutorials/estimators/cnn) "]}, {"number": 11830, "title": "Fixes #11829: Faster to import tensorflow.contrib on Python 3", "body": "As described in #11829, a lot of time is spent doing `inspect.stack()` when importing `import tensorflow.contrib` with Python 3. (There does not seem to be any issue with Python 2.)\r\n\r\nBy replacing `_inspect.stack()` with only getting the current and previous frame and then getting the frame info for just the previous frame, this unnecessary overhead is removed.\r\n\r\nThe time to import tensorflow.contrib, as reported by `time python2/3 -c \"import tensorflow.contrib\"` before and after my commit:\r\n\r\nWhen|Python 2.7.13|Python 3.6.2|\r\n------|-------------|-------------|\r\nbefore|3.2|7.5|\r\nafter|3.1|3.3|\r\n\r\nThere are unit tests that would catch if the `decorator_name` would be incorrect after my change, specifically these tests:\r\n* testSetsDecoratorNameToFunctionThatCallsMakeDecoratorIfAbsent\r\n* testUnwrapReturnsDecoratorListFromOutermostToInnermost\r\n* testUnwrapBoundMethods\r\n\r\nAll Python unit tests passed (in Docker). I didn't find any issues when linting the changed file.", "comments": ["Can one of the admins verify this patch?", "I'm not an admin but I wrote the original code. When I wrote it, I saw both my approach and what @patrikerdes, but chose mine because it was simpler and not (that I observed!) a performance problem.\r\n\r\nIf this patch fixes real performance problems, and the tests pass, we should prefer it. \r\n\r\nThanks for the patch!", "@jart, can you take a look since you're also working on speeding up the tensorflow imports.", "@jart would you have time to look at this?", "This is still a problem in tensorflow 1.3.0. Could someone look at this patch? If more information is needed, I would be happy to supply it.", "@charlesnicholson could you take a look? That seems quite a bit faster.", "@drpngx I'm not authorized to merge the request, but I gave it my whole-hearted approval on July 27. @jart seems to not be writing back, so if she's busy or uninterested I'd suggest finding another approver: there's really no reason to leave this unmerged.", "Thanks @charlesnicholson , we can take it from here!\r\n\r\nJenkins, test this please."]}, {"number": 11829, "title": "Slow to import tensorflow.contrib with Python 3 because inspect.stack is slow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.5\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.2.0-2420-g2b4a0f9a4 1.3.0-rc0\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: 0.5.2-homebrew\r\n- **CUDA/cuDNN version**: CPU only build\r\n- **GPU model and memory**: CPU only build\r\n- **Exact command to reproduce**: time python3 -c \"import tensorflow.contrib\"\r\n\r\n### The problem\r\nDoing `import tensorflow.contrib` take 7.5 seconds on my machine when doing it with Python 3.6.2. With Python 2.7.13 it takes 3.2 seconds.\r\n\r\nInvestigating this revealed that a lot of time is spent in `_inspect.stack()` in the function `make_decorator` in  `python/util/tf_decorator.py`. The stack is inspected to find the name of the caller of the function. With Python2 `inspect.stack()` is fast, but with Python 3 each call to `inspect.stack()` take approximately 0.2 seconds and there are 23 calls made, which account for the difference in time between Python 2 and 3.\r\n\r\n### References\r\nKeras by default imports tensorflow.contrib when the Tensorflow backend is used. Therefore Keras is slow to import when using Python 3: https://github.com/fchollet/keras/issues/7408\r\n\r\nThere is a stackoverflow question referencing this issue: https://stackoverflow.com/questions/45093653/import-tensorflow-contrib-module-is-slow-in-tensorflow-1-2-1", "comments": ["Is this PR also useful?  https://github.com/tensorflow/tensorflow/pull/11919\r\nWaiting on this PR: https://github.com/tensorflow/tensorflow/pull/11830", "I use `TensorFlow` and `PythonTeX` to make reproducible dynamic reports. Every time I update the code chunk that uses `tf.contrib` I have to wait about 10 seconds till the module is imported. It's so annoying. Fix it, please or suggest temporary workaround to speed up this import.\r\n\r\nThere were some patches. So which version is improved?\r\n\r\nP.S. My Python version is 3.5.2, TensorFlow 1.3.0. And I wait from 15 to 30 seconds!", "@konstunn did you sync past #11829?", "@drpngx not sure. Should I install from sources? I used to install TensorFlow from PyPI.", "Ah, yes, you have to install from source and pull from head. Alternatively, you can use a binary that we distribute. We're releasing release candidates for 1.4, which is the next version. Closer to cutting edge are (assuming you have a GPU): https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/\r\nThere's a corresponding CPU version.", "Thanks, I've simply done: `$ sudo -H pip3 install tf-nightly` to install a nightly build from PyPI. Now `import tensorflow.contrib` takes 1.4 seconds to run. An order of magnitude better! Good job!", "Woohoo!\n\nOn Wed, Oct 11, 2017, 5:55 AM Konstantin Gorbunov <notifications@github.com>\nwrote:\n\n> Thanks, I've simply done: $ sudo -H pip3 install tf-nightly to install a\n> nightly build from PyPI. Now import tensorflow.contrib takes 1.4 seconds\n> to run. An order of magnitude better! Good job!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11829#issuecomment-335799259>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTGUVikgzkec_uEPtayawvxsycULks5srLrcgaJpZM4Ol6ZN>\n> .\n>\n"]}, {"number": 11828, "title": "Add missing grpc dependency to include the proto_util.h header", "body": "", "comments": ["@nlopezgi", "windows cmake test looks a bit concerning, seems related to protobuf change, but is that related?", "Doesn't seem like it could be related since it's a different build system.\r\n\r\nClicking the Rebuild button in Jenkins to try again.", "Looks like Windows passes now. Merging this to fix the issue."]}, {"number": 11827, "title": "Add more coverage of file_io methods in gcs_smoke tests.", "body": "Add more coverage of file_io methods in gcs_smoke tests.\r\n\r\nAlso adds some missing assertions in existing test cases.", "comments": []}, {"number": 11826, "title": "Adding disabling mechanism for plugins", "body": "This adds a scheme to allow 3rd party XLA drivers to disable python tests using the disabled-manifest mechanism already in existence.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@vrv @hawkinsp this is the replacement for the other test disabling mechanism\r\n", "@tensorflow-jenkins Test this please", "this is the same spectral ops test which is failing on all of the PRs.  I suspect it isn't anything to do with these changes.\r\n\r\n"]}, {"number": 11825, "title": "Why remove tolerate_dup_recv from LocaRendezvous.", "body": "I did not find a PR for this [patch](https://github.com/tensorflow/tensorflow/commit/cbfd50ff0f01e1825922230a8bc6e5766da98dd7). Maybe this discussion belongs to that particular commit page. If so, I will move it there.\r\n\r\nCan somebody explain the rationale behind this patch? Current contrib/verbs uses exactly this flag to transfer tensors. Note it is set true [here](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L34). Removing it totally breaks verbs. I am asking if this patch is absolutely needed since there is no simple workaround for contrib/verbs without this flag. ", "comments": ["/CC @zffchen78 ", "@junshi15 @reedwm @poxvoculi - How do we proceed here ? \r\nThe tolerate_dup_recv is needed by the verbs code.", "Can we revert [this patch](https://github.com/tensorflow/tensorflow/commit/cbfd50ff0f01e1825922230a8bc6e5766da98dd7)? Just looking at the patch alone, I feel it is cosmetic (cleaning up a parameter that is not used in the core code). Or maybe I missed something important.", "While we are looking for the motivation behind this change, we will also investigate how to adapt verbs code to accommodate this change.", "It was removed because we no longer needed it for our internal networking, where it was needed to deal with hardware errors in an earlier generation of machines.  Turning this feature on comes at a cost of higher memory use, since tensors backing sent values cannot be garbage collected until the end of the step.  Also, there may be some upcoming feature whose implementation was made more difficult by allowing this option.\r\n\r\nI should have noticed you were using this, and pinged you: sorry!  How important is this feature to verbs support?  It is a convenience, or is it not possible to make the service reliable without it?", "@poxvoculi This is a corner stone of the current verbs design, where the receiving side could make multiple Recv attempts. Since the pinned memory is shared, a single Recv attempt may not be successful if the shared memory is used by other transmission.\r\n\r\nBut no worries, let us see if we can do without it.", "@junshi15 You mentioned about shared pinned memory, and I am wondering if this could be resolved by increasing the amount of pinned memory permitted for multiple concurrent transmissions. For sake of easier implementation, a memory pool allowing growth may fit your needs, e.g. BFCAllocator or PoolAllocator.", "@byronyi Using more memory can alleviate memory access conflicts, but can not totally eliminate it, since we don't know a priori what is the max parallelism. Once a conflict happens, some kind of retry is needed. We have been relying on tolerate_dup_recv for the re-tries.", "@junshi15 \r\nMaybe I'm missing something, but I don't see where we have a shared pinned memory in the design.\r\nEach RdmaTensorBuffer (hashed by its name) has it's own pinned buffer on both sides.\r\nI thought the necessity of the tolerate_dup_recv in the verbs code is the RDMA_BUFFER_REQUEST (first Tensor transfer, change in tensor size) which will follow a second [RecvAsyncLocal](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L822).", "@shamoya Yes, there are three cases we have multiple RecvAsyncLocal. You listed the first two.\r\n\r\n1) initial transfer, both sides need to step up the buffer first, then transfer. Two calls to RecvAsyncLocal.\r\n\r\n2) change of tensor size. similar to (1).\r\n\r\n3) concurrent transfer. Note the buffer is hashed by the tensor name, without the step_id. If the same tensor with two different step_ids need to be transferred at the same time, there will be a collision at the buffer. When this happens, one tensor needs a re-try. I think I have seen this kind of situation in Async training cases, but I did not dig deeper.\r\n\r\n", "Thanks @junshi15 for explaining.\r\n\r\nSo, how do we go from here?\r\nAssuming we don't have duplicated recv support, we need to send the Tensor in one shot in all cases.\r\nMaybe we can revert to RDMA_SEND on those cases ?\r\n\r\nIdo", "@shamoya With my limited understanding of RDMA_SEND, I do not see how it can solve the problem.\r\nWith RDMA_SEND, the receiver specifies destination MR. But the MR needs to be created first. To create it, the receiver needs to know its size. The size is on the sender side. So it seems to me you still need to duplicate recv at least twice, first, get the size; second, send the tensor.\r\n\r\nCorrect me if I am wrong, my understanding of RDMA_SEND is that it requires the same ordering at both sender and receiver. If the sender post_send A, B and C, in that order, but the receiver post_recv B, C and A. Then A goes to B's buffer, B goes to C's buffer, C goes to A's buffer. I do not know if we can enforce the ordering strictly.\r\n\r\nI do not have a simple solution to work around duplicate_recv. One solution will be to buffer the tensors and its callback, if any, inside RDMARendezvous. We do local_recv once, if that tensor can not be sent for any reason, i.e. MR not initialized, size changed, buffer busy, etc, we will just make a full copy of the tensor and put it in a table inside RDMARendezvous. For future recv of the same tensor, we look it up here instead of going to that waiter-table. Basically, we need to duplicate that waiter-table inside RDMARendezvous. Not a clean solution, but maybe doable.\r\n\r\nWhat do you think?\r\n\r\n\r\n", "Thanks @junshi15 for the feedback.\r\nFor the first issue I thought about maybe having some kind of a tensor buffers pools (max size), and maybe use it for the above cases, but you're about the second issue (ordering) and overall this is a not a good plan.\r\n\r\nYour idea sounds good and maybe we could progress with it.\r\nLet me try to do a quick check and get back to you.", "Do you want me to get in on this?\r\nI was thinking of duplicating the waiter table into the Tensor-Buffer, instead of the rendezvous.\r\nSo the reaction to RDMA_MESSAGE_BUFFER_RESPONSE would be similar to SendNextItem() but would replace RecvLocalAsync with a lookup in it's local table.\r\nI think a seperate queue for items is also required for this to work.", "@yanivbl6 @junshi15 \r\nSince we are now buffer and the tensor (to be sent), need to think how to tell the device (in which the tensor reside) not to free the it (device->context->Ref() ) or we can already allocate the local buffer and copy it to there already ?", "@yanivbl6 Feel free to contribute. You may want to coordinate with @shamoya who has spent a lot time on this. I have been occupied by projects at work and won't have much time on this in the near future.\r\n@shamoya I think the waiter should handle reference counting. The original reference counting was wrong, as you pointed out. We need to fix this in the new patch.\r\n", "Thanks @junshi15 , me and @yanivbl6 are working together.\r\nBTW, I found what did the missing UnRef() we had in the tolerate_dup_recv - [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/rendezvous.cc#L268)\r\nAt the end of the step, someone is pulling all the remaining items in the table and deleting them.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This issue is addressed by https://github.com/tensorflow/tensorflow/pull/12705"]}, {"number": 11824, "title": "Add maxout op to tf.contrib.layers", "body": "Hey,\r\n\r\nI just wanted to activate this PR #5528 \r\n\r\n- Ported maxout layer from !5528\r\n- Improved documentation\r\n- Created test units\r\n\r\nCould someone please review this one?\r\n\r\nThanks", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (25/25 (100%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext3.iad.github.net ([192.30.252.194]:51275 helo=github-smtp2b-ext-cp1-prd.iad.github.net)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.89)\n\t(envelope-from <noreply@github.com>)\n\tid 1dcv2h-0005IT-Mo\n\tfor mazecreator@mazecreator.com; Wed, 02 Aug 2017 09:55:05 -0500\nDate: Wed, 02 Aug 2017 08:05:15 -0700\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1501686315;\n\tbh=FfdDhXGZL8lXJKdnq6ZnjpeEbvP7tAM16+QYP2tsirc=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=RsYfkylWFieyQ0qlV1smmKuhvHS4Knf3XA6wkeLVcp337RJroqZ3vyrglB+oYFAHv\n\t aEzEkRosppZ2m7G8s/E12M+CfpHWDsU8bgUuqdL0fVI9yVbmwceKfkBQ9yf1Mz54kY\n\t YrzrrKRREoQjacxaevf3hf74PbylUBh71cwyt2/0=\nFrom: Tiago Freitas Pereira <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/11824/issue_event/1189721598@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/11824@github.com>\nReferences: <tensorflow/tensorflow/pull/11824@github.com>\nSubject: Re: [tensorflow/tensorflow] Add maxout op to tf.contrib.layers\n (#11824)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5981ea2b3fc4a_22323fd4ec969c303779af\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: tiagofrepereira2012\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0e174358a31fa4c19e53ac716c2ad3eaf826f692992cf000000011599ac2b92a169ce0eaba4bd@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoKh-6QcONiZ1rVIU1XiwEhSJAptkks5sUJArgaJpZM4Olq41>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n\n----==_mimepart_5981ea2b3fc4a_22323fd4ec969c303779af\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nClosed #11824.\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/11824#event-1189721598\n----==_mimepart_5981ea2b3fc4a_22323fd4ec969c303779af\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>Closed <a href=\"https://github.com/tensorflow/tensorflow/pull/11824\" class=\"issue-link js-issue-link\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11824\" data-id=\"246129853\" data-error-text=\"Failed to load issue title\" data-permission-text=\"Issue title is private\">#11824</a>.</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/11824#event-1189721598\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoPsktuGHrbDjOJG3i3_snUsihn0aks5sUJArgaJpZM4Olq41\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoHbCZ4OOKUu5BQHX7d8DQ0vrrIC8ks5sUJArgaJpZM4Olq41.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/11824#event-1189721598\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"DESCRIPTION\",\"message\":\"Closed #11824.\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/11824#event-1189721598\"}}}</script>\n----==_mimepart_5981ea2b3fc4a_22323fd4ec969c303779af--\n", "Up to you, but I think smaller files are better for many reasons\n\nOn Wed, Aug 2, 2017 at 8:05 AM, Tiago Freitas Pereira <\nnotifications@github.com> wrote:\n\n> Closed #11824 <https://github.com/tensorflow/tensorflow/pull/11824>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11824#event-1189721598>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQTKazBTIO-xvMfS5bv0bHNvLRstks5sUJA9gaJpZM4Olq41>\n> .\n>\n\n\n\n-- \n - Alex\n", "Can one of the admins verify this patch?", "CLAs look good, thanks!\n\n<!-- ok -->", "So, the changes were made.\r\nPlease let me know if there's anything (I will be off for three weeks).  \r\nIn the future I will put in place this feature https://arxiv.org/abs/1511.02683", "Rebasing is a good idea.\r\n\r\nYour errors come from pylint and buildifier. I'll paste them here:\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n3699d3698\r\n<         \"layers/maxout.py\",\r\n3701a3701\r\n>         \"layers/maxout.py\",\r\n3793d3792\r\n< \r\n\r\nThis means your source files need to be sorted in the BUILD rule.\r\n\r\nThen the pylint errors are\r\n\r\npylint took 116 s\r\n\r\n\r\nFAIL: Found 39 non-whitelited pylint errors:\r\ntensorflow/python/layers/maxout_test.py:37: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout_test.py:38: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:39: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:40: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:42: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout_test.py:43: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:44: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:45: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:46: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:48: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout_test.py:49: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:50: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:51: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:52: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:54: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout_test.py:55: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:56: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:57: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout_test.py:58: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 6\r\n\r\ntensorflow/python/layers/maxout.py:31: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout.py:55: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout.py:59: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout.py:83: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout.py:88: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:90: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:91: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:93: [W0311(bad-indentation), ] Bad indentation. Found 4 spaces, expected 2\r\n\r\ntensorflow/python/layers/maxout.py:94: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:95: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:96: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:97: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:98: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 6\r\n\r\ntensorflow/python/layers/maxout.py:101: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:102: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:105: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:106: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 6\r\n\r\ntensorflow/python/layers/maxout.py:107: [W0311(bad-indentation), ] Bad indentation. Found 16 spaces, expected 8\r\n\r\ntensorflow/python/layers/maxout.py:108: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\ntensorflow/python/layers/maxout.py:110: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 4\r\n\r\nto fix these use 2-space indentation instead of 4-space indentation", "Thanks for the prints, I managed to solve the issues with pylint, but the sanity checks are still not green and the log is not very clear (http://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/6766/consoleFull).\r\n\r\nCan I have some help?\r\n\r\nThanks", "Add a blank line above line 3794 of tensorflow/python/BUILD to make buildifier happy.\r\n\r\nFor the py_test error I have no idea what it means, and the instruction to fix seems wrong to me. Let's make the other change and rerun the tests.", "All green :-)\r\n\r\nThanks for the help", "Jenkins, test this please.", "I observed that the failures in the tests are not related with my contribution. \r\n\r\nShall I do something?\r\n\r\nCheers", "Jenkins, test this please.", "Jenkins, test this please.\r\n\r\nThere were issues with the executors.", "Any news from this one?\r\n\r\nCan I help with something?", "Jenkins, test this please."]}, {"number": 11823, "title": "ERROR message when using tf.SyncReplicasOptimizer", "body": "I'm running distributed tensorflow with estimators, in order to it in sync mode I'm using tf.SyncReplicasOptimizer, but casually (specially after evaluation) I see the following error on the master:\r\n```\r\nERROR:tensorflow:==================================\r\nObject was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\r\n<tf.Tensor 'report_uninitialized_variables/boolean_mask/Gather:0' shape=(?,) dtype=string>\r\n\r\n['File \"cifar10_main.py\", line 538, in <module>\\n    tf.app.run()', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File \"cifar10_main.py\", line 518, in main\\n    hooks), run_config=config)', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 210, in run\\n    return _execute_schedule(experiment, schedule)', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 47, in _execute_schedule\\n    return task()', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 501, in train_and_evaluate\\n    hooks=self._eval_hooks)', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 681, in _call_evaluate\\n    hooks=hooks)', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 292, in evaluate\\n    name=name)', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 638, in _evaluate_model\\n    features, labels, model_fn_lib.ModeKeys.EVAL)', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 545, in _call_model_fn\\n    features=features, labels=labels, **kwargs)', 'File \"cifar10_main.py\", line 331, in _resnet_model_fn\\n    gradvars, global_step=tf.train.get_global_step())', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 252, in apply_gradients\\n    variables.global_variables())', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 170, in wrapped\\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 139, in _add_should_use_warning\\n    wrapped = TFShouldUseWarningWrapper(x)', 'File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 96, in __init__\\n    stack = [s.strip() for s in traceback.format_stack()]']\r\n==================================\r\n\r\n```\r\nCode available at:https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py\r\n", "comments": ["The [file](https://github.com/mari-linhares/models/blob/f24c44d5b94c516ac630110ca6f32eb8a3f023f9/tutorials/image/cifar10_estimator/cifar10_main.py) your pull request modified only appears to support single-process TensorFlow, not distributed TensorFlow.\r\n\r\nWhat command(s) did you run that caused the error?", "It's using experiments (tf.estimators) so it supports distributed TensorFlow. This Error happened after the master executed evaluation, and once the worker finished training. I executed this with 1 master, 1 worker and 1 ps.", "What commands did you run to start the master, worker, and ps?", "You need to have a TF_CONFIG environment variable in each machine, example for TF_CONFIG in the master:\r\n\r\n```python\r\ncluster = {'master': ['localhost:8000'],\r\n           'ps': ['localhost:8888'],\r\n           'worker': ['localhost:8009']}\r\n\r\nTF_CONFIG = json.dumps(\r\n  {'cluster': cluster,\r\n   'task': {'type': master, 'index': 0},\r\n   'model_dir': 'gs://<bucket_path>/<dir_path>',\r\n   'environment': 'cloud'\r\n  })\r\n```\r\nThen you can run the following script in all the machines.\r\n```shell\r\npython cifar10_main.py --run_experiment=True --num_workers=2 --train_batch_size=64 --num_gpus=1 --data_dir=<data_dir> --model_dir=gs://<bucket_path>/<dir_path> --sync=True\r\n```\r\n\r\n", "Thanks for the info, I did not know about TF_CONFIG. I was not able to reproduce locally by running 3 processes on my local machine, but I didn't try on separate machines. How long does it usually take before the error occurs? I ran for about 30 minutes.\r\n\r\nAlso, when running with 1 master, 1 worker, and 1 ps, did you set --num_workers=1 (it's set to 2 in your example above).\r\n\r\n@ali01 Can you look into this? I wonder if this is related to #11753.", "@reedwm you should be able to reproduce it locally... I think I missed the `--sync=True` flag, make sure you have it and you should see it, sorry.\r\n\r\n*Also, when running with 1 master, 1 worker, and 1 ps, did you set --num_workers=1 (it's set to 2 in your example above).*\r\n\r\nYes, the master counts as one worker as well since it also does training.\r\n\r\n\r\n", "Also, you don't need to run distributed to see the error, if you run locally with `--sync=True` you should be able to see it. Thanks!", "I also see this error sometimes (specially in the last \"saving checkpoints\" step):\r\n\r\n```\r\nINFO:tensorflow:Saving checkpoints for 10001 into gs://<path>\r\nINFO:tensorflow:Coordinator stopped with threads still running: Thread-2\r\n\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 254, in _run\r\n    coord.request_stop(e)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 211, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 238, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1063, in _single_operation_run\r\n    target_list_as_strings, status, None)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\nCancelledError: TakeGrad operation was cancelled\r\n\t [[Node: sync_replicas/AccumulatorTakeGradient_15 = AccumulatorTakeGradient[_class=[\"loc:@sync_replicas/conditional_accumulator_15\"], dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sync_replicas/conditional_accumulator_15, sync_replicas/AccumulatorTakeGradient_15/num_required)]]\r\n```\r\n", "I'm able to reproduce locally with `--sync=True`, by running the three terminal commands in three different terminal windows:\r\n```\r\nCUDA_VISIBLE_DEVICES= TF_CONFIG='{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"localhost:12347\"], \"ps\": [\"localhost:12346\"], \"master\": [\"localhost:12345\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}, \"model_dir\": \"/tmp/model\"}' python cifar10_main.py --run_experiment=True --num_workers=2 --train_batch_size=64 --num_gpus=0 --data_dir ~/cifar-10-batches-bin --model_dir=/tmp/model --is_cpu_ps=True --sync=True\r\n```\r\n```\r\nCUDA_VISIBLE_DEVICES= TF_CONFIG='{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"localhost:12347\"], \"ps\": [\"localhost:12346\"], \"master\": [\"localhost:12345\"]}, \"task\": {\"index\": 0, \"type\": \"ps\"}, \"model_dir\": \"/tmp/model\"}' python cifar10_main.py --run_experiment=True --num_workers=2 --train_batch_size=64 --num_gpus=0 --data_dir ~/cifar-10-batches-bin --model_dir=/tmp/model --is_cpu_ps=True --sync=True\r\n```\r\n```\r\nCUDA_VISIBLE_DEVICES= TF_CONFIG='{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"localhost:12347\"], \"ps\": [\"localhost:12346\"], \"master\": [\"localhost:12345\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}, \"model_dir\": \"/tmp/model\"}' python cifar10_main.py --run_experiment=True --num_workers=2 --train_batch_size=64 --num_gpus=0 --data_dir ~/cifar-10-batches-bin --model_dir=/tmp/model --is_cpu_ps=True --sync=True\r\n```\r\n\r\nI consistently get the \"Object was never used\" error on the master after the first evaluation.\r\n\r\nThe stack trace in the error message (from the [first comment](https://github.com/tensorflow/tensorflow/issues/11823#issue-246125815)) is hard to read since its printed as a Python list. The stack trace, with line breaks, is\r\n```\r\nFile \"cifar10_main.py\", line 538, in <module>\r\n    tf.app.run()\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\nFile \"cifar10_main.py\", line 518, in main\r\n    hooks), run_config=config)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 210, in run\r\n    return _execute_schedule(experiment, schedule)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 47, in _execute_schedule\r\n    return task()\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 501, in train_and_evaluate\r\n    hooks=self._eval_hooks)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 681, in _call_evaluate\r\n    hooks=hooks)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 292, in evaluate\r\n    name=name)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 638, in _evaluate_model\r\n    features, labels, model_fn_lib.ModeKeys.EVAL)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 545, in _call_model_fn\r\n    features=features, labels=labels, **kwargs)\r\nFile \"cifar10_main.py\", line 331, in _resnet_model_fn\r\n    gradvars, global_step=tf.train.get_global_step())\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 252, in apply_gradients\r\n    variables.global_variables())\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 170, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 139, in _add_should_use_warning\r\n    wrapped = TFShouldUseWarningWrapper(x)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 96, in __init__\r\n    stack = [s.strip() for s in traceback.format_stack()]\r\n```\r\n\r\nStill waiting for @ali01 to respond.", "Thank you for spending your time reproducing this @reedwm! Can you train again for 1 epoch and see if you can get the other Error I mentioned on the last comment?", "I was not able to reproduce the other error. I ran the three terminal commands above for 4000 steps, which I believe is several epochs. I did get the original \"Object was never used\" error after every evaluation, however.", "I see, thank you @reedwm.", "@mari-linhares have you ever fixed the \"Object was never used\" error?", "@pickou not really :/", "I also encountered a similar error when attempting to use SyncReplicasOptimizer for MNIST, and was wondering if any progress has been made on this issue. Thanks.", "I was able to reproduce but have know idea why the problem occurs. Someone needs to look at SyncReplicasOptimizer, but unfortunately most people who worked on it no longer work on TensorFlow.\r\n\r\n/CC @ispirmustafa @panyx0718, can either of you take a look?", "@mari-linhares, we apologize this slipped through the cracks. Is this still an issue for you, and are you interested in the solution?", "Hey @aselle, thank you for the response, I think it's a valuable fix considering that estimators are still the easiest way to run distributed tensorflow.", "/CC @josh11b do you know who should handle issues with SyncReplicasOptimizer?", "Nagging Assignees @reedwm, @josh11b: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "SyncReplicaOptimizer has been unsupported for some time and now is\nofficially deprecated.\n\nMany cases can now use MirroredStrategy or CollectiveAllReduceStrategy\ninstead.\n\n\n\nOn Sun, Nov 11, 2018, 10:38 AM Alfred Sorten Wolf <notifications@github.com\nwrote:\n\n> Nagging Assignees @reedwm <https://github.com/reedwm>, @josh11b\n> <https://github.com/josh11b>: It has been 29 days with no activity and\n> this issue has an assignee. Please update the label and/or status\n> accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11823#issuecomment-437693400>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOjT14FU_YNAU_e7xmJa72p52VlDQEovks5uuG6cgaJpZM4Olpx5>\n> .\n>\n", "Closing, since SyncReplicaOptimizer is no longer maintained.", "Is the fact that SyncReplicasOptimizer is deprecated documented somewhere? Certainly not in the API doc https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer.", "@guoshimin  you can check the deprecation message here at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer.py\r\n\r\nI think this is a quite recent change", "Can anyone suggest a workaround of this case? What would be the best replace of SyncReplicasOptimizer? I'm running inception from research repo and I'm facing the same issue when running distributed tensorflow.", "The recommended approach is to use tf.distribute.*Strategy (such as MirroredStrategy), but I have not personally tested it with Inception.", "Thank you @josh11b. Can you suggest a distributed learning example where mirror strategy and/or parameterserver strategy is used? I would like to look at some cases before try on inception.", "@josh11b Also, is there an easier workaround if I turn it into asynchronized parameter server by discarding the SyncReplicasOptimizer?", "Turning off SyncReplicasOptimizer should give you regular parameter server behavior. You can see our distributed training guide at https://www.tensorflow.org/guide/distribute_strategy"]}, {"number": 11822, "title": "tfcompile of tf.sin and tf.cos", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7.12\r\n\r\n### Describe the problem\r\nI tried to use tfcompile for AOT compilation of a graph where are use tf.sin and tf.cos functions.\r\ntfcompile raises following error:\r\n```\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Not found: No registered 'Sin' OpKernel for XLA_CPU_JIT devices compatible with node Sin = Sin[T=DT_FLOAT](alpha/read)\r\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[Node: Sin = Sin[T=DT_FLOAT](alpha/read)]]\r\nF tensorflow/compiler/aot/tfcompile_main.cc:140] Check failed: ::tensorflow::Status::OK() == (tensorflow::tfcompile::Main(flags)) (OK vs. Not found: No registered 'Sin' OpKernel for XLA_CPU_JIT devices compatible with node Sin = Sin[T=DT_FLOAT](alpha/read)\r\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[Node: Sin = Sin[T=DT_FLOAT](alpha/read)]])\r\n```\r\n", "comments": ["It looks like xla doesn't support double size complex #'s. Does this happen when you use tf.complex64 instead?\r\n@tatatodd, is anyone working on extending complex support in xla?", "This happens when I use any of the float data types.\r\nUsing tf.complex64 or tf.complex128 raises:\r\n\r\n```\r\nF tensorflow/compiler/aot/tfcompile_main.cc:140] Check failed: ::tensorflow::Status::OK() == (tensorflow::tfcompile::Main(flags)) (OK vs. Invalid argument: Unsupported type in DataTypeToPrimitiveType complex64)\r\n```\r\n```\r\nF tensorflow/compiler/aot/tfcompile_main.cc:140] Check failed: ::tensorflow::Status::OK() == (tensorflow::tfcompile::Main(flags)) (OK vs. Invalid argument: Unsupported type in DataTypeToPrimitiveType complex128)\r\n```", "It haapens with tf.float32 as well? Or do you mean it happens for any complex types?", "FYI I believe sin/cos for floating types should be supported now, but I don't know that complex types are currently supported by the tf2xla bridge.", "You are right. I just rebuild tfcompile with the current version of the code.\r\nsin/cos for floating types is supported now.\r\n\r\nThe complex types are not supported:\r\n```\r\nINVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType complex64\r\n```"]}, {"number": 11821, "title": "Gradient for self_adjoint_eigvals fails (self_adjoint_eig works fine)", "body": "As also noted in #1915 already (but that ticket had been closed), there is a bug in the implementation of `self_adjoint_eigvals` (whereas `self_adjoint_eig` works fine, but it is rather inefficient to compute eigenvectors and their gradients if I only require the eigenvalues!).\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no - stock usage of `tf.self_adjoint_eigvals` - minimal failing example below\r\n- **OS Platform and Distribution**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: v1.2.1-2-gc996c7b\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.5.2 (binary install)\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GeForce GTX 1070 with 8 GB memory\r\n- **Exact command to reproduce**: see below\r\n\r\n## Minimal failing example:\r\n```python\r\na = tf.random_normal((10000,3))\r\ncovar = tf.matmul(tf.transpose(a), a)\r\neigvals, eigvect_holder = tf.self_adjoint_eig(covar)\r\neigvects = tf.transpose(eigvect_holder)\r\npure_eigvals = tf.self_adjoint_eigvals(covar)\r\ntf.Session().run(tf.gradients(eigvals, a))  # works fine\r\ntf.Session().run(tf.gradients(pure_eigvals, a))  # throws the error below\r\n```\r\n\r\n### Error message:\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    340     try:\r\n--> 341       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    342       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   1666       raise ValueError(\"No attr named '\" + name + \"' in \" +\r\n-> 1667                        str(self._node_def))\r\n   1668     x = self._node_def.attr[name]\r\n\r\nValueError: No attr named '_XlaCompile' in name: \"SelfAdjointEigV2_16\"\r\nop: \"SelfAdjointEigV2\"\r\ninput: \"MatMul_5\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"compute_v\"\r\n  value {\r\n    b: false\r\n  }\r\n}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\r\n    670           graph_def_version, node_def_str, input_shapes, input_tensors,\r\n--> 671           input_tensors_as_shapes, status)\r\n    672   except errors.InvalidArgumentError as err:\r\n\r\n/usr/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInvalidArgumentError: Shape must be rank 2 but is rank 1 for 'gradients_4/SelfAdjointEigV2_16_grad/MatMul' (op: 'MatMul') with input shapes: [0], [0].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-53-c60afbcdc03b> in <module>()\r\n      5 pure_eigvals = tf.self_adjoint_eigvals(covar)\r\n      6 tf.Session().run(tf.gradients(eigvals, a))\r\n----> 7 tf.Session().run(tf.gradients(pure_eigvals, a))\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    538                 # functions.\r\n    539                 in_grads = _MaybeCompile(\r\n--> 540                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n    541               else:\r\n    542                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    344       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    345     except ValueError:\r\n--> 346       return grad_fn()  # Exit early\r\n    347 \r\n    348   if not xla_compile:\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>()\r\n    538                 # functions.\r\n    539                 in_grads = _MaybeCompile(\r\n--> 540                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n    541               else:\r\n    542                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/linalg_grad.py in _SelfAdjointEigV2Grad(op, grad_e, grad_v)\r\n    203           math_ops.matmul(\r\n    204               array_ops.matrix_diag(grad_e) + f * math_ops.matmul(\r\n--> 205                   v, grad_v, adjoint_a=True),\r\n    206               v,\r\n    207               adjoint_b=True))\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\r\n   1814     else:\r\n   1815       return gen_math_ops._mat_mul(\r\n-> 1816           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n   1817 \r\n   1818 \r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py in _mat_mul(a, b, transpose_a, transpose_b, name)\r\n   1215   \"\"\"\r\n   1216   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\r\n-> 1217                                 transpose_b=transpose_b, name=name)\r\n   1218   return result\r\n   1219 \r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py in apply_op(self, op_type_name, name, **keywords)\r\n    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    766                          input_types=input_types, attrs=attr_protos,\r\n--> 767                          op_def=op_def)\r\n    768         if output_structure:\r\n    769           outputs = op.outputs\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   2506                     original_op=self._default_original_op, op_def=op_def)\r\n   2507     if compute_shapes:\r\n-> 2508       set_shapes_for_outputs(ret)\r\n   2509     self._add_op(ret)\r\n   2510     self._record_op_seen_by_control_dependencies(ret)\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in set_shapes_for_outputs(op)\r\n   1871       shape_func = _call_cpp_shape_fn_and_require_op\r\n   1872 \r\n-> 1873   shapes = shape_func(op)\r\n   1874   if shapes is None:\r\n   1875     raise RuntimeError(\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in call_with_requiring(op)\r\n   1821 \r\n   1822   def call_with_requiring(op):\r\n-> 1823     return call_cpp_shape_fn(op, require_shape_fn=True)\r\n   1824 \r\n   1825   _call_cpp_shape_fn_and_require_op = call_with_requiring\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py in call_cpp_shape_fn(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\r\n    608     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\r\n    609                                   input_tensors_as_shapes_needed,\r\n--> 610                                   debug_python_shape_fn, require_shape_fn)\r\n    611     if not isinstance(res, dict):\r\n    612       # Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\r\n\r\n/home/stj/pio/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\r\n    674       missing_shape_fn = True\r\n    675     else:\r\n--> 676       raise ValueError(err.message)\r\n    677 \r\n    678   if missing_shape_fn:\r\n\r\nValueError: Shape must be rank 2 but is rank 1 for 'gradients_4/SelfAdjointEigV2_16_grad/MatMul' (op: 'MatMul') with input shapes: [0], [0].\r\n```\r\n\r\nThanks!", "comments": ["@rmlarsen can you take a look? The line [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L207) checks if `grad_v` is None, but for `tf.self_adjoint_eigvals`, `grav_v` will be a tensor of shape `[0]`, not None.", "Any updates on this?", "Oops, sorry for not noticing this! Let me take a look.", "I found the problem. Submitting a fix for review now. Should be on Github in a few days.", "The fix was submitted internally at Google and should be pushed to Github by tomorrow at the latest. Closing."]}, {"number": 11820, "title": "Added SNAPPY support in CMake scripts", "body": "Windows CMake builds do not include SNAPPY compression library support, thus the resulting applications are not able to parse TensorFlow models that were compressed with SNAPPY.\r\nThis pull request adds SNAPPY as ExternalProject, just as it's done with all other 3rd party libraries.\r\n\r\nUnfortunately I cannot verify this on a Linux machine right now, so option is disabled for Linux by default.\r\n\r\nThere is a relevant Stackoverflow question: https://stackoverflow.com/questions/45304986/cant-read-saved-tensorflow-model-failed-to-seek-to-header-entry\r\n\r\nExample command line to build snappy as a part of TF:\r\n```\r\ncmake .. -G\"Visual Studio 15 2017 Win64\" -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_ENABLE_SNAPPY_SUPPORT=ON\r\nMSBuild /p:PreferredToolArchitecture=x64 /p:Configuration=Release snappy.vcxproj\r\n```\r\n\r\nThis is my first contribution, so I apologize in advance for any guidelines I might have violated. Feedback is appreciated \ud83d\udc4d", "comments": ["Can one of the admins verify this patch?", "@saxenasaurabh Saurabh, could you take a look at this, please?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Google bot, please test CLA once again.", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please.", "Jenkins, test this please", "This breaks the windows cmake build. Mind taking a look at the failure?", "Changed a few things as per discussion above with @mrry \r\n", "Jenkins, test this please (can I do that? :) )", "Looks good to me!\r\n\r\n@tensorflow-jenkins test this please.", "@mrry Looks like the tests were terminated abruptly. I hope it's a glitch of a build system?", "Looks like it, yes. Annoyingly, it seemed to be doing well on the Windows build. Let's try again!\r\n\r\n@tensorflow-jenkins, test this please.", "@tensorflow-jenkins test this please."]}, {"number": 11819, "title": "Cmake support snappy", "body": "Windows CMake builds do not include SNAPPY compression library support, thus the resulting applications are not able to parse TensorFlow models that were compressed with SNAPPY.\r\nThis pull request adds SNAPPY as ExternalProject, just as it's done with all other 3rd party libraries.\r\n\r\nUnfortunately I cannot verify this on a Linux machine right now, so option is disabled for Linux by default.\r\n\r\nThere is a relevant Stackoverflow question: https://stackoverflow.com/questions/45304986/cant-read-saved-tensorflow-model-failed-to-seek-to-header-entry\r\n\r\nThis is my first contribution, so I apologize in advance for any guidelines I might have violated. Feedback is appreciated \ud83d\udc4d ", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Seems like I used different emails for my commits and for the Contributor License Agreement. Let me close this pull request and create another one, it'll be easier."]}, {"number": 11818, "title": "cxx", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}]