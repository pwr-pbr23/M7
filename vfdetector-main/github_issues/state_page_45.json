[{"number": 44789, "title": "TF Lite Micro : Hosted Model: \"Didn't find op for builtin opcode 'SQUEEZE'\"", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS, Programming via PlatformIO\r\n- TensorFlow installed from (source or binary): Followed Hello-World example, copied TF Lite dependencies into PlatformIO project\r\n- Tensorflow version (commit SHA if source): Downloaded today ; aab9c69b692904df384f0e2a1b4f31cfc21fee42\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32\r\n\r\n**Describe the problem**\r\nI have setup a PlatformIO project, based on the Hello World example, that simply loads in model.cc. \r\n\r\nThe project runs fine for the supplied model, but replacing it with **mobilenet_v1_0.25_128 (hosted tf lite model)** results in the following error when `AllocateTensors()` is called:\r\n\r\n```Calling AllocateTensors\r\nDidn't find op for builtin opcode 'SQUEEZE' version '1'\r\n\r\nFailed to get registration from op code SQUEEZE\r\n \r\nFailed starting model allocation.\r\n\r\nAllocateTensors() failed\r\n```\r\n\r\n Any help would be appreciated!\r\n\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nSimply deployed the project I defined in PlatformIO, that was previously working with a different model. My main class looks like this, using `g_model` defined within `model.cc`: \r\n\r\n```#include \"tensorflow/lite/micro/all_ops_resolver.h\"\r\n#include \"constants.h\"\r\n#include \"model.h\"\r\n#include \"output_handler.h\"\r\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n#include \"tensorflow/lite/schema/schema_generated.h\"\r\n#include \"tensorflow/lite/version.h\"\r\n#include <Arduino.h>\r\n\r\n// Globals, used for compatibility with Arduino-style sketches.\r\n//namespace {\r\ntflite::ErrorReporter* error_reporter = nullptr;\r\nconst tflite::Model* model = nullptr;\r\ntflite::MicroInterpreter* interpreter = nullptr;\r\nTfLiteTensor* input = nullptr;\r\nTfLiteTensor* output = nullptr;\r\nint inference_count = 0;\r\n\r\n// Create an area of memory to use for input, output, and intermediate arrays.\r\n// Minimum arena size, at the time of writing. After allocating tensors\r\n// you can retrieve this value by invoking interpreter.arena_used_bytes().\r\nconst int kModelArenaSize = 754;\r\n// Extra headroom for model + alignment + future interpreter changes.\r\nconst int kExtraArenaSize = 554 + 16 + 100;\r\nconst int kTensorArenaSize = 9*1024; //kModelArenaSize + kExtraArenaSize;\r\nuint8_t * tensor_arena;\r\n//}  // namespace\r\n\r\n// The name of this function is important for Arduino compatibility.\r\nvoid setup() {\r\n  tensor_arena = (uint8_t*) heap_caps_calloc(kTensorArenaSize, 1, MALLOC_CAP_SPIRAM | MALLOC_CAP_8BIT);\r\n  //tensor_arena = (uint8_t*) heap_caps_calloc(kTensorArenaSize, 1, MALLOC_CAP_SPIRAM | MALLOC_CAP_8BIT);\r\n  // Set up logging. Google style is to avoid globals or statics because of\r\n  // lifetime uncertainty, but since this has a trivial destructor it's okay.\r\n  // NOLINTNEXTLINE(runtime-global-variables)\r\n  static tflite::MicroErrorReporter micro_error_reporter;\r\n  error_reporter = &micro_error_reporter;\r\n\r\n  // Map the model into a usable data structure. This doesn't involve any\r\n  // copying or parsing, it's a very lightweight operation.\r\n  model = tflite::GetModel(g_model);\r\n  if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n    TF_LITE_REPORT_ERROR(error_reporter,\r\n                         \"Model provided is schema version %d not equal \"\r\n                         \"to supported version %d.\",\r\n                         model->version(), TFLITE_SCHEMA_VERSION);\r\n    return;\r\n  }\r\n\r\n  // This pulls in all the operation implementations we need.\r\n  // NOLINTNEXTLINE(runtime-global-variables)\r\n  static tflite::AllOpsResolver resolver;\r\n\r\n  // Build an interpreter to run the model with.\r\n  static tflite::MicroInterpreter static_interpreter(\r\n      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n  interpreter = &static_interpreter;\r\n\r\n  TF_LITE_REPORT_ERROR(error_reporter, \"Calling AllocateTensors\");\r\n  // Allocate memory from the tensor_arena for the model's tensors.\r\n  TfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n  if (allocate_status != kTfLiteOk) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, \"AllocateTensors() failed\");\r\n    return;\r\n  }\r\n  TF_LITE_REPORT_ERROR(error_reporter, \"Successful Allocation\");\r\n\r\n  // Obtain pointers to the model's input and output tensors.\r\n  input = interpreter->input(0);\r\n  output = interpreter->output(0);\r\n\r\n  // Keep track of how many inferences we have performed.\r\n  inference_count = 0;\r\n}\r\n\r\n// The name of this function is important for Arduino compatibility.\r\nvoid loop() {\r\n  delay(10);\r\n}\r\n```\r\n\r\n", "comments": ["Can you please refer existing examples and add those OPS needed by your model?\r\n\r\nThis piece is from main_functions.cc of person_detection example:\r\n\r\n```\r\n  static tflite::MicroMutableOpResolver<3> micro_op_resolver;\r\n  micro_op_resolver.AddAveragePool2D();\r\n  micro_op_resolver.AddConv2D();\r\n  micro_op_resolver.AddDepthwiseConv2D();\r\n```", "@vikramdattu OK I will try manually adding operations. I thought that this wasn't necessary when using AllOpsResolver?\r\n\r\nIn my code:\r\n\r\n``` static tflite::AllOpsResolver resolver;```", "@EliZucker, you're right. Using ```MicroMutableOpResolver``` is an optimization to reduce the footprint. ```AllOpsResolver``` is sufficient if you're prototyping.\r\n\r\nThe underlying issue here is that TfLite Micro supports a subset of the Ops that TfLite supports and Squeeze is on of the (currently) unsupported ops.\r\n\r\nThe entire list can be seen [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels).\r\n\r\nChanging the label of this github issue to feature request since the resolution would be to add a reference implementation of Squeeze to TFLM.\r\n"]}, {"number": 44776, "title": "Efficient Ragged Tensor support on TFlite", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0.dev20201106\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n![image](https://user-images.githubusercontent.com/26551254/98852580-36a13f00-249b-11eb-8fb6-1c8147ed8fda.png)\r\nWhile tf.RaggedTensor use int64 tensor as its row_splilts, TFLite does not support int64 indexing or slicing.\r\nTherefore the model using RaggedTensor as input is converted like the figure above, using unnecessary flexStridedSlices.\r\nFurther, I don't understand why RaggedTensor operation should be converted into whole lot of StridedSlices.\r\nIt should be fused into single operation for efficiency.\r\nSo I argue that RaggedTensor support on TFLite should be revised.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nThe one who use RaggedTensor for their TFLite model. For example, the one who are developing natural language processing models.\r\n\r\n**Any Other info.**\r\n", "comments": ["Can you share an example how RaggedTensor is used in your case?\r\n\r\nThere's no concept of RaggedTensor in execution graph, they are represented as a tensor of values and a tensor with indices.\r\n\r\nWe have made some effort to make a good support on NLP models when using ragged tensors, but may not be enough.\r\nIt would be good to collect more use cases to have a better understanding on what should be covered.\r\n\r\nMeanwhile, you can use flex delegate to make it run just like in tensorflow, when complete tflite support is missing.\r\n\r\n", "I use some ragged tensor indexing and slicing \r\nbecause each layer in my model only require subset of hidden features from the previous layer"]}, {"number": 44773, "title": "EfficientNet loading slowly from SavedModel format", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04.5 LTS** (see colab notebook for details)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below):  **v2.3.0-0-gb36436b087 2.3.0**\r\n- Python version: **3.6.9**\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\nWhen saving an instance of the EfficientNet to disk in SavedModel format and loading the same model again the loading routine takes (depending on the variation of EfficientNet) ~20 to **~120** seconds. As (please correct me if I am mistaken here) the SavedModel format is supposed to replace the frozen graphs in TensorFlow 2.0 I assume that this format is encouraged to deploy models for inference using a GPU and should be loadable quickly. It especially seems strange to me that **loading the model takes orders of magnitude longer than building it from scratch**. Please see the exact loading times for the EfficientNet architectures in [this colab notebook](https://colab.research.google.com/gist/soyers/49c40641d97e91aa3191733f123ac799/efficientnetloadingtimessavedmodel.ipynb).\r\nAs an alternative I had a look at the TensorFlow Lite format which, sadly, does not feature support for NVidia GPUs. My investigation on the loading times, however, show that EfficientNet models in TensorFlow Lite format are loaded much faster (<<< 1 second) than from SavedModel format. Please see [this colab notebook](https://colab.research.google.com/gist/soyers/f54d60badf1374349222c3ae9b7481c2/efficientnetloadingtimestflite.ipynb) for TFLite loading times.\r\nI would appreciate any advice here as the loading times are essential in our use cases. Thank you!\r\n\r\n**Describe the expected behavior**\r\nThe model is created at least comparably fast from SavedModel format as it takes to build it from scratch.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/soyers/49c40641d97e91aa3191733f123ac799/efficientnetloadingtimessavedmodel.ipynb\r\n", "comments": ["@soyers,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/05566ff6fc5f8ff8b58eeb5a59d79507/44773.ipynb). However with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/a85e031c468e60e05f89e2be2f91e13b/44773-tf-nightly.ipynb#scrollTo=839rdnL-ewIT), the load time is reduced by half. Please check the attached gist for reference. Thanks!", "Hi @amahendrakar,\r\nthank you for your efforts! I am glad that the coming realease of TensorFlow will introduce significant improvements in this respect.\r\nIt still strikes me a bit that even in the nightly build the loading time from SavedModel is ~5x as long as building the model from scratch.\r\nMaybe there are further improvements prossible, but I guess that's the way it is currently. Are there any for improvements for inference (maybe an option to export a frozen graph as SavedModel) such that low latency inference with GPUs is possible as it was in TF 1?\r\n\r\nThanks again for having a look and sharing your insights! I would be very grateful about an answer to the question above but apart from that please feel free to close this issue!", "I was able to reproduce the issue with [TF 2.8](https://colab.research.google.com/gist/chunduriv/d91131a5d83fe89b38f75147bf998c3c/44773.ipynb). Please check the attached gist for reference. Thanks!"]}, {"number": 44752, "title": "Loading images with non-inferred labels", "body": "**System information**\r\n- OS: macOs Catalina\r\n- Device: MacBook Pro 2017\r\n- GPU: Radeon Pro 555 2 GB & Intel HD Graphics 630 1536 MB\r\n- Memory: 16 GB 2133 MHz LPDDR3\r\n- Python Version: 3.7.9\r\n\r\n`tf.version.GIT_VERSION: v2.3.0-54-gfcc4b966f1`\r\n`tf.version.VERSION: 2.3.1`\r\n\r\n**Current behavior**\r\n`image_dataset_from_directory()` seemingly ignores `labels` argument\r\n\r\n**Expected behavior**\r\nWhen running `train_ds = image_dataset_from_directory(my_dir, labels=my_label_list)` with `my_label_list` containing integer labels with the exact number of elements as the number of images in `my_dir` (and in alphanumerically correct order with relation to the image filenames) I would expect `train_ds.class_names` to consist of the integer labels passed.\r\n\r\nFrom the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) I would expect the directory structure to be ignored since I have supplied the `labels` parameter. However, images are only loaded when I nest them in a sub-directory (in this example named `images/`). This sub-directory is then assigned as the sole class label.\r\n\r\n```python\r\n>>> print(train_ds.class_names)\r\n['images']\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSome sample images [here](https://drive.google.com/file/d/1msGXyy-8aDyeqPmsRCfeWiS3iAbzLLMm/view?usp=sharing) and follow this directory structure:\r\n```\r\ntest_dir/\r\n\u2514\u2500\u2500\u2500images/\r\n```\r\nThen run the following example:\r\n```python\r\nimage_dir = '/foo/bar/test_dir/'\r\nlabel_list = [1,2,3,1,1]\r\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n    image_dir,\r\n    labels=label_list,\r\n    validation_split=0.2,\r\n    subset='training',\r\n    shuffle=False,\r\n)\r\nprint(train_ds.class_names)\r\n```", "comments": ["I have tried in colab with TF version 2.3, nightly version and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e4b12d66766306aae5502909fb776a67/untitled511.ipynb).Thanks!", "The `class_names` it is temporary fixed at https://github.com/tensorflow/tensorflow/pull/44769 but we could talk in that PR about the behavior that we want for directory/subdirectory walking of `image_paths`.", "The PR fixed another issue", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/a9e7c6ba293a3b0d2d949a8c45d47a1f/44752.ipynb). Thanks!"]}, {"number": 44737, "title": "Discussion of Apollo3 TFLu PRs", "body": "@tensorflow/micro\r\n\r\nHi @advaitjain, or whoever it may concern.\r\n\r\nThank you for the [explanation closing my last PR](https://github.com/tensorflow/tensorflow/pull/43831). I'd like to restructure and improve those features so that they can be accepted into the project. From what I understand there are two main concerns:\r\n\r\n* keeping PRs focused to one issue/feature each\r\n* trying to deduplicate code for examples\r\n\r\nThese should be fairly easy to do. I propose three PRs:\r\n\r\n1. Fix existing issues with Apollo3 EVB performance\r\n2. Update underlying AmbiqSuite SDK version to 2.5.1\r\n3. Add targets for additional SFE Apollo3 based boards \r\n\r\nIn the last PR I initiated deduplication efforts for the additional boards. Generally it went well but I did have issues with one target in particular. I may reach out for some guidance on that when the time comes. \r\n\r\nPlease, let me know if this approach is acceptable.\r\n\r\nThank you,\r\noclyke\r\n\r\n", "comments": ["Thanks @oclyke for making this issue.\r\n\r\nBreaking up the PRs into smaller chunks is much appreciated. In addition, let me share the desired end state and some additional PRs that I think we will need here.\r\n\r\n### Desired End State\r\nWe want to consolidate all the Apollo3 based boards into a single `TARGET` with an additional specifier for the specific `BOARD`.\r\n\r\nWhat this means is that the invocations should be:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=apollo3 BOARD=sparkfun_edge\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=apollo3 BOARD=apollo3evb\r\n```\r\n\r\nThis is in contrast to the current approach of having a separate `TARGET` for `sparkfun_edge`, `apollo3evb` and more.\r\n\r\nWhat this also means is that there will be many more common files, including:\r\n * micro/apollo3/micro_time.cc (instead of `sparkfun_edge/micro_time.cc` and `apollo3evb/micro_time.cc` etc.)\r\n * micro/apollo3/debug_log.cc\r\n * micro/examples/micro_speech/apollo3/audio_provider.cc\r\n * etc.\r\n\r\nAny places where there are differences between the boards (for example the audio_provider) will be handled via ifdefs and helper functions, as needed. \r\n\r\n### Next steps\r\n\r\n * Let's continue the conversation on this issue until we are aligned on where we want to end up.\r\n * As a first step, even before #44740, let's consolidate sparkfun_edge and apollo3evb along the lines of what I describe above with the following PRs (so that we can avoid breaking things):\r\n   1. make a directory tensorflow/lite/micro/apollo3 add a README with some context and you listed as a maintainer (along the lines of [this readme](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels/arc_mli#embarc-mli-library-based-optimizations-of-tensorflow-lite-micro-kernels-for-arc-platforms)).\r\n    1. make a new apollo3 makefile target with debug_log, micro_time etc.\r\n    1. Add that target to [test_all.sh](https://github.com/tensorflow/tensorflow/blob/07b9eccccfc13e687a1de4ad08cd49c8954236a5/tensorflow/lite/micro/tools/ci_build/test_all.sh) similar to [test_sparkfun.sh](https://github.com/tensorflow/tensorflow/blob/07b9eccccfc13e687a1de4ad08cd49c8954236a5/tensorflow/lite/micro/tools/ci_build/test_sparkfun.sh)\r\n    1. Individual PRs that make the examples work for sprakfun_edge with `TARGET=apollo3 BOARD=sparkfun_edge`\r\n    1. Individual PRs that make the examples work for apollo3evb with `TARGET=apollo3 BOARD=apollo3vb` -- this step will mean adding in #ifdefs and some refactoring of the code from the previous step (and might encompass some of the changes from #44740).\r\n    1. Delete the existing makefiles and code in the sparkfun_edge and apollo3evb directories.\r\n\r\n * Then start sending PRs to support the new boards ...\r\n\r\nLet me know what you think. Since this is a fairly significant refactor that I am requesting, I'm happy to get on a video call to discuss this further.\r\n\r\nTagging @petewarden and @njeffrie in case they have any additional comments.", "@advaitjain thanks for working with me on this. i've completed steps i-iii and vi in #44309 \r\n\r\nto distinguish between boards i added ```-DTFLU_APOLLO3_BOARD_$(BOARD)``` to the PLATFORM_FLAGS. when ```BOARD``` is changed on the command line it is necessary to use files that have been compiled with that option, therefore the GENDIR is specialized in the main [Makefile](https://github.com/tensorflow/tensorflow/pull/44930/files#diff-84c8d7e242143d674d6e26ab56443970ed8e1f3db0fcb1d0e8e127eaebbe1a1a)."]}, {"number": 44714, "title": "LeakSanitizer: detected memory leaks", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):libtensorflow.so\r\n- TensorFlow version (use command below):\r\n- Python version: C++\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version:  10.1\r\n- GPU model and memory: nvidia 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nwhen inference in C++, LeakSanitizer detected memory leaks, as follow:\r\n![image](https://user-images.githubusercontent.com/73547638/98616400-b44c3a00-2337-11eb-9501-c15c34c9420a.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nNo memory leaks\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@zhangsanfeng86 \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Also, please post text of errors and warnings and so on instead of pictures. Helps with copy-paste, searchability and deduplication.", "@mihaimaruseac  Sorry for late reply!\r\nWhen load libtensorflow.so and libtensorflow_framework.so in C++,  LeakSanitizer detected memory leaks,as follow:\r\n\r\n```console\r\n==33901==ERROR: LeakSanitizer: detected memory leaks\r\n\r\nDirect leak of 24 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7fb65b997448 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe0448)\r\n    #1 0x7fb6510f9e4a in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc81e4a)\r\n    #2 0x7fb6510e8a7e in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc70a7e)\r\n    #3 0x7fb64e6d6984 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x108b984)\r\n    #4 0x7fb658df910d in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0x898110d)\r\n    #5 0x7fb64e6c6855 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x107b855)\r\n    #6 0x7fb64dd8e2fc in tensorflow::(anonymous namespace)::LoadSavedModelInternal(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::string const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, tensorflow::SavedModelBundle*) [clone .constprop.337] (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x7432fc)\r\n    #7 0x7fb64dd8f3a3 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::string const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, tensorflow::SavedModelBundle*) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x7443a3)\r\n    #8 0x7fb65107fb41 in TF_LoadSessionFromSavedModel (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc07b41)\r\n    #9 0x555c39258d56 in Model::Model(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&) /p/F2_CPP/ext/CppFlow/src/Model.cpp:25\r\n    #10 0x555c3910e268 in FastSpeech2::Initialize(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /p/F2_CPP/src/FastSpeech2.cpp:19\r\n    #11 0x555c3912cc76 in Voice::Voice(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /p/F2_CPP/src/Voice.cpp:271\r\n    #12 0x555c3911cfe5 in main /p/F2_CPP/src/TensorflowTTSCppInference.cpp:13\r\n    #13 0x7fb64f769b96 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21b96)\r\n\r\nIndirect leak of 4096 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7fb65b997448 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe0448)\r\n    #1 0x7fb6510f9d00 in void std::vector<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> >, std::allocator<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> > > >::_M_emplace_back_aux<tensorflow::kernel_factory::OpKernelRegistrar*>(tensorflow::kernel_factory::OpKernelRegistrar*&&) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc81d00)\r\n    #2 0x7fb6510fa02a in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc8202a)\r\n    #3 0x7fb6510e8a7e in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc70a7e)\r\n    #4 0x7fb64e6d6984 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x108b984)\r\n    #5 0x7fb658df910d in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0x898110d)\r\n    #6 0x7fb64e6c6855 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x107b855)\r\n    #7 0x7fb64dd8e2fc in tensorflow::(anonymous namespace)::LoadSavedModelInternal(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::string const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, tensorflow::SavedModelBundle*) [clone .constprop.337] (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x7432fc)\r\n    #8 0x7fb64dd8f3a3 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::string const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, tensorflow::SavedModelBundle*) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x7443a3)\r\n    #9 0x7fb65107fb41 in TF_LoadSessionFromSavedModel (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc07b41)\r\n    #10 0x555c39258d56 in Model::Model(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&) /p/F2_CPP/ext/CppFlow/src/Model.cpp:25\r\n    #11 0x555c3910e268 in FastSpeech2::Initialize(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /p/F2_CPP/src/FastSpeech2.cpp:19\r\n    #12 0x555c3912cc76 in Voice::Voice(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /p/F2_CPP/src/Voice.cpp:271\r\n    #13 0x555c3911cfe5 in main /p/F2_CPP/src/TensorflowTTSCppInference.cpp:13\r\n    #14 0x7fb64f769b96 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21b96)\r\n\r\nIndirect leak of 400 byte(s) in 400 object(s) allocated from:\r\n    #0 0x7fb65b997448 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe0448)\r\n    #1 0x7fb6510f9ef6 in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc81ef6)\r\n    #2 0x7fb6510e8a7e in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc70a7e)\r\n    #3 0x7fb64e6d6984 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x108b984)\r\n    #4 0x7fb658df910d in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/p/F2_CPP/deps/lib/libtensorflow.so.2+0x898110d)\r\n    #5 0x7fb64e6c6855 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x107b855)\r\n    #6 0x7fb64dd8e2fc in tensorflow::(anonymous namespace)::LoadSavedModelInternal(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::string const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, tensorflow::SavedModelBundle*) [clone .constprop.337] (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x7432fc)\r\n    #7 0x7fb64dd8f3a3 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::string const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, tensorflow::SavedModelBundle*) (/p/F2_CPP/deps/lib/libtensorflow_framework.so.2+0x7443a3)\r\n    #8 0x7fb65107fb41 in TF_LoadSessionFromSavedModel (/p/F2_CPP/deps/lib/libtensorflow.so.2+0xc07b41)\r\n    #9 0x555c39258d56 in Model::Model(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&) /p/F2_CPP/ext/CppFlow/src/Model.cpp:25\r\n    #10 0x555c3910e268 in FastSpeech2::Initialize(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /p/F2_CPP/src/FastSpeech2.cpp:19\r\n    #11 0x555c3912cc76 in Voice::Voice(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /p/F2_CPP/src/Voice.cpp:271\r\n    #12 0x555c3911cfe5 in main /p/F2_CPP/src/TensorflowTTSCppInference.cpp:13\r\n    #13 0x7fb64f769b96 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21b96)\r\n\r\nSUMMARY: AddressSanitizer: 4520 byte(s) leaked in 402 allocation(s).\r\n```", "Can you also post some code that you are using?", "https://github.com/TensorSpeech/TensorFlowTTS/blob/3a37400101f05c566cc709c1e0ef6349d48a36f3/examples/cppwin/TensorflowTTSCppInference/ext/CppFlow/src/Model.cpp#L25", "@mihaimaruseac Is this still the old https://github.com/tensorflow/tensorflow/issues/23557?", "Likely. Will try to look into this", "same problem.", "@mihaimaruseac any news on this?", "Still facing this issue, are there any workarounds @mihaimaruseac ?"]}, {"number": 44713, "title": "//tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test fails on s390x ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ub18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): Ubuntu 7.5.0-3ubuntu1~18.04\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nI am running Tensorflow 2.3.1 on s390x and  `//tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test` is failing on running bazel tests.\r\nThe TC was earlier failing with error: ```Internal: TargetRegistry::lookupTarget failed: No available targets are compatible with triple \"x86_64-pc-linux\"```\r\n\r\nTo fix this, I added support for s390x in [test_target_triple_helper.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h)\r\nThis is the code change I did:\r\n```\r\ndiff --git a/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h b/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h\r\nindex 857de4a814..1fb04f821a 100644\r\n--- a/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h\r\n+++ b/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h\r\n@@ -21,8 +21,13 @@ limitations under the License.\r\n static const char kTargetCpuForHost[] = \"ppc\";\r\n static const char kTargetTripleForHost[] = \"ppc64le-ibm-linux-gnu\";\r\n #else\r\n+#if (defined(__s390x__) && (__BYTE_ORDER__ == __ORDER_BIG_ENDIAN__))\r\n+static const char kTargetCpuForHost[] = \"\";\r\n+static const char kTargetTripleForHost[] = \"systemz-none-linux-gnu\"; \r\n+#else\r\n static const char kTargetCpuForHost[] = \"\";\r\n static const char kTargetTripleForHost[] = \"x86_64-pc-linux\";\r\n #endif\r\n+#endif\r\n\r\n #endif\r\n```\r\n\r\nEven after making this change,  the test case is still failing. The LLVM is identifying `systemz-none-linux-gnu` as target but not able to calculate vector_register_byte_size.\r\nThe error output looks like this:\r\n```\r\n==================== Test output for //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test:\r\n[==========] Running 1 test from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters\r\n[ RUN      ] CodegenReduceOnArchWithNoVectorRegisters.Test\r\n2020-11-09 22:19:12.755885: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1555500000 Hz\r\ntensorflow/compiler/xla/service/cpu/vectorized_reduce_with_no_vector_registers_test.cc:86: Failure\r\nExpected equality of these values:\r\n  vector_register_byte_size_for_x86_64\r\n    Which is: 0\r\n  16\r\n[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test (4 ms)\r\n[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters (4 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test suite ran. (4 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test\r\n```\r\n**Describe the expected behavior**\r\nThe LLVM function should return correct values for vector_register_byte_size and the test case should pass.\r\n\r\n\r\n**Other info / logs** \r\nI noticed the previous issue regarding XLA Testcases and have already taken the changes done in the PR [39912](https://github.com/tensorflow/tensorflow/issues/39912)", "comments": ["Hi @r4nt A very happy new year to you! Could you find anything on this issue?", "@skribm9 It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "@sushreebarsa These problems still exist on TF 2.5.0:\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test\r\n-----------------------------------------------------------------------------\r\n[==========] Running 1 test from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters\r\n[ RUN      ] CodegenReduceOnArchWithNoVectorRegisters.Test\r\n2021-08-11 14:25:59.351811: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1555500000 Hz\r\ntensorflow/compiler/xla/service/cpu/vectorized_reduce_with_no_vector_registers_test.cc:81: Failure\r\nExpected equality of these values:\r\n  vector_register_byte_size_for_x86_64\r\n    Which is: 0\r\n  16\r\n[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test (64 ms)\r\n[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters (64 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test suite ran. (65 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test\r\n\r\n 1 FAILED TEST\r\n```"]}, {"number": 44710, "title": "On-disk cache for compiled functions", "body": "Is there a way to cache compiled tf.functions in between sessions with auto-load? As a reference, numba has the option cache=True:\r\nhttps://numba.readthedocs.io/en/stable/user/jit.html?highlight=cache#cache\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n2.3.1\r\n\r\n\r\n", "comments": ["Hi @bdch1234l, can you provide some more information on your use case? Specifically, is there a use case you have where this is causing performance issues? Additionally, are you referring to caching JIT compilation of concrete functions?\r\nAlso linking to the [\"Better Performance with tf.function\"](https://www.tensorflow.org/guide/function#setup) guide in case you haven't seen it yet.", "Hi @nikitamaia, thanks for looking into this.\r\n\r\nMy use case is that I have a fairly complex code-base which is very performance-critical, and this is currently using numba to accelerate it. Given the simpler syntax which came with TF 2.0, I am looking into if I can migrate the numba-code to instead be accelerated by TF (along the lines of [this](https://github.com/google/tf-quant-finance) project e.g.), but I was hoping to avoid incurring any warm-up overhead in the process. The cache-functionality of numba basically saves down a compiled function to disk, to avoid re-compiling the next session. I guess the TF-equivalent would be to save down the compiled and optimized graph which comes from the decoration with tf.function.", "> When you save a tf.Module, any tf.Variable attributes, tf.function-decorated methods, and tf.Modules found via recursive traversal are saved. (See the Checkpoint tutorial for more about this recursive traversal.) However, any Python attributes, functions, and data are lost. This means that when a tf.function is saved, no Python code is saved.\r\nIf no Python code is saved, how does SavedModel know how to restore the function?\r\nBriefly, tf.function works by tracing the Python code to generate a ConcreteFunction (a callable wrapper around tf.Graph). When saving a tf.function, you're really saving the tf.function's cache of ConcreteFunctions.\r\nTo learn more about the relationship between tf.function and ConcreteFunctions, see the tf.function guide.\r\n\r\n@bdch1234 You can check about what happens with concrete functions at https://www.tensorflow.org/guide/saved_model?hl=en", "Got it, thanks @bhack. So if I understand it correctly, when saving a Module or so any functions etc this depends on will also be saved. But I would still always need to manually save and restore the \"outer\" object then?", "Python code wrapped by `@tf.function` is not saved  you're really saving the tf.function's cache of ConcreteFunctions", "Right, ok. But what I mean is that when saving/loading the cache of ConcreteFunctions, there is no way to have that done automatically? I would always need to save/restore manually?", "Can you share a very, very minimal but runnable coden example with save and load of what you want?", "Sure, please see below:\r\n```python\r\n@tf.function\r\ndef my_complex_operation(x):\r\n  # This would then be some very complex operation, so \r\n  # TF could take a while to produce the final optimized DAG.\r\n   return x+1 \r\n```\r\nSo assume the above function is defined in a library. I load the library and call it, the DAG is compiled and optimized. Every subsequent call to `my_complex_operation` will be super-fast, but the next time I re-run my program I will have to recompile the graph again, unless I manually save and restore the ConcreteFunction (to my understanding).\r\n\r\nCorresponding solution when accelerated using numba, would be to use `cache=True`:\r\n```python\r\n@njit(cache=True)\r\ndef my_complex_operation(x):\r\n   return x+1 \r\n```\r\nSame as for tensorflow, when I load the library and call it the first time, the function is compiled and optimized. But when the cache-flag is set to True, the compiled specialized function gets saved to a specified directory on disk. So the next time I re-run the program it will not need to recompile, it just loads the compiled version from the cache.", "Yes I meant a runnable (copy, paste, run) minimal example with a save model and load.", "The question I had in this thread was if something similar to the numba's cache=True exists for TensorFlow (and if not make a feature-request for it). I don't see how this will be answered by writing code to save and restore models manually just to show you what, the complexity of the operations?\r\nIf it is useful, the runnable versions of my previous examples are given below.\r\n```python\r\n# TensorFlow version\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n@tf.function\r\ndef my_complex_operation(x):\r\n    return x+1\r\n\r\nmy_complex_operation(np.random.rand(10))\r\n```\r\n\r\n```python\r\n# Numba version\r\n\r\nfrom numba import njit\r\nimport numpy as np\r\n\r\n@njit(cache=True)\r\ndef my_complex_operation(x):\r\n    return x+1\r\n\r\nmy_complex_operation(np.random.rand(10))\r\n```\r\nThe difference is that the compile-overhead for numba only happens once, even if I close down the python-session and start over again. Is that possible to achieve with TensorFlow?", "From the early comment I was talking about load and saved model. I don't think you have an helper to directly save and load single function cache.", "Well, then you just answered my question. Thank you.\r\n\r\n@nikitamaia, might it then be a good idea to bring back the \"type:feature\"-label? Or is this something you can directly say that TensorFlow will not support?", "I meant there is code to save concrete function  but I think is not exposed in single public API  call. You can check https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/saved_model/save.py\r\n\r\nI suggest you to check the in-memory map cache flow at https://pgaleone.eu/tensorflow/tf.function/2019/03/21/dissecting-tf-function-part-1/#tffunction-layman-explanation", "@bdch1234 have you seen the resources shared in the above post?", "@nikitamaia, I have but it still not addressing the question I had. \r\n\r\nWe have established that a concrete function can be saved manually if needed, but there is no functionality for automatic saving and loading of it (like numba's cache), hence my feature-request. Also, the second link is just a general high-level description of how tf.function works. The closest thing to being useful for this thread in there is that the compiled and optimized graphs are stored in an in-memory map (i.e. the cache), but to make use of that I would need to manually save and load stuff again.", "Yes this was just a little bit of support info if you want to try to prepare a PR.\r\n@bdch1234 I don't know if you are interested but I think for something like this you need to prepare a design RFC PR at https://github.com/tensorflow/community/", "Got it. Well then thanks for providing the additional info @bhack.", "Different end use case, but #43800 seems like it could result in a similar feature. Specifically [this comment.](https://github.com/tensorflow/tensorflow/issues/43800#issuecomment-704411480)"]}, {"number": 44703, "title": "Support string sorting", "body": "\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently string sorting in the graph is unsupported:\r\n\r\n### Does not work:\r\n\r\n```python\r\ntf.sort(['d', 'a', 'b'])\r\nInvalidArgumentError: Value for attr 'T' of string is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, complex64, complex128\r\n\t; NodeDef: {{node Neg}}; Op<name=Neg; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]> [Op:Neg]\r\n```\r\n\r\n### Works (but not in the graph):\r\n\r\n```python\r\n>>> x = tf.constant(['d', 'a', 'b'])\r\n>>> tf.convert_to_tensor(sorted(x.numpy()), tf.string)\r\n<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'a', b'b', b'd'], dtype=object)>\r\n```\r\n\r\nCan `tf.sort` be amended to allow string sorting?\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who might need to string-sort in the graph\r\n\r\n**Any Other info.**\r\n", "comments": ["I checked the code of operation, the problem is that tf.sort actually call the TOPK OP that does not support tf.string input type but only real number type.\r\n```c++\r\nREGISTER_OP(\"TopKV2\")\r\n    .Input(\"input: T\")\r\n    .Input(\"k: int32\")\r\n    .Output(\"values: T\")\r\n    .Output(\"indices: int32\")\r\n    .Attr(\"sorted: bool = true\")\r\n    .Attr(\"T: realnumbertype\")  // real num type only\r\n    .SetShapeFn(TopKShapeFn);\r\n```\r\nIf change of string type is acceptable with interface design and the scope of change ?\r\nif it is acceptable, I'd like to commit the c++ code of impl.", "@jvishnuvardhan "]}, {"number": 44698, "title": "Training takes forever when applying distribute strategy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly \r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 11.0/8.04\r\n- GPU model and memory:3x V100 32G\r\n\r\n`v1.12.1-45362-gec957ef376 2.5.0-dev20201108`\r\n\r\n\r\n**Describe the current behavior**\r\nApply my code with `tf.distribute.MirroredStrategy()` then it take about 10 times slower to start training. And RAM keeps being filled, growing almost linearly.\r\n**Describe the expected behavior**\r\n-Shouldn't take so long to start, it takes about 15min to start while start almost immediately without any strategy.  \r\n- RAM grows linearly makes it impossible to train.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\ntf.distribute.experimental_set_strategy(strategy)\r\nraw_image_dataset = tf.data.TFRecordDataset(...)\r\nimage_feature_description = {\r\n            'height': tf.io.FixedLenFeature([], tf.int64),\r\n            'width': tf.io.FixedLenFeature([], tf.int64),\r\n            'depth': tf.io.FixedLenFeature([], tf.int64),\r\n            'label': tf.io.FixedLenFeature([], tf.string),\r\n            'image': tf.io.FixedLenFeature([], tf.string),\r\n}\r\n\r\ndef augument(example_proto):\r\n            exp = tf.io.parse_single_example(example_proto, image_feature_description)\r\n            img = tf.io.decode_jpeg(exp['image'])\r\n            label = tf.io.parse_tensor(exp['label'], tf.float32)\r\n            img = tf.cast(img, tf.float32)\r\n            img = img / 255.0\r\n            img = tf.image.random_brightness(img, max_delta=0.1)\r\n            img = tf.clip_by_value(img, 0.0, 1.0)\r\n            return img, label\r\n\r\ntrain_dataset = raw_image_dataset.map(augument,  num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\r\ntrain_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n...\r\n\r\ndef train_step(self, img):\r\n    with tf.GradientTape() as tape:\r\n        loss = model(img)\r\n    variables = tape.watched_variables()\r\n    grads = tape.gradient(loss, variables)\r\n    optimizer.apply_gradients(zip(grads, variables))\r\n    return tf.reduce_mean(loss)\r\n\r\n@tf.function\r\ndef distributed_train_step(self, img):\r\n    per_replica_losses = strategy.run(self.train_step, args=(img,))\r\n    return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)\r\n\r\nfor td in train_dataset:\r\n    distributed_train_step(td)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nBefore training, it logs:\r\n```\r\nINFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 2313 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 2313 all-reduces with algorithm = nccl, num_packs = 1\r\n```\r\nThe last two lines take so long to be printed. ", "comments": ["@gitlabspy \r\n\r\nPlease, help us with complete code snippet with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Not sure if it'll help but you may try \r\n\r\n`train_dataset = strategy.experimental_distribute_dataset(train_dataset)` \r\n\r\nOr, to wrap your preprocessing function you could use \r\n\r\n`strategy.experimental_distribute_dataset_from_function(argument)`\r\n\r\nto batch your data across your workers. Seems like each worker is trying to work on the entire dataset from this snippet instead of doing a data parallelism approach (which I kind of assume you are trying to do from the code). This would likely yield the issues you are seeing with speed and RAM, etc. \r\n\r\ntf.DistributedDataset info can be found here - https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset and distribute dataset from function here - https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_datasets_from_function\r\n\r\nThis may speed things up. Someone more tensorflow-ey than me can correct me if I'm wrong here, but it should also prefetch automatically for each worker.\r\n\r\nThat being said, the universal disclaimer to distributed strategies, more GPUs =/= more faster. The additional computational overhead and CPU/GPU latency and CPU aggregation of weights can absolutely work against you if your problem isn't \"big enough\" to require a distributed strategy. This could also be a factor in why your code is running slower, and without knowing the size/scale of the data you are using this is potentially a consideration as well. \r\n\r\nAnd, in regards to it taking 15 minutes to start, your model/loss isn't shown here. tf is building the graph for your model and losses, etc during that time. The overhead before the training starts is tensorflow compiling your graph. Which, by the way, will need to be copied to all the GPUs in order to run a data-parallel distributed strategy. So if you copy that graph to 8 GPUs you can see how that time will rack up before training begins. \r\n", "@levimcclenny Thanks for your suggestion! I did `train_dataset = strategy.experimental_distribute_dataset(train_dataset)` but I forgot to put it in my code. I also try remove randomness from map function by removing `tf.image.random_brightness(img, max_delta=0.1)` but it doesnt help at all. \r\n\r\nThe reason for slow start-up, I think it should be caused by the dataset. Before training, I test the model by feeding random data and it works. But when it runs the looping code:\r\n```\r\nfor td in train_dataset:\r\n    distributed_train_step(td)\r\n```\r\njust like I described above, it logs some `batch-all-reduce` things. \r\n```\r\nINFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1\r\n...\r\nINFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1\r\n```\r\nuntil about 5 min :\r\n```\r\nINFO:tensorflow:batch_all_reduce: 2313 all-reduces with algorithm = nccl, num_packs = 1\r\n```\r\nthen another 5 min:\r\n```\r\nINFO:tensorflow:batch_all_reduce: 2313 all-reduces with algorithm = nccl, num_packs = 1\r\n```\r\nthen another 5 min, it starts training.\r\n\r\nMy dataset is kinda large, about 600K 64x64x3 images. Same model works fine without distribute strategy, no linearly memory grow. But wrap training code with distribute strategy ( like described above), memory grows linearly and even keep growing when finished a loop. ", "That's odd behavior, I have been recently building a MirroredStrategy distributed training loop in the latest stable (v2.3.1) that seems to behave normally.. perhaps try with that version instead of nightly? I'm not sure other than that. ", "Some info from profiler, it might be helpful? \r\n\r\n![Screen Shot 2020-11-10 at 10 06 50 PM](https://user-images.githubusercontent.com/61307585/98684464-2a38bb80-23a1-11eb-9e63-4ec045972322.png)\r\n![Screen Shot 2020-11-10 at 10 05 12 PM](https://user-images.githubusercontent.com/61307585/98684465-2c027f00-23a1-11eb-8f12-5c827ac3d1a4.png)\r\n", "Hi @gitlabspy, I'm linking to the [GPU Performance Guide](https://www.tensorflow.org/guide/gpu_performance_analysis). I would suggest following the steps there first. Let me know if that still doesn't help resolve the issue.\r\n", "hi @gitlabspy i have met something very similar as yours, the only difference is that i have never seen the last two lines.\r\nwhat ever i run use distribution strategy in my local computer, like the official example [https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/distribute/custom_training.ipynb?hl=zh-cn#scrollTo=dzLKpmZICaWN](url) \r\nwhen i run the cell of \r\n```python\r\nwith strategy.scope():\r\n  # `experimental_run_v2`\u5c06\u590d\u5236\u63d0\u4f9b\u7684\u8ba1\u7b97\u5e76\u4f7f\u7528\u5206\u5e03\u5f0f\u8f93\u5165\u8fd0\u884c\u5b83\u3002\r\n  @tf.function\r\n  def distributed_train_step(dataset_inputs):\r\n    per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                      args=(dataset_inputs,))\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                           axis=None)\r\n \r\n  @tf.function\r\n  def distributed_test_step(dataset_inputs):\r\n    return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\r\n\r\n  for epoch in range(EPOCHS):\r\n    # \u8bad\u7ec3\u5faa\u73af\r\n    total_loss = 0.0\r\n    num_batches = 0\r\n    for x in train_dist_dataset:\r\n      total_loss += distributed_train_step(x)\r\n      num_batches += 1\r\n    train_loss = total_loss / num_batches\r\n\r\n    # \u6d4b\u8bd5\u5faa\u73af\r\n    for x in test_dist_dataset:\r\n      distributed_test_step(x)\r\n\r\n    if epoch % 2 == 0:\r\n      checkpoint.save(checkpoint_prefix)\r\n\r\n    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\r\n                \"Test Accuracy: {}\")\r\n    print (template.format(epoch+1, train_loss,\r\n                           train_accuracy.result()*100, test_loss.result(),\r\n                           test_accuracy.result()*100))\r\n\r\n    test_loss.reset_states()\r\n    train_accuracy.reset_states()\r\n    test_accuracy.reset_states()\r\n```\r\n\r\nthe last thing it print is\r\n```python\r\nWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nWARNING:tensorflow:From <ipython-input-14-6439d0e9d271>:5: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrenamed to `run`\r\nINFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n```\r\n\r\n\r\ni run these codes in debug mode, the whole codes is below(change the above colab link to .py file):\r\n```python\r\n# -*- coding: utf-8 -*-\r\n\"\"\"custom_training.ipynb\r\n\r\nAutomatically generated by Colaboratory.\r\n\r\nOriginal file is located at\r\n    https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/distribute/custom_training.ipynb\r\n\r\n##### Copyright 2019 The TensorFlow Authors.\r\n\"\"\"\r\n\r\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n# https://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n\"\"\"# \u4f7f\u7528 tf.distribute.Strategy \u8fdb\u884c\u81ea\u5b9a\u4e49\u8bad\u7ec3\r\n\r\n<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n  <td>\r\n    <a target=\"_blank\" href=\"https://tensorflow.google.cn/tutorials/distribute/custom_training\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\" />\u5728 TensorFlow.org \u4e0a\u67e5\u770b</a>\r\n  </td>\r\n  <td>\r\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/distribute/custom_training.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\" />\u5728 Google Colab \u4e0a\u8fd0\u884c</a>\r\n  </td>\r\n  <td>\r\n    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/distribute/custom_training.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\" />\u5728 GitHub \u4e0a\u67e5\u770b\u6e90\u4ee3\u7801</a>\r\n  </td>\r\n  <td>\r\n    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/tutorials/distribute/custom_training.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\" />\u4e0b\u8f7d\u8be5 notebook</a>\r\n  </td>\r\n</table>\r\n\r\n\u672c\u6559\u7a0b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 [`tf.distribute.Strategy`](https://tensorflow.google.cn/guide/distribute_strategy) \u6765\u8fdb\u884c\u81ea\u5b9a\u4e49\u8bad\u7ec3\u5faa\u73af\u3002 \u6211\u4eec\u5c06\u5728\u6d41\u884c\u7684 MNIST \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684 CNN \u6a21\u578b\u3002 \u6d41\u884c\u7684 MNIST \u6570\u636e\u96c6\u5305\u542b\u4e86 60000 \u5f20\u5c3a\u5bf8\u4e3a 28 x 28 \u7684\u8bad\u7ec3\u56fe\u50cf\u548c 10000 \u5f20\u5c3a\u5bf8\u4e3a 28 x 28 \u7684\u6d4b\u8bd5\u56fe\u50cf\u3002 \r\n\r\n\u6211\u4eec\u7528\u81ea\u5b9a\u4e49\u8bad\u7ec3\u5faa\u73af\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u662f\u56e0\u4e3a\u5b83\u4eec\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u548c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u66f4\u597d\u7684\u63a7\u5236\u3002\u800c\u4e14\uff0c\u4f7f\u5b83\u4eec\u8c03\u8bd5\u6a21\u578b\u548c\u8bad\u7ec3\u5faa\u73af\u7684\u65f6\u5019\u66f4\u5bb9\u6613\u3002\r\n\"\"\"\r\n\r\n# \u5bfc\u5165 TensorFlow\r\nimport tensorflow as tf\r\n\r\n# \u5e2e\u52a9\u5e93\r\nimport numpy as np\r\nimport os\r\n\r\nprint(tf.__version__)\r\n\r\n\"\"\"## \u4e0b\u8f7d\u6d41\u884c\u7684 MNIST \u6570\u636e\u96c6\"\"\"\r\n\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n\r\n# \u5411\u6570\u7ec4\u6dfb\u52a0\u7ef4\u5ea6 -> \u65b0\u7684\u7ef4\u5ea6 == (28, 28, 1)\r\n# \u6211\u4eec\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u6211\u4eec\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u662f\u5377\u79ef\u5c42\r\n# \u800c\u4e14\u5b83\u9700\u8981\u4e00\u4e2a\u56db\u7ef4\u7684\u8f93\u5165 (\u6279\u5927\u5c0f, \u9ad8, \u5bbd, \u901a\u9053).\r\n# \u6279\u5927\u5c0f\u7ef4\u5ea6\u7a0d\u540e\u5c06\u6dfb\u52a0\u3002\r\ntrain_images = train_images[..., None]\r\ntest_images = test_images[..., None]\r\n\r\n# \u83b7\u53d6[0,1]\u8303\u56f4\u5185\u7684\u56fe\u50cf\u3002\r\ntrain_images = train_images / np.float32(255)\r\ntest_images = test_images / np.float32(255)\r\n\r\n\"\"\"## \u521b\u5efa\u4e00\u4e2a\u5206\u53d1\u53d8\u91cf\u548c\u56fe\u5f62\u7684\u7b56\u7565\r\n\r\n`tf.distribute.MirroredStrategy` \u7b56\u7565\u662f\u5982\u4f55\u8fd0\u4f5c\u7684\uff1f\r\n\r\n*   \u6240\u6709\u53d8\u91cf\u548c\u6a21\u578b\u56fe\u90fd\u590d\u5236\u5728\u526f\u672c\u4e0a\u3002\r\n*   \u8f93\u5165\u90fd\u5747\u5300\u5206\u5e03\u5728\u526f\u672c\u4e2d\u3002\r\n*   \u6bcf\u4e2a\u526f\u672c\u5728\u6536\u5230\u8f93\u5165\u540e\u8ba1\u7b97\u8f93\u5165\u7684\u635f\u5931\u548c\u68af\u5ea6\u3002\r\n*   \u901a\u8fc7\u6c42\u548c\uff0c\u6bcf\u4e00\u4e2a\u526f\u672c\u4e0a\u7684\u68af\u5ea6\u90fd\u80fd\u540c\u6b65\u3002\r\n*   \u540c\u6b65\u540e\uff0c\u6bcf\u4e2a\u526f\u672c\u4e0a\u7684\u590d\u5236\u7684\u53d8\u91cf\u90fd\u53ef\u4ee5\u540c\u6837\u66f4\u65b0\u3002\r\n\r\n\u6ce8\u610f\uff1a\u60a8\u53ef\u4ee5\u5c06\u4e0b\u9762\u7684\u6240\u6709\u4ee3\u7801\u653e\u5728\u4e00\u4e2a\u5355\u72ec\u5355\u5143\u5185\u3002 \u6211\u4eec\u5c06\u5b83\u5206\u6210\u51e0\u4e2a\u4ee3\u7801\u5355\u5143\u7528\u4e8e\u8bf4\u660e\u76ee\u7684\u3002\r\n\"\"\"\r\n\r\n# \u5982\u679c\u8bbe\u5907\u672a\u5728 `tf.distribute.MirroredStrategy` \u7684\u6307\u5b9a\u5217\u8868\u4e2d\uff0c\u5b83\u4f1a\u88ab\u81ea\u52a8\u68c0\u6d4b\u5230\u3002\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nprint ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\n\"\"\"## \u8bbe\u7f6e\u8f93\u5165\u6d41\u6c34\u7ebf\r\n\r\n\u5c06\u56fe\u5f62\u548c\u53d8\u91cf\u5bfc\u51fa\u6210\u5e73\u53f0\u4e0d\u53ef\u8bc6\u522b\u7684 SavedModel \u683c\u5f0f\u3002\u5728\u4f60\u7684\u6a21\u578b\u4fdd\u5b58\u540e\uff0c\u4f60\u53ef\u4ee5\u5728\u6709\u6216\u6ca1\u6709\u8303\u56f4\u7684\u60c5\u51b5\u4e0b\u8f7d\u5165\u5b83\u3002\r\n\"\"\"\r\n\r\nBUFFER_SIZE = len(train_images)\r\n\r\nBATCH_SIZE_PER_REPLICA = 64\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\nEPOCHS = 10\r\n\r\n\"\"\"\u521b\u5efa\u6570\u636e\u96c6\u5e76\u5206\u53d1\u5b83\u4eec\uff1a\"\"\"\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \r\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE) \r\n\r\ntrain_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\ntest_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\r\n\r\n\"\"\"## \u521b\u5efa\u6a21\u578b\r\n\r\n\u4f7f\u7528 `tf.keras.Sequential` \u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u3002\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528\u6a21\u578b\u5b50\u7c7b\u5316 API \u6765\u5b8c\u6210\u8fd9\u4e2a\u3002\r\n\"\"\"\r\n\r\ndef create_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu'),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Conv2D(64, 3, activation='relu'),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n  return model\r\n\r\n# \u521b\u5efa\u68c0\u67e5\u70b9\u76ee\u5f55\u4ee5\u5b58\u50a8\u68c0\u67e5\u70b9\u3002\r\ncheckpoint_dir = './training_checkpoints'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n\r\n\"\"\"## \u5b9a\u4e49\u635f\u5931\u51fd\u6570\r\n\r\n\u901a\u5e38\uff0c\u5728\u4e00\u53f0\u53ea\u6709\u4e00\u4e2a GPU / CPU \u7684\u673a\u5668\u4e0a\uff0c\u635f\u5931\u9700\u8981\u9664\u53bb\u8f93\u5165\u6279\u91cf\u4e2d\u7684\u793a\u4f8b\u6570\u3002\r\n\r\n*\u90a3\u4e48\uff0c\u4f7f\u7528 `tf.distribute.Strategy` \u65f6\u5e94\u8be5\u5982\u4f55\u8ba1\u7b97\u635f\u5931\uff1f*\r\n\r\n* \u4e3e\u4e00\u4e2a\u4f8b\u5b50\uff0c\u5047\u8bbe\u60a8\u67094\u4e2a GPU\uff0c\u6279\u91cf\u5927\u5c0f\u4e3a64. \u8f93\u5165\u7684\u4e00\u4e2a\u6279\u6b21\u5206\u5e03\u5728\u5404\u4e2a\u526f\u672c\uff084\u4e2a GPU\uff09\u4e0a\uff0c\u6bcf\u4e2a\u526f\u672c\u83b7\u5f97\u7684\u8f93\u5165\u5927\u5c0f\u4e3a16\u3002\r\n\r\n* \u6bcf\u4e2a\u526f\u672c\u4e0a\u7684\u6a21\u578b\u4f7f\u7528\u5176\u5404\u81ea\u7684\u8f93\u5165\u6267\u884c\u6b63\u5411\u4f20\u9012\u5e76\u8ba1\u7b97\u635f\u5931\u3002 \u73b0\u5728\uff0c\u76f8\u8f83\u4e8e\u5c06\u635f\u8017\u9664\u4ee5\u5176\u5404\u81ea\u8f93\u5165\u4e2d\u7684\u793a\u4f8b\u6570\uff08BATCH_SIZE_PER_REPLICA = 16\uff09\uff0c\u5e94\u5c06\u635f\u5931\u9664\u4ee5GLOBAL_BATCH_SIZE\uff0864\uff09\u3002\r\n\r\n*\u4e3a\u4ec0\u4e48\u8fd9\u6837\u505a\uff1f*\r\n\r\n* \u9700\u8981\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u5728\u6bcf\u4e2a\u526f\u672c\u4e0a\u8ba1\u7b97\u68af\u5ea6\u4e4b\u540e\uff0c\u5b83\u4eec\u901a\u8fc7 **summing** \u6765\u4f7f\u5f97\u5728\u81ea\u8eab\u5728\u5404\u4e2a\u526f\u672c\u4e4b\u95f4\u540c\u6b65\u3002\r\n\r\n*\u5982\u4f55\u5728TensorFlow\u4e2d\u6267\u884c\u6b64\u64cd\u4f5c\uff1f*\r\n* \u5982\u679c\u60a8\u6b63\u5728\u7f16\u5199\u81ea\u5b9a\u4e49\u8bad\u7ec3\u5faa\u73af\uff0c\u5982\u672c\u6559\u7a0b\u4e2d\u6240\u793a\uff0c\u60a8\u5e94\u8be5\u5c06\u6bcf\u4e2a\u793a\u4f8b\u635f\u5931\u76f8\u52a0\u5e76\u5c06\u603b\u548c\u9664\u4ee5 GLOBAL_BATCH_SIZE \uff1a\r\n`scale_loss = tf.reduce_sum(loss) * (1. / GLOBAL_BATCH_SIZE)` \u6216\u8005\u4f60\u53ef\u4ee5\u4f7f\u7528`tf.nn.compute_average_loss` \u6765\u83b7\u53d6\u6bcf\u4e2a\u793a\u4f8b\u7684\u635f\u5931\uff0c\u53ef\u9009\u7684\u6837\u672c\u6743\u91cd\uff0c\u5c06GLOBAL_BATCH_SIZE\u4f5c\u4e3a\u53c2\u6570\uff0c\u5e76\u8fd4\u56de\u7f29\u653e\u7684\u635f\u5931\u3002\r\n\r\n* \u5982\u679c\u60a8\u5728\u6a21\u578b\u4e2d\u4f7f\u7528\u6b63\u5219\u5316\u635f\u5931\uff0c\u5219\u9700\u8981\u8fdb\u884c\u7f29\u653e\u591a\u4e2a\u526f\u672c\u7684\u635f\u5931\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528`tf.nn.scale_regularization_loss`\u51fd\u6570\u6267\u884c\u6b64\u64cd\u4f5c\u3002\r\n\r\n* \u5efa\u8bae\u4e0d\u8981\u4f7f\u7528`tf.reduce_mean`\u3002 \u8fd9\u6837\u505a\u4f1a\u5c06\u635f\u5931\u9664\u4ee5\u5b9e\u9645\u7684\u6bcf\u4e2a\u526f\u672c\u4e2d\u6bcf\u4e00\u6b65\u90fd\u4f1a\u6539\u53d8\u7684\u6279\u6b21\u5927\u5c0f\u3002\r\n\r\n* \u8fd9\u79cd\u7f29\u5c0f\u548c\u7f29\u653e\u662f\u5728 keras\u4e2d `modelcompile`\u548c`model.fit`\u4e2d\u81ea\u52a8\u5b8c\u6210\u7684\r\n\r\n* \u5982\u679c\u4f7f\u7528`tf.keras.losses`\u7c7b\uff08\u5982\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50\u6240\u793a\uff09\uff0c\u5219\u9700\u8981\u5c06\u635f\u5931\u51cf\u5c11\u660e\u786e\u6307\u5b9a\u4e3a\u201cNONE\u201d\u6216\u8005\u201cSUM\u201d\u3002 \u4f7f\u7528 `tf.distribute.Strategy` \u65f6\uff0c`AUTO`\u548c`SUM_OVER_BATCH_SIZE` \u662f\u4e0d\u80fd\u4f7f\u7528\u7684\u3002 \u4e0d\u80fd\u4f7f\u7528 `AUTO` \u662f\u56e0\u4e3a\u7528\u6237\u5e94\u660e\u786e\u8003\u8651\u5230\u5728\u5206\u5e03\u5f0f\u60c5\u51b5\u4e0b\u4ed6\u4eec\u60f3\u505a\u7684\u54ea\u4e9b\u51cf\u5c11\u662f\u6b63\u786e\u7684\u3002\u4e0d\u80fd\u4f7f\u7528`SUM_OVER_BATCH_SIZE`\u662f\u56e0\u4e3a\u76ee\u524d\u5b83\u53ea\u6309\u6bcf\u4e2a\u526f\u672c\u6279\u6b21\u5927\u5c0f\u8fdb\u884c\u5212\u5206\uff0c\u5e76\u6309\u7167\u7528\u6237\u7684\u526f\u672c\u6570\u8fdb\u884c\u5212\u5206\uff0c\u8fd9\u5bfc\u81f4\u4e86\u5b83\u4eec\u5f88\u5bb9\u6613\u4e22\u5931\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u8981\u6c42\u7528\u6237\u8981\u660e\u786e\u8fd9\u4e9b\u51cf\u5c11\u3002\r\n\"\"\"\r\n\r\nwith strategy.scope():\r\n  # \u5c06\u51cf\u5c11\u8bbe\u7f6e\u4e3a\u201c\u65e0\u201d\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u5728\u4e4b\u540e\u8fdb\u884c\u8fd9\u4e2a\u51cf\u5c11\u5e76\u9664\u4ee5\u5168\u5c40\u6279\u91cf\u5927\u5c0f\u3002\r\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n      reduction=tf.keras.losses.Reduction.NONE)\r\n  # \u6216\u8005\u4f7f\u7528 loss_fn = tf.keras.losses.sparse_categorical_crossentropy\r\n  def compute_loss(labels, predictions):\r\n    per_example_loss = loss_object(labels, predictions)\r\n    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n\r\n\"\"\"## \u5b9a\u4e49\u8861\u91cf\u6307\u6807\u4ee5\u8ddf\u8e2a\u635f\u5931\u548c\u51c6\u786e\u6027\r\n\r\n\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u8ddf\u8e2a\u6d4b\u8bd5\u7684\u635f\u5931\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528`.result()`\u968f\u65f6\u83b7\u53d6\u7d2f\u79ef\u7684\u7edf\u8ba1\u4fe1\u606f\u3002\r\n\"\"\"\r\n\r\nwith strategy.scope():\r\n  test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n\r\n  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='train_accuracy')\r\n  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='test_accuracy')\r\n\r\n\"\"\"## \u8bad\u7ec3\u5faa\u73af\"\"\"\r\n\r\n# \u5fc5\u987b\u5728`strategy.scope`\u4e0b\u521b\u5efa\u6a21\u578b\u548c\u4f18\u5316\u5668\u3002\r\nwith strategy.scope():\r\n  model = create_model()\r\n\r\n  optimizer = tf.keras.optimizers.Adam()\r\n\r\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\r\n\r\nwith strategy.scope():\r\n  def train_step(inputs):\r\n    images, labels = inputs\r\n\r\n    with tf.GradientTape() as tape:\r\n      predictions = model(images, training=True)\r\n      loss = compute_loss(labels, predictions)\r\n\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n    train_accuracy.update_state(labels, predictions)\r\n    return loss \r\n\r\n  def test_step(inputs):\r\n    images, labels = inputs\r\n\r\n    predictions = model(images, training=False)\r\n    t_loss = loss_object(labels, predictions)\r\n\r\n    test_loss.update_state(t_loss)\r\n    test_accuracy.update_state(labels, predictions)\r\n\r\nwith strategy.scope():\r\n  # `experimental_run_v2`\u5c06\u590d\u5236\u63d0\u4f9b\u7684\u8ba1\u7b97\u5e76\u4f7f\u7528\u5206\u5e03\u5f0f\u8f93\u5165\u8fd0\u884c\u5b83\u3002\r\n  @tf.function\r\n  def distributed_train_step(dataset_inputs):\r\n    per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                      args=(dataset_inputs,))\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                           axis=None)\r\n \r\n  @tf.function\r\n  def distributed_test_step(dataset_inputs):\r\n    return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\r\n\r\n  for epoch in range(EPOCHS):\r\n    # \u8bad\u7ec3\u5faa\u73af\r\n    total_loss = 0.0\r\n    num_batches = 0\r\n    for x in train_dist_dataset:\r\n      total_loss += distributed_train_step(x)\r\n      num_batches += 1\r\n    train_loss = total_loss / num_batches\r\n\r\n    # \u6d4b\u8bd5\u5faa\u73af\r\n    for x in test_dist_dataset:\r\n      distributed_test_step(x)\r\n\r\n    if epoch % 2 == 0:\r\n      checkpoint.save(checkpoint_prefix)\r\n\r\n    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\r\n                \"Test Accuracy: {}\")\r\n    print (template.format(epoch+1, train_loss,\r\n                           train_accuracy.result()*100, test_loss.result(),\r\n                           test_accuracy.result()*100))\r\n\r\n    test_loss.reset_states()\r\n    train_accuracy.reset_states()\r\n    test_accuracy.reset_states()\r\n\r\n\"\"\"\u4ee5\u4e0a\u793a\u4f8b\u4e2d\u9700\u8981\u6ce8\u610f\u7684\u4e8b\u9879\uff1a\r\n\r\n* \u6211\u4eec\u4f7f\u7528`for x in ...`\u8fed\u4ee3\u6784\u9020`train_dist_dataset`\u548c`test_dist_dataset`\u3002\r\n* \u7f29\u653e\u635f\u5931\u662f`distributed_train_step`\u7684\u8fd4\u56de\u503c\u3002 \u8fd9\u4e2a\u503c\u4f1a\u5728\u5404\u4e2a\u526f\u672c\u4f7f\u7528`tf.distribute.Strategy.reduce`\u7684\u65f6\u5019\u5408\u5e76\uff0c\u7136\u540e\u901a\u8fc7`tf.distribute.Strategy.reduce`\u53e0\u52a0\u5404\u4e2a\u8fd4\u56de\u503c\u6765\u8de8\u6279\u6b21\u3002\r\n* \u5728\u6267\u884c`tf.distribute.Strategy.experimental_run_v2`\u65f6\uff0c`tf.keras.Metrics`\u5e94\u5728`train_step`\u548c`test_step`\u4e2d\u66f4\u65b0\u3002\r\n* `tf.distribute.Strategy.experimental_run_v2`\u8fd4\u56de\u7b56\u7565\u4e2d\u6bcf\u4e2a\u672c\u5730\u526f\u672c\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u6709\u591a\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406\u6b64\u7ed3\u679c\u3002 \u60a8\u53ef\u4ee5\u6267\u884c`tf.distribute.Strategy.reduce`\u6765\u83b7\u53d6\u6c47\u603b\u503c\u3002 \u60a8\u8fd8\u53ef\u4ee5\u6267\u884c`tf.distribute.Strategy.experimental_local_results`\u6765\u83b7\u53d6\u6bcf\u4e2a\u672c\u5730\u526f\u672c\u4e2d\u7ed3\u679c\u4e2d\u5305\u542b\u7684\u503c\u5217\u8868\u3002\r\n\r\n## \u6062\u590d\u6700\u65b0\u7684\u68c0\u67e5\u70b9\u5e76\u8fdb\u884c\u6d4b\u8bd5\r\n\r\n\u4e00\u4e2a\u6a21\u578b\u4f7f\u7528\u4e86`tf.distribute.Strategy`\u7684\u68c0\u67e5\u70b9\u53ef\u4ee5\u4f7f\u7528\u7b56\u7565\u6216\u8005\u4e0d\u4f7f\u7528\u7b56\u7565\u8fdb\u884c\u6062\u590d\u3002\r\n\"\"\"\r\n\r\neval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='eval_accuracy')\r\n\r\nnew_model = create_model()\r\nnew_optimizer = tf.keras.optimizers.Adam()\r\n\r\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\r\n\r\n@tf.function\r\ndef eval_step(images, labels):\r\n  predictions = new_model(images, training=False)\r\n  eval_accuracy(labels, predictions)\r\n\r\ncheckpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\r\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n\r\nfor images, labels in test_dataset:\r\n  eval_step(images, labels)\r\n\r\nprint ('Accuracy after restoring the saved model without strategy: {}'.format(\r\n    eval_accuracy.result()*100))\r\n\r\n\"\"\"## \u8fed\u4ee3\u4e00\u4e2a\u6570\u636e\u96c6\u7684\u66ff\u4ee3\u65b9\u6cd5\r\n\r\n### \u4f7f\u7528\u8fed\u4ee3\u5668\r\n\r\n\u5982\u679c\u4f60\u60f3\u8981\u8fed\u4ee3\u4e00\u4e2a\u5df2\u7ecf\u7ed9\u5b9a\u6b65\u9aa4\u6570\u91cf\u800c\u4e0d\u9700\u8981\u6574\u4e2a\u904d\u5386\u7684\u6570\u636e\u96c6\uff0c\u4f60\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u8fed\u4ee3\u5668\u5e76\u5728\u8fed\u4ee3\u5668\u4e0a\u8c03\u7528`iter`\u548c\u663e\u5f0f\u8c03\u7528`next`\u3002 \u60a8\u53ef\u4ee5\u9009\u62e9\u5728 tf.function \u5185\u90e8\u548c\u5916\u90e8\u8fed\u4ee3\u6570\u636e\u96c6\u3002 \u8fd9\u662f\u4e00\u4e2a\u5c0f\u7247\u6bb5\uff0c\u6f14\u793a\u4e86\u4f7f\u7528\u8fed\u4ee3\u5668\u5728 tf.function \u5916\u90e8\u8fed\u4ee3\u6570\u636e\u96c6\u3002\r\n\"\"\"\r\n\r\nwith strategy.scope():\r\n  for _ in range(EPOCHS):\r\n    total_loss = 0.0\r\n    num_batches = 0\r\n    train_iter = iter(train_dist_dataset)\r\n\r\n    for _ in range(10):\r\n      total_loss += distributed_train_step(next(train_iter))\r\n      num_batches += 1\r\n    average_train_loss = total_loss / num_batches\r\n\r\n    template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\r\n    print (template.format(epoch+1, average_train_loss, train_accuracy.result()*100))\r\n    train_accuracy.reset_states()\r\n\r\n\"\"\"### \u5728 tf.function \u4e2d\u8fed\u4ee3\r\n\r\n\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528`for x in ...`\u6784\u9020\u5728 tf.function \u5185\u90e8\u8fed\u4ee3\u6574\u4e2a\u8f93\u5165`train_dist_dataset`\uff0c\u6216\u8005\u50cf\u4e0a\u9762\u90a3\u6837\u521b\u5efa\u8fed\u4ee3\u5668\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u6f14\u793a\u4e86\u5728 tf.function \u4e2d\u5305\u88c5\u4e00\u4e2a epoch \u5e76\u5728\u529f\u80fd\u5185\u8fed\u4ee3`train_dist_dataset`\u3002\r\n\"\"\"\r\n\r\nwith strategy.scope():\r\n  @tf.function\r\n  def distributed_train_epoch(dataset):\r\n    total_loss = 0.0\r\n    num_batches = 0\r\n    for x in dataset:\r\n      per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                        args=(x,))\r\n      total_loss += strategy.reduce(\r\n        tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\r\n      num_batches += 1\r\n    return total_loss / tf.cast(num_batches, dtype=tf.float32)\r\n\r\n  for epoch in range(EPOCHS):\r\n    train_loss = distributed_train_epoch(train_dist_dataset)\r\n\r\n    template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\r\n    print (template.format(epoch+1, train_loss, train_accuracy.result()*100))\r\n\r\n    train_accuracy.reset_states()\r\n\r\n\"\"\"### \u8ddf\u8e2a\u526f\u672c\u4e2d\u7684\u8bad\u7ec3\u7684\u635f\u5931\r\n\r\n\u6ce8\u610f\uff1a\u4f5c\u4e3a\u901a\u7528\u7684\u89c4\u5219\uff0c\u60a8\u5e94\u8be5\u4f7f\u7528`tf.keras.Metrics`\u6765\u8ddf\u8e2a\u6bcf\u4e2a\u6837\u672c\u7684\u503c\u4ee5\u907f\u514d\u5b83\u4eec\u5728\u526f\u672c\u4e2d\u5408\u5e76\u3002\r\n\r\n\u6211\u4eec *\u4e0d* \u5efa\u8bae\u4f7f\u7528`tf.metrics.Mean` \u6765\u8ddf\u8e2a\u4e0d\u540c\u526f\u672c\u7684\u8bad\u7ec3\u635f\u5931\uff0c\u56e0\u4e3a\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u8fdb\u884c\u635f\u5931\u7f29\u653e\u8ba1\u7b97\u3002\r\n\r\n\u4f8b\u5982\uff0c\u5982\u679c\u60a8\u8fd0\u884c\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\u7684\u8bad\u7ec3\u4f5c\u4e1a\uff1a\r\n* \u4e24\u4e2a\u526f\u672c\r\n* \u5728\u6bcf\u4e2a\u526f\u672c\u4e0a\u5904\u7406\u4e24\u4e2a\u4f8b\u5b50\r\n* \u4ea7\u751f\u7684\u635f\u5931\u503c\uff1a\u6bcf\u4e2a\u526f\u672c\u4e3a[2,3]\u548c[4,5]\r\n* \u5168\u5c40\u6279\u6b21\u5927\u5c0f = 4\r\n\r\n\u901a\u8fc7\u635f\u5931\u7f29\u653e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u635f\u5931\u503c\u6765\u8ba1\u7b97\u6bcf\u4e2a\u526f\u672c\u4e0a\u7684\u6bcf\u4e2a\u6837\u672c\u7684\u635f\u5931\u503c\uff0c\u7136\u540e\u9664\u4ee5\u5168\u5c40\u6279\u91cf\u5927\u5c0f\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff1a`\uff082 + 3\uff09/ 4 = 1.25`\u548c`\uff084 + 5\uff09/ 4 = 2.25`\u3002\r\n\r\n\u5982\u679c\u60a8\u4f7f\u7528 `tf.metrics.Mean` \u6765\u8ddf\u8e2a\u4e24\u4e2a\u526f\u672c\u7684\u635f\u5931\uff0c\u7ed3\u679c\u4f1a\u6709\u6240\u4e0d\u540c\u3002 \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u4f60\u6700\u7ec8\u5f97\u5230\u4e00\u4e2a`total`\u4e3a 3.50 \u548c`count`\u4e3a 2 \u7684\u7ed3\u679c\uff0c\u5f53\u8c03\u7528`result\uff08\uff09`\u65f6\uff0c\u4f60\u5c06\u5f97\u5230`total` /`count` = 1.75\u3002 \u4f7f\u7528`tf.keras.Metrics`\u8ba1\u7b97\u635f\u5931\u65f6\u4f1a\u901a\u8fc7\u4e00\u4e2a\u7b49\u4e8e\u540c\u6b65\u526f\u672c\u6570\u91cf\u7684\u989d\u5916\u56e0\u5b50\u6765\u7f29\u653e\u3002\r\n\r\n### \u4f8b\u5b50\u548c\u6559\u7a0b\r\n\u4ee5\u4e0b\u662f\u4e00\u4e9b\u4f7f\u7528\u81ea\u5b9a\u4e49\u8bad\u7ec3\u5faa\u73af\u6765\u5206\u53d1\u7b56\u7565\u7684\u793a\u4f8b\uff1a\r\n\r\n1. [\u6559\u7a0b](training_loops.ipynb) \u4f7f\u7528 `MirroredStrategy` \u6765\u8bad\u7ec3 MNIST \u3002\r\n2. [DenseNet](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/densenet/distributed_train.py) \u4f7f\u7528 `MirroredStrategy`\u7684\u4f8b\u5b50\u3002\r\n1. [BERT](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_classifier.py) \u4f7f\u7528 `MirroredStrategy` \u548c`TPUStrategy`\u6765\u8bad\u7ec3\u7684\u4f8b\u5b50\u3002\r\n\u6b64\u793a\u4f8b\u5bf9\u4e8e\u4e86\u89e3\u5982\u4f55\u5728\u5206\u53d1\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5982\u4f55\u8f7d\u5165\u4e00\u4e2a\u68c0\u6d4b\u70b9\u548c\u5b9a\u671f\u751f\u6210\u68c0\u67e5\u70b9\u7279\u522b\u6709\u5e2e\u52a9\u3002\r\n2. [NCF](https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_keras_main.py) \u4f7f\u7528 `MirroredStrategy` \u6765\u542f\u7528 `keras_use_ctl` \u6807\u8bb0\u3002\r\n3. [NMT](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/nmt_with_attention/distributed_train.py) \u4f7f\u7528 `MirroredStrategy`\u6765\u8bad\u7ec3\u7684\u4f8b\u5b50\u3002\r\n\r\n\u66f4\u591a\u7684\u4f8b\u5b50\u5217\u5728 [\u5206\u53d1\u7b56\u7565\u6307\u5357](../../guide/distribute_strategy.ipynb#examples_and_tutorials)\u3002\r\n\r\n## \u4e0b\u4e00\u6b65\r\n\r\n\u5728\u4f60\u7684\u6a21\u578b\u4e0a\u5c1d\u8bd5\u65b0\u7684`tf.distribute.Strategy` API\u3002\r\n\"\"\"\r\n```\r\ni run the codes above, when it run to line 233 \r\n```python\r\ntotal_loss += distributed_train_step(x)\r\n```\r\nit stucked, and the exact place is line 221  in the method of distributed_train_step.\r\n```python\r\nreturn strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n axis=None)\r\n```\r\n\r\nif i annotate line 217 (@tf.function) like this:\r\n```python\r\nwith strategy.scope():\r\n  # `experimental_run_v2`\u5c06\u590d\u5236\u63d0\u4f9b\u7684\u8ba1\u7b97\u5e76\u4f7f\u7528\u5206\u5e03\u5f0f\u8f93\u5165\u8fd0\u884c\u5b83\u3002\r\n  # @tf.function\r\n  def distributed_train_step(dataset_inputs):\r\n    per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                      args=(dataset_inputs,))\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                           axis=None)\r\n```\r\nit will stuck at line 201:\r\n```python\r\noptimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\n this is similar to keras distribution training example, i have tested the keras distribution strategy official example in debug mode, it stucked when the optimazer applies the gradient. i lost the codes, if you want these codes, i will find it out later.\r\n\r\nmy environment is :\r\nubuntu 16.04\r\npython 3.8\r\ntensorflow 2.3.1\r\ncuda 10.1\r\ni test in  GeForce GTX 1070  and GeForce RTX 2080 super, both had the some behavior.\r\ni hope these informations will help.", "@wujiren So you think `@tf.function` slows things down? I don't think removing `tf.function` is a good idea tho. \r\n@nikitamaia Here's my trace view for one loop of training. Idk if I use `tf.profiler` the right way:\r\n```\r\ntf.profiler.experimental.start('logdir')\r\nfor step in range(num_steps):\r\n    train_step()\r\ntf.profiler.experimental.stop()\r\n```\r\nNot bar in GPU part...? The gpu usage is 50-100% I pretty sure GPU is used when training. And I can't see any nccl things.\r\n![Screen Shot 2020-11-11 at 4 44 08 PM](https://user-images.githubusercontent.com/61307585/98789662-8d782b80-243d-11eb-8ccd-65e5ac6a87aa.png)\r\n![Screen Shot 2020-11-11 at 4 50 11 PM](https://user-images.githubusercontent.com/61307585/98789958-fd86b180-243d-11eb-91fb-9db178f4332a.png)\r\n", "> @wujiren So you think `@tf.function` slows things down? I don't think removing `tf.function` is a good idea tho.\r\n\r\nhi @gitlabspy \r\nremove @tf.function is because in debug mode the breakpoint would not available. whether i remove it or not ,it will stuck anyway, but it is strange that with or without @tf.function, the codes stucked at different places, if without tf.function, it stoped like a keras model.", "@nikitamaia any suggestion?", "Did you go through the performance guide? Generally, the way to debug these issues is by analyzing the performance with a single GPU first, and then add on multiple GPUs. There are several common scenarios described in that guide and one of them could provide some insight into the issue you're facing.\r\n\r\nThere are different ways of setting up the profiler, but the recommended way for doing this with a custom training loop [can be found here.](https://www.tensorflow.org/guide/profiler#profiling_custom_training_loops)\r\n\r\nA few different things to try to help us get to the root cause:\r\n- Use `strategy.scope()` instead of `tf.distribute.experimental_set_strategy(strategy)`\r\n- Try setting `tf.distribute.ReductionToOneDevice` to see if NCCL is causing problems\r\n- Use `MirroredStrategy`, but with only one GPU. Monitor the memory in this case.", "`tf.distribute.experimental_set_strategy(strategy)` , `strategy.scope()`  and  `tf.distribute.ReductionToOneDevice` are used.\r\nDoesn't seems\r\nSingle GPU: \r\n![Screen Shot 2020-11-14 at 7 11 07 PM](https://user-images.githubusercontent.com/61307585/99146143-50679f80-26b0-11eb-971f-9a847aad4276.png)\r\nTwo GPU:\r\n![Screen Shot 2020-11-14 at 7 32 57 PM](https://user-images.githubusercontent.com/61307585/99146142-4d6caf00-26b0-11eb-92d4-ecc7546ce6e1.png)\r\n![Screen Shot 2020-11-14 at 7 34 34 PM](https://user-images.githubusercontent.com/61307585/99146157-7f7e1100-26b0-11eb-90c0-d2775244bdf8.png)\r\n\r\nStill got memory leak and slow start.", "What does the equivalent trace look like if you replace MirroredStrategy with `tf.distribute.get_strategy()` ?", "Almost same as single GPU with MirroredStrategy.\r\n![Screen Shot 2020-11-17 at 1 48 08 PM](https://user-images.githubusercontent.com/61307585/99351543-c4d25680-28db-11eb-863c-90cb1de685e9.png)\r\n", "I encounter another memory leak issue without distribute strategy.\r\n```\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\ntfb = tfp.bijectors\r\ntfd = tfp.distributions\r\ndef augument(img):\r\n    img = tf.cast(img, tf.float32) / 255.\r\n    return img\r\n\r\n(train_data, _), (test_data, _)  = tf.keras.datasets.cifar10.load_data()\r\ntrain_data = tf.data.Dataset.from_tensor_slices(train_data).map(augument).batch(128)\r\n\r\noutput_shape = (32, 32 ,3)\r\ntransformed_distribution = tfd.TransformedDistribution(\r\n    distribution=   tfd.MultivariateNormalDiag( tf.zeros(\r\n                                                (tf.math.reduce_prod(list(output_shape)), )\r\n                                                ),\r\n                                             tf.ones(\r\n                                                (tf.math.reduce_prod(list(output_shape)), )\r\n                                                )\r\n                                           ),\r\n    bijector    =   tfb.Glow(output_shape=output_shape,\r\n                            coupling_bijector_fn=tfb.GlowDefaultNetwork,\r\n                            exit_bijector_fn=tfb.GlowDefaultExitNetwork),\r\n    name='Glow_distribution')\r\noptimizer = tf.keras.optimizers.Adam(1e-4)\r\n\r\n@tf.function\r\ndef train_step(x):\r\n    with tf.GradientTape() as tape:\r\n        log_prob_loss = - transformed_distribution.log_prob(x) \r\n    variables = tape.watched_variables()\r\n    grads = tape.gradient(log_prob_loss, variables)\r\n    optimizer.apply_gradients(zip(grads, variables))\r\n    return tf.reduce_mean(log_prob_loss)\r\nfor epoch in range(100):\r\n    count = 0\r\n    for td in train_data:\r\n        loss = train_step(td)\r\n        print(loss, '*',end='\\r')\r\n\r\n```\r\nMaybe you can try this? @nikitamaia  ", "I tried reinstalled tfnightly and it still didn't work. Any harmful ops I used in above code? It occupied 16G memory when it finished building graph and growing 0.05G/sec... ", "Hi @gitlabspy, there's a lot to unpack in this thread. Let's try and isolate the issue here. In the new example you have provided, are you seeing the leak on GPU only? Or also with CPU? Can you also test in nightly?\r\nI was not able to repro the issue, as I ran into the error `AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Glow'`(I am also not very familiar with tf-probability, as that is managed in a different repo.)", "@nikitamaia Hi, my issue does not leak on GPU, it leak on physical memory. And the above tfp code I provided is kinda different from this case but they lead to kinda same behaviour which is memory leak and also same speed...\r\n\r\n I shouldn't have confused you with these tfp codes but I observed they both have to take a bit time ( 5min or more depends on how big the model is) to compile the autograph. And within this compiling time, I assumed it is compiling time, the physical memory grows gradually until it meets some point like 1G, it suddenly goes to 6~9G depended on model. When it finished compiling, it starts training and physical memory grows in some certain speed like I described above. \r\n\r\nIt was the newly nightly when I filed this issue. I'll try currently new nightly. ", "I noticed that as the model (not only the one in this issue) gets more complex, increase the gpus will increase the compile time ( I guess?), which means more complex model gets slower to start training and more gpus make it much slower... any suggestions....?"]}, {"number": 44688, "title": "Memory leak in TF 2.3.1 when using Multi-worker training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tensorflow 2.3.1 container\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Tensorflow 2.3.1 container\r\n- TensorFlow version (use command below): Tensorflow 2.3.1 container\r\n- Python version: Tensorflow 2.3.1 container\r\n\r\n**Describe the current behaviour**\r\nWhen using MultiWorkerMirroredStrategy, there is a memory leak happening during training. The memory leak is not present when only one node is used.\r\n\r\nHere is the memory consumption of a single node trainning\r\n<img width=\"1641\" alt=\"single\" src=\"https://user-images.githubusercontent.com/10571507/98474204-94146200-21f5-11eb-85bf-1e1c02f196f0.png\">\r\n\r\nHere is the memory consumption of multi-node training (1 chief and 1 worker)\r\n<img width=\"1643\" alt=\"chief\" src=\"https://user-images.githubusercontent.com/10571507/98474883-c625c400-21f5-11eb-9803-c0f529f717b5.png\">\r\n\r\n<img width=\"1635\" alt=\"worker\" src=\"https://user-images.githubusercontent.com/10571507/98474979-dd64b180-21f5-11eb-80c8-d2b1e34c7fc4.png\">\r\n\r\nThe code is identical for both cases. The only thing that changes is the TF_CONFIG which is provided be GCP AI Platform.\r\n\r\n**Describe the expected behaviour**\r\n\r\nThe memory consumption should not be increasing by time (epochs).\r\n\r\n**Standalone code to reproduce the issue**\r\nThe code is available at [this](https://github.com/kalosisz/tf-gcp-multi-worker-example) repo.\r\n\r\n", "comments": ["Maybe related to #42616 ", "@kalosisz \r\n\r\nCan you please try with TF nightly version and see if the issue still persists.Thanks!", "@ravikyram \r\n\r\nI have changed the order of operations on the dataset to cache, shuffle, repeat, batch, prefetch. I have re-run the code on 2.3.1 and nightly (see the repo). \r\n\r\nThe leak is even bigger for nightly. Here are the memory footprints of nightly (master and worker, resp.)\r\n\r\n<img width=\"1619\" alt=\"Screenshot 2020-11-09 at 12 29 21\" src=\"https://user-images.githubusercontent.com/10571507/98535978-4cd6b180-2287-11eb-902e-74b6ab4e3522.png\">\r\n\r\n<img width=\"1619\" alt=\"Screenshot 2020-11-09 at 12 29 33\" src=\"https://user-images.githubusercontent.com/10571507/98535988-4fd1a200-2287-11eb-8dfd-5845bc6005fd.png\">\r\n\r\nThe same under 2.3.1 (master and worker, resp)\r\n\r\n<img width=\"1623\" alt=\"Screenshot 2020-11-09 at 12 31 01\" src=\"https://user-images.githubusercontent.com/10571507/98536097-83143100-2287-11eb-9664-f13c9c818dc9.png\">\r\n\r\n<img width=\"1622\" alt=\"Screenshot 2020-11-09 at 12 31 08\" src=\"https://user-images.githubusercontent.com/10571507/98536105-85768b00-2287-11eb-8807-b39ba3e9dc7e.png\">\r\n\r\n\r\n\r\n\r\n", "Hi @kalosisz, I took a look at the repo you have provided. I want to try and reproduce this on AI Platform Training. Can you provide the values you used for the various flags (eg learning rate, layers) as well as the data? \r\n\r\nA few other things that could help to isolate the problem: can you call model.fit but without validation data and let me know what you see? Also you're running this with CPU only machines correct? Is there still a leak if you use two GPUs instead?", "Hi @nikitamaia,\r\n\r\nYes, I'm using CPU only. The machines are *n1-standard-4*.\r\n\r\nThe data is available here `gs://szilard-ml-sandbox-eu-public/random_data_tfrecords`. The bucket is located in the EU and *requester pays* is switched on. \r\n\r\nI've tried to remove the validation, but it did not help (run on nightly). The parameters I've run all the trials are `--layer-size 100 10 --learning-rate 0.01 --epochs 50 --data-base-path gs://szilard-ml-sandbox-eu-public/random_data_tfrecords --training-examples 2799589 --validation-examples 350198 --evaluation-examples 350213`\r\n\r\n<img width=\"1623\" alt=\"Screenshot 2020-11-09 at 21 59 32\" src=\"https://user-images.githubusercontent.com/10571507/98595889-e7121600-22d6-11eb-8768-d2e1549ddbcf.png\">\r\n\r\n<img width=\"1636\" alt=\"Screenshot 2020-11-09 at 21 59 41\" src=\"https://user-images.githubusercontent.com/10571507/98595902-e9747000-22d6-11eb-8d8f-b3e214f6db3d.png\">\r\n\r\nI haven't tried to run thin with GPU, mainly because it'd be a bit costly for me.", "Because the requester pays param is on it seems I need the name of your project in order to access the data. I'm seeing the error message `Bucket is requester pays bucket but no user project provided.`", "According to the documentation, you should use the `-u` option with you project. Please see [link](https://cloud.google.com/storage/docs/using-requester-pays#using)\r\n\r\nPlease let me know if it helped.", "Thanks, that worked. I was able to access the data. However, when trying to train on AI platform and locally, I am getting the error `UnboundLocalError: local variable 'logs' referenced before assignment`\r\n\r\n[Update] Never mind. That error message was due to an issue that occurred when trying to copy over the data. Looks like the AI Platform job is working for me now. I will try and repro this with GPUs and also without using AI Platform and will update this thread.", "hey @nikitamaia, any updates on the matter? :slightly_smiling_face:  ", "I was able to repro an increase in memory on AI Platform and also on GCE VMs. Unfortunately, no big updates after this initial investigation. But I will definitely keep this thread updated.", "Thanks for the info. Out of curiosity, did the issue arrise in case of GPUs as well?", "I met the similar problem in my project. \r\nIn my project,  the reason may be `tf.Dataset.repeat(epochs)` which will increase the memory until it grows to the peak of memory (which base on the epochs).  However, it's sad to me that my model require a minimum of 5 epochs and the memory only can support epochs <= 3.\r\n\r\nyou can try to set a small epochs in your data.py", "@zhenql my example is here to demonstrate the leak it does not how real world implication, however it might just have. \r\ncertainly, the number of epoch is not an issue, if not trained in distributed fashion. should the MultiWorkerMirroredStrategy be not considered experimental, these issue mustn't persist.", "@kalosisz well, my project is also trained with `MultiWorkerMirroredStrategy`, and the memory leak is relevant to `tf.Dataset.repeat(epochs)` in the distributed environment (certainly, single machine can not see the memory leak).  I just can located the memory leak might be here of my project. \r\nI'm not sure whether your problem is related to `repeat()` or not but you can have a try if like. \r\n\r\naddition:\r\nwhen I change the epochs to a small number, the memory-steps curve like the mountain (have peak, broken line) instead of oblique line.", "Hey @nikitamaia, has there been any update from TF side? The error still persists in v2.4.0, even though the MultiWorker is apparently stable. Thanks!"]}, {"number": 44675, "title": "Bad performance of tf.data.Dataset API when dealing with windowed inputs / timeseries problems / 3D arrays", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 2.4.0-dev20201006\r\nPython version: 3.7\r\n\r\n**Describe the current behavior**\r\nI work on timeserie forecasting, involving manipulating 3-D arrays as inputs of my models (batch_size, nb_timesteps, nb_features).\r\nI used to rely on Keras \"Sequence\" objects to feed the \"fit\" function.\r\nYet, I recently chose to move on tf.data.Dataset API to have a more robust input pipeline. In addition, this allow using named features, which is a huge advantage. Nonetheless, I found that usage and performances of tf datasets could be improved when it comes to window data:\r\n\r\n1.  windowing a dataset of named features is slow and not very intuitive. I thought this could maybe be improved (see issue https://github.com/tensorflow/tensorflow/issues/43703)\r\n\r\n2. datasets of windows tend to be heavy. While shuffling, the dataset \"shuffle\" method necessarily pre-buffers all samples to use for training, resulting in huge memory usage, which makes training of such models not tractable with the Dataset API. Note that for timeseries problems, as we start from chronologically-ordered data, it is very important to get a \"perfect\" shuffling, hence we cannot reduce the shuffle buffer size (it is necessary to set it equal or greater than the dataset cardinality). I was thinking: maybe the \"Shuffle\" method could be improved so that it does not pre-buffer all possible samples, but rather does the following:\r\n           - index the whole dataset, in case cardinality can be known\r\n           - shuffle the index list\r\n           - pick-up a new batch based on the shuffled index list, at each training step\r\nThis would be very similar to the way the Keras Sequence works. However, I think the Keras Sequence lack a lot of other interesting features brought by the Dataset API. So it would be good if we could get the best of the two in the Dataset API, which now seems to be the standard for data input pipelines in tf.\r\n\r\n**Describe the expected behavior**\r\nimproved performance and architecture for windowed datasets, using tf.data.Dataset API", "comments": ["I am having the same issue. Are there any fixes?"]}, {"number": 44670, "title": "BUG: Keras SaveModel does not properly save optimizer state", "body": "EDIT: looks like this is a dupe of #42749, I'll leave this up for now in case since that issue does not have as reproducible high level example, but feel free to close.\r\n\r\nThis happens at least for Adam (does not apply to SGD for example, did not test with others).\r\n\r\nTested on `tf-nightly` and `tf==2.3.0`.\r\n\r\nTL;DR: running a `tf.kerasModel` through `tf.keras.models.load(model.save)` does not properly preserve the state of optimizers for certain optimizers (see #42749 for more details).\r\n\r\nThe [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#the_savefile_includes_2) read:\r\n\r\n> The savefile includes:\r\n> * The model architecture, allowing to re-instantiate the model.\r\n> * The model weights.\r\n> * The state of the optimizer, allowing to resume training exactly where you left off.\r\n\r\nFull example:\r\n\r\n```python3\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\n# Define a minimal model\r\ninp = keras.layers.Input((1, ))\r\nout = keras.layers.Dense(1)(inp)\r\nm1 = keras.Model(inp, out)\r\nm1.compile(loss=\"mae\", optimizer=\"adam\")\r\n\r\n# Create some test data\r\nX, y = np.random.random((100, )), np.random.random((100, ))\r\n\r\n# Fit the model to the test data to get everything initialized\r\nm1.fit(X, y, verbose=0)\r\n\r\n\r\ndef roundtrip(model: keras.Model) -> keras.Model:\r\n    save_dir = \"/tmp/mymodel\"\r\n    model.save(save_dir)\r\n    restored = keras.models.load_model(save_dir)\r\n    return restored\r\n\r\n\r\n# Create a copy of the fitted m1\r\nm2 = roundtrip(m1)\r\n\r\n# Weights are preserved correctly, this passes\r\nnp.testing.assert_allclose(m1.predict(X), m2.predict(X))\r\n\r\n# New lets train once more round\r\nm1.fit(X, y, verbose=0)\r\n# Since optimizer weights/state is not preserved, this fit call\r\n# results in different weights in m2, which makes the predictions differ\r\nm2.fit(X, y, verbose=0)\r\ntry:\r\n    np.testing.assert_allclose(m1.predict(X), m2.predict(X), rtol=0.1)  # large relative tolerance\r\nexcept AssertionError:\r\n    print(\"AssertionError: model predictions differ\")\r\n\r\n# Diagnosis: optimizer weights are not preserved\r\nweights1 = m1.optimizer.get_weights()\r\nm3 = roundtrip(m1)\r\nweights3 = m3.optimizer.get_weights()\r\n\r\ntry:\r\n    assert weights1 == weights3\r\nexcept AssertionError:\r\n    print(\"AssertionError: optimizer weights differ\")\r\n    print(f\"    {weights1}\\n    vs\\n    {weights3}\")\r\n\r\n# Further, we can't even restore the weights without training!\r\ntry:\r\n    m3.optimizer.set_weights(weights1)\r\nexcept Exception as e:\r\n    print(str(e).split(\" Provided weights:\")[0])\r\n```", "comments": ["@adriangb \r\n\r\nI have tried in colab with TF version 2.3, nightly version(`2.5.0-dev20201108`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/19c8028887615b61abedc1b46e2aec98/untitled494.ipynb).You are also seeing the same behavior?\r\nThanks!", "Yes", "This hacked together fix may be helpful to others with the same problem:\r\n[notebook](https://colab.research.google.com/drive/14ECRN8ZQDa1McKri2dctlV_CaPkE574I?authuser=1#scrollTo=pzj0mIeokkSs&line=1&uniqifier=1)\r\n\r\n```python3\r\ndef _temp_create_all_weights(self, var_list):\r\n    self._create_all_weights_orig(var_list)\r\n    try:\r\n        self.set_weights(self._restored_weights)\r\n    except ValueError:\r\n        # Weights don't match, eg. when optimizer was pickled before any training\r\n        pass\r\n    delattr(self, \"_restored_weights\")\r\n    self._create_all_weights = self._create_all_weights_orig\r\n\r\n\r\ndef _restore_optimizer_weights(optimizer, weights) -> None:\r\n    optimizer._restored_weights = weights\r\n    optimizer._create_all_weights_orig = optimizer._create_all_weights\r\n    optimizer._create_all_weights = MethodType(_temp_create_all_weights, optimizer)\r\n\r\n\r\ndef unpack_keras_optimizer(opt_serialized, weights):\r\n    \"\"\"Reconstruct optimizer.\r\n    \"\"\"\r\n    optimizer: keras.optimizers.Optimizer = keras.optimizers.deserialize(opt_serialized)\r\n    _restore_optimizer_weights(optimizer, weights)\r\n    return optimizer\r\n\r\n\r\ndef pack_keras_optimizer(optimizer: keras.optimizers.Optimizer):\r\n    \"\"\"Support for Pythons's Pickle protocol in Keras Optimizers.\r\n    \"\"\"\r\n    opt_serialized = keras.optimizers.serialize(optimizer)\r\n    weights = optimizer.get_weights()\r\n    return unpack_keras_optimizer, (opt_serialized, weights)\r\n```", "Hi any updates here? I believe this is a major bug. If a user saves a model and loads it they will get unexpected results with no errors.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/04ebadb01a5faee8b42c53762ad803c6/44670.ipynb). Thanks!"]}, {"number": 44658, "title": "tf.linalg.pinv also for complex matrices", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nRight now, as stated in [the docs](https://www.tensorflow.org/api_docs/python/tf/linalg/pinv#args), the `pinv` operator accepts only float-like matrices.\r\nIt would be nice to have it accept complex matrices as well, like `numpy` does.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople working with complex matrices.\r\nI typically work with MRI data which is naturally complex, but I guess this can be applied to other domains.\r\n\r\n**Any Other info.**\r\n\r\nI checked in the latest [`tf.experimental.numpy`](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy), and it's absent. But maybe the complex-supported `pinv` should be better there, I don't know. ", "comments": ["So it's possible to define the `pinv` for complex matrices using the same structure as the [currently implemented `pinv`](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/ops/linalg_ops.py#L484-L552), getting rid of the `dtype` checking.\r\n\r\nYou can check this in [this colab](https://colab.research.google.com/drive/1NlZas6ljrnWw7ye1HhB05WrZdk2XJV2u?usp=sharing), with a test against the `numpy` implementation.\r\nI am not sure it works in all edge cases so I do not guarantee anything.\r\n\r\nHowever, I am willing to just submit this as a PR if you think it's enough to just change the `dtype` checking.\r\n\r\nFor reference here is the implementation:\r\n\r\n```python\r\ndef pinv(a, rcond=None):\r\n    \"\"\"Taken from\r\n    https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/ops/linalg/linalg_impl.py\r\n    \"\"\"\r\n    dtype = a.dtype.as_numpy_dtype\r\n\r\n    if rcond is None:\r\n        def get_dim_size(dim):\r\n            dim_val = a.shape[dim]\r\n            if dim_val is not None:\r\n                return dim_val\r\n            return tf.shape(a)[dim]\r\n\r\n        num_rows = get_dim_size(-2)\r\n        num_cols = get_dim_size(-1)\r\n        if isinstance(num_rows, int) and isinstance(num_cols, int):\r\n            max_rows_cols = float(max(num_rows, num_cols))\r\n        else:\r\n            max_rows_cols = tf.cast(tf.maximum(num_rows, num_cols), dtype)\r\n        rcond = 10. * max_rows_cols * np.finfo(dtype).eps\r\n\r\n    rcond = tf.convert_to_tensor(rcond, dtype=dtype, name='rcond')\r\n\r\n    # Calculate pseudo inverse via SVD.\r\n    # Note: if a is Hermitian then u == v. (We might observe additional\r\n    # performance by explicitly setting `v = u` in such cases.)\r\n    [\r\n        singular_values,  # Sigma\r\n        left_singular_vectors,  # U\r\n        right_singular_vectors,  # V\r\n    ] = tf.linalg.svd(\r\n        a, full_matrices=False, compute_uv=True)\r\n\r\n    # Saturate small singular values to inf. This has the effect of make\r\n    # `1. / s = 0.` while not resulting in `NaN` gradients.\r\n    cutoff = tf.cast(rcond, dtype=singular_values.dtype) * tf.reduce_max(singular_values, axis=-1)\r\n    singular_values = tf.where(\r\n        singular_values > cutoff[..., None], singular_values,\r\n        np.array(np.inf, dtype))\r\n\r\n    # By the definition of the SVD, `a == u @ s @ v^H`, and the pseudo-inverse\r\n    # is defined as `pinv(a) == v @ inv(s) @ u^H`.\r\n    a_pinv = tf.matmul(\r\n        right_singular_vectors / tf.cast(singular_values[..., None, :], dtype=dtype),\r\n        left_singular_vectors,\r\n        adjoint_b=True)\r\n\r\n    if a.shape is not None and a.shape.rank is not None:\r\n      a_pinv.set_shape(a.shape[:-2].concatenate([a.shape[-1], a.shape[-2]]))\r\n\r\n    return a_pinv\r\n```"]}, {"number": 44624, "title": "cuDNN LSTMs not utilized after loading a model using `tf.saved_model.load`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.1**\r\n- Python version: **3.7**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **10.1/7.6.2**\r\n- GPU model and memory: **GeForce GTX 1080 Ti computeCapability: 6.1 coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s**\r\n\r\n**Describe the current behavior**\r\n\r\nTraining a (keras) LSTM model on GPU utilizes (as expected) the cuDNN version of the LSTM cells.\r\nAfter saving this model using `tf.saved_model.save` and reloading it using `tf.saved_model.load`, it utilizes the \r\nNON cuda optimized LSTM cells in inference. (loading it using `tf.keras.models.load_model` it is utillizing the cuDNN cells)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe save/load process shouldn't change the graph. The model should use the same cells as in training after save/load.  I know that I can't expect the same functionality as if I use the higher level keras load/save methods. Consequently, it is acceptable if the automatic device detection does not work. However, it is highly not expected that the used cells change after save/load. If the automatic device detection is not supported, the cells, which were active at the moment of saving, should be loaded by default. Like it is now, it is not possible to create a saved_model in a straightforward way which utilizes the cuDNN cells, if NOT loaded using keras (which is not possible in other APIs: cc, java, ...)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport os\r\nimport time\r\nimport shutil\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nassert len(tf.config.list_physical_devices('GPU'))>0, \"You have to run this on a GPU machine, otherwise you do not see the effect.\"\r\nprint(tf.config.list_physical_devices('GPU'))\r\n\r\nmodel_save_path='tmp'\r\n\r\nlstm_layer = keras.layers.LSTM(64, input_shape=(None, 28))\r\nmodel = keras.models.Sequential([lstm_layer, keras.layers.Dense(10)])\r\n\r\nmnist = keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        optimizer=\"sgd\", metrics=[\"accuracy\"])\r\nmodel.fit(x_train[0:128], y_train[0:128], validation_data=(x_test, y_test), batch_size=64, epochs=1)\r\ntf.saved_model.save(model, model_save_path)\r\n\r\n# Keras load + inference\r\nmodel_reload_keras = tf.keras.models.load_model(model_save_path)\r\ninp = tf.ones(shape=[8, 512, 28])\r\nfor i in range(501):\r\n    # Avoid load offset.\r\n    if i == 1:\r\n        startT = int(round(time.time() * 1000))\r\n    model_reload_keras.predict_on_batch(inp)\r\nendT = int(round(time.time() * 1000))\r\nprint('Keras load -- inference time: ' + str(endT-startT) + 'ms')\r\n\r\n# TF load + inference\r\nmodel_reload_tf = tf.saved_model.load(str(model_save_path))\r\ninfer = model_reload_tf.signatures['serving_default']\r\nfor i in range(501):\r\n    # Avoid load offset.\r\n    if i == 1:\r\n        startT = int(round(time.time() * 1000))\r\n    infer(inp)\r\nendT = int(round(time.time() * 1000))\r\nprint('TF load -- inference time (should be as fast as previous run [on GPU]): ' + str(endT - startT) + 'ms')\r\n\r\nshutil.rmtree(model_save_path)\r\n```\r\n\r\nKind regards & Thanks\r\nTobias\r\n", "comments": ["@TobiasGruening \r\nI ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6595185b4ca9dab658afb14d74b09eb9/untitled458.ipynb) as i see a different issue", "> @TobiasGruening\r\n> I ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6595185b4ca9dab658afb14d74b09eb9/untitled458.ipynb) as i see a different issue\r\n\r\n@Saduf2019 Thanks for your time. I have fixed a little error in the code above. \r\nAdditionally, I added an assertion, because it is mandatory to run the test on a GPU machine to reproduce the effect.", "We also reproduced the behaviour in the nightly build:\r\n```\r\nKeras load -- inference time: 7024ms (seems to use cuDNN cells)\r\nTF load -- inference time (should be as fast as previous run [on GPU]): 46187ms (seems NOT to use cuDNN cells)\r\n```\r\nKind regards,\r\nTobias", "@TobiasGruening I updated two lines in your code to save and load a Keras model (instead of saving and loading Tensorflow model).\r\n\r\n```\r\n# tf.saved_model.save(model, model_save_path)\r\nmodel.save(model_save_path)\r\n\r\n# Keras load + inference\r\n# model_reload_keras = tf.saved_model.load(model_save_path)\r\nmodel_reload_keras = tf.keras.models.load_model(model_save_path)\r\n\r\n```\r\n\r\nWith the above modification, I see the following results. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d7560fc947a810fc048584b2c6f57b8c/untitled458.ipynb)\r\n\r\n```\r\nKeras load -- inference time: 4891ms\r\nTF load -- inference time: 3440ms\r\n```\r\n\r\nPlease let me know what you think. Please close the issue if this was resolved for you. Thanks!\r\n\r\n", "@jvishnuvardhan Thanks for your time. Please have a look at the code in my initial post. I already updated these lines of code a few days ago. But this is NOT my issue. My issue is the time discrepancy, if running this test on a GPU machine!\r\nIf you copy my code to your gist you will get an assertion if not running on a GPU. If running the test on a GPU you will get significantly DIFFERENT inference times. This is because the cuDNN LSTM cells are not used if loading the model using TensorFlow load, which is not as expected. Furthermore, the solution is NOT (as described in my initial post) to load the model using Keras. It is about the behavior of the TensorFlow save and load which is CHANGING the graph!\r\n\r\nKind regards,\r\nTobias", "Any news on this?", "Was able to reproduce your issue in Tensorflow GPU 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ba7a1fb127f74876b0773426929f7a28/44624.ipynb). Thanks!"]}, {"number": 44613, "title": "[TF 2.4] KerasTensor breaks typing compatibility", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-rc0 and tf-nightly\r\n- Python version: 3.8\r\n\r\n## The Problem\r\nSince version 2.4 functional Keras models use `KerasTensor` instead of `tf.Tensor` as layer output types. Unfortunately `KerasTensor` doesn't subclass `tf.Tensor` which breaks `isinstance(x, tf.Tensor)` checks:\r\nhttps://github.com/tensorflow/tensorflow/blob/6d9e0887f6bce8fbeeb430364e520d05350d96d5/tensorflow/python/keras/engine/keras_tensor.py#L63\r\n\r\nThe release notes recommend to use `tf.is_tensor` instead.\r\nIn my opinion this is not really Pythonic and breaks compatibility with the [TF Types RFC](https://github.com/tensorflow/community/blob/master/rfcs/20200211-tf-types.md) (/cc @mdanatg) wich even mentiones that `tf.is_tensor` is expected to be deprecated eventually.\r\n\r\nConcretely, switching from `isinstance(x, tf.Tensor)` to `tf.is_tensor` is also not possible in all cases. E.g. this breaks usage of static typecheckers like `pytype` or `typeguard`:\r\n\r\nA common pattern which can also be found in TensorFlow Addons (/cc @seanpmorgan) is the following:\r\n\r\n```python\r\nfrom typeguard import typechecked\r\nimport tensorflow as tf\r\n\r\n@typechecked\r\ndef foo(x: tf.Tensor):\r\n    print(x.dtype)\r\n\r\nfoo(tf.keras.Input(shape=(32, 32, 3)))  # Throws in TF 2.4 since `isintance` is used for typechecking\r\n```\r\n\r\n## Possible solutions\r\n\r\n1. Make `KerasTensor` a subclass of `tf.Tensor`. @mihaimaruseac @fchollet is there a reason why this isn't currently the case?\r\n\r\n2. Make `KerasTensor` a subclass of [`types.Tensor`](https://github.com/tensorflow/tensorflow/blob/6d9e0887f6bce8fbeeb430364e520d05350d96d5/tensorflow/python/types/core.py#L40-L54). I don't see any disadvantage of doing this in general, but it wouldn't really fix this issue since `types.Tensor` is not exposed as part of the public API yet so users would need to rely on private TensorFlow APIs\r\n\r\n3. In usercode this could be fixed by directly relying on `KerasTensor` to replace the usage of `tf.Tensor` with:\r\n   ```python\r\n   from typing import Union\r\n   from tensorflow.python.keras.engine.keras_tensor import KerasTensor\r\n\r\n   TensorType = Union[tf.Tensor, KerasTensor]\r\n   ```\r\n   I do not think this is a proper solution since it requires users to rely on internal APIs and implementation details that might change in the future.\r\n\r\nI am currious to hear back from you on what the bestpractices for type checking of Tensors are, or whether I am just missing somthing obvious here.", "comments": ["@tomerk can you take a look please?", "cc @mdanatg per discussion in SIG Addons meeting", "Indeed, `is_tensor` intentionally diverges from `isinstance(*, tf.Tensor)`, and we ultimately hope to deprecate it. For now, you may use it and we'll support backward compatibility, but we encourage using isinstance whenever possible, so it makes sense to resolve this particular issue.\r\n\r\nOption 1 is unfortunately\u00a0not feasible because the TensorFlow infra does not support subclassing `tf.Tensor`.\r\n\r\nThat leaves either option 2 or 3, depending whether or not we want `KerasTensor` to be considered a \"TensorFlow Tensor\" - we're currently discussing this and should have a resolution soon. Either alternative would indeed require exposing an appropriate public symbol, because we definitely don't want to create a dependency on internal symbols.", "@mdanatg Thanks a lot for taking a look.", "@lgeiger \r\nIs this still an issue", "> Is this still an issue\r\n\r\n@Saduf2019 Yes", "@mdanatg Any idea if a public KerasTensor or tf.Tensor subclass will land in TF2.4? ", "Unfortunately the branch cut for 2.4 already happened a couple of weeks ago, so this will definitely not be fixed by then.\r\n\r\nWe're still discussing the final solution with @tomerk. For 2.4, I believe the only solution is to use option 3 above, with a large note around it calling that it's a temporary measure.", "@mdanatg Thanks for the info. It's a bit cumbersome for library authors who want to support multiple versions of TensorFlow, but the workaround works fine.\r\n\r\n> Unfortunately the branch cut for 2.4 already happened a couple of weeks ago, so this will definitely not be fixed by then.\r\n\r\nI probably missed the RFC for the `KerasTensor` changes, otherwise I would've opened the issue earlier.\r\nIn projects I maintain we don't usually test against `tf-nightly` as broken builds are mostly false positives that get resolved shortly afterwards, but maybe I'll reevaluate that to catch issues like this before the release branch gets cut.", "@lgeiger \r\nIs this still an issue.", "@Saduf2019 yes this is still an issue. The current temporary workaround relies on private TensorFlow function which isn't a longterm solution. See https://github.com/tensorflow/tensorflow/issues/44613#issuecomment-732215759", "Is there already a decision made on whether a fix for this will make it into the 2.5 release?", "We discussed creating a TF interface that KerasTensor can use, but unfortunately it not be available in 2.5.", "With TF 2.6.0 this issue is now is even more subtle since `tensorflow/python/keras` was replaced with `keras`, however the old import still work. This leads to the following weirdness in TF 2.6.0:\r\n```python\r\nfrom keras.engine.keras_tensor import KerasTensor  # imported from keras-team/keras\r\n\r\nfrom tensorflow.python.keras.engine.keras_tensor import KerasTensor as KerasTensorFromTF # This import should not exist anymore\r\n\r\nassert KerasTensorFromTF == KerasTensor   # This breaks!!!\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.backend import is_keras_tensor\r\nfrom tensorflow.python.keras.backend import is_keras_tensor as is_keras_tensor_tf  # this import should not exist anymore\r\n\r\nassert is_keras_tensor(tf.keras.Input([10]))\r\nassert is_keras_tensor_tf(tf.keras.Input([10])) # This breaks!!!\r\n```\r\nCheckout [this notebook](https://colab.research.google.com/drive/1-TcroLTdI4fx4dk5P3Cq_vk5AfiYuVOo?usp=sharing).\r\n\r\nI opened #50613 as a separate issue to discuss removal of `tensorflow/python/keras` from the PIP package to hopefully fix this confusion.", "This is still an issue! Has a solution for this made it to any of the upstream branches yet?", "Is there a way to convert KerasTensor to Tensorflow Tensor/Numpy Array? I'm trying to save the output of a model's dense layer into an numpy array after training, but with no avail ;-;"]}, {"number": 44609, "title": "Make TensorFlow Lite available as Swift Package Manager package or XCFramework", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Lite 2.3.0\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now, TensorFlow Lite is available as a CocoaPod or as source for iOS developers. CocoaPods cannot be used from inside a Swift Package Manager (SPM) package. Since SPM is the official package manager from Apple and the way forward, it would be nice to have TensorFlow Lite available as an SPM package or an XCFramwork, in addition to the current CocoaPods distribution.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\niOS developers who are using SPM for module manager or who are not using CocoaPods anymore.\r\n\r\n**Any Other info.**\r\nI have tried creating my own SPM package or XCFramework from the source but could not get it to work because of the native code. But I'm confident it should be possible.", "comments": ["This isn't a solution, but I spent a fair amount of time digging into this trying to find a workaround. This is a little brain dump of my progress.\r\n\r\nI was able to get a custom build of TFLite built and packaged as an XCFramework in a private SPM module, but it required a handful of hacks to get working properly. I'm not really comfortable with bazel though, so I did all of this with writing a bash script over a handful of bazel operations.\r\n\r\nMy workflow is:\r\n\r\n1. Build a device static framework with \r\n\r\n```\r\nbazel build --config=ios --ios_multi_cpus=\"armv7,arm64\" //tensorflow/lite/experimental/ios:TensorFlowLiteC_framework\r\n# copy output file to temporary build dir\r\n```\r\n\r\n2. Build a simulator static framework with \r\n\r\n```\r\nbazel build --config=ios --ios_multi_cpus=\"i386,x86_64\" //tensorflow/lite/experimental/ios:TensorFlowLiteC_framework\r\n# copy output file to temporary build dir\r\n```\r\n\r\n3. Write some `Info.plist` files into the framework folders since bazel seems to omit them and Xcode will silently fail to install your app on a device if they are missing.\r\n\r\n4. Create an xcframework with something like\r\n\r\n```\r\nxcodebuild -create-xcframework \\\r\n   -output \"${OUTPUT}\" \\\r\n   -framework \"${BUILD_DIR}/dev/TensorFlowLiteC.framework\" \\\r\n   -framework \"${BUILD_DIR}/sim/TensorFlowLiteC.framework\r\n```\r\n\r\n5. At this point, I made a sample private git repo that vendored the swift code and the `TensorFlowLiteC.xcframework` file and then added a `Package.swift` file to make it work as a source for SPM.  The [instructions on the apple developer site](https://developer.apple.com/documentation/swift_packages/distributing_binary_frameworks_as_swift_packages) give some helpful instructions for shipping a binary framework with SPM.\r\n\r\nOne limitation is that the xcframeworks will require you to manually disable the iOS simulator for `arm64` architecture because tensorflow (rooted in a problem in bazel-core) assumes that `arm64` is always an ios device, not a simulator. See the bug I filed here for details: https://github.com/bazelbuild/rules_apple/issues/980\r\n\r\nTo \"disable the iOS simulator for `arm64`\", you need to set the following Xcode build setting:\r\n\r\n```\r\nEXCLUDED_ARCHS[sdk=iphonesimulator*] = arm64\r\n```\r\n\r\n\r\n\r\n", "@yyoon and @teijeong have we done any investigation into supporting the Swift Package Manager?", "We haven't done any investigation, and I'll need to learn more about Swift Package Manager to understand how much effort it would be required to support it.\r\n\r\ncc/ @morganchen12 who might have some experience about this.\r\n", "Probably the most challenging part of adding SPM support is you will need to add SPM support for all of your dependencies. SPM packages are also defined per-repository, so one Package.swift file at the root of the tensorflow repo would be responsible for representing Tensorflow, TensorFlowLiteSwift, TensorFlowLiteObjC, TensorFlowLiteC, and any other modules that are buildable in a Swift target.\r\n\r\nIt looks like TensorFlowLiteSwift and TensorFlowLiteObjC have [very shallow dependency graphs](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec?q=TensorFlowLiteC.podspec), so adding SPM support should be straightforward.", "Today I hit the error building the framework with Xcode 12.3 `Building for iOS Simulator, but the linked and embedded framework 'ITLogin.framework' was built for iOS + iOS Simulator.` Which was working with Xcode 12.2 With Xcode 12.3 it is not a feature anymore, it is a bugfix.\r\nIt is time to move to XCFrameworks. Lots of big and small companies are already doing this.\r\n\r\nhttps://gist.github.com/evnik introduced conversion script as a temporary solution: https://gist.github.com/evnik/6762d5c3a4b21f61f13b100e03b62c38", "Yeah, I think XCFramework makes more sense than Swift Package Manager approach, as XCFramework supports packaging compiled binaries as we currently do today.\r\n\r\nAs far as I can tell, CocoaPods supports XCFrameworks already, but bazel still doesn't support building for iOS arm64 simulator (for M1-based macs) nor building XCFrameworks natively. We might need to do some manual work in the interim, with something like the script @greenzeal linked.\r\n\r\n@greenzeal In what exact situation were you seeing that specific error? Can you be a bit more specific and help me reproduce that issue? What does your project structure look like?", "@yyoon \r\nI am trying to create my framework, but have a dependency on TensorFlowLiteC.framework.\r\nto reproduce:\r\n- Xcode 12.3 \r\n- Having a TensorFlowLiteC.framework embedded\r\n- Build for iOS Simulator with release configuration\r\n\r\nThe compiler will complain like this: `Building for iOS Simulator, but the linked and embedded framework 'ITLogin.framework' was built for iOS + iOS Simulator.`\r\n\r\nAnother simple way to reproduce:\r\n- Xcode 12.3\r\n- create dummy Xcode project\r\n- `pod init`\r\n- add to pods `pod 'TensorFlowLiteObjC'` (it will download its dependency TensorFlowLiteC.framework)\r\n- `pod install`\r\n- Build for iOS Simulator with release configuration\r\n\r\nI tried different ways to find a solution like building the Bazel and then TensorFlowLiteC from the source, but all lead to that error.\r\nBesides tried to build TensorFlowLiteC normally and then using the patch mentioned [here](https://gist.github.com/indragiek/e14162c0098d97ee976bceae9441f04d) after with lipo added third architecture to simulator build in the XCFramework.\r\n\r\nI would really appreciate if anyone has a workaround. Or maybe it's possible to build TensorFlowLiteC.framework using Xcode (xcodebuild)?", "btw also I will be happy if there is a way, even a manual one. And will be happy to try to automate it and share it here.", "@greenzeal I could create a dummy project as you described and confirmed that I could reproduce.\r\n\r\nDo you really need to build your framework for arm64 simulator?\r\nIf not, you could go to the target build settings, and add `arm64` as an excluded architecture for `Any iOS Simulator SDK` for both debug and release configs.\r\n\r\n![image](https://user-images.githubusercontent.com/1309817/104322969-eab95780-5528-11eb-8b76-d73767dfcb5b.png)\r\n\r\nIf you have a pressing need to build for arm64 simulator, you might want to try this:\r\nhttps://github.com/bazelbuild/rules_apple/issues/980#issuecomment-738645357\r\n\r\nThis would let you build a TensorFlowLiteC slice for arm64 simulator (haven't had a chance to try it myself). Then, you could add the build output to the \"Frameworks and Libraries\" list of your framework project.", "Excluding arm64 is a temporary option indeed. \r\n\r\nI had tried with a patch, but had no luck yet to make it work. Will try again definitely.\r\n\r\n\r\n ", "What is the current situation of the issue? @yyoon @teijeong @jdduke \r\n\r\nI have tried to create `TensorFlowLiteC.xcframework` following the gist shared by @greenzeal ([message](https://github.com/tensorflow/tensorflow/issues/44609#issuecomment-757868653)) and [steps](https://github.com/tensorflow/tensorflow/issues/44609#issuecomment-724346317) described by @mrosales . I have created `TensorFlowLiteC` Swift Package and `TensorFlowLite` Swift Package (which depends on the former, `TensorFlowLiteC`) and successfully built, run and archived my app. However, it has failed at `Upload to App Store` stage (you can find error logs at the end). It would be nice to have official `TensorFlowLiteC.xcframework` to depend on. Most of the popular packages such as `firebase` have SPM support for their `binaryTarget`s.\r\n\r\nI have used official `TensofFlowLiteC.framework` distributed in `Cocoapods` and `lipo` to create `simulator` and `ios` frameworks, and added an `Info.plist` having relevant fields for a framework. Note that I had to separate `TensorFlowLiteC.xcframework` to its own package to work around some build issues on Xcode. \r\n\r\n## Errors\r\n\r\nI have tried several workarounds to eliminate some errors like first two below.\r\n\r\nI applied the [solution in swift forums](https://forums.swift.org/t/swift-package-binary-framework-issue/41922/3) to eliminate:\r\n\r\n```\r\nERROR ITMS-90680: \"Invalid directory. The bundle Payload/MyApp.app/PlugIns/TensorFlowLiteC.framework is not contained in a correctly named directory. It should be under \"Frameworks\".\"\r\n```\r\n and it seems worked out.\r\n\r\nAnd, I am not sure if it is relevant but I tried to set `ENABLE_BITCODE=NO` for my project to get rid of the error below:\r\n\r\n```\r\nERROR ITMS-90635: \"Invalid Mach-O Format. The Mach-O in bundle \"MyApp.app/Frameworks/TensorFlowLiteC.framework\" isn\u2019t consistent with the Mach-O in the main bundle. The main bundle Mach-O contains arm64(bitcode), while the nested bundle Mach-O contains arm64(machine code). Verify that all of the targets for a platform have a consistent value for the ENABLE_BITCODE build setting.\"\r\n```\r\n\r\nBut as you can see below, __now__ I have something similar and several others:\r\n\r\n```\r\nERROR ITMS-90635: \"Invalid Mach-O Format. The Mach-O in bundle \"MyApp.app/Frameworks/TensorFlowLiteC.framework\" isn\u2019t consistent with the Mach-O in the main bundle. The main bundle Mach-O contains arm64(machine code), while the nested bundle Mach-O contains arm64(machine code). Verify that all of the targets for a platform have a consistent value for the ENABLE_BITCODE build setting.\"\r\nERROR ITMS-90171: \"Invalid Bundle Structure - The binary file 'MyApp.app/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC' is not permitted. Your app can\u2019t contain standalone executables or libraries, other than a valid CFBundleExecutable of supported bundles. Refer to the Bundle Programming Guide at https://developer.apple.com/go/?id=bundle-structure for information on the iOS app bundle structure.\"\r\nERROR ITMS-90124: \"The binary is invalid. The executable 'MyApp.app/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC' has type 'OBJECT' that is not valid. Only 'EXECUTE' is permitted.\"\r\nERROR ITMS-90125: \"The binary is invalid. The encryption info in the LC_ENCRYPTION_INFO load command is either missing or invalid, or the binary is already encrypted. This binary does not seem to have been built with Apple's linker.\"\r\nERROR ITMS-90210: \"Missing load commands. The executable at 'MyApp.app/Frameworks/TensorFlowLiteC.framework' does not have the necessary load commands. Try rebuilding the app with the latest Xcode version. If you are using third party development tools, contact the provider.\"```", "Also looping in @miaout17 ", "Any progress here? Is there nobody using TensorFlowLiteC.framework on m1 simulator now?", "i am having the same issues and i do not manage with the offered work arounds, would appreciate any direct help of how to currently solve this problem of using TensorFlowLiteSwift as a dependency! help!", "Is there any progress?", "Any updates on this issue? What is the way to compile for arm64 simulator?", "Please?"]}, {"number": 44606, "title": "Functionality of writing back Tflite model with multiple subgraphs that is currently loaded in the Tflite Interpreter.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWe can currently write model that have single subgraph using **tflite::SubgraphWriter** present in  **tensorflow/lite/experimental/writer/writer_lib.h** . For my use case I need to update some model weights without converting a new model. So I load model in interpreter, update its weights and write it back to the disk. \r\nBelow is example of loading a model and writing it back. And I can use the rewritten model for future inference.\r\n\r\n    //load model\r\n    std::unique_ptr<tflite::FlatBufferModel> model = \r\n    tflite::FlatBufferModel::BuildFromFile(\"model.tflite\");\r\n\r\n    //build interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model,resolver)(&interpreter);\r\n\r\n    interpreter->AllocateTensors();\r\n\r\n    //write out subgraph\r\n    std::cout<<\"writing out single subgraph\"<<std::endl;\r\n    \r\n    tflite::SubgraphWriter(interpreter->subgraph(0)).Write(\"model.tflite\");\r\n\r\nBut problem occurs when the model has multiple subgraphs e.g when model contains while_loop. In that case I am unable to write it back completely as writer api is made to write a single subgraph. If I write back a model containing while_loop  same code as above and load the model again and allocate tensors I get following error.\r\n```\r\n   ERROR: tensorflow/lite/kernels/while.cc:136 op_data->cond_subgraph_index < subgraphs->size() was not true.\r\n   ERROR: Node number 0 (WHILE) failed to prepare.\r\n```\r\n\r\nThe script for model containing while is:\r\n\r\n   ```\r\nimport tensorflow as tf \r\n   import tensorflow.compat.v1 as tfv1\r\n\r\n   tfv1.disable_eager_execution()\r\n   with tfv1.Session(graph = tf.Graph()) as sess:\r\n      i = tfv1.placeholder(shape = (5,1),dtype = tf.float32)\r\n      a = tfv1.placeholder(shape = (5,1),dtype = tf.float32)\r\n      z = np.zeros(1)\r\n      k = tf.constant(z,dtype = tf.int32,shape=(1,))\r\n      c = lambda i,a,k: tf.less(k,10)\r\n      b = lambda i,a,k:(tf.add(i,a),tf.add(a,a),(k+1),)\r\n      r = tf.while_loop(c,b,[i,a,k])\r\n      converter = tfv1.lite.TFLiteConverter.from_session(sess,[i,a],[r[0]])\r\n      model = converter.convert()\r\n      with open('model.tflite','wb') as model_file:\r\n         model_file.write(model)\r\n```\r\n\r\n**Will this change the current api? How?**\r\nIn the writer_lib there would be an extra api that would take interpreter as input (or all interpreter subgraphs), and write back a tflite model that is represented by its subgraphs.\r\n@abattery, @tensorflower-gardener could be please give any insights in this or any workaround to this problem.\r\n\r\n**Who will benefit with this feature?**\r\nUse cases that needs to update weights of the model can do that by just passing updated parameters instead of converting and downloading the whole model. \r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @jvishnuvardhan, could you please provide any insights or workaround to this problem. ", "@abattery, @tensorflower-gardener could be please give any suggestion on how to solve this problem.", "Hey @ramanps05 we don't have a concrete timeline for this yet, but I will take a look at this next week or so.", "Hi @srjoglekar246 , thanks for the update. I also wanted to ask if functionality to do this task is already there but I can't figure it out.", "It essentially boils down to making the [writer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/writer/writer.cc#L37) write all subgraphs in the Interpreter, instead of just the `primary_subgraph`. The TFLite `Interpreter` has `subgraph(index)` & `subgraphs_size()` methods that we can use to accomplish this.\r\n\r\nFeel free to give it a try if feel inclined :-). ", "I tried the following method - \r\n```std::cout<<\"writing all subgraphs\"<<std::endl;\r\n    FILE * fp = fopen(\"while_rewrite.tflite\",\"wb\");\r\n    if(!fp)\r\n        std::cout<<\"file open failed\"<<std::endl;\r\n    // the current model has 3 subgraphs \r\n    for(int i=0;i<3;i++){\r\n        tflite::SubgraphWriter writerx(interpreter->subgraph(i));\r\n        std::unique_ptr<uint8_t[]> temp_buffer;\r\n        size_t temp_size =0;\r\n        writerx.GetBuffer(&temp_buffer,&temp_size);\r\n        if(fwrite(temp_buffer.get(),1,temp_size,fp)!=temp_size){\r\n            std::cout<<\"writer failed\"<<std::endl;\r\n        }\r\n        temp_buffer.reset();\r\n\r\n    }\r\n    fclose(fp);\r\n```\r\nBut it didn't work, when I am loading the model again I a still getting the same error. \r\n```\r\nERROR: tensorflow/lite/kernels/while.cc:136 op_data->cond_subgraph_index < subgraphs->size() was not true.\r\nERROR: Node number 0 (WHILE) failed to prepare.\r\n```\r\nHere I have written all subgraphs in a single file. I think I am missing out on some key information about subgraphs in implementing it. ", "Hmm. This looks like a bug with the writer. Lemme take a look and get back to you.", "Hi @srjoglekar246, were you able to resolve this. ", "Started looking into this.\r\n\r\nFor your Control Flow conversion to work, you need to use the TF2 converter like this:\r\n\r\n```\r\nclass CFModel(tf.Module):\r\n  @tf.function(\r\n      input_signature=(\r\n          tf.TensorSpec(shape=[5, 1], dtype=tf.float32, name='i'),\r\n          tf.TensorSpec(shape=[5, 1], dtype=tf.float32, name='a'),\r\n      ))\r\n  def cf_model(self, i, a):\r\n    z = np.zeros(1)\r\n    k = tf.constant(z,dtype = tf.int32,shape=(1,))\r\n    c = lambda i,a,k: tf.math.less(k,10)\r\n    b = lambda i,a,k: (tf.add(i,a),tf.add(a,a),(k+1),)\r\n    r = tf.while_loop(c,b,[i,a,k])\r\n    return r[0]\r\n\r\n\r\ndef main(argv):\r\n  to_save = tf.saved_model.save(\r\n      CFModel(), '/path/to/folder')\r\n  converter = lite.TFLiteConverterV2.from_saved_model(\r\n      '/path/to/folder')\r\n  tflite_model = converter.convert()\r\n  open('/path/to/folder/model.tflite',\r\n       'wb').write(tflite_model)\r\n```\r\n\r\nNow the writer tool needs to be refactored to write multiple subgraphs into a single flatbuffer builder. Working on that...", "Does changing converter method from ```from_session``` to ```from_saved_model```change how tflite flatbuffer represents while?\r\nbecause I think it will have a difference in that case. ", "It shouldn't change how WHILE is represented, its just that the TF1 converter may not use our new MLIR converter that converts WHILE (if it does for you in float, maybe you are already using the new converter).\r\n\r\nTechnically, the code I posted should produce the same TFLite model for you (in terms of what it does).", "Okay, but I think that TF1 ```from_session``` converter use mlir converter because during converstion process I see logs with ```tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc``` ", "Aah, then great. It safer to use `from_saved_model` though, since we usually update that API with newer conversion features (mainly because SavedModel is now TF's recommended model format).", "I use ```from_session``` mainly because it is easier to have inputs with varying first dimensions, and in case of multiple outputs I can easily name them according to my choice. I am not able to name outputs in the case of ```from_saved_model```, according to my choice.\r\n \r\nWere you able to write multiple subgraphs with your approach. I feel like in ```while_loop``` one of the subgraphs is inherently different (I think it's ```cond_subgraph```) from others, because if I write them separately, I am able to view two of them in ```netron``` but not the middle one.  ", "hi @srjoglekar246 , were you able to resolve the bug in writer tool. ", "Hi @srjoglekar246 , I am also facing this issue with writing back while loop op since it has multiple subgraphs.\r\nWhen writing subgraphs in loop using the SubgraphWriter, i am not able to write the conditional subgraph alone.\r\n\r\nWere you able to solve the bug ?", "I am currently on vacation, but I have made progress on this. There are\nsome internal usages of this tool, so I have to be careful not to break\nanyone. This should be fixed within a week once I come back in office\nTuesday. Sorry about the delay :(\n\nOn Fri, Nov 27, 2020 at 8:17 AM prasan001 <notifications@github.com> wrote:\n\n> Hi @srjoglekar246 <https://github.com/srjoglekar246> , I am also facing\n> this issue with writing back while loop op since it has multiple subgraphs.\n> When writing subgraphs in loop using the SubgraphWriter, i am not able to\n> wrige the conditional subgraph alone.\n>\n> Were you able to solve the bug ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44606#issuecomment-734903760>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAQAQXI4Q46JJFVB2OZUFPLSR7GIHANCNFSM4TK6SDQA>\n> .\n>\n-- \n*Sachin Joglekar |* * srjoglekar@google.com <srjoglekar@google.com>|* *|*\n*+1-650-309-9486 *\n", "Thanks @srjoglekar246 for the update. ", "Hi @srjoglekar246 , how is the issue resolution coming along. ", "The code is under active review, so should be submitted soon :-)", "Thanks @srjoglekar246 ,", "Hey @srjoglekar246,\r\nGreat to hear that you made progress.\r\nIs the patch ready ?\r\nI tried by looping around each subgraph but still i am not able to write the conditional subgraph properly.\r\nCould you please tell how did you approach or share the patch.", "@prasan001 The patch requires more work than simply looping over the subgraphs :-). \r\nThe end result is an API like `ModelWriter`, similar to `SubgraphWriter`, that takes in an `Interpreter*` ptr. We need to maintain common data structures for entities common to the graph (op codes, buffers, etc)\r\nWill report back once the code changes are in.", "Hi @srjoglekar246, thanks for the details. Are the changes in. ", "The changes just [went in](https://github.com/tensorflow/tensorflow/commit/6de1358c3398428c2c55aa36302235a076393fc9).", "@ramanps05  or @prasan001 could you check if the solution works? I also added a README to the directory, so that might be helpful.", "Thanks @srjoglekar246 , \r\nThough the model writer is still not working for me when I tested using a while loop tflite model.\r\nThis was my script for testing writing back the model\r\n```std::unique_ptr<tflite::FlatBufferModel> model = \r\ntflite::FlatBufferModel::BuildFromFile(\"model_while.tflite\");\r\n\r\n//build interpreter\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model,resolver)(&interpreter);\r\n\r\ninterpreter->AllocateTensors();\r\n\r\n//write out subgraph\r\ntflite::ModelWriter writer(interpreter.get());\r\nstd::string filename = \"model_while_rewrite.tflite\"; \r\nwriter.Write(filename);\r\n```\r\n\r\nWhen I try to load the rewritten model, I get the error \r\n```ERROR: Invalid tensor index 7 in outputs. The subgraph has 7 tensors\r\n\r\nERROR: Invalid tensor index 7 in node outputs. The subgraph has 7 tensors\r\n\r\nERROR: tensorflow/lite/kernels/while.cc:144 cond_subgraph->outputs().size() != 1 (0 != 1)\r\nERROR: Node number 0 (WHILE) failed to prepare.\r\n```\r\nAlso, the written model is slightly **smaller** in shape. Original model, which I converted using the following script\r\n\r\n> Started looking into this.\r\n> \r\n> For your Control Flow conversion to work, you need to use the TF2 converter like this:\r\n> \r\n> ```\r\n> class CFModel(tf.Module):\r\n>   @tf.function(\r\n>       input_signature=(\r\n>           tf.TensorSpec(shape=[5, 1], dtype=tf.float32, name='i'),\r\n>           tf.TensorSpec(shape=[5, 1], dtype=tf.float32, name='a'),\r\n>       ))\r\n>   def cf_model(self, i, a):\r\n>     z = np.zeros(1)\r\n>     k = tf.constant(z,dtype = tf.int32,shape=(1,))\r\n>     c = lambda i,a,k: tf.math.less(k,10)\r\n>     b = lambda i,a,k: (tf.add(i,a),tf.add(a,a),(k+1),)\r\n>     r = tf.while_loop(c,b,[i,a,k])\r\n>     return r[0]\r\n> \r\n> \r\n> def main(argv):\r\n>   to_save = tf.saved_model.save(\r\n>       CFModel(), '/path/to/folder')\r\n>   converter = lite.TFLiteConverterV2.from_saved_model(\r\n>       '/path/to/folder')\r\n>   tflite_model = converter.convert()\r\n>   open('/path/to/folder/model.tflite',\r\n>        'wb').write(tflite_model)\r\n> ```\r\n> \r\n> Now the writer tool needs to be refactored to write multiple subgraphs into a single flatbuffer builder. Working on that...\r\n\r\nhas size **2712** bytes while the rewritten model has size **1948** bytes. So it looks like it is not writing the whole model. \r\n\r\n", "@ramanps05 The smaller size is expected, mainly because the tool skips writing parts of the model that are unnecessary or default (like strings, empty quantization params, etc).\r\n\r\nI re-ran the tool on the model generated from that script, and it seems to run fine with our benchmark tool. I saw a visualization of the re-written model & it seems to have one input on the subgraph. See the attached re-written model.\r\nCan you post your inference code?\r\n\r\n\r\n[rewritten.tflite.zip](https://github.com/tensorflow/tensorflow/files/5711282/rewritten.tflite.zip)\r\n", "Hi @srjoglekar246,\r\nI tried generating the model using the sample script you shared above for while loop, and got the same error as @ramanps05.\r\n\r\nThe below is the code i was running:\r\n```\r\ntflite::FlatBufferModel::BuildFromFile(\"while_model.tflite\"); \r\n\r\ntflite::ops::builtin::BuiltinOpResolver res; \r\n\r\nstd::unique_ptr<tflite::Interpreter> interpreter; \r\n\r\ntflite::InterpreterBuilder(*model, res)(&interpreter); \r\n\r\ninterpreter->AllocateTensors(); \r\ninterpreter->Invoke();\r\n\r\nwriter(interpreter.get()); \r\n\r\nstd::string filename = \"rewritten_while_model.tflite\"; \r\n\r\nwriter.Write(filename);\r\n```\r\n\r\nFirst time, i load the while model and write the model using your ModelWriter as above.\r\nThen if I try to build interpreter on the rewritten while model, i was also getting same error as @ramanps05 \r\n\r\n```\r\nERROR: Invalid tensor index 7 in node outputs. The subgraph has 7 tensors.\r\nERROR: tensorflow/lite/kernels/while.cc:144 cond_subgraph->outputs().size() != 1 (0 != 1)\r\nERROR: Node number 0 (WHILE) failed to prepare.\r\n\r\nERROR: Invoke called on model that is not ready.\r\nSegmentation fault(core dumped)\r\n```\r\n\r\nI suspect we are missing something while handling the outputs of conditional subgraph.", "> @ramanps05 The smaller size is expected, mainly because the tool skips writing parts of the model that are unnecessary or default (like strings, empty quantization params, etc).\r\n> \r\n> I re-ran the tool on the model generated from that script, and it seems to run fine with our benchmark tool. I saw a visualization of the re-written model & it seems to have one input on the subgraph. See the attached re-written model.\r\n> Can you post your inference code?\r\n> \r\n> [rewritten.tflite.zip](https://github.com/tensorflow/tensorflow/files/5711282/rewritten.tflite.zip)\r\n\r\n@srjoglekar246  I am attaching my inference code.\r\n[while_test.zip](https://github.com/tensorflow/tensorflow/files/5714929/while_test.zip)\r\nIt runs fine for the first time. But gives the error as mentioned above if I run it again. \r\n", "hi @srjoglekar246 , could you look at whether we are using ModelWriter correctly or not. ", "Yup, taking a look...", "@ramanps05 @prasan001 Could you try the model in the zipped file on the [above](https://github.com/tensorflow/tensorflow/issues/44606#issuecomment-747603830) comment? Does that also give an error, like your re-written model?\r\n\r\nAlso, could you try with a different file-name for the rewritten model?", "HI @srjoglekar246 ,\r\n- yes I used your model that you have sent. I was able to run it for the first run, i.e load and execute but when I rewrote the model using https://github.com/tensorflow/tensorflow/files/5714929/while_test.zip I was not able to load it again. \r\n- Also, even if I try using a different file name, I am still getting the same issue. \r\n\r\nIf you were able to rewrite it and there is no issue in my model writing approach, than there might be difference in the way we are building the intrepreter or/and generating the model. I am using python api to convert the model to tflite. ", "Attached is the model I got from serializing the rewritten model. If that doesn't work for you, then there might be an issue with your model-running code. I get an output like this:\r\n\r\n```\r\nOutput of the model is -\r\n0\r\n0\r\n0\r\n0\r\n0\r\n```\r\nis that correct?\r\n\r\nNote that my scripts for serialization & evaluating are different. (I don't see how that should affect things if you use different file names for loading & serializing the interpreter). My inference code:\r\n```\r\n  std::string model_file_path = \".../rewritten2.tflite\";\r\n  std::unique_ptr<FlatBufferModel> model;\r\n  std::unique_ptr<ops::builtin::BuiltinOpResolver> resolver;\r\n  std::unique_ptr<Interpreter> interpreter;\r\n  model = FlatBufferModel::BuildFromFile(model_file_path.c_str());\r\n  resolver.reset(new ops::builtin::BuiltinOpResolver);\r\n  InterpreterBuilder(*model, *resolver)(&interpreter);\r\n  if (!interpreter) {\r\n    return kTfLiteError;\r\n  }\r\n\r\n  interpreter->AllocateTensors();\r\n\r\n  float* input_1 = interpreter->typed_input_tensor<float>(0);\r\n  float* input_2 = interpreter->typed_input_tensor<float>(1);\r\n\r\n  for (int i = 0; i < 5; i++) {\r\n    input_1[i] = 0;\r\n    input_2[i] = i + 1;\r\n  }\r\n\r\n  // print outputs\r\n  float* inf_output = interpreter->typed_output_tensor<float>(0);\r\n  std::cout << \"\\nOutput of the model is - \\n\";\r\n  for (int i = 0; i < 5; i++) {\r\n    LOG(INFO)  << inf_output[i] << ' ';\r\n  }\r\n```\r\n\r\nFor serializing, I use `tensorflow/lite/tools/serialization/writer.cc`\r\n\r\n[rewritten2.tflite.zip](https://github.com/tensorflow/tensorflow/files/5731247/rewritten2.tflite.zip)\r\n\r\n\r\n", "@srjoglekar246 you are getting 0 output because you are not doing ```interpreter->Invoke()``` before reading the outputs. \r\nThough I am still getting the error while loading the rewritten model.\r\n- **Tf workspace** - tf master, cloned about an hour before, with head on ```commit b220d07eab20c869a31cc660bf497552d710cfc9```\r\n- **Build file and cpp** - [tflite_test.zip](https://github.com/tensorflow/tensorflow/files/5732000/tflite_test.zip)\r\n- **Model writer include path** - ```#include \"tensorflow/lite/tools/serialization/writer_lib.h\"```\r\n- **Model writer usage** - ```tflite::ModelWriter writer(interpreter.get());\r\n  std::string filename = \"model_while.tflite\";\r\n  writer.Write(filename);```\r\n\r\n- **Error on running second time** - \r\n```ERROR: Invalid tensor index 7 in outputs. The subgraph has 7 tensors\r\n\r\nERROR: Invalid tensor index 7 in node outputs. The subgraph has 7 tensors\r\n\r\nERROR: tensorflow/lite/kernels/while.cc:144 cond_subgraph->outputs().size() != 1 (0 != 1)\r\nERROR: Node number 0 (WHILE) failed to prepare.\r\n\r\nSegmentation fault (core dumped)```\r\n\r\n", "Model is same as this [https://github.com/tensorflow/tensorflow/files/5731247/rewritten2.tflite.zip](url)", "I think I found whats wrong. The original authors of the tool added a condition that prevents writing tensors whose allocation type doesn't match some pre-set values. The model in question contains a dynamic tensor (whose output shape isn't known until runtime), and it doesn't get serialized because of that condition.\r\n\r\nMaybe you could try remove [this if- block](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/serialization/writer_lib.cc#L192) in `writer_lib.cc`:\r\n\r\n```\r\nif (tensor->allocation_type != kTfLiteArenaRw &&\r\n          tensor->allocation_type != kTfLiteMmapRo &&\r\n          tensor->allocation_type != kTfLiteArenaRwPersistent)\r\n        continue;\r\n```\r\n\r\nDoes that fix the issue?\r\nI will send a change that should hopefully fix this.", "Hi @srjoglekar246 , I tried the above method. It worked for the simple while loop that I have shared with you. But I got segmentaion fault while writing another model using it. The model has while loops as well as many other operations. I will not be able to share the model with you due to privacy reasons. \r\nBut simply commenting ```if loop``` is breaking something else, at least that I can say.\r\nI will further try to test in which operation it is causing the problem, and let you know if I find anything. ", "We found a another bug related to variable tensors (for LSTMs etc), which we are about to fix.\r\n\r\nSince this tool was written a while back for some limited use-cases, unfortunately you would need to debug where the error is yourself :-(\r\nI don't have too many cycles to debug myself, but I can help landing any bug-fixes you figure out.", "My model has LSTMs too, so that might be the bug. Though there could also be something else too. I will try to find if any other bug is there.", "FYI, we just landed the fix for LSTMs. Do check your model again, to see if the issue is fixed?", "Hi @srjoglekar246 , I checked the new fix with my model. It was same as before i.e not working for my model. I did some debugging and found that the model is failing for matmul operation.\r\n- If I use current ```model_writer``` in tf master, ```while_loop``` is written back fine but for any model involving ```matmul``` op I am getting following during model writing. \r\n```seg fault - nullptr dereference in ExportTensor```\r\nI am attaching two [models](https://github.com/tensorflow/tensorflow/files/5751077/matmul_models.zip) that I used for testing one is a single matmul operation and other is matmul followed by while loop. ", "I think the library isn't handling optional tensors right (the 3rd input to Matmul is optional for TFLite).\r\nCan you try to add the following line [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/serialization/writer_lib.cc#L192):\r\n\r\n```\r\nif (!tensor->bytes && !tensor->dims) continue;\r\n```\r\n\r\nThis skips trying to write a null tensor.", "HI @srjoglekar246 , thanks it worked. But would it cause any problem with other operations ?", "I don't think so; it only fixes behavior for optional tensors"]}, {"number": 44590, "title": "Layer.add_loss has wrong behaviour when building Sequential model in for loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1     2.3.1\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: Not using CUDA\r\n- GPU model and memory: Not using GPU\r\n\r\n\r\n\r\nThis code:\r\n```\r\n    import tensorflow as tf\r\n\r\n    tf.random.set_seed(0)\r\n    model = tf.keras.Sequential()\r\n    inputs = tf.keras.Input(shape=(1,))\r\n    model.add(inputs)\r\n    layer1 = tf.keras.layers.Dense(1)\r\n    layer1.add_loss(lambda :tf.reduce_sum(layer1.kernel))\r\n    model.add(layer1)\r\n    layer2 = tf.keras.layers.Dense(1)\r\n    layer2.add_loss(lambda :tf.reduce_sum(layer2.kernel))\r\n    model.add(layer2)\r\n\r\n    print(model.get_weights())\r\n    print(model.losses)\r\n```\r\n\r\nCorrectly produces:\r\n   [array([[-0.7206192]], dtype=float32), array([0.], dtype=float32), array([[0.19195998]], dtype=float32), array([0.], \r\n   dtype=float32)]\r\n   [<tf.Tensor: shape=(), dtype=float32, numpy=-0.7206192>, <tf.Tensor: shape=(), dtype=float32, numpy=0.19195998>]\r\n\r\nI.e. the first kernel weight is the first loss and the second kernel weight is the second loss.\r\n\r\n\r\nI would expect the following code to produce the same result:\r\n\r\n```\r\n    tf.random.set_seed(0)\r\n    model = tf.keras.Sequential()\r\n    inputs = tf.keras.Input(shape=(1,))\r\n    model.add(inputs)\r\n    for i in range(2):\r\n        layer = tf.keras.layers.Dense(1)\r\n        layer.add_loss(lambda :tf.reduce_sum(layer.kernel))\r\n        model.add(layer)\r\n\r\n    print(model.get_weights())\r\n    print(model.losses)\r\n```\r\n\r\nBut it produces:\r\n    [array([[-0.7206192]], dtype=float32), array([0.], dtype=float32), array([[0.19195998]], dtype=float32), array([0.], \r\n    dtype=float32)]\r\n    [<tf.Tensor: shape=(), dtype=float32, numpy=0.19195998>, <tf.Tensor: shape=(), dtype=float32, numpy=0.19195998>]\r\nI.e. it takes the second kernel weight for both losses.\r\n\r\n\r\n", "comments": ["@merplumander \r\n\r\nPlease, refer similar issue #44048 and see if it helps you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I think this issue is a different one. I'm not attempting to use the same layer instance twice. The variable \"layer\" holds two different instances in the for loop. This also happens when the layer instances have different numbers of units for example.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nlayer_units = [2, 1]\r\n\r\ntf.random.set_seed(0)\r\nmodel = tf.keras.Sequential()\r\ninputs = tf.keras.Input(shape=(1,))\r\nmodel.add(inputs)\r\nlayer1 = tf.keras.layers.Dense(layer_units[0])\r\nlayer1.add_loss(lambda :tf.reduce_sum(layer1.kernel))\r\nmodel.add(layer1)\r\nlayer2 = tf.keras.layers.Dense(layer_units[1])\r\nlayer2.add_loss(lambda :tf.reduce_sum(layer2.kernel))\r\nmodel.add(layer2)\r\n\r\nprint(model.losses)\r\n```\r\n\r\nCorrectly produces:\r\n`[<tf.Tensor: shape=(), dtype=float32, numpy=-1.4183385>, <tf.Tensor: shape=(), dtype=float32, numpy=-0.6315678>]`\r\n\r\n\r\n\r\nWhereas:\r\n\r\n```\r\nlayer_units = [2, 1]\r\n\r\ntf.random.set_seed(0)\r\nmodel = tf.keras.Sequential()\r\ninputs = tf.keras.Input(shape=(1,))\r\nmodel.add(inputs)\r\nfor i in range(2):\r\n    layer = tf.keras.layers.Dense(layer_units[i])\r\n    layer.add_loss(lambda :tf.reduce_sum(layer.kernel))\r\n    model.add(layer)\r\n\r\nprint(model.losses)\r\n```\r\n\r\nIncorrectly produces:\r\n`[<tf.Tensor: shape=(), dtype=float32, numpy=-0.6315678>, <tf.Tensor: shape=(), dtype=float32, numpy=-0.6315678>]`\r\n\r\n\r\nSo the code with the for loop incorrectly uses the loss of the last layer for both layers.", "I found a workaround for my specific use case, but I still think this is a bug and should be addressed in general.", "I have tried in colab with TF version 2.3.1 and nightly version(`2.5.0-dev20201116`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/cf06e08a197f11de3d6733613d0e4e5f/untitled518.ipynb). Thanks!", "I am able to reproduce the issue but am having trouble pinpointing it. I am having trouble deciding whether the bug is caused in model.losses or model.add_loss(). If you attempt to print model.losses in the for loop before line 10, as seen below, in the buggy test case the program throws an attribute error: \r\n\r\n`import tensorflow as tf\r\n\r\ntf.random.set_seed(0)\r\nmodel = tf.keras.Sequential()\r\ninputs = tf.keras.Input(shape=(1,))\r\nmodel.add(inputs)\r\nfor i in range(2):\r\n    layer = tf.keras.layers.Dense(1)\r\n    print(model.losses)\r\n    layer.add_loss(lambda :tf.reduce_sum(layer.kernel))\r\n    model.add(layer)\r\nprint(model.get_weights())\r\nprint(model.losses)`\r\n\r\n`[]\r\nTraceback (most recent call last):\r\n  File \"GitTestCase2.py\", line 10, in <module>\r\n    print(model.losses)\r\n  File \"/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1470, in losses\r\n    loss_tensor = regularizer()\r\n  File \"/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1546, in _tag_callable\r\n    loss = loss()\r\n  File \"GitTestCase2.py\", line 11, in <lambda>\r\n    layer.add_loss(lambda :tf.reduce_sum(layer.kernel))\r\nAttributeError: 'Dense' object has no attribute 'kernel'\u2029`", "Thanks for looking into it!", "I am working with evonbevern to try to find this bug, after a bit of testing it does not seem to be the model.losses, we are currently trying to see if it is something in the add_loss function, or if the problem exists elsewhere", "So after playing with the code a bit more, the first large difference I have found between the provided test cases is that in each test case, the second time in each that layer.add_loss(lambda :tf.reduce_sum(layer.kernel)) is called, in add_loss() is another function, _tag_callable(), that returns different values, the correct value for test case 1 but the incorrect value in test case 2. In _tag_callable() is another call to a function loss() that modifies the loss variable in _tag_callable(). I am having trouble either finding where loss() is located or if this is some sort of other call. Tracing and investigating loss() will help tell if there is a incorrect value being returned somewhere or if the first value is being overwritten. evonbevern and I are students at the university of michigan investigating this bug as a final project, and this is both of our first time working with and contributing to such a large code bank so any help or advice would be appreciated! ", "Sounds like you guys are doing a good job so far \ud83d\udc4d\r\nI\u2019m also working on my final project, so we\u2019re all in the same boat ^^", "Well after another 12 hours of playing with the code, I have narrowed down where the issue may be but have yet to solve it. The line loss() I believe is an overloaded operator that returns the tensor information including the numpy value. The second time through the loop this loss() value returns the wrong value but I do not think it modifies the value at all so  I believe the bug has to be somewhere in the line layer = tf.keras.layers.Dense(layer_units[i]). Even though the second time through the loop is a new layer instance, I believe there must be some reference to the original held in memory somewhere that gets overwritten. I am still unsure why this is though. I had a lot of trouble getting the numpy information out of both the layer and model references intermittently while running the test case making it very difficult to narrow where numpy is changed for testing.", "@merplumander \r\nIs this still an issue, can you please try on latest tf version and update us.", "Hi,\r\nthanks for following up on this. It's still an issue for me on tf version 2.4.1.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/05382c3748387ba6797c95b683971a8c/44590.ipynb). Thanks!"]}, {"number": 44585, "title": "next() not raising StopIteration in tf.python.keras.preprocessing.image.DirectoryIterator", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: no\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.1\r\n- Mobile device: n/a\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.6\r\n- Bazel version: n/a\r\n- GCC/Compiler version: n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nI would like to iterate over image files in a directory using `flow_from_directory()` but calling `next()` does not raise `StopIteration` at the end of the data. Instead, it produces an infinite loop. Is this expected the behavior?\r\n\r\nThis relates to issues [45](https://github.com/keras-team/keras-preprocessing/issues/45), [226](https://github.com/keras-team/keras-preprocessing/issues/226), and [269](https://github.com/keras-team/keras-preprocessing/issues/269) in the keras preprocessing repo. I understand development has moved to tensorflow, hence posting it here.\r\n\r\n\r\n**Describe the expected behavior**\r\nI expected `StopIteration` to be raised.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe code below was adapted from [here](https://github.com/keras-team/keras-preprocessing/issues/226#issuecomment-516648880).\r\n\r\n```\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, save_img\r\nimport numpy as np\r\nimport os\r\n\r\nbatch_size = 3\r\nn_images = 5\r\n\r\ndirectory = '/tmp/test'\r\nclasses = ['dog', 'cat']\r\n\r\nfor i in range(n_images):\r\n    label = np.random.choice(classes)\r\n    os.makedirs(os.path.join(directory, label), exist_ok=True)\r\n    filename = os.path.join(directory, label, str(i) + '.jpg')\r\n    save_img(filename, np.ones((64, 64, 3)))\r\n\r\ngen = ImageDataGenerator().flow_from_directory(\r\n    '/tmp/test',\r\n    class_mode=None,\r\n    batch_size=batch_size\r\n)\r\n\r\nfor _ in range(gen.__len__()):\r\n    imgs = next(gen)\r\n    print(imgs.shape)\r\n\r\ngen.reset()\r\nwhile True:\r\n    imgs = next(gen)\r\n    print(imgs.shape)\r\n```\r\n\r\n**Other info / logs**\r\nn/a", "comments": ["@stephengmatthews \r\nI ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/0a2d008b43a7d31de6efb51fb6cea04d/untitled458.ipynb) please confirm, this doesn't seem like a bug at tf end.", "Thank you. My assumption is that `next()` handles the end of iteration, similar to Python's built-in [`next()`](https://docs.python.org/3.8/library/functions.html#next) and `tf.data.Iterator`'s [`get_next()`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator#get_next).\r\n\r\nCould the use case for `next()` looping indefinitely be clarified, please? This [training workflow guide](https://www.tensorflow.org/guide/data#processing_multiple_epochs) suggests that the use case is similar to using `Dataset.repeat()` \"to iterate over a dataset in multiple epochs\".\r\n\r\n[Older versions](https://faroit.com/keras-docs/2.1.1/preprocessing/image/) of the keras API state that \"data will be looped over (in batches) indefinitely\". The tensorflow API also mentions this in an [example (see penultimate line)](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#examples_2). I wonder whether it could be clarified explicitly in the documentation (rather than buried as a comment in a code snippet). For me at least, [older versions](https://faroit.com/keras-docs/2.1.1/preprocessing/image/) of the keras API were much easier to understand that the loop is indefinite without some digging around.\r\n\r\nWhat are your thoughts on this?"]}, {"number": 44571, "title": "TFRecordWriter create in parent process can't work properly in child process", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI generate tfrecord from COCO semantic segmentation dataset with the following code. I get the error message \"tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12\" when I load the generated tfrecord.\r\n\r\n```python\r\n#!/usr/bin/python3\r\n\r\nfrom os.path import join;\r\nfrom math import ceil;\r\nfrom multiprocessing import Process, Lock;\r\nfrom pycocotools.coco import COCO;\r\nimport numpy as np;\r\nimport cv2;\r\nimport tensorflow as tf;\r\n\r\nPROCESS_NUM = 80;\r\n\r\ndef parse_function(serialized_example):\r\n\r\n  feature = tf.io.parse_single_example(\r\n    serialized_example,\r\n    features = {\r\n      'image': tf.io.FixedLenFeature((), dtype = tf.string, default_value = ''),\r\n      'shape': tf.io.FixedLenFeature((3,), dtype = tf.int64),\r\n      'label': tf.io.VarLenFeature(dtype = tf.float32)\r\n    }\r\n  );\r\n  shape = tf.cast(feature['shape'], dtype = tf.int32);\r\n  data = tf.io.decode_jpeg(feature['image']);\r\n  data = tf.reshape(data, shape);\r\n  data = tf.cast(data, dtype = tf.float32);\r\n  label = tf.sparse.to_dense(feature['label'], default_value = 0);\r\n  label = tf.reshape(label, (shape[0], shape[1])); # label.shape = (h, w)\r\n  return data, label;\r\n\r\ndef create_dataset(image_dir, label_dir, trainset = True):\r\n\r\n  anno = COCO(join(label_dir, 'instances_train2017.json' if trainset else 'instances_val2017.json'));\r\n  writer = tf.io.TFRecordWriter('trainset.tfrecord' if trainset else 'testset.tfrecord');\r\n  if writer is None:\r\n    print('invalid output file!');\r\n    exit(1);\r\n  imgs_for_each = ceil(len(anno.getImgIds()) / PROCESS_NUM);\r\n  handlers = list();\r\n  lock = Lock();\r\n  for i in range(PROCESS_NUM):\r\n    handlers.append(Process(target = worker, args = (anno, writer, image_dir, anno.getImgIds()[i * imgs_for_each:(i+1) * imgs_for_each] if i != PROCESS_NUM - 1 else anno.getImgIds()[i * imgs_for_each:], lock)));\r\n    handlers[-1].start();\r\n  for handler in handlers:\r\n    handler.join();\r\n  writer.close();\r\n\r\ndef worker(anno, writer, image_dir, image_ids, lock):\r\n  for image in image_ids:\r\n    img_info = anno.loadImgs([image])[0];\r\n    img = cv2.imread(join(image_dir, img_info['file_name']));\r\n    if img is None:\r\n      print('can\\'t open image %s' % (join(image_dir, img_info['file_name'])));\r\n      continue;\r\n    mask = np.zeros((img_info['height'], img_info['width']));\r\n    for category in anno.getCatIds():\r\n      annIds = anno.getAnnIds(imgIds = image, catIds = category);\r\n      anns = anno.loadAnns(annIds);\r\n      for ann in anns:\r\n        # for every instance of category in current image\r\n        instance_mask = anno.annToMask(ann);\r\n        mask = np.maximum(mask, instance_mask * category);\r\n    trainsample = tf.train.Example(features = tf.train.Features(\r\n      feature = {\r\n        'image': tf.train.Feature(bytes_list = tf.train.BytesList(value = [tf.io.encode_jpeg(img).numpy()])),\r\n        'shape': tf.train.Feature(int64_list = tf.train.Int64List(value = list(img.shape))),\r\n        'label': tf.train.Feature(float_list = tf.train.FloatList(value = tf.reshape(mask, (-1,))))\r\n      }\r\n    ));\r\n    lock.acquire();\r\n    writer.write(trainsample.SerializeToString());\r\n    lock.release();\r\n\r\nif __name__ == \"__main__\":\r\n\r\n  assert tf.executing_eagerly();\r\n  from sys import argv;\r\n  if len(argv) != 4:\r\n    print('Usage: %s <train image dir> <test image dir> <anno dir>' % (argv[0]));\r\n    exit(1);\r\n  create_dataset(argv[1], argv[3], True);\r\n  create_dataset(argv[2], argv[3], False);\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nthe tfrecord should be loaded without problem.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nthe above code can generate tfrecord from COCO2017. the generated tfrecord can't be loaded correctly. you may want to comment the line generating trainset.tfrecord. the smaller testset.tfrecord can be generated more quickly.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n>Traceback (most recent call last):\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2102, in execution_mode\r\n    yield\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 758, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2610, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12 [Op:IteratorGetNext]\r\n\r\n>During handling of the above exception, another exception occurred:\r\n\r\n>Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 736, in __next__\r\n    return self.next()\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 772, in next\r\n    return self._next_internal()\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 764, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2105, in execution_mode\r\n    executor_new.wait()\r\n  File \"/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 67, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12\r\n", "comments": ["@breadbread1984,\r\nOn running the code I am facing an error stating `IndexError: list index out of range`, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ed665bb8511ba816819998fbdc05c864/44571.ipynb). \r\n\r\nCould you please provide the commands/arguments along with all the supporting files required to run the code?", "Alternatively, you can run the code on [Google Colab](https://colab.research.google.com/) and share the notebook with us.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/13463#issuecomment-460947809) from a similar issue and let us know if it helps. Thanks!", "@amahendrakar i tried the solution given in the similar issue. it doesn't help in my case. my problem may caused by multiprocessing. without it, the generated tfrecord is ok. but generating tfrecord with single process takes a lot of time.\r\nyou can execute the code with the following command\r\n```bash\r\npython3 create_datasets.py <path/to/train2017> <path/to/val2017> <path/to/annotations>\r\n```\r\ntrain2017 val2017 annotations are three directories of COCO2017 dataset.\r\n\r\nto run the code, you need a multicore cpu. you don't need to generate the trainset.tfrecord which takes too long. so you may want to comment the following line\r\n\r\n```python\r\ncreate_dataset(argv[1], argv[3], True);\r\n```\r\nafter generating testset.tfrecord. you could reproduce the error message through\r\n```python\r\ntestset = tf.data.TFRecordDataset('testset.tfrecord');\r\nfor sample in testset:\r\n  pass;\r\n```", "I found TFRecordWriter created within child process works without problem. Whereas, TFRecordWriter created in the parent process can't work properly in child process. The following code is a modification of the problem code. it can work successfully. but I have to load dataset from several separate tfrecord files. \r\n\r\nSo, please solve the problem that **TFRecordWriter created in parent process can't work properly in child process**. The solution can facilitate dataset generation with multiprocessing.\r\n\r\n```python\r\n#!/usr/bin/python3\r\n\r\nfrom os import mkdir;\r\nfrom os.path import join, exists;\r\nfrom shutil import rmtree;\r\nfrom math import ceil;\r\nfrom multiprocessing import Process;\r\nfrom pycocotools.coco import COCO;\r\nimport numpy as np;\r\nimport cv2;\r\nimport tensorflow as tf;\r\n\r\nPROCESS_NUM = 80;\r\n\r\ndef parse_function(serialized_example):\r\n\r\n  feature = tf.io.parse_single_example(\r\n    serialized_example,\r\n    features = {\r\n      'image': tf.io.FixedLenFeature((), dtype = tf.string, default_value = ''),\r\n      'shape': tf.io.FixedLenFeature((3,), dtype = tf.int64),\r\n      'label': tf.io.VarLenFeature(dtype = tf.float32)\r\n    }\r\n  );\r\n  shape = tf.cast(feature['shape'], dtype = tf.int32);\r\n  data = tf.io.decode_jpeg(feature['image']);\r\n  data = tf.reshape(data, shape);\r\n  data = tf.cast(data, dtype = tf.float32);\r\n  label = tf.sparse.to_dense(feature['label'], default_value = 0);\r\n  label = tf.reshape(label, (shape[0], shape[1])); # label.shape = (h, w)\r\n  return data, label;\r\n\r\ndef create_dataset(image_dir, label_dir, trainset = True):\r\n\r\n  anno = COCO(join(label_dir, 'instances_train2017.json' if trainset else 'instances_val2017.json'));\r\n  if exists('trainset' if trainset else 'testset'): rmtree('trainset' if trainset else 'testset');\r\n  mkdir('trainset' if trainset else 'testset');\r\n  imgs_for_each = ceil(len(anno.getImgIds()) / PROCESS_NUM);\r\n  handlers = list();\r\n  filenames = list();\r\n  for i in range(PROCESS_NUM):\r\n    filename = ('trainset_part_%d' if trainset else 'testset_part_%d') % i;\r\n    filenames.append(join('trainset' if trainset else 'testset', filename));\r\n    handlers.append(Process(target = worker, args = (join('trainset' if trainset else 'testset', filename), anno, image_dir, anno.getImgIds()[i * imgs_for_each:(i+1) * imgs_for_each] if i != PROCESS_NUM - 1 else anno.getImgIds()[i * imgs_for_each:])));\r\n    handlers[-1].start();\r\n  for handler in handlers:\r\n    handler.join();\r\n\r\ndef worker(filename, anno, image_dir, image_ids):\r\n  writer = tf.io.TFRecordWriter(filename);\r\n  for image in image_ids:\r\n    img_info = anno.loadImgs([image])[0];\r\n    img = cv2.imread(join(image_dir, img_info['file_name']));\r\n    if img is None:\r\n      print('can\\'t open image %s' % (join(image_dir, img_info['file_name'])));\r\n      continue;\r\n    mask = np.zeros((img_info['height'], img_info['width']));\r\n    for category in anno.getCatIds():\r\n      annIds = anno.getAnnIds(imgIds = image, catIds = category);\r\n      anns = anno.loadAnns(annIds);\r\n      for ann in anns:\r\n        # for every instance of category in current image\r\n        instance_mask = anno.annToMask(ann);\r\n        mask = np.maximum(mask, instance_mask * category);\r\n    trainsample = tf.train.Example(features = tf.train.Features(\r\n      feature = {\r\n        'image': tf.train.Feature(bytes_list = tf.train.BytesList(value = [tf.io.encode_jpeg(img).numpy()])),\r\n        'shape': tf.train.Feature(int64_list = tf.train.Int64List(value = list(img.shape))),\r\n        'label': tf.train.Feature(float_list = tf.train.FloatList(value = tf.reshape(mask, (-1,))))\r\n      }\r\n    ));\r\n    writer.write(trainsample.SerializeToString());\r\n  writer.close();\r\n\r\nif __name__ == \"__main__\":\r\n\r\n  assert tf.executing_eagerly();\r\n  from sys import argv;\r\n  if len(argv) != 4:\r\n    print('Usage: %s <train image dir> <test image dir> <anno dir>' % (argv[0]));\r\n    exit(1);\r\n  create_dataset(argv[2], argv[3], False);\r\n  create_dataset(argv[1], argv[3], True);\r\n```", "Colab notebook crashes on running the given code snippet. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/914b8ec60de3faee6dcce97f743dca74/44571.ipynb). Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/04ecf3266e1fd0bb53658dd3c75fd00a/44571.ipynb). Thanks!"]}, {"number": 44566, "title": "Enforce single optimized kernel implementation", "body": "Currently we can provide potentially multiple optimized kernel implementations via the `TAGS` parameter to the TFLM Makefile.\r\n\r\nWhile this was originally supported, we don't have any use-case for it and are moving towards enforcing a single optimized kernel implementation. This will also reduce some complexity in the makefile and allow for improved debugging.\r\n\r\nCorresponding internal issue: http://b/170501366", "comments": []}, {"number": 44565, "title": "from_tensor_slices not compatible with sparse data", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab / TF 2.3.0\r\n\r\n**Describe the current behavior**\r\nIt's stated in the documentation that `Dataset` is able to handle `SpareTensors` on top of `Ragged` tensors and the other standard data types. \r\n\r\nIssue is `from_tensor_slices` is not able to handle a list of `SparseTensors`, but `from_tensors` which accepts a single datapoint is able to instantiate it. Currently `Dataset.from_generator` is not able to handle `Sparse` datatype either (in the current release at least) https://github.com/tensorflow/tensorflow/pull/41981 so I'm not sure how one is supposed to handle datasets that are compromised of both Sparse and dense data. \r\n\r\n\r\n**Describe the expected behavior**\r\n`from_tensor_slices` should accept a list of `SparseTensors` given that there is exactly the same number of items in the list as the other features passed into `from_tensor_slices` \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab Link](https://colab.research.google.com/drive/1952M7pHEgpl0KSFPkHz7gu9BpxGejdfu?usp=sharing\r\n)\r\n**Other info / logs** \r\n\r\nI'm not sure if it's just me, but when I read this [part of the documentation](https://www.tensorflow.org/guide/data#dataset_structure) I was under the impression that this feature would be supported. Thus you may identify this as a `feature_request` rather than a bug if I've misunderstood the documentarian. \r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c36fffc4912e041dd75b2d011bdb6c84/44565.ipynb). Thanks!", "@amahendrakar No worries! I can potentially look into creating a fix for this in the next couple of days and submit a PR. Though do you know of any workarounds to deal with such a dataset using the `Dataset` API? (Sparse + Dense)?", "@amirhmk Each component of the argument passed to `Dataset.from_tensors` or `Dataset.from_tensor_slices` must be convertible to a single object representable by a `tf.TypeSpec`. `list(SparseTensor)` doesn't satisfy this. There are a couple ways to make it work:\r\n\r\n1. Define a single sparse tensor and take slices out of it:\r\n```python\r\nsparse = tf.SparseTensor(indices=[[0, 0, 0], [0, 1, 2], [1, 0, 0], [1, 1, 1]], \r\n    values=[1, 2, 3, 4], dense_shape=[2, 3, 4])\r\ndataset = tf.data.Dataset.from_tensor_slices(sparse)\r\n```\r\n\r\n2. Produce sparse tensors using Dataset.from_generator:\r\n```python\r\nsparse_1 = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\r\nsparse_2 = tf.SparseTensor(indices=[[0, 0], [1, 1]], values=[3, 4], dense_shape=[2, 2])\r\ndef gen():\r\n  yield sparse_1\r\n  yield sparse_2\r\n\r\ndataset = tf.data.Dataset.from_generator(gen, output_signature=tf.SparseTensorSpec(dtype=tf.int32))\r\n```", "@aaudiber Thank you for your comment, that makes sense. I like the first approach, I just have to keep track of the indices that represent each data point after merging them into a single `SparseTensor` which shouldn't be too bad. \r\n\r\nAs for the second approach, I think `from_generator` accepts `SparseTensorSpec` in the `pre-release` version. Do you know when this will become a `stable` version?  ", "@amirhmk It will be available in TF 2.4.0, which should be released in the next week or two.", "Sounds good. I still think this feature would be nice to have, as in each data point has some sort of a `sparse` tensor attached to it. Feel free to close if there is a strong reason not to do that. Thanks!", "I was trying to ingest sparse data into `tf.data.Dataset` by using the `from_generator` API. \u03a4he example below uses `scipy.coo_matrix`. Trying that with on-prem design for Sparse Arrays which follows the COO representation I was not able to pass the \r\n```\r\nrow = np.array([0, 3, 1, 0])\r\ncol = np.array([0, 3, 1, 2])\r\ndata = np.array([4, 5, 7, 9])\r\na = coo_matrix((data, (row, col)), shape=(4, 4))\r\nreturn tf.data.Dataset.from_generator(\r\n            generator=cls._generator,\r\n            output_signature=(\r\n                tf.SparseTensorSpec(dtype=tf.int32)\r\n            )\r\n            ,args=(a,),\r\n        )\r\n```\r\nI was getting the following error: \r\n\r\n```\r\nTypeError(\"Failed to convert object of type %s to Tensor. \"\r\nTypeError: Failed to convert object of type <class 'SparseArray'> to Tensor. Contents: ....\r\n```\r\nThinking that the culprit may be the on-prem implementation of sparse data I tried the same with `scipy.coo_matrix`\r\n\r\nI get the same error:\r\n\r\n```\r\nAttempt to convert a value (<4x4 sparse matrix of type '<class 'numpy.int64'>' with 4 stored elements in COOrdinate format>) with an unsupported type (<class 'scipy.sparse.coo.coo_matrix'>) to a Tensor.\r\n\r\n```\r\nAfter some debugging I saw that the error comes from the evaluation of the arguments of generator function,  Which makes sense since in the docs here clearly states that the args should be tf.Tensors.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator\r\n\r\n```\r\nargs | (Optional.) A tuple of\u00a0tf.Tensor\u00a0objects that will be evaluated and passed to\u00a0generator\u00a0as NumPy-array arguments.\r\n```\r\nHowever after the introduction of `SparseTensors` and `SparseTensorSpec` I'm not sure if it's just me, but when I read this part of the documentation I was under the impression that these `scipy.coo_matrix` would be translated into `SparseTensors` or at least to `index`,`value` Numpy-arrays by exploiting the `data`,`row`,`col` attr of the COO sparse format (e.g. `scipy.coo_matrix`). Is there any walk-around on how someone can ingest sparse data using `from_generator`? Or is there any formal way that I am not aware?\r\n\r\n\r\nUsing tensorflow==2.4.1", "I am doing something very similar to @ktsitsi but instead inside the function that I pass to from_generator() I am converting the coo matrix to SparseTensor and specifying like @ktsitsi the SparseTensorSpec into the output_signature -> the error message I get is the following:\r\n\r\n`TypeError: Cannot convert value SparseTensor Spec to a TensorFlow DType.`", "@sat2000pts [This colab](https://colab.research.google.com/drive/1z99pOCh4pl9J_A8HQPsDE9AA0DL_52T1) demonstrates how to convert from coo_matrix to SparseTensor: \r\n\r\nThe crux is\r\n\r\n```python\r\ncoo = coo_matrix((data, (row, col)), shape=shape)\r\ntf_sparse = tf.sparse.SparseTensor(list(zip(coo.row, coo.col)), coo.data, coo.shape)\r\n```", "@aaudiber thanks for the reply. \r\nJust a bit more details: I am using the sklearn count_vectorizer to transform the text and then convert it a coo_matrix. \r\nNow with the code you send I convert it to SparseTensor and the error is the same: `TypeError: Cannot convert value SparseTensorSpec(TensorShape(None), tf.int32) to a TensorFlow DType.`\r\n\r\nI don't think this plays a role but I also yield an simple integer ->  ` ds = tf.data.Dataset.from_generator(rdd_generator, (tf.SparseTensorSpec(dtype=tf.int32), tf.int32), (tf.TensorShape([None,vec_shape]), tf.TensorShape([1])))`", "@sat2000pts Can you open a new issue, and include code to reproduce the error you're seeing?"]}, {"number": 44563, "title": "Error using TensorBoard callback in graph mode in TF 2.4", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0rc0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nAttempting to use `tf.keras.callbacks.TensorBoard` with eager execution disabled results in an error in TF 2.4.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be no error, callback should work as normal.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ninp = tf.keras.Input((1,))\r\nout = tf.keras.layers.Dense(1)(inp)\r\n\r\nmodel = tf.keras.Model(inp, out)\r\n\r\nmodel.predict(\r\n    np.zeros((32, 1)),\r\n    callbacks=[tf.keras.callbacks.TensorBoard(log_dir=\"test\")],\r\n)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../tmp.py\", line 11, in <module>\r\n    model.predict(\r\n  File \"...\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 982, in predict\r\n    return func.predict(\r\n  File \"...\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\", line 706, in predict\r\n    return predict_loop(\r\n  File \"...\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\", line 217, in model_iteration\r\n    callbacks = cbks.configure_callbacks(\r\n  File \"...\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 115, in configure_callbacks\r\n    callback_list = CallbackList(callbacks)\r\n  File \"...\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 237, in __init__\r\n    self._should_call_train_batch_hooks = any(\r\n  File \"...\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 238, in <genexpr>\r\n    cb._implements_train_batch_hooks() for cb in self.callbacks)\r\n  File \"...\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 2310, in _implements_train_batch_hooks\r\n    return self._should_trace  # Only call batch hooks when tracing is enabled\r\nAttributeError: 'TensorBoard' object has no attribute '_should_trace'\r\n```", "comments": ["Was able to reproduce the issue with [TF v2.4.0rc0](https://colab.research.google.com/gist/amahendrakar/ae78370adfbd22d169f7ead5f6d579ad/44563.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/adac0577dd7fc3bbe4883428bc420fb8/44563-tf-nightly.ipynb). However, code works fine with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/187bb5be27ba643b1af9a5b556d92671/44563.ipynb). Please find the attached gist. Thanks!", "@drasmuss thanks for filing the issue, is there a reason that you disable eager execution ? ", "I have found that using the old training_v1 Keras implementation (which is what you get when disabling eager execution) is still faster for some models.", "Hi @drasmuss can you share what the models are that you've found it's faster for? We want to make sure to fix up any performance regressions like that.", "You can see an example here https://github.com/nengo/nengo-dl/blob/master/nengo_dl/tests/test_benchmarks.py#L238, specifically comparing the two test cases\r\n```\r\n(benchmarks.lmu(1000, 1, native_nengo=True), True, 100, True, 1.3, 1.5),\r\n(benchmarks.lmu(1000, 1, native_nengo=True), True, 100, False, 1.05, 1.25),\r\n```\r\nThe second case runs the same benchmark but with\r\n```\r\ntf.compat.v1.disable_eager_execution()\r\ntf.compat.v1.disable_control_flow_v2()\r\n```\r\nand it runs about 0.25s (20%) faster.", "Here is a Colab gist demonstrating the same thing if that helps https://colab.research.google.com/gist/drasmuss/45826df4e27dc6a21be961690d2a043f/performance-demo.ipynb", "@drasmuss Is this still an issue? I ran your code (colab with out GPU) with `tf-nightly` and I see almost similar execution times as shown below. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/84158a41c4ab907a05675bee13a98ad5/performance-demo.ipynb). Thanks!\r\n\r\n\r\n#### Please note that these results are without GPU\r\n```\r\nExecution time: 14.798911161999968\r\nEager 14.798911161999968\r\n\r\nExecution time: 14.530144375999953\r\nNon-eager 14.530144375999953\r\n```", "I think we need to test it on the GPU in order to see whether this is still an issue, it's hard to know whether the CPU results are indicative of GPU performance.", "Had a chance to test this, and it looks like the problem is the same (possibly worse) in `tf-nightly`.  Here are my results (note I couldn't get tf-nightly to run with GPU support on Colab, so this is on a local RTX 3090):\r\n\r\n**tf-nightly**\r\nEager: 0.8343444939237088\r\nNon-eager: 0.6216731490567327\r\n(~33% slowdown in eager mode)\r\n\r\nAlso note that tf-nightly seems to be significantly slower than older versions of tensorflow, e.g.\r\n\r\n**tensorflow 2.2.2**\r\nEager: 0.7077119939494878\r\nNon-eager: 0.5124576301313937\r\n(~20% slowdown in tf-nightly vs tf 2.2; possibly related to https://github.com/tensorflow/tensorflow/issues/46515)", "same issue for me with keras 2.3.1\r\n```\r\nfrom gym import Env\r\nfrom gym.spaces import Discrete, Box\r\n\r\nclass FooEnv(Env):\r\n    metadata = {'render.modes': ['human']}\r\n\r\n    def __init__(self, training=True):\r\n        self.training = training\r\n        self.action_space = Discrete(5)\r\n        self.size = 11\r\n        self.observation_space = Box(low=np.array([0, 0]), high=np.array([self.size - 1, 1]))\r\n        self.position = -1\r\n        self.brightness = 0.0\r\n\r\n        self.state = [self.position, self.brightness]\r\n        self.action_steps = 30\r\n\r\n        self.action = -1\r\n\r\n        self.done = False\r\n        self.reward = 0\r\n        self.info = {}\r\n        self.seed(seed=45)\r\n\r\n    def get_position(self):\r\n        noise_factor = 0\r\n        position = 1 - (self.position / 5) * (self.brightness) + noise_factor\r\n        return position\r\n\r\n    def step(self, action, ext_position=-10):\r\n\r\n        action -= 2\r\n        self.action = action\r\n\r\n        self.brightness += int(action) / 10\r\n        if self.brightness > 1:\r\n            self.brightness = 1\r\n        if self.brightness < 0:\r\n            self.brightness = 0\r\n\r\n        if ext_position == -10:\r\n            self.position += self.get_position()\r\n        else:\r\n            self.position = ext_position\r\n        if self.position < 0:\r\n            self.position = 0\r\n        if self.position > self.size - 1:\r\n            self.position = self.size - 1\r\n        self.state = [self.position, self.brightness]\r\n\r\n        if 10 > self.position >= 0:\r\n            self.reward = 1 - abs(self.position / 10 - self.brightness)\r\n\r\n        elif self.position >= 10:\r\n            self.reward = -100\r\n\r\n        self.action_steps -= 1\r\n\r\n        self.done = self.check()\r\n\r\n        self.info = {}\r\n        return self.state, self.reward, self.done, self.info\r\n\r\n    def check(self):\r\n        done = self.done\r\n        if self.action_steps == 0:\r\n            done = True\r\n        return done\r\n\r\n    def reset(self):\r\n        self.position = -1\r\n        self.brightness = 0.0\r\n        self.state = [self.position, self.brightness]\r\n        self.action_steps = 30\r\n        self.reward = 0\r\n        self.action = -0\r\n        self.done = False\r\n        self.info = {}\r\n        return self.state\r\n\r\n    def render(self, mode='human', close=False):\r\n        for i in range(self.size):\r\n            if self.position >= i > self.position - 1:\r\n                print(\"+\", end='')\r\n            else:\r\n                print(\"-\", end='')\r\n        if self.done:\r\n            print(\"X| Pos:\" + str(self.position) + \" Brightness:\" + str(self.brightness) + \" Done:\" + str(self.done) +\r\n                  \" Reward:\" + str(self.reward) + \" Steps:\" + str(self.action_steps) + \" Action:\" + str(self.action))\r\n        else:\r\n            print(\"O| Pos:\" + str(self.position) + \" Brightness:\" + str(self.brightness) + \" Done:\" + str(self.done) +\r\n                  \" Reward:\" + str(self.reward) + \" Steps:\" + str(self.action_steps) + \" Action:\" + str(self.action))\r\n\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Flatten\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nenv = FooEnv()\r\nenv.seed(0)\r\n\r\nstates = env.observation_space.shape\r\nactions = env.action_space.n\r\n\r\n\r\ndef build_model(states, actions):\r\n    model = Sequential()\r\n    model.add(Flatten(input_shape=(1,) + states))\r\n    model.add(Dense(24, activation='relu'))\r\n    model.add(Dense(24, activation='relu'))\r\n    model.add(Dense(actions, activation='linear'))\r\n    return model\r\n\r\nfrom rl.agents import DQNAgent\r\n\r\nfrom keras.callbacks import TensorBoard\r\nfrom rl.callbacks import ModelIntervalCheckpoint, FileLogger\r\nfrom rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\r\nfrom rl.memory import SequentialMemory\r\n\r\nmodel = build_model(states, actions)\r\nmodel.summary()\r\n\r\ndef build_agent(model, actions):\r\n\r\n    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1, value_min=0.1, value_test=0.05,\r\n                                  nb_steps=500)\r\n    memory = SequentialMemory(limit=10000, window_length=1)\r\n\r\n    dqn = DQNAgent(model=model, memory=memory, policy=policy, enable_double_dqn=True,\r\n                   nb_actions=actions, gamma=.98, nb_steps_warmup=100, target_model_update=1e-2)\r\n    return dqn\r\n\r\ncallbacks = [TensorBoard(log_dir='./weights_test')]\r\n\r\ndqn = build_agent(model, actions)\r\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\r\ndqn.fit(env, nb_steps=10000, log_interval=1000, nb_max_episode_steps=50, visualize=False, verbose=1,\r\n        callbacks=callbacks)\r\n```\r\n", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/e4e9c829b88c5222d04894f255f20535/untitled56.ipynb)...Thanks !"]}, {"number": 44555, "title": "tf.convert_to_tensor is slow when converting list of numpy arrays", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 (via docker on Ubuntu 20.04)\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: - \r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n-   **Python version**: 3.6.9\r\n-   **Bazel version (if compiling from source)**: -\r\n-   **GCC/Compiler version (if compiling from source)**: -\r\n-   **CUDA/cuDNN version**: 11.0\r\n-   **GPU model and memory**: Quadro M2200, 4035MiB\r\n-   **Exact command to reproduce**:\r\n```python3\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport timeit\r\n\r\n# make some random \"images\"\r\nnp_list = [np.random.randint(0, 256, (200, 200, 3)) for _ in range(1000)]\r\n\r\ndef convert_as_list():\r\n    tensor = tf.convert_to_tensor(np_list)\r\n\r\ndef convert_as_single_array():\r\n    tensor = tf.convert_to_tensor(np.asarray(np_list))\r\n\r\n# just some operation to initialize\r\ntf.convert_to_tensor([1])\r\n\r\nprint(f\"convert_as_list: {timeit.Timer(convert_as_list).timeit(1)} s\")\r\nprint(f\"convert_as_single_array: {timeit.Timer(convert_as_single_array).timeit(1)} s\")\r\n```\r\n\r\n### Describe the problem\r\nUsing lists of numpy arrays instead of a single numpy array results in significantly slower execution time of `tf.convert_to_tensor()`.\r\nThe toy example above gives the following output on my machine, which represents a ~600 % slowdown:\r\n```\r\nconvert_as_list: 36.3590992190002 s\r\nconvert_as_single_array: 0.6024578830001701 s\r\n```\r\nUsing numpy arrays that are `float` instead of `int` shows the same general behaviour.\r\n\r\n### Source code / logs\r\n\r\n<details>\r\n  <summary>Complete script output including tensorflow logs. Click to expand!</summary>\r\n\r\n```\r\n2020-11-03 14:41:56.450816: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-03 14:41:58.019674: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-11-03 14:41:58.027457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.027779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: Quadro M2200 computeCapability: 5.2\r\ncoreClock: 1.036GHz coreCount: 8 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 82.08GiB/s\r\n2020-11-03 14:41:58.027817: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-03 14:41:58.029158: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-11-03 14:41:58.030388: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-11-03 14:41:58.030662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-11-03 14:41:58.032077: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-03 14:41:58.032935: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-11-03 14:41:58.035874: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-11-03 14:41:58.036017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.036329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.036605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-11-03 14:41:58.036921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-03 14:41:58.041972: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2899885000 Hz\r\n2020-11-03 14:41:58.042290: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ee49520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-03 14:41:58.042306: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-11-03 14:41:58.066763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.067127: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3e7aa050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-11-03 14:41:58.067145: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro M2200, Compute Capability 5.2\r\n2020-11-03 14:41:58.067303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.067560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: Quadro M2200 computeCapability: 5.2\r\ncoreClock: 1.036GHz coreCount: 8 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 82.08GiB/s\r\n2020-11-03 14:41:58.067583: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-03 14:41:58.067599: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-11-03 14:41:58.067615: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-11-03 14:41:58.067632: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-11-03 14:41:58.067650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-03 14:41:58.067667: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-11-03 14:41:58.067683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-11-03 14:41:58.067747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.068006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.068224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-11-03 14:41:58.068247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-03 14:41:58.459390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-03 14:41:58.459438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-11-03 14:41:58.459448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-11-03 14:41:58.459633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.459972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-03 14:41:58.460242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2394 MB memory) -> physical GPU (device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nconvert_as_list: 36.3590992190002 s\r\nconvert_as_single_array: 0.6024578830001701 s\r\n\r\nProcess finished with exit code 0\r\n```\r\n</details>\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6c286d9b6cd4266398a02d8373f3c73b/44555.ipynb). Thanks!", "I ran into the same behaviour again today using `tf.data.Dataset.from_tensors()`. Slow with a list of `np.array`, fast with a single `np.array`.", "same issue on tensorflow 2.4.0", "Any updates on this?", "Please see [#47470 ](https://github.com/tensorflow/tensorflow/issues/47470). Along the same lines, please take a look at the source of `convert_to_tensor()` [here](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/framework/ops.py#L1482)\r\nNote that the function returns a graph capture, which means that the creation of a Tensor from a Python list _may_ be traced, unlike from np.arrays. See [here](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/framework/func_graph.py#L713).\r\n\r\nThis is my speculation, please correct me if I am wrong.\r\n"]}, {"number": 44551, "title": "Does TensorFlow Micro support self define output type for each layer?", "body": "@tensorflow/micro\r\n\r\n**Describe the problem**\r\nIn our project, we want to deploy a CNN model in QualComm sensorhub. We quantize the model firstly, the input of the model is int8 data. But in order to improve inference accuracy, we want to use uint8 data to receive the first convolution layer output and input float data to FullyConnected layer. Therefore, the data type change process is as follow: int8 input -> first conv -> uint8 output -> second conv -> ... last conv -> float output -> fullyconnected. Dose TensorFlow Micro support this? \r\n\r\n", "comments": []}, {"number": 44539, "title": "tensorflow/go@v2.3.1: but does not contain package go/core/protobuf/for_core_protos_go_proto", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```sh\r\nStep 21/27 : RUN $go test github.com/tensorflow/tensorflow/tensorflow/go\r\n ---> Running in b85702eb3116\r\ngo: finding github.com/tensorflow/tensorflow v2.3.1+incompatible\r\ngo: downloading github.com/tensorflow/tensorflow v2.3.1+incompatible\r\ngo: extracting github.com/tensorflow/tensorflow v2.3.1+incompatible\r\n/go/go/packages/pkg/mod/github.com/tensorflow/tensorflow@v2.3.1+incompatible/tensorflow/go/saved_model.go:24:2: cannot find package\r\n/go/go/packages/pkg/mod/github.com/tensorflow/tensorflow@v2.3.1+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.3.1+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\r\n```\r\n\r\n", "comments": ["Also v2.4.0+incompatible\r\n\r\n```\r\n/go/pkg/mod/github.com/tensorflow/tensorflow@v2.4.0+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.4.0+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\r\n```", "I also have this problem.", "This looks like a duplicate of #43847.\r\n\r\nIssue is still present with v2.5.0-rc0+incompatible.\r\n\r\n`../../../go/pkg/mod/github.com/tensorflow/tensorflow@v2.5.0-rc0+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.4.1+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto`\r\n\r\nI'm trying to find some version that does work. Between this bug and bugs #35133 and #34580 it's really hard to find any actually working combination of versions. Not a great first impression.", "Having the same problem on TF2.2", "@bool64-llc Could you please  let us know if the issue still persists ? Thank you!", "Also, we currently don't have any person with Go expertise left in the team, so we'd probably need to separate the go bindings to a separate repository and give full control to community to fix these.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I also have this problem on version 2.7.0"]}, {"number": 44515, "title": "attr return values handled differently than dict by tf.distribute.Strategy.run", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 1050 Ti 4GB\r\n\r\n**Describe the current behavior**\r\nIf I set up two virtual GPUs and run the following snippet,\r\n```python\r\nimport attr\r\nimport tensorflow as tf\r\n\r\n@attr.s\r\nclass WrappedTensor(object):\r\n    tensor = attr.ib()\r\n\r\n@tf.function\r\ndef func_dict(x):\r\n    def func_dict_per_replica(x):\r\n        return x\r\n    print('Result of Strategy.run(func_dict_per_replica):',\r\n          tf.distribute.get_strategy().run(func_dict_per_replica, (x,)))\r\n\r\n@tf.function\r\ndef func_attr(x):\r\n    def func_attr_per_replica(x):\r\n        return x\r\n    print('Result of Strategy.run(func_attr_per_replica):',\r\n          tf.distribute.get_strategy().run(func_attr_per_replica, (x,)))\r\n\r\nwith tf.distribute.MirroredStrategy().scope():\r\n    data = tf.data.Dataset.from_tensors(tf.constant(0, shape=[2]))\r\n    data = tf.distribute.get_strategy().experimental_distribute_dataset(data)\r\n    v = next(iter(data))\r\n    print()\r\n    x = {'tensor': v}\r\n    print('Input signature of func_dict:', func_dict.get_concrete_function(x).structured_input_signature)\r\n    print()\r\n    x = WrappedTensor(v)\r\n    print('Input signature of func_attr:', func_attr.get_concrete_function(x).structured_input_signature)\r\n```\r\nI get the following output:\r\n```\r\nResult of Strategy.run(func_dict_per_replica): {'tensor': PerReplica:{\r\n  0: <tf.Tensor 'x:0' shape=(1,) dtype=int32>,\r\n  1: <tf.Tensor 'x_1:0' shape=(1,) dtype=int32>\r\n}}\r\nInput signature of func_dict: (({'tensor': PerReplicaSpec(TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))},), {})\r\n\r\nResult of Strategy.run(func_attr_per_replica): PerReplica:{\r\n  0: WrappedTensor(tensor=<tf.Tensor 'x:0' shape=(1,) dtype=int32>),\r\n  1: WrappedTensor(tensor=<tf.Tensor 'x_1:0' shape=(1,) dtype=int32>)\r\n}\r\nInput signature of func_attr: ((WrappedTensor(tensor=PerReplicaSpec(TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))),), {})\r\n```\r\n\r\n**Describe the expected behavior**\r\nAs shown in the output, the input signatures of both functions are generated by replacing values in the structures with the respective `PerReplicaSpec`, so here `attr` objects are treated like a nested structure similar to a `dict`. One would then expect the result of the two functions follow a similar treatment. However, the result of `func_attr` is `WrappedTensor` objects stored inside `PerReplica` objects, as opposed to `PerReplica` objects wrapped inside `WrappedTensor` objects, so `attr` is again not treated like a nested structure here, inconsistent with the input signatures.\r\n\r\nI personally think tensorflow should treat `attr` objects like a nested structure (that is, change the behavior of `func_attr` to match that of `func_dict`), because then you can write member functions that handle distributed computation automatically when the attributes of a `attr` object are `DistribtuedValue`s.\r\n\r\n**Standalone code to reproduce the issue**\r\nSee the code snippet above.\r\n\r\n**Other info / logs**\r\nN/A", "comments": ["Thanks for filing the issue. I think it is a fair ask and `attrs` indeed should be treated as a nested structure across TF stack.", "@Qrox It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "Tested with tensorflow 2.5.0 and the output is the same as before, so this hasn't been fixed."]}, {"number": 44510, "title": "tf.device scope not working correctly", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y es\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ppc64le-linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary)\r\n- TensorFlow version (use command below):\r\n- Python version: python3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1.243\r\n- GPU model and memory: V100 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ntf.device command does not correctly assign a GPU device to tf.keras layers on node with 4 GPUs so cannot implement model parallelism. All layers appear on device GPU:0 with the exception of some IO based on output of tf.debugging.set_log_device_placement(True)\r\n\r\n**Describe the expected behavior**\r\ntf.keras layers are correctly assigned to a device.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ntf.debugging.set_log_device_placement(True)\r\n\r\nprint(\"On GPU:1\")\r\ninputs = keras.Input(shape=(784,))\r\nwith tf.device(\"/device:GPU:1\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\r\n   x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n   print(x)\r\n   assert x.device.endswith(\"/GPU:1\")\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI have a larger test problem that will run on a 4 GPU node. If you turn off the assert statement, then using nvidia-smi you can see that all memory and computational work is happening on GPU:0 and almost none is assigned to other GPUs. Happy to supply this code if needed.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTensor(\"dense/Identity:0\", shape=(None, 256), dtype=float32)\r\nTraceback (most recent call last):\r\n  File \"py_test.py\", line 11, in <module>\r\n    assert x.device.endswith(\"/GPU:1\")\r\nAssertionError\r\n", "comments": ["@JohnTaylor2000 \r\n\r\nWill it be possible to share complete code snippet .Please, share the output of `tf.config.list_physical_devices()`.Thanks!", "# NOTE : uses input data file mnist.npz\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nimport time\r\nimport socket\r\n\r\nimport os\r\n\r\ntf.debugging.set_log_device_placement(True)\r\n\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\ngpus = tf.config.list_physical_devices('GPU')\r\nprint(' gpus = ', gpus)\r\n\r\ndef get_compiled_model():\r\n    # Make a simple 2-layer densely-connected neural network.\r\n    inputs = keras.Input(shape=(784,))\r\n    print(\"On GPU:0\")\r\n    with tf.device(\"/device:GPU:0\"):\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       #assert x.device.endswith(\"/GPU:0\")\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n    print(\"On GPU:1\")\r\n    with tf.device(\"/device:GPU:1\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       #assert x.device.endswith(\"/GPU:1\")\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n    print(\"On GPU:2\")\r\n    with tf.device(\"/device:GPU:2\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       #assert x.device.endswith(\"/GPU:2\")\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n       x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n    print(\"On GPU:3\")\r\n    with tf.device(\"/device:GPU:3\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\r\n      x = keras.layers.Dense(256, activation=\"relu\")(x)\r\n      #assert x.device.endswith(\"GPU:3\")\r\n      x = keras.layers.Dense(256, activation=\"relu\")(x)\r\n      x = keras.layers.Dense(256, activation=\"relu\")(x)\r\n      x = keras.layers.Dense(256, activation=\"relu\")(x)\r\n      x = keras.layers.Dense(256, activation=\"relu\")(x)\r\n      x = keras.layers.Dense(256, activation=\"relu\")(x)\r\n      outputs = keras.layers.Dense(10)(x)\r\n    model = keras.Model(inputs, outputs)\r\n \r\n    opt = keras.optimizers.Adam()\r\n    model.compile(\r\n        optimizer=opt,\r\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=[keras.metrics.SparseCategoricalAccuracy()],\r\n        experimental_run_tf_function=False,\r\n    )\r\n    return model\r\n\r\n\r\ndef get_dataset():\r\n    batch_size = 32\r\n    num_val_samples = 10000\r\n\r\n    # Return the MNIST dataset in the form of a `tf.data.Dataset`.\r\n    path = '/g/g92/jtaylor/workspace/TFnew/mnist.npz'\r\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path)\r\n\r\n    # Preprocess the data (these are Numpy arrays)\r\n    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\r\n    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\r\n    y_train = y_train.astype(\"float32\")\r\n    y_test = y_test.astype(\"float32\")\r\n\r\n    # Reserve num_val_samples samples for validation\r\n    x_val = x_train[-num_val_samples:]\r\n    y_val = y_train[-num_val_samples:]\r\n    x_train = x_train[:-num_val_samples]\r\n    y_train = y_train[:-num_val_samples]\r\n    return (\r\n        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\r\n        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\r\n        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\r\n    )\r\n\r\nmodel = get_compiled_model()\r\n\r\n# Train the model on all available devices.\r\ntrain_dataset, val_dataset, test_dataset = get_dataset()\r\nmodel.fit(train_dataset, epochs=2, validation_data=val_dataset)\r\n\r\n# Test the model on all available devices.\r\nmodel.evaluate(test_dataset)\r\n```", "Output of tf.config.list_physical_devices() as requested:-\r\n```python\r\nNum GPUs Available:  4\r\n gpus =  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), \r\nPhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), \r\nPhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), \r\nPhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\r\n```", "@JohnTaylor2000 \r\n\r\nPlease, let us know which TF version you are using?\r\nThanks!", "PowerAi Tensorflow version 2.1.0\r\nAlso tensorflow 2.2.0 on x86_64", "@JohnTaylor2000,\r\nIf I understand your requirement correctly, you want to utilize all the `GPUs` while `Training your Model`. If that is the case, the best `API` to use is [Distributed Training](https://www.tensorflow.org/guide/distributed_training). Even the Dataset can be Distributed using [Distributed Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy).\r\n\r\nThanks!", "The actual model that I am working on is too large to fit into GPU memory. I have a data parallel code using horovod that runs on hundreds of GPUs but now need to use a larger model.  To do this I need to spread the layers across multiple GPUs.", "Hi all,  just wondering if you have been able to run the test code that I have provided and/or need any further help?", "Wondering if you can confirm that this is an issue based on my supplied bug report and if you have a solution? This is a roadblock to my research so I am keen to have this resolved - any help greatly appreciated!", "Any developments on this issue please?\r\n", "Any help available? I have had no response since 5 November?", "I think TF currently does not support model parallelism with keras model like what you have written. However, I believe they do support model parallelism with primitive operations. Also, it is almost like they do not have any docs about model parallelism which is an issue. \r\nMaybe you can try something like wrapping all __init__, __build__ and __call__ methods inside with tf.device(the device you want).\r\nExample code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ntf.debugging.set_log_device_placement(True)\r\n\r\nclass Dense_in_gpu(tf.keras.layers.Layer):\r\n    def __init__(self, gpu_to_use, units,activation=None,**kwargs):\r\n        self.gpu_to_use=gpu_to_use\r\n        with tf.device(f\"/GPU:{self.gpu_to_use}\"):\r\n            self.dense=tf.keras.layers.Dense(units,activation=activation)\r\n        super().__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        with tf.device(f\"/GPU:{self.gpu_to_use}\"):\r\n            return self.dense(inputs)\r\n\r\n\r\ninputs = tf.keras.Input(shape=(784,))\r\n#run in the first gpu\r\nx = Dense_in_gpu(0,64, activation=\"relu\")(inputs)\r\n#run in the second gpu\r\nx = Dense_in_gpu(1,64, activation=\"relu\")(x)\r\n#run in the third gpu\r\noutputs = Dense_in_gpu(2,10)(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.predict_on_batch(np.random.rand(64,784))\r\n\r\ntf_dataset=tf.data.Dataset.from_tensor_slices((np.random.rand(64,784),np.random.rand(64,10)))\r\ntf_dataset=tf_dataset.batch(64)\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(), loss=tf.keras.losses.MeanSquaredError())\r\nmodel.fit(tf_dataset)\r\n```", "Thank you for the interesting solution to this problem. I am investigating whether this will work on the model that I have developed.\r\n\r\nI noticed that the latest versions of tf.keras.layers no longer has the attribute 'device' which means, as you say, that Keras no longer supports model parallelism where we assign layers to a device.\r\n\r\nInterestingly, the development of GPUs with much larger memory eg 80GB on the A100 and the proposed new Grace architecture which allow high bandwidth access to CPU memory, will reduce the need for model parallelism. However, this will likely be offset by the desire to build bigger more complex model systems using the Keras functional API.", "@JohnTaylor2000 \r\nIs this still an issue, could you please try on the latest tf version and let us know.", "I have been able to test this using TensorFlow 2.6.0 using a test code. I do not have access to 2.7.0 at the moment.\r\n\r\nStill seeing the same problem when using a dense layer tf.keras.layers.Dense the response was \r\nAttributeError: 'KerasTensor' object has no attribute 'device'. I also observed that the code intended to run on 4 GPUs runs primarily on a single GPU ie the 'with tf.device(\"GPU:3\"): ' is ignored.\r\n\r\nhere is a simple test where the print command will generate the error above:-\r\n\r\n    with tf.device(\"GPU:3\"):\r\n        x = tf.keras.layers.Dense(num_hidden_units, activation=\"relu\")(x)\r\n        x = tf.keras.layers.Dense(num_hidden_units, activation=\"relu\")(x)\r\n        x = tf.keras.layers.Dense(num_hidden_units, activation=\"relu\")(x)\r\n        x = tf.keras.layers.Dense(num_hidden_units, activation=\"relu\")(x)\r\n        x = tf.keras.layers.Dense(num_hidden_units, activation=\"relu\")(x)\r\n        print(x.device)\r\n\r\n", "Hi @JohnTaylor2000 ! You can use [set_visible_devices](https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices) to disable and enable specific GPU's during operation. Have you tried the same in 2.8 version yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 44487, "title": "TPUStrategy.run fails on non-primary thread with \"No OpKernel was registered\", \"TPUReplicatedInput\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included here\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): (a) Google Collab, (b) Debian 10 (Buster), Google Cloud VM TF2.3.1 image\r\n- TensorFlow installed from (source or binary): (a) Google Collab, (b) Google Cloud VM TF2.3.1 image\r\n- TensorFlow version (use command below): (a) Google Collab, (b) v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: (a) Google Collab, (b) 3.7.3 (Google Cloud VM TF2.3.1 image)\r\n- GPU model and memory: (a) Google Collab TPU, (b) Google Cloud TPU, us-central1-f, v2-8, 2.3.1\r\n\r\n**Describe the current behavior**\r\n\r\nRunning TPUStrategy.run on a secondary thread crashes\r\n\r\n**Describe the expected behavior**\r\n\r\nRunning TPUStrategy.run on a secondary thread behaves the same as on the primary thread, succeeding.\r\n\r\nThis is breaking for applications like inference from AlphaZero-style MCTS self-play originating from C++, using TPUStrategy.experimental_distribute_dataset, where either the primary C++ thread is required to perform other housekeeping, or where multiple prediction threads help to keep the inference pipeline more full. However, this minimal Python-only example gives the same error. The error differs slightly when experimental_distribute_dataset is used, but the final \"Op:__inference_tpu_function_177\" is common.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis snippet runs in a Google Collab notebook:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport threading\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\n@tf.function\r\ndef double(x):\r\n  return x * 2.0\r\n\r\ndef test():\r\n  input = tf.range(5, dtype=tf.float32)\r\n  strategy.run(double, args=(input,))\r\n\r\ntest()\r\nprint(\"TPUStrategy.run works on primary thread\")\r\n\r\nthread = threading.Thread(target=test)\r\nthread.start()\r\nthread.join()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nException in thread Thread-5:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"<ipython-input-2-c35bbe6196e2>\", line 15, in test\r\n    strategy.run(double, args=(input,))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 279, in run\r\n    return self.extended.tpu_run(fn, args, kwargs, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 1095, in tpu_run\r\n    return func(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 814, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'TPUReplicatedInput' used by {{node input0}} with these attrs: [T=DT_INT32, index=0, is_mirrored_variable=false, N=8, is_packed=false]\r\nRegistered devices: [CPU, TPU, TPU_SYSTEM, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[input0]] [Op:__inference_tpu_function_177]\r\n", "comments": ["I have tried in colab with TF version 2.3 and nightly version(`2.5.0-dev20201029`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b49a6c3ee54fd24f47f8ea0a3d44dc36/untitled482.ipynb). Thanks!", "Unassigning self, as I'm no longer working on TF, thanks!", "If you change your test function to:\r\n\r\n```\r\ndef test():\r\n  with tf.device(\"/job:worker\"):\r\n     input = tf.range(5, dtype=tf.float32)\r\n     strategy.run(double, args=(input,))\r\n```\r\n\r\ndoes that work? I think it is related to a new thread is losing the existing device stack information.", "That does work, thanks a lot. Are there any performance implications adding the \"/job:worker\" context there? Do you know why that's only required for TPUStrategy.run as opposed to strategy-less calls, either naked or 'with tf.device(\"/TPU:0\")'? Would this be a bug to fix or would API guidance be given?\r\n\r\nWorkaround code for reference:\r\n```\r\nimport tensorflow as tf\r\nimport threading\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\n@tf.function\r\ndef double(x):\r\n  return x * 2.0\r\n\r\ndef test():\r\n  with tf.device(\"/job:worker\"):\r\n     input = tf.range(5, dtype=tf.float32)\r\n     output = strategy.run(double, args=(input,))\r\n     print(input, output)\r\n\r\ntest()\r\nprint(\"TPUStrategy.run works on primary thread\")\r\n\r\nthread = threading.Thread(target=test)\r\nthread.start()\r\nthread.join()\r\nprint(\"TPUStrategy.run works on secondary thread with /job:worker context\")\r\n```", "It shouldn't have performance regressions.  The reason is TPUs are a 2VM setup, you run the python code on the coordinator, but the actual execution is on the TPU worker. To avoid all users need to manually set the device placement, we enter the TPU worker device scope automatically in `tf.config.experimental_connect_to_cluster(resolver)` API, and it can be configured by the `make_master_device_default` argument. The problem is the new thread doesn't inherit existing TF device stack, so the default device becomes the coordinator again. I guess this is more like an API issue. @allenlavoie Allen do you know if it is expected that a new thread should clear out all existing device stack?", "Got it. So it sounds like this would run on the CPU on a secondary thread, and on the TPU on the primary thread?\r\n\r\n```\r\ndef test():\r\n  input = tf.range(5, dtype=tf.float32)\r\n  output = double(input)\r\n```\r\n\r\nAnd this would run successfully on a secondary thread on the first TPU device because you've given it context back?\r\n\r\n```\r\ndef test():\r\n  with tf.device(\"/TPU:0\"):\r\n    input = tf.range(5, dtype=tf.float32)\r\n    output = double(input)\r\n```\r\n", "The device stack is thread local so multiple threads can manage their device stacks independently. I don't think anyone has considered the inheritance behavior on launching a new thread; it does seem like inheriting the device stack from the parent context would be nice, but I don't know how feasible that would be. Sounds like a reasonable request at least.", "> And this would run successfully on a secondary thread on the first TPU device because you've given it context back?\r\n\r\nYes", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/9b0f5aef5fa91c5ae61125296085f284/44487.ipynb). Thanks!", "Re @sachinprasadhs, yes you need to write your test function as\r\n\r\n```\r\ndef test():\r\n  with tf.device(\"/job:worker\"):\r\n    input = tf.range(5, dtype=tf.float32)\r\n    strategy.run(double, args=(input,))\r\n```\r\n\r\nAs when you run the function with a new thread, the previous device context is lost. So you need to manually express that when in multiple threads.", "By modifying the context for def test(): the code runs successfully in Tensorflow 2.8. Attaching gist [here](https://colab.sandbox.google.com/gist/sachinprasadhs/c3a6a7d99423350ccf59bf26ba0f5c87/44487.ipynb) for reference. Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}]