[{"number": 27686, "title": "axis parameter in tf.random_shuffle", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes, if you help me to do\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNo, current behavior will not change\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nnow code is \r\nx = tf.transpose(x, [3, 0, 1, 2])\r\nx = tf.random_shuffle(x)\r\nx = tf.transpose(x, [1, 2, 3, 0]\r\n\r\ncode will be\r\nx = tf.random_shuffle(x, axis=-1)\r\n**Any Other info.**\r\n", "comments": ["@GeorgeKaspar Could you provide more details about the feature and its use cases? What is the current gap that you wish to fill this feature? Thank you!", "I work on https://arxiv.org/pdf/1901.03447.pdf\r\nAnd with code need shuffling across different axes\r\nI think, that tensorflow has limited support for permutations of data\r\nIn that article, we have specific randomized permutations and standard implementation without C++ extensions on tensorflow very slow...\r\nIt is one reason add permutation across different axes", "This is definitely needed. In RL, my batch data from replicator looks like \r\n```\r\nunroll_batch = Shape[H, B, C]\r\n```\r\n\r\nand I have two sets of decoys, need to shuffle one by the time dimension, and another by the batch. \r\n\r\nSo i want to do \r\n```\r\ndecoy_1 = tf.random_shuffle(unroll, axis=0)\r\ndecoy_2 = tf.random_shuffle(unroll, axis=1)\r\n```\r\n\r\n"]}, {"number": 27559, "title": "Poincare hyperbolic embeddings support", "body": "As you might have seen Facebook released hyperbolic embeddings training; \r\nIs there any plan to support those in Tensorflow?\r\n\r\nhttps://github.com/facebookresearch/poincare-embeddings", "comments": ["Hi, I'd be happy to undertake this if this is still of interest to you. I have written a prototype in tensorflow some time ago.", "yes that would be great. ", "Alright, sounds good.", "Working on this with @bellettif ."]}, {"number": 27544, "title": "tf.add_n is inaccurate for float16", "body": "**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf.add_n` is insufficiently accurate with `float16` precision for large scale training, for two reasons:\r\n\r\n1. It sums the numbers serially, rather than in tree order.  As a result, we get stuff like this:\r\n\r\n```\r\n>>> f = lambda x: tf.constant(x, dtype=tf.float16)\r\n>>> tf.add_n([f(1)]+[f(2**-11)]*7).numpy()\r\n1.0\r\n>>> tf.add_n([f(2**-11)]*7+[f(1)]).numpy()\r\n1.004\r\n```\r\n\r\n2. It uses intermediate `float16` values, even though it would be cheap to use intermediate `float32` values.\r\n\r\nI'd like to fix this by (1) using tree order in the Eigen kernels and (2) making the op cast up to `float32`, do the sum in `float32`, then downcast back to `float16`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople using `float16` training (primarily GPU users).\r\n\r\n**Any Other info.**\r\n\r\n`tf.add_n` is memory bandwidth limited, so there should be negligible slowdown.", "comments": ["Would want some confirmation that such a PR would be accepted before writing it.", "for x in inputs:\r\n    if x.dtype == dtypes.float16:\r\n      x = cast(x,  dtypes.float32, name=name)\r\nI have added these lines in tensorflow/python/ops/math_ops.py/add_n function. I guess these lines will solve your problem no 2, if yes I will make a pr.", "@shashvatshahi1998 Unfortunately that the wrong place to fix it: it has to be in C++ and CUDA.  Users of `float16` typically want to avoid `float32` temporaries where possible, and doing the cast inside the loop in the kernel means no extra memory usage.", "Hi @girving .Was able to replicate the issue with TF v2.5 , please find the[ gist ](https://colab.research.google.com/gist/mohantym/80904339db2688ddfe85e7f889da7460/27544.ipynb)here .Thanks!", "Issue still persist in `2.7.0`, `2.8.0` and also in `tf-nightly (2.9.0-dev20220303)`. Please find the [gist here](https://colab.research.google.com/gist/chunduriv/03fa03aba089802d014e25669c47f4f6/54336.ipynb). Thanks!"]}, {"number": 27537, "title": "Unexpected UnicodeDecodeError: invalid continuation byte when reading lines from a file", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nUnexpected and undocumented runtime exception/error when handling malformed data.\r\n\r\n**Describe the expected behavior**\r\nExpected a \"TypeError\" or an empty list as a result.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport csv\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ninput_file_name = sys.argv[1]\r\n\r\nwith tf.gfile.Open(input_file_name, \"r\") as f:\r\n  reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\r\n  for line in reader:\r\n    print(line)\r\n```\r\nRun with the path to the attached file as a command line argument.\r\n\r\n**Other info / logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow_bug.py\", line 9, in <module>\r\n    for line in reader:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 220, in \\_\\_next\\_\\_\r\n    return self.next()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 214, in next\r\n    retval = self.readline()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 184, in readline\r\n    return self._prepare_value(self._read_buf.ReadLineAsString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 100, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 107, in as_str_any\r\n    return as_str(value)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 80, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte\r\n\r\n[corrupted_file1.zip](https://github.com/tensorflow/tensorflow/files/3047460/corrupted_file1.zip)\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thanks!\r\n", "This is the complete code snipped. Run it with the path to the attached file as its first argument.\r\n\r\n```\r\nimport csv\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ninput_file_name = sys.argv[1]\r\n\r\nwith tf.gfile.Open(input_file_name, \"r\") as f:\r\n  reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\r\n  for line in reader:\r\n    print(line)\r\n```", "@alter-bug-tracer If `tf.gfile.Open` is replaced with `open`, a similar error show up as well:\r\n```\r\n  File \"/usr/lib/python3.5/encodings/ascii.py\", line 26, in decode\r\n    return codecs.ascii_decode(input, self.errors)[0]\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xed in position 0: ordinal not in range(128)\r\n```\r\n\r\nIt might be better to improve the error message in tf, though it seems the error message of `tf.gfile.Open` is inline with `open`?", "This looks like it isn't an issue with the unicode ops. The file looks to me like it wishes it has a BOM (0xEF 0xBB 0xBF) at the start, but instead has (0xED 0xBB 0xBF). This is not a valid encoding, so python internals are throwing a unicode error when they try to convert it using decode().\r\n\r\nI'm not sure how this is unexpected. It seems like throwing an error on corrupted input is a reasonable behavior here. Is the idea that gfile should trap it and throw a TypeError? That's the error thrown when there's an internal type mismatch, but everything here seems to be going to plan -- read type is `bytes` -- until the conversion to a unicode string is attempted.\r\n\r\nIf you want to use the unicode ops, you could read the file as bytes type and convert to unicode using substitution characters which will then load even corrupted data providing a substitution char if it finds corrupted unicode like this.", "@rohan100jain ", "Likely related to #33563", "Was able to reproduce the issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/18bb9100a51813086b6a434a9731b1a8/27537.ipynb).", "Was able to reproduce the issue in Tensorflow 2.6, please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/f15da65211c5ad2f892a32064337b8e1/27537.ipynb#scrollTo=zLBTcNFUN8dA)."]}, {"number": 27500, "title": "[Grappler] RemoveIdentityTranspose also removes conjugate", "body": "\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.13\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 10.0\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n\r\nDuring graph optimization, `RemoveIdentityTranspose` also removes a transpose if it conjugates the input (see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc#L1092))\r\n\r\nSimple example:\r\n```\r\nimport tensorflow as tf\r\nwith tf.Graph().as_default():\r\n    sess = tf.Session()\r\n    a = tf.placeholder(tf.complex64)\r\n    data = [[1j], [1j]]\r\n    print(sess.run(tf.transpose(a, (0, 1), conjugate=True), {a: data}))  # not optimized\r\n    print(sess.run(tf.transpose(a, (0, 1), conjugate=True) + 1, {a: data}))  # optimized, no conjugate will be applied\r\n    print(sess.run(tf.conj(a) + 1, {a: data}))\r\n```\r\n\r\nOutput:\r\n```\r\n[[0.-1.j]\r\n [0.-1.j]]\r\n[[1.+1.j]\r\n [1.+1.j]]\r\n[[1.-1.j]\r\n [1.-1.j]]\r\n```\r\n\r\nThis can happen when using einsum/tensordot and a probably related issue is [#19771](https://github.com/tensorflow/tensorflow/issues/19771)\r\n\r\nA possible fix would be to also check for `IsConjugateTranspose` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc#L1137) ", "comments": ["Hi @jheymann85 ! PR [#40223](https://github.com/tensorflow/tensorflow/pull/40223) has fixed this issue. Attaching resolved [gist](https://colab.sandbox.google.com/gist/mohantym/c8f048bd7c6f73d54e9051cd28e70ca5/git_27500.ipynb) for reference. Can we move this issue to closed status now? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 27457, "title": "Workaround for self-attention using tf.matmul", "body": "We are currently trying to convert a tensorflow model to a tf-lite graph. In particular we are working on a transformer which uses self-attention. The problem with that operation seems to be that `tf.matmul` receives two non-constant inputs which is, according to [the docs](https://www.tensorflow.org/lite/guide/ops_compatibility), not allowed:\r\n\r\n> `tf.matmul` - *as long as the second argument is constant and transposition is not used*\r\n\r\nThis seems like a major limitation for the self-attention mechanism. Is there any workaround available for this?\r\n\r\n(see also [stackoverflow question](https://stackoverflow.com/questions/55491752/workaround-for-using-tf-matmul-with-two-non-constant-inputs))", "comments": ["Currently matmul is not fully supported by tflite, and it will be resolve into fully_connected op during conversion.\r\n\r\nsince your use case self-attention needs it, we can definitely look into it.\r\n\r\ndo you have a model help us debug?\r\n\r\nthanks!", "Hi @stefan-falk ,\r\n\r\nSorry for belated, I saw that this transformer example is well converted into TFLite with tf-nightly.\r\nhttps://www.tensorflow.org/tutorials/text/transformer\r\nCould you try it?", "@renjie-liu It seems that the Keras MultiHeadAttention has been massaged to use fully_connected, instead of BatchMatMul, during the TFLite conversion. Can you please confirm that and point me to where that is done? TIA. ", "It's being done here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/tf_tfl_passes.cc#L216-L219", "Hi @renjie-liu , thank you for the quick reply. I verified that the `unfold_batchmatmul` is an attribute of `TFLiteConverterBase`, which is the base class of  `TFLiteConverterBaseV1`. \r\n\r\nHowever, in my experiment, a multi-head attention implemented in TF1 still results in `BatchMatMul` after conversion by `tf.compat.v1.lite.TFLiteConverter`. Can you please confirm whether `BatchMatMul` in TF1 models can be unfolded by `tf.compat.v1.lite.TFLiteConverter`?\r\n\r\nNote that `experimental_new_converter` is `True` when `tf.compat.v1.lite.TFLiteConverter` was used. ", "I've included the implementation below. There are two calls to `tf.linalg.matmul()` to compute the attention score and attention probability matrices. Converting this TF1 model with `tf.compat.v1.lite.TFLiteConverter` produces `BatchMatMul` instead of the unfolded implementation with FC. \r\n\r\n```\r\n  qkv = tf.layers.dense(inputs, embed_dim*3, name=name+'/atten/qkv')\r\n\r\n  bh_size, seq_len = tf.shape(inputs)[0], tf.shape(inputs)[1]\r\n\r\n  # compute query, key, value matrices\r\n  qkv = tf.reshape(qkv, shape=(bh_size, seq_len, 3, nb_heads, -1))\r\n  qkv = tf.transpose(qkv, perm=(2, 0, 3, 1, 4))\r\n  q, k, v = tf.split(qkv, num_or_size_splits=3)\r\n  q, k, v = [tf.squeeze(t, axis=0) for t in (q, k, v)]\r\n\r\n  # compute self attention weights, 'bhid,bhdi->bhii'\r\n  dots = tf.linalg.matmul(q, k, transpose_b=True, name=name+'/atten/atten_score') * scale\r\n  attn = tf.nn.softmax(dots, axis=-1)\r\n\r\n  # compute weighted value matrix, 'bhii,bhid->bhid'\r\n  out = tf.linalg.matmul(attn, v, name=name+'/atten/atten_prob')\r\n  out = tf.transpose(out, perm=(0, 2, 1, 3))\r\n  out = tf.reshape(out, shape=(bh_size, seq_len, embed_dim))\r\n\r\n  # compute layer output\r\n  out = tf.layers.dense(out, embed_dim, name=name+'/atten/proj')\r\n```", "Unroll batch_matmul is the default behavior however it only works when the input shape is static, I think if you set a static shape, the batch_matmul should be unrolled.", "Thanks for pointing that out. Problem solved!"]}, {"number": 27449, "title": "[Enhancement] Automatically chose either memory or storage to cache in tf.data API", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAt the moment, one has to keep in mind what the systems resources will be when using the tf.data pipeline API. Specifically, when using the https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache operation, I have to ask myself whether it's worth caching the data and whether to do so in memory or storage.\r\n\r\nAs I both switch systems and datasets a lot I often chose wrong and end up with filled swap memory or extremely slow epochs because I have caching disabled.\r\n\r\nWhat I would like to see, although I understand the complexities involved, is some sort of dynamic cache where the systems itself decides what to use (memory, storage or no caching), depending on available resources and the computations to cache.\r\n\r\nA intermediate solution might be one where the user can select how much memory (percentage) to use to cache and for the remaining required cache to use storage so we get some hybrid memory/storage solution.\r\n\r\n**Will this change the current api? How?**\r\nIt will either add to it or change the current cache() parameters. How and what's best here is not up to me.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using the tf.data API looking to improve performance.", "comments": []}, {"number": 27437, "title": "LSTM quantization aware training and fully quanitzation in TfLite", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0, 1.13.1, and tf-nightly\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nI'm not able to figure it out at this moment...\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, I can convert a LSTM model to .tflite file. But, I have no idea how to do fully quantization on it (meaning quantization-aware training + TfLite fully quantization). For me, post training quantization is not very useful, since I'd like to do 8-bit calculations during inference.\r\n\r\n**Will this change the current api? How?**\r\nNot sure. Maybe api like tf.contrib.quanitze.create_training_graph() or tf.contrib.quanitze.create_eval_graph() needs to be changed?\r\n\r\n**Who will benefit with this feature?**\r\nDevices that need 8-bit calculation can directly benefit from this feature. \r\n\r\n**Any Other info.**\r\nI know there are a few issues related to this problem. But there is a lack of official reply on it. I'm writing to ask if it's still in the recent plan? Thanks in advance.", "comments": ["Same issue here. Any progress/updates on this one? ", "Very much interested in this feature as well."]}, {"number": 27381, "title": "Generating fully-quantized models in Pre-trained checkpoints (.chkpt) or GraphDef (.pb)  format", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.12.0-rc2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBy Using the TensorFlow Lite Converter tflite_convert to optimize the TensorFlow graphs and convert them to the TensorFlow Lite format for 8-bit inference. \r\n\r\nCan we have a feature to optimize the TensorFlow graphs and convert them to .chkpt or .pb format?  So, We can Initialize the WB in tensorflow and do inference with 8-bit.\r\n\r\n**Will this change the current API? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nThe developers who want only Int-8 quantization model(WB) and not depend on TFLite.\r\n\r\n**Any Other info.**\r\nNo", "comments": ["@nullbyte91 Could you describe the feature and its context with little more detailas? Thanks!", "@jvishnuvardhan \r\nI'm trying to understand quantize aware training with cifarnet from models/research/slim/nets/ and Extract INT8 WB and dump it in checkpoints (.chkpt) or GraphDef (.pb) format not tflite. \r\n \r\n**Prepare a network for quantization and training:**\r\n   Add fake quantization layers to the model graph by adding tf.contrib.quantize.create_training_graph() before optimization procedure and train the network. For example, \r\nI train the network for 10000 steps and used tf.contrib.quantize.create_training_graph(quant_delay=90000). The value 90000 indicates that the model will be trained for 90000 steps with floating-point weights and activations, before the quantization process begins.\r\n\r\n**Prepare the Graph for Inference:**\r\n1. Add fake quantization layers to the graph by adding tf.contrib.quantize.create_eval_graph() before graph_def = graph.as_graph_def(). \r\n2. Export the inference graph to a protobuf file. For CifarNet this is done using:\r\npython export_inference_graph.py --model_name=cifarnet --dataset_name=cifar10 --\r\noutput_file=/tmp/cifarnet_inf_graph.pb.\r\n3. Freeze the graph using the freeze_graph tool. You can specify any checkpoint during training for this. The command to freeze the graph is:\r\npython -m tensorflow.python.tools.freeze_graph \\\r\n--input_graph=<your_graph_location> \\\r\n--input_checkpoint=<your_chosen_checkpoint> \\\r\n--input_binary=true \\\r\n--output_graph=<output_directory> \\\r\n--ouput_node_names=<output_nodes>\r\n\r\n**If we look at the Quantization aware training doc from Tensorflow,\r\nThe previously demonstrated after-rewrite eval graph only simulates quantization. To generate real fixed-point computations from a trained quantization model, convert it to a fixed-point kernel. TensorFlow Lite supports this conversion from the graph resulting from create_eval_graph.**\r\n\r\n**Quantize the Graph:**\r\ntflite_convert --graph_def_file=<your_frozen_graph> \\\r\n--output_file=<your_chosen_output_location> \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--output_arrays=<your_output_arrays> \\\r\n--input_arrays=<your_input_arrays> \\\r\n--mean_values=<mean of input training data> \\\r\n--std_dev_values=<standard deviation of input training data>\r\n\r\nBut, It converts frozen graph which has a fake quantization layer to fully-quantized TensorFlow Lite model. I actually want to skip this part, converts the frozen graph to .chpt or .pb format.\r\n\r\nThere are tools available(flatbuffer, netron) to view and extract the information from the tflite file. But, None of them converts into numpy array, .pb or .chpt format. It just a visualization tool.\r\n\r\nPlease let me know, If you requires any other information.", "I also have this demand (pb format quantization model), please help us, thanks! @jvishnuvardhan ", "I would also like to estimate the performance drop from uint8 quantization of both weights and activations, but without having to use tflite. \r\nIt would be nice to be able to have a frozen_graph.pb with quantized operations that can be evaluated using regular tensorflow inference.", "Hi @nullbyte91 ,  \r\n\r\nI also need fully quantized models for inference in Tensorflow. I found this issue raised several months ago. Do you have any way to convert to .pb format now?\r\n", "@ZhangShuoAlreadyExistst No. There is a way to dump the tfLite Quantization numbers to JSON format but its a tedious task. ", "@nullbyte91  Thanks for your reply.", "Hi @nullbyte91 ,\r\ncan you guide me how to use quantization aware with a pretrain weight(32bit)(ckpt)?\r\nI have already trained a 32bit weight. Can I make it be a pretrain weight and train by quantization aware(tf.contrib.quantize.create_training_graph(quant_delay=300000))?\r\n\r\nThank you.", "I aslo like to have this feature. It would be nice if we can have a quantization in regular tensorflow."]}, {"number": 27380, "title": "[Feature Request] Support Sparse Tensors in tf.linalg operations", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0a0\r\n- Are you willing to contribute it (Yes/No): Maybe, if there are any pointers on how to go about implementing something like this.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently the `tf.linalg` operations seem to only support dense tensors. I have verified `tf.linalg.solve`. Consider the following variables\r\n\r\n```\r\nA = tf.sparse.SparseTensor(\r\n    indices=[[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]],\r\n    values=[3., 2., -1., 2., -2., 4., -1., 0.5, -1.],\r\n    dense_shape=(3, 3)\r\n)\r\n\r\nb = tf.sparse.SparseTensor(\r\n    indices=[[0, 0], [1, 0], [2, 0]],\r\n    values=[-1., 2., 0.],\r\n    dense_shape=(3, 1)\r\n)\r\n```\r\n\r\nnow if I do\r\n\r\n```\r\ntf.linalg.solve(A, b)\r\n```\r\n\r\nthis errors out with\r\n\r\n```\r\nValueError: Attempt to convert a value (<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f28a1e3fc50>) with an unsupported type (<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>) to a Tensor.\r\n```\r\n\r\nHowever, if I do\r\n\r\n```\r\ntf.linalg.solve(tf.sparse.to_dense(A), tf.sparse.to_dense(b))\r\n```\r\n\r\nit works, giving\r\n\r\n```\r\n<tf.Tensor: id=11, shape=(3, 1), dtype=float32, numpy=\r\narray([[-1.0000006],\r\n       [ 2.0000014],\r\n       [ 2.0000012]], dtype=float32)>\r\n```\r\n\r\nwhich is roughly the correct answer. Example taken from the [scipy sparse matrix factorized](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.factorized.html) documentation.\r\n\r\n**Will this change the current api? How?**\r\n\r\nInstead of erroring out, the `linalg` operations will work for sparse inputs just as well as for dense inputs.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nI came across this while implementing [NVIDIA's FastPhotoStyle](https://github.com/NVIDIA/FastPhotoStyle) in TensorFlow 2.0 + Keras.\r\n\r\nThe algorithm essentially has 2 steps,\r\n\r\n- A `PhotoWCT` transformation that is `WCT` but uses maxpooling argvalues as unpooling masks\r\n- A Photorealistic smoothing step that restores geometric artifacts for objects distorted using regular style transform\r\n- An additional post processing step that applies a GPU based smoothing filter for further reducing structural defects\r\n\r\nThe NVIDIA implementation provides a [`photo_wct` net implementation in PyTorch](https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_wct.py). However for the second step, [a CPU implementation in scipy is used](https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_smooth.py). The second step essentially solves a closed form system of equations. \r\n\r\nConsider [line 48](https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_smooth.py#L48), what it is essentially doing is creating a diagonal matrix of size (width x height) of the image. So for a 512x512 image, in dense form it will have to create a matrix of 2^36 floats.\r\n\r\nThe matrix is later used to solve a system of equations (https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_smooth.py#L52).\r\n\r\n2^36 tf.float32 values will occupy 2^8 = 256GB of memory, which will definitely overflow. Ideally, I should be able to work with tensorflow's `linalg` module just the same as the scipy implementation. An additional flexibility with TensorFlow is that it will place operations on GPUs automatically. Additionally, I can put it in a keras `Lambda` layer to add it to a model.\r\n\r\n\r\n**Any Other info.**\r\n\r\nNo", "comments": ["FYI: We are working towards this goal, but I cannot give you any firm dates. Stay tuned!", "@rmlarsen any updates on this? I am planning ahead and I'd need to know if I should wait for `expm` to work with sparse tensors or if I should implement everything with dense tensors.", "Please check https://github.com/tensorflow/addons/pull/2396", "Are there any updates in this regard? Especially a sparse solver (like linalg.solve) would be great."]}, {"number": 27314, "title": "[tflite]:operator of type Floor for which the quantized form is not yet implemented", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (or github SHA if from source):1.13\r\n- code on jupyter: \r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nmodel_path='./VESPCN.pb'\r\ninput_arrays=['images_next_curr',\"images_prev_curr\"]\r\noutput_arrays=['Tanh_2']\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(model_path, input_arrays, output_arrays,\r\n                                                      input_shapes={'images_next_curr':[1, 360, 640, 6],'images_prev_curr':[1, 360, 640, 6]})\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.allow_custom_ops=True\r\nconverter.inference_type=tf.lite.constants.QUANTIZED_UINT8\r\nconverter.default_ranges_stats=(0,6)\r\nconverter.quantized_input_stats={'images_next_curr':(0,1),'images_prev_curr':(0,1)}\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nConverterError: TOCO failed. See console for info.\r\n2019-03-30 17:14:23.618847: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 739 operators, 1251 arrays (0 quantized)\r\n2019-03-30 17:14:23.638961: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 739 operators, 1251 arrays (0 quantized)\r\n2019-03-30 17:14:23.683782: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 376 operators, 643 arrays (2 quantized)\r\n2019-03-30 17:14:23.718247: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 376 operators, 643 arrays (2 quantized)\r\n2019-03-30 17:14:23.762560: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 300 operators, 459 arrays (2 quantized)\r\n2019-03-30 17:14:23.774999: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 300 operators, 459 arrays (2 quantized)\r\n2019-03-30 17:14:23.789415: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 271 operators, 430 arrays (2 quantized)\r\n2019-03-30 17:14:23.806875: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 271 operators, 430 arrays (2 quantized)\r\n2019-03-30 17:14:23.821737: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 271 operators, 430 arrays (2 quantized)\r\n2019-03-30 17:14:23.836560: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 271 operators, 430 arrays (2 quantized)\r\n2019-03-30 17:14:23.841059: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 271 operators, 430 arrays (2 quantized)\r\n2019-03-30 17:14:23.842802: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Floor for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fb07e2fd700 (most recent call first):\r\n  File \"/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/disk1/zww/anaconda2/envs/test/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n```\r\n\r\n**Describe the problem**\r\nI try to deploy a VESPCN(video SR) model on android, but after I have taken some time to do custom ops , I failed to quantize the model with this problem, as far as I'm concerned, since the\r\ndata is uint8 type, there is no need of quantization for floor op. can you tell me how to solve this problem,  thanks\r\n\r\n", "comments": ["@jdduke", "@suharshs any thoughts on how to treat round/floor/ceil in the context of quantized data?", "yeah,if you haven't solve this problem, I will try to solve this with your thoughts.\r\n@jdduke "]}, {"number": 27303, "title": "Need tf.signal.rfft op in TFLite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.13.1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nIf I pass the SELECT_TF_OPS option then I get:\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, SHAPE, SPLIT, SPLIT_V, STRIDED_SLICE, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: RFFT.\r\n```\r\nIf I don't pass the SELECT_TF_OPS option then I get:\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, SHAPE, SPLIT, SPLIT_V, STRIDED_SLICE, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: ComplexAbs, Cos, LinSpace, RFFT.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nThe code to create the model is pretty short since I hardcoded a bunch of parameters for now:\r\n\r\n    with tf.Graph().as_default(), tf.Session() as sess:\r\n        # input sound data as a waveform\r\n        waveform = tf.placeholder(tf.float32, [None])\r\n        # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].\r\n        pcm = tf.math.scalar_mul(1/32768.0, waveform)\r\n\r\n        # compute Short Time Fourier Transform\r\n        stft = tf.signal.stft(pcm, frame_length=400, frame_step=160, fft_length=512)\r\n        spectrogram = tf.abs(stft)\r\n\r\n        # Warp the linear scale spectrograms into the mel-scale.\r\n        num_spectrogram_bins = stfts.shape[-1].value\r\n        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 125.0, 7500.0, 64\r\n        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n          num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\r\n          upper_edge_hertz)\r\n        mel_spectrogram = tf.tensordot(\r\n          spectrogram, linear_to_mel_weight_matrix, 1)\r\n        mel_spectrogram.set_shape(spectrogram.shape[:-1].concatenate(\r\n          linear_to_mel_weight_matrix.shape[-1:]))\r\n\r\n        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n        log_mel_spectrogram = tf.log(mel_spectrogram + 1e-6)\r\n        # with the model loaded and input/output tensors defined, convert to tf.lite\r\n        converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [log_mel_spectrogram])\r\n        converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n        tflite_model = converter.convert()\r\n\r\n**Any other info / logs**\r\nAssuming the SELECT_TF_OPS option produces a model that will work on TFLite on iOS, then I guess all I need is RFFT.  Thank you!\r\n\r\n", "comments": ["I checked the master branch and I see this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/c77e7e56de56c624116cf9eea340b4f96f032c85#diff-ed4b7d597384e8e4b1210b7558a16640\r\n\r\nLooks like someone already added what I needed, it's just not released yet :-)  Sorry, I should have dug into the source before filing this issue.\r\n\r\nIf anyone else runs into this, it'll probably be available in the release after 1.13.1, or build from master.", "I tried this with the nightly build from March 31, 2019 and unfortunately this op doesn't seem to actually work.  The converter lets it go because it's on the whitelist but when I try to allocate tensors in TF Lite with a model using it, the allocation fails.  Here's example code that requires numpy and tensorflow (the stft op calls the rfft op):\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    target_path = \"/tmp/log_mel_spectrogram.tflite\"\r\n    with tf.Graph().as_default(), tf.Session() as sess:\r\n        # input sound data as a waveform\r\n        waveform = tf.placeholder(tf.float32, [None])\r\n        # Convert to mono PCM samples in the range [-1, 1].\r\n        pcm = tf.math.scalar_mul(1/32768.0, waveform)\r\n\r\n        # compute Short Time Fourier Transform\r\n        stft = tf.signal.stft(pcm, frame_length=400, frame_step=160, fft_length=512)\r\n\r\n        # with the model loaded and input/output tensors defined, convert to tf.lite\r\n        converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [stft])\r\n        converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n        tflite_model = converter.convert()\r\n        open(target_path, \"wb\").write(tflite_model)\r\n\r\n    # verify we can execute the converted model (should print out an array)\r\n\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=target_path)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(output_data)\r\n\r\nIt fails at the `interpreter.allocate_tensors()` line with the following:\r\n\r\n    ---------------------------------------------------------------------------\r\n    RuntimeError                              Traceback (most recent call last)\r\n    <ipython-input-11-85ffc6e57901> in <module>\r\n         21 # Load TFLite model and allocate tensors.\r\n         22 interpreter = tf.lite.Interpreter(model_path=target_path)\r\n    ---> 23 interpreter.allocate_tensors()\r\n         24 \r\n         25 # Get input and output tensors.\r\n\r\n    ~/python_envs/tensorflow_master_monolithic/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n         93   def allocate_tensors(self):\r\n         94     self._ensure_safe()\r\n    ---> 95     return self._interpreter.AllocateTensors()\r\n         96 \r\n         97   def _safe_to_run(self):\r\n\r\n    ~/python_envs/tensorflow_master_monolithic/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n        104 \r\n        105     def AllocateTensors(self):\r\n    --> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n        107 \r\n        108     def Invoke(self):\r\n\r\n    RuntimeError: tensorflow/lite/kernels/split_v.cc:129 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 || input_type == kTfLiteInt16 was not true.Node number 1 (SPLIT_V) failed to prepare.\r\n\r\nChanging the output tensor to the `pcm` tensor instead of `stft` works fine `converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [pcm])` and prints out a result, so seems like although the RFFT op is whitelisted- it doesn't actually work?", "Hi @jpangburn, I'm curious if the model works if you change\r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\nto\r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nMy guess is that some of our builtin ops don't support the full suite of types (e.g., complex types) in all operators, which may be required when using rfft. Can you give that a try and let us know how it goes? Thanks.", "Hi @jdduke , I tried that and I get\r\n\r\n    RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 0 (Flex) failed to prepare.\r\n\r\nI also read somewhere that TF lite doesn't support `[None]` as an input dimension so I tried fixing it to 16000 (one second worth of data at 16k sample rate) so the code looks like this:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    target_path = \"/tmp/log_mel_spectrogram.tflite\"\r\n    with tf.Graph().as_default(), tf.Session() as sess:\r\n        # input sound data as a waveform\r\n        waveform = tf.placeholder(tf.float32, [16000])\r\n        # Convert to mono PCM samples in the range [-1, 1].\r\n        pcm = tf.math.scalar_mul(1/32768.0, waveform)\r\n\r\n        # compute Short Time Fourier Transform\r\n        stft = tf.signal.stft(pcm, frame_length=400, frame_step=160, fft_length=512)\r\n\r\n        # with the model loaded and input/output tensors defined, convert to tf.lite\r\n        converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [stft])\r\n        converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n        tflite_model = converter.convert()\r\n        open(target_path, \"wb\").write(tflite_model)\r\n\r\n    # verify we can execute the converted model (should print out an array)\r\n\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=target_path)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(output_data)\r\n\r\nBut that gets the following error:\r\n\r\n> 2019-04-08 09:26:39.664498: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: RFFT\r\n2019-04-08 09:26:39.675630: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 8 operators, 20 arrays (0 quantized)\r\n2019-04-08 09:26:39.675859: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 8 operators, 20 arrays (0 quantized)\r\n2019-04-08 09:26:39.676099: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 8 operators, 19 arrays (0 quantized)\r\n2019-04-08 09:26:39.676148: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (80 vs. 400)Dimensions must match\r\nFatal Python error: Aborted\r\n\r\nI don't know why it has a problem with the dimensions as it works fine in regular TF.  For example, if you throw this line in right after the `stft =` line it prints out an array just fine `print(sess.run([stft], {waveform: np.array(np.random.random_sample((16000,)), dtype=np.float32)}))`.", "We're still working out some issues when running inference with the TF ops from Python (see https://www.tensorflow.org/lite/guide/ops_select#python_pip_package). We're hoping to have that resolved for the 1.14 release.\r\n\r\nHave you tried running with a [manual TFLite build](https://www.tensorflow.org/lite/guide/ops_select#running_the_model) (either with C++ or Java)? We'll be releasing pre-built .aar/cocoapods in the near future, which should make this easier. ", "I just tried it with an iOS build (from the `tensorflow/lite/experimental/swift` directory) and it gets the same error as the full code sample I provided earlier:\r\n\r\n> TensorFlow Lite Error: tensorflow/lite/kernels/split_v.cc:129 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 || input_type == kTfLiteInt16 was not true.\r\nTensorFlow Lite Error: Node number 1 (SPLIT_V) failed to prepare.\r\n\r\nThat's with the code sample that successfully creates the model but won't run it.  In the Swift code I create a TFLite interpreter and pass it 16k worth of float32 test data.  I don't know what the difference is between that build and the manual build you are referring to, are they the same or different?\r\n", "If you don't mind, feel free to forward me your converted ..tflite model (and/or the source model), and I'd be happy to troubleshoot further.", "Oh, sorry, I see now.  That build you pointed me to has support for \"select tensorflow ops\" which the build I was using probably doesn't.\r\n\r\nRegarding the tflite model, you can generate it yourself from the code I provided which just requires a python environment with numpy and tensorflow:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    target_path = \"/tmp/log_mel_spectrogram.tflite\"\r\n    with tf.Graph().as_default(), tf.Session() as sess:\r\n        # input sound data as a waveform\r\n        waveform = tf.placeholder(tf.float32, [None])\r\n        # Convert to mono PCM samples in the range [-1, 1].\r\n        pcm = tf.math.scalar_mul(1/32768.0, waveform)\r\n\r\n        # compute Short Time Fourier Transform\r\n        stft = tf.signal.stft(pcm, frame_length=400, frame_step=160, fft_length=512)\r\n\r\n        #print(sess.run([stft], {waveform: np.array(np.random.random_sample((16000,)), dtype=np.float32)}))\r\n        # with the model loaded and input/output tensors defined, convert to tf.lite\r\n        converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [stft])\r\n        converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n        tflite_model = converter.convert()\r\n        open(target_path, \"wb\").write(tflite_model)\r\n\r\nThe commented out print line provides a sample way to invoke it (in regular TF).  If you need to remove the `[None]` parameter then you could replace it with [16000] to let it process one second worth of data and that would be usable too.  I've also provided the model zipped up if that's easier.\r\n[log_mel_spectrogram.tflite.zip](https://github.com/tensorflow/tensorflow/files/3055577/log_mel_spectrogram.tflite.zip)\r\n\r\n", "@jpangburn can you please share the latest status on this? Have you been able to find a workaround?", "@dkashkin I don't know what the current status of it on TF is, I wrote the RFFT out manually in code to solve it for my problem.  I submitted this because the TFLite documentation asked people to let them know what operations are actually needed.  As this is needed for audio signal processing to use the Google sound model [Audioset](https://research.google.com/audioset/) it seemed like a fair request :-)\r\n\r\nWriting it out manually was not easy for me and I imagine other people will want to use Audioset on a mobile device, so this is a useful thing to provide in TFLite- but again, I don't know its status.", "We've implemented rfft2d as a custom op which can be used optionally, we could probably do the same for rfft. I've filed an internal request to explore implementing this as a proper builtin op. Thanks for your patience.", "Thanks @jpangburn!  Can you please clarify - did you end up manually coding the entire preprocessing algorithm (generating spectrograms based on raw audio) or is there any opportunity to run Fast Fourier Fansform inside Tensorflow Light? PS. I am trying to improve performance of my app by offloading this heavy preprocessing to TFLite in hope that it will leverage GPU whenever available... Appreciate your help :)", "Hi @dkashkin yes, I manually coded it to generate spectrograms from raw audio mainly with a bunch of vDSP calls in Swift (e.g. vDSP_DFT_Execute to execute the FFT with a vectorized implementation).  If you're on iOS and unfamiliar with vDSP look here [https://developer.apple.com/documentation/accelerate/vdsp](https://developer.apple.com/documentation/accelerate/vdsp).  Supposedly highly optimized.  It ran fast enough for what I was trying to do.\r\n\r\nIf you're doing the AudioSet stuff and are trying to follow VGGish but get it working with TFLite, as of when I submitted this- this was the best option for iOS IMHO.  If you're writing for Android, then obviously you would need to use their equivalent.  If @jdduke is able to get this in TFLite, that would make this WAAAY easier.  I imagine we're not the only ones wanting to use AudioSet on TFLite :-)  I'm glad you chimed in.  Would help to write more portable code too if we could get that whole thing in TFLite- I'd like my stuff to run on Android as well but coding that again in another language is not something I really want to do haha!", "Thanks @jpangburn I agree with you - everybody who tries to do signal processing for mobile is wasting a lot of time because TFLite still cannot do FFT :(\r\nPS. If you are interested I'd be happy to share my Kotlin code that generates MFCC on Android. The only problem is - it's dog slow (3 seconds per spectrogram on low end phones which is not acceptable). I keep my fingers crossed that  @jdduke finds a quick solution...", "+1, this would be great to have ", "This is great discussion, thanks all for the feedback.\r\n\r\nAs there are a number of related ops to FFT (RFFT(2D/3D), IRFFT(2D/3D), FFT(2D/3D), ComplexAbs), it would be good to know *precisely* which op variants are required for your models so we can prioritize support accordingly. If you can link to specific source models, that would be extremely helpful. Thanks!", "ComplexAbs, RFFT (2D) are the impacted ops for me. The part of the model impacted is just this https://www.tensorflow.org/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms line-for-line. ", "Hi! I'm the author of `tf.signal`. Sorry that `tf.signal.stft` doesn't fully work yet.\r\n\r\nIn case it's helpful, you can replace the RFFT and ComplexAbs in your network with an RDFT matrix multiply. Here's an example: \r\n\r\nhttps://github.com/tensorflow/magenta/blob/cf80d19fc0c2e935821f284ebb64a8885f793717/magenta/music/melspec_input.py#L64-L90\r\n\r\nThat file has some other tflite compatibility tricks that are no longer required I believe (e.g. `tf.signal.frame` should be supported natively now). \r\n", "Also, I believe `tf.abs(tf.signal.stft(...))` works with `SELECT_TF_OPS` if you use the \r\n\"new\" converter (i.e. enable `experimental_new_converter` [here](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter?version=stable)).", "@rryan big THANK YOU for tf.signal! It's awesome. ", "@rryan can you please give us some more details on why you think the experimental_new_converter should support tf.signal.stft? I just tried this option and although it generates a slightly different tflite file, the tflite inference still fails in this test:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nwith tf.Graph().as_default(), tf.Session() as sess:\r\n    waveform = tf.placeholder(tf.float32, [None])\r\n    result = tf.abs(tf.signal.stft(waveform, frame_length=400, frame_step=160, fft_length=512))\r\n    converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [result])\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.experimental_enable_mlir_converter = True\r\n    tflite_model = converter.convert()\r\n    open(\"/content/test.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"/content/test.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\n```\r\n\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexConst) failed to prepare.", "> @rryan big THANK YOU for tf.signal! It's awesome.\r\n\r\nThanks :) Glad it's useful.\r\n\r\n> @rryan can you please give us some more details on why you think the experimental_new_converter should support tf.signal.stft? \r\n\r\nI think the binary you use to run the model would need the flex delegate linked in. Instructions here:\r\nhttps://www.tensorflow.org/lite/guide/ops_select#running_the_model\r\n\r\nI don't know how to make this work for Python -- and the instructions [here](https://www.tensorflow.org/lite/guide/ops_select#python_pip_package) don't say how. Maybe @jdduke knows?\r\n\r\n", "@jdduke Regarding what models, the one I was trying to get a TFLite model for is a Tensorflow model: https://github.com/tensorflow/models/tree/master/research/audioset/vggish.  The tough one for me was a numpy call \"np.fft.rfft\" inside mel_features.py's stft_magnitude() method.  From my original post, I also tried to use tf.signal.stft to replace this, but it was calling RFFT and that didn't work at the time in TFLite.", "Thanks a ton, I got this to work with tflite on tensorflow 1.15 on fixed size audio with the magenta code.", "Thanks again @rryan! I just tested the tflite-compatible graph generated by Magenta. It works on Android, and seems to be roughly 5 times faster than my Kotlin based SFFT. The tflite model is quite large though (4.6MB for 224x224 spectrogram). ", "@rryan Thanks! Using the magenta code helped, up to the point where I use `tf.signal.mfccs_from_log_mel_spectrograms`, which apparently needs RFFT / FlexRFFT as well. Any hint for a replacement to make it TFLite compatible without making use of TensorFlow ops (avoiding the use of flex delegates)?", "> @rryan Thanks! Using the magenta code helped, up to the point where I use `tf.signal.mfccs_from_log_mel_spectrograms`, which apparently needs RFFT / FlexRFFT as well. Any hint for a replacement to make it TFLite compatible without making use of TensorFlow ops (avoiding the use of flex delegates)?\r\n\r\nYea, that's right -- calculating MFCCs requires the DCT, which is implemented using an FFT :-/. \r\n\r\nYou would need to replace the DCT that it uses with something that uses an RDFT instead of RFFT. Here's a colab showing how to do it and checking for tf.signal compatibility: \r\nhttps://colab.research.google.com/drive/1C9jyM4CtW2Yn9xsPXt31cZnQuNe7sjU8\r\n\r\nI just noticed that the Magenta `_naive_rdft` function doesn't quite match `tf.signal.rfft`'s conventions, so if you're trying to use `tf.signal.rfft` in training but `_naive_rdft` in tf.lite, you'll need to make sure they match:\r\n* Use \"right\" padding instead of center padding.\r\n* Take the complex conjugate of the result (flip the sign of the imaginary component).\r\n\r\nHere are the relevant code references in tf.signal that the above colab re-implements:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3d8914bb88b666b813d2ce025ee10cc59fd39422/tensorflow/python/ops/signal/mfcc_ops.py#L97-L109\r\nhttps://github.com/tensorflow/tensorflow/blob/3d8914bb88b666b813d2ce025ee10cc59fd39422/tensorflow/python/ops/signal/dct_ops.py#L120-L130", "@rryan Thank you, that's of great help!\r\n\r\nNow I get on my 2.0.0.dev20190731 installation when calling `allocate_tensors` after loading the tflite model:\r\n`RuntimeError: tensorflow/lite/kernels/strided_slice.cc ellipsis_mask is not implemented yet.Node number 22 (STRIDED_SLICE) failed to prepare.`\r\n\r\nI will comment further once I find the time to dig into this, hopefully next week.", "Ah, bummer. I vaguely remembered that `...` in a strided slice was unsupported. If you know the rank you can simply hard-code the dimensions. Otherwise you can replace strided slice with a `tf.slice` (which is much more awkward). Sorry about all the trouble :(.", "Thank you again for the great and fast support! I will go with the hard coded dimensions for now. Thanks again.", "As expected, replacing `[..., :axis_dim]` by `[:, :, :axis_dim]` and `[..., :dct_coefficient_count]` by `[:, :, :dct_coefficient_count]` worked in my case. Thanks! :)", "@padoremu can you please share the full code snippet that works for you? This workaround  sounds promising!", "@dkashkin My python file has 350 lines of code, so I wouldn't want to paste it. Is it allowed to attach a python file as a TXT file?\r\n", "@padoremu I'd hugely appreciate if you can create a public gist on Github and post a link to it. Thank you!", "@dkashkin Thanks for the hint. There you go: [public gist](https://gist.github.com/padoremu/8288b47ce76e9530eb288d4eec2e0b4d)\r\n\r\nI haven't compared the output to that of the TF functions yet though. The file also contains other test functions for problems with TF / TFLite in the context of audio processing, as described in the file header.", "Really helpful discussion, the magenta implementation worked for me as a replacement for tf.signal.stft. On the model I am using, I need to convert back to time-domain via tf.signal.inverse_stft, which is currently failing. Does anyone knows of a tflite-friendly implementation of inverse_stft? I can code it on Kotlin, but would be awesome if I could just do everything on the model.", "Thanks @rryan @padoremu for the very helpful discussion. I was able to save Yamnet as tflite using your suggestions. \ud83d\ude05", "> Thanks @rryan @padoremu for the very helpful discussion. I was able to save Yamnet as tflite using your suggestions. \ud83d\ude05\r\n\r\nHi @antonyharfield , to save Yamnet as tflite, did you mean to use rdft to replace stft and abs? Can you share that part of code?\r\n\r\nThanks and I've got the answer by reading rryan's reply. By the way, the tflite model for yamnet is about 16M?\r\nAnd I tried quantized tflite model, the predictions are much different from those by tflite. How about yours.", "Just in case if you are interested to train sounds detection model on Audioset and run it with TFLite, please feel free to try [kws_streaming](https://github.com/google-research/google-research/blob/master/kws_streaming).\r\nIt already has dozen of different neural network topologies (including attention one) which are outperforming standard conv neural nets. All models are compatible with TFLite and designed for running on mobile phone, also you can qunatize your models and run them in streaming mode if needed.", "Has anyone verified if one of the alternative TFLite compatible approaches produce the same results as the tf.signal.stft approach? I tried the code by @padoremu (which seems to be based on magneta) and while that code works and I get some reasonable log_mel_spectrograms, they are not exactly the same as the ones from tf.signal.", "I got nearly identical results. You may want to check some parameters\ninside the code, making sure they are consistent with yours.\n\nOn Sun, 19 Apr 2020 at 12:44, navid-a <notifications@github.com> wrote:\n\n> Has anyone verified if one of the alternative TFLite compatible approaches\n> produce the same results as the tf.signal.stft approach? I tried the code\n> by @padoremu <https://github.com/padoremu> (which seems to be based on\n> magneta) and while that code works and I get some reasonable\n> log_mel_spectrograms, they are not exactly the same as the ones from\n> tf.signal.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27303#issuecomment-616029963>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACN3SSUF3S3RNMBWOD5KH6TRNJ6ULANCNFSM4HCM6POQ>\n> .\n>\n", "@sailorbj I have put my YAMNet code in a repo and also written up the steps if you are still interested: [Converting YAMNet audio detection model for TFLite](https://medium.com/@antonyharfield/converting-the-yamnet-audio-detection-model-for-tensorflow-lite-inference-43d049bd357c)", "Is there any ETA on the arrival of the rfft op in tflite? It  would make the whole audio-detection pipeline in Android and IOS much, much easier :/", "Working based on @padoremu 's gist, I replaced `_fixed_frame()` with `tf.signal.frame`.\r\nThis allows for variable sized inputs and maintains compatibility with TFLite.\r\n\r\nEdit is as simple as:\r\n![image](https://user-images.githubusercontent.com/53020268/90424907-85019600-e084-11ea-8a76-563ebe7e3744.png)", "@JanSo19 Seems like it's in their roadmap.\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/y4gAAavb4zs\r\nhttps://www.tensorflow.org/lite/guide/roadmap", "Has anyone had success with audio_microfrontend_op from tensorflow.lite.experimental.microfrontend.python.ops?", "> Working based on @padoremu 's gist, I replaced `_fixed_frame()` with `tf.signal.frame`.\r\n> This allows for variable sized inputs and maintains compatibility with TFLite.\r\n> \r\n> Edit is as simple as:\r\n> ![image](https://user-images.githubusercontent.com/53020268/90424907-85019600-e084-11ea-8a76-563ebe7e3744.png)\r\n\r\nsign, not working for me. Due to the BatchMul incompatibility with tflite", "> @dkashkin Thanks for the hint. There you go: [public gist](https://gist.github.com/padoremu/8288b47ce76e9530eb288d4eec2e0b4d)\r\n> \r\n> I haven't compared the output to that of the TF functions yet though. The file also contains other test functions for problems with TF / TFLite in the context of audio processing, as described in the file header.\r\n\r\n@padoremu Great work !! This is a damn saver. So how to do inference in TFlite if the input is fixed? (e.g. I'm training in batch 256 but it is obviously that I can't inference with a fixed batch of 256, padding empty will help?)", "@Zepyhrus Thanks, glad to hear that it helped! Unfortunately I can't help with further TF Lite related questions, as it pretty soon turned out that TF Lite was too heavyweight for our needs and TF Lite Micro neither supported STFT nor RNNs, and it didn't seem like that would happen soon. So we went a completely different way, which we are very happy with (but using TF for training).", "> @Zepyhrus Thanks, glad to hear that it helped! Unfortunately I can't help with further TF Lite related questions, as it pretty soon turned out that TF Lite was too heavyweight for our needs and TF Lite Micro neither supported STFT nor RNNs, and it didn't seem like that would happen soon. So we went a completely different way, which we are very happy with (but using TF for training).\r\n\r\nBig fan of your code, `playground` shows your great expertise in both tf and audio processing. Do you mind sharing a little bit of your solution for this? We will stuck to tflite though. ", "@padoremu ", "> as it pretty soon turned out that TF Lite was too heavyweight for our needs\r\n\r\nHi @padoremu, can you be more explicit about this? Was it binary size? Latency? Memory usage? ", "@Zepyhrus Thank you for the compliments. :-) Actually originally I am a computer/machine vision guy, but getting along with audio in the meanwhile.\r\n\r\nIf it was a private project, I would be happy to share, but the company I work for doesn't want to do so. But I can say, in the end I implemented a small framework for inference in C/C++ from scratch, because I didn't find any suitable project that would support pulling in TF models. Ours implements the layers that we need, supporting the features that we need. Another guy who loves doing the assembly part took care of optimizing the bottleneck functions and implementing the STFT. Our framework produces the exact same results as TF (except floating point arithmetic related deviations). Overall it wasn't such a big deal (implementing the training would have been) since we only need a subset, and it was fun. I can also recommend this for educational purposes. ;-)\r\n\r\nI think it's not that uncommon that people implement their own thing for deployment when it comes to DSPs. The difference probably is that not so many implement it as a generic framework.\r\n\r\n@jdduke Our decision was made about a year ago, so I hope I can recall this precisely. In first place, it was binary size. If I remember well, TF Lite resulted in something in the order of magnitude of 50 MB and for TF Lite Micro, it was around 5 MB I think. Maybe there were also some dependencies involved for TF Lite. TF Lite Micro was pretty much what we were looking for for our needs (slim, minimalistic, minimum overhead, rather easy to extend). So one could say everything that comes on top with TF Lite compared to TF Lite Micro was too much for our needs.\r\n\r\nI also filed a ticket regarding RNN support in TF Lite Micro in February 2020 (#36688), which is still open, and I haven't received any feedback until today. So I spent some time figuring out if it could be an option to implement the missing RNN functionality into TF Lite Micro myself. What killed it in the end for me was the TF Lite model conversion, which is mandatory for using TF Lite Micro. There were four variants of converting an RNN with the TF Lite converter, all of them lacking different functionality on the TF Lite Micro side; the above-mentioned ticket tensorflow/tflite-micro#907 explains in detail.\r\n\r\nIn the end, this model conversion was just too confusing, the flag `experimental_new_converter` added some uncertainty, and the idea was basically, if we have to bypass that conversion and implement our own conversion + implementing RNN support in TF Lite Micro, we can as well just implement the whole thing and tailor it exactly to our needs. Our binary size is well < 1 MB by means of the above-mentioned comparison and we have minimal latency. We use dynamic memory allocation though (in contrast to TF Lite Micro).\r\n\r\nAnd we didn't care that much about the STFT for productive use, since we wanted to implement that ourselves anyways.\r\n\r\nSo please don't get me wrong, it was a rational deployment decision, and we are extremely happy with and thankful for being able to use TF for training and prototyping!\r\n", "Thanks for the detailed response.\r\n\r\n> In first place, it was binary size. If I remember well, TF Lite resulted in something in the order of magnitude of 50 MB \r\n\r\nHmm, for ARM64 builds, TFLite is ~150KB for the base runtime, and ~2MB for all builtin ops. I suppose an unstripped, debug build could weigh in at 50MB, or if you were using the target that links in select TF ops.\r\n\r\n> So please don't get me wrong, it was a rational deployment decision, and we are extremely happy with and thankful for being able to use TF for training and prototyping!\r\n\r\nFair enough. I'll follow up on the TFLite Micro ticket for RNN, as RNN support is on our radar for 2021.", "> @Zepyhrus Thank you for the compliments. :-) Actually originally I am a computer/machine vision guy, but getting along with audio in the meanwhile.\r\n> \r\n> If it was a private project, I would be happy to share, but the company I work for doesn't want to do so. But I can say, in the end I implemented a small framework for inference in C/C++ from scratch, because I didn't find any suitable project that would support pulling in TF models. Ours implements the layers that we need, supporting the features that we need. Another guy who loves doing the assembly part took care of optimizing the bottleneck functions and implementing the STFT. Our framework produces the exact same results as TF (except floating point arithmetic related deviations). Overall it wasn't such a big deal (implementing the training would have been) since we only need a subset, and it was fun. I can also recommend this for educational purposes. ;-)\r\n> \r\n> I think it's not that uncommon that people implement their own thing for deployment when it comes to DSPs. The difference probably is that not so many implement it as a generic framework.\r\n> \r\n> @jdduke Our decision was made about a year ago, so I hope I can recall this precisely. In first place, it was binary size. If I remember well, TF Lite resulted in something in the order of magnitude of 50 MB and for TF Lite Micro, it was around 5 MB I think. Maybe there were also some dependencies involved for TF Lite. TF Lite Micro was pretty much what we were looking for for our needs (slim, minimalistic, minimum overhead, rather easy to extend). So one could say everything that comes on top with TF Lite compared to TF Lite Micro was too much for our needs.\r\n> \r\n> I also filed a ticket regarding RNN support in TF Lite Micro in February 2020 (#36688), which is still open, and I haven't received any feedback until today. So I spent some time figuring out if it could be an option to implement the missing RNN functionality into TF Lite Micro myself. What killed it in the end for me was the TF Lite model conversion, which is mandatory for using TF Lite Micro. There were four variants of converting an RNN with the TF Lite converter, all of them lacking different functionality on the TF Lite Micro side; the above-mentioned ticket tensorflow/tflite-micro#907 explains in detail.\r\n> \r\n> In the end, this model conversion was just too confusing, the flag `experimental_new_converter` added some uncertainty, and the idea was basically, if we have to bypass that conversion and implement our own conversion + implementing RNN support in TF Lite Micro, we can as well just implement the whole thing and tailor it exactly to our needs. Our binary size is well < 1 MB by means of the above-mentioned comparison and we have minimal latency. We use dynamic memory allocation though (in contrast to TF Lite Micro).\r\n> \r\n> And we didn't care that much about the STFT for productive use, since we wanted to implement that ourselves anyways.\r\n> \r\n> So please don't get me wrong, it was a rational deployment decision, and we are extremely happy with and thankful for being able to use TF for training and prototyping!\r\n\r\n@padoremu Veeeeeeery smooth discussion, we are using .pb for feature extraction and .tflite for further inference in the end. I have been following the Fourier Transformation features in tf for a long time and finally it seems to come to an end. I was as well a CV guy and working on audios and NLP projects now, a more mathematical noob rather than an engineer guy, so do you mind me having your email? Some discussion is always more than welcome, you can hit me via sherkfung@gmail.com. ", "@jdduke Thanks for your feedback!\r\n\r\n> Hmm, for ARM64 builds, TFLite is ~150KB for the base runtime, and ~2MB for all builtin ops. I suppose an unstripped, debug build could weigh in at 50MB, or if you were using the target that links in select TF ops.\r\n\r\nHmm, it was an x86/x64 build. I remember that I created my own cmake file, trying to find the minimal subset that would link. I suppose you are right that I didn't get that perfectly right, maybe it was with -g. I don't remember regarding select TF ops. Thanks for the hint.\r\n\r\n> Fair enough. I'll follow up on the TFLite Micro ticket for RNN, as RNN support is on our radar for 2021.\r\n\r\nThank you, sounds great. I will definitely follow that!\r\n", " Currently TFLite supports RFFT, STFT supports is implicitly since TF basically uses RFFT to implements STFT.", "> Currently TFLite supports RFFT, STFT supports is implicitly since TF basically uses RFFT to implements STFT.\r\n\r\ncan you give more details? I need stft and istft for Android deployment", "\r\n\r\n\r\n\r\n> Currently TFLite supports RFFT, STFT supports is implicitly since TF basically uses RFFT to implements STFT.\r\n\r\nMy model using STFT still fail at inference with the following error:\r\ntensorflow/lite/kernels/reshape.cc:58 stretch_dim != -1 (0 != -1)Node number 26 (RESHAPE) failed to prepare.\r\nI filed an issue with more details here: https://github.com/tensorflow/tflite-support/issues/352 but got no answer.\r\n\r\nAny news on this? Thanks a lot!", "Waiting for the same problem as Pierre Motard..", "The current issue seems to be that stft itself is not supported because tflite uses different functions to do the DFT compared to full tensorflow. You have to currently do a work around by manually using rfft and frame. This also doesn't return the phase, which probably isn't something most people need, but would be nice to have. ", "> @dkashkin Thanks for the hint. There you go: [public gist](https://gist.github.com/padoremu/8288b47ce76e9530eb288d4eec2e0b4d)\r\n> \r\n> I haven't compared the output to that of the TF functions yet though. The file also contains other test functions for problems with TF / TFLite in the context of audio processing, as described in the file header.\r\n\r\nThanks @Path-A so stft is still not supported. I will use this gist from padomeru who manually wrote everything (it looks like including rfft and frame) but you are saying that now tf.rfft and tf.frame are supported right now? So it reduces the amount to write by hand which is less optimized.\r\n\r\nI have just seen on tensorflow's implementation of stft (https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/ops/signal/spectral_ops.py#L38-L96) that it only uses frame and rfft, so I don't see why it would work to myself rewrite the stft using these tensorflow operations rfft and frame.", "@renjie-liu / @jdduke: are there any plans to support RFFT/STFT in TF Lite with dynamic input? Currently it is not possible (the issue mentioned by @pierremotard 4 posts above is the example of this impossibility).", "If anyone's still interested, I have a tensorflow lite friendly (no select ops required) MFCC layer implemented [here](https://github.com/shahruk10/kaldi-tflite/blob/master/kaldi_tflite/lib/layers/dsp/mfcc.py). See the [README](https://github.com/shahruk10/kaldi-tflite#creating-a-feature-extraction-model) for usage.\r\n\r\n- You can use variable length input but would obviously need to reallocate tensor buffers in Tensorflow lite.\r\n\r\n- The computation is based on [Kaldi](https://github.com/kaldi-asr/kaldi). The output of the layer should be the same as the output of equivalent Kaldi binary, [`compute-mfcc-feats`](https://github.com/kaldi-asr/kaldi/blob/master/src/featbin/compute-mfcc-feats.cc). "]}, {"number": 27298, "title": "Tensorflow 2.0 tf.name_scope has no effect on weights created by keras functional api", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.7.1\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ntf.name_scope does not effect the name of weights created by keras.layers\r\n\r\noutput in tf 2.0.0-alpha0 \r\n```\r\ndense/kernel:0\r\ndense/bias:0\r\n```\r\n\r\n**Describe the expected behavior**\r\ntf.name_scope is applied to wights created by keras.layers\r\noutput in tf 1.13.1 is the expected behavior\r\n```\r\nblock/dense/kernel:0\r\nblock/dense/bias:0\r\n```\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\ninputs = tf.keras.Input(shape=[2])\r\nwith tf.name_scope('block'):\r\n    outputs = tf.keras.layers.Dense(10)(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nfor w in model.weights:\r\n    print(w.name)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI was able to reproduce the output of tf 1.13 in tf 2.0.0 by using the following code\r\n```\r\nfrom tensorflow.python.keras.backend import get_graph\r\nwith get_graph().as_default(), tf.name_scope('block'):\r\n    outputs = tf.keras.layers.Dense(10)(inputs)\r\n```\r\nkeras.layers is using the `keras graph` which overrides the name_scope generated by   tf.name_scope in the `default graph`\r\n\r\nI think the problem is actually caused by tf.name_scope incorrectly set `_has_symbolic_input_in_eager` to False when we are building graph using the keras functional api. since the input here is keras.Input, the name_scope should be applied to keras graph instead of default graph\r\n", "comments": ["In TF 2.0 the Eager Execution is set as **Default**, therefore in eager execetion the TF completely **ignores** the **name_scope**. I think this ops is particularly designed for Graphs.\r\n\r\nTherefore you can do all the above things when Graphs are set as Default\r\n```python\r\nwith tf.Graph().as_default():  \r\n  inputs = tf.keras.Input(shape=[2])\r\n  with tf.name_scope('block'):\r\n      outputs = tf.keras.layers.Dense(10)(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nfor w in model.weights:\r\n    print(w.name)\r\n```\r\n", "I would disagree here, in a real eager execution scenario, keras layers does work with tf.name_scope. \r\n```\r\ninputs = np.random.random((1, 2))\r\nwith tf.name_scope('block'):\r\n    dense = tf.keras.layers.Dense(10, input_shape=(2, ))\r\n    outputs = dense(inputs)\r\nfor w in dense.weights:\r\n    print(w.name)\r\n```\r\noutput:\r\n```\r\nblock/dense/kernel:0\r\nblock/dense/bias:0\r\n```\r\n\r\nI don't think keras should have inconsistent behavior when the input type change from numerical to symbolic.  tf.name_scope v1 used to take a `variable` parameter for this exact case, but it's removed in tf2.\r\n\r\nIn fact, an old [keras tutorial](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html) specifically mentioned tf.name_scope is compatible with the functional api which is no longer true in tf2.\r\n\r\nI don't know if this is indeed the expected outcome, but this definitely needs more clarification since name_scope is pretty essential for building complex models.\r\n", "Maybe you're right. Lets see what does the Member of Tensorflow have to say about this.", "Please do not hesitate to add something to my Stackoverflow issue [TensorFlow 2.0: how to group graph using tf.keras? tf.name_scope/tf.variable_scope not used anymore?](https://stackoverflow.com/questions/55318952/tensorflow-2-0-how-to-group-graph-using-tf-keras-tf-name-scope-tf-variable-sco/55376814?noredirect=1#comment97651032_55376814)\r\n\r\nHowever, I also think this is actually a bug. `tf.Variable` works as intended:\r\n```\r\nIn [3]: with tf.name_scope(\"foo\"): \r\n   ...:     with tf.name_scope(\"bar\"): \r\n   ...:         v = tf.Variable([0])\r\n\r\nIn [4]: v                                                                                                                                                                                                           \r\nOut[4]: <tf.Variable 'foo/bar/Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)>\r\n```\r\nThe variable is placed within the scope `foo/bar`.\r\n\r\nEager execution, as already demonstrated, also works using `tf.keras.layers`...\r\n```\r\nIn [5]: inputs = np.random.rand(10,1) \r\n    ...: with tf.name_scope(\"foo\"): \r\n    ...:     with tf.name_scope(\"bar\"): \r\n    ...:         dense_layer = tf.keras.layers.Dense(1) \r\n    ...:         dense_output = dense_layer(inputs)\r\n\r\nIn [6]: dense_layer.weights                                                                                                                                                                                        \r\nOut[6]:\r\n[<tf.Variable 'foo/bar/dense_1/kernel:0' shape=(1, 1) dtype=float64, numpy=array([[1.40278891]])>,\r\n <tf.Variable 'foo/bar/dense_1/bias:0' shape=(1,) dtype=float64, numpy=array([0.])>]\r\n```\r\n\r\n...while simply building a model with `tf.keras.layers` completeley ignores `tf.name_scope`:\r\n```\r\nIn [7]: with tf.name_scope(\"foo\"): \r\n    ...:     with tf.name_scope(\"bar\"): \r\n    ...:         inputs = tf.keras.Input(shape=(1,)) \r\n    ...:         dense_layer = tf.keras.layers.Dense(1)(inputs) \r\n    ...:         model = tf.keras.Model(inputs=inputs, outputs=dense_layer) \r\n    ...:         model.compile(optimizer=tf.optimizers.SGD(), loss=tf.losses.MeanSquaredError())\r\n\r\nIn [8]: for l in model.layers: \r\n    ...:     for w in l.weights: \r\n    ...:         print(w.name) \r\n    ...:                                                                                                                                                                                                            \r\nOut[8]: \r\ndense_2/kernel:0\r\ndense_2/bias:0\r\n```\r\n", "^ name_scope is definitely not working very nicely in tf2, throwing in some of my findings here.\r\n\r\nIn eager mode with symbolic input, tf.name_scope pushes scope into default context here https://github.com/tensorflow/tensorflow/blob/6a603d8cf6df64cb9bd7bbac33245a02821cf198/tensorflow/python/framework/ops.py#L6400\r\n\r\nIn keras, when layers gets call with symbolic input, it tries to build a graph and replace the current context with the keras graph here https://github.com/tensorflow/tensorflow/blob/6a603d8cf6df64cb9bd7bbac33245a02821cf198/tensorflow/python/keras/engine/base_layer.py#L580\r\n\r\nHowever, if the input is numpy, `__call__` follows this code path \r\nhttps://github.com/tensorflow/tensorflow/blob/6a603d8cf6df64cb9bd7bbac33245a02821cf198/tensorflow/python/keras/engine/base_layer.py#L644\r\n\r\nwhich correctly inherits the parent tf.name_scope\r\n\r\n\r\nThe weirdness happens because tf.name_scope is not in the same graph as keras model. I think the best solution might be providing tf.name_scope an extra parameter (like it did before) to specify the input type. But if there is a more TFonic way to merge scopes without doing it explicitly that be great.\r\n", "I'm seeing this too ... does anyone know in what direction this is moving?", "I could reproduce the issue in `tf-nightly-2.0-preview`. Please take a look at the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/d517caf07e65b06cb4209c55e1de0025/tf27298.ipynb). Thanks!", "@zzh8829 @rchao are there any updates? Any plans to fix the `name_scope` integration?", "I found the name scope worked correctly if I additionally wrapped the `with tf.name_scope(...` with `with tensorflow.python.keras.backend.get_graph().as_default():`. This was sort of mirroring this line: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6a603d8cf6df64cb9bd7bbac33245a02821cf198/tensorflow/python/keras/engine/base_layer.py#L580\r\n\r\nMy guess was that the name scopes were specific to eager mode vs graph-based mode, and the normal Keras model construction forced the graph based mode - so you want to specify the name scope in graph mode. This is a very vague guess, and I don't know if this causes other issues.\r\n\r\nUsing the normal `tensorflow.keras.backend` also didn't work - I had to use `tensorflow.python.keras.backend`.", "I find similar problem, and here is one solution for name_scope control with Sequential rather than tf.name_scope for tf 2.0 keras\uff1a\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\ninputs = tf.keras.Input(shape=[2,10])\r\ndense = tf.keras.layers.Dense(10)\r\nseq = tf.keras.Sequential(name='block')\r\nseq.add(dense)\r\noutputs = seq(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nfor w in model.weights:\r\n    print(w.name)\r\n\r\n2.0.0\r\nblock/dense/kernel:0\r\nblock/dense/bias:0\r\n\r\n``` ", "It works as expected inside the 'build' phase. You're in a functional graph and name_space stack there.", "@jvishnuvardhan @zzh8829 any updates? @rchao does not seem to be active in this issue.", "@zzh8829 I think this was resolved in recent `tf-nightly`. I could not reproduce with `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/1ece1f2942cfb0aadbd5a72e693e867d/untitled56.ipynb) is the colab gist for your reference.\r\n\r\nPlease verify once and if this was resolved for you then close the issue. Thanks!", "@zzh8829 Can you verify once and close the issue if this was resolved for you. Thanks!", "@zzh8829 I am closing this issue as this was resolved in recent `tf-nightly`. Please feel free to reopen if this issue was not resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27298\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27298\">No</a>\n", "@jvishnuvardhan this issue not resolved in `2.2.0`, including latest nightly.\r\n\r\nPlease see this gist https://gist.github.com/Daniel451/03d173825235369be46b27e06e5e109a", "@Daniel451 Can you please open a new issue with your standalone code. I think the second example in your code uses graph context and providing scope name works well as shown below. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/79327bcdc98a09d3f72afcec18d076c7/untitled109.ipynb).\r\n```\r\nwith tf.name_scope(\"foo2\"): \r\n     with tf.name_scope(\"block1\") as scope: \r\n         inputs = tf.keras.Input(shape=(1,)) \r\n         dense_layer = tf.keras.layers.Dense(1,name=scope)(inputs) \r\n         model = tf.keras.Model(inputs=inputs, outputs=dense_layer) \r\n         model.compile(optimizer=tf.optimizers.SGD(), loss=tf.losses.MeanSquaredError())\r\n\r\nfor l in model.layers:\r\n  print(l.name)\r\n  for w in l.weights: \r\n    print(w.name)\r\n```\r\nOutput is \r\n\r\n```\r\ninput_2\r\nfoo2/block1/\r\nfoo2/block1/kernel:0\r\nfoo2/block1/bias:0\r\n```\r\nPlease open a new issue for further question on this issue. Thanks!", "@jvishnuvardhan I can't help but notice that the `tf.name_scope` is a no-op in that example.  The following gives the same output:\r\n\r\n```\r\nscope = 'foo2/block1'\r\ninputs = tf.keras.Input(shape=(1,)) \r\ndense_layer = tf.keras.layers.Dense(1,name=scope)(inputs) \r\nmodel = tf.keras.Model(inputs=inputs, outputs=dense_layer) \r\nmodel.compile(optimizer=tf.optimizers.SGD(), loss=tf.losses.MeanSquaredError())\r\n\r\nfor l in model.layers:\r\n\tprint(l.name)\r\n\tfor w in l.weights: \r\n\tprint(w.name)\r\n```\r\n\r\nWhat we want is for `tf.name_scope` to work, not a tutorial on how to work around it by manually calculating values for `name=`", "How is this still not resolved in tf 2.3.0?", "@tgsmith61591 I have been able to use it since tf 2.1\r\nHave you put it under a `with` block? Like\r\n\r\n```\r\nclass Douche(tf.keras.Model):\r\n    def __init__(self,out_labels):\r\n        self.shitty = tf.keras.layers.Dense(4)\r\n        self.wok = tf.keras.layers.Dense(out_labels)\r\n    def call(x):\r\n        with tf.name_scope(\"Turd\"):\r\n            x = self.shitty(x)\r\n        with tf.name_scope(\"Sandwich\"):\r\n            x = self.wok(x)\r\n        return x\r\n```", "@nicolasshu this is **not** what this issue is about. The problem persists during graph mode / when constructing the model.\r\n\r\nYour example is just another variation of @jvishnuvardhan demonstration that given an actual call / eager mode `name_scope` is inferred correctly. However, as @phsyron already stated, the issue persists for graph mode / model construction -> your example only works, because you subclass `tf.keras.Model` and infer `name_scope` in `call`, i.e. *during* runtime with actual inputs `x` given. We were talking about correctly inferring `name_scope` during model construction and only once as well as a correct inferring during graph mode.", "@jvishnuvardhan I actually lost track of this issue, but it still persists. If opening a new issue is necessary I'll do this and supply a standalone Gist soon. Thank you for your help & time.", "@Daniel451 If you don't mind, please open a new issue as there are many comments above to follow the actual problem. In the new issue, you can refer this issue for further details. Thanks! ", "I was wondering how `tf.keras.layers.Layer` is always able to append the prefix to all the operations inside while `tf.name_scope` doesn't work for `tf.keras.Input`. Looks like the key lies in [here](https://github.com/tensorflow/tensorflow/blob/da6568a/tensorflow/python/keras/engine/base_layer.py#L875-L878).", "Reopened it as this is still an issue with `tf-nightly` 2.9.0-dev20220222. Thanks!"]}, {"number": 27266, "title": "Incorrect flops calculation for operations with complex numbers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): via Anaconda\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Code to reproduce the issue**\r\n```\r\ntf.reset_default_graph()\r\nx = tf.constant(2 + 3j, dtype='complex64')\r\ny = tf.constant(3 + 2j, dtype='complex64')\r\nz = x * y\r\n\r\nsession = tf.Session()\r\nsession.run(tf.global_variables_initializer())\r\n\r\nrun_meta = tf.RunMetadata()\r\nopts = tf.profiler.ProfileOptionBuilder.float_operation()    \r\nflops = tf.profiler.profile(run_meta=run_meta, cmd='op', options=opts) \r\nprint(flops.total_float_ops) \r\n```\r\nThe output is:\r\n```\r\n1\r\n```\r\n\r\n**Describe the current behavior**\r\nMultiplication of complex numbers typically requires 6 floating point operations (4 real multiplications and 2 real additions) and not just 1 floating point operation. The [functions that calculate the flops statistics](https://github.com/tensorflow/tensorflow/blob/f43d458a318d4d97298710654f1692f6e8364f82/tensorflow/python/profiler/internal/flops_registry.py) do not seem to distinguish between floats and complex numbers.\r\n\r\n**Expected behavior**\r\nFor complex numbers, addition and multiplication should require 2 and 6 flops (with real numbers) respectively.\r\n\r\n", "comments": ["I ended up customizing the flops registry of tensorflow to account for complex numbers:\r\n\r\nhttps://github.com/saugatkandel/second-order-phase-retrieval/blob/master/benchmarks/ops/tensorflow/flops_registry_custom.py", "@saugatkandel the link in your post (flops_registry_custom.py) throws 404 error. Thanks!", "@saugatkandel Thank you for reporting the issue! If you have already fixed this in your branch, please consider opening a PR so we can merge it in.", "@saugatkandel Correct location of your code\r\nhttps://github.com/saugatkandel/second-order-phase-retrieval/blob/master/sopt/benchmarks/ops/tensorflow/flops_registry_custom.py\r\nThanks!", "@jvishnuvardhan  Yes, that is the code  - thanks!\r\n\r\n@penpornk Since I am modifying the RegisterStatistics class from tensorflow.python.framework.ops in addition to the the flops registry, I am not sure if a PR is appropriate?", "Can we close this?", "@saugatkandel, \r\n\r\nCan you please respond to the above comment.If you think PR is not appropriate, can you please let us know how you want this issue to be resolved. Thanks!\r\n", "For my work, I used some custom code to account for complex64 numbers:\r\n\r\nhttps://github.com/saugatkandel/sopt/blob/master/sopt/benchmarks/ops/tensorflow/flops_registry_custom.py\r\n\r\nNote that my \"fix\" only applies to complex64 numbers (not complex128), and is intended for TensorFlow 1.x code.  This is quite limited and is not a full fix for the issue at hand. \r\n\r\nSince I am no longer working on this project, I will not be able to make the appropriate changes and submit a PR. As such, someone at TensorFlow might have to make the appropriate fixes."]}, {"number": 27223, "title": "No way to use string type with Lite C APIs.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10, Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: a26413ef0a179dd93b5228b031de83dce97a8684\r\n- Python version: 3.7 but I'm writing TfLite with C\r\n- Installed using virtualenv? pip? conda?: see above\r\n- Bazel version (if compiling from source): see above\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to implement smartreply using go-tflite. Most of features can be implemented with C APIs. But no way to manipulate dynamic-buffer. Do you have plan to add APIs for dynamic-buffers?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nNo way to explain to reproduce.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "Updated", "Thanks for flagging. Yes, we're still finalizing both the C++/C APIs for string tensor interop. This is definitely on our radar.", "If the API will be implemented, I want to specify allocator type since the buffer might be passed from the external.", "@karimnosseir can you follow up on this one?"]}, {"number": 27107, "title": "Implement WALS matrix factorization in Tensorflow 2.0", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0-master\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nImplement tf.contrib.factorization.WALSMatrixFactorization and tf.contrib.factorization.WALSModel in Tensorflow 2.0 (ideally in a distributed manner)\r\n\r\n**Will this change the current api? How?**\r\nMost likely.\r\nThe current WALSMatrixFactorization and WALSModel lie under tf.contrib.factorization, which contains some other methods, in particular clustering ones, such as KMeans or GMM.\r\nThe factorization itself makes sense under tf.linalg instead (the other methods need further assesment) while the estimator may be somewhere.\r\nIn other words, the factorization would probably benefit from being decoupled from the model implementation.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using the current factorization who wants (or needs) to upgrade to Tensorflow 2.0.\r\n\r\n\r\n**Any Other info.**\r\nThere is an open issue related to WALS that could be of interest to take into account if this feature is considered: https://github.com/tensorflow/tensorflow/issues/21991", "comments": ["This is an excellent recommendation (no pun intended), and we would welcome the contribution! :slightly_smiling_face: \r\n\r\nIf you are looking for a template for WALS implementation, the Google Cloud Platform team has a TensorFlow 1.x example located [here](https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals). Let us know if you have any questions!", "@dynamicwebpaige semi-related (also working on the documentation of this), but I have a GSoC proposal for this...\r\n\r\nregarding the template code, that is just a simple wrapper around `WALSModel` and has zero indication regarding distributed training / input matrix sharding.\r\n\r\nin issues [#26298](https://github.com/tensorflow/tensorflow/issues/26928) @walidk   links to some [test code](https://github.com/tensorflow/tensorflow/blob/f07558116ac7c90858cf0572a1bca1e50e208a37/tensorflow/contrib/factorization/python/ops/wals_test.py#L141) which does...\r\n\r\nhowever, the test code uses `WALSMatrixFactorization` which is the `tf.estimator` implementation of `WALSModel`, where sharding still needs to be done by the user.\r\n\r\nSo it brings me to my question regarding the title of this issue. \r\n\r\nAccording the `tf.contrib` the `factorization` submodule has an uncertain future. Perhaps parts will be integrated into tf properly but what parts is not clear (at least from the [sunset table](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md))\r\n\r\n> \r\n```\r\nfactorization | agarwal-ashish | delete (rebuild in core pending)?\r\n```\r\n\r\nSo there are actually a few issues that need to be addressed:\r\n\r\n1. fate of factorization (`WALSModel` inclusive) \r\n\r\n2. whether or not, in TF 2.0 `WALSMatrixFactorization` (a `tf.estimator`) should be the main interface opened to users (which is not the case in linked template). Notably, `tf.estimator` is more or less dead. Technically premade estimator's will still be around, but active support is not and I doubt we will be seeing them whenever tf 3.0 comes around.\r\n\r\n3. if `tf.contrib.factorization` migrates to `tf.factorization`, whether or not the OP (@wileeam ) means simple making an example (which I also working on ) or if the OP meant refactor of `tf.contrib.factorization` for tf 2.0 as needed\r\n\r\n\r\n\r\n@agarwal-ashish  perhaps you can lend some insight regarding the fate of `tf.contrib.factorization` and / or guidance for distributed / shared training\r\n", "@agarwal-ashish  @walidk @dynamicwebpaige  @wileeam any updates?", "Status?", "Any update?", "@SumNeuron I haven't had the time to actually look further into this unfortunately.\r\nOn the other hand it seems that NCF is now favoured in Tensorflow 2 instead of WALS for recommendations as stated in the official models repository: https://github.com/tensorflow/models/tree/master/official/recommendation.\r\n\r\nHowever, Tensorflow would benefit from some knowledge sharing from Google after the recent release (in beta though) of this (W)ALS algorithm in their platform (BigQuery ML): https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-matrix-factorization\r\nI still believe (W)ALS matrix factorization is relevant, particularly in large-scale scenarios. Although this could open the pandora box of matrix factorization methods.", "Factorization, similar to other contrib packages, is not being maintained as part of core TensorFlow. However I do believe that that WALS is a very important algorithm and I would love to see it being driven by some active community. \r\nFWIW, @walidk has done extensive studies on WALS and related algorithms, including neural ones, and can shed more light on how well it performs.", "I agree, WALS remains an important algorithm in recommender systems, and the state of the art for some problems.\r\n\r\nOn the topic of MLP-learned similarity (such as NCF) Vs. dot product similarity (such as matrix factorization or two-tower models): I would recommend trying a dot product model by default instead of NCF. It is unclear whether NCF performs as well in practice as dot product models, see for example the following study: https://arxiv.org/pdf/1911.07698.pdf", "I have implemented the WALS low rank factorization in tensorflow 2.0 and got some great results from it. Here is the link to the repository https://github.com/ravgupta11/WALS-matrix-factorisastion-tensorflow-2.0.git . It is multithreaded in calculation of latent factors using vectorized map ( https://www.tensorflow.org/api_docs/python/tf/vectorized_map )."]}, {"number": 27093, "title": "Export user_data from TfLiteContext", "body": "**System information**\r\n- TensorFlow version (you are using): 13fe6ef76e9f882584fe63a2b7f292266dedf847\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThinking binding of Python or Java, no way to map TfLiteContext to Python/Java objcet. Current implementation of TfLiteRegistration does not have user_data to store object that can be used for mapping Python or Java. So we can't implement custom op with Python or Java.\r\n\r\n```cpp\r\n  static TfLiteRegistration registration = {\r\n      .init = nullptr,\r\n      .free = nullptr,\r\n      .prepare =\r\n          [](TfLiteContext* context, TfLiteNode* node) {\r\n              // call Python/Java\r\n             //PyObject* obj = context->user_data;\r\n          }\r\n   };\r\n```\r\nIf TfLiteContext have user_data (typed void*), we can call Python or Java in the function (init/free/prepare/...).\r\n\r\n**Will this change the current api? How?**\r\n\r\nSo I suggest to add field user_data into TfLiteContext and TfLiteRegistration.\r\n\r\n```cpp\r\nregistration.user_data = GetMyUserData();\r\n```\r\n```cpp\r\nMyUserData* my_user_data = (MyUserData*)context->user_data;\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThe authors of binding for Python/Java or [Me](https://github.com/mattn/go-tflite)\r\n\r\n**Any Other info.**\r\n\r\nHowever, we need more APIs to implement custom op which is implemened with C.\r\n\r\nex: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/models/smartreply/ops", "comments": ["This is a patch I expected.\r\n```diff\r\ndiff --git a/tensorflow/lite/c/c_api_internal.h b/tensorflow/lite/c/c_api_internal.h\r\nindex 4cffd2927d..12f08328dd 100644\r\n--- a/tensorflow/lite/c/c_api_internal.h\r\n+++ b/tensorflow/lite/c/c_api_internal.h\r\n@@ -468,6 +468,10 @@ typedef struct TfLiteContext {\r\n \r\n   // Pointer to the op-level profiler, if set; nullptr otherwise.\r\n   void* profiler;\r\n+\r\n+  // Pointer to the `user_data`. This is used to be passed extra data from\r\n+  // TfLiteRegistration.\r\n+  void* user_data;\r\n } TfLiteContext;\r\n \r\n typedef struct _TfLiteRegistration {\r\n@@ -527,6 +531,10 @@ typedef struct _TfLiteRegistration {\r\n   // Note: It is the responsibility of the registration binder to set this\r\n   // properly.\r\n   int version;\r\n+\r\n+  // Pointer to the `user_data`. This is used to pass extra data to\r\n+  // TfLiteContext.\r\n+  void* user_data;\r\n } TfLiteRegistration;\r\n \r\n // The flags used in `TfLiteDelegate`. Note that this is a bitmask, so the\r\ndiff --git a/tensorflow/lite/core/subgraph.h b/tensorflow/lite/core/subgraph.h\r\nindex e336140c12..79dc822c9e 100644\r\n--- a/tensorflow/lite/core/subgraph.h\r\n+++ b/tensorflow/lite/core/subgraph.h\r\n@@ -304,6 +304,7 @@ class Subgraph {\r\n   void* OpInit(const TfLiteRegistration& op_reg, const char* buffer,\r\n                size_t length) {\r\n     if (op_reg.init == nullptr) return nullptr;\r\n+    context_->user_data = op_reg.user_data;\r\n     return op_reg.init(context_, buffer, length);\r\n   }\r\n \r\n@@ -311,6 +312,7 @@ class Subgraph {\r\n   void OpFree(const TfLiteRegistration& op_reg, void* buffer) {\r\n     if (op_reg.free == nullptr) return;\r\n     if (buffer) {\r\n+      context_->user_data = op_reg.user_data;\r\n       op_reg.free(context_, buffer);\r\n     }\r\n   }\r\n@@ -318,12 +320,14 @@ class Subgraph {\r\n   // Prepare the given 'node' for execution.\r\n   TfLiteStatus OpPrepare(const TfLiteRegistration& op_reg, TfLiteNode* node) {\r\n     if (op_reg.prepare == nullptr) return kTfLiteOk;\r\n+    context_->user_data = op_reg.user_data;\r\n     return op_reg.prepare(context_, node);\r\n   }\r\n \r\n   // Invoke the operator represented by 'node'.\r\n   TfLiteStatus OpInvoke(const TfLiteRegistration& op_reg, TfLiteNode* node) {\r\n     if (op_reg.invoke == nullptr) return kTfLiteError;\r\n+    context_->user_data = op_reg.user_data;\r\n     return op_reg.invoke(context_, node);\r\n   }\r\n \r\n```\r\n", "Thanks for the patch, @miaout17, could you ptal?"]}, {"number": 27047, "title": "tf.train.BytesList should accept bytearray as an input type", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): I don't know how to write a *.proto file, so no\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIf something is called a `BytesList`, then I should be allowed to pass in a `bytearray` to it\r\nCurrently I cannot.\r\nFor example:\r\n```\r\nimport tensorflow as tf\r\nimg = bytearray([1, 2, 3, 4])\r\nprint(img)\r\nfeat = tf.train.Feature(bytes_list=tf.train.BytesList(value=[img]))\r\n```\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will make the tf.train.BytesList class more intuitive to use\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone trying to do image recognition\r\n\r\n**Any Other info.**\r\n", "comments": ["The argument **value** accepts list of bytes as an argument, **NOT** bytearray in ``tf.train.BytesList``.\r\n\r\nSo here you can implement the above code -\r\n```python\r\nimport tensorflow as tf\r\nimg = bytes([1,2,3,4])\r\nprint(img)\r\nfeat = tf.train.Feature(bytes_list=tf.train.BytesList(value=[img]))\r\n```", "@iggy12345 Please let us know whether it was resolved or not? If it was not resolved, please describe the feature with more details and context. Provide a use case where the feature is useful. Thanks!", "I would argue that if it accepts bytes(), then it would be intuitive that tensorflow should accept bytearray() as well", "I could reproduce the issue with TF1.12. and TF1.13.1. Thanks!"]}, {"number": 27029, "title": "Make documentation link to C++ code", "body": "**System information**\r\n- TensorFlow version: All\r\n- Doc Link: All the Python API documentation, for example: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose\r\n\r\n**Describe the documentation issue**\r\nThe Python API documentation often points to the Python code (on github) where the operation is defined. For the example, for `tf.nn.conv2d_transpose()`, it links to [this code](https://www.tensorflow.org/code/stable/tensorflow/python/ops/nn_ops.py).\r\n\r\nUnfortunately, most operations are fairly thin wrappers around C++ operations, and since the link from Python to C++ is automatically generated (in this example, it's `gen_nn_ops.conv2d_backprop_input()`), it is not trivial to find the corresponding C++ code (the mapping is in Bazel code, really hard to find). Many people have been bothered by this problem, as you can see by searching for gen_nn_ops on StackOverflow, for example this question: https://stackoverflow.com/questions/41147734/looking-for-source-code-of-from-gen-nn-ops-in-tensorflow\r\n\r\nIt would be great if the documentation could point to both the Python function and the C++ operation. In this case, it would be https://www.tensorflow.org/versions/r2.0/api_docs/cc/class/tensorflow/ops/conv2-d-backprop-input and the source code is in [tensorflow/core/kernels/conv_grad_input_ops.cc](https://github.com/tensorflow/tensorflow/blob/94be8f012aa59730570bf71e6ba7cd2aa432a589/tensorflow/core/kernels/conv_grad_input_ops.cc#L265).\r\n\r\nTo find it, I had to search locally on my computer to find the `gen_nn_ops.py` file, and I found that `gen_nn_ops.conv2d_backprop_input()` just called the `Conv2DBackpropInput` operation. But then I had to go back to github to search for its C++ source code (since the TensorFlow binary does not include it), and it was tricky to find, since the C++ operation is also dynamically registered, so the actual name of the function is `Conv2DCustomBackpropInputOp`. Searching for `\"REGISTER_KERNEL_BUILDER  Conv2DBackpropInput\"` helps.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nI'm not sure where I could contribute this fix.", "comments": []}, {"number": 27009, "title": "How horovod integrate itself into tf distribute strategy?", "body": "Hi,\r\nI read from tensorflow TFC that horovod team is collobrating with tf team upon the HorovodDistributionStrategy.\r\nRFC: https://github.com/tensorflow/community/blob/master/rfcs/20181016-replicator.md#use-case-4-users-who-will-create-new-distributionstrategy-implementations\r\n\r\nCan anyone share with us more details about how to integrate horovod into d.s.? Because there's a lot of difference between them: e.g. launcher mode\r\n\r\nThanks!\r\n", "comments": []}, {"number": 26961, "title": "Dynamic ksize and strides with AvgPool", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n1.11.0\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nYes - but unfamiliar with contribution process and I think it would be very similar to MaxPoolV2\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdding an AvgPoolV2 with dynamic ksize, much like MaxPoolV2. This is very related to issue #11875\r\n\r\n**Will this change the current api? How?**\r\nIt would add a new function gen_nn_ops.avg_pool_v2\r\n\r\n**Who will benefit with this feature?**\r\nAnybody looking to use an average pooling layer with dynamic ksize. For example, when running a graph with an input of an unspecified size when the ksize is dependent on the size of the input\r\n\r\n**Any Other info.**\r\n", "comments": ["Sorry I don't have enough experience here, you need someone who has worked on the ops, I have only touched the python layer on top.", "I can work on this, but re-defining op kernels sounds a very cool thing to do, @davideh29-portfolio do you want to contribute?", "Any update on this?", "We need dynamic stride support for avgpool"]}, {"number": 26938, "title": "How to use tf.embedding_lookup_sparse_with_distributed_aggregation in feature_column&estimator ?", "body": "Hi, I want to use `tf.embedding_lookup_sparse_with_distributed_aggregation`  in my training program. The model is built with tf.estimator and tf.feature_column . \r\n\r\nBut the default is using `safe_embedding_lookup_sparse ` which will cost lots of network traffic.  \r\n\r\n\r\nSo what is the best way to use distributed_aggregation with feature_column ? \r\n\r\n I think one way is to modify tf.feature_colmun source code. but is there any others ?", "comments": ["@ericyue Could you provide more details and a short code to demonstrate the issue? Thanks!", "@jvishnuvardhan   When I use `tf.feature_column.embedding_column` with estimator , it creates embeddings and do embedding-lookup using  `tf. embedding_lookup_sparse `by default,  which will cost lots of network traffic within the parameter-server and workers.  \r\n\r\nSo I want to known is there a elegant way to replace `tf. embedding_lookup_sparse ` with `tf.embedding_lookup_sparse_with_distributed_aggregation` ? \r\n\r\nthe simple demo is `tf.estimator.DNNLinearCombinedClassifier` and with `tf.feature_column.embedding_column`\r\n\r\nhow to optimize the network traffic in this case by useing `tf.embedding_lookup_sparse_with_distributed_aggregation`? \r\n", "@jhseu @jvishnuvardhan Can you give me some advice about this? thanks :)\r\n\r\n", "One option would be to subclass EmbeddingColumn and override the _get_dense_tensor_internal_helper mmethod.", "@ericyue hi, I come up with the same problem like you.  when I train a wide&deep&cross model . use feature_column and embedding_lookup, the network io is very big( about 10Gbps) which have harm to train efficiency. I have find the meituan technical report [\u7f8e\u56e2WDL\u4f18\u5316](https://tech.meituan.com/2018/04/08/tensorflow-performance-bottleneck-analysis-on-hadoop.html) . Did you solve your problem? can you give me some adivces?"]}, {"number": 26873, "title": "Incorrect epoch number in TensorBoard callback when using batch-level metrics", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): `tensorflow==1.13.1` and `tf-nightly==1.14.1.dev20190318` (`v1.12.0-10310-g3db23915df 1.14.1-dev20190318`)\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Describe the current behavior**\r\nWhen collecting batch-level metrics, epochs are incorrectly numbered with the current `_samples_seen`\r\n\r\n**Describe the expected behavior**\r\nWhen collecting batch-level metrics, epochs should be numbered with the epoch number\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow import keras\r\n\r\n\r\nclass MyTensorBoard1(keras.callbacks.TensorBoard):\r\n    def _write_custom_summaries(self, step, logs=None):\r\n        print(logs, \"at step =\", step, \"with regular callback\")\r\n\r\n\r\nclass MyTensorBoard2(keras.callbacks.TensorBoard):\r\n    def _write_custom_summaries(self, step, logs=None):\r\n        print(logs, \"at step =\", step, \"with per-batch callback\")\r\n\r\n\r\nlayer = keras.layers.Input(shape=(1,))\r\nmodel = keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer='adam', loss='mse')\r\nCallback1 = MyTensorBoard1()\r\nCallback2 = MyTensorBoard2(update_freq='batch')\r\nmodel.fit(x=list(range(6)), y=list(range(6)), epochs=10, callbacks=[Callback1, Callback2])\r\n```\r\n\r\nTo run this code in current nightly, remove `Callback1` and focus on output of first epoch. Notice `at step = 6` in first epoch:\r\n\r\n```\r\nEpoch 1/10\r\n{'batch_loss': 0.0} at step = 0 with per-batch callback\r\n{'epoch_loss': 0.0} at step = 6 with per-batch callback\r\n6/6 [==============================] - 0s 15ms/sample - loss: 0.0000e+00\r\n```\r\n\r\n**Other info / logs**\r\nIn my opinion, the bug is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/python/keras/callbacks.py#L1158\r\n\r\nand still here:\r\nhttps://github.com/tensorflow/tensorflow/blob/c66b603990b9404dc1eb57de9d595aa0ffc8197f/tensorflow/python/keras/callbacks.py#L1288\r\n\r\nIn `on_epoch_end`, `step` should always be set to `epoch`.", "comments": ["To explicitly include a thought of mine:\r\n\r\nMy reporting this issue is based on the expectation that `epoch_` metrics should behave the same regardless of how often `batch_` metrics are written, such that `epoch_` metrics are always comparable across runs with different `update_freq`. \r\n\r\nOf course, if `batch_` and `epoch_` metrics are assumed to be interpreted in conjunction for one run, there is a need to know how to interleave the two, in which case I understand the current behavior somewhat. (But then, why does `on_batch_end` use `self._total_batches_seen`, while `on_epoch_end` uses `self._samples_seen`?) In that case, however, TensorBoard should have an option to ignore the x values from summaries - this is the issue I have referenced above this comment.", "@bersbersbers Could you post this in Tensorboard repo [here](https://github.com/tensorflow/tensorboard/issues). They are actively resolving the issues in Tboard repo so    it will get better visibility and faster resolution if you post it there. Close this issue when you post it on tensorboard repo. Thanks!", "@jvishnuvardhan are you certain this a TensorBoard issue? If my guesses above are correct, it's `python/keras/callbacks.py` in the TensorFlow repo which causes the issue. If you certain that TensorBoard is the correct repo to report this issue, I am happy to report it there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26873\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26873\">No</a>\n"]}, {"number": 26842, "title": "SVD handles small singular values poorly", "body": "**System information**\r\n- Have I written custom code: Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on WSL, also Colab\r\n- TensorFlow installed from (source or binary): binary (pip, no GPU)\r\n- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.6.8 (anaconda)\r\n\r\n**The problem**\r\nPerforming an SVD of [this float32 matrix](https://github.com/tensorflow/tensorflow/files/2979211/matrix.zip) using `tf.svd()` (on CPU) results in `NaN` values in the `u` and `v` factors (no `NaN`s in the singular values). Numpy's (MKL's) SVD copes with this matrix fine, and returns many more nonzero singular values than the TF implementation. TF's SVD seems to zero singular values below a threshold. This is fine, assuming these values would have otherwise been inaccurate. However, the NaN's occur in the singular vectors corresponding to the *nonzero* singular values. Note also that not all the singular values are in descending order! This seems like a bug.\r\n\r\n**Describe the expected behavior**\r\nTensorFlow's SVD should be more robust to poorly-conditioned matrices.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.enable_v2_behavior()\r\nM = np.load(\"matrix.npy\")\r\ns_tf, u, v = tf.svd(M)\r\nprint(tf.norm(u).numpy(), tf.norm(v).numpy())  # prints \"nan nan\"\r\n\r\nu, s_np, vh = np.linalg.svd(M)\r\nprint(np.linalg.norm(u), np.linalg.norm(vh))  # prints some numbers\r\n\r\nprint(s_np)\r\nprint(s_tf.numpy())  # smaller singular values are zeroed\r\n```\r\n\r\nI think this is essentially the same as #8905, except that here I am using single precision. The ability of the SVD to handle ill-conditioned matrices is important for our application.\r\n", "comments": ["This is also similar to https://github.com/tensorflow/tensorflow/issues/9234. Actually, the following code\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution() # for tf 1.x\r\na = tf.ones([1000,1000]) #  ones as very ill conditioned matrix\r\ntf.linalg.svd(a) # default device cpu\r\n```\r\nis enough to trigger segfault. This can be reproduced in current default google colab setup (tf1.15) and my local intel optimized tf2.0 enviroment (it is worth noting that svd doesn't call mkl routine even for intel optimized tf, observed by set `export MKL_VERBOSE=1`). SVD op on GPU is ok with no error or nan, though.\r\n\r\nI think it is urgent to shift the implementation of SVD op to ?gesdd instead of ?gesvd, just like what pytorch does https://github.com/pytorch/pytorch/pull/11194. The speed difference is huge as one can benchmark by the following script in jupyter:\r\n\r\n```python\r\ndef svd_time_benchmark(shape):\r\n    a = np.random.uniform(size=shape)\r\n    print(\"numpy\")\r\n    %timeit _ = np.linalg.svd(a)\r\n    print(\"scipy lapack svd\")\r\n    %timeit _ = sp.linalg.lapack.dgesvd(a)\r\n    print(\"scipy lapack sdd\")\r\n    %timeit _ = sp.linalg.lapack.dgesdd(a)\r\n    print(\"tf cpu\")\r\n    a1 = tf.constant(a)\r\n    with tf.device(\"/cpu:0\"):\r\n        %timeit _ = tf.linalg.svd(a1)\r\n    print(\"tf gpu\")\r\n    with tf.device(\"/gpu:0\"):\r\n        %timeit _ = tf.linalg.svd(a1)\r\n    print(\"torch cpu\")\r\n    a2 = torch.Tensor(a,  device=torch.device(\"cpu\")).double()\r\n    %timeit _ = torch.svd(a2)\r\n    print(\"torch gpu\")\r\n    a3 = torch.Tensor(a).cuda().double()\r\n    %timeit _ = torch.svd(a3)\r\n```\r\n\r\nSome typical results for 2 * Xeon 5120 or 1 * 2080TI is the following: (1000*1000 random matrix)\r\n```\r\nnumpy\r\n240 ms \u00b1 3.71 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\nscipy lapack svd\r\n1.14 s \u00b1 23.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\nscipy lapack sdd\r\n227 ms \u00b1 7.79 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\ntf cpu\r\n1.14 s \u00b1 1.21 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\ntf gpu\r\n4.55 s \u00b1 8.83 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\ntorch cpu\r\n223 ms \u00b1 3.04 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\ntorch gpu\r\n280 ms \u00b1 6.17 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nThe GPU implementation of SVD op in tensorflow (provided by cuSolver) is even more terrible than GPU one in pytorch (provided by MAGMA).\r\n\r\nI am talking about two aspects: speed(the computation time) and stableness(the possible segfault or nan) for SVD op above. Obviously, tensorflow seems to be bad at both with current implementation.", "Was able to reproduce the error in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/27082cbbdeac4a85d13bf99439d03137/26842.ipynb).", "Issue still persists in `2.6.0` and `nightly`. Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/e26298a50f49bc631d0af7f91e339ab4/-26842.ipynb) "]}, {"number": 26697, "title": "some tests in //tensorflow/lite/testing/ fail to build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit 6583b83dc9393118c60a9c11f15453c650fab89f\r\n- Python version: 2.7.15rc1\r\n- Installed using virtualenv? pip? conda?: sources\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the problem**\r\nBuild errors occur in zip tests\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel test -k --verbose_failures --config=opt -- //tensorflow/lite/testing/...\r\n\r\n**Any other info / logs**\r\n```\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/testing/BUILD:21:2: Couldn't build file tensorflow/lite/testing/gather.zip: Executing genrule //tensorflow/lite/testing:gather.zip.files failed (Se\r\ngmentation fault): bash failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/lite/testing/generate_examples --toco bazel-out/host/bin/tensorflow/lite/toco/toco  --zip_to_output gathe\r\nr.zip  bazel-out/k8-opt/genfiles/tensorflow/lite/testing'): bash failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/lite/testing/generate_examples --toco bazel-out/host/bin/tensorflow/lite/toco/toco  --zip_to_output gathe\r\nr.zip  bazel-out/k8-opt/genfiles/tensorflow/lite/testing')\r\n2019-03-14 11:48:24.311471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz\r\n2019-03-14 11:48:24.316264: I tensorflow/compiler/xla/service/service.cc:167] XLA service 0x4648a70 executing computations on platform Host. Devices:\r\n2019-03-14 11:48:24.316342: I tensorflow/compiler/xla/service/service.cc:174]   StreamExecutor device (0): <undefined>, <undefined>\r\nW0314 11:48:24.329004 140192886912768 deprecation.py:323] From /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/lite/testing/generate_examples\r\n.runfiles/org_tensorflow/tensorflow/lite/testing/generate_examples.py:511: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.remove_training_nodes`\r\n/bin/bash: line 1:  8879 Segmentation fault      (core dumped) bazel-out/host/bin/tensorflow/lite/testing/generate_examples --toco bazel-out/host/bin/tensorflow/lite/toco/toco --zip_to_output gather.zip bazel-ou\r\nt/k8-opt/genfiles/tensorflow/lite/testing\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/testing/BUILD:21:2: Couldn't build file tensorflow/lite/testing/gather_toco-flex.zip: Executing genrule //tensorflow/lite/testing:gather_toco-flex.\r\nzip.files failed (Segmentation fault): bash failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n```", "comments": []}, {"number": 26671, "title": "googletest.h used in open source project", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit 6583b83dc9393118c60a9c11f15453c650fab89f\r\n- Python version: 2.7.15rc1\r\n- Installed using virtualenv? pip? conda?: sources\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\nDuring compilation I get an error:\r\n```\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/testing/kernel_test/BUILD:81:1: Couldn't build file tensorflow/lite/testing/kernel_test/_objs/input_generator_test/input_generator_test.o: C++ comp\r\nilation of rule '//tensorflow/lite/testing/kernel_test:input_generator_test' failed (Exit 1)\r\ntensorflow/lite/testing/kernel_test/input_generator_test.cc:21:44: fatal error: testing/base/public/googletest.h: No such file or directory\r\ncompilation terminated.\r\n```\r\nI guess that ```googletest.h``` is some internal Google header file which shouldn't be use in open source projects.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel test tensorflow/lite/testing/kernel_test:input_generator_test\r\n\r\n**Any other info / logs**\r\n@liyunlu0618 could you take a look on that?\r\n\r\nOther similar problems from the same commit\r\n- tensorflow/lite/testing/kernel_test/generate_diff_report.cc:\r\n```\r\ntensorflow/lite/testing/kernel_test/generate_diff_report.cc:22:3: error: 'string' was not declared in this scope\r\n   string base, test, output;\r\n```\r\n- tensorflow/lite/testing/kernel_test/util_test.cc\r\n```\r\ntesting/base/public/googletest.h: No such file or directory\r\n```\r\n- tensorflow/lite/testing/kernel_test/diff_analyzer_test.cc:\r\n```googletest.h``` is not used but ```FLAGS_test_tmpdir``` which probably comes from this header is used so it should be removed as well.\r\n\r\n", "comments": ["I have met the same problem when running the test for `tensorflow/lite/testing/kernel_test/util_test.cc`.", "Google Test is a [publicly released open source library](https://github.com/google/googletest) that can be used by any other open source project. I see it is included as depenency in the `util_test.cc` target: https://github.com/tensorflow/tensorflow/blob/656e7a55387fac2df1b786013adbde79cdc95fd5/tensorflow/lite/testing/kernel_test/BUILD#L33-L49\r\n\r\nCan you recompile from scratch and attach the log? Most likely the download of googletest archive didn't succeed and you missed that in the rest of the compilation log.", "@mihaimaruseac I think the error is caused by [#include \"testing/base/public/googletest.h\"](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/testing/kernel_test/util_test.cc#L21). The error message is as below:\r\n```\r\nERROR: /home/abc/tensorflow/tensorflow/lite/testing/kernel_test/BUILD:33:1: C++ compilation of rule '//tensorflow/lite/testing/kernel_test:util_test' failed (Exit 1)\r\ntensorflow/lite/testing/kernel_test/util_test.cc:21:10: fatal error: testing/base/public/googletest.h: No such file or directory\r\n #include \"testing/base/public/googletest.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/lite/testing/kernel_test:util_test failed to build\r\n```\r\nThis PR #33123 is submitted to fix it. Could you please take a look?", "Confirmed that I cannot compile `tensorflow/lite/testing/kernel_test:input_generator_test` although I can compile other targets using Google test. Assigning to lite/ code owners for review."]}, {"number": 26642, "title": "Will LSTMBlockFusedCell be supported in tensorflow 2.0?", "body": "I noticed that keras in tensorflow 2.0 unifies backend between CPU and GPU with CuDNN, but it's a little slow on CPU. Will you add LSTMBlockFusedCell or LSTMBlockCell to tf 2.0, since they have better performance?\r\n", "comments": ["Thanks for checking. I think LSTMBlockFusedCell or LSTMBlockCell won't be in the core API for 2.0. Having said that, I could update the cpu kernel to be a more performant version like LSTMBlockCell. The only down side I can see is that user will loss the python level debugability since it will be one fused op. \r\n\r\nI will discuss the trade off within teams and post it here.", "Many of the good functions from **tf.contrib** are eliminated, such as CRF conditional random field etc how can one use these packages ( by using source code from these packages)? @qlzh727 \r\nany suggestions pls", "@protoget @qlzh727 Any updates on this?", "How can we find the author(s) and see if they are willing to support these two in Addons?", "Sorry for the very late reply. There has been some work to add the fused c kernel to TF, and they haven't be expose to python side yet. I have some other issue to work with at the moment, will take a closer look within this month.", "Thanks @qlzh727 we appreciate your work. \r\nI would be fine if `LSTMBlockFusedCell` and `TimeReversedFusedRNN` move to `addons` or even willing to do the work and add them myself if I can find a simple instruction as to how to move something from `contrib` to `addons`.", "@qlzh727 Thank you for your reply!", "@qlzh727 Since we can benefit from the python level debugability when using keras.LSTMCell and keras.RNN, I think improve the performance of LSTM/GRU with a single fused op is possible.\r\n\r\nIn PyTorch, CuDNN is used for LSTM on GPU, and it will fall back to a fused lstm kernel when using CPU. So is it possible to integrate the LSTMBlockFusedCell into keras.LSTM when CPU is available.\r\n\r\n(https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent_v2.py#L910) vs (https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM)", "I believe the underlying LSTMBlockFusedCell op is already available in TF 2.0 as https://www.tensorflow.org/api_docs/python/tf/raw_ops/BlockLSTM. See https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/rnn/python/ops/lstm_ops.py#L654 for more information on how it is used. The only difference I see is the default cell_clip value is now 3.0. Also, there is a BlockLSTMV2, which is different somehow (https://www.tensorflow.org/api_docs/python/tf/raw_ops/BlockLSTMV2).", "@npuichigo,\r\nCan you please respond to the above comment? Thanks! ", "@rmothukuru Sorry for the late reply. I checked the recent status and it seems LSTMBlockFusedCell is only exposed in raw_ops. LSTMCell in keras still doesn't use a fused kernel, but I don't know whether the performance of LSTMCell with tf.function is on par with a fused kernel.", "Hi @npuichigo!\r\nWe are checking to see whether you still need help in this issue .Have you checked this  [document on LSTMCell](https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/rnn_cell/LSTMCell) from TF 2.7 yet?", "have switched to pytorch for a long time. no need for help currently", "> Hi @npuichigo! We are checking to see whether you still need help in this issue .Have you checked this [document on LSTMCell](https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/rnn_cell/LSTMCell) from TF 2.7 yet?\r\n\r\nI hope I am not missing anything obvious, but doesn't the recommendation in that link (i.e. `Please use ... tf.contrib.rnn.LSTMBlockFusedCell for better performance on CPU`) requires you to not be using tensorflow 2.0+? Why is this the recommendation on the (currently 2.8) version of the documentation when it isn't supported?", "@coleary-hyperscience I always want a LSTM implementation which is a fused kernel on CPU and can seamlessly switch to CuDNN on GPU. The recommendations here don't meet my needs, but PyTorch LSTM does.\r\n\r\nAs for the document, I think it's out of date since there's no more tf.contrib anymore. It was the original motive of this issue, to find an alternative in tensorflow 2.0", "Ok @npuichigo ! Reopening as it still seems to be a valid feature request . Thank you!"]}, {"number": 26627, "title": "Improve tf_compile to allow emitting HLO module instead of/in addition to executable", "body": "Currently tf_compile (tensorflow/compiler/aot) always emits executable. But there are use cases where dumping HLO module is more desired. This is a tracking bug to implement it.", "comments": []}, {"number": 26574, "title": "Is it possible to set the random seed for tf.feature_column.categorical_column_with_hash_bucket on operation-level\uff1f", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.rc1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n API:tf.feature_column.categorical_column_with_hash_bucket can not set random seed\r\n**Will this change the current api? How?** \r\nYes, It will add a new parametre to set the random seed.\r\n**Who will benefit with this feature?**\r\nThe one who want to get two different hash bucket on the same field.\r\n**Any Other info.**\r\n", "comments": ["I think it's a reasonable request. Rohan, what do you think?"]}, {"number": 26498, "title": "Bazel always rebuild all code even I just modify single cpp file when config=cuda is used ", "body": "**System information**\r\n\r\n- TensorFlow version (you are using): master branch\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen I change any file in tensorflow/compiler, bazel always rebuild all code when config=cuda is used, I did not hit the bug after remove config=cuda.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who follow https://www.tensorflow.org/install/source\r\n\r\n**Any Other info.**", "comments": ["Unfortunately, that is the artifact of how TF code and build rules are organized.\r\nMany files are included in many targets, and it will trigger almost a full rebuild.\r\n\r\nModular tensorflow will help with this, but until that lands, I am afraid I cannot really offer a solution to this.", "Would it make sense to delete `config=cuda` from the docs here? Looks like it's not needed?\r\nhttps://www.tensorflow.org/install/source#gpu_support", "No, you can't get rid of that. Bazel is meant to be distributed. If you have some old PCs you can make a small farm or grid to help speed it up. They have clusters and grids and can compile it in minutes.", "@perfinion for recommendations here.\r\nI have some internal hacks I use so I may not be as familiar as him with this issue."]}]