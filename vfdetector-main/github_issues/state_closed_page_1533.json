[{"number": 6935, "title": "Update v5 references in os_setup.md to v5.1", "body": "Fixes #6934", "comments": ["Simple doc change, merging."]}, {"number": 6934, "title": "Download and setup instructions issue, incorrect version of CuDNN", "body": "[Instructions](https://www.tensorflow.org/get_started/os_setup) indicate to use CuDNN 5 with Tensorflow for GPU with CUDA Tookit 8.0 and Python 3.5 under Ubuntu/Linux. After doing that, when using the Tensorflow Python API I get a run-time error:\r\n\r\n`E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.`\r\n\r\nand the computation stops.\r\n\r\nIf instead I use CuDNN 5.1, the computation goes through correctly.\r\n\r\nI have installed Tensorflow 0.12.1 for Python 3.5 with GPU with:\r\n\r\n`sudo -H pip3 install tensorflow-gpu`\r\n\r\nunder Ubuntu 16.04 64-bit.\r\n\r\n```\r\n> python3 -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.12.1\r\n```\r\n\r\n[Here](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb) a program that can be used to reproduce the issue.\r\n\r\nI believe installation instructions should be updated to reference CuDNN 5.1.", "comments": ["The instructions seem to say to use cudnn v5.1 in most places -- are you confused by the one or two places that say 'v5' without qualifying the minor version?", "I had used CuDNN v5 at first based on this:\r\n\r\n```\r\n# Ubuntu/Linux 64-bit, GPU enabled, Python 3.5\r\n# Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see \"Installing from sources\" below.\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl\r\n```\r\n\r\nI didn't notice that elsewhere it says 5.1.", "Sent a PR."]}, {"number": 6933, "title": "cuDevicePrimaryCtxSetFlags not found issue on Windows 7", "body": "I have successfully loaded tensorflow in python 3.5. But when I start a session, it gave me the following error and python will crash:\r\n\r\n>>> import tensorflow as tf\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll\r\nlocally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll lo\r\ncally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll l\r\nocally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll local\r\nly\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll\r\nlocally\r\n>>> sess = tf.Session()\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_driver.cc:94] Check failed: s.ok() could not find cuDevice\r\nPrimaryCtxSetFlags in libcuda DSO; dlerror: cuDevicePrimaryCtxSetFlags not found\r\n\r\nMy graphic card is NVDIA  quadro K1100M with cuda compute capability 3.0. I have cuda toolkit 8.0 and cudnnv5.1 installed. I tried different nvdia drivers but none is working. If the driver is too old, it will give me insufficient version number to be used with cuda. If the drive is new, my computer just freeze and saying the graphic card is not removable or cannot ejected. Could someone help me with this? Thanks.", "comments": ["This questions is probably better asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests.\r\n\r\nClosing this out here.\r\n\r\n(FYI @mrry)"]}, {"number": 6932, "title": "layers.core.Dense doesn't make use of argument activity_regularizer", "body": "As it says on the tin, the ``Dense`` class of ``tensorflow.python.layers.core`` accepts an ``activity_regularizer`` argument, but never makes use of this argument. Not sure if this is intentional or an oversight, so pointing it out.", "comments": ["I believe it does get used by a call in the parent class (see [`python/layers/base.py:270`](https://github.com/tensorflow/tensorflow/blob/1288301651c2e9610202a8657ff569ac4bf5392f/tensorflow/python/layers/base.py#L270)).\r\n\r\nClosing this out. CCing @fchollet in case I have misunderstood and he wants to reopen this."]}, {"number": 6931, "title": "Tensorflow builds on Windows are broken", "body": "The corresponding build projects are red on both Tensorflow CI (http://ci.tensorflow.org/job/tf-master-win-bzl/) and Bazel CI (http://ci.bazel.io/job/TensorFlow/).", "comments": ["It's trying to access an `install` file. Not sure what this could be. @zheng-xq it looks like it broke last week.", "Internally there is a fix, but we have not been able to make a successful push due to other breakages.", "Unfortunately both CIs are still red, I think it should be reopened.", "Sorry, that was an Auto-close due to fix <issue> pattern.\r\n", "Some more fixes in. Now looking into tensorforest issues on GPU.\r\nLooks like they arrived broken.\r\nWill see how I can triage them.", "We have decided to disable contrib tests that were failing for now, as contrib does not have official support.\r\nThey will be disabled until contrib library (tensor_forest) owners triage those bugs separately.\r\n\r\nThis bug now should be fixed. (The remaining issues will be tracked separately)."]}, {"number": 6930, "title": "Is Python 3.3 still supported?", "body": "Docs say Python 3.3 is supported.\r\n\r\nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup says:\r\n\r\n> The TensorFlow Python API supports Python 2.7 and Python 3.3+.\r\n\r\nI'm not too bothered about Python 3.3, but if it's no longer supported, please update the docs.\r\n\r\n\r\n### Environment info\r\nTravis CI\r\nOperating System: Ubuntu 14.04.5 LTS\r\nPython 3.3.6\r\npip 8.1.2\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n```\r\n\r\n", "comments": ["@gunan @yifeif : I believe this is a docs error? We release pip packages for Python 3.4 and 3.5 only, but is the API compatible with 3.3 (built from sources?)", "If you are building from sources, I dont see a reason why it wont work with py3.3\r\nOur binaries are only for py3.4 and py3.5\r\n\r\nHow about this, could you try manually downloading python 3.4 or 3.5 wheel file, rename it by replacing all occurences of \"34\" (or \"35\") in the name to \"33\". Then install that downloaded package.\r\nFor 3.6, users reported that this works, it might work as well for python 3.3", "@hugovk we have uploaded 3.3 binary to pypi. Could you give it another try?", "The 3.3 wheel is pip installing and with a quick test is working fine. Thank you!\r\n\r\n```\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n  Downloading tensorflow-0.12.1-cp33-cp33m-manylinux1_x86_64.whl (43.1MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 43.1MB 33kB/s \r\nCollecting protobuf>=3.1.0 (from tensorflow)\r\n  Downloading protobuf-3.1.0.post1-py2.py3-none-any.whl (347kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348kB 793kB/s \r\nRequirement already satisfied: wheel>=0.26 in /home/travis/virtualenv/python3.3.6/lib/python3.3/site-packages (from tensorflow)\r\nRequirement already satisfied: numpy>=1.11.0 in /home/travis/virtualenv/python3.3.6/lib/python3.3/site-packages (from tensorflow)\r\nRequirement already satisfied: six>=1.10.0 in /home/travis/virtualenv/python3.3.6/lib/python3.3/site-packages (from tensorflow)\r\nRequirement already satisfied: setuptools in /home/travis/virtualenv/python3.3.6/lib/python3.3/site-packages (from protobuf>=3.1.0->tensorflow)\r\nInstalling collected packages: protobuf, tensorflow\r\nSuccessfully installed protobuf-3.1.0.post1 tensorflow-0.12.1\r\n```"]}, {"number": 6929, "title": "Documentation of ctc_beam_search_decoder is inconsistent", "body": "The documentation of ctc_beam_search_decoder describes the `input` argument as:\r\n\r\n    inputs: 3-D `float` `Tensor`, size\r\n        `[max_time x batch_size x num_classes]`.  The logits.\r\n\r\nImplying that they should be the linear projections. However, by inspecting the [unit tests](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py#L96) and the [implementation](https://github.com/tensorflow/tensorflow/blob/679f95e9d8d538c3c02c0da45606bab22a71420e/tensorflow/core/util/ctc/ctc_beam_search.h#L290), it seems these inputs should actually be log-probabilities. In this case, the documentation should read:\r\n\r\n    inputs: 3-D `float` `Tensor`, size\r\n      `[max_time x batch_size x num_classes]`.\r\n    The output symbol log-probabilities.", "comments": ["@ebrevdo : Would you like to add some clarifications?", "@MarkDaoust Could you take a look?", "I don't see a problem here.\r\n\r\nFeel free to correct me if I'm wrong, but \"logits\" seems to be used throughout TensorFlow to refer to  un-normalized log probabilities like these. see [softmax_cross_entropy_with_logits] (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) and friends for example.\r\n\r\nIn the test, [line 191](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py#L191), we see that this exactly what they're passing in. The rows start out summing to 1, then they take the log and add an arbitrary offset.\r\n\r\nI'm not familiar with the implementation details, but the first thing they do in the implementation is subtract the max (for numerical stability)."]}, {"number": 6928, "title": " how to resolve warning: Unable to load cuDNN DSO ", "body": "Hello, \r\nI recently installed tensorflow GPU version 0.12.1 and python 3.5.2 for windows. \r\nThere is CUDA GTX660 driver already installed on my PC.  \r\n\r\nI am trying some sample programs and when ever i am using import tensorflow  I get a warning and then the result. How to resolve the issue ? What else should I be installing ? \r\n\r\nI tried to download cuDNN, select all in cuDNN Dowload Survey will make any difference? I will be working on RNN implementations and have many FFT calculations.\r\nAt the moment, I choose deeplearning framework -> tensorflow library and I have downloaded cuDNN v5.1 Library for Windows 10 which gave me a \"cudnn-8.0-windows10-x64-v5.1.zip\" file , but there is no executable file or something to install.  should I be copying those in to any folder or did i downloaded the wrong one ?\r\n\r\n`C:\\Users\\raady\\AppData\\Local\\Programs\\Python\\Python35\\python.exe \"D:/Lab Project Files/TF/Practice Files/test.py\"\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3459] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\nWARNING:tensorflow:From D:/Lab Project Files/TF/Practice Files/test.py:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 660\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0325\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.65GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0 \r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y \r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:01:00.0)`", "comments": ["You need to unzip and copy the `dll` to a location that is already in your `PATH`. Ideal location would be where the other CUDA dll are installed.", "Thanks for the not @kingtaurus\r\n@raady07 : I'm closing this issue out as such questions are more suited for [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow). We try to keep the github issues focused on bugs and feature requests. Thanks!", "for me the solution was to use 5.1 instead of 6.0"]}, {"number": 6927, "title": "External optimizer with apply_gradients", "body": "Hi all,\r\n\r\nI'm trying to define some new optimization algorithm. I found an interface about external optimizer [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/external_optimizer.py), it says that i need to subclass it and implement _minimize. However I need to call apply_gradients when using the external optimizer. Does anyone know how can i implement it?\r\n\r\nThanks.", "comments": ["Hi, could you please ask this on stackoverflow? We are trying to keep this list for bugs/feature requests, thanks :)", "Sure, thank you for reminding. I put it on [here](http://stackoverflow.com/questions/41724406/how-to-implement-an-external-optimizer-in-tensorflow-with-apply-gradients-method). Hope to get answers :)"]}, {"number": 6926, "title": "Add Maven artifacts to central mvnrepository", "body": "For Java/Scala users, specifically - those using the Spark/Hadoop environment - it would help to have two artifacts:\r\nexample/feature compiled protos\r\n\r\nTFRecordFileFormat for Hadoop/Spark io\r\n", "comments": ["We will be looking into those, particularly for the [Java API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java) and all that is needed for it.\r\n\r\nHowever, might be a while. \r\n\r\nFYI @jhseu ", "Thanks for the uodate.\nI'm willing to contributes some time for this, if this may help\n--franji\n\nOn Wed, Jan 18, 2017, 6:40 PM Asim Shankar <notifications@github.com> wrote:\n\n> We will be looking into those, particularly for the Java API\n> <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java>\n> and all that is needed for it.\n>\n> However, might be a while.\n>\n> FYI @jhseu <https://github.com/jhseu>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6926#issuecomment-273528002>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE7A5zzFh-XCBShDQTFXV8rtEahmEQu-ks5rTkCOgaJpZM4Lm2nu>\n> .\n>\n", "Don't forget that you can deploy and reference native binaries. As an example see [Netty's native capability](http://netty.io/wiki/forked-tomcat-native.html). Which would mean a super easy way of consuming TensorFlow from JVM languages with minimal setup - awesome!", "Starting with release 1.1, this will be done. I'll close this issue out when I update the documentation (Java README). But in the mean time, note that you can use the release candidate version by adding:\r\n\r\n```xml\r\n<dependency>\r\n  <groupId>org.tensorflow</groupId>\r\n  <artifactId>tensorflow</artifactId>\r\n  <version>1.1.0-rc0-windows-fix</version>\r\n</dependency>\r\n```\r\n\r\nto your project's `pom.xml`.", "@asimshankar Is the JNI library GPU-enabled or CPU-only?", "The JNI library contained in the Maven package is CPU only.\r\n\r\nHowever, the way the code is setup is that it looks for the `tensorflow_jni` library in the system path  (see [`NativeLibrary.java`](https://github.com/tensorflow/tensorflow/blob/7303b63/tensorflow/java/src/main/java/org/tensorflow/NativeLibrary.java#L48)). Thus, to enable GPU support, download the JNI archive mentioned in the README and make that available to the JVM (e.g., via `-Djava.library.path`)", "can i use this maven artifact on windows. Thus, native dynamic library ( libtensorflow.so )can not be used on windows, so how can i build it without native library ?", "Yes, you can use it on Windows. There was a slight issue with RC0 on Windows, but you can use the following:\r\n\r\n```xml\r\n <dependency>\r\n   <groupId>org.tensorflow</groupId>\r\n   <artifactId>tensorflow</artifactId>\r\n   <version>1.1.0-rc0-windows-fix</version>\r\n </dependency>\r\n ```\r\n", "Hi,\r\n\r\nLike the netty native library, the convention with Maven is to use the `classifier` tag to specify OS specific distros. \r\nSee [here](http://netty.io/wiki/forked-tomcat-native.html#wiki-h2-3) for an example.\r\n\r\nYou can even use the classifier to distinguish between pure-java implementations and those that come packaged with GPU specific binaries.\r\n\r\nI may have missed something, but it seems like we will need to install the GPU native libraries manually. \r\n\r\nIf so, can be please avoid this!\r\n\r\nAs developers, we should be able to integrate TensorFlow with a single `<dependency>` declaration, such that any platform specificity is fully resolved. e.g. \r\n\r\n```xml\r\n<dependency>\r\n  <groupId>org.tensorflow</groupId>\r\n  <artifactId>tensorflow</artifactId>\r\n  <version>1.1.0</version>\r\n  <classifier>windows-gpu</classifier>\r\n</dependency>\r\n```\r\n\r\n1. It's really easy to integrate TensorFlow\r\n2. All build aspects, including native libraries are fully prepackaged, expanded to filesystem, and bound, accordingly at runtime.\r\n3. The entire build process on a 'clean' machine (i.e. no TensorFlow libraries installed on path) is easily possible - which makes it easy to integrate with CI servers.\r\n\r\nThis is all standard stuff for Maven and all Java devs would expect it!\r\n\r\nkind regards\r\nFuzz", "@dazraf : A single Maven dependency is all that is needed (irrespective of OS).  The same:\r\n\r\n```xml\r\n<dependency>\r\n   <groupId>org.tensorflow</groupId>\r\n   <artifactId>tensorflow</artifactId>\r\n   <version>1.1.0-rc0-windows-fix</version>\r\n</dependency>\r\n```\r\n\r\nWork for Linux and OS X and when the 1.1 release is finalized, using `<version>1.1.0</version>` will work for all three OSes as well. So I'm not quite sure I follow your concern.\r\n\r\nThat said, there are some caveats:\r\n\r\n- No, no GPU support included in the pom yet. But this is something we'll look into for the future. Till then, the workaround would be to download the GPU JNI library and make that available in the system path.\r\n- All the platforms are packaged together, without the use of classifiers. This makes the download a little larger. I was looking into classifiers but came across this: http://maven.40175.n5.nabble.com/How-to-deploy-with-classifier-tp5523009p5523417.html Admittedly. I'm not a Maven expert so wasn't sure how to interpret that.", "My concerns are exactly your caveats :-)\r\n* The classifiers minimise the size of the distro\r\n* They keep everything in the maven \"world\" - native libraries and JVM classes. Which means that builds can be fully controlled through maven (without manual processes for installation of libraries).\r\n\r\nAm I allowed to submit a PR for this? I can help fix this. ", "@dazraf : You're welcome to send in a PR, though I'd like to make sure I understand the proposed flow. (FYI: The PR would operate somewhere under [`tensorflow/java/maven`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/maven)).\r\n\r\nTo help me understand the proposed flow, consider the following scenario: User X authors a library libX that depends on TensorFlow. X does not know what platforms libX will be used on, since that choice would be made by applications that depend on libX. In this case, what happens? Does User X have to define platform-specific classifiers for libX and force libX clients to select one? Does this keep propagating? Or are end applications required to select the TensorFlow classifier even though they have no direct dependency on TensorFlow (perhaps they have to add the `os-maven-plugin` for OS but still need to explicitly choose between CPU and GPU) - which may be fine, but is more than a 4 line addition to their dependencies? (I'm guessing this is why the thread I linked to above discourages use of classifiers for different \"flavors\", but perhaps I'm misinterpreting)\r\n\r\nNote that even as-is, everything is in the maven \"world\" - builds can be fully controlled through maven with the GPU caveat. It seems to me that including GPU support in the `.jar` for `libtensorflow_jni` is somewhat orthogonal to the use of classifiers?\r\n\r\nI would be tempted to keep the dependency as simple as possible (e.g., \"add these 4 lines, that's it\"), even if advanced users (for whom the size of the distro is a real concern) have to do something extra.\r\n\r\nLooking at the [netty link you pointed to](http://netty.io/wiki/forked-tomcat-native.html#artifacts), this seems similar to the approach used by netty where they provide the `netty-tcnative-boringssl-static` artifact for the common simple use case and suggest that more nuanced users use a different artifact/classifier. Is your proposal intended to address these \"advanced\" users without affecting the simpler 4-line-addition-to-pom.xml flow?", "@asimshankar : I see your point. So adapting what I wrote:\r\n\r\n1. as a libX developer, I want to bind to a single dependency that allows me to write my logic. \r\n2. as a user of libX, in another java project, I would like to reference libX and also add a dependency to leverage my GPU. \r\n\r\nDo these make sense?\r\nThe model you proposed works perfectly for `1.`. A single reference `org.tensorflow:tensorflow:<version>` to bind to is nice and easy.\r\n\r\nFor `2.`, I'm looking for a maven packaging of the GPU specific libraries - something like `org.tensorflow:tensorflow-windows-gpu:<version>` such that at runtime, `tensorflow` discovers `tensorflow-windows-gpu`, unpacks it, and binds to its native lib.\r\n\r\nThis is how the `netty-native` libraries work - netty looks for a well-known class on the classpath, which is provided by the `-native-` library. Netty invokes this class, which in turn unpacks the binary (it not already present) in a well known location etc ..\r\n\r\nSo in summary, the proposition is that Java developers of `2.` would prefer the installation of the GPU library to be controllable by maven.\r\n\r\nIs that a clearer narrative?", "Clearer, but pardon me, I'm not there just yet :)\r\n\r\nIgnoring GPU for a moment, the way the packaging currently works is that [`NativeLibrary.java`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/NativeLibrary.java) finds the appropriate native lib from the classpath, unpacks it and loads it.\r\n\r\nIn principle, the GPU-enabled JNI libraries could also be packaged the same way with a change to `NativeLibrary.java` so that it checks for GPU support and looks for the corresponding native library. Is that what you had in mind?\r\n\r\nAnd then presumably, the top level application can choose to use GPU or not by adding a dependency on the `libtensorflow_jni-gpu` artifact or something?", "@asimshankar thanks for figuring out how to embed the native lib, it really saves a lot of hassle.", "One little nuance to be aware of going forward.   In hosted environments like Apache Flink where each 'job' runs in a separate classloader, numerous instances of the JNI library might simultaneously become loaded.  The fact that the extracted resource has a generated temp name makes this possible; otherwise the JVM would complain \"Library xyz already loaded in another classloader\".  I'm hopeful that side-by-side loading won't have a negative effect on TF."]}, {"number": 6925, "title": "support for depth pooling in maxpool3d?", "body": "Is depth pooling in the works for MaxPool3D? Any pointers on how I should/could get this going myself? \r\n```\r\nUnimplementedError (see above for traceback): Pooling is not yet supported on the depth dimension.\r\n[[Node: max_pool_1 = MaxPool3D[T=DT_FLOAT, ksize=[1, 2, 2, 2, 64], padding=\"VALID\", strides=[1,\r\n 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Relu_1)]]\r\n```\r\nHere's an example of how I'm using it:\r\n```Python\r\ndef maxpool3d(input_, kd, kh, kw, stride=1,scope=None):\r\n    input_cdim = input_.get_shape().as_list()[-1]\r\n    batch_size = input_.get_shape().as_list()[0]\r\n    kernel = [batch_size, kd, kh, kw, input_cdim]\r\n    return tf.nn.max_pool3d(input_, kernel,\r\n        strides=[1,stride,stride,stride,1], padding=\"VALID\", name=scope)\r\n# ...\r\n# Perform 2 3D convolutions without max pooling.\r\nx = tf.nn.relu(conv3d(x, 4, 4, 4, 64, scope=\"conv_1\"))\r\nx = tf.nn.relu(conv3d(x, 4, 4, 4, 64, scope=\"conv_2\"))\r\n\r\n# Downsample with max pooling.\r\nx = maxpool3d(x, 2, 2, 2, scope=\"max_pool_1\")\r\n```", "comments": ["@mjanusz @vrv @zheng-xq : Any comments here?", "I don't know of anyone working on it, would be a nice contribution.\r\n\r\nI don't know whether cudnn supports pooling along this dimension, but if it does, it should be possible to relax this constraint.  But you'd have to find out by looking at the cudnn pooling API to see whether it is possible to pool across spatial dimensions and depth dimension at the same time.\r\n\r\nI'm sure Eigen's implementation of 3d pooling (CPU) doesn't do simultaneous pooling across more than the spatial dimensions, and it would probably be a lot of work to enable this.\r\n\r\nFor now, to get your work done: maybe first maxpool across the spatial dimensions, reshape / transpose the tensor and then maxpool across the depth dimension, followed by a final reshape / transpose.  It's unlikely that someone is going to add this on our side any time soon.", "Thanks for the info Vijay. The depth dimension *is* a spatial dimension with the volumetric data I'm working with. I'll be sure to look more closely into the cudnn api on 3D pooling. Thanks again!", "Indeed, I didn't come up with the name :) By Spatial, Eigen refers to 'planes, rows, columns', with 'depth' not being part of the spatial dimension, based on the API.\r\n\r\nEdit: presumably this naming comes as an extension of 2D pooling on images, which treats height and width of an image as 'spatial'.  The number of channels / depth of an image isn't really 'spatial' in the same way the x/y coordinators of an image are.", "All of this helps Vijay. Thank you for spelling that out for me.", "any updates...?", "There is no active work on pooling over the \"channels\" dimension that I'm aware of. Note that pooling works over all 3 spatial dimensions. The code of the OP should read:\r\n\r\nkernel = [1, kd, kh, kw, 1]\r\n\r\n  instead of:\r\n\r\nkernel = [batch_size, kd, kh, kw, input_cdim]\r\n\r\nin order to accomplish this.", "Hi @odellus!We are checking to see if you still need help in this issue .Have you tried latest stable version TF 2.6  yet ,Many more features and bug fixes has been done in 2.x versions. Please create a new issue if the issue is replicating in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I was wanting to do 3D convolutions on voxel data but the rise of graph convolutions on mesh inputs has made this request somewhat antiquated. Closing."]}, {"number": 6924, "title": "iOS returns different results in simple and camera examples using retrained inception model", "body": "I have retrained the inception model to recognize custom set of images. I have tried the simple and camera examples and replaced the .pb and .txt files with my own files. I am able to get a reliable result of probability of 0.99 when using the simple example (replaced the grace_hopper with my image). However, I got different results when using the camera example. The config I am using is as follows, is there any config I need to make? Thank you.\r\n\r\n```\r\nconst int wanted_input_width = 299;\r\nconst int wanted_input_height = 299;\r\nconst int wanted_input_channels = 3;\r\nconst float input_mean = 128.0f;\r\nconst float input_std = 128.0f;\r\nconst std::string input_layer_name = \"Mul\";\r\nconst std::string output_layer_name = \"final_result\";\r\n```", "comments": ["This question is probably better asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests.\r\n\r\nClosing this out. FYI @petewarden "]}, {"number": 6923, "title": "cmake vs bazel", "body": "Hello I am writing with regard compiling tensorflow on linux clusters. \r\nAs far as I understood bazel recompiles everything it needs. On out cluster we use spack (https://github.com/LLNL/spack) to compile new software and provide modules to the users. With spack it's easy to track dependencies and provenance of a given installation.\r\nBy recompiling everything bazel's need the dependencies are messed up and we don't understand how tensorflow has been compiled from a spack point of view. \r\nWhy is it stated\" N.B. We provide Linux build instructions primarily for the purpose of testing the build. We recommend using the standard Bazel-based build on Linux.\"?\r\nActually, how can we tell to bazel to use system-provided libraries like boost?\r\n\r\nThanks very much,\r\n\r\nNicola\r\n\r\n", "comments": ["I'm closing this issue since it's not a bug or feature request. But I'm happy to answer your Bazel questions here since Bazel is still relatively new and we want the community to feel comfortable with it.\r\n\r\nLike Spack, Bazel was also designed for supercomputers^Wextremely large clusters of inexpensive hardware. Unlike Spack, Bazel is designed so every piece of software exists only at one version, in order to make dependencies tractable.\r\n\r\nBeyond that cursory glance, I don't really know much about Spack and I don't understand your question or how it relates to cmake. Speaking of which, we actually have a cmake build in our contrib directory. If you're familiar with cmake, it may suit you, although it's community supported.", "Thanks for your answer. Spack relies mainly on cmake or configure to build packages. In configure or cmake you usually need to specify where a particular library is.\r\nI am working to create a spack package to tensorflow based on cmake. The idea is that you can install tensorflow with cuda support and gcc compiler like: spack install tensorflow +cuda %gcc. \r\nSo, I am perfectly fine with using cmake, and it is well integrated in spack. My concern is related to the statement:  We provide Linux build instructions primarily for the purpose of testing the build. If I would like to provide tensorflow for production purpose either bazel or cmake would do? Is there any difference between the two way of building tensorflow?\r\n\r\nThanks", "The difference is that Bazel is the build system we actually use. The make, cmake, and xcode build systems are contributed / supported by the community and are only capable of building a subset of TensorFlow. We smoke test make and cmake so we don't break it when we make changes. So it has some official support. But we strongly recommend using Bazel if it's capable of running on your system.", "There is a bazel package in spack (https://github.com/LLNL/spack/tree/develop/var/spack/repos/builtin/packages/bazel).  And also a tensorflow pr(https://github.com/LLNL/spack/pull/2043). The problem is to understand how to prevent bazel from recompiling libraries and tell bazel to use libraries already compiled by spack. Any idea?", "Examples? Like eigen, png, gif, jpeg, nasm, highwayhash, farmhash, pcre, swig, curl, proto, grpc, boringssl, etc. defined in [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl) and [WORKSPACE](https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE)? TensorFlow's Bazel configuration is actually doing a deceptively large amount of work. It builds all those things from scratch. So you could potentially have a very long road ahead of you. \r\n\r\nBlaze was designed to be hermetically sealed and not take anything into consideration anything that isn't defined in its own build graph. It's basically the Gentoo of build systems. So the solution is to put the stuff built by Spack into the build graph, using plain old BUILD code.\r\n\r\nIf I were you, and I was determined to do what you want to do, I would probably start by patching TensorFlow to eliminate its default WORKSPACE file. Then I would redefine all those http_archive rules with build rules that basically point at the things that spack built. It would be super painful.\r\n\r\nFor example, you can define a cc_library() where the srcs list contains a .so file, if I remember correctly. That .so file could just be a symlink to some compiled file in a spack output directory.", "how do I create a tensorflow workspace outside the tensorflow examples folder? I want to put my tensorflow code in a repository, but having to put it in the examples folder all the time make it kinda inconvenient.", "I have the same question as yours."]}, {"number": 6922, "title": "Getting sparse tensors from graph by name", "body": "Standard (dense) tensors can be easily retrieved by name from a graph using:\r\n\r\n`graph.get_tensor_by_name(\"tensor_name:tensor_index\")`\r\n\r\nComparable functionality is provided for operations, but it's not clear to me how to retrieve a sparse tensor in a similar way.\r\nIs such functionality provided at the moment? Having to pass the object around makes the code more involved than it should be.", "comments": ["Yes, it's possible, you just have to figure out the names of tensors that are storing the sparse tensor data. There are some details here\r\n\r\nhttps://www.google.com/url?hl=en&q=https://groups.google.com/a/tensorflow.org/d/msgid/discuss/CADtzJKPqcpYLP2LEgXEUqTpXLbD7YW5coALcR-RZrUirhovydg%2540mail.gmail.com?utm_medium%3Demail%26utm_source%3Dfooter&source=gmail&ust=1484809569744000&usg=AFQjCNFKi8wGndSsYXdUx6Qb5X_WwrPu6w\r\n\r\nI think this question is much better suited for stackoverflow since this list is for bugs/feature requests to tensorflow itself"]}, {"number": 6921, "title": "Added Intel MKL graph optimization code and the ability to enable MKL when running configure.", "body": "- Added Intel MKL graph optimization code.\r\n\r\n- Modified configure file to build with MKL support.\r\n\r\n- Included the MKL binary in Tensorflow wheel.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "style changes lgtm (need to resolve a conflict it looks like)", "Merging is done. Awaiting review completion.", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please", "@mahmoud-abuzaina please reformat the build file by running buildifier or apply the suggested fixes here: https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/2931/console", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@mahmoud-abuzaina thanks for the contribution. Merging now."]}, {"number": 6920, "title": "can tensorflow CPU version be updated to GPU version without uninstalling ?", "body": "python 3.5 \r\ntensorflow 0.11.0\r\nHello,\r\nI wanted to do FFT operations using tensorflow. does this CPU version support working of that ? \r\n\r\nIn some post I have seen we cannot perform FFT on CPU version of tensorflow. If so can I change the tensorflow version of CPU to GPU without installing it ? \r\n\r\nI need to work on the LSTM algorithm as well further. \r\n", "comments": ["This seems like a much better question for stackoverflow, this tracker is for bugs/feature requests in the tensorflow itself"]}, {"number": 6919, "title": "tensorflow:Drop an example - too long", "body": "warining :tensorflow:Drop an example - too long\r\nI want to know what is this warning's meaning or where i can find the doc about it  ,thk u tell  me \r\n", "comments": ["can you please ask on stackoverflow instead? This list is for tensorflow bugs/feature requests"]}, {"number": 6918, "title": "Improved support for libxsmm", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "contrib/learn is pushing to be kicked out of continuous tests.\r\nJenkins, test this please.", "@tensorflow-jenkins test this please."]}, {"number": 6917, "title": " How to init the graph once and reuse the graph to serve all the requests", "body": "I want to provide a restful api or rpc for client to input some pictures and output their features, but every request will start a new graph and load all train variables from the disk, and this is very slow, how can I init the graph once and reuse the graph to serve all the requests, Thank You for your help", "comments": ["Could you please ask this on stackoverflow? This list is for tensorflow bugs/feature requests"]}, {"number": 6916, "title": "Fix warning initialize_all_variables is deprecated", "body": "Replace initialize_all_variables with global_variables_initializer.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "PR merged. Thank you, @bostelk "]}, {"number": 6915, "title": "Fixing typos in README", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 6914, "title": "Binaries should target CUDA architectures 6.0, 6.1 to avoid long delays on startup", "body": "The public docker images and pip packages are not compiled against CUDA architecture 6.0 or 6.1, which can cause a delay of more than a minute during initialization when executed on corresponding hardware (P100, GTX 10 series etc.) as the CUDA driver runs JIT compilation on all of the internal kernels.\r\n\r\nGiven that the public binaries are now built against CUDA 8.0, it would make sense to have them target archs 6.0 and 6.1.\r\n\r\nMinimal reproducer (run with CUDA_CACHE_DISABLE=1):\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nrand_init = tf.random_uniform([32, 8], -1.0, 1.0)\r\nrand_var  = tf.Variable(rand_init)\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\n```\r\n\r\n### Related issues\r\nhttps://github.com/tensorflow/tensorflow/issues/3651\r\n\r\n### Environment info\r\nOperating System: Linux\r\n", "comments": ["Gunhan, how hard would it be to do for our pip releases ? ", "@martinwicke \r\n", "cc @yifeif too", "Not that difficult, but this came up before and the result was to limit our pip packages to 2 cuda compute capabilities. 3.0, we wont drop.\r\nif we do 6.1, we will drop 5.2\r\nWe will see a worse degradation for more people IMO once we do that.", "Let's chat more, I think we have to figure out a way to distribute (potentially large) binaries that works for more people.\r\n\r\n@benbarsdell : what do other frameworks do?  Do they release binaries that contain the code for every possible compute capability under the sun too?", "My recommendation would be to include, minimally, 3.5, 5.2, 6.0, and 6.1.  Since you also want 3.0, then that makes five.\r\n\r\nCompute capability 3.7 is ISA-identical to 3.5; only the on-chip resources increased, so it's only in specific circumstances that building 3.7 separately is worthwhile, and I'd say it's fair to skip it here.  Compute capability 5.0 was first-gen Maxwell (GM10x), and the only chips in that line were on the smaller end of the spectrum -- most people doing compute went straight from Kepler (3.x) to the 2nd-gen Maxwell (GM20x, aka compute capability 5.2).\r\n\r\nThe only other ones that exist that I've omitted are the ones for mobile GPUs.", "Thanks @cliffwoolley, very helpful!  From what Martin told me, the current issue is that pypi has a limitation on the size of the pips, and adding those two compute capabilities will probably bloat the size of the GPU pip wheel to something too large.  So I wish I could say this is trivial to change, but it might require some more work on the pip/pypi side first.  ", "@vrv Here's another idea. The issue has to do more with Docker containers than with pip packages uploaded to pypi. The dev container build runs Bazel explicitly, while the non-dev container build pulls the pip package from pypi. Perhaps the builds can be set up in such a way that the non-dev container also builds from source and adds the compute capabilities mentioned by @cliffwoolley, while the pip build to be pushed to pypi is done separately, and constrains the compute capabilities so as to satisfy pypi's space limitations? That way, at least all Docker users would have more compute capabilities so as to limit the dependence on the JIT, and only bare-metal pip users would be affected by the JIT the first time on Pascal GPUs?  Just an idea.", "Yup, that's a reasonable solution too, and something that could be done pretty quickly.  \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L92  Can you try editing that to add 6.0,6.1 and see if the immediate problem gets resolved?  (Feel free to send us a PR if so).\r\n\r\nIn the meantime, we'd like to make this also available for our non-docker folks, so we'll also work on that if we can.", "@vrv Great! See [PR #6941](https://github.com/tensorflow/tensorflow/pull/6941).", "Another solution could be to separate the binaries based on the compute capabilities they support.  We already release many binaries...so I'm not sure if this would be considered crossing the line to too many.", "Due to constraints on our pip package sizes, this is currently infeasible.\r\nHowever we are working on a few longer term initiatives which may tackle this issue too."]}, {"number": 6913, "title": "Feature: log-log scale", "body": "This PR addresses #6532 and adds a toggle-able chart configuration sidebar, which should be useful for adding additional chart configuration options in the future, i.e. #1141\r\n\r\n![tb-scales2](https://cloud.githubusercontent.com/assets/4740651/22040029/551cf23e-dcb6-11e6-8d78-ee71462d500e.gif)\r\n\r\nAs it stands right now, the log scale is only applied to the x-axis when the unit is `steps` or `relative`, otherwise it ignores the configuration and uses a linear scale for `wall`. I'm not sure if thats the best approach and happy to make any modifications.\r\n\r\nThis is my first foray into Polymer and would appreciate any feedback :)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@tarrencev please review and sign the CLA.", "I'm still waiting on permission to sign the CLA from our legal team. Will close out for now.", "Any updates on the CLA? I would love to have this feature.", "Same here"]}, {"number": 6912, "title": "Pip installation failing", "body": "Fresh python3 install using `brew install python3`.\r\n\r\nWhen I run `pip3 install tensorflow`, I get a \r\n\r\n`Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow`\r\n\r\nI've tried reinstalling python but I get the same error message.", "comments": ["this might help \r\n[http://stackoverflow.com/questions/38896424/tensorflow-not-found-in-pip](url)", "This kind of installation support is generally better handled on stackoverflow, hence closing. Feel free to reopen if you think there's a bug in tensorflow installation procedure, in which case, please fill out the template/provide more details"]}, {"number": 6911, "title": "Switching branches forces ./configure and bazel rebuild", "body": "Right now switching branches requires running ./configure which then makes next `bazel test` rebuild everything.\r\n\r\nIn particular on 32 core Xeon, switching branches/`./configure` makes `bazel test learning/python/...` run 17 minutes longer because it's building stuff from scratch. This makes it hard for open-source contributors to work on more than one Pull Request at a time.\r\n\r\nIn particular, the error from `bazel test` when switching git branches is as follows:\r\n\r\n```\r\nERROR: /home/yaroslav/tensorflow.git/tensorflow/tensorflow/core/BUILD:1194:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed:\\\r\n error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited\\\r\n with status 1.\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 260, in <module>\r\n    generate(args.generate)\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 211, in generate\r\n    (old_branch, new_branch))\r\nRuntimeError: Run ./configure again, branch was 'refs/heads/match-once-fix' but is now 'refs/heads/symbolic-gradient-fix'\r\n_\r\n```", "comments": ["Not sure how feasible it is to relax this check, maybe someone familiar with bazel can comment, cc @damienmg ", "Why is changing branch needs reconfiguration?\r\n\r\nAs the future, we will need less and less ./configure (like in next bazel release) and should be able to keep the configuration in another cache (so you can switch in between branch and get cache hits, probably in a few months).\r\n", "cc @aselle who added this check", "It should be possible to have a ./configure --update-git or something that does only the part that of configure that needs to be redone. The point of this is so that __git_version__ is correct in compiled binaries, which is important for support and bug fixing, but I agree that having to rebuild everything just to update the file that bazel uses to acquire __git_version__ is onerous.", "I think as we removed `bazel clean` from configure script, this issue should now be fixed.\r\nI will close this issue, but please feel free to reopen if this is still not resolved after installing `bazel 0.5.0` and using master.", "I got the same error on branching from r1.2,\r\n`RuntimeError: Run ./configure again, branch was 'refs/heads/r1.2' but is now 'refs/heads/trace_learning_XLA_ops'\r\n`\r\nUsing Ubuntu 16.04 and bazel 0.5.2 provided by apt-get.", "So I've just tried at head and this is happening again. After switching branches, I get this from bazel\r\n\r\n```\r\nERROR: /home/yaroslav/Dropbox/git0/tensorflow/tensorflow/core/BUILD:1620:1: Couldn't build file tensorflow/core/util/version_info.cc: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 264, in <module>\r\n    generate(args.generate)\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 215, in generate\r\n    (old_branch, new_branch))\r\nRuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/bytes_in_use'\r\nERROR: /home/yaroslav/Dropbox/git0/tensorflow/tensorflow/core/BUILD:1620:1: Couldn't build file tensorflow/core/util/version_info.cc: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 264, in <module>\r\n    generate(args.generate)\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 215, in generate\r\n    (old_branch, new_branch))\r\nRuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/bytes_in_use'\r\n```\r\n\r\nThis is using today's master at github.com/tensorflow/tensorflow/commit/ea94bbe9 , and Bazel  0.5.4", "Looks like commenting out the lines which throw this exception in gen_git_source.py fixes the issue", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 6910, "title": "Feature Request: Allow disabling the use of host pinned memory", "body": "Right now, TensorFlow will always use pinned memory (allocated by the stream executor with `cuMemHostAlloc`) whenever it knows that a tensor will need to be transferred to the GPU (or something like that).\r\n\r\nWhen a TensorFlow process crashes (does not exit gracefully) due to segfaults or other unforeseen circumstances, the PoolAllocator destructor is not called, and the pinned memory is not freed. When pinned memory is not freed, the CUDA driver does not actually release it, and so memory usage grows over time if you have processes that are not exiting gracefully. After this happens for a while, the machine needs to swap and/or runs out of pinned memory, and if there is no swap space enabled, the node dies (and the only way to recover it is a hard-reboot, not accessible via the network).\r\n\r\nAs far as we can tell, there is no workaround to this, and we simply need to disable the pinned memory allocator if we want to be absolutely certain that this cannot happen. Right now, TensorFlow does not allow you to do so.\r\n\r\nProposal: There is currently an environment variable called `TF_CUDA_HOST_MEM_LIMIT_IN_MB` which used to set a maximum size for the `BFCAllocator` that does CUDA host memory allocation. In order to implement this feature, we would check whether the value of that environment variable is zero, and, if it is zero, use a non-pinned memory allocator as the sub-allocator for the BFCAllocator.\r\n\r\nIs that acceptable?\r\n\r\n[This](https://github.com/tensorflow/tensorflow/issues/3701) issue was probably related.", "comments": ["@zheng-xq @vrv  : Any suggestions for workarounds here? ", "That seems reasonable to me, though I wonder if there's another way to get the allocators cleaned up on destruction.  I'm not entirely sure why the ProcessState singleton needs to own the allocators, but I'll find out.", "@zheng-xq and I chatted offline, it sounds like this is an NVidia driver bug -- @gibiansky do you have the ability to upgrade your nvidia driver?\r\n\r\nWe also might prefer having another env var instead of giving double duty to this one.", "We do but it's a very large amount of effort as it requires an upgrade to the entire cluster. Are you sure this is fixed in newer drivers? We're running 361.93.02 from mid-October of 2016, which is after the issue I linked to was resolved.", "I remember hearing it's fixed (or at least, more bug fixes in general) in 367.*, but I'm not sure.  Just mentioning it as an option to test out, since I'm sure neither of us want to add more code/config if we can avoid it :)", "After testing this more thoroughly with just calls to CUDA runtime APIs, we cannot confirm that the pinned memory allocator is causing this problem. I'm closing this issue until (and if) we can confirm that pinned memory is causing our issues.\r\n\r\nThanks!"]}, {"number": 6909, "title": "Use local variable for match_input_once to avoid saving it in checkpoint", "body": "Fixes #6786 ", "comments": ["Can one of the admins verify this patch?", "Are we certain that users don't want the additional reproducibilty of saving this list of filenames in the checkpoint?  If it isn't saved, you might run this again and get a different answer and be surprised that suddenly your training is behaving differently.", "In the original issue, the surprise was you run `tf.train.match_filenames_once` was run to generate list of testing filenames. Then pre-trained model was loaded from checkpoint, which overwrote this list with training filenames. 100% test set accuracy, woohoo!\r\n\r\nSaved filenames are useful to restore while training a model, but are not useful to restore when loading this model for testing. It seems there needs to be some kind of distinction between model parameters and checkpointed things that are not model parameters. \r\n", "Jenkins, test this please.", "The failure is due to unrelated timeout\r\n```\r\n//tensorflow/contrib/layers:feature_column_ops_test                     TIMEOUT in 65.0s\r\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/contrib/layers/feature_column_ops_test/test.log\r\n```", "btw, unrelated timeout issue has been fixed", "@tensorflow-jenkins test this please", "@yaroslavvb I'm afraid that this broke an internal test. I'll be reverting this in the next push, while I investigate.", "@rmlarsen my first guess would be that the test is trying to restore \"matching_filenames\" variable from checkpoint", "@yaroslavvb thanks for the hint. Will take a look later."]}, {"number": 6908, "title": "[Windows] Enable tf.contrib.tensor_forest in CMake build and tests", "body": "Fixes error message appears when loading an OpKernel without the corresponding op. For example:\r\n\r\n```\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\n```\r\n\r\nSince we're building the kernels anyway, this change also runs the tensor_forest tests.\r\n\r\nFixes #6500. (Note however that the hybrid ops are not yet ported to work on Windows.)", "comments": ["@gunan This should probably be cherry-picked into 1.0.0. Would you prefer this PR in a different form (e.g. against the 1.0 branch)?", "Merging to master first is better, i will look into this in an hour\n\nOn Jan 17, 2017 4:11 PM, \"Derek Murray\" <notifications@github.com> wrote:\n\n> @gunan <https://github.com/gunan> This should probably be cherry-picked\n> into 1.0.0. Would you prefer this PR in a different form (e.g. against the\n> 1.0 branch)?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6908#issuecomment-273342341>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOf4uSF_Gx6XmAyXNezJ24JtZC5Mkks5rTVjFgaJpZM4Ll_jv>\n> .\n>\n"]}, {"number": 6907, "title": "Fix the error of erase session variables (Android JNI)", "body": "When calling the tensorflow_inference_jni.cc of TENSORFLOW_METHOD(close) function, the session variables is not erased from the map of session variables according its id.(at tensorflow_inference_jni.cc line 217)\r\nThis bug reason is assign large data type to the small data type (at tensorflow_inference_jni.cc line 84) that causes some bytes missing.\r\n\r\n### Illustrate \r\n```\r\n# tensorflow_inference_jni.cc file\r\n\r\n...\r\n 49 struct SessionVariables {\r\n 50   std::unique_ptr<tensorflow::Session> session; \r\n\r\n 52   long id = -1;\r\n       ...\r\n     };\r\n\r\n...\r\n 69 inline static SessionVariables* GetSessionVars(JNIEnv* env, jobject thiz) {\r\n       ...\r\n 72   jfieldID fid = env->GetFieldID(clazz, \"id\", \"J\");\r\n 73   assert(fid != nullptr);\r\n 74   const int64 id = env->GetLongField(thiz, fid);\r\n        ...\r\n 80   std::map<int64, SessionVariables*>& sessions = *GetSessionsSingleton();\r\n 81   if (sessions.find(id) == sessions.end()) {\r\n 82     LOG(INFO) << \"Creating new session variables for \" << std::hex << id;\r\n 83     SessionVariables* vars = new SessionVariables;\r\n 84     vars->id = id; // Bug assign int64 to long\r\n 85     sessions[id] = vars;\r\n         ...\r\n256 JNIEXPORT jint JNICALL TENSORFLOW_METHOD(close)(JNIEnv* env, jobject thiz) {\r\n       ...\r\n267   std::map<int64, SessionVariables*>& sessions = *GetSessionsSingleton();\r\n268   sessions.erase(vars->id);\r\n       ...\r\n```\r\nThe GetSessionVars function line 72~74 that get the long type id from java and assign to const int64 variable. At the line 84 that assigns int64 id variable to the id of SessionVariables, but the type of the id of SessionVariables struct is long. Assign large type to small type that causes some bytes missing. \r\nAt the line 85, using the int64 type of id to set the key of the SessionVariables map, so  TENSORFLOW_METHOD(close) function erases the session variables according to vars->id that the vars->id will never be found and erased from map.\r\n\r\n### Fix\r\nI change the type of the id of SessionVariables struct to int64.\r\n```\r\n struct SessionVariables {\r\n   std::unique_ptr<tensorflow::Session> session;\r\n\r\n   int64 id = -1;  // Change data type\r\n   int num_runs = 0;\r\n   int64 timing_total_us = 0;\r\n \r\n   InputMap input_tensors;\r\n   std::vector<std::string> output_tensor_names;\r\n   std::vector<tensorflow::Tensor> output_tensors;\r\n };\r\n```", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins Test this, please.", "Hi @andrewharp\r\nI looked the build log of jenkins, the failed reason is  '//tensorflow/tensorboard/backend:server_test                            TIMEOUT in 65.0s'.\r\n\r\nI have compared my branch with the latest of master branch of tensorflow, the difference only my modification part.\r\n\r\nDo you know the build failed reason?", "@andrewharp The tensorboard server_test timeout happens on and off. It looks unrelated. PTAL and I can merge the PR after your approval.", "@tensorflow-jenkins test this pleaes\r\ntesting again since last test results are two days old.", "@tensorflow-jenkins test this please"]}, {"number": 6906, "title": "Relaxing inferenced shape", "body": "Inferenced shape will put restriction on feeding, so sometimes it is desirable to relax inferenced shape. Currently this can be done by `lambda x: tf.cond(tf.constant(True), lambda: x, lambda: tf.placeholder(x.dtype))`, or `tf.placeholder_with_default` if gradient is not required (`placeholder_with_default` has no gradient registered).\r\n\r\nI also read a comment in `identity_op.cc`: \r\n\r\n>PlaceholderWithDefault does the same thing as Identity, but has a\r\n>different shape function (and constant value function) registered.\r\n\r\nPossibly register a gradient for `placeholder_with_default`?", "comments": ["@vrv : Any thoughts here? I'm not quite sure what the position is on feeding arbitrary nodes in the graph as it can can mess with shape inference, gradient computations (see #6903 that @gaohuazuo also filed).", "I have no thoughts because I don't really understand the problem.  Specifying a placeholder without shape allows feeding any shape data.\r\n\r\nI don't think we register gradients for placeholders either.  I think providing more context into what you're trying to do will help us out.", "I don't understand the problem either, but you can specify the gradient for placeholder_with_default using `override_gradient_map` (it'll return `None` instead of crashing with error on gradient computation)", "Thanks for the replies. What I'm doing is to explore the behaviour of the discriminator in GAN. The graph is roughly the following:\r\n\r\n![untitled diagram](https://cloud.githubusercontent.com/assets/10446514/22094828/4b0cfaf0-de4b-11e6-97e3-f3c6b263b04d.png)\r\n\r\nDuring training, gradients have to propagate through `x` to `G`. On the other hand, I want to feed some arbitrary `x` to `D` for testing. I feel that this should be suitable for `placeholder_with_default`, but it has no gradient and thereby prevents training using the same graph. Note that I either feed `x` or use the gradient, but never both, so this should be a good use case of `placeholder_with_default`.\r\n\r\nI guess it is a protective measure that `placeholder_with_default` doesn't have a gradient. However, this isn't quite meaningful, as other ops don't have the same protection, and `placeholder_with_default` has nothing special to do with feeding, except its name.\r\n\r\nIn short, I would suggest using `identity` as the gradient of `placeholder_with_default`, because it does no harm and some good.", "You should be able to just feed whatever you want to D during testing (and presumably since you don't need the gradient during testing, you're not fetching anything below D either).  Doing so will induce a subgraph that cuts out everything before it.", "Hm, connecting this back to now what I think your original request is, that you want to avoid having the input to 'x' be a well known shape, I think I understand why you want this.  It seems plausible to add the identity gradient for placeholder_with_default.\r\n\r\ncc @mrry who added placeholder_with_default\r\n\r\nIt might also be possible to set the shape of 'x' in python using set_shape() to something that has an unknown shape or partially unknown shape before connecting it to D.\r\n", "Registering `tf.identity()` as the gradient of `tf.placeholder_with_default()` makes sense to me as a first step.\r\n\r\nThe only downside&mdash;as @gaohuazuo implied&mdash;is that if you feed a value for a `tf.placeholder_with_default()` **and** consume its gradient, the gradient will depend on the default value, which might run counter to expectations. I'm open to suggestions on what the right behavior should be here.", "backprop op for`tf.identity` propagates backprops unchanged, so in this sense it doesn't depend on input value", "Actually, it seems the author is requesting that `tf.placeholder_with_default` has 0 gradient rather than gradient of identity", "Let me summarize a bit:\r\n1. Registering `identity` as the gradient for `placeholder_with_default` solves this issue perfectly.\r\n2. There are problems related to feeding, but they are not specific to `placeholder_with_default` nor its gradient, and should be addressed separately in #6903.\r\n3. I guess we can register `identity` for the gradient and close this issue.\r\n\r\nP.S I originally thought that a 'good' gradient for `placeholder_with_default` would solve this issue and #6903, but that's not the case (see #6903).", "@gaohuazuo: Thanks for the summary.  Given that as @yaroslavvb mentioned, the gradient is independent of the fed input (it's not clear how the gradient should be different than identity when fed), I think it seems reasonable to add the gradient.  Would you like to send a PR for this with a test to validate your expectations?"]}]