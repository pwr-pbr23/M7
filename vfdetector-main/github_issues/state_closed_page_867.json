[{"number": 27494, "title": "'Sequential' object has no attribute 'total_loss'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution \r\nWindows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\n1.13.1\r\n- Python version:\r\n3.7.1 \r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI try to fit a Sequential model with both a training dataset and a validation dataset with fit_generator function. After of running it shows - 'Sequential' object has no attribute 'total_loss'-\r\n\r\n**Describe the expected behavior**\r\nTraining should work fine.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport random\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport multiprocessing \r\nimport tensorflow as tf\r\nfrom resizeimage import resizeimage\r\n\r\ndef images_list(path):\r\n    images_list = []\r\n    XY = []\r\n    with open(path,\"r\") as file:\r\n        images_list = file.read().split('\\n')\r\n        XY = [row.split(\" \") for row in images_list if len(row.split(\" \")) > 1]\r\n    return np.asarray(XY)\r\n\r\ndef load_images(X,Y,i):\r\n    root = 'E:\\\\images\\\\rvl-cdip\\\\rvl-cdip\\\\images'\r\n    img_matrixes = []\r\n    labels = []\r\n    length = len(X)\r\n    for index in range(len(X)):\r\n        matrix = Image.open(os.path.join(root, X[index].replace('/',\"\\\\\")))\r\n        img_matrixes.append(resize_image_500(matrix))\r\n        labels.append(Y[index])\r\n        \r\n    img_matrixes = np.asarray(img_matrixes)\r\n    labels = np.asarray(labels)\r\n    \r\n    assert len(img_matrixes) == len(labels)\r\n          \r\n    #print(\"{}: Loaded {} images\".format(i,length))\r\n          \r\n    return np.reshape(img_matrixes,(img_matrixes.shape[0],500,500,1)),labels\r\n\r\ndef resize_image(img):\r\n    np_img = np.asarray(img)\r\n    \r\n    if(np_img.shape[1] < 3235):\r\n        missing_width = 3235 - np_img.shape[1]\r\n        white_matrix = np.empty((1000,missing_width),dtype=float)\r\n        white_matrix.fill(255)\r\n        np_img = np.hstack((np_img, white_matrix))\r\n        \r\n    assert np_img.shape[0] == 1000\r\n    assert np_img.shape[1] == 3235\r\n    \r\n    return np_img\r\n\r\ndef resize_image_500(img):\r\n    resized = resizeimage.resize_cover(img, [500, 500])\r\n    np_img = np.asarray(resized)\r\n    \r\n    assert np_img.shape[0] == 500\r\n    assert np_img.shape[1] == 500\r\n    \r\n    return np_img\r\n\r\ndef iterate_minibatches(inputs, targets, batchsize):\r\n    assert len(inputs) == len(targets)\r\n    indices = np.arange(len(inputs))\r\n    np.random.shuffle(indices)\r\n    i = 0 \r\n    for start_idx in np.arange(0, len(inputs) - batchsize + 1, batchsize):\r\n        excerpt = indices[start_idx:start_idx + batchsize]\r\n        i+=1\r\n        yield load_images(inputs[excerpt], targets[excerpt],i)\r\n\r\nfrom tensorflow.keras import layers\r\n\r\nmodel = tf.keras.models.Sequential()\r\n#H1\r\nmodel.add(layers.Conv2D(8, kernel_size=(5, 5), strides=(1, 1),\r\n                 activation='relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n#H2\r\nmodel.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1),\r\n                 activation='relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n#H3\r\nmodel.add(layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\r\n                 activation='relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\r\n#H4\r\nmodel.add(layers.Conv2D(64, kernel_size=(5, 5), strides=(1, 1),\r\n                 activation='relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\r\n#H5\r\nmodel.add(layers.Flatten())\r\n#Dense\r\nmodel.add(layers.Dense(1024, activation='relu'))\r\nmodel.add(layers.Dense(1024, activation='relu'))\r\n\r\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\r\n              optimizer='adam',\r\n              metrics=[tf.keras.metrics.Accuracy()])\r\n#Training data\r\nXY_train =  images_list(train_path)\r\nX_train = XY_train[:,0]\r\nY_train = XY_train[:,1].astype(int)\r\n\r\n#Testing data\r\nXY_test =  images_list(test_path)\r\nX_test = XY_test[:,0]\r\nY_test = XY_test[:,1].astype(int)\r\n\r\n#Validation data\r\nXY_val = images_list(valid_path)\r\nX_val = XY_val[:,0]\r\nY_val = XY_val[:,1].astype(int)\r\n\r\nbatch_size = 750\r\nhistory = model.fit_generator(generator=iterate_minibatches(X_train, Y_train,batch_size),\r\n                                  validation_data=iterate_minibatches(X_test, Y_test, batch_size),\r\n                                  # validation_data=None,\r\n                                  steps_per_epoch=len(X_train)//batch_size,\r\n                                  validation_steps=len(X_test)//batch_size,\r\n                                  verbose=1,\r\n                                  epochs=100,\r\n                                  use_multiprocessing=True,\r\n                                  workers=multiprocessing.cpu_count() \r\n                             )\r\n```\r\n\r\n```\r\nbatch_size = 750\r\nhistory = model.fit_generator(generator=iterate_minibatches(X_train, Y_train,batch_size),\r\n                                  validation_data=iterate_minibatches(X_test, Y_test, batch_size),\r\n                                  # validation_data=None,\r\n                                  steps_per_epoch=len(X_train)//batch_size,\r\n                                  validation_steps=len(X_test)//batch_size,\r\n                                  verbose=1,\r\n                                  epochs=100,\r\n                                  use_multiprocessing=True,\r\n                                  workers=multiprocessing.cpu_count() \r\n                             )\r\n\r\nWARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-10-367767e34155> in <module>\r\n      8                                   epochs=100,\r\n      9                                   use_multiprocessing=True,\r\n---> 10                                   workers=multiprocessing.cpu_count()\r\n     11                              )\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1424         use_multiprocessing=use_multiprocessing,\r\n   1425         shuffle=shuffle,\r\n-> 1426         initial_epoch=initial_epoch)\r\n   1427 \r\n   1428   def evaluate_generator(self,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\r\n    123 \r\n    124   batch_function = _make_execution_function(\r\n--> 125       model, mode, class_weight=class_weight)\r\n    126 \r\n    127   # Create the queue for the generator.\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py in _make_execution_function(model, mode, class_weight)\r\n    425   if mode == 'train':\r\n    426     if not context.executing_eagerly():\r\n--> 427       model._make_fit_function()\r\n    428     f = functools.partial(model.train_on_batch, class_weight=class_weight)\r\n    429   elif mode == 'test':\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _make_fit_function(self)\r\n   1924     ]\r\n   1925     self._make_train_function_helper(\r\n-> 1926         '_fit_function', [self.total_loss] + metrics_tensors)\r\n   1927 \r\n   1928   def _make_test_function_helper(self, fn_name, outputs, metric_updates=None):\r\n\r\nAttributeError: 'Sequential' object has no attribute 'total_loss'\r\n```", "comments": ["I have tried train_on_batch() and test_on_batch() , those works fine", "In input layer, you haven't provided input_shape.", "@dshahid380  Thanks a lot it works", "@grigvardanyan - Is the issue resolved. If yes, can we close it now ?", "@dshahid380 - thank you for your suggestion", "Closing the issue since it looks resolved."]}, {"number": 27493, "title": "Add tensorflow_framework to data of tf_cc_shared_object", "body": "https://github.com/tensorflow/tensorflow/pull/22797 added the versioned\r\ntensorflow_framework libraries. The pip wheel had both the .so.1 and\r\n.so.1.13.1 libs but was missing the unversioned .so lib. This adds it\r\nback.\r\nThe full .so.1.13.1 lib is probably not needed here but removing it\r\nshould come separately later on.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["I have not figured out why the symlink is not preserved yet but lets just get this in first so things aren't broken anymore. Even if the symlinking cant be done, we can probably remove the .so.1.13.1 file since the soname of the file is just .so.1 thats the most important one to keep. the full .so.1.13.1 is more frequently used on linux distros and probably doesnt matter for the pip wheel.", "@gunan Okay, I made `tf_binary_additional_data_deps()` and also fixed up lib_package. there is some duplication of the files in the lib_package tar files but we can fix that later. probably using `pkg_tar(symlink=`."]}, {"number": 27492, "title": "Use of tf.custom_gradient prevents garbage collection of Graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip install tensorflow==1.13.1\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nUsing an op decorated with `tf.custom_gradient` prevents the `tf.Graph` it was created in from being garbage collected later. So if a script creates multiple such graphs in a row, it will use more and more memory unnecessarily.\r\nI believe it's due to the `Graph` being held as an indirect reference to the custom gradient registry.\r\n\r\n**Describe the expected behavior**\r\n\r\nIdeally such a `Graph` would not be referenced forever, allowing it to be garbage-collected.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport gc\r\nimport objgraph\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.custom_gradient\r\ndef op(x):\r\n    def grad(dy):\r\n        return dy * 10.0\r\n    return x, grad\r\n\r\n\r\ndef run(use_custom_gradient):\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        x = tf.constant([1, 2, 3])\r\n        if use_custom_gradient:\r\n            print(\"(Using custom_gradient)\")\r\n            x = op(x)\r\n        sess.run(x)\r\n\r\n\r\ndef log():\r\n    gc.collect()\r\n    print(\r\n        \"number of uncollected graphs:\",\r\n        len(objgraph.by_type('Graph')))\r\n\r\n\r\nfor use_custom_gradient in [False, True, True, False, False, True]:\r\n    run(use_custom_gradient)\r\n    log()\r\n```\r\n\r\noutputs:\r\n\r\n```\r\nnumber of uncollected graphs: 0\r\n(Using custom_gradient)\r\nnumber of uncollected graphs: 1\r\n(Using custom_gradient)\r\nnumber of uncollected graphs: 2\r\nnumber of uncollected graphs: 2\r\nnumber of uncollected graphs: 2\r\n(Using custom_gradient)\r\nnumber of uncollected graphs: 3\r\n```\r\n\r\n(ideally it should always say `number of uncollected graphs: 0`)\r\n\r\n**Other info / logs**\r\nn/a\r\n", "comments": ["Yes, this can be a problem. Marking as contributions welcome as I won't have time to dig into this soon, and someone interested in finding the backreferences to those graphs might have a better luck.", "Hey, I'd like to take a crack at this!", "@astropeak Great! Have you looked at python's GC module and how it lets you find out all references to a living object? That's where I'd start", "great blog detailing (with links) about GC \r\nhttps://rushter.com/blog/python-garbage-collector/", "It turns out that the graph object is finally referenced by the global variable '_gradient_registry', which is in 'tensorflow/python/framework/ops.py'.\r\n\r\nThis is how (for the above example):\r\nNotation: 'A -> B' means A is referenced by B\r\n\r\ngraph ->\r\nx ->\r\nresult object in custom_gradients.py at line 183 ->\r\nflat_result  in custom_gradients.py ->\r\nclosure tape_grad_fn  in custom_gradients.py ->\r\nclosure internal_grad_fn  in custom_gradients.py ->\r\n_gradient_registry\r\n\r\nI found the reference relationship between 'closure tape_grad_fn' and 'flat_result' could be broken, since 'closure tape_grad_fn' only uses 'len(flat_result)'. After that reference is broken, the problem disappears. So this could be a solution. Do you have any opinions? If this solution looks good, I will create a pull request soon.", "That sounds like the correct fix. Would love a pull request!", "I guess this is a pretty old issue now, but I just came across a similar incarnation, which I don't think is fixed by #28826.\r\n\r\nUsing the example from `custom_gradient`, which is:\r\n```\r\n@tf.custom_gradient\r\ndef log1pexp(x):\r\n  e = tf.exp(x)\r\n  def grad(dy):\r\n    return dy * (1 - 1 / (1 + e))\r\n  return tf.math.log(1 + e), grad\r\n```\r\nnotice that the `grad` function holds a reference to the `e` tensor (which references its containing graph) via its closure. Breaking the reference from `tape_grad_fn` to that `grad` function doesn't really seem possible.\r\n\r\nI suppose one way to fix that version of the issue would be to break the reference between `grad` and `e`; instead of holding `e` in its closure, maybe `grad` could get `e` from an input, just like built-in gradient functions? Indeed, we have the op in `internal_grad_fn` ([link](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/custom_gradient.py#L413)) but just drop it on the floor. Perhaps if that op were plumbed through to `grad` we'd be able to avoid using closures?\r\n\r\nFWIW I don't see this issue when using `@tf.function` instead of `tf.Graph()`s, seemingly because with `@tf.function` the `e` tensor doesn't hold a reference to any graph. I don't know enough about `@tf.function`s to say whether that's by design or by accident though.", "Came up with a workaround, in case anybody else needs it. Trick is to only let the custom gradient machinery get access to a weak reference to your actual gradient function (and any subsequent objects captured in its closure). Note that to avoid the gradient function then getting GC'd too *early* we also need to keep a strong reference around for the lifetime of the graph, which we can do with a graph collection:\r\n```\r\ndef custom_custom_gradient(f):\r\n    def _f(*x):\r\n        y, grad = f(*x)\r\n\r\n        if tf.compat.v1.executing_eagerly():\r\n            new_grad = grad\r\n        else:\r\n            tf.compat.v1.get_default_graph().add_to_collection(\r\n                \"custom_gradient_functions\", grad\r\n            )\r\n\r\n            grad_weak = weakref.ref(grad)\r\n\r\n            def new_grad(*grad_ys):\r\n                return grad_weak()(*grad_ys)\r\n\r\n        return y, new_grad\r\n\r\n    return tf.custom_gradient(_f)\r\n```\r\n\r\nNote that this workaround could also be introduced into TF itself to fix this bug, although I imagine there are more legit approaches that could be taken.", "@matthen,\r\nYour code could be run without any error. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/bb2e518503624af19cd09c857644dbdb/gh_27492.ipynb) of the working code. Please let us know if we can close this issue? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27492\">No</a>\n"]}, {"number": 27491, "title": "Using .numpy() with the tf.function decorator.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): `2.0.0-alpha0`\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe .numpy() method doesn't work in a function with @tf.function decorator.\r\n\r\nFor example, the following code is excuted properly\r\n```python\r\n# Calculate neighbor list using ASE (a third party library).\r\ndef neighborlist(r):\r\n    atoms = ase.Atoms(positions=r.numpy(), pbc=True, cell=[10,10,10])\r\n    i, j = ase.neighborlist.neighbor_list(\"ij\", atoms, cutoff=3)\r\n\r\n    i = tf.constant(i)\r\n    j = tf.constant(j)\r\n\r\n    return i, j\r\n\r\n# Positions of 4 particles.\r\npositions = tf.Variable(tf.random.uniform(shape=[4,3]))\r\n# Calculate neighbor list.\r\nneighborlist(v)\r\n```\r\nOutput:\r\n```\r\n(<tf.Tensor: id=134, shape=(42,), dtype=int64, numpy=\r\n array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\r\n        3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6])>,\r\n <tf.Tensor: id=135, shape=(42,), dtype=int64, numpy=\r\n array([1, 2, 3, 4, 5, 6, 6, 4, 5, 2, 0, 3, 0, 1, 3, 4, 5, 6, 5, 6, 4, 2,\r\n        1, 0, 0, 1, 2, 3, 5, 6, 6, 4, 3, 1, 0, 2, 4, 0, 1, 2, 3, 5])>)\r\n```\r\n\r\nBut the following code gives errors.\r\n```python\r\n# Calculate neighbor list using ASE (a third party library).\r\n@tf.function\r\ndef neighborlist(r):\r\n    # r automatically casted to np.array.\r\n    atoms = ase.Atoms(positions=r.numpy(), pbc=True, cell=[10,10,10])\r\n    i, j = ase.neighborlist.neighbor_list(\"ij\", atoms, cutoff=3)\r\n\r\n    #return ri, rj\r\n    i = tf.constant(i)\r\n    j = tf.constant(j)\r\n\r\n    return i, j\r\n\r\n# Positions of 4 particles.\r\npositions = tf.Variable(tf.random.uniform(shape=[4,3]))\r\n# Calculate neighbor list.\r\nneighborlist(v)\r\n```\r\n\r\nError:\r\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-30-21258fe33b35> in <module>\r\n      2 positions = tf.Variable(tf.random.uniform(shape=[4,3]))\r\n      3 # Calculate neighbor list.\r\n----> 4 neighborlist(v)\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    424     # This is the first call of __call__, so we have to initialize.\r\n    425     initializer_map = {}\r\n--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    427     if self._created_variables:\r\n    428       try:\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    368     self._concrete_stateful_fn = (\r\n    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 370             *args, **kwds))\r\n    371 \r\n    372     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1311     if self._input_signature:\r\n   1312       args, kwargs = None, None\r\n-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1314     return graph_function\r\n   1315 \r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1578           or call_context_key not in self._function_cache.missed):\r\n   1579         self._function_cache.missed.add(call_context_key)\r\n-> 1580         graph_function = self._create_graph_function(args, kwargs)\r\n   1581         self._function_cache.primary[cache_key] = graph_function\r\n   1582         return graph_function, args, kwargs\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   1510             arg_names=arg_names,\r\n   1511             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 1512             capture_by_value=self._capture_by_value),\r\n   1513         self._function_attributes)\r\n   1514 \r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    692                                           converted_func)\r\n    693 \r\n--> 694       func_outputs = python_func(*func_args, **func_kwargs)\r\n    695 \r\n    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    316         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    318     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    319 \r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    684                   optional_features=autograph_options,\r\n    685                   force_conversion=True,\r\n--> 686               ), args, kwargs)\r\n    687 \r\n    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    390     return _call_unconverted(f, args, kwargs)\r\n    391 \r\n--> 392   result = converted_f(*effective_args, **kwargs)\r\n    393 \r\n    394   # The converted function's closure is simply inserted into the function's\r\n\r\n/tmp/tmplnfiywp3.py in tf__neighborlist(r)\r\n      3   do_return = False\r\n      4   retval_ = None\r\n----> 5   atoms = ag__.converted_call('Atoms', ase, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (), {'positions': r.numpy(), 'pbc': True, 'cell': [10, 10, 10]})\r\n      6   i, j = ag__.converted_call('neighbor_list', ase.neighborlist, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), ('ij', atoms), {'cutoff': 3})\r\n      7   i = ag__.converted_call('constant', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (i,), {})\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in numpy(self)\r\n    826       return self.read_value().numpy()\r\n    827     raise NotImplementedError(\r\n--> 828         \"numpy() is only available when eager execution is enabled.\")\r\n    829 \r\n    830   @deprecated(None, \"Prefer Dataset.range instead.\")\r\n\r\nNotImplementedError: numpy() is only available when eager execution is enabled.\r\n```\r\n\r\nThe main problem of above issue is here. If the function (neighborlist) is inside a model,\r\n\r\n```python\r\nclass Model(...):\r\n    ....\r\n    @tf.function\r\n    def call(self, ...):\r\n        ....\r\n        i, j = neighborlist(x)\r\n        ....\r\n```\r\n\r\n`Model.call()` will give errors.\r\nIt should be pointed out that I don't need the gradients of the opreration (neighborlist).\r\nI think it is common case that using operations with the value of tensors in erger mode. So it can be problematic.\r\n\r\nI tried to resolve the problem by using `tf.py_function` but I failed. How can I resolve the problem?\r\n\r\n**Will this change the current api? How?**\r\nProbably not.\r\n**Who will benefit with this feature?**\r\nUse complex operations inside the tensorflow graph.\r\n**Any Other info.**\r\n", "comments": ["I just realized that the following code works properly\r\n```python\r\n# Calculate neighbor list using ASE (a third party library).\r\ndef neighborlist(r):\r\n    # r automatically casted to np.array.\r\n    atoms = ase.Atoms(positions=r, pbc=True, cell=[10,10,10])\r\n    i, j = ase.neighborlist.neighbor_list(\"ij\", atoms, cutoff=5)\r\n\r\n    return i, j\r\n\r\n@tf.function\r\ndef tf_neighborlist(r):\r\n    i, j = tf.py_function(neighborlist, inp=[r], Tout=[tf.int64,tf.int64])\r\n    return i, j\r\n\r\n# Positions of 4 particles.\r\npositions = tf.Variable(tf.random.uniform(shape=[4,3]))\r\n# Calculate neighbor list.\r\ntf_neighborlist(positions)\r\n```\r\n\r\nOutput:\r\n```\r\n(<tf.Tensor: id=128, shape=(12,), dtype=int64, numpy=array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3])>,\r\n <tf.Tensor: id=129, shape=(12,), dtype=int64, numpy=array([1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2])>)\r\n```\r\n\r\nEnd the issue.", "Sorry for reopening this issue, but I still didn't find a solution about how we can convert a tensor to scalar or list inside a function with the `@tf.function ` decorator. I want to get the `dense_shape`, `values` and `indices` from a sparse tensor to calculate the label error rate.\r\n\r\nOutside of the `@tf.function` decorator it works fine, but I want to use the default Keras Model without creating a custom one. I'm using Tensorflow version 2.2.0 on Google Colab with GPU runtime.", "@Victor-Almeida - did you find a solution to this issue? Thank you ", "@AiaHaruv No, I didn't. The workaround is to pass the tensor to a function outside the scope of the `@tf.function` decorator to be able to use `.numpy()`.", "@Victor-Almeida  I'm running into the same issue. Could you post a simple example for you suggestion. \r\n\r\nAlso, the issue I'm running into is from tensorflow object detection repo (https://github.com/tensorflow/models/issues/9778) where the distributed training step is wrapped in tf.function call and not able to access the training metrics returned. \r\nAlternatively, running the training is eager mode is causing drastic difference in speed for these big models. I want to keep the graph mode on and also access the .numpy values.  \r\n\r\n@Sangwon91 \r\nAny suggestions or workarounds are welcome.\r\n\r\nThanks !", "I was working on similar lines but with a 0-dimensional tensor and using just using  seemed to work for me with `@tf.function` too using the built-in type functions here is an example:\r\n\r\n```py\r\n@tf.function\r\ndef samplefunction(x):\r\n    x = tf.convert_to_tensor(x)\r\n    tf.print(int(x)) # tf.print(float(x))\r\n\r\nsamplefunction(1)\r\n```", "The tensorflow offers tf.py_function to deal with any kind of issue you meet in @tf.function. But the performance will be lose extremely when you use the @tf.function though.."]}, {"number": 27490, "title": "ImportError: cannot import name model_fn", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Springdale Linux 7.5 (Verona)\r\n- TensorFlow version: 1.13.1\r\n- TensorFlow-GPU version: 1.12\r\n- Python version: 3.4.9\r\n- Installed using virtualenv? pip? conda?: using pip inside a virtualenv\r\n- CUDA/cuDNN version: 7\r\n\r\n**Describe the problem**\r\n\r\nI'm attempting to use a `layer_norm` with the line:\r\n```\r\nlayer_norm = tf.contrib.layers.layer_norm\r\n```\r\n but am thrown the following stack trace:\r\n```\r\nFile \"projectdir/venv/lib64/python3.4/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"projectdir/venv/lib64/python3.4/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib64/python3.4/importlib/__init__.py\", line 109, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 2254, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\r\n  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\r\n  File \"projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/__init__.py\", line 48, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/tpu/__init__.py\", line 73, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model\r\n  File \"projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 71, in <module>\r\n    from tensorflow.python.estimator import model_fn as model_fn_lib\r\nImportError: cannot import name 'model_fn'\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI ran the lines:\r\n```\r\nimport tensorflow as tf\r\nlayer_norm = tf.contrib.layers.layer_norm\r\n```\r\n", "comments": ["I executed the above two lines in my TF 1.13.1 setup. It passed with warning message. Can you please check if you are importing any other contrib module as well?\r\n```WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", " I slove this problem by `pip uninstall tensorflow_estimator` then `pip install tensorflow_estimator==1.13.0`,my tensorflow_gpu version is 1.13.2", "I had the same issue using **tensorflow object-detection api**, the problem was that` tensorflow-estimator `and `tensorflow` had incompatible versions, so I upgraded `tensorflow-estimator` to the version required by `tensorflow`. Now I have `tensorflow==1.15.0` and `tensorflow-estimator==1.15.1` and the error is gone.", "> I had the same issue using **tensorflow object-detection api**, the problem was that`tensorflow-estimator`and `tensorflow` had incompatible versions, so I upgraded `tensorflow-estimator` to the version required by `tensorflow`. Now I have `tensorflow==1.15.0` and `tensorflow-estimator==1.15.1` and the error is gone.\r\n\r\nThis actually works! Thank you very much!"]}, {"number": 27489, "title": "LSTMBlockFusedCell does not support using outputprojectionwrapper", "body": "I want to replace LSTMCell to LSTMBlockFusedCell for better performance.\r\nbut I have found that the LSTMBlockFusedCell does not have the project operation of the output.\r\nso I add the outputprojectionwrapper to the LSTMBlockFusedCell by `cell = tf.contrib.rnn.OutputProjectionWrapper( tf.contrib.rnn.LSTMBlockFusedCell(config.hidden_size, forget_bias=0.0, cell_clip=config.cell_clip, use_peephole=config.use_peephole), output_size=n_outputs)`, but I got an error `TypeError: The argument 'cell' (<tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2c08f1eb90>) is not an RNNCell: 'output_size' property is missing, 'state_size' property is missing, 'zero_state' method is missing.`\r\nhow to add projection in LSTMBlockFusedCell ?\r\n\r\n**System information**\r\n- TensorFlow version (you are using):conda install tensorflow 1.9.0\r\n- Are you willing to contribute it (Yes/No): Yes", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "OK\uff0cI am sorry\uff0c\r\nbut, if I have a LSTMCell with peephole and projections, How could I do to speedup the Inference time?\r\nthe cudnnCell and LSTMBlockFusedCell all cannot use directly.", "I am closing this issue as it is a support questions more suited for Stackoverflow. Thanks!"]}, {"number": 27488, "title": "tf.function decorator converts ragged tensors to their dense equivalent #26505", "body": "tf.function decorator converts ragged tensors to their dense equivalent #26505 ", "comments": ["This is just for basic review purpose , just to check that I am on right track or not. There are still some changes which I have to add further.", "@edloper would know better whether you're on the right track. I think he was also working on this.\r\n\r\nUnit tests would be good so we know what we're fixing (but check with Edward to see what he's doing so you're not stepping on each other's toes).", "@edloper can you please review it once and tell me my mistakes?", "@edloper  can you please review it once.", "please somebody review this PR", "@alextp can you please review it and tell me, what can I do further?", "Sorry for the delay in responding to this thread.  I have had to explore some of the implications of this change especially with respect to input_signature and TensorSpecs.  I submitted [a change](https://github.com/tensorflow/tensorflow/commit/40006c34f53e2a0b3fe72079dae64168cc2c0d1d) a few days ago that updates tf.function to accept composite tensors (including ragged tensor and sparse tensor), and will follow up with another change to add support for input_signatures.\r\n", "@shashvatshahi1998 could you please resolve the conflicts? Thanks!", "The issue that this PR was meant to address fixed by commit 40006c3."]}, {"number": 27487, "title": "Non-OK-status for CudaLaunchKernel when torch is also imported", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary (pip installed)\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0a0\r\n- Python version: Python 3.6.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: CUDA Version: 10.1? (10.0.130), CUDNN 7.4.2\r\n- GPU model and memory:  GeForce RTX 2080, 7949MiB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nIf torch is imported before tensorflow, tensorflow is unable to use the GPU. It throws the following error:\r\n```\r\n2019-04-03 19:31:08.167586: F tensorflow/core/kernels/random_op_gpu.cu.cc:64] Non-OK-status: CudaLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid configuration argument\r\nAborted (core dumped)\r\n```\r\nWhich is not very helpful for figuring out what the problem is\r\n\r\n**Describe the expected behavior**\r\n\r\nBoth torch and tensorflow can be imported in any order without GPU issues\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nif the import torch line below is commented out, this code works. If tensorflow is imported before torch, it will work as well.\r\n```\r\nfrom time import time\r\n\r\nimport torch\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nfrom predictor_brain import TFBrain\r\n\r\nimport numpy as np\r\ndata = np.random.random((10000, 32))\r\nlabels = np.random.random((10000, 10))\r\n\r\nmodel = tf.keras.Sequential([\r\n# Adds a densely-connected layer with 64 units to the model:\r\nlayers.Dense(64, activation='relu', input_shape=(32,)),\r\n# Add another:\r\nlayers.Dense(64, activation='relu'),\r\n# Add a softmax layer with 10 output units:\r\nlayers.Dense(10, activation='softmax')])\r\n\r\nmodel.compile(optimizer=tf.optimizers.Adam(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\na = time()\r\nhist = model.fit(data, \r\n    labels,\r\n    epochs=10, \r\n    batch_size=32\r\n    )\r\nb = time()\r\nprint(f'time {b-a} seconds') \r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe pytorch version is:  torch==1.0.1.post2\r\n\r\nFrom executing the code above\r\n\r\n```\r\n$ python test.py\r\n2019-04-03 19:31:04.290594: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n FMA                                                                          \r\n2019-04-03 19:31:04.298331: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-04-03 19:31:05.024887: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x4e83c10 executing computations on platform CUDA. Devices:\r\n2019-04-03 19:31:05.024921: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5\r\n2019-04-03 19:31:05.046149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3493095000 Hz\r\n2019-04-03 19:31:05.047498: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x551b970 executing computations on platform Host. Devices:\r\n2019-04-03 19:31:05.047521: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-04-03 19:31:05.048183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:\r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:42:00.0\r\ntotalMemory: 7.76GiB freeMemory: 7.65GiB\r\n2019-04-03 19:31:05.048206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-04-03 19:31:07.731519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-03 19:31:07.731583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0\r\n2019-04-03 19:31:07.731590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N\r\n2019-04-03 19:31:07.732016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6809 MB\r\n memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:42:00.0, compute capability: 7.5)\r\n2019-04-03 19:31:08.167586: F tensorflow/core/kernels/random_op_gpu.cu.cc:64] Non-OK-status: CudaLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, blo\r\nck_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid configuration argument\r\nAborted (core dumped)\r\n```", "comments": ["Exactly the same issue here, any response?", "Any updates on this?", "very good", "any updates?", "any update?", "Same issue! I only installed and imported tensorflow-gpu, no pytorch!", "Exactly the same issue here, any response?", "Same issue here... :-(", "I was not able to reproduce this using TF 2.0 on a P100.  I had to remove the `TFBrain` import since I don't know where I can get that package.\r\n\r\nThis is the output from the program: https://gist.github.com/sanjoy/37c7644327c32cf91738070ab1789984 and this is the output form `pip list`: https://gist.github.com/sanjoy/410d42bc298855aa038c03e9a84d4b81", "any updates?\r\n\r\nFacing this issue with only tensorflow-gpu and multiple graphs (tf 1.14)", "Switching the import of torch and tensorflow did fix it for me. So you have to import `tensorflow` first and then `torch`\r\n```\r\nfrom time import time\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nimport torch\r\n\r\nimport numpy as np\r\ndata = np.random.random((10000, 32))\r\nlabels = np.random.random((10000, 10))\r\n\r\nmodel = tf.keras.Sequential([\r\n# Adds a densely-connected layer with 64 units to the model:\r\nlayers.Dense(64, activation='relu', input_shape=(32,)),\r\n# Add another:\r\nlayers.Dense(64, activation='relu'),\r\n# Add a softmax layer with 10 output units:\r\nlayers.Dense(10, activation='softmax')])\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\na = time()\r\nhist = model.fit(data, \r\n    labels,\r\n    epochs=10, \r\n    batch_size=32\r\n    )\r\nb = time()\r\nprint(f'time {b-a} seconds') \r\n```", "Had a similar issue. As @hoverjmt pointed out. Importing TensorFlow first fixed it for me", "I tried the code on colab with TF v2.5 and didn't face the issue reported ,please check the gist [here ](https://colab.research.google.com/gist/sushreebarsa/afee29f0e5ff31a9f4f67e31e4e2ed11/untitled308.ipynb)..Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27487\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27487\">No</a>\n"]}, {"number": 27486, "title": "What's happened with tensorflow.org?", "body": "What's happened with tensorflow.org? It just show a Service Unavailable hint.", "comments": ["https://www.tensorflow.org is up and running may be you were experiencing internet issues. Please give it a try again. Thanks!", "Thanks, It's available now.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "Browser cache corruption can cause this too;  On the \"service unavailable\" page, type crtl-shift-R to reload the page from source.  "]}, {"number": 27485, "title": "Failed to load the native TensorFlow runtime.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.12.0\r\n- Python version:2.7\r\n- Installed using  conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: \r\n\r\n\r\nImportErrorTraceback (most recent call last)\r\n<ipython-input-1-81ca492b35f0> in <module>()\r\n      7 #os.environ[\"THEANO_FLAGS\"]  = \"device=gpu%d\"%(1)\r\n      8 import numpy as np\r\n----> 9 import tensorflow as tf\r\n     10 #import theano.tensor as T\r\n     11 from keras.utils import np_utils\r\n\r\n/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors", "comments": ["You  have to lower your cuda version to 9.0 since TF 1.12 comes with pre-built binaries to support cuda 9.0\r\nelse you can lower your cuda version to 10.0 and use TF 1.13.1", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27485\">No</a>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27484, "title": "How to save a model after trained and restore the model in another script?", "body": "### Hello guys I need help. I'm begin my studies in TensorFlow and I qhave some questions. For example, I'm using this example in my first neural network (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb). \r\n**My question is, using this example from Oficial TensorFlow (https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough), how to save my model and restore the model saved in another script? Thanks a lot\r\nThe full code is this:**\r\n\r\n`from` __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport os\r\nimport matplotlib.pyplot as plt\r\n\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nprint(\"TensorFlow version: {}\".format(tf.__version__))\r\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))\r\n\r\ntrain_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\r\n\r\ntrain_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\r\n                                           origin=train_dataset_url)\r\n\r\nprint(\"Local copy of the dataset file: {}\".format(train_dataset_fp))\r\n\r\n# column order in CSV file\r\ncolumn_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\r\n\r\nfeature_names = column_names[:-1]\r\nlabel_name = column_names[-1]\r\n\r\nprint(\"Features: {}\".format(feature_names))\r\nprint(\"Label: {}\".format(label_name))\r\n\r\nclass_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\r\n\r\nbatch_size = 32\r\n\r\ntrain_dataset = tf.contrib.data.make_csv_dataset(\r\n    train_dataset_fp,\r\n    batch_size, \r\n    column_names=column_names,\r\n    label_name=label_name,\r\n    num_epochs=1)\r\n\r\nfeatures, labels = next(iter(train_dataset))\r\n\r\nfeatures\r\n\r\n\r\nplt.scatter(features['petal_length'].numpy(),\r\n            features['sepal_length'].numpy(),\r\n            c=labels.numpy(),\r\n            cmap='viridis')\r\n\r\nplt.xlabel(\"Petal length\")\r\nplt.ylabel(\"Sepal length\");\r\n\r\n\r\ndef pack_features_vector(features, labels):\r\n  \"\"\"Pack the features into a single array.\"\"\"\r\n  features = tf.stack(list(features.values()), axis=1)\r\n  return features, labels\r\n\r\n\r\ntrain_dataset = train_dataset.map(pack_features_vector)\r\nfeatures, labels = next(iter(train_dataset))\r\n\r\nprint(features[:5])\r\n\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\r\n  tf.keras.layers.Dense(10, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(3)\r\n])\r\n\r\npredictions = model(features)\r\npredictions[:5]\r\n\r\n\r\ntf.nn.softmax(predictions[:5])\r\n\r\nprint(\"Prediction: {}\".format(tf.argmax(predictions, axis=1)))\r\nprint(\"    Labels: {}\".format(labels))\r\n\r\n\r\ndef loss(model, x, y):\r\n  y_ = model(x)\r\n  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\r\n\r\n\r\nl = loss(model, features, labels)\r\nprint(\"Loss test: {}\".format(l))\r\n\r\n\r\ndef grad(model, inputs, targets):\r\n  with tf.GradientTape() as tape:\r\n    loss_value = loss(model, inputs, targets)\r\n  return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nglobal_step = tf.Variable(0)\r\n\r\nloss_value, grads = grad(model, features, labels)\r\n\r\nprint(\"Step: {}, Initial Loss: {}\".format(global_step.numpy(),\r\n                                          loss_value.numpy()))\r\n\r\noptimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)\r\n\r\nprint(\"Step: {},         Loss: {}\".format(global_step.numpy(),\r\n                                          loss(model, features, labels).numpy()))\r\n\r\n## Note: Rerunning this cell uses the same model variables\r\n\r\nfrom tensorflow import contrib\r\ntfe = contrib.eager\r\n\r\n# keep results for plotting\r\ntrain_loss_results = []\r\ntrain_accuracy_results = []\r\n\r\nnum_epochs = 201\r\n\r\nfor epoch in range(num_epochs):\r\n  epoch_loss_avg = tfe.metrics.Mean()\r\n  epoch_accuracy = tfe.metrics.Accuracy()\r\n\r\n  # Training loop - using batches of 32\r\n  for x, y in train_dataset:\r\n    # Optimize the model\r\n    loss_value, grads = grad(model, x, y)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables),\r\n                              global_step)\r\n\r\n    # Track progress\r\n    epoch_loss_avg(loss_value)  # add current batch loss\r\n    # compare predicted label to actual label\r\n    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\r\n\r\n  # end epoch\r\n  train_loss_results.append(epoch_loss_avg.result())\r\n  train_accuracy_results.append(epoch_accuracy.result())\r\n  \r\n  if epoch % 50 == 0:\r\n    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\r\n                                                                epoch_loss_avg.result(),\r\n                                                                epoch_accuracy.result()))\r\n\r\nfig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\r\nfig.suptitle('Training Metrics')\r\n\r\naxes[0].set_ylabel(\"Loss\", fontsize=14)\r\naxes[0].plot(train_loss_results)\r\n\r\naxes[1].set_ylabel(\"Accuracy\", fontsize=14)\r\naxes[1].set_xlabel(\"Epoch\", fontsize=14)\r\naxes[1].plot(train_accuracy_results);\r\n\r\ntest_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\r\n\r\ntest_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\r\n                                  origin=test_url)\r\n\r\n\r\ntest_dataset = tf.contrib.data.make_csv_dataset(\r\n    test_fp,\r\n    batch_size, \r\n    column_names=column_names,\r\n    label_name='species',\r\n    num_epochs=1,\r\n    shuffle=False)\r\n\r\ntest_dataset = test_dataset.map(pack_features_vector)\r\n\r\ntest_accuracy = tfe.metrics.Accuracy()\r\n\r\nfor (x, y) in test_dataset:\r\n  logits = model(x)\r\n  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\r\n  test_accuracy(prediction, y)\r\n\r\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\r\n\r\ntf.stack([y,prediction],axis=1)\r\n\r\n\r\npredict_dataset = tf.convert_to_tensor([\r\n    [5.1, 3.3, 1.7, 0.5,],\r\n    [5.9, 3.0, 4.2, 1.5,],\r\n    [6.9, 3.1, 5.4, 2.1]\r\n])\r\n\r\npredictions = model(predict_dataset)\r\n\r\nfor i, logits in enumerate(predictions):\r\n  class_idx = tf.argmax(logits).numpy()\r\n  p = tf.nn.softmax(logits)[class_idx]\r\n  name = class_names[class_idx]\r\n  print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))\r\n\r\n`\r\n\r\n\r\n\r\n", "comments": ["Have you asked on stackoverflow?", "@vrjesus This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "@vrjesus : maybe this StackOverflow page helps: https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model?rq=1\r\n\r\n@muddham : very welcoming awesome wording. Is it OK for you if this is copied and used as a template? ", "@serv-inc Sure. You are welcome to use it as  a template.\r\n@vrjesus This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\nI am closing the issue. Please check [here](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/keras/save_and_restore_models.ipynb) for a detailed tutorial on saving and restoring model. Thanks!\r\n"]}, {"number": 27483, "title": ".", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 27482, "title": "Have option to extend tf.image beyond 3-4 channels", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@CountingClouds Could you provide more details on the feature and its context? What is missing currently and also mention for which use cases, this feature will be helpful. thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27481, "title": "Fixed a Typo", "body": "Updated Generate2.py", "comments": ["@MarkDaoust Please Review this!!"]}, {"number": 27480, "title": "[TF 2.0 Build] Fix bug caused by typo in check_bazel_version()", "body": "This PR resolves issue #27450.\r\nPreviously typo enters '0.24' instead of '0.24.0' for max bazel version in call to 'check_bazel_version' function, causing build to fail when compiling from source. Such typo is now fixed.\r\nThis PR may also relates to @perfinion 's comment in #27399 where he states that he will re-access min max version everywhere else later.", "comments": ["You are welcomed! By the way, this is my first time contributing to Tensorflow. Is there anything I can improve upon in terms of opening issue and making PR?", "> You are welcomed! By the way, this is my first time contributing to Tensorflow. Is there anything I can improve upon in terms of opening issue and making PR?\r\n\r\nCongrats :) The PR is great. One small thing for future PRs i'll point out is that github has special tags, if you put `Closes: https://github.com/tensorflow/tensorflow/issues/27450` at the bottom of the commit then github can automatically close the corresponding issue when the PR is merged. (no need to change this PR tho). and Thanks for the PR :).", "@perfinion Thank you so much for the advice!", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 27479, "title": "modify doc's example of tf.print to wotk TF 2.0", "body": "fixed #27478", "comments": ["@hksonngan can you please resolve conflicts", "@rthadur done.", "This PR is in the process of being rolled back since it broke some tests. Could you return the output of `print_v2` (should be an `Operation`) rather than just calling it? Then we can re-submit.", "@allenlavoie like this [4a5cfae](https://github.com/hksonngan/tensorflow/commit/4a5cfae6017c2b4f6e541b400d513e5fffda9265)?", "Yes, that looks right to me."]}, {"number": 27478, "title": "tf.print the example doesn't work with TF 2.0", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/print\r\n\r\n\r\n**Describe the documentation issue**\r\nthe example doesn't work with TF 2.0\r\n```python\r\n    tf.enable_eager_execution()\r\n    @tf.contrib.eager.defun\r\n    def f():\r\n        tensor = tf.range(10)\r\n        tf.print(tensor, output_stream=sys.stderr)\r\n        return tensor\r\n    range_tensor = f()\r\n```\r\n```python\r\n    sess = tf.Session()\r\n    with sess.as_default():\r\n        tensor = tf.range(10)\r\n        print_op = tf.print(\"tensors:\", tensor, {2: tensor * 2},\r\n                            output_stream=sys.stdout)\r\n        with tf.control_dependencies([print_op]):\r\n          tripled_tensor = tensor * 3\r\n        sess.run(tripled_tensor)\r\n```\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": []}, {"number": 27477, "title": "building pip package causes IndexError: list index out of range", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): VC14\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nTensorFlow successfully compiles a windows version, but building a pip package fails\r\n\r\n```\r\n(tf_113) C:\\Users\\pvenkat2\\Desktop\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nINFO: Elapsed time: 41316.964s, Critical Path: 35436.10s\r\nINFO: 4534 processes: 4534 local.\r\nINFO: Build completed successfully, 6007 total actions\r\n\r\n(tf_113) C:\\Users\\pvenkat2\\Desktop\\tensorflow>bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/Users/pvenkat2/Desktop/tensorflow_pkg\r\nWed Apr 3 09:43:33 PDT 2019 : === Preparing sources in dir: /tmp/tmp.sDKWoKSGMv\r\nUnzipping simple_console_for_windows.zip to create runfiles tree...\r\nUnzip finished.\r\n/c/Users/pvenkat2/Desktop/tensorflow /c/Users/pvenkat2/Desktop/tensorflow\r\n/c/Users/pvenkat2/Desktop/tensorflow\r\nWed Apr 3 10:43:07 PDT 2019 : === Building wheel\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow\\include\\tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow\\include\\Eigen'\r\nwarning: no files found matching '*.h' under directory 'tensorflow\\include\\google'\r\nwarning: no files found matching '*' under directory 'tensorflow\\include\\third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow\\include\\unsupported'\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 304, in <module>\r\n    keywords='tensorflow tensor machine learning',\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\dist.py\", line 955, in run_commands\r\n    self.run_command(cmd)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 179, in run\r\n    self.run_command('build')\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\command\\build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\site-packages\\setuptools\\command\\build_ext.py\", line 75, in run\r\n    _build_ext.run(self)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\command\\build_ext.py\", line 339, in run\r\n    self.build_extensions()\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\command\\build_ext.py\", line 448, in build_extensions\r\n    self._build_extensions_serial()\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\command\\build_ext.py\", line 473, in _build_extensions_serial\r\n    self.build_extension(ext)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\site-packages\\setuptools\\command\\build_ext.py\", line 196, in build_extension\r\n    _build_ext.build_extension(self, ext)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\command\\build_ext.py\", line 558, in build_extension\r\n    target_lang=language)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\ccompiler.py\", line 717, in link_shared_object\r\n    extra_preargs, extra_postargs, build_temp, target_lang)\r\n  File \"C:\\Users\\pvenkat2\\AppData\\Local\\conda\\conda\\envs\\tf_113\\lib\\distutils\\_msvccompiler.py\", line 461, in link\r\n    build_temp = os.path.dirname(objects[0])\r\nIndexError: list index out of range\r\n\r\n```\r\n", "comments": ["on windows, you need to add this flag when running bazel build on the pip package:\r\n`--define=no_tensorflow_py_deps=true`\r\n\r\nSo, the pip package build command becomes:\r\n`bazel build --define=no_tensorflow_py_deps=true--config=opt //tensorflow/tools/pip_package:build_pip_package`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27477\">No</a>\n"]}, {"number": 27476, "title": "TFLiteConverter: converter.convert() does not work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): not applicable\r\n- GCC/Compiler version (if compiling from source): not applicable\r\n- CUDA/cuDNN version: not applicable\r\n- GPU model and memory: not applicable\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying make an new tflite quantized.\r\nThe code bellow run normally until the line:\r\n`input_arrays = converter.get_input_arrays()`\r\n\r\nCan you help me please?\r\n\r\n**_Follows the error:_**\r\n\r\nD:\\quantization\\venv\\Scripts\\python.exe D:/quantization/venv/quantization.py\r\nTF VERSION:  1.13.1\r\n2019-04-03 15:21:12.605076: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nWARNING:tensorflow:From D:/quantization/venv/quantization.py:25: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.gfile.GFile.\r\nTraceback (most recent call last):\r\n  File \"D:/quantization/venv/quantization.py\", line 30, in <module>\r\n    input_arrays = converter.get_input_arrays()\r\n  File \"D:\\quantization\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 471, in get_input_arrays\r\n    return [_tensor_name(tensor) for tensor in self._input_tensors]\r\n  File \"D:\\quantization\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 471, in <listcomp>\r\nLoading Graph\r\n    return [_tensor_name(tensor) for tensor in self._input_tensors]\r\n  File \"D:\\quantization\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 217, in tensor_name\r\n    return x.name.split(\":\")[0]\r\nAttributeError: 'str' object has no attribute 'name'\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\ntf.enable_eager_execution()\r\n\r\nimport sys\r\nimport os\r\nif sys.version_info.major >= 3:\r\n    import pathlib\r\nelse:\r\n    import pathlib2 as pathlib\r\n\r\npbfiles_path = \"D:\\\\Ricardo\\\\Ambientes Virtuals do Python\\\\virtualTFr1.13\\\\Scripts\\\\pbfiles\"\r\nsaved_model_dir = str(sorted(pathlib.Path(pbfiles_path).glob(\"*.pbtxt\" or \"*.pb\"))[-1])\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\r\nwith tf.Graph().as_default() as graph: \r\n\r\n\r\n    with tf.Session() as sess:\r\n        print(\"Loading Graph\")\r\n\r\n        modelFile = '/pbfiles/saved_model.pb'\r\n        with gfile.FastGFile(modelFile, 'rb') as f:\r\n            converter = tf.lite.TFLiteConverter.from_session(sess,\r\n                                                             input_tensors={\"input_1\"},\r\n                                                             output_tensors={\"conv2d_19/Sigmoid\"})\r\n            converter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\n            input_arrays = converter.get_input_arrays()\r\n            tflite_model = converter.convert()\r\n            open(\"quant_converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n", "comments": ["@ricardobnjunior you can try to define the minmax value and quantized_input_stats, hope this should help.\r\n\r\n\r\nRegards\r\nAmit", "@ricardobnjunior Please let us know whether @amitsrivastava78 suggestion helped in resolving the issue? If it was resolved already, please close the issue. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27475, "title": "Tests for ParallelInterleaveDatasetOp", "body": "This PR adds \r\n\r\n- the comparison function to identify whether two tensor vectors have the same elements regarding/regardless of order.\r\n\r\n- Tests for `ParallelInterleaveDatasetOp`.\r\n\r\ncc: @jsimsa \r\n", "comments": ["@jsimsa I will use this thread to communicate with you about the flaky issue in ParallelInterleaveDatasetOpTest, because I did not receive the notification when you pinged me under the commit(https://github.com/tensorflow/tensorflow/commit/32ff91cf4284861994b6bba2cfb8deeb2f840b7c).  \r\n\r\nI'm wondering if you are able to see which test case causes the timeout? That will be helpful for me to debug as I could not reproduce it on my mac yet.", "It is the test that I disabled. Here is a stack trace:\r\n\r\n```\r\n[ RUN      ] ParallelInterleaveDatasetOpTest/ParameterizedParallelInterleaveDatasetOpTest.Roundtrip/0\r\n*** SIGTERM received by PID 8235 (TID 8235) from PID 1; stack trace: ***\r\nPC: @     0x7fd078133ccd  (unknown)  prodkernel::api::base::InlinedSyscall6()\r\n    @     0x7fd077f83e15       1376  FailureSignalHandler()\r\n    @     0x7fd07786c9a0  (unknown)  (unknown)\r\n    @     0x7fd078133c5a         64  prodkernel::api::base::Futex::WaitBitsetAbsoluteTimeout()\r\n    @     0x7fd078133b78         48  prodkernel::api::base::Futex::WaitAbsoluteTimeout()\r\n    @     0x7fd078133b9f         32  prodkernel::api::base::Futex::Wait()\r\n    @     0x7fd0781339de         64  prodkernel::api::base::Futex::WaitUntil()\r\n    @     0x7fd0781335c5        304  absl::synchronization_internal::Waiter::Wait()\r\n    @     0x7fd077f7ed7f        320  AbslInternalPerThreadSemWait\r\n    @     0x7fd07814602d         32  absl::synchronization_internal::PerThreadSem::Wait()\r\n    @     0x7fd078145b74         64  absl::Mutex::DecrementSynchSem()\r\n    @     0x7fd078142119        304  absl::CondVar::WaitCommon()\r\n    @     0x7fd07814234a         64  absl::CondVar::Wait()\r\n    @     0x5600a0440830         48  tensorflow::condition_variable::wait()\r\n    @     0x5600a045775e        112  tensorflow::data::(anonymous namespace)::ParallelInterleaveDatasetOp::Dataset::ParallelInterleaveIterator::GetNextInternal()\r\n    @     0x5600a0468518        224  tensorflow::data::DatasetBaseIterator::GetNext()\r\n    @     0x7fd081319b03       1760  tensorflow::data::(anonymous namespace)::ParameterizedParallelInterleaveDatasetOpTest_Roundtrip_Test::TestBody()\r\n    @     0x7fd0809c5784         96  testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n    @     0x7fd0809b4af2        112  testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n    @     0x7fd0809a47c3         96  testing::Test::Run()\r\n    @     0x7fd0809a4f4d        112  testing::TestInfo::Run()\r\n    @     0x7fd0809a5508        112  testing::TestSuite::Run()\r\n    @     0x7fd0809ae4f5        256  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x7fd0809c97f4         96  testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n    @     0x7fd0809b6ee2        128  testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n    @     0x7fd0809ae0aa        112  testing::UnitTest::Run()\r\n    @     0x7fd081092931         16  RUN_ALL_TESTS()\r\n    @     0x7fd0810926bd         32  main\r\n    @     0x7fd0776dabbd        208  __libc_start_main\r\n    @     0x5600a04023e9  (unknown)  _start\r\n```", "I changed `DISABLED_Roundtrip` --> `Roundtrip` to run the tests multiple times, but did not get any failures. The stack trace you posted here is helpful. Will take further look and keep you updated. ", "I could reproduce the flaky issue now using the command ` bazel test -c opt //tensorflow/core/kernels/data:parallel_interleave_dataset_op_test --runs_per_test=20000`."]}, {"number": 27474, "title": "Shutdown crash while running this script (most times)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMS WINDOWS 10 x64\r\n\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n\r\n- TensorFlow version (use command below):\r\n2.0.0-a0\r\n\r\n- Python version:\r\nPython 3.7.1\r\n\r\n- CUDA/cuDNN version:\r\ncuda_10.1.105_418.96_win10.exe\r\ncudnn-10.1-windows10-x64-v7.5.0.56.zip\r\n\r\n- GPU model and memory:\r\nGforce GTX 1050 Ti (DELL laptop)\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nEXECUTION OF LINE:\r\nC:\\Users\\steph>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'\r\n\r\n\r\n**Describe the current behavior**\r\nDurring running of this script, the computer shutdown crashes and reboots.\r\n\r\n**Describe the expected behavior**\r\nFinish the 100 Epochs of training and save the model *.h5\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n[Python Scripts.zip](https://github.com/tensorflow/tensorflow/files/3040051/Python.Scripts.zip)\r\n\r\nThe data is from Kaggle dogs-vs-cats.zip (file too big to attach: extract all to same subdirectory)\r\n\r\nThe crash seems to occur most often during the 100 Epochs (2nd) fit.\r\n\r\n", "comments": ["I am inclined to think it is power supply issue. TF utilizes max cpu, gpu capabilities and draws high power. Can you please use [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true) and execute your code snippet? To use gpu computation in google colab select, edit > notebook settings > hardware accelerator. Let me know how it progresses. Thanks!", "Dell thinks the problem is with the OS and wants to reinstall it.\n\nI am now getting another error. Here is the output of:\n-------------------------------------------------------\n\nimport tensorflow as tf\n\nprint(tf.__version__)\nprint(tf.test.is_gpu_available())\n\n-------------------------------------------------------\nC:\\Users\\steph\\PycharmProjects\\Image1\\TEMP_1\\venv\\Scripts\\python.exe\nC:/Users/steph/PycharmProjects/Image1/TEMP_1/mnist_conv2d_1a.py\n2.0.0-alpha0\n2019-04-12 20:21:26.598567: I\ntensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports\ninstructions that this TensorFlow binary was not compiled to use: AVX2\n2019-04-12 20:21:26.603555: I\ntensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully\nopened dynamic library nvcuda.dll\n2019-04-12 20:21:26.613716: E\ntensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit:\nCUDA_ERROR_UNKNOWN: unknown error\nFalse\n2019-04-12 20:21:26.618324: I\ntensorflow/stream_executor/cuda/cuda_diagnostics.cc:166] retrieving CUDA\ndiagnostic information for host: DESKTOP-KNR6M7Q\n2019-04-12 20:21:26.618784: I\ntensorflow/stream_executor/cuda/cuda_diagnostics.cc:173] hostname:\nDESKTOP-KNR6M7Q\n-------------------------------------------------------\n\nStephen Dicke\n\nOn Wed, Apr 10, 2019 at 8:58 PM ymodak <notifications@github.com> wrote:\n\n> I am inclined to think it is power supply issue. TF utilizes max cpu, gpu\n> capabilities and draws high power. Can you please use google colab\n> <https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true>\n> and execute your code snippet? To use gpu computation in google colab\n> select, edit > notebook settings > hardware accelerator. Let me know how it\n> progresses. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27474#issuecomment-481934867>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AmJ4iAEA-jQYh1hlyG0FKjWG6iCt4S_Uks5vfpa-gaJpZM4cbF-I>\n> .\n>\n", "I think you forgot to paste your new error stack here. However, since its a new error message and not related to original issue reported. Can you please post a new issue explaining it? We would like to keep each issue thread focusing on one problem. Thanks!"]}, {"number": 27473, "title": "Multiclass AUC", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1.\r\n- Are you willing to contribute it (Yes/No): Yes.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMulti-class AUC is a measure to examine the performance of multi-class classifiers. It is a natural extension of the well known and widely used AUC (Area Under the ROC curve) metric, which is defined for only binary classifiers. This measure was introduced in the paper 'Hand, David J., and Robert J. Till. \"A simple generalization of the area under the ROC curve for multiple class classification problems.\" Machine learning 45, no. 2 (2001): 171-186'. It is widely used as the original paper currently has 1435 citation. Multi-class AUC reduces to the standard AUC in case of two classes.\r\n\r\n**Will this change the current api? How?**\r\nNo. It will be just an additional function which can be used as a metric.\r\n\r\n**Who will benefit with this feature?**\r\nMachine learning practitioners and researchers, especially those in the medical field, for whom class separability and false positives are very important.\r\n\r\n**Any Other info.**\r\nI wrote the TensorFlow implementation and will be happy to contribute.", "comments": ["This probably should be added to the add ons repo: https://github.com/tensorflow/addons. ", "@pavithrasv  Oh, ok. I can prepare a PR or should I create a feature request there?", "I think feature request would be best for now, then we can decide whether it would be useful as an addon."]}, {"number": 27472, "title": "10x performance loss when using eager execution with tf.keras.fit(tf.data.Dataset) ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS (Bionic Beaver)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen training `tf.keras.fit(dataset)` with eager execution, where dataset is a `tf.data.Dataset`, I am finding a ~10x performance loss as compared to turning off eager execution. \r\n\r\n**Describe the expected behavior**\r\nIn the code attached below, I have tested training with and without eager execution, and with and without tf.data.Dataset.\r\n\r\nHere are the training times for one epoch of training on a 4 core CPU:\r\nEager               + tf.data.Dataset : 219s  - 22ms/step\r\nWithout Eager + tf.data.Dataset : 25s  - 3ms/step\r\nEager               + numpy dataset : 26s  - 259us/sample\r\nWithout Eager + numpy dataset : 26s  - 257us/sample\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nuse_eager     = True\r\nuse_TFDataset = True\r\n\r\nif not use_eager:\r\n    tf.compat.v1.disable_eager_execution()\r\n\r\n# Build dataset\r\nn_data = 10**5\r\nmy_data = np.random.random((n_data,10,1))\r\nmy_targets = np.random.randint(0,2,(n_data,1))\r\ndata = ({'x_input':my_data}, {'target':my_targets})\r\n\r\n# Create tf.data.Dataset\r\nBATCH_SIZE = 10\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\ndataset = dataset.batch(BATCH_SIZE)\r\ndataset = dataset.prefetch(1)\r\n\r\n#Build model\r\nx_input = tf.keras.layers.Input((None,1), name='x_input')\r\nRNN = tf.keras.layers.SimpleRNN(100, name='RNN')(x_input)\r\nhidden = tf.keras.layers.Dense(100, name='hidden')(RNN)\r\ndense = tf.keras.layers.Dense(1, name='target')(hidden)\r\nmy_model = tf.keras.models.Model(inputs = [x_input], outputs = [dense])\r\nmy_model.compile(optimizer='SGD', loss = 'binary_crossentropy')\r\n\r\n# Train model\r\nif use_TFDataset:\r\n    my_model.fit(dataset, epochs = 1, steps_per_epoch=n_data//BATCH_SIZE) # divide by BATCH_SIZE to keep the number of training steps the same\r\nelse:\r\n    my_model.fit(x = my_data, y = my_targets, epochs = 1, batch_size= BATCH_SIZE)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I could reproduce the issue with 2.0.0-alpha0. Even-though the time/step is little different from what you have listed above but the trend is similar. Thanks! ", "I'm not able to reproduce your 22 ms/step results. I tweaked your example (just to add some extra logging):\r\n```\r\nimport functools\r\nimport multiprocessing\r\nimport timeit\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nuse_eager     = True\r\nuse_TFDataset = True\r\n\r\n# Explicitly set since 1.x and 2.0 have different defaults\r\nif use_eager:\r\n  tf.compat.v1.enable_eager_execution()\r\nelse:\r\n  tf.compat.v1.disable_eager_execution()\r\n\r\n# Build dataset\r\nn_data = int(1e5)\r\nmy_data = np.random.random((n_data,10,1))\r\nmy_targets = np.random.randint(0,2,(n_data,1))\r\ndata = ({'x_input':my_data}, {'target':my_targets})\r\n\r\n# Create tf.data.Dataset\r\nBATCH_SIZE = 10\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\ndataset = dataset.batch(BATCH_SIZE)\r\ndataset = dataset.prefetch(1)\r\n\r\n#Build model\r\nx_input = tf.keras.layers.Input((None,1), name='x_input')\r\nRNN = tf.keras.layers.SimpleRNN(100, name='RNN')(x_input)\r\nhidden = tf.keras.layers.Dense(100, name='hidden')(RNN)\r\ndense = tf.keras.layers.Dense(1, name='target')(hidden)\r\nmy_model = tf.keras.models.Model(inputs = [x_input], outputs = [dense])\r\nmy_model.compile(optimizer='SGD', loss = 'binary_crossentropy')\r\n\r\n# Fit on one step to create a train_fn\r\nmy_model.fit(dataset, epochs = 1, steps_per_epoch=1)\r\n\r\nTIME_IN_GRAPH = 0.\r\ndef time_call(fn):\r\n  @functools.wraps(fn)\r\n  def timed_fn(*args, **kwargs):\r\n    fn_start_time = timeit.default_timer()\r\n    outputs = fn(*args, **kwargs)\r\n    call_time = timeit.default_timer() - fn_start_time\r\n    global TIME_IN_GRAPH\r\n    TIME_IN_GRAPH += call_time\r\n    return outputs\r\n  return timed_fn\r\n\r\nmy_model.train_function = time_call(my_model.train_function)\r\n\r\n\r\nst = timeit.default_timer()\r\n\r\n# Train model\r\nif use_TFDataset:\r\n  my_model.fit(dataset, epochs = 1, steps_per_epoch=n_data//BATCH_SIZE) # divide by BATCH_SIZE to keep the number of training steps the same\r\nelse:\r\n  my_model.fit(x = my_data, y = my_targets, epochs = 1, batch_size= BATCH_SIZE)\r\nrun_time = timeit.default_timer() - st\r\nprint(\"CPU count: {}\".format(multiprocessing.cpu_count()))\r\nprint(\"{:.1f} us / sample\".format(run_time / n_data * 1e6))\r\nprint(\"{:.1f}% of time spent in tf runtime\".format(TIME_IN_GRAPH / run_time * 100))\r\n```\r\n\r\nAnd when I run in colab with `tf-nightly-2.0-preview` I get:\r\n```\r\n10000/10000 [==============================] - 43s 4ms/step - loss: 0.7020\r\nCPU count: 2\r\n429.8 us / sample\r\n66.3% of time spent in tf runtime\r\n```\r\nand with `use_eager=False`:\r\n```\r\n10000/10000 [==============================] - 32s 3ms/step - loss: 7.7126\r\nCPU count: 2\r\n320.2 us / sample\r\n82.9% of time spent in tf runtime\r\n```\r\n\r\nThe % in runtime is measuring the time actually spent running the step; the other part is overhead from other bookkeeping in the run loop code. So you can see that the actual time to run the ops is very similar, and the eager path is just spending more time in Python. There are plans to optimize the run code, but 1-1.5 ms/step isn't terribly onerous so other things have taken priority.\r\n\r\nIn general, it's hard to get good performance on tiny models because overheads dominate. If you increase the batch size to 100 you get:\r\n```\r\n1000/1000 [==============================] - 7s 7ms/step - loss: 7.6920\r\nCPU count: 2\r\n70.6 us / sample\r\n77.9% of time spent in tf runtime\r\n```\r\n\r\nand batch_size=1000\r\n```\r\n100/100 [==============================] - 4s 40ms/step - loss: 0.8114\r\nCPU count: 2\r\n40.5 us / sample\r\n90.7% of time spent in tf runtime\r\n```", "If I install tensorflow with `!pip install tensorflow==2.0.0-alpha0` in colab, I get similar performance to what I reported in the original post with `use_eager=True`:\r\n```\r\n10000/10000 [==============================] - 185s 19ms/step - loss: 0.6997\r\nCPU count: 2\r\n1853.4 us / sample\r\n```\r\nHowever, if I install tensorflow with `!pip install tf-nightly-2.0-preview` which (as of Apr. 25) gives me tf version `2.0.0-dev20190424` I get the same performance as you with `use_eager=True`\r\n```\r\n10000/10000 [==============================] - 35s 3ms/step - loss: 0.6998\r\nCPU count: 2\r\n349.6 us / sample\r\n```\r\nSo it seems like this problem somehow must have been fixed between the version in `2.0.0-alpha0` and  `2.0.0-dev20190424`.", "So setting `run_eagerly=use_eager` in `compile` causes the 10x drop in tf-nightly-2.0-preview. (You'll have to remove the monkey patch on the execution function in my example, because it goes through a different path.)\r\n\r\nFor context, when eager execution is enabled (in the global tensorflow context), keras will still try to encapsulate the model in a Graph and then use that Graph to create an eager function. However if it can't do that it will fall back to a pure eager mode. (That's what `run_eagerly=True` forces) So presumably what happened is that since the 2.0 preview was cut whatever was forcing the model out of pure eager mode has been fixed, so it can now take advantage of the fast path.\r\n\r\nI set the middle dense layer to have 1000000 units, and the difference dropped to ~20%, because at that point there is a lot more time spend doing matmuls in the c++ backend to amortize the extra overhead from not having a graph.\r\n\r\nI'm going to go ahead and close this since it seems to be fixed now. Thanks for the report!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27472\">No</a>\n"]}, {"number": 27471, "title": "Linking of _pywrap_tensorflow_internal.so fails with no obvious cause", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 18.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile device, laptop\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: source\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 6.5.0\r\n- CUDA/cuDNN version: cuda 10.0 / cudnn 7\r\n- GPU model and memory: GeForce GT 650M\r\n\r\n**Describe the problem**\r\n\r\nSteps to reproduce:\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout v1.13.1\r\n./configure\r\nbazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nOutput from building, building fails during linking:\r\n\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Invocation ID: f796fcbd-0770-4160-9f04-3535b3dcccbd\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/python/BUILD:2986:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/python/BUILD:77:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/gan/BUILD:136:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/bfh/extra/repos/tensorflow/tensorflow/python/BUILD:4057:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/bfh/.cache/bazel/_bazel_bfh/530929906cac2e0c8d776b7911283383/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-7 \\\r\n    PATH=/bin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/bfh/.pyenv/versions/3.7.2/bin/python \\\r\n    PYTHON_LIB_PATH=/home/bfh/.pyenv/versions/3.7.2/lib/python3.7/site-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.0 \\\r\n    TF_CUDA_VERSION=10.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/k8-opt/bin/_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Wl,--version-script bazel-out/k8-opt/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-soname,_pywrap_tensorflow_internal.so -Wl,-z,muldefs -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 15.002s, Critical Path: 14.56s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nSo it seems that linking of `_pywrap_tensorflow_internal.so` fails. However I see no way of diagnosing why that linking failed. The setup should be pretty straight-forward, so I am really wondering why it fails.\r\n\r\nAny and all help is very appreciated. Thanks in advance.", "comments": ["After trying multiple configuration combinations, it seems to work if I disable XLA JIT support. I wonder why that is, and if I could possibly get it to compile with it enabled.", "Apologies for the delay in response. Your graphics card [GeForce GT 650M](https://developer.nvidia.com/cuda-gpus) has cuda compute capability (ccc) of 3.0 TF GPU requires ccc 3.5 or higher. Thanks!\r\n"]}, {"number": 27470, "title": "pip install tf-nightly-2.0-preview No matching distribution found for python 3.7", "body": "Has anything changed with the pip naming of this? I see it was updated 7 hours ago and I think it worked yesterday.\r\n\r\nCould also be something to do with the pip chain.", "comments": ["@cottrell  Please provide details about what platform you are using (operating system, architecture). Also, did you compile from source or install a binary?\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Can you you confirm that that release is actually disted and available? python 3.7 tf-nightly-2.0-preview.\r\n\r\nI suspect it isn't and no amount of details beyond python version and pip name will help. \r\n\r\nIf not, I will provide more.", "And I am on Mac OS trying this now. Will try later on Ubuntu.\r\n", "I can confirm this is an issue with Python 3.7.2, at least on macOS 10.14.4. Installation **does** work on Python 3.6.8.\r\n\r\nBoth tests were conducted in virtual environments created using `python3 -m venv` and `python3.6 -m venv`, respectively.\r\n", "I have also tested across two MacOS distribs on python 3.6 and 3.7 and one linux on python 3.7, 3.6. \r\n\r\nIt looks like a problem *only* with (MacOS, python 3.7). The other combinations worked as far as I could tell. \r\n\r\n\r\nHere are some details from one instance:\r\n\r\n\r\n(37) ~ $ uname -a\r\nDarwin MacBook-Pro-3.local 17.7.0 Darwin Kernel Version 17.7.0: Fri Nov  2 20:43:16 PDT 2018; root:xnu-4570.71.17~1/RELEASE_X86_64 x86_64\r\n(37) ~ $ python --version\r\nPython 3.7.3", "Can confirm as well, use 3.6", "This is an important issue for me as well, because my team has upgraded to 3.7 and this prevents my dev environment from matching production.", "Any updates on this? I thought we were going to be serious with 3.7 support at the start of this year.", "I think we fixed this for nightlies:\r\nhttps://pypi.org/project/tf-nightly-2.0-preview/#files\r\n\r\nI also see 3.7 whl files for 2.0 beta 1:\r\nhttps://pypi.org/project/tensorflow/2.0.0b1/#files\r\n\r\nI will mark this as fixed, please let us know if these are not the packages you are looking for.", "I am still getting this error with Python 3.7.3, Tensorflow-2.0.0-alpha0 and macOS 10.14.6", "Am still getting this. Can only install 2.0.0b1", "Am wondering if it is picking up the MacOS versions as different. Am on 10.14.6 (18G84) at the moment and the nightlies are for 10.9 if I'm reading it correctly.", "While try `!pip install tf-nightly-2.0-preview` I am still getting this error:\r\n\r\n> Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)\r\n\r\nMy environment is: Python 3.7.3, Windows 10 build 1903", "Can you post the output of `pip debug --verbose`?", "@mihaimaruseac , \r\nI did `pip install tf-nightly-2.0-preview==2.0.0.dev20190909 debug --verbose`\r\nand produced full output is in attachment.\r\n[pip_install_tf-nightly-2.0-preview_debug.log](https://github.com/tensorflow/tensorflow/files/3594621/pip_install_tf-nightly-2.0-preview_debug.log)\r\n\r\nFor all available files got message:\r\n\r\n> it is not compatible with this Python\r\n>     Skipping link\r\n\r\nAlso my python's version is:\r\n\r\n> (base) C:\\Users\\eerem>python -V\r\n> Python 3.7.3", "`pip debug --verbose` is different than `pip install debug --verbose`. The second tries to install a package called `debug`, whereas the first one prints debug information about your pip environment.\r\n\r\nPlease post output of `pip debug --verbose`", "@mihaimaruseac , here is output for `pip debug --verbose`\r\n\r\n> pip version: pip 19.2.3 from c:\\anaconda3\\lib\\site-packages\\pip (python 3.7)\r\n> sys.version: 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\r\n> sys.executable: c:\\anaconda3\\python.exe\r\n> sys.getdefaultencoding: utf-8\r\n> sys.getfilesystemencoding: utf-8\r\n> locale.getpreferredencoding: cp1251\r\n> sys.platform: win32\r\n> sys.implementation:\r\n>   name: cpython\r\n> Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\r\n> Config variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect\r\n> Compatible tags: 14\r\n>   cp37-cp37m-win_amd64\r\n>   cp37-none-win_amd64\r\n>   py3-none-win_amd64\r\n>   cp37-none-any\r\n>   cp3-none-any\r\n>   py37-none-any\r\n>   py3-none-any\r\n>   py36-none-any\r\n>   py35-none-any\r\n>   py34-none-any\r\n>   py33-none-any\r\n>   py32-none-any\r\n>   py31-none-any\r\n>   py30-none-any\r\n\r\n[pip_debug_output.log](https://github.com/tensorflow/tensorflow/files/3597003/pip_debug_output.log)\r\n\r\nPreviously I was confused by message ERROR: unknown command \"debug\", but now realized that had pip version 19.1.1 that is not the last one.", "`cp37-cp37m-win_amd64` is not present at [tf-nightly-2.0-preview files](https://pypi.org/project/tf-nightly-2.0-preview/#files)\r\n\r\nSeems we are not releasing windows pips for 2.0 nightly on python 3.7.\r\n\r\nCan you try instead with the 2.0 rc0 release? `pip install tensorflow==2.0rc0` should work.", "All other releases as  2.0.0-alpha0, 2.0.0-beta0, 2.0.0-beta1, 2.0.0-rc0 are installed without that problem. Only tf-nightly-2.0-preview is affected. I need the nightly build because I faced with not closed functional issues in main releases.", "If the issue works in nightly it will also land in the next release.\r\n\r\nWe will work on providing python 3.7 for nightly.", "Always usage of stable version of tf-nightly is recommended, use command\r\npip install tf-nightly", "Locking conversation as issue is already resolved."]}, {"number": 27469, "title": "Strange cuda error", "body": "**System information**\r\n-  Ubuntu 16.04\r\n- TensorFlow installed from source\r\n- TensorFlow newest version\r\n- Python version 3.4\r\n- Installed using virtualenv\r\n- Bazel version 1.22.0\r\n- CUDA/cuDNN version 10.0/7\r\n- GPU Tesla K10.G1.8GB\r\n\r\nI compiled TensorFlow from source because compute capatibility is only 3.0.\r\nDuring training I got the strange error below. I guess it is a CUDA problem but I am not sure. Do you have any idea?\r\n\r\n**ERROR MESSAGE:**\r\n\r\nF tensorflow/stream_executor/cuda/cuda_driver.cc:184] Check failed: is_host_ptr == points_to_host_memory (0 vs. 1)dst pointer is not actually on GPU: 0x4304b80500\r\nFatal Python error: Aborted\r\n\r\nThread 0x00007f204affd700 (most recent call first):\r\n  File \"/usr/lib/python3.4/threading.py\", line 290 in wait\r\n  File \"/usr/lib/python3.4/queue.py\", line 167 in get\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 159 in run\r\n  File \"/usr/lib/python3.4/threading.py\", line 920 in _bootstrap_inner\r\n  File \"/usr/lib/python3.4/threading.py\", line 888 in _bootstrap\r\n\r\nThread 0x00007f2050d9f700 (most recent call first):\r\n  File \"/usr/lib/python3.4/threading.py\", line 290 in wait\r\n  File \"/usr/lib/python3.4/queue.py\", line 167 in get\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 159 in run\r\n  File \"/usr/lib/python3.4/threading.py\", line 920 in _bootstrap_inner\r\n  File \"/usr/lib/python3.4/threading.py\", line 888 in _bootstrap\r\n\r\nThread 0x00007f2088ac5700 (most recent call first):\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1410 in _call_tf_sessionrun\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1322 in _run_fn\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1337 in _do_call\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1331 in _do_run\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1155 in _run\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 932 in run\r\n  File \"/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 1286 in restore\r\n  File \"/opt/l2019-04-03 17:13:00.027397: F tensorflow/stream_executor/cuda/cuda_driver.cc:184] Check failed: is_host_ptr == points_to_host_memory (0 vs. 1)dst pointer is not actually on GPU: 0x4304b81100\r\n\r\n\r\n", "comments": ["Thank you for reaching out to us. Can you please provide minimum reproducible code snippet to help us proceed further and verify what is going wrong", "The error occurred during a training run of Google Bert (https://github.com/google-research/bert/run_squad.py). Everything went well on the CPU version of TensorFlow, but after switching to GPU the error appeared.", "I used google colab with gpu accelerator and the code works successfully. Can you please use google colab and confirm?. I am inclined to think it is cuda issue. TF requires minimum cuda compute capability of 3.5 for gpu acceleration.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Did you ever find the solution for this? \r\n\r\ntensorflow/stream_executor/cuda/cuda_driver.cc:182] Check failed: is_host_ptr == points_to_host_memory (0 vs. 1)dst pointer is not actually on GPU: 0", "So I got rid of this error by testing and loading a different frozen graph. There should be some layer in the graph which needs them to be loaded on CPU during the pre-processing stage. But I am not totally sure.  "]}, {"number": 27468, "title": "tf-nightly: cannot find -ltensorflow_framework (new naming convention?)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Jessie, macOS Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14.1-dev20190402\r\n- Python version: 2.7.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen building [Horovod](https://github.com/horovod/horovod), we link against the `libtensorflow_framework.so` using `tf.sysconfig.get_link_flags()`.  \r\n\r\nThe most recent nightly builds (starting 2019-04-02) have replaced `libtensorflow_framework.so` with two separate .so files: `libtensorflow_framework.so.1` and `libtensorflow_framework.so.1.13.1`.  However, `tf.sysconfig.get_link_flags()` still references [-ltensorflow_framework](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/sysconfig.py#L78), which causes our build process to fail.\r\n\r\nWe found that replacing this line with `flags.append('-l:libtensorflow_framework.so.1')` fixed the issue with gcc.\r\n\r\nMy question is: is this intended to be the new library organization for TensorFlow going forward? If we were to submit a PR to update `tf.sysconfig` to use the correct libraries filenames, is that expected to work for the foreseeable future, or are these new artifacts temporary or accidents?  Alternatively, could your build process be updated to generate symlinks so `tf.sysconfig` will continue to work as is?\r\n\r\nThanks.\r\n", "comments": ["cc @gunan @martinwicke", "@perfinion this looks related to #22797? Maybe we just have to change sysconfig to match?", "Fixed by #27493, can be closed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27468\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27468\">No</a>\n"]}, {"number": 27467, "title": "OOM ERROR  when computing_gradients on single gpu(TITAN Xp 12gb)", "body": "I was trying to training network with 4k resolution and I'm using adam optimizer  and compute gradient are not fitting in single seperate gpu\r\n**My system config is 4GPU(TITAN Xp 12gb) and 48 cpu processor and 128gb ram**\r\n\r\nCan somebody help me to solve how compute gradient for single graph with batch 1 on multi gpu without getting OOM\r\nBelow is the stack trace of my error\r\n**2019-04-03 16:43:30.883434: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_2_bfc) ran out of memory trying to allocate 384.00MiB.  Current allocation summary follows.\r\n2019-04-03 16:43:30.883548: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 38, Chunks in use: 37. 9.5KiB allocated for chunks. 9.2KiB in use in bin. 6.3KiB client-reques\r\nted in use in bin.\r\n2019-04-03 16:43:30.883579: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 24, Chunks in use: 24. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 12.0KiB client-req\r\nuested in use in bin.\r\n2019-04-03 16:43:30.883598: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 50, Chunks in use: 50. 50.2KiB allocated for chunks. 50.2KiB in use in bin. 50.0KiB client-req\r\nuested in use in bin.\r\n2019-04-03 16:43:30.883617: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 34, Chunks in use: 34. 68.0KiB allocated for chunks. 68.0KiB in use in bin. 68.0KiB client-req\r\nuested in use in bin.\r\n2019-04-03 16:43:30.883635: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 23, Chunks in use: 23. 94.8KiB allocated for chunks. 94.8KiB in use in bin. 94.8KiB client-req\r\nuested in use in bin.\r\n2019-04-03 16:43:30.883652: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 9, Chunks in use: 9. 72.0KiB allocated for chunks. 72.0KiB in use in bin. 72.0KiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883669: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 2, Chunks in use: 2. 36.0KiB allocated for chunks. 36.0KiB in use in bin. 36.0KiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883685: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-03 16:43:30.883703: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883721: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 5, Chunks in use: 5. 704.0KiB allocated for chunks. 704.0KiB in use in bin. 704.0KiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883737: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 8, Chunks in use: 8. 2.03MiB allocated for chunks. 2.03MiB in use in bin. 2.03MiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883754: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks: 7, Chunks in use: 7. 3.81MiB allocated for chunks. 3.81MiB in use in bin. 3.81MiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883771: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks: 11, Chunks in use: 11. 11.00MiB allocated for chunks. 11.00MiB in use in bin. 11.00MiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883795: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks: 9, Chunks in use: 9. 19.50MiB allocated for chunks. 19.50MiB in use in bin. 19.50MiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883813: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks: 15, Chunks in use: 15. 80.00MiB allocated for chunks. 80.00MiB in use in bin. 80.00MiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883830: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks: 27, Chunks in use: 27. 311.00MiB allocated for chunks. 311.00MiB in use in bin. 311.00MiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883848: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks: 24, Chunks in use: 23. 568.00MiB allocated for chunks. 544.00MiB in use in bin. 544.00MiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883864: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks: 35, Chunks in use: 35. 1.67GiB allocated for chunks. 1.67GiB in use in bin. 1.65GiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883881: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks: 16, Chunks in use: 16. 1.46GiB allocated for chunks. 1.46GiB in use in bin. 1.46GiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883897: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks: 14, Chunks in use: 14. 2.56GiB allocated for chunks. 2.56GiB in use in bin. 2.56GiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883913: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks: 8, Chunks in use: 7. 4.43GiB allocated for chunks. 4.12GiB in use in bin. 4.12GiB client-requested in use in bin.\r\n2019-04-03 16:43:30.883930: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 384.00MiB was 256.00MiB, Chunk State:\r\n2019-04-03 16:43:30.883953: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 312.18MiB | Requested Size: 0B | in_use: 0, prev:   Size: 384.00MiB | Requested Size: 384.00MiB | in_use: 1\r\n2019-04-03 16:43:30.883970: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f3c000000 of size 1280\r\n2019-04-03 16:43:30.883983: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f3c000500 of size 256\r\n2019-04-03 16:43:30.883995: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f3c000600 of size 256\r\n019-04-03 16:43:30.884175: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84c25000 of size 6912\r\n2019-04-03 16:43:30.884187: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84c26b00 of size 589824\r\n2019-04-03 16:43:30.884198: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cb6b00 of size 294912\r\n2019-04-03 16:43:30.884210: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cfeb00 of size 256\r\n2019-04-03 16:43:30.884221: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cfec00 of size 4096\r\n2019-04-03 16:43:30.884233: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cffc00 of size 8388608\r\n2019-04-03 16:43:30.884246: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f854ffc00 of size 8192\r\n2019-04-03 16:43:30.884259: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85501c00 of size 8192\r\n2019-04-03 16:43:30.884271: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85503c00 of size 20480\r\n2019-04-03 16:43:30.884283: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85508c00 of size 256\r\n2019-04-03 16:43:30.884295: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85508d00 of size 2097152\r\n2019-04-03 16:43:30.884306: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85708d00 of size 8192\r\n2019-04-03 16:43:30.884318: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8570ad00 of size 1024\r\n2019-04-03 16:43:30.884331: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8570b100 of size 16777216\r\n2019-04-03 16:43:30.884343: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8670b100 of size 2048\r\n2019-04-03 16:43:30.884355: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8670b900 of size 67108864\r\n2019-04-03 16:43:30.884367: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8a70b900 of size 4096\r\n2019-04-03 16:43:30.884379: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8a70c900 of size 134217728\r\n2019-04-03 16:43:30.884391: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9270c900 of size 4194304\r\n2019-04-03 16:43:30.884402: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f92b0c900 of size 2048\r\n2019-04-03 16:43:30.884414: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f92b0d100 of size 9437184\r\n2019-04-03 16:43:30.884426: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9340d100 of size 2048\r\n2019-04-03 16:43:30.884437: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9340d900 of size 4194304\r\n2019-04-03 16:43:30.884449: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9380d900 of size 4194304\r\n2019-04-03 16:43:30.884460: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f93c0d900 of size 2048\r\n2019-04-03 16:43:30.884471: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f93c0e100 of size 9437184\r\n2019-04-03 16:43:30.884482: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9450e100 of size 2048\r\n2019-04-03 16:43:30.884493: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9450e900 of size 4194304\r\n2019-04-03 16:43:30.884505: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9490e900 of size 2097152\r\n2019-04-03 16:43:30.884516: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f94b0e900 of size 2048\r\n2019-04-03 16:43:30.884527: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f94b0f100 of size 9437184\r\n2019-04-03 16:43:30.884538: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9540f100 of size 2048\r\n2019-04-03 16:43:30.884549: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9540f900 of size 4194304\r\n2019-04-03 16:43:30.884561: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9580f900 of size 1048576\r\n2019-04-03 16:43:30.884572: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9590f900 of size 1024\r\n2019-04-03 16:43:30.884584: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9590fd00 of size 2359296\r\n2019-04-03 16:43:30.884595: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95b4fd00 of size 1024\r\n2019-04-03 16:43:30.884607: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95b50100 of size 1048576\r\n2019-04-03 16:43:30.884618: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95c50100 of size 1048576\r\n2019-04-03 16:43:30.884629: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95d50100 of size 1024\r\n2019-04-03 16:43:30.884640: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95d50500 of size 2359296\r\n2019-04-03 16:43:30.884651: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95f90500 of size 1024\r\n2019-04-03 16:43:30.884662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95f90900 of size 1048576\r\n2019-04-03 16:43:30.884673: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96090900 of size 1048576\r\n2019-04-03 16:43:30.884684: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96190900 of size 1024\r\n2019-04-03 16:43:30.884695: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96190d00 of size 2359296\r\n2019-04-03 16:43:30.884706: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f963d0d00 of size 1024\r\n2019-04-03 16:43:30.884717: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f963d1100 of size 1048576\r\n2019-04-03 16:43:30.884728: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f964d1100 of size 1048576\r\n2019-04-03 16:43:30.884739: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f965d1100 of size 1024\r\n2019-04-03 16:43:30.884750: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f965d1500 of size 2359296\r\n2019-04-03 16:43:30.884761: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96811500 of size 1024\r\n2019-04-03 16:43:30.884772: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96811900 of size 1048576\r\n2019-04-03 16:43:30.884783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96911900 of size 1048576\r\n2019-04-03 16:43:30.884794: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96a11900 of size 1024\r\n2019-04-03 16:43:30.884805: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96a11d00 of size 2359296\r\n2019-04-03 16:43:30.884817: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96c51d00 of size 1024\r\n2019-04-03 16:43:30.884828: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96c52100 of size 1048576\r\n2019-04-03 16:43:30.884840: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96d52100 of size 524288\r\n2019-04-03 16:43:30.890317: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 10 Chunks of size 6291456 totalling 60.00MiB\r\n2019-04-03 16:43:30.890331: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 8388608 totalling 8.00MiB\r\n2019-04-03 16:43:30.890344: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 9437184 totalling 27.00MiB\r\n2019-04-03 16:43:30.890358: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 23 Chunks of size 12582912 totalling 276.00MiB\r\n2019-04-03 16:43:30.890371: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 16777216 totalling 16.00MiB\r\n2019-04-03 16:43:30.890384: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 22 Chunks of size 25165824 totalling 528.00MiB\r\n2019-04-03 16:43:30.890398: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 50320896 totalling 47.99MiB\r\n2019-04-03 16:43:30.890411: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 31 Chunks of size 50331648 totalling 1.45GiB\r\n2019-04-03 16:43:30.890425: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 62914560 totalling 180.00MiB\r\n2019-04-03 16:43:30.890439: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 67108864 totalling 64.00MiB\r\n2019-04-03 16:43:30.890452: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 88080384 totalling 84.00MiB\r\n2019-04-03 16:43:30.890465: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 14 Chunks of size 100663296 totalling 1.31GiB\r\n2019-04-03 16:43:30.890479: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 134217728 totalling 128.00MiB\r\n2019-04-03 16:43:30.890895: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 13 Chunks of size 201326592 totalling 2.44GiB\r\n2019-04-03 16:43:30.890927: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 402653184 totalling 1.12GiB\r\n2019-04-03 16:43:30.890942: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 805306368 totalling 3.00GiB\r\n2019-04-03 16:43:30.890958: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 10.77GiB\r\n2019-04-03 16:43:30.890975: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:\r\nLimit:                 11916971213\r\nInUse:                 11564461824\r\nMaxInUse:              11564461824\r\nNumAllocs:                     371\r\nMaxAllocSize:            805306368\r\n\r\n2019-04-03 16:43:30.891029: W tensorflow/core/common_runtime/bfc_allocator.cc:279] **************************************************************************************************__\r\nTraceback (most recent call last):\r\n  File \"trianpy\", line 216, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"trian.py\", line 172, in main\r\n    results = sess.run(fetches,options=run_options)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n         [[Node: percept_loss/perceptual_loss/vgg_19/Relu_19/_505 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:2\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_1462_percept_loss/perceptual_loss/vgg_19/Relu_19\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:2\"]()]]\r\n         [[Node: club_loss/add_1/_1623 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_6336_club_loss/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]**\r\n ", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27466, "title": "Doc fix: replace `batch` with `batch_size`, and clarify wording", "body": "", "comments": ["Apparently there was a segfault during the tests, it can't be due to my changes, I only modified the documentation."]}, {"number": 27465, "title": "[DOC 1.x] save_keras_model requires eager mode", "body": "**System information**\r\n- TensorFlow version: 1.13.1\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/saved_model/save_keras_model?hl=en\r\n\r\n\r\n**Describe the documentation issue**\r\nI followed [this tutorial](https://www.tensorflow.org/tutorials/keras/save_and_restore_models) and this [API reference page](https://www.tensorflow.org/api_docs/python/tf/contrib/saved_model/save_keras_model?hl=en) to save a trained Keras model as a SavedModel, which in my use case is required to upload it to GCP ML Engine.\r\nBut I encounter a non documented `AssertionError` specifying I should use eager mode.\r\nThis seems related to [this commit](https://github.com/tensorflow/tensorflow/commit/6603c69fa71d6ebdee717863079ca34308c9ddb1).\r\nDid I understand something wrong or should this compatibility requirements also be documented for `save_keras_model` (at least with `serving_only` set to `True`) ?", "comments": ["@slapersonne In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Sure, here is a simplified snippet : \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\n\r\nprint(tf.__version__)\r\n\r\n\r\ndef train_model(\r\n        training_features,\r\n        prediction_features\r\n):\r\n    model = Sequential()\r\n    model.add(Dense(64, activation='relu', input_shape=([len(training_features)])))\r\n    model.add(Dense(len(prediction_features), activation='sigmoid'))\r\n\r\n    model.summary()\r\n\r\n    tf.contrib.saved_model.save_keras_model(model, \"./saved_models\", serving_only=True)\r\n```\r\n\r\nAnd here is the output I get when running this snippet : \r\n\r\n```\r\nUsing TensorFlow backend.\r\n1.13.1\r\nWARNING:tensorflow:From /my_venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ndense_1 (Dense)              (None, 64)                2304\r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 2)                 130\r\n=================================================================\r\nTotal params: 2,434\r\nTrainable params: 2,434\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/my_project/trainer/task.py\", line 75, in <module>\r\n    model = model.train_model(training_features, prediction_features)\r\n  File \"/my_project/trainer/model.py\", line 18, in train_model\r\n    tf.contrib.saved_model.save_keras_model(model, \"./saved_models\", serving_only=True)\r\n  File \"/my_venv/lib/python3.6/site-packages/tensorflow/contrib/saved_model/python/saved_model/keras_saved_model.py\", line 136, in save_keras_model\r\n    signatures=training_utils.trace_model_call(model, input_signature))\r\n  File \"/my_venv/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 711, in save\r\n    \"tf.saved_model.save is not supported when graph building. \"\r\nAssertionError: tf.saved_model.save is not supported when graph building. tf.enable_eager_execution() must run first when calling it from TensorFlow 1.x.\r\n```\r\n\r\nFor information, please note that when I set `serving_only` to False, I get an other error : \r\n```\r\n  File \"/my_project/trainer/model.py\", line 18, in train_model\r\n    tf.contrib.saved_model.save_keras_model(model, \"./saved_models\", serving_only=False)\r\n  File \"/my_venv/lib/python3.6/site-packages/tensorflow/contrib/saved_model/python/saved_model/keras_saved_model.py\", line 138, in save_keras_model\r\n    _save_v1_format(model, export_dir, custom_objects, as_text, input_signature)\r\n  File \"/my_venv/lib/python3.6/site-packages/tensorflow/contrib/saved_model/python/saved_model/keras_saved_model.py\", line 190, in _save_v1_format\r\n    checkpoint_path = _export_model_variables(model, path)\r\n  File \"/my_venv/lib/python3.6/site-packages/tensorflow/contrib/saved_model/python/saved_model/keras_saved_model.py\", line 162, in _export_model_variables\r\n    model.save_weights(checkpoint_prefix, save_format='tf', overwrite=True)\r\nTypeError: save_weights() got an unexpected keyword argument 'save_format'\r\n```", "@slapersonne I ran it in Google colab with TF1.13.1 and TF-nightly without any error. Could you try running it in colab and then upgrade TF on your system to TF1.13.1 and run again.\r\n```\r\n!pip install tensorflow==1.13.1\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\n\r\nprint(tf.__version__)\r\n\r\n\r\ndef train_model(\r\n        training_features,\r\n        prediction_features\r\n):\r\n    model = Sequential()\r\n    model.add(Dense(64, activation='relu', input_shape=([len(training_features)])))\r\n    model.add(Dense(len(prediction_features), activation='sigmoid'))\r\n\r\n    model.summary()\r\n\r\n    tf.contrib.saved_model.save_keras_model(model, \"./saved_models\", serving_only=True)\r\n```\r\n\r\nYou could also try with the tf-nightly by replacing !pip install tensorflow==1.13.1 with !pip install tf-nightly in the first line. \r\n\r\nPlease let me know how it progresses. Thanks!", "Hello @jvishnuvardhan , thank you for your answer.\r\nI tried it on Google colab, but I still encounter the above AssertionError when calling the `train_model` function : https://colab.research.google.com/drive/1yYCJF_nTwjikxF6Ym0G6xcjg7yDXk-Wc", "@slapersonne model is not complete with all the components. Please follow the [tutorial](https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_restore_models.ipynb#scrollTo=0HZbJIjxyX1S) and post any queries on Stackoverflow. \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan yes, this is a simplified sample to expose the problem, not the one I am using.\r\nI am not stuck with this as I found an other way to save my model (through a `SavedModelBuilder`).\r\nBut as mentioned in the issue title and description, I think this is a documentation issue because there is no mention of this `AssertionError`, or of any incompatibility when not in eager mode and `serving_only` is set to `True`.\r\nIn the above tutorial you linked, we get the `AssertionError` when adding `serving_only=True` : \r\n\r\n<img width=\"1064\" alt=\"Screen Shot 2019-04-24 at 09 50 14\" src=\"https://user-images.githubusercontent.com/11258206/56641935-add37000-6676-11e9-8f04-f65e9ce054e8.png\">\r\n\r\nIf this an expected behavior which does not have to be documented, I am sorry for the inconvenience and I would be glad to close the issue.", "@slapersonne This is a stale issue. Save and restore guide is [here](https://www.tensorflow.org/guide/saved_model). \r\n\r\nIs this still an issue? Could you please check and let us know. If this was resolved, please close the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]