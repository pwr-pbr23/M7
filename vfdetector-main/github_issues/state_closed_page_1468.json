[{"number": 8906, "title": "Avoid use of sufficient_statistics in moments()", "body": "When some of the dimensions are unknown, `tf.nn.sufficient_statistics()` uses `reduce_prod` to guess the size.\r\nBut `reduce_prod` is not differentiable on GPU (#8841). This commit avoids the usage of `sufficient_statistics` in `tf.nn.moments()`.\r\n\r\nBy this way, calling `moments()` won't involve any transfer between CPU and GPU. This can solve, for example, fchollet/keras#5802.\r\n\r\nI didn't know how to test things, so I edited my source a little, downloaded `/tensorflow/python/ops/nn_test.py`, imported mine and added lines like:\r\n```python\r\ntf.nn.moments = patched_moments\r\n```\r\nand run the test. It passed all; the unshifted one had unstable output. But I might have done something wrong, so please double-check it.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I have tested @Namnamseo 's solution and seems to work like a charm. It is also 5-10% faster.\r\n\r\nI started receiving \"tensorflow:Tried to colocate gradients\" warnings after upgrading from 0.12.1 to 1.0.1 and with this patch the issue is resolved. Tested on Ubuntu 14.04.\r\n\r\nI hope that the PR will be soon confirmed and merged to master.", "@ebrevdo if you have cycles, please check, o/w LMK", "either vincent or nathan silberman should look at this one.\n\nOn Mon, Apr 10, 2017 at 11:03 AM, drpngx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> if you have cycles, please check,\n> o/w LMK\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8906#issuecomment-293030142>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimz9Ypkx_O0WDgO57GMonRMKm8ANnks5rum7qgaJpZM4MwyBb>\n> .\n>\n", "@nathansilberman @vincentvanhoucke Could you take a look?", "This seems sane given the lack of GPU implementation of `reduce_prod`. On cursory inspection it doesn't seem that it would cause a memory increase. @benoitsteiner back when this code was conceived, using `reduce_mean` was way slower than `reduce_sum` followed by a normalization step. No longer an issue?", "Oh right, there were names... Should I do a commit applying those changes, then?", "@Namnamseo Commit and push to your forked project to update the PR. :)", "I'm catching up, it would take me a day or two to review this.", "SG, thanks!", "Also can you run batch_norm_benchmark.py and report the results before and after?\r\n", "For the benchmark, I built the clone of my repo, made it into a wheel, installed it in a virtualenv and ran:\r\n```sh\r\npython3 batch_norm_benchmark.py --benchmarks=BatchNormBenchmark\r\n```\r\nI'm attaching the result below. This is the right way to do the benchmark, right? (it looks like, but I'm a little confused...)\r\n\r\nI haven't made the test for the device placement. I think I got the basic ideas on testing by reading `nn_test.py` or such, but I couldn't find some kind of reference or doc to help me out. To run tests, can I just execute the script like above, with some proper arguments?", "This is the result of `batch_norm_benchmark.py`.\r\nXeon E5-2680v2 and Tesla K40c are used.\r\nIs the result consistent with what is expected?\r\n\r\n```\r\nForward convolution (lower layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.028565 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.026461 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.047894 secs\r\n=== op vs py: -7.4% ===\r\n=== py vs slow: 81.0% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.009358 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.004293 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.007882 secs\r\n=== op vs py: -54.1% ===\r\n=== py vs slow: 83.6% ===\r\nForward/backward convolution (lower layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.250261 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.255890 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.338562 secs\r\n=== op vs py: 2.2% ===\r\n=== py vs slow: 32.3% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.064522 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.050912 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.066883 secs\r\n=== op vs py: 3.7% ===\r\n=== py vs slow: -49.4% ===\r\nForward convolution (higher layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.025934 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.014913 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.029672 secs\r\n=== op vs py: -42.5% ===\r\n=== py vs slow: 99.0% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.005782 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.002708 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.004732 secs\r\n=== op vs py: -53.2% ===\r\n=== py vs slow: 74.8% ===\r\nForward/backward convolution (higher layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.163545 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.177408 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.233097 secs\r\n=== op vs py: 8.5% ===\r\n=== py vs slow: 31.4% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.041899 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.030979 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.039969 secs\r\n=== op vs py: -26.1% ===\r\n=== py vs slow: 29.0% ===\r\nForward fully-connected.\r\ncpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.001318 secs\r\ncpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002452 secs\r\n=== py vs slow: 86.0% ===\r\ngpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.000731 secs\r\ngpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.000627 secs\r\n=== py vs slow: -14.2% ===\r\nForward/backward fully-connected.\r\ncpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.005600 secs\r\ncpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.007404 secs\r\n=== py vs slow: 32.2% ===\r\ngpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.007516 secs\r\ngpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.007703 secs\r\n=== py vs slow: 2.5% ===\r\n```", "To compare, this is the result without the commits:\r\n```\r\nForward convolution (lower layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.029457 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.027027 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.049702 secs\r\n=== op vs py: -8.3% ===\r\n=== py vs slow: 83.9% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.009391 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.004322 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.007878 secs\r\n=== op vs py: -54.0% ===\r\n=== py vs slow: 82.3% ===\r\nForward/backward convolution (lower layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.250639 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.256498 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.341756 secs\r\n=== op vs py: 2.3% ===\r\n=== py vs slow: 33.2% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.059267 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.046172 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.061998 secs\r\n=== op vs py: 4.6% ===\r\n=== py vs slow: -44.9% ===\r\nForward convolution (higher layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.024732 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.015260 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.029841 secs\r\n=== op vs py: -38.3% ===\r\n=== py vs slow: 95.6% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.005794 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.002716 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.004691 secs\r\n=== op vs py: -53.1% ===\r\n=== py vs slow: 72.7% ===\r\nForward/backward convolution (higher layers).\r\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.164564 secs\r\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.174291 secs\r\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.240674 secs\r\n=== op vs py: 5.9% ===\r\n=== py vs slow: 38.1% ===\r\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.038100 secs\r\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.027701 secs\r\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.036765 secs\r\n=== op vs py: -27.3% ===\r\n=== py vs slow: 32.7% ===\r\nForward fully-connected.\r\ncpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.001692 secs\r\ncpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002541 secs\r\n=== py vs slow: 50.2% ===\r\ngpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.000689 secs\r\ngpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.000595 secs\r\n=== py vs slow: -13.6% ===\r\nForward/backward fully-connected.\r\ncpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.005530 secs\r\ncpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.007350 secs\r\n=== py vs slow: 32.9% ===\r\ngpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.005176 secs\r\ngpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.005340 secs\r\n=== py vs slow: 3.2% ===\r\n```\r\n\r\nWhile running the benchmark, the workstation was not perfectly idle -- some tasks were running on it. Thus there is a limited reliability on the exact times. But, as the workstation has a lot of(40 logical) cores, and the very GPU used here (among the ones equipped with the workstation) was idle, and the job was not I/O intensive, I think it would be okay.\r\n\r\nIn fact, I noticed a typo on `batch_norm_benchmark.py` in line 201; `t2` is used twice, affecting the result for the fourth test. Look at the GPU benchmark of Forward/backward convolution (lower layers), the \"py vs slow\" percentage (which should be (slow - py) / py * 100) is certainly wrong. I suppose someone(maybe I?) should fix this ;)", "You're correct about the typo. Can you add the fix to this PR and edit the text to note the fix?\r\nThe benchmark results look good to me, but I'll let @sguada have the final word.", "Fix typo in #9266", "Jenkins, test this please."]}, {"number": 8905, "title": "SVD float64 NaN bug", "body": "I've encountered a tf.float64 matrix (of size 60 x 200) such that tf.svd of it returns NaNs, while\r\nnp.linalg.svd works fine.\r\nConverting the matrix into tf.float32 and then converting back to tf.float64 makes everything works with TF too (while being a tiny perturbation).\r\n\r\nHere is an example Jupyter notebook: https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/mf9e2eqg2isupce/Scary%20matrix.ipynb?dl=0\r\n\r\nYou can download pickled matrix here: https://www.dropbox.com/s/b8wex6voladtgw1/scary_matrix.cPickle?dl=0\r\n\r\nI'm using Conda Python 2.7.13 (tried on Mac and Ubuntu) and a fresh version of tensorflow from pip (tried both cpu and gpu versions).", "comments": ["Could you take a look @rmlarsen?\r\n", "Pickle can do arbitrary code execution when loaded, so it's a poor choice for sharing data. Can you save it with another file format instead, e.g., with `numpy.save`?", "Sure, I updated the notebook to load np array, here is the new file: https://www.dropbox.com/s/ou7egsfdmmwxhd6/scary_matrix.np?dl=0\r\n", "Update: I found a float32 20 x 150 matrix with the same behavior (NaNs in tf.svd, but works fine with np.linalg.svd). Here is the new matrix file: [.np file](https://www.dropbox.com/s/a326frisvq7hxxw/float32_scary_mat.np?dl=0#)\r\n\r\nAnd here is the updated [Jupyter notebook](https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/mf9e2eqg2isupce/Scary%20matrix.ipynb?dl=0) with both examples.", "@rmlarsen, any update?", "BTW, I'm seeing possibly related bug in https://github.com/tensorflow/tensorflow/issues/9234 the difference is that it happens in float32, and sometimes produces segfaults instead of NaNs\r\n\r\nThe matrix in question is in https://storage.googleapis.com/tensorflow-community-wheels/svd_in and and can be opened as \r\n`matrix0 = np.genfromtxt('svd_in', delimiter= \",\").astype(dtype)\r\n`", "As a temporary workaround I replaced tf.svd with np.linalg.svd via tf.py_func module. It's really slow though.\r\n\r\n```Python\r\ndef replace_tf_svd_with_np_svd():\r\n  \"\"\"Replaces tf.svd with np.svd. Slow, but a workaround for tf.svd bugs.\r\n  For details see\r\n  https://github.com/tensorflow/tensorflow/issues/8905\r\n  \"\"\"\r\n  if hasattr(tf, 'original_svd'):\r\n    # This function has been already called and tf.svd is already replaced.\r\n    return\r\n  tf.original_svd = tf.svd\r\n\r\n  def my_svd(tensor, full_matrices=False, compute_uv=True):\r\n    dtype = tensor.dtype\r\n    u, s, v = tf.py_func(np.linalg.svd, [tensor, full_matrices, compute_uv],\r\n                         [dtype, dtype, dtype])\r\n    s_, u_, v_ = tf.original_svd(tensor, full_matrices, compute_uv)\r\n    s = tf.reshape(s, s_.get_shape())\r\n    u = tf.reshape(u, u_.get_shape())\r\n    v = tf.reshape(v, v_.get_shape())\r\n    # Converting numpy order of v dims to TF order.\r\n    order = range(tensor.get_shape().ndims)\r\n    order[-2], order[-1] = order[-1], order[-2]\r\n    v = tf.transpose(v, order)\r\n    return s, u, v\r\n\r\n  tf.svd = my_svd\r\n```", "@Bihaqo  -- make sure you have scipy with MKL (it gets overwritten with non-MKL numpy every time you install tensorflow, so you have to do \"conda install scipy/numpy\"). That version will do SVD on multiple cores so its faster than TensorFlow", "After some experimenting, it seems NaNs are connected with ill-conditioning. For instance, take first 100 MNIST examples, run SVD on covariance matrix -> NaN. If I take first 1000 MNIST examples instead, works fine.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8904, "title": "Support Kernels for SVM", "body": "Kernels in support vector machines are really usefull. I would like to test some object detection algorithms in tensorflow using svm and a radial basis kernel.\r\n\r\nI am willing to help implementing kernels for support vector machines, though I am not familiar with the sdca optimization. \r\n\r\nIs there any design doc, or plan how to add more features to the svm estimator api?", "comments": ["You might if anybody has done anything in this vein already by asking on stackoverflow. I don't know of any work internally to build out the svm functionality more or any design doc.\r\n@martinwicke, do you have any knowledge of others interested in this or a good course of action? Marking as contributions welcome for now.", "@petrosmol should have the best context for this. I believe there is work in progress for kernel-SVMs.", "TL;DR: Yes, we are actively working on kernel methods in general. As of last week there is a [kernel_methods](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/kernel_methods) package which is part of tensorflow.contrib. Kernel SVMs are supported indirectly (with some more work on user's behalf, see below). Note that this is all very recent so you need to build from source (and some changes might still need some time to propagate). Read on for details.\r\n\r\n* We don't have support for *exact* kernel methods and AFAIK, there is no plan to support these any time soon. However, we do support primal (approximate) kernel methods. We have implemented Random Fourier Features (RFF) that approximate the RBF kernel. This is the equivalent of sklearn's RBFSampler. See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/mappers/random_fourier_features.py). You can use RFF to transform your input features and then apply a linear model on top of it. These are typically much faster than exact kernels and perform pretty well in practice (quality-wise). Note that this works with dense input features only (so images should be fine)\r\n* If you want to use a pre-packaged estimator, we have implemented a Linear Classifier that uses RFF to preprocess the input features. This is not kernel SVMs, you can think of it as a \"kernelization\" of the LinearClassifier model from tensorflow.contrib.learn. The loss used is logistic (for binary) and cross entropy (for multiclass). See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/kernel_estimators.py) for the estimator and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/kernel_estimators_test.py) for sample usage.\r\n* If you really want to SVMs, you have the following 2 options:\r\n   * If your problem is binary classification you can use the pre-packed SVM estimator (which uses SDCA) but you should first apply [RFF](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/mappers/random_fourier_features.py) to your features (as part of your input_fn). Make sure you update your feature columns accordingly.\r\n   * For multiclass, you can implement your own 1-vs-all or 1-vs-1 using binary SVMs. There is no multiclass support for SVMs currently (but there are plans for it)", "Thank you! I will try the different options. Binary SVM is sufficient for me right now.", "Hello, @petrosmol !  Has multiclass SVMs api been supported in TF version 1.6 now? ", "There is no pre-packaged multiclass SVM Estimator yet (it is in the future plans though). But since this bug was opened, I have implemented multiclass SVM loss (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/losses.py). If you want to turn this into an Estimator, possibly the easiest way is to create a class that inherits from [_KernelEstimator](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/kernel_estimators.py) and pass to its constructor a head that uses the multiclass SVM loss (see KernelLinearClassifier model for instance that uses a cross-entropy multiclass loss)", "Thank you very much! @petrosmol  I will try your advice.  :-  )", "Hello, @petrosmol ! I would like to know whether the implemented multiclass SVM loss  (see [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/losses.py]) in tensorflow could be optimized by Gradient Descent Algorithm. ", "@Lidaguo. Yes, the loss should work with any gradient based optimizer. I have tried it in the past in some experiments with MNIST with no issues AFAIR.", "Hello, @petrosmol ! Thanks for your particular reply. But now, I have another problem. In my case, I'd like to use the implemented tf.contrib.kernel_methods.RandomFourierFeatureMapper to transform the features from network, and then use the transformed features to construct my loss function.  All the things go well at the beginning of my training procedure. However,  with the increasing of the training epochs, the occupation of GPU grow up. After tens of epochs, the OOM error of GPU occurs. If I don't use the transformation, everything works fine. my simple code as following:\r\n`\r\nkernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim = 100, output_dim = 1000,  stddev =5.0, name='rffm')\r\n\r\nmodel_train = My_model(..., kernel_mapper)\r\n\r\nfor epoch_index in xrange(total_epochs):\r\n    run_epoch(..., model_train)\r\n\r\n`\r\nHere is the definition of the model,\r\n\r\n`\r\nclass My_model(..., kernel_mapper):\r\n     ....\r\n     input_feature = ....\r\n     mapped_feature = kernel_mapper.map(input_feature)\r\n    ....\r\n\r\n`\r\nI guess that the  tf.contrib.kernel_methods.RandomFourierFeatureMapper class always stores values for each epoch(or each called situation) and don't free the Memory. Is there any solution for this?", "@Lidaguo.\r\n\r\nHi, RandomFourierFeatureMapper should be able to handle input/output dimensions of 100/1000. Related question: Do you see the same issue on CPU?\r\n\r\nIs it possible that you add the map ops multiple times (one per epoch) in your training graph and that's why your are getting OOM errors after a few iterations? \r\nRandomFourierFeatureMapper only adds map OPs (and does not store any values in a way that is different than what any TF graph would do it).\r\n\r\nCan you make sure that, under the hood, your call does not call .map method multiple times?\r\n\r\nFor debugging, there are ways to print the size of the TF graph after each iteration/epoch. If this is growing with each iteration then you are likely adding ops to your graph during training (which is bad).\r\n\r\nIf the above suggestions don't help you fix the problem, send me the full code so that I can reproduce the issue.\r\n\r\nThanks\r\nPetros", "@petrosmol . Hello, I did add the map ops multiple times in one per epoch in my training graph. I have fixed this problem. Thank you very much. And now, I have a problem about the implemented multiclass SVM loss (see [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/losses.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/kernel_methods/python/losses.py)), like the following picture:\r\n![e](https://user-images.githubusercontent.com/15832113/38450361-03dc76d6-3a4f-11e8-8010-780365f9e72f.png)\r\n\r\nIn my case, \r\n`self._labels = tf.placeholder(tf.int32, [None,],name='data_labels')`\r\nDose it mean that the **_labels_** or **_logits_** sent into the multiclass SVM loss class  must be a constant tensor ?\r\n", "@Lidaguo \r\nHard to tell what is going on without seeing code. But both labels and logits should work with unspecified shape. For instance the following code should work:\r\n\r\nimport numpy as np\r\n\r\nBATCH_SIZE = None\r\nNUM_CLASSES = None\r\n\r\nlogits_np = np.array([[1.2, -1.4, -1.0], [1.4, 1.8, 4.0], [0.5, 1.3, -1.0]])\r\nlabels_np = np.array([0, 2, 1], dtype=np.int32)\r\nlogits = tf.placeholder(\r\n    tf.float32, shape=(BATCH_SIZE, NUM_CLASSES), name='logits')\r\nlabels = tf.placeholder(tf.int32, shape=[BATCH_SIZE,], name='labels')\r\nloss = tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(labels, logits)\r\n\r\nwith tf.Session() as sess:\r\n  loss_eval = sess.run(loss, feed_dict={logits: logits_np, labels: labels_np})\r\n  print(loss_eval)\r\n\r\nInitial version of kernel_methods/python/losses.py was not working with unknown shape though. This was fixed end of 2017 (see https://github.com/tensorflow/tensorflow/commit/2dce1ee2e37e314772467bfa150b57b8d4311598#diff-c7afce59582d7278b51608c8024bf615)\r\n\r\nSo, you might need to upgrade to a more recent version of tf or patch these changes locally.", "@petrosmol \r\nYes! I have ungraded the version of tf. Now, my model works. Thank you very much. : - )"]}, {"number": 8903, "title": "How to change  a classification model to a regression model ? ", "body": "I am using pre-trained Alexnet as shown below. I want to use that model for regression with 6 outputs (Xcoordinate (range (0,227),Ycoordinate (range (0,227),height (range (20,50), width (range (20,50), sine(theta), cos(theta)). (range of theta is -180 to 180 degrees)\r\nThese are the following things I change - \r\n1. changed loss function to MSE. \r\n2. changed the output layer from 1000 to 6. \r\n3. changed from RELU to linear activation function last layer. \r\nNow, I am not getting proper valued of sine and cosine above (it should be in the range of (-1 to 1)), I am getting out of bound values. What should I do, How should I keep a bound one the values. Also, should I keep a bout on other parameters as well. What should I do incorporate those changes?  \r\nWhat are the other changes should I make to use this model for regression.? \r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass AlexNet(object):\r\n\r\n  def __init__(self, x, keep_prob, num_classes, skip_layer,\r\n               weights_path = 'DEFAULT'):\r\n\r\n    # Parse input arguments into class variables\r\n    self.X = x\r\n    self.NUM_CLASSES = num_classes\r\n    self.KEEP_PROB = keep_prob\r\n    self.SKIP_LAYER = skip_layer\r\n\r\n    if weights_path == 'DEFAULT':\r\n      self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\r\n    else:\r\n      self.WEIGHTS_PATH = weights_path\r\n\r\n    # Call the create function to build the computational graph of AlexNet\r\n    self.create()\r\n\r\n  def create(self):\r\n\r\n    # 1st Layer: Conv (w ReLu) -> Pool -> Lrn\r\n    conv1 = conv(self.X, 11, 11, 96, 4, 4, padding = 'VALID', name = 'conv1')\r\n    pool1 = max_pool(conv1, 3, 3, 2, 2, padding = 'VALID', name = 'pool1')\r\n    norm1 = lrn(pool1, 2, 2e-05, 0.75, name = 'norm1')\r\n\r\n        # 2nd Layer: Conv (w ReLu) -> Pool -> Lrn with 2 groups\r\n    conv2 = conv(norm1, 5, 5, 256, 1, 1, groups = 2, name = 'conv2')\r\n    pool2 = max_pool(conv2, 3, 3, 2, 2, padding = 'VALID', name ='pool2')\r\n    norm2 = lrn(pool2, 2, 2e-05, 0.75, name = 'norm2')\r\n\r\n        # 3rd Layer: Conv (w ReLu)\r\n    conv3 = conv(norm2, 3, 3, 384, 1, 1, name = 'conv3')\r\n\r\n        # 4th Layer: Conv (w ReLu) splitted into two groups\r\n    conv4 = conv(conv3, 3, 3, 384, 1, 1, groups = 2, name = 'conv4')\r\n\r\n        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\r\n    conv5 = conv(conv4, 3, 3, 256, 1, 1, groups = 2, name = 'conv5')\r\n    pool5 = max_pool(conv5, 3, 3, 2, 2, padding = 'VALID', name = 'pool5')\r\n\r\n        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\r\n    flattened = tf.reshape(pool5, [-1, 6*6*256])\r\n    fc6 = fc(flattened, 6*6*256, 4096, name='fc6',relu =True)\r\n    dropout6 = dropout(fc6, self.KEEP_PROB)\r\n\r\n        # 7th Layer: FC (w ReLu) -> Dropout\r\n    fc7 = fc(dropout6, 4096, 4096, name = 'fc7',relu =False)\r\n    dropout7 = dropout(fc7, self.KEEP_PROB)\r\n\r\n        # 8th Layer: FC and return unscaled activations (for tf.nn.softmax_cross_entropy_with_logits)\r\n    self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu = False, name='fc8')\r\n\r\n\r\n\r\n  def load_initial_weights(self, session):\r\n    \"\"\"\r\n    As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/ come\r\n    as a dict of lists (e.g. weights['conv1'] is a list) and not as dict of\r\n    dicts (e.g. weights['conv1'] is a dict with keys 'weights' & 'biases') we\r\n    need a special load function\r\n    \"\"\"\r\n\r\n    # Load the weights into memory\r\n    weights_dict = np.load(self.WEIGHTS_PATH, encoding = 'bytes').item()\r\n\r\n    # Loop over all layer names stored in the weights dict\r\n    for op_name in weights_dict:\r\n\r\n      # Check if the layer is one of the layers that should be reinitialized\r\n      if op_name not in self.SKIP_LAYER:\r\n\r\n        with tf.variable_scope(op_name, reuse = True):\r\n\r\n          # Loop over list of weights/biases and assign them to their corresponding tf variable\r\n          for data in weights_dict[op_name]:\r\n\r\n            # Biases\r\n            if len(data.shape) == 1:\r\n\r\n              var = tf.get_variable('biases', trainable = False)\r\n              session.run(var.assign(data))\r\n\r\n            # Weights\r\n            else:\r\n\r\n              var = tf.get_variable('weights', trainable = False)\r\n              session.run(var.assign(data))\r\n\r\n\r\n\r\n\"\"\"\r\nPredefine all necessary layer for the AlexNet\r\n\"\"\"\r\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\r\n         padding='SAME', groups=1):\r\n  \"\"\"\r\n  Adapted from: https://github.com/ethereon/caffe-tensorflow\r\n  \"\"\"\r\n  # Get number of input channels\r\n  input_channels = int(x.get_shape()[-1])\r\n\r\n  # Create lambda function for the convolution\r\n  convolve = lambda i, k: tf.nn.conv2d(i, k,\r\n                                       strides = [1, stride_y, stride_x, 1],\r\n                                       padding = padding)\r\n\r\n  with tf.variable_scope(name) as scope:\r\n    # Create tf variables for the weights and biases of the conv layer\r\n    weights = tf.get_variable('weights', shape = [filter_height, filter_width, input_channels/groups, num_filters])\r\n    biases = tf.get_variable('biases', shape = [num_filters])\r\n\r\n\r\n    if groups == 1:\r\n      conv = convolve(x, weights)\r\n\r\n    # In the cases of multiple groups, split inputs & weights and\r\n    else:\r\n      # Split input and weights and convolve them separately\r\n      #input_groups = tf.split(value=x, num_split= groups, split_dim=3)\r\n      #input_groups = tf.split(split_dim=3, num_split= groups,value=x)\r\n      input_groups = tf.split(axis = 3, num_or_size_splits=groups, value=x)\r\n     # weight_groups = tf.split(value =weights, num_split=groups, split_dim=3)\r\n      weight_groups = tf.split(axis = 3, num_or_size_splits=groups, value=weights)\r\n\r\n      output_groups = [convolve(i, k) for i,k in zip(input_groups, weight_groups)]\r\n\r\n      # Concat the convolved output together again\r\n      #conv = tf.concat( values = output_groups,concat_dim = 3)\r\n      conv = tf.concat(axis = 3, values = output_groups)\r\n\r\n\r\n    # Add biases\r\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list())\r\n\r\n    # Apply relu function\r\n    relu = tf.nn.relu(bias, name = scope.name)\r\n\r\n    return relu\r\n\r\n#def fc(x, num_in, num_out, name, relu = True):\r\ndef fc(x, num_in, num_out, name, relu):\r\n  with tf.variable_scope(name) as scope:\r\n\r\n    # Create tf variables for the weights and biases\r\n    weights = tf.get_variable('weights', shape=[num_in, num_out], trainable=True)\r\n    biases = tf.get_variable('biases', [num_out], trainable=True)\r\n\r\n    # Matrix multiply weights and inputs and add bias\r\n    act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\r\n\r\n    if relu == True:\r\n      # Apply ReLu non linearity\r\n      relu = tf.nn.relu(act)\r\n      return relu\r\n    else:\r\n      return act\r\n\r\n\r\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name, padding='SAME'):\r\n  return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\r\n                        strides = [1, stride_y, stride_x, 1],\r\n                        padding = padding, name = name)\r\n\r\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\r\n  return tf.nn.local_response_normalization(x, depth_radius = radius, alpha = alpha,\r\n                                            beta = beta, bias = bias, name = name)\r\n\r\ndef dropout(x, keep_prob):\r\n  return tf.nn.dropout(x, keep_prob)\r\n\r\n```\r\n\r\nNow the code for the loss function ad optimizer is\r\n\r\n```\r\n    # Op for calculating the loss\r\nwith tf.name_scope(\"cross_ent\"):\r\n    loss = tf.reduce_mean(tf.squared_difference(score, y))\r\n\r\n    # Train op\r\nwith tf.name_scope(\"train\"):\r\n      # Get gradients of all trainable variables\r\n    gradients = tf.gradients(loss, var_list)\r\n    gradients = list(zip(gradients, var_list))\r\n\r\n      # Create optimizer and apply gradient descent to the trainable variables\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\r\n\r\n``` \r\nAnything I should change in this part? \r\nOr any comments, or anything I should take care of to change the model from classifcation to regression. \r\nI am new to tensorflow and deep learning", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8902, "title": "Allow SavedModelBuilder to overwrite existing folders", "body": "This adds the feature #8852 requested", "comments": ["Can one of the admins verify this patch?", "Thanks for doing this!", "thanks for adding this!", "No problem! :)", "@tensorflow-jenkins test this please", "Sorry, we have to revert this. It is breaking some tests we require (which were sadly not run yet). ", "@martinwicke Could you let me know what the problem was, so I could fix it?", "I'm sorry the initial advice was the other way around, but we had some more discussions about this and we're very certain that we do not want to make this easy. It's a bad idea (tm), take it from people who have lost lots of work to easy overwriting. \r\n\r\nYou can always emulate the behavior by doing the delete_recursively manually, but that I would advise against. Ideally, timestamp your directories and symlink the latest."]}, {"number": 8901, "title": "Add gzip and zlib support for FixedLengthRecordReader", "body": "This fix adds gzip and zlib support for FixedLengthRecordReader,\r\nas was discussed in #8865.\r\n\r\nWhen FixedLengthRecordReader is used, it will check for\r\ncompression_type flag and use ZlibInputStream as needed.\r\nThe usage of InputBuffer in FixedLengthRecordReader has\r\nalso been changed to BufferedInputStream to match ZlibInputStream.\r\n\r\nThis fix fixes #8865.", "comments": ["Can one of the admins verify this patch?", "@Mistobaan Thanks for the review. The PR has been updated. Please take a look.", "@Mistobaan what do you think?", "@drpngx looks fine to me", "@saxenasaurabh please take a look", "@saxenasaurabh LMK if you have cycles or we can find another reviewer.", "Thanks @saxenasaurabh for the review. The reason that `footer_cache_` is needed is that `zlib/gzip` are essentially non-seekable. In other words, once a byte is read, we could not go backward. We can only start from the beginning again to find the previous location. In addition, `zlib/gzip` does not give us the uncompressed file size before hand. (we could certainly do a two-pass but that is likely inefficient if the file is large.)\r\n\r\nIn case of FixedlengthRecord, a footer might be possible. However, at the time we try to read the next bytes, we don't know if the next bytes belongs to a record, or a footer. Assume the record is `1` bytes and the footer is `2` bytes:\r\n```\r\nHRFF\r\n```\r\nWhen we read the first `R`, we don't know if it is record, or footer. We only knows it is a record when we read two `FF` + EOF. At this point, we knows that `FF` is the footer, thus `R` can only be record.\r\n\r\nFor that we maintains a `footer_cache_` so that we could find out the record `R` without seek back the file. (gzip/zlib is non-seekable and we could not find the length before hand).\r\n\r\nI will update the PR to add a detailed explanation.", "Also, it looks like the format of the `FixedLengthRecord` now accepts `hop_bytes` (added in another PR). This may complicate the processing for non-seekable files (like gzip/zlib).\r\n\r\nI will take a closer look and see what we could do to handle `hop_bytes` as well.", "@yongtang let us know what you find.", "Thanks @saxenasaurabh @vrv. I managed to update the PR and the `hop_bytes_` should have been covered as well.\r\n\r\nIt use similar an approach of caching a buffer for `hop_bytes`.\r\n\r\nBasically the purpose is to avoid the need to randomly seek to a position of the file stream. Now the file stream (possibly gzip or zlib) could be \"read through\" without moving backward. (gzip or zlib does not support moving backward as they are not \"seekable\").\r\n\r\nThe comment has been expanded to explain the logic but let me know if more explanation is needed.", "I splited the PR into two commits. The first commit switches the InputBuffer to BufferedInputStream only (and drop the need of `Seek()` call). The second commit adds the gzip and zlib process. That may make it easy to follow.", "@yongtang it looks like somments from @saxenasaurabh haven't been addressed -- can you make sure or at least comment on them?  Thanks!", "@vrv @saxenasaurabh Thanks for the review. The review comments should have been covered. Please take a look.", "Jenkins, test this please.", "API review is fine with this change in principle, as long as it is backwards compatible (defaults for all new attrs, and those defaults give old behavior).", "Can one of the admins verify this patch?", "@yongtang any progress?", "Thanks all for the review and help and sorry for the delay. The PR has been updated. Please take a look and let me know if there are any issues.", "@tensorflow-jenkins test this please", "@saxenasaurabh can you take another look and approve?", "@saxenasaurabh Can you check this please? In particular make sure that all ops are backwards compatible.\r\n\r\nI'm assuming this needs to be rebased.", "Jenkins, test this please.", "It looks like the test fails because of an API change. @martinwicke could you confirm that it's OK?", "Thanks @saxenasaurabh for the review. The PR has been updated. Please take a look.", "Thanks @saxenasaurabh for the review. The PR has been updated. Please take a look.", "Jenkins, test this please.", "Thanks @saxenasaurabh for the review. The PR has been updated. I also updated the api golden so that api compatibility could pass. Please take a look.", "Jenkins, test this please.", "Thanks @josh11b @drpngx for the review. The PR has been updated. Please take a look.", "Jenkins, test this please."]}, {"number": 8900, "title": "ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory ", "body": "I am trying to use tensorflow-gpu on my system. I have re-installed it many times, it gives the error give below. But when I use tensorflow-cpu it works fine. I have cuda 8.0 toolkit installed and cudnn 5.1\r\n```\r\nTraceback (most recent call last):\r\n  File \"finetune.py\", line 17, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/saurabh/code/env/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/saurabh/code/env/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/saurabh/code/env/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/saurabh/code/env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/home/saurabh/code/env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["I suspect you didn't look for the common reasons and solution url included in that message. It unfortunately doesn't work, because it is a broken link that we should fix. Still, please try the stuff listed here:\r\n[See common installation problems](https://www.tensorflow.org/install/install_linux)\r\n\r\n@gunan, can we fix the error message to give the proper url in the code?", "Okay! Is there any other way I can safely install tensorflow-gpu and work with? I am using python 2.7 right now in virtual env. I have already installed tensorflow using the link you suggested it gives the same error! \r\nAlso, is there any issue with cudnn or cuda toolkit installation? ", "I have fixed the error and tensorflow works with GPU! (with the help of stackover flow [Iink)](http://stackoverflow.com/questions/43162667/importerror-libcudart-so-8-0-cannot-open-shared-object-file-no-such-file-or-d) just re-installed everything. It worked. \r\nOne important thing is  after` pip install --upgrade tensorflow-gpu`, it showed the same error. Thus, I used\r\n`export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl \r\n` Then it resolved the error by itself. Also, I was giving wrong **$CUDA_HOME** path.", "@aselle looks like the code is already fixed to point to the correct url, but it did not make it into the release yet.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/pywrap_tensorflow.py#L49\r\nWith the next release, the error message should be fixed.\r\n\r\n@talos19 It looks like TF 0.10 is working for you. TF 0.10 was the last release with cuda 7.5 support. All the other release not working for you, and this version working for you means you have cuda 7.5 installed, contrary to what you reported in the issue.\r\nIf you would like to use the latest version of TF, you need to upgrade both your CUDA and cuDNN installations."]}, {"number": 8899, "title": "da nda activate tensorflow", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": []}, {"number": 8898, "title": "Import Error  Couldn't open CUDA library libcudnn.so.5.", "body": "\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\ni was installing tensorflow  GPU version on ubuntu x86-64\r\nbut I found an error:\r\n\r\n` >>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcudnn.so.5. LD_LIBRARY_PATH: /home/lunasdejavu/Downloads:/usr/local/cuda-8.0/lib64\r\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:3517] Unable to load cuDNN DSO\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally`\r\n\r\ni tried to install again and again follow the instructions \r\nit is still useless.\r\nI tried the NVIDIA_CUDA-8.0_Samples then make. no error after `all`\r\n\r\ncan somebody help me... i was working on this setting for almost 24 hours....\r\n### Environment info\r\nOperating System:\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \r\n64 bit\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\ni can't use the command\r\n\r\nbut the packages are cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb\r\ncudnn-8.0-linux-x64-v6.0.tgz\r\n\r\nIf installed from binary pip package, provide:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n \r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\ni searched the manual of th\r\n cd <installpath>\r\n    export LD_LIBRARY_PATH=`pwd`:$LD_LIBRARY_PATH\r\n\r\n    Add <installpath> to your build and link process by adding -I<installpath> to your compile\r\n    line and -L<installpath> -lcudnn to your link line.\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["From the issue template:\r\n```\r\nInstalled version of CUDA and cuDNN:\r\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\r\ni can't use the command\r\n```\r\nUntil you can find out what files are in there, it's going to be hard for you to see if there libcudnn in there, you're going to have difficulty solving the problem. You could install libcudnn in a different directory and add it to your LD_LIBRARY_PATH list.", "As an aside, this is an error that is covered on the install documentation.\r\nhttps://www.tensorflow.org/install/install_linux\r\n", "i still can't fix this problem \r\n`ls -l /path/to/cuda/lib/libcud*\r\nls: cannot access '/path/to/cuda/lib/libcud*': No such file or directory\r\n`\r\nbut  i knew how to fix the original problem.\r\nTensorflow  hasn't supported  cudnn 6.0 yet.\r\nso i changed to cudnn5.1. it is working now.", "Just for the sake of completeness regarding the command:\r\nYou're supposed to change `/path/to/cuda/lib` to the path on your machine which should be something like `/usr/include/cuda/` or  `/opt/cuda-7.5/` or `/home/username/Downloads/cuda/`.\r\nIf everything is working now, you could close the issue.", "Shouldn't the solution be adding it the permanent library config path? instead of `export LD_LIBRARY_PATH=/usr/local/cuda/lib64/` you can do \r\n```\r\nsudo sh -c \"echo '/usr/local/cuda/lib64\\n/usr/local/cuda/lib' >> /etc/ld.so.conf.d/nvidia.conf\"\r\n\r\nsudo ldconfig\r\n```", "This is what I did (I think - I've retraced my steps, removing and recreating links and clearing the LD_LIBRARY_PATH in order to reproduce the problem).\r\n\r\n\r\n(tensorflow)$python\r\n>>>import tensorflow\r\n\r\nEverything breaks loose\r\n[lots of messages]\r\nImportError: libcudnn.so.5: cannot open shared object file: No such file or directory\r\n\r\nI registered as a developer and downloaded libcudnn.so.6,the latest, as some had suggested. I installed with  dpkg or gdebi (wrapper - cannot remember, it was very late)\r\nWith these packages/installers things may end up in /etc/alternatives,\r\nor /usr/targets/x86_64/..., with a link to them somewhere not in your paths\r\n\r\nSo ... used ln to create a link. I don't care that it's actually libcudnn.so.6 .. \r\nI'm doing this as an experiment.  Some say 6 works and it's the latest for cuda toolkit 8.\r\n\r\n### sudo ln -s /etc/alternatives/libcudnn_so /usr/local/cuda/lib64/libcudnn.so.5\r\n\r\nWe get this link\r\n/usr/local/cuda/lib64/libcudnn.so.5 -> /etc/alternatives/libcudnn_so\r\nAdd /usr/local/cuda/lib64 to your LD_LIBRARY_PATH:\r\n### export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\r\n\r\nThen in my tensorflow environment things are quiet\r\n\r\n(tensorflow) dude@beauty:~/tensorflow$ python\r\nPython 3.5.2 (default, Nov 17 2016, 17:05:23) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>>\r\n\r\nHopefully this will work in your system.  ", "I have cudnn7 installed in /opt/cuda/lib64\r\n\r\n```\r\nsudo ln -s libcudnn.so.7.0.1 libcudnn.so.5 \r\n```\r\nfixed the \r\n```\r\nImportError: libcudnn.so.5: cannot open shared object file: No such file or directory\r\n```\r\n\r\nSeems to be working ok. Of course, since cudnn5.1 is the officially supported version, might be setting myself up for future pain!", "HI @arekaykay \r\n\r\nI have a strange problem with tensorflow in python3.5. I have cudnn 5.1 in my cuda directory, but tensorflow always complain the following,\r\n\r\n`ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory`\r\n\r\nI tried to reinstall tensorflow 1.3, but this does not solve my problem. \r\n\r\nI have also tried to create a symbolic link to libcudnn.so.6 by the following,\r\n\r\n`sudo ln -s libcudnn.so.5.1.10 libcudnn.so.6`\r\n\r\nAlthough the link was created successfully, but tensorflow still complain the same thing as shown above. \r\nSO why tensorflow complains not be able to use cudnn 6 when I have cudnn5.1 installed? \r\n\r\nTHanks a lot.\r\n", "Hi @Kevinpsk ,\r\n\r\nWhat are the contents of the directory where you have you .so files (i.e. output of `ls -al`)?\r\n\r\nMy workaround was for the case where I had a new version of cudNN (7.0) and tensorflow wanted an old version  (5.1).\r\n\r\nYour case seems to be the opposite. TensorFlow wants a version of cudNN (6.0) that is newer than what you have (5.1). Upgrading cudNN might be the answer.\r\n\r\nAnyway, I am going to try to upgrade TensorFlow to 1.3 and let you know how it goes. I have cudNN 7.0 installed.\r\n\r\n", "@Kevinpsk \r\nAfter upgrading to `TensorFlow 1.3`, first I got the error\r\n```\r\nImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n```\r\nThen I did the following\r\n```\r\ncd /opt/cuda/lib64\r\nsudo ln -s libcudnn.so.7.0.1 libcudnn.so.6\r\n```\r\nThis fixed the problem. Again, since cudNN 6 is the officially supported version, this might create problems down the line. `TensorFlow 1.4` will be officially on cudNN 7.\r\n\r\nSo, AFAICT, cudNN upgrade might fix the problem you are seeing.", "If it works, it works. \u00a0Nothing is clean or bug free ever.\n\n      From: arekaykay <notifications@github.com>\n To: tensorflow/tensorflow <tensorflow@noreply.github.com> \nCc: JT-SV <joaquintrigueros@yahoo.com>; Comment <comment@noreply.github.com>\n Sent: Friday, August 18, 2017 6:06 PM\n Subject: Re: [tensorflow/tensorflow] Import Error Couldn't open CUDA library libcudnn.so.5. (#8898)\n   \n@Kevinpsk\nAfter upgrading to TensorFlow 1.3, first I got the errorImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\nThen I did the followingcd /opt/cuda/lib64\nsudo ln -s libcudnn.so.7.0.1 libcudnn.so.6\nThis fixed the problem. Again, since cudNN 6 is the officially supported version, this might create problems down the line. TensorFlow 1.4 will be officially on cudNN 7.So, AFAICT, cudNN upgrade might fix the problem you are seeing.\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.  \n\n   ", "Hi @arekaykay\r\n\r\nThanks a lot for your help. I remember that I first installed cudnn 6, but neither tensorflow nor theano works. So I switched to cudnn 5.1, and theano works fine with my CNN, but tensorflow complained with the message I posted above. Anyway, since I am training my network now, I will try again with cudnn 6 with tensorflow afterwards. \r\n\r\nAnother issue is that it seems that tensorflow 1.3 does not support cudnn 6 (cudnn 5.1 works fine) on windows 7, because it will issue an error. Not sure if this a windows issue or cudnn version issue. \r\n\r\nOn tensorflow website, install guide page, it still states that tensorflow requires cudnn5.1, so it will be good if they provide info on which version of tensorflow support which version of cudnn.\r\n\r\nShuokai", "when I upgraded the tensorflow 1.3 in ubuntu14.04, I got the same error saying\"ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\", while my cudnn is only cudnn5.1. in the path of /usr/local/cuda/lib64, libcudnn.so.5.1.10 is found. what should I do? ", "@Kevinpsk \r\n\r\nWhile the install guide page says that `TensorFlow` requires `cudNN 5.1`,  [release notes for 1.3](https://github.com/tensorflow/tensorflow/blob/r1.3/RELEASE.md) says that all pre-built binaries were built with `cudNN 6`. I suspect the install guide page has not been updated.\r\n\r\n@helxsz \r\nMy suggestion would be to upgrade `cudNN 5.1` to `cudNN 6`", "@arekaykay and @helxsz\r\n\r\nYeah, upgrading to cudnn v6 solve the problem. \r\n\r\nThanks", "@Kevinpsk \r\nhi, it's strange, but even after upgrading to v6 I still get the same error. In cuda/lib64  I have libcudnn.so, libcudnn.so.6 and libcudnn.so.6.0.21 yet same error appears. Is there something which also needs to be changed?", "That's strange. There should not be this problem again. Have you tried\ncreate a virtual environment using conda and reinstall python and\ntensorflow? Normally this will sort it out.\n\nOn 25 August 2017 at 06:03, VladimirBelov92 <notifications@github.com>\nwrote:\n\n> @Kevinpsk <https://github.com/kevinpsk>\n> hi, it's strange, but even after upgrading to v6 I still get the same\n> error. In cuda/lib64 I have libcudnn.so, libcudnn.so.6 and\n> libcudnn.so.6.0.21 yet same error appears. Is there something which also\n> needs to be changed?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8898#issuecomment-324824491>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AI0xcsuJRW9qlGnKoaVRWWW7_H5YFIofks5sblW2gaJpZM4MwkNl>\n> .\n>\n", "I was using tensor2tensor 1.1.9 on top of tensorflow 1.3.0rc1, and then I upgraded tensor2tensor to 1.2.0 because of a new change I wanted to adopt. However tensor2tensor 1.2.0 installation uninstalled the existing tensorflow and upgraded it to 1.3.0 also. Now tensorflow can't run with the error message\r\n\r\n> ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory \r\n\r\nwhile I have libcudnn.so.5.1.10 only. Does upgrading cudnn fix the problem? Or should I create a symln pointing to the existing cudnn?", "Upgrade cudnn to version 6", "Download the cudnn6 and extract it into lib64 file path, in my case: /usr/local/cuda-8.0/lib64 \r\nEnsure that you append the lib64 file path (/usr/local/cuda-8.0/lib64) to the LD_LIBRARY_PATH environment variable"]}, {"number": 8897, "title": "outputs indifferent to input when applying `merge_duplicate_nodes`", "body": "I think I might have located the root of #8698\r\nIt seems that `merge_duplicate_nodes` is the reason that `quantize_nodes` malfunctions.\r\nWhatever I do, whenever I apply a `merge_duplicate_nodes` somewhere during a graph transformation, the output becomes completely indifferent to the input.\r\n\r\n(Unfortunately) I don't get any error messages concerning this...", "comments": ["@petewarden , could you please take a look. Thanks so much.", "Variants of this problem include constant values from some intermediate layer onwards.\r\nSo the input won't make it past there, which results in the overall output being constant.", "I should probably mention that it makes inference a lot faster ...\r\nUnfortunately, faster doesn't always mean better ...\r\nHow could `merge_duplicate_nodes` possibly eliminate all of these computations?", "@Androbin, could you confirm that replacing `deduped_graph_def` with `merged_graph_def` in \r\n[quantize_nodes.cc line 942](https://github.com/tensorflow/tensorflow/blob/a667a52ee909975389e48ffe30fb4ff9570635da/tensorflow/tools/graph_transforms/quantize_nodes.cc#L942) causes the problem to go away?", "@andrehentz \r\nI replaced `deduped_graph_def` with `merged_graph_def` in `quantize_nodes`, recompiled the `graph_transforms` tool and reinserted the `quantize_nodes` transformation.\r\n\r\nAfter doing so, I can confirm to get changing values during inference whereas previously, the output was indifferent to the input.", "As mentioned, this bug does step in place whenever `merge_duplicate_nodes` is used (not just with `quantize_nodes`).", "And I should probably mention that I am doing the inference on Android.\r\nUnfortunately, I am currently unable to test this on Ubuntu because the original graph, from which the quantized version is derivated, is currently broken due to another issue.", "@Androbin \r\nI found an issue with dequantization on Android and I'm wondering if building with \r\n   `-DTENSORFLOW_DISABLE_META` \r\nworks for you. I'd appreciate if you would try it out. Thanks in advance!", "@andrehentz \r\nThanks for the hint!\r\nSeems like I can't rely on `nightly-android` builds anymore...", "The following should work:\r\n`bazel build -c opt --copt=\"-DTENSORFLOW_DISABLE_META\" //tensorflow/contrib/android:libtensorflow_inference.so --config=android_arm`\r\n\r\nBut if you are OK waiting, we should have a fix for quantization in the nightly by the end of next week. I'm not sure it will fix this particular problem you are experiencing, though.\r\n\r\nMeanwhile, could you perhaps provide steps for reproducing the failures you are seeing with `merge_duplicate_nodes`? Did you retrain an inception model?", "@andrehentz \r\nThanks, this should work now.\r\nHappy to hear about a fix in sight.\r\n\r\nIn my particular case, I didn't use a prepared model but a custom one in the scope of a research project.\r\nI can provide the structure of the graph, if this was helpful.\r\nAnyway, currently any graph transform containing a `merge_duplicate_nodes` e.g. `quantize_nodes` causes this issue with my model.", "I'd be curious to know which nodes are removed when you run transform_graph with \r\n  `--transforms='merge_duplicate_nodes'`\r\nfrom reading the code I understand only nodes that have the same op, inputs and attributes (by the way of a hash function) are removed.\r\n", "@andrehentz \r\nI will inspect this and list anything unusual here!", "@andrehentz \r\nMeanwhile, which ABI does `--config=android_arm` resolve to?\r\n`{ arm64-v8a, armeabi-v7a, armeabi }`", "armeabi-v7a (but I think any of the ones you mentioned should be fine)", "I tried recompiling and running computing a diff but since the output of `transform_graph` is binary, I wasn't able to extract, exactly which nodes are removed.", "@Androbin Did the new bazel build for `libtensorflow_inference.so` work for you?\r\n\r\nEdit: building with --copt=\"-DTENSORFLOW_DISABLE_META\"  solved the problem for me.", "@kwotsin I will recompile, retrain and requantize and comment the results as soon as I have them.", "Might take time though, there are currently issues with building the inference library #11182.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 8896, "title": "corrected \"check_graph\" to \"summarize_graph\"", "body": "`\"check_graph\"` may have been the previous name for `\"summarize_graph\"`\r\nthe surrounding text suggests that both refer to the same tool", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 8895, "title": "image_retraining/retrain.py not found", "body": "### Environment info\r\nOperating System: Windows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n\r\n- cuda_8.0.61_win10.exe\r\n- cudnn-8.0-windows10-x64-v5.1\r\nI have installed tensorflow for gpu using pip install tensorflow-gpu( also downloaded the nightly for vanishing certain warnings)\r\n- tensorflow_gpu-1.1.0rc0-cp35-cp35m-win_amd64.whl\r\n\r\n\r\n## What am I trying to do?\r\nI want to use the tensorflow for poets for transfer learning on my images **## ### WITHOUT USING DOCKER**\r\n## What my problem is?\r\nThe tensorflow directory in the site-packages does not have the image_retraining folder at all.\r\nBut the **tensorflow-master-cpu on github** has examples/image_retraining. It has many other files and directories.\r\n \r\n### My question is..\r\nCan I copy and paste the tensorflow/examples/image_retraining or the tensorflow subfolder to my tensorflow folder in site-packages?\r\nor\r\nDo I install tensorflow for cpu ? Will the retraining work?\r\nI don't want to use docker. Please help me @Carmezim \r\n", "comments": ["Hi @Srigowri, if you want to run the code without Docker you can just install TensorFlow locally on your machine. \r\nRegarding CPU or GPU both would work for the purpose of running the code, it will depend if you have a compatible GPU and CUDA and cuDNN installed.  All the details on how to use TensorFlow with GPU support are on https://www.tensorflow.org/install/install_windows.\r\n\r\nTo use the code with your images one way is to set the default folder path to where your images are stored [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L903). The other way, which is recommended is to use `--image_dir` as instructed [in the codelab](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#6).\r\n\r\nYou will need Bazel for Windows to build the retrainer so just copying the folder won't work. You can download Bazel [here](https://github.com/bazelbuild/bazel/releases). Make sure to properly add the Bazel binary directory to your `%PATH%`. To set where the graph will be created you may use `output_graph` and `output_labels`.\r\n\r\nLet's sum it up:\r\n1. Install TensorFlow \r\n2. Download Bazel and add its directory to `%PATH%`\r\n3. Retrieve the model files\r\n4. [Build it with Bazel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L42)\r\n5.  Go through [this section](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#4) to learn how to classify new images with the retrainer. You can read the `# In Docker` comments as \"in your command prompt\".\r\n6. Then [here](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#6) to learn about training categories you want.\r\n\r\nIf you have any further questions feel free to ask.", "`retrain.py` only really depends on having tensorflow installed. So I think we can skip bazel entierly.\r\n\r\nJust downloading it to your working directory should be enough:\r\n\r\n    curl -O https://raw.githubusercontent.com/tensorflow/tensorflow/r1.1/tensorflow/examples/image_retraining/retrain.py\r\n\r\nFrom there you can just run it like any other python script. For me, this works without any trouble:\r\n\r\n    python retrain.py {add your args here}\r\n\r\nLet me know if it doesn't work for you, but I think it should.", "@MarkDaoust good to know bazel is not necessary that simplifies a lot, I assumed according to the [code documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L38) it was though."]}, {"number": 8894, "title": "tensorflow/core/kernels/split_op.cc:159:26: error: non-constant-expression cannot be narrowed from type 'int64' (aka 'long long') to 'int' in initializer list [-Wc++11-narrowing]", "body": "Building tensorflow android error:\r\n*  os: ubuntu16.04\r\n* tensorflow: 1.0.1\r\n\r\nBefore error comes, I followed the tips in [android README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md)\r\n\r\n```\r\n bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a\r\n\r\n```\r\n\r\nthen I just got these error:\r\n\r\n```\r\nERROR: /media/work/CodeSpace/AISpace/tensorflow/tensorflow/core/kernels/BUILD:3944:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections ... (remaining 73 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/core/kernels/resize_nearest_neighbor_op.cc:20:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:152:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMap.h:242:106: error: non-constant-expression cannot be narrowed from type 'long long' to 'int' in initializer list [-Wc++11-narrowing]\r\n        const Index index = m_dimensions.IndexOfRowMajor(array<Index, NumDims>{{firstIndex, secondIndex, otherIndices...}});\r\n                                                                                                         ^~~~~~~~~~~~\r\ntensorflow/core/kernels/resize_nearest_neighbor_op.cc:69:24: note: in instantiation of function template specialization 'Eigen::TensorMap<Eigen::Tensor<const int, 4, 1, int>, 16, MakePointer>::operator()<long long, int>' requested here\r\n          std::copy_n(&input_data(b, in_y, in_x, 0), st.channels,\r\n\r\n```\r\nI installed android sdk and ndk via android-studio, and here is my settings in WORKSPACE:\r\n```\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    # Ensure that you have the build_tools_version below installed in the\r\n    # SDK manager as it updates periodically.\r\n    build_tools_version = \"25.0.2\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/media/work/android/android-sdk\",\r\n)\r\n#\r\n# Android NDK r12b is recommended (higher may cause issues with Bazel)\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/media/work/android/android-sdk/ndk-bundle\",\r\n    # This needs to be 14 or higher to compile TensorFlow. \r\n    # Note that the NDK version is not the API level.\r\n    api_level=14)\r\n```\r\n\r\nAnybody occurred this problem too? I don't know what happed.\r\n", "comments": ["Make sure you're building with NDK r12b, as other versions are known to cause issues with Bazel.", "Try building with `bazel build --copt=-Wno-c++11-narrowing` to disable this error. NDK13 switched from GCC to Clang, which has this check enabled by default."]}, {"number": 8893, "title": "cuDNN 6.0 compatibility", "body": "cuDNN v6.0 was released on Mar 23, are there any plans for adding compatibility (building against compatibility version 6000)?", "comments": ["Closed as a duplicate of [issue 8828](https://github.com/tensorflow/tensorflow/issues/8828)"]}, {"number": 8892, "title": "Error when building Tensorflow", "body": "When I use `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` to build tensorflow, I got so many unknown warnings and one error as follow.\r\n```\r\nERROR: /home/jackie/.cache/bazel/_bazel_jackie/c3ef17997092dfc0d7384ef6a12887e6/external/nccl_archive/BUILD:33:1: C++ compilation of rule '@nccl_archive//:nccl' failed: clang failed: error executing command /usr/bin/clang -MD -MF bazel-out/local_linux-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/broadcast.cu.pic.d ... (remaining 56 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: error: Unsupported CUDA gpu architecture: sm_61\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 162.278s, Critical Path: 35.42s\r\n```\r\n\r\nFor more information,\r\n```\r\nUbuntu 16.04\r\nCUDA 8.0\r\ncuDNN v5.1\r\nGTX-1080\r\ntensorflow-gpu-1.1\r\n```\r\nbazel\r\n```\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n```\r\nclang\r\n```\r\nclang version 3.8.0-2ubuntu4 (tags/RELEASE_380/final)\r\nTarget: x86_64-pc-linux-gnu\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\nFound candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0\r\nFound candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/6.0.0\r\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/5.4.0\r\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/6.0.0\r\nSelected GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0\r\nCandidate multilib: .;@m64\r\nSelected multilib: .;@m64\r\nFound CUDA installation: /usr/local/cuda\r\n```\r\nconfigure\r\n```\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n```", "comments": ["Looks like this is a problem between clang and cuda 8.0.\r\nOne option, you can use gcc as your host compiler.\r\nWe have not fully tested clang yet.", "@gunan Yeah, that's right. I just tested gcc to compile and it works though with warnings. Thanks!", "Warnings are known issues, we are working on fixing them.\r\nThanks for verifying, closing the issue."]}, {"number": 8891, "title": "Added Convolutional LSTM", "body": "Added an implementation of convolutional lstms (https://arxiv.org/abs/1506.04214). Related to this issue #4536 . ", "comments": ["Can one of the admins verify this patch?", "I think that if you want to use the cell with `dynamic_rnn` wrapper, this will not work because it expects a 3D tensor for the scan. Am I right?", "No; dynamic_rnn supports tensors with shape 2D+ as input.\n\nOn Mon, Apr 3, 2017 at 3:55 AM, Marco Ciccone <notifications@github.com>\nwrote:\n\n> I think that if you want to use the cell with dynamic_rnn wrapper, this\n> would not work because it needs a 3D tensor for the scan. Am I right?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-291110646>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzzSM3oi2wehVGpIT1mQmBzoYUbaks5rsNA4gaJpZM4MwTz9>\n> .\n>\n", "Thanks @ebrevdo, since when it is supported? I'm having an error with my implementation that the shape must be 3D so I'm asking. Do I need to specify any abstract method or something else?\r\n\r\nI tag @carlthome because I think he's interested too", "The tensorflow nightlies should have this bug fixed.\n\nOn Apr 3, 2017 1:24 PM, \"Marco Ciccone\" <notifications@github.com> wrote:\n\n> Thanks @ebrevdo <https://github.com/ebrevdo>, since when it is supported?\n> I'm having an error with my implementation that the shape must be 3D so I'm\n> asking. Do I need to specify any abstract method or something else?\n>\n> I tag @carlthome <https://github.com/carlthome> because I think he's\n> interested too\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-291262718>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1wXVGdavJIWvCLsI83MMZxSU6uxks5rsVWNgaJpZM4MwTz9>\n> .\n>\n", "It doesn't seem so, [here](https://github.com/tensorflow/tensorflow/blob/89082724cf5808e151399a067bc2d7fe2ade23fd/tensorflow/python/ops/rnn.py#L493-L496) we have a reshape that fails if I have more than 2 dimensions. Am I missing something? I can open an issue in case I'm right\r\n", "Any update on this review?", "I don't think this works with `tf.nn.dynamic_rnn`.", "@loliverhennigh Do you plan to come back again on this?", "I updated some of the issues mention above", "@carlthome @ebrevdo let us know what the next steps are!", "Hi @loliverhennigh!  Thanks for taking the time to sit down and implement this.\r\n\r\nHave you seen sonnet's [ConvLSTM module](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py#L648)?  They have a very nice API and implementation that is sort-of but not 100% identical to yours.  Would you be interested in writing a ConvLSTM matching this API / impl?\r\n\r\nNote also we've gotten rid of _checked_scope in favor of RNNCell subclassing tf.layers.Layer; so we no longer override __call__ but instead have a \"def call(self, inputs, state):\"", "friendly ping for @loliverhennigh ", "Oh cool, I had not seen sonnet yet. I could definitely rewrite my ConvLstm to match that one. I like that it has support for 1,2, and 3d convs.", "That would be great!\n\nOn May 4, 2017 9:12 PM, \"Oliver Hennigh\" <notifications@github.com> wrote:\n\nOh cool, I had not seen sonnet yet. I could definitely rewrite my ConvLstm\nto match that one. I like that it has support for 1,2, and 3d convs.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-299365527>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABtim8lvf_7MzpKKjLTzcYEg5UDALT8wks5r2qGugaJpZM4MwTz9>\n.\n", "You can email me directly with any questions.\n\nOn May 5, 2017 7:48 AM, wrote:\n\n> That\n>\n> On May 4, 2017 9:12 PM, \"Oliver Hennigh\" <notifications@github.com> wrote:\n>\n>> Oh cool, I had not seen sonnet yet. I could definitely rewrite my\n>> ConvLstm to match that one. I like that it has support for 1,2, and 3d\n>> convs.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-299365527>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim8lvf_7MzpKKjLTzcYEg5UDALT8wks5r2qGugaJpZM4MwTz9>\n>> .\n>>\n>\n", "Ok, thanks!\n\nOn May 5, 2017 9:50 AM, \"ebrevdo\" <notifications@github.com> wrote:\n\n> You can email me directly with any questions.\n>\n> On May 5, 2017 7:48 AM, wrote:\n>\n> > That\n> >\n> > On May 4, 2017 9:12 PM, \"Oliver Hennigh\" <notifications@github.com>\n> wrote:\n> >\n> >> Oh cool, I had not seen sonnet yet. I could definitely rewrite my\n> >> ConvLstm to match that one. I like that it has support for 1,2, and 3d\n> >> convs.\n> >>\n> >> \u2014\n> >> You are receiving this because you were mentioned.\n> >> Reply to this email directly, view it on GitHub\n> >> <https://github.com/tensorflow/tensorflow/pull/\n> 8891#issuecomment-299365527>,\n> >> or mute the thread\n> >> <https://github.com/notifications/unsubscribe-auth/ABtim8lvf_\n> 7MzpKKjLTzcYEg5UDALT8wks5r2qGugaJpZM4MwTz9>\n> >> .\n> >>\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-299485380>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEv26A2U6l8p3nkTIOpaZmMbMiNfgrv4ks5r2zdFgaJpZM4MwTz9>\n> .\n>\n", "Can one of the admins verify this patch?", "@loliverhennigh any updates on this?", "If nothing else, I'd like to make an effort to get this into contrib. I have a working ConvLSTM and ConvGRU (with optional layernorm) [here](https://github.com/carlthome/tensorflow-convlstm-cell) that works with 2D, 3D and 4D. Would just need to get it into the coding style of TensorFlow (e.g. qualified imports), add the deprecated stuff for backwards compatibility and make it NHWC/NCHW agnostic.", "Sorry for being so slow. I just added the refactor version based on @ebrevdo code. Now it supports 1d 2d and 3d convs. I have a kernel test for the 2D LSTM conv and have checked that the 1D and 3D work however I am still working on adding them to the kernel test file. \r\n\r\nPS. @ebrevdo The code you sent me was great! I couldn't help noticing that the Conv3dLSTM is not actually implemented though. the 1D and 2D both have class Conv1DLSTM and Conv2DLSTM but there is none for 3D. Thanks again", "I added kernel tests for the 1D and 3D lstm conv. Everything looks to be working well now.", "@ebrevdo can you please review this?", "Will look tomorrow!\n\nOn May 16, 2017 9:40 PM, \"Rasmus Munk Larsen\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> can you please review this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-301983539>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8Ov7-1ZtIQuMfK3AppLJqt0TMxrks5r6npXgaJpZM4MwTz9>\n> .\n>\n", "I see a lot of deltas unrelated to your code: caused by merge conflicts?\nYou're bringing back stale code unrelated to the conversation lstm. Can you\nfix that?\n\nOn May 16, 2017 10:54 PM, \"Eugene Brevdo\" <ebrevdo@google.com> wrote:\n\nWill look tomorrow!\n\nOn May 16, 2017 9:40 PM, \"Rasmus Munk Larsen\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> can you please review this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-301983539>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8Ov7-1ZtIQuMfK3AppLJqt0TMxrks5r6npXgaJpZM4MwTz9>\n> .\n>\n", "Whoops, sorry about that. Everything should be good now", "@tensorflow-jenkins test this please", "@ebrevdo it looks like this is ready for review.", "Thanks!  Can you add unit tests with inputs whose shapes are placeholders\nof unknown shape and/or input_shape arguments that are tensors?  Or do you\nnever expect input_shape to be unknown at build time?\n\nAlso, can you add spaces after commas in your code?\n\nOn May 19, 2017 9:44 AM, \"Rasmus Munk Larsen\" <notifications@github.com>\nwrote:\n\n> Assigned #8891 <https://github.com/tensorflow/tensorflow/pull/8891> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#event-1089855330>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-COj7PoW9yRfHDNHk-FfG11lP4Fks5r7cb6gaJpZM4MwTz9>\n> .\n>\n", "I think the spacing after commas is better now.\r\n\r\nI have been thinking about the placeholder values for inputs_shape however I don't believe this is possible with this type of implementation. The reason being that you need to know the input shape to make the correct sized zero_state. I don't really see anyway to do this if the input shape is unknown. Does my reasoning seem correct on this?\r\n", "@loliverhennigh, if you don't know a tensor shape until runtime just use `tf.shape`.", "Sorry for dropping the ball on this review!  Could you rebase on master and ping when ready for re-review?", "If @loliverhennigh doesn't mind - I'd like to help get this finished up.", "@carlthome could you address all of @ebrevdo comments and report when ready?\r\n\r\nThanks\r\n", "@drpngx, I think you've confused me with this pull request's author (that would be @loliverhennigh). If @loliverhennigh doesn't get back to this fairly soon I'll try to get this merged in a new pull request though.", "@carlthome woops sorry about that. Thanks for volunteering!", "Hey, sorry I haven't gotten back to this. I'm visiting my family this week\nand have been busy. Feel free to address the issues if you want. Otherwise\nI'll try to do it in a day or two when I get home.\n\nOn Jun 26, 2017 5:53 PM, \"drpngx\" <notifications@github.com> wrote:\n\n> @carlthome <https://github.com/carlthome> woops sorry about that. Thanks\n> for volunteering!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-311213103>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEv26JYuv8bLhpU-DI8socRFfwInpy5Tks5sIERngaJpZM4MwTz9>\n> .\n>\n", "Thanks! No hurry on our end.", "Jenkins, test this please.", "I believe I corrected all comments made by @ebrevdo.\r\n\r\nPS, I missed a correction the first time and had to push twice.", "Ping for @loliverhennigh on the last comment about adding tests.", "I think this fixes the variable batch size problem and also allows for variable image sizes. The kernel tests now reflect this. I think this might be all good now", "Jenkins, test this please\n\nOn Aug 5, 2017 12:57 PM, \"Lukasz Kaiser\" <notifications@github.com> wrote:\n\n> *@lukaszkaiser* approved this pull request.\n>\n> The code looks good to me, thanks! (One could remove the conv_dims or have\n> just 1 class, but the current version looks consistent with our layers, so\n> I don't have any strong opinion on that.)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#pullrequestreview-54521631>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbcki11vjQA84KiDDjx0oIr1lBEmsks5sVMkSgaJpZM4MwTz9>\n> .\n>\n", "Jenkins, test this please.", "The Linux build is transient, but there is a windows build error:\r\n\r\n```\r\n08:30:35      4>measuring_cost_estimator.obj : error LNK2019: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::SanitizeThreadSuffix(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?SanitizeThreadSuffix@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V23@@Z) referenced in function \"public: __cdecl tensorflow::grappler::MeasuringCostEstimator::MeasuringCostEstimator(class tensorflow::grappler::Cluster *,int,int)\" (??0MeasuringCostEstimator@grappler@tensorflow@@QEAA@PEAVCluster@12@HH@Z) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\r\n08:30:35      4>queue_runner.obj : error LNK2001: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::SanitizeThreadSuffix(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?SanitizeThreadSuffix@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V23@@Z) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\r\n08:30:35      4>single_machine.obj : error LNK2001: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::SanitizeThreadSuffix(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?SanitizeThreadSuffix@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V23@@Z) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\r\n08:30:35      4>C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\Release\\pywrap_tensorflow_internal.dll : fatal error LNK1120: 1 unresolved externals [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\r\n08:30:35      4>Done Building Project \"C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\r\n08:30:35      1>Done Building Project \"c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_python_build_pip_package.vcxproj\" (default targets) -- FAILED.\r\n08:30:35 \r\n```", "This is a python only change, I think?\n\nOn Aug 7, 2017 8:10 AM, \"drpngx\" <notifications@github.com> wrote:\n\n> The Linux build is transient, but there is a windows build error:\n>\n> 08:30:35      4>measuring_cost_estimator.obj : error LNK2019: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::SanitizeThreadSuffix(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?SanitizeThreadSuffix@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V23@@Z) referenced in function \"public: __cdecl tensorflow::grappler::MeasuringCostEstimator::MeasuringCostEstimator(class tensorflow::grappler::Cluster *,int,int)\" (??0MeasuringCostEstimator@grappler@tensorflow@@QEAA@PEAVCluster@12@HH@Z) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\n> 08:30:35      4>queue_runner.obj : error LNK2001: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::SanitizeThreadSuffix(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?SanitizeThreadSuffix@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V23@@Z) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\n> 08:30:35      4>single_machine.obj : error LNK2001: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::SanitizeThreadSuffix(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?SanitizeThreadSuffix@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V23@@Z) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\n> 08:30:35      4>C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\Release\\pywrap_tensorflow_internal.dll : fatal error LNK1120: 1 unresolved externals [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj]\n> 08:30:35      4>Done Building Project \"C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\n> 08:30:35      1>Done Building Project \"c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_python_build_pip_package.vcxproj\" (default targets) -- FAILED.\n> 08:30:35\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8891#issuecomment-320737738>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyfi2aA2Y03b1ch_ZqYtAI-P5Yokks5sV1L6gaJpZM4MwTz9>\n> .\n>\n", "Right. Merging.", "Hi @loliverhennigh I am trying to use it together with `dynamic_rnn` but got some errors. May I understand the `input_shape` better? Say I want the input to be video of size 32x32, 3 channels for RGB. Each video clip has 10 frames. What exactly should I feed to the `input_shape` argument? And let's say the batch_size is 5, how should the size of the `inputs` be if it is used together with `dynamic_rnn`? \r\nThank you\uff01", "Hey @Icnature, I wrote a little silly example of how to use the `conv2dlstmcell` with the `dynamic_rnn` stuff here\r\n https://github.com/loliverhennigh/dynamic_rnn_conv_lstm/blob/master/mnist_deep.py#L57. To answer your question though, your input shape should be `[32,32,3]`. You can have your inputs be any shape that is supported by the dynamic rnn thing. In the example above the shape is `[batch_size, seq_length, height, width, channels]`. If you change `time_major` to True I think it will be `[seq_length, batch_size, height, width, channels]`. Hopefully that answers your question ok! ", "Thanks @loliverhennigh ! I figured out my mistake. I was actually trying `Conv1DLSTMcell`. The `kernel_shape` should be a list instead of an integer.", "@loliverhennigh \r\nI feel like it is little bit disturbing to not be able to choose the padding style, stride, activation function, bias, etc like tf.contrib.layers.conv2d .\r\nIs there a specific reason for that?", "In the original  paper they use peephole connections (i.e. the gates depend on the hidden state) but as far as I can tell these connections are not present in the implementation. Is this intentional? Might be worth it do mention in the documentation in that case.", "@Linusnie for now you can use my implementation [here](https://github.com/carlthome/tensorflow-convlstm-cell) instead. I find peepholes important, by the way.", "Hi @loliverhennigh!\r\nWill I be able to use this for inputs with dynamic image shapes (consequently the cell shape would be changing too)?\r\n\r\nI get a typical `TypeError: int() argument must be a string or a number, not 'Tensor'` for the code below. Am I doing something exceptionally wrong?\r\n```\r\ndef _convlstm_layer(x):\r\n\t# x is shaped [batch_size, seq_len, _, _, 1]\r\n\tshape = [tf.shape(x)[2], tf.shape(x)[3], tf.shape(x)[4]]\r\n        cell = tf.contrib.rnn.Conv2DLSTMCell(input_shape=shape,kernel_shape=[3,3], output_channels=8)\r\n\t(outputs, state) = tf.nn.dynamic_rnn(cell, x, time_major=False, dtype=tf.float32)\r\n\treturn outputs\r\n```"]}, {"number": 8890, "title": "tensorboard shows nothing ", "body": "I am using tensorflow 1.0.1and python 3.4\r\nWhen I run my code, it shows no error. However tensorboard show that \"No graph definition files were found. \"\r\n\r\nMy tensorboard debug is as follows:\r\n\r\nINFO:tensorflow:TensorBoard is in debug mode.\r\nINFO:tensorflow:Starting TensorBoard in directory /home/swx/lk\r\n**INFO:tensorflow:TensorBoard path_to_run is: {'/home/swx/lk/=': None}**\r\nINFO:tensorflow:Event Multiplexer initializing.\r\nINFO:tensorflow:Event Multiplexer done initializing\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: /home/swx/lk/=\r\nINFO:tensorflow:Done with AddRunsFromDirectory: /home/swx/lk/=\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\n**INFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()**\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.002 secs\r\nINFO:tensorflow:TensorBoard is tag: b'41'\r\n\r\n\r\n\r\ncan any one offer some help?\r\n\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nOn the other hand make sure you have written summaries (you didn't include your reproducible code). See the tensorboard documentation https://www.tensorflow.org/get_started/summaries_and_tensorboard\r\n\r\n"]}, {"number": 8889, "title": "CP gradients test fix", "body": "Disable gradients_test testWarnings for python versions 3.6 or above.", "comments": []}, {"number": 8888, "title": "Tensorboard broken on latest source", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n-rw-r--r-- 1 root root   556000 Mar  4 15:04 libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Mar  4 15:04 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Mar  4 15:04 libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root root   415432 Mar  4 15:04 libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Mar  4 15:04 libcudart_static.a\r\n-rwxr-xr-x 1 root root 84163560 Mar  4 15:06 libcudnn.so\r\n-rwxr-xr-x 1 root root 84163560 Mar  4 15:06 libcudnn.so.5\r\n-rwxr-xr-x 1 root root 84163560 Mar  4 15:06 libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814 Mar  4 15:06 libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n9738902a46f633c10149d02584484db0b1f2626a\r\n\r\n2. The output of `bazel version`\r\n```\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nFails with the following error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/tensorboard.py\", line 33, in <module>\r\n    from tensorflow.tensorboard.backend import application\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/backend/application.py\", line 47, in <module>\r\n    from tensorflow.tensorboard.plugins.projector import projector_plugin\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/plugins/projector/projector_plugin.py\", line 30, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins.projector import projector_config_pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 59, in <module>\r\n    from tensorflow.contrib import tensorboard\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorboard/__init__.py\", line 22, in <module>\r\nragha@ragha-gpu:~/dsb-2017$ tensorboard\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/tensorboard.py\", line 33, in <module>\r\n    from tensorflow.tensorboard.backend import application\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/backend/application.py\", line 47, in <module>\r\n    from tensorflow.tensorboard.plugins.projector import projector_plugin\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/plugins/projector/projector_plugin.py\", line 30, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins.projector import projector_config_pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 59, in <module>\r\n    from tensorflow.contrib import tensorboard\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorboard/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.tensorboard import plugins\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorboard/plugins/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins import projector\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorboard/plugins/projector/__init__.py\", line 34, in <module>\r\n    from tensorflow.tensorboard.plugins.projector import projector_plugin\r\nImportError: cannot import name projector_plugin\r\n```", "comments": ["Hi, I have the same problem. I don't know the reason but I solved by the following step\r\nsudo vim /usr/local/bin/tensorboard\r\nadd  \r\n\r\nfrom tensorflow.contrib import bayesflow \r\n\r\nbefore\r\n \r\nfrom tensorflow.tensorboard.tensorboard import main\r\n\r\nI noticed if just import bayesflow and then import main, it is fine. But if changing the order, it will have error like you.", "Thanks. That does make it run again.", "@gunan and @jart, could you look into this?\r\n", "Possibly cause could be b1d6de28c5600e179d979cc635c812aca955d046 PTAL @dsmilkov.", "It was related to a circular dependency. I have a fix - will send an internal change for review now.", "Please keep me update about this also! I got exactly the same problem with `tensorboard`.\r\n\r\nThanks", "Thanks for preparing the fix @dsmilkov.", "Commit 663eaa05bf1bfa2e84d60b16dc6f4d49cecc8b9a should fix it. Feel free to reopen if the issue still exists. ", "It's ok now.", "For tensorflow 1.3.0, I got this error again:\r\n`Traceback (most recent call last):\r\n  File \"/home/kevin/.local/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n  File \"/home/kevin/.local/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py\", line 34, in <module>\r\n    from tensorflow.tensorboard.backend import server\r\n  File \"/home/kevin/.local/lib/python2.7/site-packages/tensorflow/tensorboard/backend/server.py\", line 37, in <module>\r\n    from tensorflow.tensorboard.backend import handler\r\n  File \"/home/kevin/.local/lib/python2.7/site-packages/tensorflow/tensorboard/backend/handler.py\", line 43, in <module>\r\n    from tensorflow.tensorboard.plugins import REGISTERED_PLUGINS\r\n  File \"/home/kevin/.local/lib/python2.7/site-packages/tensorflow/tensorboard/plugins/__init__.py\", line 20, in <module>\r\n    from tensorflow.tensorboard.plugins.projector.plugin import ProjectorPlugin\r\n  File \"/home/kevin/.local/lib/python2.7/site-packages/tensorflow/tensorboard/plugins/projector/plugin.py\", line 27, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME\r\nImportError: cannot import name PROJECTOR_FILENAME`", "Please file tensorboard issues under the new tensorboard repository at github.com/tensorflow/tensorboard"]}, {"number": 8887, "title": "Embedding visualizations for hi-res images training (feature request)", "body": "Hi Tensorflow people, thank you all.\r\nI wonder if I could train my image classification model on my normal sized images (1080px X 1920px) but still use Tensorboard's cool embedding visualizations. I prefer to keep the images with the highest resolution possible since it proved to be important for the classification.\r\n\r\nIn the tutorial (https://www.tensorflow.org/get_started/embedding_viz) the developers specify tensorflow currently supports sprites up to 8192px X 8192px, meaning you can either use a lot of low-res images (fine for MNIST 28X28 and CIFAR-10 32X32) or a few high-res images (a too small training set size). So, I was wondering if there could be a way around it.\r\n\r\n**What if, we could use full-res photos for training, but downsize them for the thumbnails needed to make up the sprite?**\r\nThat way we could still get a sense of which picture is which in the visualization, but let the model train on higher quality data.\r\n\r\n1. Can I do it myself, by creating a low-res (down-sized) copy of my entire database beforehand, and use it to create the sprite etc., will the embedding event still correlate to the same source image (and the right label in the metadata)?\r\n2. Instead, should I put inside the training code itself, a small procedure for resizing of the full-res image after each bottleneck calculation, and then store that thumbnail in a separate folder - making sure the embedding log in the metadata actually corresponds to the right thumbnail?\r\n\r\n***************************************\r\nEnvironment info:\r\n\r\nUbuntu 16.04.02 (64 bit)\r\ntensorflow 0.12.1 CPU only (64 bit). I have a NVIDIA GTX 1050ti waiting to be used if crucial for this task.\r\n\r\nI'm am a kind of a coding noob so forgive my inaccuracies and ignorance. I'm relatively new (6 months) to tensorflow and CNNs in general. I've been transfer-training inception V3 on classification of large (1080px X 1920px) images, divided to 10 labels (folders). \r\nThe reason I'm asking and not just diving deep into it is that I'll have to spend a lot of time to resize my images and create the perfect sprite image and metadata file, but won't have the confidence that the trained data corresponds to the sprite image. So I want to see if it's even possible to begin with - **to get full certainty of visualized thumbnail corresponding to actual full-res image used for training.**\r\n\r\nThank you for this great platform!! Tensorboard is a very powerful tool and I'm very excited to unlock the embedding visualizations' potential. @dandelionmane\r\n", "comments": ["@dandelionmane, could you provide direction on this feature and if it is necessary and how to go about it?", "Thanks @aselle !\r\n\r\nHere's some bits I found that may help (I hope I'm not totally off here):\r\n\r\n1. This sprite builder [here](https://github.com/oduerr/dl_tutorial/blob/master/tensorflow/debugging/embedding.ipynb), line [16]. I still need to figure out how to build it straight from the source images (after resizing them) and not from a tensor already containing the images as used there.\r\n\r\n2. Similarly, generating the metadata file, like [here](https://github.com/normanheckscher/mnist-tensorboard-embeddings/blob/master/mnist_t-sne.py), line [69]\r\n\r\n3. To refine the question - should the sprite and metadata files be built continuously with every step in the training, or can they be prepared in advance?\r\n\r\nThanks again!\r\n", "The sprite sheet for the embedding visualization is independent of your training data. The sprite sheet won't be used as input for the training process. \r\n\r\nTo ensure that a sprite of the sprite sheet corresponds to an embedding, you need to make sure that you compute and add the embeddings to tensorboard in the same order that you have in the sprite sheet.\r\nMost probably you'll only add embeddings to tensorboard every x iterations. So you could shuffle your images during training and when you reach iteration x, feed them in order. \r\n\r\n```\r\nprojector_config = projector.ProjectorConfig()\r\nemb_conf = self.projector_config.embeddings.add()\r\nemb_conf.metadata_path = \"tsv_filename\"\r\nemb_conf.sprite.image_path = \"spritesheet_name\"\r\ntrain_images = [img1, img2 ...]  # same order as in your spritesheet\r\n\r\n\r\ninference_op = ... # the inference op of your graph, taking images an input, outputs the embeddings\r\nloss_op = loss(inference_op)  # compute the loss based on your embeddings\r\ntrain_op = tf.Optimizer().minimize(loss_op)  # the traninig op of your graph\r\nsess = tf.Session()\r\nfor step in range(1000):\r\n    sess.run(train_op)\r\n    if step % 100 == 0:\r\n        for img_batch in train_images:\r\n            embeddings += sess.run(inference_op, feed={placeholder: img_batch})\r\n       ... routine to add and save the embeddings with tensorboard\r\n            \r\n```\r\n\r\nRegarding your third question: The only situation where creating the sprites during training might be useful is if you alter the images during training. However if you only apply small changes, e.g. add some color noise, the resulting embedding won't be too different and you'd only get a lot of images on the same spot.", "Thanks for the clarification and code @PhilJd ! \r\nSo, hopefully the low res images in the sprite image will be classified similarly to their full-res sources.\r\nI'll give it a go! Thanks again", "Closing this issue, since as PhilJd correctly points out, the sprites are independent from the training data."]}, {"number": 8886, "title": "Branch 151877152", "body": "", "comments": ["Waiting on an internal fix for the failures. Closing for now."]}, {"number": 8885, "title": "rc1 RELEASE.md", "body": "Updating the authors in RELEASE.md and updating new bug fixes.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 8883, "title": "Change scalar_summary to summary.scalar", "body": "scalar_summary is deprecated and issues a warning. This was reported in bug #8878", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Looks like this change breaks `head_test`.\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/3964/consoleFull\r\n\r\nCould you also fix the failures?", "@tensorflow-jenkins test this please"]}, {"number": 8882, "title": "Minor fix to saved_model/builder_impl.py docstring", "body": "I believe that there is a minor problem with the python API documentation at https://www.tensorflow.org/api_docs/python/tf/saved_model/builder/SavedModelBuilder. The first line of the \"typical usage\" example should read \r\n`builder = saved_model.builder.SavedModelBuilder(export_dir)`\r\nrather than \r\n`builder = saved_model_builder.SavedModelBuilder(export_dir)`. \r\n\r\nThis pull request should fix the issue. \r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "jenkins, test this please.", "It is possibly because the author got used to 'from tensorflow.python.saved_model import builder as saved_model_builder'. This is reflected in file https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/saved_model_test.py"]}, {"number": 8881, "title": "Add bazel clean back info the configure script for 1.1 release.", "body": "", "comments": []}, {"number": 8880, "title": "Add back bazel clean into configure script.", "body": "", "comments": ["Jenkins, test this please.", "Look like there is an issue we missed due to missing bazel clean.\r\nI am looking into root causing this, and solving the problem\r\n", "Could you please take the time and explain just briefly, what kind of issue this is about?\r\nI would like to know under which circumstances cleaning is actually necessary to recompiling successfully.\r\nThis would be good to know because this comes undeniably with some speed penalty.\r\n\r\n> Bazel's design is such that these problems are fixable; we consider such bugs a high priority, and will do our best fix them. If you ever find an incorrect incremental build, please file a bug report. We encourage developers to get out of the habit of using `clean` and into that of reporting bugs in the tools.", "https://github.com/bazelbuild/bazel/issues/2759", "@gunan from what the referenced issue says, this should only be an issue on macOS?", "We first observed this on macos, but then reproduced the same problem on\ndifferent platforms as well.\n\nOn Apr 24, 2017 6:16 AM, \"Androbin\" <notifications@github.com> wrote:\n\n> @gunan <https://github.com/gunan> from what the referenced issue says,\n> this should only be an issue on macOS?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8880#issuecomment-296663709>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOYQ99X4BiQqb-wm0bvXQdtkoOcwGks5rzKCngaJpZM4MwDdL>\n> .\n>\n", "As far as I know, the according fix was already available and now introduced in the recent release of bazel."]}, {"number": 8879, "title": "could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "body": "Hi,\r\nI installed tensorflow 1.0.1 GPU version on my Macbook Pro with GeForce GT 750M. Also installed CUDA 8.0.71 and cuDNN 5.1. I am running  a tf code that works fine with non CPU tensorflow but on GPU version , I get this error (once a while it works too).\r\n\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n\r\nWhat is happening here? Is there a bug in tensorflow. Please advise.\r\n\r\nThanks\r\n", "comments": ["@gunan, may have insight, but in general, mac support for nvidia gpus is relatively poor, so it is difficult for us to support them.", "Looks like previous instances of the same error messages were usually cuDNN version mismatches.\r\nMaybe we have a cuDNN patch version mismatch.\r\nWhat is your full cuDNN version? I will check our build machines on Monday, then we can compare.\r\n\r\nIf this is caused by cuDNN version you may need to build TF from source.", "My cuDNN version is 5.1 (OSX). I noticed if there is more GPU memory available then command line version of the program works but jupyter notebook crashes with this error. ", "5.1 are only major and minor cudnn versions. There is also a 3rd integer, patch version. You can check this via looking into your `cudnn.h` header file.\r\n\r\nI feel like there is more we can get from your logs.\r\nCould you paste your full terminal output to pastebin and share its link here?", "Here it is:\r\n#define CUDNN_MAJOR      5\r\n#define CUDNN_MINOR      1\r\n#define CUDNN_PATCHLEVEL 10\r\n\r\nAlso terminal output:\r\nhttps://pastebin.com/9D2983ex\r\nThanks so much\r\n\r\n", "@crack00ns I just encountered this when I tried to free up the GT 750m by taking out the external monitor, which switches the display to Iris Pro.  Running the imagenet tutorial it crashes out like you described:\r\n\r\n> Total memory: 2.00GiB\r\n> Free memory: 1.72GiB\r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\n> W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n> F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\nHowever, as soon as I enabled Discrete graphics again by using the external monitor it **started to work again**.  So I suggest you **use gfxCardStatus to switch on the Nvidia GPU** before you run the code. \r\n\r\nGiven that from 1.1 onwards MAC GPU will be unsupported, I will move away to a Linux desktop with a GTX 1080 Ti soon....", "Thank you so much. Using gfxCardStatus and forcing it to Discrete mode seems to work! ", "Looks like the issue is resolved?\r\nI will close this issue, please let me know if it is not resolved yet.", "@gunan I encountered similar problem, how to resolve it?", "@wangg12 I have a simple workaround to fix this. It is probably some memory related issue. Get gfxcardstatus and force switch to integrated and then switch back to Discrete only. It kinda resets the graphics card. Check the GPU memory with cuda-smi. This process frees up GPU memory. Run your code again. Should work. I assume  you are using macOS. ", "@crack00ns No, I am using ubuntu14.04. I dont know what gfxcardstatus is.", "@wangg12 Ah then try resetting GPU with nvidia-smi. Check your GPU memory too.  This is some memory related bug/issue in my opinion. ", "Maybe this is related to cuda driver version. I tried the same code on another machine which has higher driver version, everything goes fine. But I'am not 100% sure because I can't update the driver version to test it for now. Thanks anyway @crack00ns. ", "You're welcome. Could be driver issue too. Perhaps @gunan could provide better insight. Workaround works for me for now. ", "I'm having this issue and can't figure out why it's happening because Theano works. The interesting thing is that I've to run theano with sudo or pygpu can't find cudnn handle either. If I try to run this TF script with sudo I crash immediately with the good old `Library not loaded: @rpath/libcudnn.5.dylib` that I can't seem to make go away no matter what tricks I try.\r\n\r\nI'm on MacOS 10.12 with 1080ti, CUDA 8 and cudnn 5.1.\r\n\r\nThe log is pretty much the same as above:\r\n```\r\n2017-05-01 13:58:31.475182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.00GiB\r\nFree memory: 8.84GiB\r\n2017-05-01 13:58:31.475192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0\r\n2017-05-01 13:58:31.475195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y\r\n2017-05-01 13:58:31.475202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)\r\n2017-05-01 13:59:08.504593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)\r\n---------------------------------\r\nRun id: resnet_cifar10\r\nLog directory: /tmp/tflearn_logs/\r\n---------------------------------\r\nPreprocessing... Calculating mean over all dataset (this may take long)...\r\nMean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\r\n---------------------------------\r\nTraining samples: 50000\r\nValidation samples: 10000\r\n--\r\n2017-05-01 13:59:34.352806: E tensorflow/stream_executor/cuda/cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-05-01 13:59:34.352824: E tensorflow/stream_executor/cuda/cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-05-01 13:59:34.352831: F tensorflow/core/kernels/conv_ops.cc:659] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\n\r\nI tested the out of memory angle but it doesn't seem to be the issue because if I set TF to have `config.allow_soft_placement = True` and `config.gpu_options.allow_growth = True`, and watch cuda-smi executing I can see I've over 8GB of memory left once it crashes.", "On Ubuntu 16.04 calling 'nvidia-smi' fixed the problem. thanks to @crack00ns ", "@jagadeesr how do you  solve the problem, I use \"sudo nvidia-smi -r -i 0\" but display:\r\n\"GPU Reset is not supported on devices running as primary GPU.\r\nTerminating early due to previous errors.\"\r\nwhat should I do.\r\nThank you very much!\r\n", "@SunTiecheng I was able to run 'nvidia-smi' without sudo. After running this cmd, it worked for me. No arguments passed to the cmd.", "@jagadeesr  It still can't work for me. But thank you very much!", "I faced this issue and in my case the root cause was not reseting device via nvidia-smi --reset-gpu --id=0 for example, but by disabling CNMeM.\r\n\r\nI work with Theano and this is enabled there via entry in ~/.theanorc as following:\r\n[lib]\r\ncnmem = 1\r\n\r\nSo removing this in my case helped resolution of this or very similar issue.\r\n\r\nUnfortunately TensorFlow uses its own memory management and doesn't utilize cnmem delivered by nvidia, so I don't know how to configure this here.\r\n\r\nThere unfortunately isn't anything like externally **configurable** memory manager for GPU in Tensorflow as per my understanding and you can tune this only directly via code => gpu usage\r\n\r\nNote: my knowledge of TensorFlow is limited, just pointing out what I have discovered and how resolved issue with Theano on mobile graphic.\r\n", "I am having the same issue : \r\n\r\nLinux Mint 18.1 Serena\r\nCUDA 8.0\r\nlibcudnn 5.1\r\ntensorflow-gpu            1.3.0 \r\nkeras  (using tf as a backend)\r\n\r\n```\r\n E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-08-21 16:53:38.788947: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-08-21 16:53:38.788956: F tensorflow/core/kernels/conv_ops.cc:672] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\n```\r\n\r\nPer another thread, this appears to be an issue with the GPU running out of memory. I have tried to use this code snippet \r\n\r\n```\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    config.gpu_options.per_process_gpu_memory_fraction = 0.1\r\n    set_session(tf.Session(config=config))\r\n\r\n```\r\n\r\nbut no luck. I have been checking nvidia-smi. I get the following : \r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.59                 Driver Version: 384.59                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:02:00.0  On |                  N/A |\r\n| 20%   30C    P8     9W / 250W |    427MiB / 11169MiB |      4%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1383    G   /usr/lib/xorg/Xorg                             271MiB |\r\n|    0      1991    G   compton                                          3MiB |\r\n|    0      2143    G   ...el-token=3B20FA9DA27556BEE46CF45A65B73A9B   107MiB |\r\n|    0      8279    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    42MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n", "Was this issue solved? - I too am having the same error with TF 1.3.0, cuDNN 6.0, CUDA 8.0 and keras. Altering the GPU-options didn't work. Many answers suggest checking for Zombie processes running in the background taking up GPU memory (I have none)", "@SimonWalsh1000 I am not sure if this helps but I was able to fix it by nuking the conda environment I was working in, uninstalling CUDA then reinstalling everything using conda (just `conda install tensorflow-gpu`, it will install all the cuda dependencies automatically). There must have been a compatibility issue somewhere between the Nvidia driver, CUDA, CUDADNN, and TF, but I'm not sure where. \r\n\r\nAlso, it seemed that the issue may have been associated with the latest NVIDIA driver. You may want to uninstall your nvidia driver as well, (`apt-get remove nvidia-XXX`).  Conda is smart enough to install that too. I am using nvidia-375. \r\n", "I think you may have needed cuDNN 6.0 for TF 1.3. ", "I also had this issue, to verify this is related memory issue you can try disabling gup and try using only cpu. \r\n      `os.environ['CUDA_VISIBLE_DEVICES'] = '-1'`", "run this fix the issue.\r\n\r\n sudo rm -rf ~/.nv", "I'm on Windows10 and encountered this issue. Running \"C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe\" doesn't solve it for me... There isn't a .nv file/directory under home directory to delete either.\r\n\r\nHas anyone solved this for Windows Environments?\r\n\r\nBelow is the result of nvidia-smi.exe\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 385.54                 Driver Version: 385.54                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050   WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   32C    P8    N/A /  N/A |     73MiB /  2048MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```", "I had the same problem:\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n\r\nThe solution for me was downgrading from cudnn 7.1.2 to 7.005.", "with tf.Graph().as_default():\r\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\r\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\r\n        with sess.as_default():\r\n\r\n\r\ni lower gpu_memory_fraction, and there is no problem", "Well, I had the same error, for me, simply **reboot** the Linux Ubuntu 16.04 solved the problem.", "Similar problem happend when having dual gpus and running the code while gpu0 is occupied.\r\nI made only the 2nd gpu visible by adding: \r\nos.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\nand it solve the problem for my case. ", "> I am having the same issue :\r\n> \r\n> Linux Mint 18.1 Serena\r\n> CUDA 8.0\r\n> libcudnn 5.1\r\n> tensorflow-gpu 1.3.0\r\n> keras (using tf as a backend)\r\n> \r\n> ```\r\n>  E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> 2017-08-21 16:53:38.788947: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n> 2017-08-21 16:53:38.788956: F tensorflow/core/kernels/conv_ops.cc:672] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\n> ```\r\n> Per another thread, this appears to be an issue with the GPU running out of memory. I have tried to use this code snippet\r\n> \r\n> ```\r\n>     from keras.backend.tensorflow_backend import set_session\r\n>     config = tf.ConfigProto()\r\n>     config.gpu_options.allow_growth = True\r\n>     config.gpu_options.per_process_gpu_memory_fraction = 0.1\r\n>     set_session(tf.Session(config=config))\r\n> ```\r\n> but no luck. I have been checking nvidia-smi. I get the following :\r\n> \r\n> ```\r\n> +-----------------------------------------------------------------------------+\r\n> | NVIDIA-SMI 384.59                 Driver Version: 384.59                    |\r\n> |-------------------------------+----------------------+----------------------+\r\n> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n> |===============================+======================+======================|\r\n> |   0  GeForce GTX 108...  Off  | 00000000:02:00.0  On |                  N/A |\r\n> | 20%   30C    P8     9W / 250W |    427MiB / 11169MiB |      4%      Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n>                                                                                \r\n> +-----------------------------------------------------------------------------+\r\n> | Processes:                                                       GPU Memory |\r\n> |  GPU       PID  Type  Process name                               Usage      |\r\n> |=============================================================================|\r\n> |    0      1383    G   /usr/lib/xorg/Xorg                             271MiB |\r\n> |    0      1991    G   compton                                          3MiB |\r\n> |    0      2143    G   ...el-token=3B20FA9DA27556BEE46CF45A65B73A9B   107MiB |\r\n> |    0      8279    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    42MiB |\r\n> +-----------------------------------------------------------------------------+\r\n> ```\r\n\r\nThanks, that is useful."]}, {"number": 8878, "title": "contrib.learn.Estimators needs updating to summary.scalar", "body": "### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root   556000 Mar 27 15:57 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Mar 27 15:57 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Mar 27 15:57 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root root   415432 Mar 27 15:57 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Mar 27 15:57 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Mar 30 20:42 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       18 Mar 30 20:42 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 root root 84163560 Mar 30 20:42 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814 Mar 30 20:42 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\ngit rev ersion\r\n```\r\nddef51c1c93f66989ac7a88b88b89f5b2a9df599\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nUsing the example at https://www.tensorflow.org/get_started/tflearn triggers the warning\r\n\r\n### What other attempted solutions have you tried?\r\nI'll make a PR for this\r\n", "comments": ["Thanks so much for taking this fix on and contributing back!", "This is fixed. "]}, {"number": 8877, "title": "nvidia-smi: No running processes found     ", "body": "I'm running Ubuntu 14.04 LTS on AWS g2.2xlarge\r\n\r\n\r\n$ python neural_gpu_trainer.py --problem=bmul\r\n \r\n...\r\n\r\nmodprobe: ERROR: could not insert 'nvidia_375_uvm': Invalid argument\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/33970755/tensorflow-not-using-gpu\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root 179466 Mar 27 16:01 /usr/local/cuda/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Mar 27 16:01 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.0\r\nlrwxrwxrwx 1 root root     19 Mar 27 16:01 /usr/local/cuda/lib/libcudart.so.7.0 -> libcudart.so.7.0.28\r\n-rwxr-xr-x 1 root root 303052 Mar 27 16:01 /usr/local/cuda/lib/libcudart.so.7.0.28\r\n-rw-r--r-- 1 root root 546514 Mar 27 16:01 /usr/local/cuda/lib/libcudart_static.a\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n$ conda install -c jjh_cio_testing tensorflow-gpu\r\n\r\n1. A link to the pip package you installed:\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nubuntu@ip-10-0-1-131:~/cuda/lib64$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally\r\n1.0.1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nubuntu@ip-10-0-1-131:~/models/neural_gpu$ python neural_gpu_trainer.py --problem=bmul\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally\r\nGenerating data for bmul.\r\ncut 1.20 lr 0.100 iw 0.80 cr 0.30 nm 64 d0.1000 gn 4.00 layers 2 kw 3 h 4 kh 3 batch 32 noise 0.00\r\nCreating model.\r\nCreating backward pass for the model.\r\nWARNING:tensorflow:Tried to colocate gpu0/gradients/gpu0/Gather_2_grad/Shape with an op target_embedding/read that had a different device: /device:GPU:0 vs /device:CPU:0. Ignoring colocation property.\r\nCreated model for gpu 0 in 6.46 s.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nmodprobe: ERROR: could not insert 'nvidia_375_uvm': Invalid argument\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: ip-10-0-1-131\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: ip-10-0-1-131\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 346.46.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  346.46  Tue Feb 17 17:56:08 PST 2015\r\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 346.46.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 346.46.0\r\nCreated model. Checkpoint dir /tmp/neural_gpu\r\nReading model parameters from /tmp/neural_gpu/neural_gpu.ckpt-500\r\nstep 600 step-time 1.15 train-size 0.051 lr 0.100000 grad-norm 1.3013 len 6 ppl 1.335808 errors 29.08 sequence-errors 23.34\r\n  bin 0 (2)\tbmul\tppl 1.08 errors 0.00 seq-errors 0.00\r\n  bin 1 (3)\tbmul\tppl 1.07 errors 0.00 seq-errors 0.00\r\n  bin 2 (4)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 3 (5)\tbmul\tppl 1.14 errors 2.90 seq-errors 5.47\r\n  bin 4 (6)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 7 (9)\tbmul\tppl 1.67 errors 23.42 seq-errors 61.72\r\n  bin 10 (12)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 13 (15)\tbmul\tppl 1.94 errors 36.26 seq-errors 96.88\r\n  bin 16 (18)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 19 (21)\tbmul\tppl 2.07 errors 41.34 seq-errors 100.00\r\n  bin 22 (24)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 25 (27)\tbmul\tppl 2.11 errors 44.47 seq-errors 100.00\r\n  bin 28 (30)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 31 (33)\tbmul\tppl 2.14 errors 44.22 seq-errors 100.00\r\n  bin 34 (36)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 37 (39)\tbmul\tppl 2.16 errors 46.27 seq-errors 100.00\r\n  bin 40 (42)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 43 (45)\tbmul\tppl 2.17 errors 45.92 seq-errors 100.00\r\n  bin 46 (48)\tbmul\tppl NA errors NA seq-errors NA\r\nstep 700 step-time 1.04 train-size 0.051 lr 0.100000 grad-norm 1.2822 len 7 ppl 1.305894 errors 24.70 sequence-errors 20.28\r\n  bin 0 (2)\tbmul\tppl 1.08 errors 0.00 seq-errors 0.00\r\n  bin 1 (3)\tbmul\tppl 1.07 errors 0.00 seq-errors 0.00\r\n  bin 2 (4)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 3 (5)\tbmul\tppl 1.09 errors 0.00 seq-errors 0.00\r\n  bin 4 (6)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 7 (9)\tbmul\tppl 1.65 errors 25.41 seq-errors 59.38\r\n  bin 10 (12)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 13 (15)\tbmul\tppl 1.94 errors 35.41 seq-errors 100.00\r\n  bin 16 (18)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 19 (21)\tbmul\tppl 2.08 errors 42.48 seq-errors 100.00\r\n  bin 22 (24)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 25 (27)\tbmul\tppl 2.14 errors 44.49 seq-errors 100.00\r\n  bin 28 (30)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 31 (33)\tbmul\tppl 2.16 errors 46.73 seq-errors 100.00\r\n  bin 34 (36)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 37 (39)\tbmul\tppl 2.20 errors 47.76 seq-errors 100.00\r\n  bin 40 (42)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 43 (45)\tbmul\tppl 2.22 errors 49.13 seq-errors 100.00\r\n  bin 46 (48)\tbmul\tppl NA errors NA seq-errors NA\r\nstep 800 step-time 1.64 train-size 0.051 lr 0.100000 grad-norm 1.3836 len 8 ppl 1.393354 errors 30.55 sequence-errors 30.53\r\n  bin 0 (2)\tbmul\tppl 1.08 errors 0.00 seq-errors 0.00\r\n  bin 1 (3)\tbmul\tppl 1.08 errors 0.00 seq-errors 0.00\r\n  bin 2 (4)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 3 (5)\tbmul\tppl 1.11 errors 0.00 seq-errors 0.00\r\n  bin 4 (6)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 7 (9)\tbmul\tppl 1.65 errors 23.67 seq-errors 63.28\r\n  bin 10 (12)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 13 (15)\tbmul\tppl 1.91 errors 37.00 seq-errors 97.66\r\n  bin 16 (18)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 19 (21)\tbmul\tppl 2.07 errors 43.81 seq-errors 100.00\r\n  bin 22 (24)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 25 (27)\tbmul\tppl 2.10 errors 44.66 seq-errors 100.00\r\n  bin 28 (30)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 31 (33)\tbmul\tppl 2.13 errors 45.06 seq-errors 100.00\r\n  bin 34 (36)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 37 (39)\tbmul\tppl 2.16 errors 45.51 seq-errors 100.00\r\n  bin 40 (42)\tbmul\tppl NA errors NA seq-errors NA\r\n  bin 43 (45)\tbmul\tppl 2.21 errors 47.88 seq-errors 100.00\r\n  bin 46 (48)\tbmul\tppl NA errors NA seq-errors NA\r\n\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n```\r\nubuntu@ip-10-0-1-131:~/cuda/lib64$ nvidia-smi\r\nFri Mar 31 17:22:46 2017       \r\n+------------------------------------------------------+                       \r\n| NVIDIA-SMI 346.46     Driver Version: 346.46         |                       \r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n| N/A   37C    P0    35W / 125W |     10MiB /  4095MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+--------------------------------------------\r\n### Logs or other output that would be helpful\r\n\r\n```\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["```\r\nubuntu@ip-10-0-1-131:~/cuda/lib64$ cd ~/NVIDIA_CUDA-7.0_Samples/1_Utilities/deviceQuery\r\nubuntu@ip-10-0-1-131:~/NVIDIA_CUDA-7.0_Samples/1_Utilities/deviceQuery$ ./deviceQuery \r\n./deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nmodprobe: ERROR: could not insert 'nvidia_375_uvm': Invalid argument\r\ncudaGetDeviceCount returned 30\r\n-> unknown error\r\nResult = FAIL\r\n```", "It seems you are using a mix of cuda 7 and cuda 7.5? Please try CUDA 8.", "I tried CUDA 8.  I got errors.  Do you know if CUDA 8 works on g2.2xlarge instances with the NVIDIA GK104GL [GRID K520]?\n\n$ lspci | grep -i nvidia\n00:03.0 VGA compatible controller: NVIDIA Corporation GK104GL [GRID K520] (rev a1)\n\n\n> On Mar 31, 2017, at 10:40 AM, Andrew Selle <notifications@github.com> wrote:\n> \n> It seems you are using a mix of cuda 7 and cuda 7.5? Please try CUDA 8.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/8877#issuecomment-290778962>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AC9i20n-sqbbFXCVQyJ-roJPglota2VFks5rrTqGgaJpZM4Mv8Hb>.\n> \n\n", "CUDA 7.5 requires driver version 352.xx or newer but you seem to be using an older version, can you update the driver?", "Yes.\n\nThis is what I installed.  Do you know the URI to the correct installer?\n\n$sudo wget http://developer.download.nvidia.com/compute/cuda/7_0/Prod/local_installers/cuda_7.0.28_linux.run\n$sudo chmod 700 ./cuda_7.0.28_linux.run\n$sudo ./cuda_7.0.28_linux.run\n$sudo update-initramfs -u\n$sudo reboot\n\n> On Mar 31, 2017, at 10:52 AM, Jonathan J. Helmus <notifications@github.com> wrote:\n> \n> CUDA 7.5 requires driver version 352.xx or newer but you seem to be using an older version, can you update the driver?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/8877#issuecomment-290782021>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AC9i2503VfzlTfScnXMqC9cfXsyJRvRAks5rrT1lgaJpZM4Mv8Hb>.\n> \n\n", "Ubuntu has a page on how to install the NVidia drivers, https://help.ubuntu.com/community/BinaryDriverHowto/Nvidia.  You can check the version using:\r\n\r\n```\r\ncat /proc/driver/nvidia/version\r\n```", "Does this mean the installation of NVIDIA-Linux-x86_64-367.57 failed?\n\nubuntu@ip-10-0-1-135:~/nvidia-graphics-drivers$ sudo ./NVIDIA-Linux-x86_64-367.57-no-compat32.run \n\nUnable to load: nvidia-installer ncurses v6 user interface\n\nUsing: nvidia-installer ncurses user interface\n-> Detected 8 CPUs online; setting concurrency level to 8.\n-> License accepted.\n-> Installing NVIDIA driver version 367.57.\n-> Running distribution scripts\n   executing: '/usr/lib/nvidia/pre-install'...\n-> done.\n-> The distribution-provided pre-install script failed!  Are you sure you want to continue? (Answer: Continue installation)\n-> Would you like to register the kernel module sources with DKMS? This will allow DKMS to automatically build a new module, if you install a different kernel later. (Answer: Yes)\n-> Installing both new and classic TLS OpenGL libraries.\n-> Installing both new and classic TLS 32bit OpenGL libraries.\n-> Will install GLVND GLX client libraries.\nLooking for install checker script at ./libglvnd_install_checker/check-libglvnd-install.sh\n   executing: '/bin/sh ./libglvnd_install_checker/check-libglvnd-install.sh'...\n   Checking for libglvnd installation.\n   Checking libGLdispatch...\n   Checking libGLdispatch dispatch table\n   Checking call through libGLdispatch\n   All OK\n   libGLdispatch is OK\n   Checking for libGLX\n   libGLX is OK\n   Checking entrypoint library libOpenGL.so.0\n   Checking call through libGLdispatch\n   Checking call through library libOpenGL.so.0\n   All OK\n   Entrypoint library libOpenGL.so.0 is OK\n   Checking entrypoint library libGL.so.1\n   Checking call through libGLdispatch\n   Checking call through library libGL.so.1\n   dlopen(\"libGL.so.1\") failed: libGL.so.1: cannot open shared object file: No such file or directory\n-> An incomplete installation of libglvnd was found. Do you want to install a full copy of libglvnd? This will overwrite any existing libglvnd libraries. (Answer: Install and overwrite existing files)\nWill install libglvnd libraries.\n-> Searching for conflicting files:\n-> done.\n-> Installing 'NVIDIA Accelerated Graphics Driver for Linux-x86_64' (367.57):\n   executing: '/sbin/ldconfig'...\n   /sbin/ldconfig.real: /usr/local/lib/libcudnn.so.6.5 is not a symbolic link\n\n-> done.\n-> Driver file installation is complete.\n-> Installing DKMS kernel module:\n-> done.\nERROR: Unable to load the 'nvidia-drm' kernel module.\nERROR: Installation has failed.  Please see the file '/var/log/nvidia-installer.log' for details.  You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.\n~                            \n\nubuntu@ip-10-0-1-135:~/nvidia-graphics-drivers$ ls -l /usr/local/lib/*dnn*\n-rwxr-xr-x 1 root root 11172416 May  7  2015 /usr/local/lib/libcudnn.so\n-rwxr-xr-x 1 root root 11172416 May  7  2015 /usr/local/lib/libcudnn.so.6.5\n-rwxr-xr-x 1 root root 11172416 May  7  2015 /usr/local/lib/libcudnn.so.6.5.48\n-rw-r--r-- 1 root root 11623922 May  7  2015 /usr/local/lib/libcudnn_static.a\nubuntu@ip-10-0-1-135:~/nvidia-graphics-drivers$ cat /proc/driver/nvidia/version\ncat: /proc/driver/nvidia/version: No such file or directory\n\n\n> On Mar 31, 2017, at 11:02 AM, Jonathan J. Helmus <notifications@github.com> wrote:\n> \n> cat /proc/driver/nvidia/version\n\n", "I successfully installed the NVIDIA 367.57 driver and now 'nvidia-smi' is working:\r\n\r\n```\r\nubuntu@ip-10-0-1-70:~/NVIDIA_CUDA-7.0_Samples/1_Utilities/deviceQuery$ nvidia-smi\r\nSat Apr  1 17:30:19 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n| N/A   41C    P0    77W / 125W |   3800MiB /  4036MiB |     92%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     19821    C   python                                        3798MiB |\r\n+-----------------------------------------------------------------------------+\r\nubuntu@ip-10-0-1-70:~/NVIDIA_CUDA-7.0_Samples/1_Utilities/deviceQuery$ \r\n\r\n```"]}, {"number": 8876, "title": "DNNClassifier init failed TypeError", "body": "Following the tutorial, when I define the classifier:\r\n\r\nclassifier = skflow.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3)\r\n\r\nit triggers the following error:\r\n\r\ninit() takes at least 3 arguments (3 given)", "comments": ["Isn't **feature_columns** a required argument?", "Yes!! That was that, very simple. It turns out that in new versions this is a required argument, whereas in earlier versions from which I was following a tutorial, it was not. Thanks a lot!", "So, what exactly is the feature_column attribute?\r\n"]}]