[{"number": 23814, "title": "[DeepLabV3+] How to get class probabilities in C++ using frozen graph?", "body": "It looks like I should get the output from layer ResizeBilinear_3? This is the layer before argmax, according to tensorflow/models#4045. @aquariusjay\r\nHowever, the numbers I got do not look like probabilities as they contained negative numbers and numbers much bigger than 1.\r\n![image](https://user-images.githubusercontent.com/2267796/48654874-4af1f480-e9c5-11e8-9b20-215e144c3566.png)\r\nFYI, the model I'm using is **deeplabv3_xception_ade20k_train** from the model zoo. It has 150 object classes + 1 unknown class so in total 151 classes.\r\n\r\nThank you!", "comments": ["You need to use softmax to get a probability distribution. Softmax doesn't change order so it's irrelevant you are just going to argmax immediately but if you want real probabilities you need softmax.", "Indeed, it's from the output from layer ResizeBilinear_3 to get the class probabilities, albeit there are negative numbers and numbers much bigger than 1.\r\n\r\nMy followup question is whether we know the fixed range, i.e. smallest negative value and largest positive value that are fixed, in order to properly normalize? Or simply apply softmax?\r\n@aquariusjay @jtavrisov \r\n\r\nThank you!", "I am also working with the inner lays ResizeBilinear_3 and Argmax. @ZF-AR did you find out anything about the fixed range?\r\n"]}, {"number": 23783, "title": "CudaRoot() during compilation should not be used ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:9.0/7.2\r\n- GPU model and memory:n/a\r\n\r\n\r\n**Describe the current behavior**\r\nIt can be seen from https://github.com/tensorflow/tensorflow/search?q=CudaRoot&unscoped_q=CudaRoot, that the `CudaRoot()` function is used by tensorflow to:\r\n\r\n1. Get the path to libdevice library\r\n2. Get the path to ptxas binary \r\n\r\nHowever, the return value of `CudaRoot()` function is a constant that's determined during compilation.\r\nAs a result, tensorflow's XLA cannot work properly, if it is used on a machine where the path to CUDA is different from the compilation machine.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe path to cuda should be dynamically obtained (by configuration or environment variables).\r\n\r\n**Code to reproduce the issue**\r\nUsing the command in the end of https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473, posted yesterday by @tfboyd .\r\n\r\n**Other info / logs**\r\nWith XLA, saw errors like:\r\n```\r\n2018-11-15 14:07:40.696214: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:794] Failed to compile ptx to cubin.  Will attempt to let GPU driver compile the ptx. Not found: /usr/local/cuda\r\n-9.0/bin/ptxas not found\r\n```\r\n", "comments": ["Partially addressed by https://github.com/tensorflow/tensorflow/commit/bafb8747983fbcf186ffb063ed39dbb0a18e3c8e, but it's unclear how a user is supposed to use it.", "According to this,\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/bafb8747983fbcf186ffb063ed39dbb0a18e3c8e#diff-888940c777f83519bd1e27349427d51eR116\r\n\r\nIs this the right approach to use it?\r\n```\r\nXLA_FLAGS=\"--xla_gpu_cuda_data_dir=/usr/\" python ml_code.py\r\n```", "Facing similar issues, tried out what @hyoo provided, still didnt work, is there a fix for this?", "Is this solved? I'm getting the same issue with it looking at /usr/local/cuda/bin/ptxas instead of where I actually want it to look...", "Same issue, but I encounted it on JAX. I solved by setting two environement variables: \r\n`export LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:$PATH\"`  \r\n`export XLA_FLAGS='--xla_gpu_cuda_data_dir=/usr/local/cuda/'`\r\nWhile, you have to point to your cuda path.\r\nAfter that, run your code as usual:\r\n`python run.py`"]}, {"number": 23762, "title": "TypeError: visualize_boxes_and_labels_on_image_array() takes at least 7 arguments (8 given)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**My code**\r\n\r\nfor image_path in TEST_IMAGE_PATHS:\r\n  image = Image.open(image_path)\r\n  image_np = load_image_into_numpy_array(image)\r\n  image_np_expanded = np.expand_dims(image_np, axis=0)\r\n  output_dict = run_inference_for_single_image(image_np, detection_graph)\r\n  vis_util.visualize_boxes_and_labels_on_image_array(\r\n      image_np,\r\n      output_dict['detection_boxes'],\r\n      output_dict['detection_classes'],\r\n      output_dict['detection_scores'],\r\n      category_index,\r\n      instance_masks=output_dict.get('detection_masks'),\r\n      use_normalized_coordinates=True,\r\n      line_thickness=8)\r\n  plt.figure(figsize=IMAGE_SIZE)\r\n  plt.imshow(image_np)\r\n\r\n**Describe the current behavior**\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-b19082c2666b> in <module>()\r\n     17       instance_masks=output_dict.get('detection_masks'),\r\n     18       use_normalized_coordinates=True,\r\n---> 19       line_thickness=8)\r\n     20   plt.figure(figsize=IMAGE_SIZE)\r\n     21   plt.imshow(image_np)\r\n\r\nTypeError: visualize_boxes_and_labels_on_image_array() takes at least 7 arguments (8 given)\r\n\r\n**Describe the expected behavior**\r\ndetected files(.jpg)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I dont know what is wrong with it....\r\nI just followed object_detection_tutorial.ipynb"]}, {"number": 23761, "title": "C++  TensorShape from std::vector or std::array", "body": "**System information**\r\n- TensorFlow version (you are using): **r1.12**\r\n- Are you willing to contribute it (Yes/No): **Yes**\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAccording to [`tensor_shape.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_shape.h), it seems like currently `TensorShape` can only be initialized through `gtl::ArraySlice` or through `std::initializer_list`. It would be nice to initialize it with `std::vector` or `std::array`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nExplained above.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nIf the shape of the Tensors used are stored in some other place as something like a `std::vector`, it will be easier to just pass the shape variable as the shape argument when creating a Tensor.", "comments": ["@mrry can't we create an ArraySlice implicitly from a std::vector?  iirc, it's basically the same as absl::Span, and absl::Span can be implicitly created from a std::vector?", "It's literally an `absl::Span` now:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3c257d337439f863759d1884c88d17c2b0d685b5/tensorflow/core/lib/gtl/array_slice.h#L28\r\n\r\nI'm pretty sure it's implicitly convertible from an `std::vector`."]}, {"number": 23754, "title": "Sparsification of huge matrix", "body": "**Describe the current behavior**\r\nCurrently, the only way in TensorFlow to convert a dense matrix into a sparse one (assuming it has only few non-zero values) is to use `tf.where` (afaik). `tf.where` does however allocate a lot of memory (it seems like 4 times the size of the dense matrix). So for huge dense matrices, this approach fails.\r\n\r\n**Describe the expected behavior**\r\nBy first counting the number of nonzero elements, then allocating the appropriate amount of memory, and then determining the non-zero values, this can be be done with very little memory (as much as there are non-zero elements).\r\nThis is the approach that the `cusparse` library follows and it works great. Unfortunately, there is no way to use e.g. `dense2csr` or `dense2coo` in TensorFlow.\r\n\r\nIn my opinion this is a bug, because `tf.where` requires far too much memory.", "comments": ["@martinwicke do you know if anyone owns tf.where / SparseTensors?", "I don't know of anyone specifically responsible for tf.where.\r\n\r\nThe fact that we don't use cusparse is an issue. Writing a specialized op for this would probably be a good idea (basically to_sparse(dense) -> tf.SparseTensor via dense2coo).", "FYI: it might not be as bad as I reported (I accidentally had a rather dense matrix); but it's clearly no where near cusparse dense2coo. Would be great if this could be improved. Or would it be possible to access the GPU memory of a tensor directly and use it in another framework (in PyTorch I can share tensors using dlpack with CuPy which has cusparse support). dlpack support for TensorFlow would be awesome.", "@asimshankar @alextp In Python (and for EagerTensors), we have Tensor.numpy(). But that still causes a transfer to host memory and possibly a copy if things are not aligned. Should we allow access to the TensorBuffer from Python? \r\n\r\nClearly, users would need to be responsible with resulting memory issues, but as long as the Python Tensor object isn't destroyed the memory should also be safe, correct?", "@ebrevdo has looked at cusparse in tf.\r\n\r\nIt's not hard to add a way to get the GPU memory of a tensor, but it's likely to be inefficient because we'd need to sync the GPU stream before that memory was useful for anything, and using anything you computed using that memory would require another sync.\r\n\r\nOTOH it's not that much C++ code to make tf ops to do exactly this thing you want.", "We have some TF code that basically adds support for CSR matrices in tensorflow, using cusparse and DT_VARIANT types.  But we don't have a maintainer.  I'm tempted to do this as a SIG and get some distributed maintenance help from e.g. external users & nvidia.  @martinwicke @alextp wdyt?", "(unfortunately I don't have enough cycles to maintain the codebase!)", "Trying to put this out as a SIG would be great.\n\nOn Tue, Nov 20, 2018 at 5:12 PM ebrevdo <notifications@github.com> wrote:\n\n> (unfortunately I don't have enough cycles to maintain the codebase!)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23754#issuecomment-440490694>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxc7FXjBukLIlsINLUSem0cLUUPj1ks5uxKiVgaJpZM4YesLe>\n> .\n>\n\n\n-- \n - Alex\n", "Probably SIG-addons is the right place for this. It'll be maintaining several ops. There's a call for content out now on the mailing list.\r\n\r\n@armando-fandango \r\n@seanpmorgan ", "@martinwicke I [started a thread](https://groups.google.com/a/tensorflow.org/forum/#!topic/addons/xS4-QKgP4WI) to keep track of new Addons that we plan to review after our initial move out of contrib\r\n\r\n@ebrevdo Would you be able to provide a link to the code for CSR matrix support using cusparse", "That code is not currently public", "There's a plan to release the code as part of a SIG repository; I don't have an ETA."]}, {"number": 23735, "title": "Riccati solver suggest", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.7.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe Discrete/Continuous Algebraic Riccati Equation is very important to linear quadratic regulator system. A solution of solving this need to be implemented in Tensorflow if one wants to combine machine learning with control theory. \r\n\r\nThe solver has been implemented in [Matlab](https://se.mathworks.com/help/control/ref/dare.html)/Scipy etc. But there are also possibilities to implement in Tensorflow CPU/GPU.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nThose who wants to combine control theory with machine learning and deep leanring\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 23725, "title": "Sparse Precision Matrix implemetations (for gaussian processes or GMRFs)", "body": "Are there any plans in the near future to implement sparse precision matrix computations for TensorFlow? My main use case when I'll be using this is for models like a Gaussian Process where a sparse precision matrix can be fed in as a Gaussian Markov Random Field to evaluate the GP (the sparse representation of an AR(1) GP is only tri-diagonal whereas the covariance matrix is dense).\r\n\r\nBeing able to evaluate on sparse matrices would be super helpful in computation speeds I think. Thoughts?\r\n\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\nOnly additions I hope\r\n\r\n**Who will benefit with this feature?**\r\nStatistical modelers with high dimensional sparse data\r\n\r\n**Any Other info.**\r\n", "comments": ["@wt-huang For what its worth, the math for the implementation is already all coded up here in C++: https://github.com/kaskr/adcomp/blob/8768ff9e28c3e6a9a162a2d93cba073a71b0d4d9/TMB/inst/include/tmbutils/density.hpp#L820-L939"]}, {"number": 23713, "title": "Audio recognition on 8,000 samples per second audio data, label_wav.py", "body": "I followed the Simple Audio Recognition tutorial given on tensorflow website. Everything went good. I downloaded the google speech command data set, trained a model using default setting, got 87% test accuracy, and then tested on some other simple audio clips using  label_wav.py code given as tutorial.\r\n\r\nBut I need to decode audio files of 8,000 sps. The results of this model is not good on 8000 sps audios. reading from tensorflow audio recognition tutorial I got to know that I need to use 8000 sps data for training.\r\n\r\nI downsampled all audio files in data set and trained the model again with this downsampled data using the following command:\r\n\r\npython tensorflow/examples/speech_commands/train.py --data_url= --data_dir=C:\\tmp\\speech_dataset --sample_rate=8000\r\n\r\nThe accuracy of this model is good i.e. Final test accuracy = 87.2% . But the problem is when I am testing (labeling) any audio file using this model and by using the code label_wav.py I am not getting the right results even for the audio files from the data set.\r\n\r\nI am not getting where the problem is? I searched alot but didn't find any solution. I also read the complete tensorflow tutorial on audio recognition.\r\n\r\nKindly Help me!!!! How to decode 8k sps audio clips using the model trained on 8k sps audio data and by using the given label_wav.py Python code?", "comments": []}, {"number": 23639, "title": "`var_list` will cause untrainable variables to be trained", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac 10.13\r\n- TensorFlow installed from (source or binary): `pip`\r\n- TensorFlow version (use command below): `v1.9.0-0-g25c197e023 1.9.0`\r\n- Python version: `3.6.5`\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n```v1.9.0-0-g25c197e023 1.9.0```\r\n\r\n**Describe the current behavior**\r\nBoth trainable and untrainable variables are updated by the optimizer\r\n\r\n**Describe the expected behavior**\r\nExpected: trainable variables are updated by optimizer, but untrainable variables are not.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable([1.0], trainable=False)\r\ny = tf.Variable([1.0], trainable=True)\r\nz = x + y\r\n\r\ntrain_op = tf.train.GradientDescentOptimizer(0.1).minimize(z, var_list=[x, y])\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(train_op)\r\n\r\nprint(sess.run(x))  # [0.9]  Unexpected\r\nprint(sess.run(y))  # [0.9]  Expected\r\n```\r\n", "comments": ["@machinaut I think this is intended behavior and not a bug. Trainable is just a flag to help manage variables collections."]}, {"number": 23610, "title": "Causal Convolutions in Tensorflow", "body": "**System information**\r\n- TensorFlow version (you are using): 1.10\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis is feature request to support **causal convolutions** in the Tensorflow layers API. Currently, causal convolutions require `tf.nn.conv2d(...)` with manual masking of weights. To improve speed, the data can be cropped/padded and then ordinary convolutions used.  \r\n\r\n**Will this change the current api? How?**\r\nYes, `tf.layers.conv2d` will have a `causal` boolean flag.\r\n\r\n**Who will benefit with this feature?**\r\nResearchers and practitioners working on auto-regressive convolutional models.\r\n\r\n**Any Other info.**\r\nAn analysis at OpenAI revealed that many open-source implementations of causal convolutions are slow, incorrect, and/or have uninterpretable code. This feature would help the ML community more easily develop architectures containing causal convolutions.", "comments": ["Adding this to the API would primarily impact Keras, so reassigning this to @fchollet.", "As far as I know, [ tf.keras.layers.Conv1D ](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) supports causal padding. Does it work for you ?", "The issue in question relates to _2D Convolutions_, which are significantly harder to implement. ", "I think generalizing the approach to causal padding used by `tf.keras.layers.Conv1D` to n-dimensional convolutions would be straight-forward enough if we assumed there was only one causal/\"time\" dimension. The 2-d case could be useful for generating audio spectrograms, the 3-d for video generation. We'd have to add an arg to the API for the user to specify which dimension is causal. Also, rather than adding a `causal` flag, Keras does causal convolutions with the `padding=\"causal\"` arg.\r\n\r\nThe implementation of causal padding in `tf.keras.layers.Conv1d` uses the crop/pad strategy described in the feature request and could be generalized to this n-dim case. @joshim5 would you like to submit a PR for this? If not I can take a look", "@omalleyt12 I agree that extending to the 2-d case under the assumption of a single casual/\"time\" dimension is straight-forward. However, the more popular case (and more often mistaken case) is when there are multiple causal dimensions, as in images.\r\n\r\nWhat do you think of extending the API to capture this more sophisticated case? ", "@fchollet What do you think about this API extension (adding `causal` arg to Conv2D that either takes a bool or the same of a masking type)?"]}, {"number": 23605, "title": "Tensorboard projector visualisation - PCA keeps loading or not working", "body": "Tensorboard projector visualisation - PCA keeps hanging.\r\nI wrote a simple NN to predict the class type of iris dataset. NN model works fine.\r\n```\r\nimport pandas as pd \r\nimport numpy as np\r\n\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn import preprocessing\r\n\r\nimport tensorflow as tf\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\niris_data = load_iris()\r\nx = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\r\ny = pd.DataFrame(iris_data.target, columns=['class'])\r\n\r\nencoder = preprocessing.OneHotEncoder(categories='auto')\r\nencoder.fit(y)\r\n#Transform\r\ny_enc = encoder.transform(y).toarray()\r\n\r\nx_train, x_test, y_train, y_test = train_test_split(x, y_enc)\r\n\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(8, name='input_layer', activation=tf.nn.relu, input_shape=(x_train.shape[1],)))\r\nmodel.add(keras.layers.Dense(4, name='hidden_layer', activation=tf.nn.relu, input_shape=(x_train.shape[1],)))\r\nmodel.add(keras.layers.Dense(3, name='out_layer', activation=tf.nn.softmax))\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.005),\r\n              loss=keras.losses.binary_crossentropy,\r\n              metrics=[keras.metrics.categorical_accuracy])\r\nmodel.fit(x_train, y_train, epochs=50, verbose=0)\r\nresult = model.predict(x_test)\r\n```\r\nNow I am trying to visualise the output of the test set. Below is the code for Tensorboard projector. I don't what I am missing but PCA keeps loading even after starting the Tensorboard several minutes ago.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.tensorboard.plugins import projector\r\nimport numpy as np\r\nimport os\r\n\r\nLOG_DIR = 'logs'  # FULL PATH HERE!!!\r\n\r\nmetadata_file = os.path.join(LOG_DIR, 'metadata.tsv')\r\nwith open(metadata_file, 'w') as f:\r\nf.write('{}\\t{}\\n'.format('class_name','class_id'))\r\nwith open(metadata_file, 'a') as f:\r\nfor i in range(len(y_test)):\r\n    c = np.nonzero(y_test[i])[0][0]\r\n    f.write('{}\\t{}\\n'.format(iris_data.target_names[c],c))\r\n\r\n\r\nembedding_var = tf.Variable(result,  name='final_layer_embedding')\r\nsess = tf.Session()\r\nsess.run(embedding_var.initializer)\r\nsummary_writer = tf.summary.FileWriter(LOG_DIR)\r\nconfig = projector.ProjectorConfig()\r\nembedding = config.embeddings.add()\r\nembedding.tensor_name = embedding_var.name\r\n\r\nembedding.metadata_path = 'metadata.tsv'\r\n\r\nprojector.visualize_embeddings(summary_writer, config)\r\nsaver = tf.train.Saver([embedding_var])\r\nsaver.save(sess, os.path.join(LOG_DIR, 'model.ckpt'), 1)\r\n```\r\nI googled to understand what I am doing wrong but I could not fix it. Despite being my model is small very I could not visualise. Any help to resolve this problem would be highly appreciable.\r\n", "comments": ["I had the same problem that was caused by an incorrect TSV file of vectors. Google Chrome console stated:\r\n```\r\nUncaught (in promise) Error: no convergence.\r\n```\r\nThe problem was that there was a text column containing metadata, between the numeric columns (vectors).\r\nSee my demo of error and solution [here](https://gist.github.com/printomi/13f2e49ce4bebee31744a66a1eca0395).", "Same issue here when I try to upload a tsv file to https://projector.tensorflow.org/. \r\n\r\nIn the console I get the error: (index):1 Uncaught (in promise) Error: no convergence.\r\nI've checked @printomi's suggestion and my file does not contain a text column.\r\n\r\nMy file looks like this:\r\nA\\tB\\tC\\tD\r\n97\\t92\\t16\\t4\r\n9\\tv69\\t48\\t64\r\n10\\t12\\t13\\t78\r\n\r\nWhat am I doing wrong here?\r\n\r\nThanks for the help!\r\n"]}, {"number": 23601, "title": "Move autograph into a separate package and into a separate repo", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: any\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: any\r\n- **TensorFlow version (use command below)**: any\r\n- **Python version**: any\r\n- **Bazel version (if compiling from source)**: any\r\n- **GCC/Compiler version (if compiling from source)**: any\r\n- **CUDA/cuDNN version**: any\r\n- **GPU model and memory**: any\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\nI guess autograph should be moved into a separate package and repo. Because I guess it may be possible to add support of other computation graph frameworks there.\r\n\r\n", "comments": ["Indeed, and we plan to do that very soon. Due to time constraints, we've initially moved it inside TF core, and from there we'll extract the reusable parts into a standalone repo. We hope to have that set up by the end of this year."]}, {"number": 23566, "title": "tf.nn.embedding_lookup_sparse Converting sparse IndexedSlices Warning", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r1.11\r\n- Python version: 3.6\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10/7.3\r\n\r\n**Describe the current behavior**\r\nUsing embedding_lookup_sparse raises the following warning:\r\n\r\n> UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \"Converting sparse IndexedSlices to a dense Tensor of unknown shape.\"\r\n\r\nLooking at embedding_lookup_sparse implementation, the issue comes from the gather after embedding lookup.\r\n\r\n```\r\n...\r\nids = sp_ids.values\r\nids, idx = array_ops.unique(ids)\r\n\r\nembeddings = embedding_lookup(params,ids,...)\r\n...\r\nembeddings = array_ops.gather(embeddings, idx)\r\n```\r\n\r\n**Describe the expected behavior**\r\nI didn't expect the sparse gradients to be passed to an op that can't back-propagate them inside \r\nembedding lookup sparse.\r\n\r\nIs there a reason this op does not call embedding_lookup on sp_ids.values directly? As opposed to\r\nlooking up the unique ids and calling gather on those rows ?\r\n", "comments": ["It was an optimization to unique ids before sending the RPC calls. It came through [here](https://github.com/tensorflow/tensorflow/commit/415ea7360d3f57249fc18e068852a8b8ce6d7f77#diff-fbcd994b5a5d10595f0307b63f247ff7).\r\n\r\n@jhseu any further comment?", "Thank you for looking into it. I suspected it could be something like that, it does have the problem I described though. There's an implicit conversion to a dense tensor, so the gradients are not sparse.", "I am having the same issue with `tf.nn.embedding_lookup`, snippet below:\r\n\r\n```\r\nembedding_matrix = tf.get_variable(\r\n                    name=\"embedding_matrix\",\r\n                    shape=[self._vocab_size, self._embedding_size],  # Python ints, [32x128]\r\n                )\r\n\r\ndecoder_inputs = tf.nn.embedding_lookup(embedding_matrix, labels)\r\n```\r\n\r\nWarning triggered when calling `tf.gradients`.\r\n\r\n\r\n", "Any development?", "@davidenunes We see that you are using old version of tensorflow 1.x which is out of support window, We recommend that you upgrade to 2.6.0  and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I am using TensorFlow 2.6 and the warning persists.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23566\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23566\">No</a>\n", "Why is this issue closed? We are still seeing the issue", "@StarWang, please do try to answer the bot form. Hopefully, they will upgrade it, so the bot does not preemptively close issues when no solution is known yet. ", "@LucaCappelletti94 I did. It's so frustrating that such important issue just got ignored and secretly closed"]}, {"number": 23562, "title": "[xrt] Add \"Free all memory\" op", "body": "**System information**\r\n- TensorFlow version (you are using): nightly\r\n- Are you willing to contribute it (Yes/No): N/A\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen a client loses track of xrt-managed state (e.g. because it errors before it could start tracking it) or because it crashes, or because other errors, the allocation stays around on the device side with no way to free it (other than a hard restart of the device). It would be useful to have an xrt op that drops all memory that xrt knows about (perhaps optionally with a preserve list, so the client can say \"I know about these, so don't free them\").\r\n\r\n**Will this change the current api? How?**\r\nThis will likely be a new xrt op.\r\n\r\n**Who will benefit with this feature?**\r\nUsers of the xrt API.\r\n\r\n**Any Other info.**\r\n@michaelisard requested to be assigned.", "comments": ["Just to clarify: if you run the ConfigureDistributedTPU Op, does all the memory get freed? I was assuming you were looking for an Op to free memory that is more lightweight than configure. If configure doesn't free memory then that is a different bug.", "Configure does not free memory as far as I can tell (it certainly doesn't deregister it from XRT, so I can pass it to another execute and it'll do something - I haven't verified whether the underlying memory sticks around). I do also want a more lightweight Op. While we're at it, I'd also like a more lightweight op to interrupt a running XRTExecute run (our models tend to look like state machines in a while loop on the device, side, so if that gets in a bad state, it can be hard to kill).", "OK we will treat 'memory not freed on configure' and 'support free-all-memory without configuring' as separate issues.\r\n\r\nI think it's going to be hard to interrupt a running program without resetting all the chips. If you need to be able to recover from a codegen error then you probably need to do a reset. If you want users to be able to recover from their own errors, I fear you will need to add support in your generated code to yield every so often.", "> I think it's going to be hard to interrupt a running program without resetting all the chips. If you need to be able to recover from a codegen error then you probably need to do a reset. If you want users to be able to recover from their own errors, I fear you will need to add support in your generated code to yield every so often.\r\n\r\nOk, in that case, just making sure that resetting the chips using configure clears all the relevant state would be good. I think right now it sometimes hangs if there's a computation currently running.", "There also something broken with the memory overview on OOM:\r\n```\r\nTensorflow error: Status: Attempting to allocate 235.02M. That was not possible. There are 446.86M free. Due to fragmentation, the largest contiguous region of free memory is 234.77M.;\r\nHBM capacity 8.00G:\r\n       Top-of-Memory reserved:  528.00M (     553,648,128 bytes)\r\n          Currently allocated:  212.71M (     223,038,464 bytes)\r\n         Requested allocation:  235.02M (     246,431,744 bytes)\r\n                               ------------\r\n      Available after request:    7.05G (   7,566,816,256 bytes)\r\n\r\n\r\n\t [[{{node XRTExecute}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n```\r\nSeems like those 7G are actually allocated, but missing from the overview."]}, {"number": 23557, "title": "Memmory leak Using tensorflow::NewSession , C++", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**: 1.12.0-rc0\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.18.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: N/A. Using Cpu Only. \r\n- **GPU model and memory**: N/A. Using Cpu Only. \r\n- **Exact command to reproduce**:\r\n\r\n## Code\r\n```\r\nclass Temp\r\n{\r\n public:\r\n  Temp(){\r\n    std::unique_ptr<tensorflow::Session> session(\r\n      tensorflow::NewSession(tensorflow::SessionOptions()));\r\n    };\r\n};\r\n\r\nint main(int argc, char **argv)\r\n{\r\n  Temp temp;\r\n  return 0;\r\n}\r\n```\r\n\r\n### Describe the problem\r\nWhen creating a unique_ptr to a session using tensorflow::NewSession, a memory leak occurs according to address sanitizer. There is a similar closed issue posted here: [https://github.com/tensorflow/tensorflow/issues/19163](https://github.com/tensorflow/tensorflow/issues/19163) , however I am still having an issue. \r\n\r\n### Source code / logs\r\n\r\n```\r\n==259==ERROR: LeakSanitizer: detected memory leaks\r\n\r\nDirect leak of 24 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7f17c81b7532 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x99532)\r\n    #1 0x7f17c1c1aca7 in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2ca7)\r\n    #2 0x7f17c1c0e19a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cc619a)\r\n    #3 0x7f17bf851fe8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_framework.so+0x62dfe8)\r\n    #4 0x7f17c61ccea4 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_cc.so+0x6284ea4)\r\n    #5 0x7f17bf8abfe8 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_framework.so+0x687fe8)\r\n    #6 0x7f17bf8ac0ac in tensorflow::NewSession(tensorflow::SessionOptions const&) (/tensorflowincludes/bin/libtensorflow_framework.so+0x6880ac)\r\n    #7 0x402bc2 in Temp::Temp() /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:38\r\n    #8 0x402245 in main /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:44\r\n    #9 0x7f17be6c582f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)\r\n\r\nIndirect leak of 4096 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7f17c81b7532 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x99532)\r\n    #1 0x7f17c1c1ab50 in void std::vector<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> >, std::allocator<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> > > >::_M_emplace_back_aux<tensorflow::kernel_factory::OpKernelRegistrar*>(tensorflow::kernel_factory::OpKernelRegistrar*&&) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2b50)\r\n    #2 0x7f17c1c1ae7e in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2e7e)\r\n    #3 0x7f17c1c0e19a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cc619a)\r\n    #4 0x7f17bf851fe8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_framework.so+0x62dfe8)\r\n    #5 0x7f17c61ccea4 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_cc.so+0x6284ea4)\r\n    #6 0x7f17bf8abfe8 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_framework.so+0x687fe8)\r\n    #7 0x7f17bf8ac0ac in tensorflow::NewSession(tensorflow::SessionOptions const&) (/tensorflowincludes/bin/libtensorflow_framework.so+0x6880ac)\r\n    #8 0x402bc2 in Temp::Temp() /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:38\r\n    #9 0x402245 in main /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:44\r\n    #10 0x7f17be6c582f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)\r\n\r\nIndirect leak of 323 byte(s) in 323 object(s) allocated from:\r\n    #0 0x7f17c81b7532 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x99532)\r\n    #1 0x7f17c1c1ad87 in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2d87)\r\n    #2 0x7f17c1c0e19a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cc619a)\r\n    #3 0x7f17bf851fe8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_framework.so+0x62dfe8)\r\n    #4 0x7f17c61ccea4 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_cc.so+0x6284ea4)\r\n    #5 0x7f17bf8abfe8 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_framework.so+0x687fe8)\r\n    #6 0x7f17bf8ac0ac in tensorflow::NewSession(tensorflow::SessionOptions const&) (/tensorflowincludes/bin/libtensorflow_framework.so+0x6880ac)\r\n    #7 0x402bc2 in Temp::Temp() /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:38\r\n    #8 0x402245 in main /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:44\r\n    #9 0x7f17be6c582f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)\r\n\r\nSUMMARY: AddressSanitizer: 4443 byte(s) leaked in 325 allocation(s).\r\n\r\n```\r\n", "comments": ["Hello, I met the same problem, my TF version is 1.3. Anybody could help us solve the problem?", "I do not think there is an actually leak. The relevant code:\r\n\r\n static XlaDeviceOpRegistrations* registrations =\r\n      RegisterXlaDeviceKernels(DEVICE_TPU_NODE, DEVICE_TPU_XLA_JIT);\r\n  (void)registrations;\r\n\r\nSo we should have a global reference to the objects but it is possible the variable gets optimized away.\r\nDo you have exact instructions (like build command) so i can reproduce the error.\r\n\r\nThanks", "The options are : \r\n/usr/bin/c++   \r\n-I/usr/local/include\r\n-I/home/developer/workspace/tensorflowleaktest/build/../projects/include \r\n-I/home/developer/workspace/tensorflowleaktest/build/../tests/include \r\n-I/home/developer/tensorflow -I/home/developer/tensorflowincludes/bin \r\n-I/home/developer/tensorflowincludes/genfiles \r\n-I/usr/local/lib/python3.5/dist-packages/tensorflow/include \r\n-I/usr/local/lib/python2.7/dist-packages/tensorflow/include \r\n-I/home/developer/tensorflowincludes/google \r\n-I/tensorflow \r\n-I/tensorflowincludes/bin \r\n-I/tensorflowincludes/genfiles \r\n-I/home/developer/workspace/tensorflowleaktest/build/../libs/abseil-cpp  \r\n-Wall \r\n-fprofile-arcs \r\n-ftest-coverage \r\n-g \r\n-std=c++14 \r\n-O0 \r\n-Wall \r\n-fprofile-arcs\r\n-ftest-coverage \r\n-g \r\n-std=c++14 \r\n-O0 \r\n-fsanitize=address \r\n-fno-omit-frame-pointer  \r\n-pthread \r\n-std=gnu++14 \r\n-o CMakeFiles/tensorflowleaktest.dir/home/developer/workspace/tensorflowleaktest/tests/src/main.cpp.o \r\n-c /home/developer/workspace/tensorflowleaktest/tests/src/main.cpp\r\n\r\nThe full output of the build command with the make can be found here attached. \r\n\r\n[build_text.txt](https://github.com/tensorflow/tensorflow/files/2857210/build_text.txt)\r\n\r\nAdditionally a pre-compiled .ii file is attached below in a zip file. (A command -save_temps was passed in to create this file. )\r\n[precompiled.zip](https://github.com/tensorflow/tensorflow/files/2857230/precompiled.zip)\r\n\r\n\r\n\r\n", "This problem seems to still exist on Tensorflow 1.15", "Any updates on this?", "Tensorflow 1.x is not supported anymore.", "@bhack I guess the best way of dealing with bugs is to wait for the EOL of the product.", "The point is that TF has not a public EOL policy. \nAlso the new bug fixes are available generally in nightly or in the next release as we don't have patch releases other then for security bugs.", "Is this still an issue? (In latest TF/patches)"]}, {"number": 23545, "title": "Unacceptable framework overhead for huge networks", "body": "I posted the same issue for Keras [here](https://github.com/keras-team/keras/issues/11592), but I guess it's more of a tensorflow problem in general.\r\nso for a large network with thousands or even more layers (the network might not be that large mathematically since each layer could contain just a small number of parameters, but u get the idea), the overhead of basically anything messing with some graph stuff becomes unacceptable, things like model compilation, XLA optimization, gradient checkpoint, and also the keras \"save_weights\" functions are of ABYSMAL performance and could take literally DAYS!! even longer than the actual training process on GPU, I noticed that every time something like the aforementioned stuff was being executed, the python script just got stuck, one CPU core skyrocketed to 100% usage and the rest 39 cores were just almost 0% usage, then it would take hours and hours before that thing was done and then we could finally move on to the actual training.\r\nIt's honestly just frustrating and unacceptable, any idea to solve this?\r\nsample script to reproduce the performance problem: https://gist.github.com/IFeelBloated/6e85b251f66941b7eb50f987c95ce69b", "comments": []}, {"number": 23472, "title": "Fail to configure on Windows(CMake3.11.0-rc2, VS2015, python3.5.3, swigwin-3.0.12)", "body": "CMake Warning at CMakeLists.txt:9 (message):\r\n  Your current cmake generator is set to use 32 bit toolset architecture.\r\n  This may cause \"compiler out of heap space\" errors when building.  Consider\r\n  using the flag -Thost=x64 when running cmake.\r\n\r\n\r\nSelecting Windows SDK version  to target Windows 10.0.17134.\r\nThe C compiler identification is MSVC 19.0.23026.0\r\nThe CXX compiler identification is MSVC 19.0.23026.0\r\nCheck for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\r\nCheck for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\r\nDetecting C compiler ABI info\r\nDetecting C compiler ABI info - done\r\nCheck for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\r\nCheck for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\r\nDetecting CXX compiler ABI info\r\nDetecting CXX compiler ABI info - done\r\nDetecting CXX compile features\r\nDetecting CXX compile features - done\r\nPerforming Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\nPerforming Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\nPerforming Test GCC_OPENMP_SUPPORT\r\nPerforming Test GCC_OPENMP_SUPPORT - Failed\r\nPerforming Test MSVC_OPENMP_SUPPORT\r\nPerforming Test MSVC_OPENMP_SUPPORT - Success\r\nFound PythonInterp: D:/Anaconda3/envs/tensorflow/python.exe (found version \"3.5.4\") \r\nFound PythonLibs: D:/Anaconda3/envs/tensorflow/libs/python35.lib (found version \"3.5.4\") \r\nFound SWIG: D:/zbl_back/code/swigwin-3.0.12/swig.exe (found version \"3.0.12\") \r\nCMake Error at tf_python.cmake:813 (string):\r\n  string sub-command REPLACE requires at least four arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:541 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:814 (string):\r\n  string sub-command REPLACE requires at least four arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:541 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:815 (string):\r\n  string sub-command REPLACE requires at least four arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:541 (include)\r\n\r\nif I use tensorflow_BUILD_PYTHON_BINDINGS=OFF, this error will not happen.", "comments": ["@skye  PTAL", "We now encourage Windows users to build via bazel instead of CMake. See https://www.tensorflow.org/install/source_windows.\r\n\r\nI'll mark this contributions welcome for now in case anyone else has a solution though.", "Facing same issue with cmake... any update on this?\r\nThanks & Regards", "they advised me to use bazel instead to cmake", "I also face the same issue"]}, {"number": 23447, "title": "CONV2 occurrence mismatch in timeline", "body": "**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed binary\r\n- TensorFlow 1.10 and 1.12 both\r\n- Python 2.7:\r\n- CUDA 9.0\r\n- cuDNN 7.3\r\n- GPU model and memory: NVIDIA V100 (16GB) x 8\r\n\r\n**Describe the current behavior**\r\ndump timeline of benchmark with following command:\r\npython tf_cnn_benchmarks.py --local_parameter_device=gpu --model=resnet50 --data_dir=/home/nvme/dataset/imagenet/tf_records/train/ --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --batch_size=128 --num_gpus=1 --trace_file=/home/timeline/benchmark_resnet50_imagenet_gpu1.json\r\n\r\nthen the number of CONV2D op send to GPU is 53\r\n![image](https://user-images.githubusercontent.com/15809856/47891157-fc087300-de8c-11e8-81d6-40446b20ec3d.png)\r\n\r\nbut GPU run so many CONV2D:\r\n![image](https://user-images.githubusercontent.com/15809856/47891199-31ad5c00-de8d-11e8-8c89-3823de6d5ecc.png)\r\n\r\nI think they should be 53 both.\r\n", "comments": []}, {"number": 23323, "title": "i encounter very strange and serious Bug with dataset feeding \uff08Emergency\uff09", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information** MAC OS 10.14 And Ubuntu 18 either\r\n- TensorFlow version (you are using): 1.11.0\r\n\r\nMy model is LSTM + DNN, dataset structure is:\r\n\r\n[\r\n    [ [1,2,3],4,5,6 ],\r\n    [ [11,22,33],44,55,66 ],\r\n    [ [4,5,6],7,8,9 ],\r\n]\r\n\r\nconvert to Dict is\r\n\r\n{\r\n    a: [ [1,2,3], [11,22,33], [4,5,6] ]\r\n    b: [4,44,7]\r\n    c: [5,55,8]\r\n    d: [6,66,9]\r\n}\r\n\r\nit's work well but when LSTM exceed 31 step, i will reshape feature \"a\" to (time_step, columns) for LSTM input, when time_step more than 31, like 32, 33..., software will crash, and show bug info below:\r\n\r\n`Retval[0] does not have value`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1295, in _do_call\r\n    return fn(*args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1375, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"estimator.py\", line 340, in <module>\r\n    tf.app.run(main)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"estimator.py\", line 275, in main\r\n    steps=args.train_steps\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1215, in _train_model_default\r\n    saving_listeners)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1409, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1148, in run\r\n    run_metadata=run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1239, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1224, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1296, in run\r\n    run_metadata=run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1314, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n``` \r\n\r\ni'm first think reason is data too long\uff0cso i change and reduce columns (32, 50) to (32, 30), (32, 10), (32, 1), it's can't work, but i change and reduce shape to (31, 50), (31, 30), (31, 60), (32, 1), (16, 60), (2, 16) it's work well, just say, when LSTM time step exceed 31, Tensorflow can't work,\r\n\r\n**Who will benefit with this fix?**\r\n\r\nevery one\r\n\r\n**Any Other info.**\r\n\r\ni use tech beblow:\r\n\r\n1. tensorflow estimator + dataset,\r\n2. RNN ( LSTM + MUILTI + Dynamic),\r\n3. DNN( fully connected layer )\r\n\r\nit's strange Bug and important Bug, even cause i can't running completely now\uff0chope official can fix Bug as soon as possible\uff0cvery thanks\r\n\r\n\r\n\r\n", "comments": ["Can you please out together a minimal example to reproduce this bug? It will help us to save time from the troubleshooting and we can focus more on the actual issue. Thanks!", "The same bug (RNN failed for length > 31) has been reported already with a minimal reproducible example, for almost a year, without a fix. The oldest issue is at this link: https://github.com/tensorflow/tensorflow/issues/15874.", "Please see my comment on #15874."]}, {"number": 23294, "title": "Dataset shard index automatic change in Estimator DistributionStrategy", "body": "**System information**\r\n- TensorFlow version (you are using): 1.10\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAccording to the cluster role defined in `Estimator`, the task index of `chief` and `worker` both start from 0. This brings trouble usage for reading data shard under distributed environment. Users should either change code to distinguish the task index between `chief` and `worker_0` or modify the `TF_CONFIG` environment to read different data shards.\r\n\r\n```\r\nd = tf.data.TFRecordDataset(FLAGS.input_file)\r\nd = d.shard(FLAGS.num_workers, FLAGS.worker_index)\r\n```\r\n\r\nThe propose of `DistributionStrategy` is to make distribution easy writing with minimal changes to model function and input function. However, we currently cannot avoid the problems described above. \r\n\r\nIs there any solutions for this problem? I am very willing to contribute for this but want to have more discussion for the design. \r\n\r\n**Will this change the current api? How?**\r\nMaybe yes.\r\n\r\n**Who will benefit with this feature?**\r\nDistributionStrategy users.\r\n\r\n**Any Other info.**\r\nAny  discussion is appreciated.", "comments": ["Hi @yuefengz @anj-s ,\r\n  This is an issue related to `DistributionStrategy`. Please take a look.\r\nThanks. ", "We will add an `InputContext` object which can be accessible in `input_fn`. This object should give an unique id and/or the number of workers. Would that work for you cases?", "@yuefengz To add an `InputContext` seems reasonable. And I am very interested in the details for usage of this interface. Could you give an example for pseudo code? Is the `input_fn` introduced by `Estimator` or `DistributionStrategy`?", "@wangsiyu Search for `InputContext`here: https://github.com/tensorflow/community/pull/25/files. We will make that changes soon.", "@yuefengz I did some work on compatibility with `input_context` and `Estimator`. Please give some comments. Thanks  https://github.com/tensorflow/estimator/pull/15"]}, {"number": 23238, "title": "intel mkl optimized tensorflow performance degradation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Deep Learning VM\r\nVersion: m10\r\nBased on: Debian GNU/Linux 9.5 (stretch) (GNU/Linux 4.9.0-8-amd64 x86_64\\n)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): deep-learning image  \r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: N/A \r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nRunning deep model and some wide linear models. Inference performance is very bad. 2-4x slower relative to running inference without MKL\r\n**Describe the expected behavior**\r\nPerformance should actually improve with intel mkl instruction set .\r\n\r\n**Code to reproduce the issue**\r\ncode for deep and wide linear model. or logistic regression example code from tensorflow example\r\n\r\n**Other info / logs**\r\nWhen running using google deep learning image version M9 on gpu machine  (image : tf-latest-cu92, version M9) . Note : the inference is only running on cpu as i turn off the visibility for cuda devices, So the tensorflow code runs only runs on cpu. The image family says they are intel optimized packages but when i rung the benchmarks with verbosity on , i do not observe any mkl related stuff.\r\n\r\nI start another deep learning image (tf-latest-cpu , version M10): Running exact same code on this machine with environment variable (export MKL_VERBOSE=1): I can observe a lot of openMP thread settings , KMP_xxx settings and mkl instructions logged with some timing information. I didn't observe any such thing in the M9 gpu image , even though in both place when i execute command   i observe following logs: \r\nM9 gpu image \r\nNumpy + Intel(R) MKL: THREADING LAYER: (null)\r\nNumpy + Intel(R) MKL: setting Intel(R) MKL to use INTEL OpenMP runtime\r\nNumpy + Intel(R) MKL: preloading libiomp5.so runtime\r\nMKL_VERBOSE Intel(R) MKL 2019.0 Product build 20180829 for Intel(R) 64 architecture Intel(R) Advanced Vector Extensions 2 (Intel(R) AVX2) enabled processors, Lnx 2.20GHz lp64 intel_thread\r\nMKL_VERBOSE SDOT(2,0x55fd25117d40,1,0x55fd25117d40,1) 1.61ms CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:16\r\n1.11.0\r\n\r\nM10 cpu image : \r\nNumpy + Intel(R) MKL: THREADING LAYER: (null)\r\nNumpy + Intel(R) MKL: setting Intel(R) MKL to use INTEL OpenMP runtime\r\nNumpy + Intel(R) MKL: preloading libiomp5.so runtime\r\n\r\nUser settings:\r\n\r\n   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n   KMP_BLOCKTIME=0\r\n   KMP_SETTINGS=1\r\n   OMP_NUM_THREADS=32\r\n\r\nEffective settings:\r\n\r\n   KMP_ABORT_DELAY=0\r\n   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\r\n   KMP_ALIGN_ALLOC=64\r\n   KMP_ALL_THREADPRIVATE=128\r\n   KMP_ATOMIC_MODE=2\r\n   KMP_BLOCKTIME=0\r\n   KMP_CPUINFO_FILE: value is not defined\r\n   KMP_DETERMINISTIC_REDUCTION=false\r\n   KMP_DEVICE_THREAD_LIMIT=2147483647\r\n   KMP_DISP_HAND_THREAD=false\r\n   KMP_DISP_NUM_BUFFERS=7\r\n   KMP_DUPLICATE_LIB_OK=false\r\n   KMP_FORCE_REDUCTION: value is not defined\r\n   KMP_FOREIGN_THREADS_THREADPRIVATE=true\r\n   KMP_FORKJOIN_BARRIER='2,2'\r\n   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_FORKJOIN_FRAMES=true\r\n   KMP_FORKJOIN_FRAMES_MODE=3\r\n   KMP_GTID_MODE=3\r\n   KMP_HANDLE_SIGNALS=false\r\n   KMP_HOT_TEAMS_MAX_LEVEL=1\r\n   KMP_HOT_TEAMS_MODE=0\r\n   KMP_INIT_AT_FORK=true\r\n   KMP_INIT_WAIT=2048\r\n   KMP_ITT_PREPARE_DELAY=0\r\n   KMP_LIBRARY=throughput\r\n   KMP_LOCK_KIND=queuing\r\n   KMP_MALLOC_POOL_INCR=1M\r\n   KMP_NEXT_WAIT=1024\r\n   KMP_NUM_LOCKS_IN_BLOCK=1\r\n   KMP_PLAIN_BARRIER='2,2'\r\n   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_REDUCTION_BARRIER='1,1'\r\n   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_SCHEDULE='static,balanced;guided,iterative'\r\n   KMP_SETTINGS=true\r\n   KMP_SPIN_BACKOFF_PARAMS='4096,100'\r\n   KMP_STACKOFFSET=64\r\n   KMP_STACKPAD=0\r\n   KMP_STACKSIZE=4M\r\n   KMP_STORAGE_MAP=false\r\n   KMP_TASKING=2\r\n   KMP_TASKLOOP_MIN_TASKS=0\r\n   KMP_TASK_STEALING_CONSTRAINT=1\r\n   KMP_TEAMS_THREAD_LIMIT=32\r\n   KMP_TOPOLOGY_METHOD=all\r\n   KMP_USER_LEVEL_MWAIT=false\r\n   KMP_VERSION=false\r\n   KMP_WARNINGS=true\r\n   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\r\n   OMP_ALLOCATOR=omp_default_mem_alloc\r\n   OMP_CANCELLATION=false\r\n   OMP_DEBUG=disabled\r\n   OMP_DEFAULT_DEVICE=0\r\n   OMP_DISPLAY_AFFINITY=false\r\n   OMP_DISPLAY_ENV=false\r\n   OMP_DYNAMIC=false\r\n   OMP_MAX_ACTIVE_LEVELS=2147483647\r\n   OMP_MAX_TASK_PRIORITY=0\r\n   OMP_NESTED=false\r\n   OMP_NUM_THREADS='32'\r\n   OMP_PLACES: value is not defined\r\n   OMP_PROC_BIND='intel'\r\n   OMP_SCHEDULE='static'\r\n   OMP_STACKSIZE=4M\r\n   OMP_TARGET_OFFLOAD=DEFAULT\r\n   OMP_THREAD_LIMIT=2147483647\r\n   OMP_TOOL=enabled\r\n   OMP_TOOL_LIBRARIES: value is not defined\r\n   OMP_WAIT_POLICY=PASSIVE\r\n   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\r\n\r\nOMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-31\r\nOMP: Info #156: KMP_AFFINITY: 32 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #179: KMP_AFFINITY: 1 packages x 16 cores/pkg x 2 threads/core (16 total cores)\r\nOMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 0 core 0 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 0 core 1 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 0 core 2 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 0 core 3 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 0 core 4 thread 1\r\n\r\nOMP: Info #250: KMP_AFFINITY: pid 8331 tid 8331 thread 0 bound to OS proc set 0\r\nMKL_VERBOSE Intel(R) MKL 2019.0 Product build 20180829 for Intel(R) 64 architecture Intel(R) Advanced Vector Extensions 2 (Intel(R) AVX2) enabled processors, Lnx 2.20GHz lp64 intel_thread\r\nMKL_VERBOSE SDOT(2,0x5622b7736500,1,0x5622b7736500,1) 2.54ms CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:16\r\n1.11.0\r\n\r\nSo i assume intel mkl is being used in M10 image where as mkl is not being used in the M9 image (Note: i have turned off visibility for cuda devices so only cpu inference is being compared) . I observe 2-4x performance degradation with intel mkl.\r\nThe mkl suggested flags are appropriate: \r\nKMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n   KMP_BLOCKTIME=0\r\n   KMP_SETTINGS=1\r\n   OMP_NUM_THREADS=32\r\n\r\nAny ideas on how to debug the root cause and get the maximum performance for my models.\r\n", "comments": ["It is \"deep and wide linear model\", can you do \"export OMP_NUM_THREADS=1\" as a first step? \r\nAnd can you please try inter_op_parallelism_threads and intra_op_parallism_threads similar to https://github.com/NervanaSystems/tensorflow-models/commit/55d55abc71483723743c0273b9c1fd8e0c7d8391#diff-00c5d001cb14a21f6d7dbf16d4e55032R90 if you haven't?  ", "@wei-v-wang : the link oyu mentioned doesnt work for me. Can you please share the link again, or may be let me know what config for inter and intra op parallelism i should try , i will post back the results here.\r\n\r\nAlso not just wide and linear models , also i am observing similar 2-3x worse latency (inference) for deep cross network model as well . Could you please perhaps explain the reasoning behing omp_num_threads = 1 as well , this will help us to understand better the internal workings.  ", "Sorry, here is the updated link: https://github.com/tensorflow/models/blob/master/official/wide_deep/wide_deep_run_loop.py#L87-L88 \r\n\r\nIf some application is not bound by compute, changing OMP_NUM_THREADS might help. \r\n\r\nI think for wide/deep models, inter_op/intra_op has been providing some help. Please definitely enable it in your model and give it a try. ", "@wei-v-wang : the link you provided change inter and intra op thread settings : but when i run the code, it still prints out : \r\n  KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n   KMP_BLOCKTIME=0\r\n   KMP_SETTINGS=1\r\n   OMP_NUM_THREADS=32\r\n\r\n, so i am not sure if it is taking affect . Are those 2 different settings ?", "In order to change OMP_NUM_THREADS, please use \"export OMP_NUM_THREADS=\". The link I provided will only change inter and intra op settings only. ", "Ok so i tried bunch of parameters : Machine type : 32 core , logical threads per core 2 \r\n i tried : num of intra op threads = OMP threads : [4, 8, 16, 32, 64]\r\ninter op threads = number of physical cores  and number of sockets [2,8,16,32]\r\n\r\nthe best performance i could get for a batch size of 1k : 48 micro secs\r\nthe best i get without mkl without much tuning (num of inter and intra op threads being the same : 16/32/64] : 23 micro secs\r\n\r\nAny other setting i need to try ?\r\nCan we know if MKL library and ISA is even being taken advantage of by looking at some ops which should definitely perform better ?\r\n\r\n ", "I definitely found setting number of OMP threads to a lower count helped and same for inter op parallelism. \r\nBut the performance for the current model is still 2-3x worse in general ", "Since it is inference, I have one last suggestion: \r\ncould you please prefix your runs with \"numactl -c 1 -m 1 python ...\" , the rest of the configurations can remain the same. This is to use just one socket to rule out memory access overhead across two sockets.\r\n\r\nIf you still observe ~2X slowness with TF w/MKLDNN, can you please share your model script with us?  ", "Sorry, I should have given out all BKMs in a batch. But, here is another important one that I've missed. \r\n\r\nexport OMP_NUM_THREADS=x \r\nexport KMP_BLOCKTIME=1 \r\nnumactl -c 1 -m 1 python ... <inter_op> <intra_op> \r\n ", "numactl -c 1 -m 1 python ... \r\nlibnuma: Warning: node argument 1 is out of range \r\n<1> is invalid ", "here is machine config : \r\nlscpu\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                32\r\nOn-line CPU(s) list:   0-31\r\nThread(s) per core:    2\r\nCore(s) per socket:    16\r\nSocket(s):             1\r\nNUMA node(s):          1\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 79\r\nModel name:            Intel(R) Xeon(R) CPU @ 2.20GHz\r\nStepping:              0\r\nCPU MHz:               2200.000\r\nBogoMIPS:              4400.00\r\nHypervisor vendor:     KVM\r\nVirtualization type:   full\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              56320K\r\nNUMA node0 CPU(s):     0-31", "i tried numactl -c 0 -m 0 python , still the best i get is around 48 micro secs with omp threads and inter op threads = 6 and the blocktime =1 ", "@patelprateek I see,  it is single socket system so numactl does not help here. Is it possible for us to get your customized model?  ", "@wei-v-wang : i will try to get you that if that really helps debugging. But i would need to get some privacy and legal approval. \r\nAre there any steps you want me to do help debug this. Basically i want to understand what ops are being used in my model (both with mkl and without mkl) , see if that helps us understand why mkl optimization degrades the performance.\r\n\r\nAs for model : i have wide and deep linear models using tf.estimator and dataset api.", "Ok, I see.  To simplify things, as you said, Wide and Deep (wide only) is a good proxy for your model. I will double check the performance comparison just using this wide and deep linear model. Hopefully learnings can be applied to your custom model. \r\nBTW, Are you using private data set or public dataset? The performance may vary depend on the dataset size you are using. ", "data set is private. i can get more details about type of features and number of crosses if that helps but this is all for inference and not for training", "@wei-v-wang : I am trying to re-write the model graph to anonymize the features. this works quite well except for few sparse features for which i have an embedding as well. Do you happen to know a tool/library that can help do this and take care of the edge acse i am missing ? \r\nMt graph rewrite code is pretty trivial by iterating over all nodes and seaching for some feature names and replacing them with some ids. For some reason i cant get the scores of model to match when i apply this translation for sparse features using embedding layer. Any caveats you know of ?", "@wei-v-wang : any updates on how can this issue be resolved ? did you guys find perf regression in the the new DL image ?", "@patelprateek Sorry for the delay, can you please try this PR https://github.com/tensorflow/tensorflow/pull/24272 ? \r\n\r\nBefore it is merged, you can use: https://github.com/Intel-tensorflow/tensorflow/tree/sriniva2/small_alloc_fix ", "still monitoring. Waiting for PR: https://github.com/tensorflow/tensorflow/pull/24777\r\n", "I think I have similar issue.", "Same here\r\n4x slowdown using MKL\r\nSo far tried the anaconda version and google container (both latest releases)\r\nHW is Xeon 6132, 2 sockets, HT on", "@dare0021 Apologize for the issue and we are starting to address this. I will provide more frequent update here. ", "Hey, Facing the same issue. inference is 3-4x slower. Is there any update or any solution?\r\n", "@patelprateek \r\n@wangcj05 \r\n@dare0021 \r\n@aashay96\r\n\r\nThis topic is opened for a very long time.\r\nIntel Optimization for Tensorflow has be improved more since this issue is created.\r\n\r\nTo release the performance potential of Tensowflow with MKL, user need to set for optimization.\r\n\r\nExample for Intel Core CPU (4 cores/socket, 1 sockets)\r\n\r\n```\r\nexport TF_ENABLE_MKL_NATIVE_FORMAT=1  \r\nexport TF_NUM_INTEROP_THREADS=1\r\nexport TF_NUM_INTRAOP_THREADS=4\r\nexport OMP_NUM_THREADS=4\r\nexport KMP_BLOCKTIME=1\r\nexport KMP_AFFINITY=granularity=fine,compact,1,0\r\n```\r\nTF_ENABLE_MKL_NATIVE_FORMAT is key optimization, friendly to Keras model inference.\r\nTF_NUM_INTEROP_THREADS could be set to 1, sockets number or other number no more than cores number.\r\nTF_NUM_INTRAOP_THREADS & OMP_NUM_THREADS are set as cores number per socket, or other value no more it.\r\nKMP_BLOCKTIME are set 0, 1 or other number. \r\n\r\nPS: the recommended value would not be right value to your model. The best way is test the performance to find the right values.\r\n\r\nNow, use could install the Tensorflow with MKL by pip and conda:\r\n```\r\npython -m pip install intel-tensorflow\r\nconda install tensorflow-mkl\r\n```\r\nOr build it from source code with '--config=mkl'.\r\n\r\nPlease refer to the [Intel\u00ae Optimization for TensorFlow* Installation Guide](https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html) ", "@patelprateek \r\nHow do you think the suggestion?\r\nIf there is still issue, could you share it?\r\n\r\nThank you!", "@patelprateek Could you please refer to this [comment ](https://github.com/tensorflow/tensorflow/issues/23238#issuecomment-737618693) and try with the latest TF versions(2.4 or later ) as older TF versions(1.x) are not actively supported.Please let us know if it helps?Thanks!"]}, {"number": 23207, "title": "Add an option to apply SavedModel via command line ", "body": "**System information**\r\n- TensorFlow version (you are using): TF-1.11\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBy default TensorFlow has an option to store checkpoint by adding '--train_dir'. However this is good when we want to continue training, but if we want to perform inference using TensorFlowServing it becomes a challenge.  Is it possible to add some flag which will store the variables, graph, in/output signature by just providing an option similar to '--train_dir'?\r\n\r\n\r\n**Will this change the current api? How?**\r\nYes, but nothing will break \r\n\r\nfor e.g. **Old API** will store only checkpoints\r\ntf_cnn_benchmarks.py --model resnet50 --num_epochs=2 --batch_size 64 --data_name=imagenet  --data_dir=/mnt/data **--train_dir=/mnt/checkpoint**  --variable_update horovod --horovod_device gpu --weight_decay=1e-4 --use_fp16\r\n\r\n**New API** should store using SaveModel\r\ntf_cnn_benchmarks.py --model resnet50 --num_epochs=2 --batch_size 64 --data_name=imagenet  --data_dir=/mnt/data **--train_dir_save_model=/mnt/checkpoint**  --variable_update horovod --horovod_device gpu --weight_decay=1e-4 --use_fp16\r\n\r\n\r\n**Who will benefit with this feature?**\r\nBy having such option, deploying the model to production will be super fast, even for the people who doesn't know anything about training code. Currently we have to either change the training code to use SaveModel or figure out a way to export graph along with variables. \r\n\r\n**Any Other info.**\r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: NO\r\nOS Platform and Distribution: Ubuntu 16.04 \r\nTensorFlow installed from: 1.11\r\nBazel version: 0.15\r\nCUDA/cuDNN version: 9.0, 7.3\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "I do understand that this will be hard to implement but at least having such option in existing example specially the benchmark example will be really useful.\r\n\r\nImagine a scenario where an user not a TF expert (for e.g. me), I can just run your resnet50 from tf_cnn_benchmark code on imagenet dataset. Use the output of training for inference on any image detection system that I want to build. I can't do that today that easily because training is producing the output in the form of checkpoint which can't be consumed by TF serving. ", "Given that this is a feature request specific to tf_cnn_benchmarks, reassigning to @reedwm ", "tf_cnn_benchmarks is not designed for production use, which is why it doesn't supported SavedModel. That said, we will accept PRs to tf_cnn_benchmarks implementing SavedModel support if they are relatively simple, easy to maintain, and have tests.\r\n\r\n@karmel, is the official resnet50 model designed for production? Perhaps it should support SavedModel", "This issue should be under the [TensorFlow benchmarks](https://github.com/tensorflow/benchmarks) project, not the main TensorFlow core project.", "If that is so, then maybe someone with the appropriate permissions should transfer this issue to the TensorFlow benchmarks project. [This](https://help.github.com/articles/transferring-an-issue-to-another-repository/) page shows how it's done."]}, {"number": 23192, "title": "Prevent Empty Checkpointable Data Structure Restores", "body": "@allenlavoie\r\n\r\nIssue tracking the \"bug\" that is caused when restoring a checkpoint where the Checkpointable object has new checkpointable data structures that do not contain any checkpointable objects themselves and therefore prevent restoration when using `load_status.assert_consumed()`.\r\n\r\n*Example Code:*\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training.checkpointable.tracking import Checkpointable\r\n\r\n\r\nclass Model(Checkpointable):\r\n\r\n  def __init__(self):\r\n    self.variable = tf.get_variable(\"variable\", [2, 2])\r\n    self.dict = {\r\n        \"test\": 1,\r\n        \"test2\": 2,\r\n    }\r\n    self.dict_var = {\r\n        \"test\": 1,\r\n        \"test2\": tf.get_variable(\"dict_var\", [2, 2])\r\n    }\r\n    self.dict_nested_var = {\r\n        \"test\": 1,\r\n        \"test2\": {\r\n            \"var\": tf.get_variable(\"dict_nested_var\", [2, 2])\r\n        }\r\n    }\r\n    self.list_var = [tf.get_variable(\"list_var\", [2, 2])]\r\n\r\n\r\nprint('SAVE')\r\nwith tf.Graph().as_default():\r\n\r\n  s = tf.Session()\r\n\r\n  m = Model()\r\n\r\n  s.run(tf.global_variables_initializer())\r\n\r\n  c = tf.train.Checkpoint(model=m)\r\n\r\n  c.save('checkpoints/', session=s)\r\n\r\nprint('RESTORE')\r\nwith tf.Graph().as_default():\r\n\r\n  s = tf.Session()\r\n\r\n  m = Model()\r\n\r\n  m.dict_new = {\r\n      \"test\": 1,\r\n      \"test2\": 2\r\n  }\r\n\r\n  c = tf.train.Checkpoint(model=m)\r\n\r\n  status = c.restore('checkpoints/-1')\r\n\r\n  status.assert_consumed().run_restore_ops(s)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@sleighsoft If you write new objects in c.restore() which were not originally in the checkpointable data structure, it would not work. This is the intent of save/restore checkpoint functionality. \r\n\r\nIf you plan to change the checkpoints, best go back to c.save() to make the change then call c.restore(). It is probably much more manageable and less confusion this way.", "I think the point is that auto-generated list/dict wrappers which don't have any variables or checkpointable objects associated with them shouldn't count as a checkpoint not matching. Otherwise `self._unused_list = []` would break an assertion, which is technically correct-ish but not helpful. I could have conditionals in my __init__ which only sometimes assign such an attribute, in which case small configuration changes would accidentally break assertions.\r\n\r\nEither way the checkpoint restore works fine, since no variables were added or tracked in a different way. It's just a question of whether the optional assertions pass or not.", "What @allenlavoie says :)"]}, {"number": 23156, "title": "axis argument for FFT ops (tf.signal.fft, tf.signal.fft2d, etc.)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.spectral.fft3d` as of now is restricted to apply FFT to the last 3 dimensions of the input, while the majority of the operations, e.g. `tf.nn.conv3d`, handle data in NDHWC format by default. In order to circumvent this limitation, users have to permute (transpose) the data just for the FFT operation, then permute them back for other operations that follow thereafter. This not only makes the code less readable, but more importantly incurs extra overhead.\r\n\r\n**Will this change the current api? How?**\r\nThis will add another argument `data_format` to `tf.spectral.fft3d` which lets users decide which dimensions should be used for FFT. `data_format` should be either `\"NDHWC\"` or `\"NCDHW\"`, and defaults to `\"NCDHW\"` for backward compatibility. If `\"NDHWC\"` is chosen as the `data_format`, the second, third and fourth dimensions will be used instead. \r\n\r\n**Who will benefit with this feature?**\r\nAny users using `fft3d` are expected to benefit from this, especially when they use FFT with other convolutional layers that accept inputs in NDHWC format.\r\n\r\n**Any Other info.**\r\nOther operations such as `tf.spectral.fft2d` may also be modified for consistency.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code N/A\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\nMobile device N/A", "@hsgkim Agreed that this is not efficient let alone error prone. \r\ntf.spectral.fft3d should cover both \"NDHWC\" and \"NCDHW\". Feel free to contribute a PR for this.", "`tf.signal.fft` (and its variants) are designed for `numpy.fft` and `scipy.signal` compatibility. The `data_format` layouts for TensorFlow's convolution operators only make sense in the context of a convolution, since there is no channel dimension in an FFT, so I do not think we should do this. Instead, I would prefer that `tf.signal.fft` had an axis or axes argument, as `numpy.fft.fft` and `numpy.fft.fft2` do.\r\n\r\nChanged the issue description. I don't think I'll have time to work on this soon, so I'm adding the contributions welcome tag.", "+1 for this. I would like `tf.signal.fft2d` to have the `axes` parameters as [`numpy.fft.fft2`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft2.html) for example (and all analogous examples). This will also enable the creation of `tf.signal.fftnd`", "would love this feature too! An axis argument would be helpful when using stereo audio, where the channel is conventionally the inner most dimension, and the axis to compute the FFT on, i.e. time/sample dimension, is the second inner most dimension.", "+1, need this."]}, {"number": 23147, "title": "Trying to connect from process to Tensorflow Server, cannot use frozen graph", "body": "Hi everyone, I have an issue which can possibly be a bug, for which I am trying to find solution for quite some time:\r\n\r\nStack Overflow:\r\nhttps://stackoverflow.com/questions/52700621/tensorflow-server-i-dont-want-to-initialize-global-variables-for-every-session\r\n\r\nDiscuss:\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/rARPP1_ilNA\r\n\r\nMy story in short is that I create processes dynamically in Python and do model predictions using a session for Tensorflow Server `tf.train.Server`. Graph is saved in global scope, as I hope that default python 'fork' mode will not create any overhead as long as data in RAM is not changed (\"copy-on-change\" policy). Anyway, it's too long and expensive for me to reload model for each new process. Perhaps there might be a more efficient way to define model on server side, but I don't know it.\r\n\r\nThe problem is that somewhy I need to run initialization of global variables (`sess.run(tf.global_variables_initializer())`) each time I create new process and open new session. I tried to do dummy sun of the model before locking graph so that all the variables must be initialized, but no success - I still get the uninitialized variable error. Can anyone help me with that?\r\n\r\nI have a small reproducible example of the problem compatible with Tensorflow 1.11:\r\n\r\nhttps://github.com/hcl14/Tensorflow-server-launched-from-child-process\r\n\r\nThe places in the code are marked with `\"PROBLEM\"` comment.\r\n\r\nAlso, maybe I am doing everything wrong and there is a better way to quickly do model predictions from dynamically created processes?\r\n", "comments": ["When I tried to call model from persistent process (i.e. not spawned dynamically for just one call, but created once on program start), I've got this error:\r\n\r\n`Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"init\" op: \"NoOp\"  is not an element of this graph.)`\r\n\r\nFollowing [stackoverflow post](https://stackoverflow.com/questions/47918382/explicit-tensorflow-session-gives-fetch-error-in-tensorflow-nmt), I changed in model inference:\r\n\r\n`sess = tf.Session('grpc://'+tf_server_address, graph=graph)`\r\n\r\nto \r\n\r\n`sess = tf.InteractiveSession('grpc://'+tf_server_address, graph=graph)`\r\n\r\nI've got persistent process working and dynamically created process hang.\r\n\r\nHanging process issues warning:\r\n\r\n```\r\nException ignored in: <generator object get_controller at 0x7f9b78ccba68>\r\nTraceback (most recent call last):\r\n  File \"/mnt/clikque-backend/pysuggestion/py36_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3815, in get_controller\r\n    if self.stack[-1] is not default:\r\nIndexError: list index out of range\r\n```\r\n\r\n\r\nWhen I use interactive session not only for evaluation, but for model loading as well\r\n```\r\n    graph = tf.get_default_graph()\r\n    \r\n    # interactive session installs itself as default\r\n    sess = tf.InteractiveSession(graph=graph)\r\n    \r\n    with sess.as_default():\r\n\r\n        NN1_MODEL = models.load_model(NN1_MODEL_FILE)\r\n```\r\n\r\n\r\nFirst run of dynamic process is Ok, but the second hangs with warnings\r\n```\r\nException ignored in: <generator object get_controller at 0x7f8ad02ba338>\r\nTraceback (most recent call last):\r\n  File \"/mnt/clikque-backend/pysuggestion/py36_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3818, in get_controller\r\n    % type(default))\r\nAssertionError: Nesting violated for default stack of <class 'tensorflow.python.client.session.InteractiveSession'> objects\r\n```", "@hcl14 Looks like your updated code snippet can fix the issue.\r\nYou should be able to get frozen graphs for dynamic processes if you set up the clusters properly.", "@wt-huang Thanks for the answer, [I've implemented some of the suggestions](https://github.com/hcl14/Tensorflow-server-launched-from-child-process) I received on Stackoverflow for my question, which I believe became popular after I posted the issue here on Github. Those involve using eager execution and writing custom tensorflow http server. \r\nHowever, it's still unclear for me how to freeze graph for dynamic processes. Does it involve using `tf.train.Server` or just opening new default sessions for each process? Could you please point me to a good example? Instructions I received on Stackoverflow are yet unclear to me.\r\n\r\n"]}, {"number": 23143, "title": "Remove output tensors 1 - 4 from FusedBatchNorm", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.10.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDownloaded frozen inference model MobileNet_v1_1.0_224 from https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\n\r\nFusedBatchNorm has five outputs. 1-4 seem to be related to training. How do I remove them?\r\n\r\nI tried the following code:\r\n\r\n\t\t\tif op.type == 'FusedBatchNorm':\r\n\t\t\t\tnew_node = node_def_pb2.NodeDef()\r\n\t\t\t\tnew_node.name = op.node_def.name\r\n\t\t\t\tnew_node.op = op.node_def.op\r\n\t\t\t\tfor i in op.node_def.input:\r\n\t\t\t\t\tnew_node.input.extend([i])\r\n\t\t\t\tfor a in op.node_def.attr:\r\n\t\t\t\t\tnew_node.attr[a].CopyFrom(op.node_def.attr[a])\r\n\t\t\t\toutput_graph.node.extend([new_node])\r\n\r\n\t\t\telse:\r\n\t\t\t\tnew_node = node_def_pb2.NodeDef()\r\n\t\t\t\tnew_node.CopyFrom(op.node_def)\r\n\t\t\t\toutput_graph.node.extend([new_node])\r\n\r\nIt still has the output tensors 1 - 4.\r\n\r\nCan this feature be added to Graph Transform tool?\r\n\r\n**Will this change the current api? How?**\r\nIt won't. It's a new transformation in Graph Transformation tool.\r\n\r\n**Who will benefit with this feature?**\r\nMobilenet users\r\n\r\n**Any Other info.**\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code? Yes\r\n\t\t\r\nOS Platform and Distribution: Ubuntu 16.04\r\n\r\nTensorFlow installed from: pip\r\n\r\nBazel version: N/A\r\n\r\nCUDA/cuDNN version: N/A \r\n\r\nGPU model and memory: No GPU support\r\n\r\nExact command to reproduce: N/A\r\n\r\nMobile device: N/A", "@santoshchilkunda This is a valid point though Batch Normalization can be used for inference as well. \r\nYou can delete the 4 extra outputs in FusedBatchNorm leaving only output y which is a 4D tensor for output data. Try to use onnx tool to remove those extra outputs.\r\n", "@wt-huang Thanks for your reply.\r\n\r\nWhen you say use onnx tool, are you referring to the following?\r\nhttps://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/handlers/frontend/batch_normalization.py\r\nI see that in version_1, they had 'consumed_inputs' which is not there in version_7. I am not familiar with consumed_inputs."]}, {"number": 23132, "title": "multiplication of IndexedSlices with Dense Tensors", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):v1.3.0-rc2-20-g0787eee 1.3.0\r\n- Are you willing to contribute it (Yes/No):\r\nNo\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI wanted to implement LambdaRank algorithm which requires a point-wise multiplication of \\lambda (dense tensor) with grads obtained by tf.gradients.  In tf, the grads of embeddings extraction will be IndexedSlices. It could be fed into optimizer's apply_grad() directly which is good but not for customized operations. Here I want to multiply dense tensor with IndexedSlices, but in vain. I have to convert IndexedSlices into dense and then multiply, which resulted in OOM  \r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nPeople interested in Learning-to-rank algorithm, and more generally whoever wants to tweak the sparse grads\r\n**Any Other info.**\r\nNone\r\n\r\nHave I written custom code: No, I only call tf API functions\r\nOS Platform and Distribution: Debian 8.11\r\nTensorFlow installed from: anaconda \r\nBazel version: N/A\r\nCUDA/cuDNN version: CUDA 8.0.44, cuDNN 6.0.21\r\nGPU model and memory: Nvidia Titan Xp\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thanks for the reply, I updates the missed fields in the original post."]}, {"number": 23085, "title": "Problem when try to decode some bmp images with tf.image.decode_bmp", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary): PIP\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):No\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version NO\r\n- GPU model and memory: Not using GPU\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nHi,\r\nI have written a python script to read bmp image from local drive using tensorflow and trying to display the image. \r\n\r\nFirst case:\r\nI am getting error when i try to use the \r\nbelow command - tf.image.decode_bmp(image_string,channels=3)\r\n\r\nThe error is as below:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: channels attribute 3 does not match bits per pixel from file 4\r\n\t [[{{node DecodeBmp}} = DecodeBmp[channels=3](ReadFile)]]\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,?,3]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/venkatesh/Desktop/Image_processing_thesis/Thesis_deep_learning/problem_tensorfile.py\", line 38, in <module>\r\n    res = sess.run(patches)\r\n  File \"/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: channels attribute 3 does not match bits per pixel from file 4\r\n\t [[{{node DecodeBmp}} = DecodeBmp[channels=3](ReadFile)]]\r\n\t [[node IteratorGetNext (defined at /home/venkatesh/Desktop/Image_processing_thesis/Thesis_deep_learning/problem_tensorfile.py:35)  = IteratorGetNext[output_shapes=[[?,?,3]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nProcess finished with exit code 1\r\n\r\nSecond case:\r\nIf i use the command tf.image.decode_bmp(image_string) instead of tf.image.decode_bmp(image_string,channels=3) \r\n\r\nI am not getting error but when i see the shape , it's showing the no of channels =4 and not 3.  I want to know why I am getting shape =4 for the second case and why getting error for some images for the first case.\r\n\r\nFor example consider 00001.bmp Image(attached in attachments) size:\r\nHeight=351\r\nWidth=572\r\nchannels=3\r\nis getting shape(351,572,4) for the second case.\r\n\r\nAdditionally\r\nAll the images works fine without tensorflow\r\nFor example: I used scipy to read the image and try to print the shapes of all images in which the no of channels are 3 and not 4.\r\n\r\nThe image which i used is one of the images form waterloo exploratory dataset. In the first 50 images i have faced the above issue for 5 images and tf.image.decode_bmp works for remaining 45 images.\r\n\r\nPlease use the below link if you like to download the waterloo exploratory dataset :\r\n[https://ece.uwaterloo.ca/~k29ma/exploration/](url)\r\n**Describe the expected behavior**\r\nRequires information about why the above error is getting displayed for some of bmp images.\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom scipy import misc\r\nimport glob\r\nimport os\r\n#Command to extract all file names\r\nlist_ground_truth=glob.glob(\"Truth_Images/*.bmp\")  \r\n\r\nnum_parallel_calls=4\r\n\r\ndef parse_fn(filename):\r\n    \"\"\"Decode the jpeg image from the filename and convert to [0, 1].\"\"\"\r\n    image_string = tf.read_file(filename)\r\n    image_decoded = tf.image.decode_bmp(image_string,channels=3)\r\n    #print(image_decoded.get_shape().dims)\r\n    # This will convert to float values in [0, 1]\r\n    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\r\n    return image\r\n\r\n\r\n# Create a dataset\r\ndataset = (tf.data.Dataset.from_tensor_slices(list_ground_truth)\r\n           .map(parse_fn, num_parallel_calls=num_parallel_calls)\r\n           .prefetch(1))\r\n\r\n\r\niterator = dataset.make_one_shot_iterator()\r\npatches = iterator.get_next()\r\nsess = tf.Session()\r\nres = sess.run(patches)\r\nprint(res.shape)\r\nmisc.imshow(res)\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n1. Load the above script. \r\n2. Change the location in the below command to where the image file which i have attached \r\nlist_ground_truth=glob.glob(\"Truth_Images/*.bmp\")  \r\n3.Run the script and if it runs without error.\r\n\r\n[problem_files.zip](https://github.com/tensorflow/tensorflow/files/2493751/problem_files.zip)\r\n\r\n**Other info / logs**\r\nAdditional information:\r\nI am trying the process as part of initial data preparation for training a convolutional network. To make it simple i have removed the unnecessary steps for recreating the issue.\r\n\r\nThanks,\r\nVenkatesh.S\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\nTensorFlow installed from (source or binary): PIP\r\nTensorFlow version (use command below):1.12\r\nPython version:3.6\r\nBazel version (if compiling from source):No\r\nGCC/Compiler version (if compiling from source):N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory: Not using GPU", "@VenkateshSatagopan The output from `tf.image.decode_bmp()` is correct and the number of channels should equal to 4. Basically, channel(=4) corresponds to RGBA whereas channel(=3) corresponds to RGB. Your image is RGBA where A denotes opacity.", "@wt-huang Thanks for the response. \r\nI tried to find the shape(for finding the no of channels) of the image using scipy and opencv before using tensorflow \r\nIn both scipy and opencv when i read image and print it's shape it shows the no of channels as 3 and not 4.\r\nIt will be helpful if you can explain  how you have decided the images to be RGBA image and not RGB image", "@VenkateshSatagopan Actually you are correct, I was looking at different images. Your attached images should be RGB, confirmed using scipy, opencv and PIL. When running the above code snippet with the following line\r\n\r\n`image_decoded = tf.image.decode_bmp(image_string, channels = 3)`\r\n\r\nI didn't encounter the error you mentioned from my end. Could you verify that the other images had no issues with this code snippet?", "@wt-huang I checked with other images among first 50 images of WED dataset and I\r\n am not finding issue when i execute the code\r\n\r\nAs i said earlier I am having issue with images 1,11,18,32,47 among first 50 images.\r\n\r\nI am not facing issue for the above images(1,11,18,32,47) when i try to specify channels=4 in the code."]}, {"number": 23061, "title": "Xla and Ignite support always true from configure.py", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from source\r\n- TensorFlow version: 1.12.0rc-1\r\n- Python version: 3.6\r\n- Installed using pip3\r\n- Bazel version 0.18.0\r\n- GCC/Compiler version 7.3.0\r\n- CUDA/cuDNN version 7.3.1\r\n- GPU model and memory: GTX Titan X (Maxwell w/12GB)\r\n- Have I written custom code: No\r\n- Mobile device: None\r\n- Exact command to reproduce: ./configure  (see below)\r\n\r\n\r\n**Describe the problem**\r\nIt looks as if the configure.py script is always setting ignite and xla support true.  \r\nTo see this, run configure and answer N to the options for ignite and xla...\r\n```\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: n\r\nNo Apache Ignite support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n```\r\nAfterwards .tf_configure.bazelrc has the following lines:\r\n```\r\nbuild:ignite --define with_ignite_support=true\r\nbuild:xla --define with_xla_support=true\r\n```\r\n\r\n\r\n**Code**\r\nIt looks to me like the following lines in configure.py are the issue:\r\nUnder def set_build_var (line 368)\r\n```\r\n  if var == '1':\r\n    write_to_bazelrc('build --define %s=true' % option_name)\r\n  elif bazel_config_name is not None:\r\n    write_to_bazelrc(\r\n        'build:%s --define %s=true' % (bazel_config_name, option_name))\r\n```\r\nIn the above, if var is 0 and there's a bazel_config_name, the option still gets set to true.  That doesn't seem like the behavior we want.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nExact command to reproduce\nMobile device", "Sorry.  Almost all of that info was in there, I thought it was sort of obvious."]}, {"number": 23056, "title": "Losses collection is not thread local so it can't be used inside model_fn call when using MirroredStrategy", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 'v1.12.0-rc0-0-g1a6dea3' 1.12.0-rc0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source): gcc-6 (Ubuntu 6.4.0-17ubuntu1) 6.4.0 20180424\r\n- CUDA/cuDNN version: 10.0/7.3.1.20\r\n- GPU model and memory: GeForce GTX 1080 Ti (11GB)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen calling `tf.losses.add_loss` inside model_fn in Estimator API, it is added to the `tf.GraphKeys.LOSSES` collection. `tf.losses.get_total_loss` is aggregating all the losses from the `tf.GraphKeys.LOSSES` collection.\r\n\r\nUnfortunately, when using `tf.contrib.distribute.MirroredStrategy` as a distribute strategy, collection is updated from all concurrent `model_fn` calls. This leads to tower losses being aggregated to total loss in other towers as well.\r\n\r\nSo if we have 4 GPUs with losses L1, L2, L3, L4, Estimator will report total loss as `a * L1 + b * L2 + c * L3 + d* L4` where a,b,c,d depends on the races encountered.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect total loss to be `L1 + L2 + L3 + L4`.\r\n\r\n**Code to reproduce the issue**\r\n\r\nWhen running this code, second call to `model_fn` causes assertion that losses collection is empty to fail.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    loss = tf.abs(features + tf.get_variable('foo', shape=()) - labels)\r\n\r\n    assert len(tf.get_collection(tf.GraphKeys.LOSSES)) == 0\r\n    tf.losses.add_loss(loss)\r\n\r\n    loss = tf.losses.get_total_loss()\r\n    train_op = tf.train.GradientDescentOptimizer(0.0).minimize(loss)\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n\r\ndef input_fn():\r\n    return tf.data.Dataset.zip((\r\n        tf.data.Dataset.from_tensors(0.).repeat(100),\r\n        tf.data.Dataset.from_tensors(0.).repeat(100)\r\n    ))\r\n\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\nestimator.train(input_fn=input_fn)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nScript output:\r\n\r\n```\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpq6v9w4nh\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpq6v9w4nh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f27bd939240>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f27bd939f60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:2\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:3\r\nINFO:tensorflow:Configured nccl all-reduce.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"<ipython-input-9-57173fdb3adc>\", line 7, in model_fn\r\n    assert len(tf.get_collection(tf.GraphKeys.LOSSES)) == 0\r\nAssertionError\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "@seemuch  Any update ?", "Nothing to update for now. \r\nI did not have the bandwidth to deal with this yet. ", "Any updates on this?", "Unfortunately we are focus on 2.0 and Keras integration currently, so will not have the bandwidth to fix 1.x / Estimator issues. I will open this up for contributions if anyone would like to pitch in."]}]