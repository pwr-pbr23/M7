[{"number": 29662, "title": "Updating NCCL from 2.3.5 to 2.4.7 broke the build", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:devel-gpu-py3 docker image\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master / HEAD / 21f595c4c6a92ade7fa650d004cdb32c7e80ecf5\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: source\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: 2 P100 16 GB GPUs\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nbazel build fails with\r\n\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/nccl_archive/BUILD.bazel:69:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/collectives/broadcast.cc':\r\n  'external/nccl_archive/src/collectives/collectives.h'\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ndocker pull tensorflow/tensorflow:devel-gpu-py3\r\ndocker run -idt --name nccl-failure docker.io/tensorflow/tensorflow:devel-gpu-py3 bash\r\ndocker attach nccl-failure\r\n\r\ncd $HOME\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow/\r\n./configure  [pressed enter for all prompts]\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nError started after Commit 5b5ada737a7f2c69c51748b0f54636b6849580a5 \"Update NCCL from 2.3.5 to 2.4.7\" was merged. \r\n", "comments": ["I ran into the same problem. Quick fix for this problem:\r\n```diff\r\ndiff --git a/third_party/nccl/archive.BUILD b/third_party/nccl/archive.BUILD\r\nindex 6604541..4dc7d9a 100644\r\n--- a/third_party/nccl/archive.BUILD\r\n+++ b/third_party/nccl/archive.BUILD\r\n@@ -55,6 +55,7 @@ cuda_rdc_library(\r\n     name = \"device\",b/third_party/nccl/archive.BUILD\r\n     srcs = [\r\n         \"src/collectives/device/functions.cu.cc\",\r\n+        \"src/collectives/device/common.h\",\r\n         \":device_srcs\",\r\n     ],\r\n     deps = [\r\n```", "A side note: if you install the NCCL development package at the same version in your environment, you will not reproduce this error.", "@byronyi which means using headers from installed NCCL instead of the one fetched by bazel?", "@freedomtan , Thank you for the fix. You should submit a pull request to get this fixed in the master branch. (I can but I don't want to take credit for your work. :) )", "@wdirons I didn't send PR because this quick fix is dirty. there should be more elegant way :)", "@wdirons in case you didn't notice it. should be fixed by 7aaf366 which rollbacked 5b5ada7 :-)", "Thank you @freedomtan , I hadn't seen it. (First time I logged in since the commit was rolled back.) Thank you!", "@chsigg Any chance this change will be submitted soon? I see that at NCCL 2.4.2 it got a serious bug for long-running jobs, and has been upgraded to 2.4.6 for PyTorch: https://github.com/pytorch/pytorch/issues/20630", "Yes, it will be resubmitted shortly."]}, {"number": 29661, "title": "Bug in exception handling of tf.histogram_fixed_width_bins ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.0.0-beta0\r\n- Python version: python3 from colab\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwhen value range for  `tf.histogram_fixed_width_bins ` is `[0.0, 0.0], it outputs an index outside `nbins`. See the code below.\r\n\r\n```\r\nnbins = 5\r\nvalue_range = [.0, .0]\r\nnew_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]\r\nindices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=5)\r\nprint(indices)\r\n\r\n```\r\nOutput is \r\n```\r\ntf.Tensor([          0 -2147483648           4           4           4           4], shape=(6,), dtype=int32)\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should show indices as [0,0,4,4,4,4] and throw a warning saying that the range needs to be updated\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nSee above\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Tried on Colab with TF version 2.0-beta and was able to replicate the issue.", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/60c13c6f2c1cc4948add81e210f150c1/2-1-template.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/cf4dab2373adf64de686a77e94cad76c/tf-nightly.ipynb) i.e. 2.2.0-dev20200327. Please find the attached gist. Thanks!", "This is fixed with tf-nightly version '2.2.0-dev20200402'. Thanks!\r\n```python\r\ntf.Tensor([0 4 4 4 4 4], shape=(6,), dtype=int32)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29661\">No</a>\n", "It is working as expected in GPU. But, the error still persists with CPU. \r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/858ba13ef8b77cf63355c84bd398e026/tf-nightly.ipynb) is the gist with GPU and [here](https://colab.research.google.com/gist/jvishnuvardhan/4b80e167f59162e373f0fc5591d9c14d/tf-nightly.ipynb) with CPU. Thanks!", "Added a PR #38899 for the fix. In comparison with `tf.histogram_fixed_width` which throws out InvalidArgument in case value_range is not monotonous increasing (implemented in C++ kernel), I think `tf.histogram_fixed_width_bins` needs to apply the same restriction, as was specified in its docstring.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29661\">No</a>\n"]}, {"number": 29660, "title": "Corrected wrong output", "body": "Corrected  an output of example shown for `tf.histogram_fixed_width_bins`.", "comments": []}, {"number": 29659, "title": "TPU estimator error with tensorflow 1.13 after adding GRU and bidirectionalrnn", "body": "I am getting the following error. This has occured after adding extra GRU and bidirectional rnn layers to my existing model. Can someone please explain what these mean?\r\n```\r\nINFO:tensorflow:Init TPU system\r\nINFO:tensorflow:Initialized TPU in 2 seconds\r\nINFO:tensorflow:Starting infeed thread controller.\r\nINFO:tensorflow:Starting outfeed thread controller.\r\nINFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\r\nINFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\r\nINFO:tensorflow:Error recorded from infeed: Bad hardware status: 0x1\r\n         [[node input_pipeline_task0/while/InfeedQueue/enqueue/4 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1112) ]]\r\n\r\nCaused by op u'input_pipeline_task0/while/InfeedQueue/enqueue/4', defined at:\r\n  File \"aoa.py\", line 1437, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"aoa.py\", line 1369, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2452, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1154, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2251, in _call_model_fn\r\n    config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2547, in _model_fn\r\n    input_holders.generate_infeed_enqueue_ops_and_dequeue_fn())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1167, in generate_infeed_enqueue_ops_and_dequeue_fn\r\n    self._invoke_input_fn_and_record_structure())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1271, in _invoke_input_fn_and_record_structure\r\n    wrap_fn(device=host_device, op_fn=enqueue_ops_fn))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2945, in _wrap_computation_in_while_loop parallel_iterations=1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3556, in while_loop\r\n    return_same_structure)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3087, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3022, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2934, in computation\r\n    with ops.control_dependencies(op_fn()):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 894, in enqueue_ops_fn\r\n    tpu_ordinal_function=tpu_ordinal_function_impl)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_feed.py\", line 526, in generate_enqueue_ops\r\n    for (shard, index) in zip(sharded_inputs, xrange(self.number_of_shards))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_feed.py\", line 458, in _generate_enqueue_op\r\n    device_ordinal=tpu_ordinal)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/ops/gen_tpu_ops.py\", line 1314, in infeed_enqueue_tuple\r\n    device_ordinal=device_ordinal, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnavailableError (see above for traceback): Bad hardware status: 0x1\r\n         [[node input_pipeline_task0/while/InfeedQueue/enqueue/4 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1112) ]]\r\n\r\nINFO:tensorflow:Error recorded from training_loop: Combined status information from 9 operations:\r\nStatus code: Unimplemented [9x]\r\n  Compilation failure: Convolution is only implemented for TPU for F32 and BF16 element types.\r\n        TPU compilation failed\r\n         [[node TPUReplicateMetadata (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1112) ]] [1x]\r\n  Compilation failure: Convolution is only implemented for TPU for F32 and BF16 element types.\r\n        TPU compilation failed\r\n         [[node TPUReplicateMetadata (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1112) ]]\r\n         [[node TPUReplicateMetadata (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1112) ]] [8x]\r\n(0 successful operations.)\r\n\r\nCaused by op u'TPUReplicateMetadata', defined at:\r\n  File \"aoa.py\", line 1437, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"aoa.py\", line 1369, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2452, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1154, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2251, in _call_model_fn\r\n    config)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1154, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2251, in _call_model_fn\r\n    config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2558, in _model_fn\r\n    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2893, in _train_on_tpu_system\r\n    device_assignment=ctx.device_assignment)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 890, in split_compile_and_shard\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 636, in split_compile_and_replicate\r\n    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/ops/gen_tpu_ops.py\", line 7366, in tpu_replicate_metadata\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```", "comments": ["@rakshanda22 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@gadagashwini Sure\r\nThese are the few lines which on addition have created that issue\r\n\r\n```\r\nwith tf.variable_scope('document', initializer=orthogonal_initializer()):\r\n    fwd_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    back_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    (of,ob), _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, back_cell, doc, dtype=tf.float32)\r\n    #h_doc = tf.nn.dropout(tf.concat(2, h), FLAGS.dropout_keep_prob)\r\n    h_doc =tf.concat([of,ob],axis=-1)\r\n    tf.logging.info(\"doc shape %s\",h_doc.shape)\r\n  \r\n  with tf.variable_scope('query', initializer=orthogonal_initializer()):\r\n    fwd_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    back_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    (of,ob), _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, back_cell, ques, dtype=tf.float32)\r\n    #h_query = tf.nn.dropout(tf.concat(2, h), FLAGS.dropout_keep_prob)\r\n    h_query = tf.concat([of,ob],axis=-1)\r\n    tf.logging.info(\"query shape %s\",h_query.shape)\r\n\r\ndef orthogonal_initializer(scale = 1.1):\r\n    ''' From Lasagne and Keras. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\r\n    '''\r\n    print('Warning -- You have opted to use the orthogonal_initializer function')\r\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\r\n      flat_shape = (shape[0], np.prod(shape[1:]))\r\n      a = np.random.normal(0.0, 1.0, flat_shape)\r\n      u, _, v = np.linalg.svd(a, full_matrices=False)\r\n      # pick the one with the correct shape\r\n      q = u if u.shape == flat_shape else v\r\n      q = q.reshape(shape) #this needs to be corrected to float32\r\n      print('you have initialized one orthogonal matrix.')\r\n      return tf.constant(scale * q[:shape[0], :shape[1]], dtype=tf.float32)\r\n    return _initializer\r\n\r\ndef softmax(target, axis, mask, epsilon=1e-12, name=None):\r\n  with tf.op_scope([target], name, 'softmax'):\r\n    max_axis = tf.reduce_max(target, axis, keep_dims=True)\r\n    target_exp = tf.exp(target-max_axis) * mask\r\n    normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)\r\n    softmax = target_exp / (normalize + epsilon)\r\n    return softmax\r\n```", "@rakshanda22 Thanks for providing the code snippet but it looks incomplete to reproduce the reported issue here. Can you help me to reproduce this. Thanks! ", "@gadagashwini I have a few layers before the GRU and everything works fine when I train them using a TPUestimator. After I added the above code snippet to my existing code I get the error with estimators as provided above. If I comment the above snippet and run the code on above layers it works perfectly fine. So my doubt is the problem could be somewhere in the above snippet.", "@rakshanda22 It will be difficult for us to reproduce the issue with above code snippet. Will it be possible for you to provide the minimal full code snippet with all the operation. Thanks!", "Here is the code\r\n\r\n```\r\n# coding=utf-8\r\n# Copyright 2018 The Google AI Language Team Authors.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\"\"\"Run BERT on SQuAD 1.1 and SQuAD 2.0.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport collections\r\nimport json\r\nfrom operator import itemgetter\r\nimport math\r\nimport os\r\nimport random\r\nimport modeling\r\nimport optimization\r\nimport tokenization\r\nimport six\r\nimport re\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nflags = tf.flags\r\n\r\nFLAGS = flags.FLAGS\r\n\r\n## Required parameters\r\nflags.DEFINE_string(\r\n    \"bert_config_file\", None,\r\n    \"The config json file corresponding to the pre-trained BERT model. \"\r\n    \"This specifies the model architecture.\")\r\n\r\nflags.DEFINE_string(\"vocab_file\", None,\r\n                    \"The vocabulary file that the BERT model was trained on.\")\r\n\r\nflags.DEFINE_string(\r\n    \"output_dir\", None,\r\n    \"The output directory where the model checkpoints will be written.\")\r\n\r\n## Other parameters\r\nflags.DEFINE_string(\"train_file\", None,\r\n                    \"SQuAD json for training. E.g., train-v1.1.json\")\r\n\r\nflags.DEFINE_string(\r\n    \"predict_file\", None,\r\n    \"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\")\r\n\r\nflags.DEFINE_string(\r\n    \"init_checkpoint\", None,\r\n    \"Initial checkpoint (usually from a pre-trained BERT model).\")\r\n\r\nflags.DEFINE_bool(\r\n    \"do_lower_case\", True,\r\n    \"Whether to lower case the input text. Should be True for uncased \"\r\n    \"models and False for cased models.\")\r\n\r\nflags.DEFINE_integer(\r\n    \"max_seq_length\", 384,\r\n    \"The maximum total input sequence length after WordPiece tokenization. \"\r\n    \"Sequences longer than this will be truncated, and sequences shorter \"\r\n    \"than this will be padded.\")\r\n\r\nflags.DEFINE_integer(\r\n    \"doc_stride\", 128,\r\n    \"When splitting up a long document into chunks, how much stride to \"\r\n    \"take between chunks.\")\r\n\r\nflags.DEFINE_integer(\r\n    \"max_query_length\", 64,\r\n    \"The maximum number of tokens for the question. Questions longer than \"\r\n    \"this will be truncated to this length.\")\r\n\r\nflags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\r\n\r\nflags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\r\n\r\nflags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\r\n\r\nflags.DEFINE_integer(\"predict_batch_size\", 8,\r\n                     \"Total batch size for predictions.\")\r\n\r\nflags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\r\n\r\nflags.DEFINE_float(\"num_train_epochs\", 3.0,\r\n                   \"Total number of training epochs to perform.\")\r\n\r\nflags.DEFINE_float(\r\n    \"warmup_proportion\", 0.1,\r\n    \"Proportion of training to perform linear learning rate warmup for. \"\r\n    \"E.g., 0.1 = 10% of training.\")\r\n\r\nflags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\r\n                     \"How often to save the model checkpoint.\")\r\n\r\nflags.DEFINE_integer(\"iterations_per_loop\", 1000,\r\n                     \"How many steps to make in each estimator call.\")\r\n\r\nflags.DEFINE_integer(\r\n    \"n_best_size\", 20,\r\n    \"The total number of n-best predictions to generate in the \"\r\n    \"nbest_predictions.json output file.\")\r\n\r\nflags.DEFINE_integer(\r\n    \"max_answer_length\", 30,\r\n    \"The maximum length of an answer that can be generated. This is needed \"\r\n    \"because the start and end predictions are not conditioned on one another.\")\r\n\r\nflags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\r\n\r\ntf.flags.DEFINE_string(\r\n    \"tpu_name\", None,\r\n    \"The Cloud TPU to use for training. This should be either the name \"\r\n    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\r\n    \"url.\")\r\n\r\ntf.flags.DEFINE_string(\r\n    \"tpu_zone\", None,\r\n    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\r\n    \"specified, we will attempt to automatically detect the GCE project from \"\r\n    \"metadata.\")\r\n\r\ntf.flags.DEFINE_string(\r\n    \"gcp_project\", None,\r\n    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\r\n    \"specified, we will attempt to automatically detect the GCE project from \"\r\n    \"metadata.\")\r\n\r\ntf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\r\n\r\nflags.DEFINE_integer(\r\n    \"num_tpu_cores\", 8,\r\n    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\r\n\r\nflags.DEFINE_bool(\r\n    \"verbose_logging\", False,\r\n    \"If true, all of the warnings related to data processing will be printed. \"\r\n    \"A number of warnings are expected for a normal SQuAD evaluation.\")\r\n\r\nflags.DEFINE_bool(\r\n    \"version_2_with_negative\", False,\r\n    \"If true, the SQuAD examples contain some that do not have an answer.\")\r\n\r\nflags.DEFINE_float(\r\n    \"null_score_diff_threshold\", 0.0,\r\n    \"If null_score - best_non_null is greater than the threshold predict null.\")\r\n\r\n\r\nclass SquadExample(object):\r\n  \"\"\"A single training/test example for simple sequence classification.\r\n\r\n     For examples without an answer, the start and end position are -1.\r\n  \"\"\"\r\n\r\n  def __init__(self,\r\n               qas_id,\r\n               question_text,\r\n               doc_tokens,\r\n               orig_answer_text=None,\r\n               start_position=None,\r\n               end_position=None,\r\n               is_impossible=False):\r\n    self.qas_id = qas_id\r\n    self.question_text = question_text\r\n    self.doc_tokens = doc_tokens\r\n    self.orig_answer_text = orig_answer_text\r\n    self.start_position = start_position\r\n    self.end_position = end_position\r\n    self.is_impossible = is_impossible\r\n\r\n  def __str__(self):\r\n    return self.__repr__()\r\n\r\n  def __repr__(self):\r\n    s = \"\"\r\n    s += \"qas_id: %s\" % (tokenization.printable_text(self.qas_id))\r\n    s += \", question_text: %s\" % (\r\n        tokenization.printable_text(self.question_text))\r\n    s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\r\n    if self.start_position:\r\n      s += \", start_position: %d\" % (self.start_position)\r\n    if self.start_position:\r\n      s += \", end_position: %d\" % (self.end_position)\r\n    if self.start_position:\r\n      s += \", is_impossible: %r\" % (self.is_impossible)\r\n    return s\r\n\r\n\r\nclass InputFeatures(object):\r\n  \"\"\"A single set of features of data.\"\"\"\r\n\r\n  def __init__(self,\r\n               unique_id,\r\n               example_index,\r\n               doc_span_index,\r\n               tokens,\r\n               token_to_orig_map,\r\n               token_is_max_context,\r\n               input_ids,\r\n               input_mask,\r\n               segment_ids,\r\n               start_position=None,\r\n               end_position=None,\r\n               is_impossible=None):\r\n    self.unique_id = unique_id\r\n    self.example_index = example_index\r\n    self.doc_span_index = doc_span_index\r\n    self.tokens = tokens\r\n    self.token_to_orig_map = token_to_orig_map\r\n    self.token_is_max_context = token_is_max_context\r\n    self.input_ids = input_ids\r\n    self.input_mask = input_mask\r\n    self.segment_ids = segment_ids\r\n    self.start_position = start_position\r\n    self.end_position = end_position\r\n    self.is_impossible = is_impossible\r\n\r\n\r\ndef read_squad_examples(input_file, is_training):\r\n  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\r\n  with tf.gfile.Open(input_file, \"r\") as reader:\r\n    input_data = json.load(reader)[\"data\"]\r\n\r\n  def is_whitespace(c):\r\n    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\r\n      return True\r\n    return False\r\n\r\n  examples = []\r\n  for entry in input_data:\r\n    for paragraph in entry[\"paragraphs\"]:\r\n      paragraph_text = paragraph[\"context\"]\r\n      doc_tokens = []\r\n      char_to_word_offset = []\r\n      prev_is_whitespace = True\r\n      for c in paragraph_text:\r\n        if is_whitespace(c):\r\n          prev_is_whitespace = True\r\n        else:\r\n          if prev_is_whitespace:\r\n            doc_tokens.append(c)\r\n          else:\r\n            doc_tokens[-1] += c\r\n          prev_is_whitespace = False\r\n        char_to_word_offset.append(len(doc_tokens) - 1)\r\n\r\n      for qa in paragraph[\"qas\"]:\r\n        qas_id = qa[\"id\"]\r\n        question_text = qa[\"question\"]\r\n        start_position = None\r\n        end_position = None\r\n        orig_answer_text = None\r\n        is_impossible = False\r\n        if is_training:\r\n\r\n          if FLAGS.version_2_with_negative:\r\n            is_impossible = qa[\"is_impossible\"]\r\n          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\r\n            raise ValueError(\r\n                \"For training, each question should have exactly 1 answer.\")\r\n          if not is_impossible:\r\n            answer = qa[\"answers\"][0]\r\n            orig_answer_text = answer[\"text\"]\r\n            answer_offset = answer[\"answer_start\"]\r\n            answer_length = len(orig_answer_text)\r\n            start_position = char_to_word_offset[answer_offset]\r\n            end_position = char_to_word_offset[answer_offset + answer_length -\r\n                                               1]\r\n            # Only add answers where the text can be exactly recovered from the\r\n            # document. If this CAN'T happen it's likely due to weird Unicode\r\n            # stuff so we will just skip the example.\r\n            #\r\n            # Note that this means for training mode, every example is NOT\r\n            # guaranteed to be preserved.\r\n            actual_text = \" \".join(\r\n                doc_tokens[start_position:(end_position + 1)])\r\n            cleaned_answer_text = \" \".join(\r\n                tokenization.whitespace_tokenize(orig_answer_text))\r\n            if actual_text.find(cleaned_answer_text) == -1:\r\n              tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\r\n                                 actual_text, cleaned_answer_text)\r\n              continue\r\n          else:\r\n            start_position = -1\r\n            end_position = -1\r\n            orig_answer_text = \"\"\r\n\r\n        example = SquadExample(\r\n            qas_id=qas_id,\r\n            question_text=question_text,\r\n            doc_tokens=doc_tokens,\r\n            orig_answer_text=orig_answer_text,\r\n            start_position=start_position,\r\n            end_position=end_position,\r\n            is_impossible=is_impossible)\r\n        examples.append(example)\r\n\r\n  return examples\r\n\r\n\r\ndef convert_examples_to_features(examples, tokenizer, max_seq_length,\r\n                                 doc_stride, max_query_length, is_training,\r\n                                 output_fn):\r\n  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\r\n\r\n  unique_id = 1000000000\r\n\r\n  for (example_index, example) in enumerate(examples):\r\n    query_tokens = tokenizer.tokenize(example.question_text)\r\n\r\n    if len(query_tokens) > max_query_length:\r\n      query_tokens = query_tokens[0:max_query_length]\r\n\r\n    tok_to_orig_index = []\r\n    orig_to_tok_index = []\r\n    all_doc_tokens = []\r\n    for (i, token) in enumerate(example.doc_tokens):\r\n      orig_to_tok_index.append(len(all_doc_tokens))\r\n      sub_tokens = tokenizer.tokenize(token)\r\n      for sub_token in sub_tokens:\r\n        tok_to_orig_index.append(i)\r\n        all_doc_tokens.append(sub_token)\r\n\r\n    tok_start_position = None\r\n    tok_end_position = None\r\n    if is_training and example.is_impossible:\r\n      tok_start_position = -1\r\n      tok_end_position = -1\r\n    if is_training and not example.is_impossible:\r\n      tok_start_position = orig_to_tok_index[example.start_position]\r\n      if example.end_position < len(example.doc_tokens) - 1:\r\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\r\n      else:\r\n        tok_end_position = len(all_doc_tokens) - 1\r\n      (tok_start_position, tok_end_position) = _improve_answer_span(\r\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\r\n          example.orig_answer_text)\r\n\r\n    # The -3 accounts for [CLS], [SEP] and [SEP]\r\n    # max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\r\n    max_tokens_for_doc = max_seq_length - max_query_length - 3\r\n\r\n    # We can have documents that are longer than the maximum sequence length.\r\n    # To deal with this we do a sliding window approach, where we take chunks\r\n    # of the up to our max length with a stride of `doc_stride`.\r\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\r\n        \"DocSpan\", [\"start\", \"length\"])\r\n    doc_spans = []\r\n    start_offset = 0\r\n    while start_offset < len(all_doc_tokens):\r\n      length = len(all_doc_tokens) - start_offset\r\n      if length > max_tokens_for_doc:\r\n        length = max_tokens_for_doc\r\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\r\n      if start_offset + length == len(all_doc_tokens):\r\n        break\r\n      start_offset += min(length, doc_stride)\r\n\r\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\r\n      tokens = []\r\n      token_to_orig_map = {}\r\n      token_is_max_context = {}\r\n      segment_ids = []\r\n      tokens.append(\"[CLS]\")\r\n      segment_ids.append(0)\r\n      # if len(query_tokens)>max_query_length:\r\n      #   tf.logging.info(\"QUESTION BIGGER %s %s\",len(query_tokens), query_tokens)\r\n      for token in query_tokens:\r\n        tokens.append(token)\r\n        segment_ids.append(0)\r\n      # if len(tokens)<61:\r\n      while len(tokens)<max_query_length+1:\r\n        tokens.append(\"[PAD]\")\r\n        segment_ids.append(0)\r\n      tokens.append(\"[SEP]\")\r\n      segment_ids.append(0)\r\n\r\n      for i in range(doc_span.length):\r\n        split_token_index = doc_span.start + i\r\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\r\n\r\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\r\n                                               split_token_index)\r\n        token_is_max_context[len(tokens)] = is_max_context\r\n        tokens.append(all_doc_tokens[split_token_index])\r\n        segment_ids.append(1)\r\n      tokens.append(\"[SEP]\")\r\n      segment_ids.append(1)\r\n\r\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n\r\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n      # tokens are attended to.\r\n      input_mask=[]\r\n      for i in input_ids:\r\n        if i:\r\n          input_mask.append(1)\r\n        else:\r\n          input_mask.append(0)\r\n      # Zero-pad up to the sequence length.\r\n      while len(input_ids) < max_seq_length:\r\n        input_ids.append(0)\r\n        input_mask.append(0)\r\n        segment_ids.append(0)\r\n      # tf.logging.info(\"lengths %s%s%s%s\",max_seq_length,len(input_ids),len(input_mask),len(segment_ids) )\r\n      assert len(input_ids) == max_seq_length\r\n      assert len(input_mask) == max_seq_length\r\n      assert len(segment_ids) == max_seq_length\r\n\r\n      start_position = None\r\n      end_position = None\r\n      if is_training and not example.is_impossible:\r\n        # For training, if our document chunk does not contain an annotation\r\n        # we throw it out, since there is nothing to predict.\r\n        doc_start = doc_span.start\r\n        doc_end = doc_span.start + doc_span.length - 1\r\n        out_of_span = False\r\n        if not (tok_start_position >= doc_start and\r\n                tok_end_position <= doc_end):\r\n          out_of_span = True\r\n        if out_of_span:\r\n          start_position = 0\r\n          end_position = 0\r\n        else:\r\n          doc_offset = max_query_length + 2\r\n          start_position = tok_start_position - doc_start + doc_offset\r\n          end_position = tok_end_position - doc_start + doc_offset\r\n\r\n      if is_training and example.is_impossible:\r\n        start_position = 0\r\n        end_position = 0\r\n\r\n      if example_index < 20:\r\n        tf.logging.info(\"*** Example ***\")\r\n        tf.logging.info(\"unique_id: %s\" % (unique_id))\r\n        tf.logging.info(\"example_index: %s\" % (example_index))\r\n        tf.logging.info(\"doc_span_index: %s\" % (doc_span_index))\r\n        tf.logging.info(\"tokens: %s\" % \" \".join(\r\n            [tokenization.printable_text(x) for x in tokens]))\r\n        tf.logging.info(\"token_to_orig_map: %s\" % \" \".join(\r\n            [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\r\n        tf.logging.info(\"token_is_max_context: %s\" % \" \".join([\r\n            \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\r\n        ]))\r\n        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\r\n        tf.logging.info(\r\n            \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\r\n        tf.logging.info(\r\n            \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\r\n        if is_training and example.is_impossible:\r\n          tf.logging.info(\"impossible example\")\r\n        if is_training and not example.is_impossible:\r\n          answer_text = \" \".join(tokens[start_position:(end_position + 1)])\r\n          tf.logging.info(\"start_position: %d\" % (start_position))\r\n          tf.logging.info(\"end_position: %d\" % (end_position))\r\n          tf.logging.info(\r\n              \"answer: %s\" % (tokenization.printable_text(answer_text)))\r\n\r\n      feature = InputFeatures(\r\n          unique_id=unique_id,\r\n          example_index=example_index,\r\n          doc_span_index=doc_span_index,\r\n          tokens=tokens,\r\n          token_to_orig_map=token_to_orig_map,\r\n          token_is_max_context=token_is_max_context,\r\n          input_ids=input_ids,\r\n          input_mask=input_mask,\r\n          segment_ids=segment_ids,\r\n          start_position=start_position,\r\n          end_position=end_position,\r\n          is_impossible=example.is_impossible)\r\n\r\n      # Run callback\r\n      output_fn(feature)\r\n\r\n      unique_id += 1\r\n\r\n\r\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\r\n                         orig_answer_text):\r\n  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\r\n\r\n  # The SQuAD annotations are character based. We first project them to\r\n  # whitespace-tokenized words. But then after WordPiece tokenization, we can\r\n  # often find a \"better match\". For example:\r\n  #\r\n  #   Question: What year was John Smith born?\r\n  #   Context: The leader was John Smith (1895-1943).\r\n  #   Answer: 1895\r\n  #\r\n  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\r\n  # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\r\n  # the exact answer, 1895.\r\n  #\r\n  # However, this is not always possible. Consider the following:\r\n  #\r\n  #   Question: What country is the top exporter of electornics?\r\n  #   Context: The Japanese electronics industry is the lagest in the world.\r\n  #   Answer: Japan\r\n  #\r\n  # In this case, the annotator chose \"Japan\" as a character sub-span of\r\n  # the word \"Japanese\". Since our WordPiece tokenizer does not split\r\n  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\r\n  # in SQuAD, but does happen.\r\n  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\r\n\r\n  for new_start in range(input_start, input_end + 1):\r\n    for new_end in range(input_end, new_start - 1, -1):\r\n      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\r\n      if text_span == tok_answer_text:\r\n        return (new_start, new_end)\r\n\r\n  return (input_start, input_end)\r\n\r\n\r\ndef _check_is_max_context(doc_spans, cur_span_index, position):\r\n  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\r\n\r\n  # Because of the sliding window approach taken to scoring documents, a single\r\n  # token can appear in multiple documents. E.g.\r\n  #  Doc: the man went to the store and bought a gallon of milk\r\n  #  Span A: the man went to the\r\n  #  Span B: to the store and bought\r\n  #  Span C: and bought a gallon of\r\n  #  ...\r\n  #\r\n  # Now the word 'bought' will have two scores from spans B and C. We only\r\n  # want to consider the score with \"maximum context\", which we define as\r\n  # the *minimum* of its left and right context (the *sum* of left and\r\n  # right context will always be the same, of course).\r\n  #\r\n  # In the example the maximum context for 'bought' would be span C since\r\n  # it has 1 left context and 3 right context, while span B has 4 left context\r\n  # and 0 right context.\r\n  best_score = None\r\n  best_span_index = None\r\n  for (span_index, doc_span) in enumerate(doc_spans):\r\n    end = doc_span.start + doc_span.length - 1\r\n    if position < doc_span.start:\r\n      continue\r\n    if position > end:\r\n      continue\r\n    num_left_context = position - doc_span.start\r\n    num_right_context = end - position\r\n    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\r\n    if best_score is None or score > best_score:\r\n      best_score = score\r\n      best_span_index = span_index\r\n\r\n  return cur_span_index == best_span_index\r\n\r\ndef AOA(q,d,batch_size):\r\n  i_att = tf.linalg.matmul(d,q,adjoint_b=True) # shape = (batch_size, self.d, self.q)\r\n  # individual attentions\r\n  tf.logging.info(\"iatt shape %s\",i_att.shape)\r\n  alpha = tf.map_fn(lambda x: tf.nn.softmax(tf.transpose(x)), i_att)\r\n  # attention-over-attentions\r\n  tf.logging.info(\"alpha shape %s\",alpha.shape)\r\n  beta_t = tf.map_fn(tf.nn.softmax, i_att)\r\n  tf.logging.info(\"beta_t shape %s\",beta_t.shape)\r\n  beta = tf.map_fn(lambda x: tf.reduce_mean(x, 0), beta_t) # shape = (batch_size, self.q, )\r\n  beta = tf.reshape(beta, [batch_size, FLAGS.max_query_length+2, 1])\r\n  tf.logging.info(\"beta shape %s\",beta.shape)\r\n  # document-level attention\r\n  s = tf.linalg.matmul(alpha,beta,adjoint_a=True) # shape = (batch_size, self.d, 1)\r\n  s=tf.reshape(s,[batch_size,FLAGS.max_seq_length-FLAGS.max_query_length-2])\r\n  tf.logging.info(\"s shape %s\",s.shape)\r\n  return s#document = self.inputs[:,:self.d]\r\n\r\ndef AOA1(q,d,batch_size,hidden_size,input_mask):\r\n  # with tf.variable_scope('document', initializer=orthogonal_initializer()):\r\n  #   fwd_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n  #   back_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n  #   h, _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, back_cell, d, sequence_length=FLAGS.max_seq_length-FLAGS.max_query_length-2, dtype=tf.float32)\r\n  #   #h_doc = tf.nn.dropout(tf.concat(2, h), FLAGS.dropout_keep_prob)\r\n  #   h_doc = tf.concat(2, h)\r\n  #   tf.logging.info(\"doc shape %s\",h_doc.shape)\r\n  \r\n  # with tf.variable_scope('query', initializer=orthogonal_initializer()):\r\n  #   fwd_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n  #   back_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n  #   h, _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, back_cell, q, sequence_length=FLAGS.max_query_length+2, dtype=tf.float32)\r\n  #   #h_query = tf.nn.dropout(tf.concat(2, h), FLAGS.dropout_keep_prob)\r\n  #   h_query = tf.concat(2, h)\r\n  #   tf.logging.info(\"query shape %s\",h_query.shape)\r\n  \r\n  M = tf.matmul(d, q, adjoint_b=True)\r\n  query_mask=tf.slice(input_mask,[0,0],[batch_size,FLAGS.max_query_length+2])\r\n  doc_mask=tf.slice(input_mask,[0,FLAGS.max_query_length+2],[batch_size,FLAGS.max_seq_length-FLAGS.max_query_length-2])\r\n  M_mask = tf.to_float(tf.matmul(tf.expand_dims(doc_mask, -1), tf.expand_dims(query_mask, 1)))\r\n  alpha = softmax(M, 1, M_mask)\r\n  tf.logging.info(\"alpha shape %s\",alpha.shape)\r\n  beta = softmax(M, 2, M_mask)\r\n  tf.logging.info(\"beta shape %s\",beta.shape)\r\n  #query_importance = tf.expand_dims(tf.reduce_mean(beta, reduction_indices=1), -1)\r\n  query_importance = tf.expand_dims(tf.reduce_sum(beta, 1) / tf.to_float(tf.expand_dims(FLAGS.max_seq_length-FLAGS.max_query_length-2, -1)), -1)\r\n  tf.logging.info(\"query imp shape %s\",query_importance.shape)\r\n  s = tf.squeeze(tf.matmul(alpha, query_importance), [2])\r\n  tf.logging.info(\"final logits shape %s\",s.shape)\r\n  return s\r\n\r\ndef orthogonal_initializer(scale = 1.1):\r\n    ''' From Lasagne and Keras. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\r\n    '''\r\n    print('Warning -- You have opted to use the orthogonal_initializer function')\r\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\r\n      flat_shape = (shape[0], np.prod(shape[1:]))\r\n      a = np.random.normal(0.0, 1.0, flat_shape)\r\n      u, _, v = np.linalg.svd(a, full_matrices=False)\r\n      # pick the one with the correct shape\r\n      q = u if u.shape == flat_shape else v\r\n      q = q.reshape(shape) #this needs to be corrected to float32\r\n      print('you have initialized one orthogonal matrix.')\r\n      return tf.constant(scale * q[:shape[0], :shape[1]], dtype=tf.float32)\r\n    return _initializer\r\n\r\ndef softmax(target, axis, mask, epsilon=1e-12, name=None):\r\n  with tf.op_scope([target], name, 'softmax'):\r\n    max_axis = tf.reduce_max(target, axis, keep_dims=True)\r\n    target_exp = tf.exp(target-max_axis) * mask\r\n    normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)\r\n    softmax = target_exp / (normalize + epsilon)\r\n    return softmax\r\n\r\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\r\n                 use_one_hot_embeddings):\r\n  \"\"\"Creates a classification model.\"\"\"\r\n  model = modeling.BertModel(\r\n      config=bert_config,\r\n      is_training=is_training,\r\n      input_ids=input_ids,\r\n      input_mask=input_mask,\r\n      token_type_ids=segment_ids,\r\n      use_one_hot_embeddings=use_one_hot_embeddings)\r\n\r\n  final_hidden = model.get_sequence_output()\r\n  final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\r\n  batch_size = final_hidden_shape[0]\r\n  seq_length = final_hidden_shape[1]\r\n  hidden_size = final_hidden_shape[2]\r\n  ques=tf.slice(final_hidden,[0,0,0],[batch_size,FLAGS.max_query_length+2,hidden_size])\r\n  doc=tf.slice(final_hidden,[0,FLAGS.max_query_length+2,0],[batch_size,seq_length-FLAGS.max_query_length-2,hidden_size])\r\n  \r\n  with tf.variable_scope('document', initializer=orthogonal_initializer()):\r\n    fwd_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    back_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    (of,ob), _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, back_cell, doc, dtype=tf.float32)\r\n    #h_doc = tf.nn.dropout(tf.concat(2, h), FLAGS.dropout_keep_prob)\r\n    h_doc =tf.concat([of,ob],axis=-1)\r\n    tf.logging.info(\"doc shape %s\",h_doc.shape)\r\n  \r\n  with tf.variable_scope('query', initializer=orthogonal_initializer()):\r\n    fwd_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    back_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    (of,ob), _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, back_cell, ques, dtype=tf.float32)\r\n    #h_query = tf.nn.dropout(tf.concat(2, h), FLAGS.dropout_keep_prob)\r\n    h_query = tf.concat([of,ob],axis=-1)\r\n    tf.logging.info(\"query shape %s\",h_query.shape)\r\n\r\n  ques_start=tf.contrib.layers.fully_connected(ques,int(hidden_size/2),activation_fn=tf.nn.relu)\r\n  ques_end=tf.contrib.layers.fully_connected(ques,int(hidden_size/2),activation_fn=tf.nn.relu)\r\n  doc_start=tf.contrib.layers.fully_connected(doc,int(hidden_size/2),activation_fn=tf.nn.relu)\r\n  doc_end=tf.contrib.layers.fully_connected(doc,int(hidden_size/2),activation_fn=tf.nn.relu)\r\n\r\n  # start_logits=AOA(ques_start,doc_start,batch_size)\r\n  # end_logits=AOA(ques_end,doc_end,batch_size)\r\n\r\n  start_logits=AOA1(ques_start,doc_start,batch_size,hidden_size,input_mask)\r\n  end_logits=AOA1(ques_end,doc_end,batch_size,hidden_size,input_mask)\r\n\r\n  # output_weights = tf.get_variable(\r\n  #     \"cls/squad/output_weights\", [2, int(hidden_size)],\r\n  #     initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n\r\n  # output_bias = tf.get_variable(\r\n  #     \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\r\n\r\n\r\n  # tf.logging.info(\"  final hidden input= %s\" % (final_hidden.get_shape()))\r\n  \r\n  # final_hidden_matrix = tf.reshape(final_hidden,\r\n  #                                  [batch_size * seq_length, int(hidden_size)])\r\n\r\n  # logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\r\n  # logits = tf.nn.bias_add(logits, output_bias)\r\n\r\n  # logits = tf.reshape(logits, [batch_size, seq_length, 2])\r\n  # tf.logging.info(\"input logit shape = %s\",logits.shape)\r\n  # logits = tf.transpose(logits, [2, 0, 1])\r\n  # tf.logging.info(\"output logit shape = %s\",logits.shape)\r\n\r\n  # unstacked_logits = tf.unstack(logits, axis=0)\r\n\r\n  # (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\r\n\r\n  return (start_logits, end_logits)\r\n\r\n\r\ndef model_fn_builder(bert_config, init_checkpoint, learning_rate,\r\n                     num_train_steps, num_warmup_steps, use_tpu,\r\n                     use_one_hot_embeddings):\r\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\r\n\r\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\r\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\r\n\r\n    tf.logging.info(\"*** Features ***\")\r\n    for name in sorted(features.keys()):\r\n      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\r\n\r\n    unique_ids = features[\"unique_ids\"]\r\n    input_ids = features[\"input_ids\"]\r\n    input_mask = features[\"input_mask\"]\r\n    segment_ids = features[\"segment_ids\"]\r\n\r\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n    (start_logits, end_logits) = create_model(\r\n        bert_config=bert_config,\r\n        is_training=is_training,\r\n        input_ids=input_ids,\r\n        input_mask=input_mask,\r\n        segment_ids=segment_ids,\r\n        use_one_hot_embeddings=use_one_hot_embeddings)\r\n\r\n    tvars = tf.trainable_variables()\r\n\r\n    initialized_variable_names = {}\r\n    scaffold_fn = None\r\n    if init_checkpoint:\r\n      (assignment_map, initialized_variable_names\r\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\r\n      if use_tpu:\r\n\r\n        def tpu_scaffold():\r\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n          return tf.train.Scaffold()\r\n\r\n        scaffold_fn = tpu_scaffold\r\n      else:\r\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n\r\n    tf.logging.info(\"**** Trainable Variables ****\")\r\n    for var in tvars:\r\n      init_string = \"\"\r\n      if var.name in initialized_variable_names:\r\n        init_string = \", *INIT_FROM_CKPT*\"\r\n      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\r\n                      init_string)\r\n\r\n    output_spec = None\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n      seq_length = modeling.get_shape_list(input_ids)[1]\r\n\r\n      def compute_loss(logits, positions):\r\n        one_hot_positions = tf.one_hot(\r\n            positions, depth=FLAGS.max_seq_length, dtype=tf.float32)\r\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\r\n        loss = -tf.reduce_mean(\r\n            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\r\n        return loss\r\n\r\n      start_positions = features[\"start_positions\"]\r\n      end_positions = features[\"end_positions\"]\r\n\r\n      start_loss = compute_loss(start_logits, start_positions)\r\n      end_loss = compute_loss(end_logits, end_positions)\r\n\r\n      total_loss = (start_loss + end_loss) / 2.0\r\n\r\n      train_op = optimization.create_optimizer(\r\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n\r\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n          mode=mode,\r\n          loss=total_loss,\r\n          train_op=train_op,\r\n          scaffold_fn=scaffold_fn)\r\n    elif mode == tf.estimator.ModeKeys.PREDICT:\r\n      predictions = {\r\n          \"unique_ids\": unique_ids,\r\n          \"start_logits\": start_logits,\r\n          \"end_logits\": end_logits,\r\n      }\r\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\r\n    else:\r\n      raise ValueError(\r\n          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\r\n\r\n    return output_spec\r\n\r\n  return model_fn\r\n\r\n\r\ndef input_fn_builder(input_file, seq_length, is_training, drop_remainder):\r\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\r\n\r\n  name_to_features = {\r\n      \"unique_ids\": tf.FixedLenFeature([], tf.int64),\r\n      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\r\n      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\r\n      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\r\n  }\r\n\r\n  if is_training:\r\n    name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\r\n    name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\r\n\r\n  def _decode_record(record, name_to_features):\r\n    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\r\n    example = tf.parse_single_example(record, name_to_features)\r\n\r\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\r\n    # So cast all int64 to int32.\r\n    for name in list(example.keys()):\r\n      t = example[name]\r\n      if t.dtype == tf.int64:\r\n        t = tf.to_int32(t)\r\n      example[name] = t\r\n\r\n    return example\r\n\r\n  def input_fn(params):\r\n    \"\"\"The actual input function.\"\"\"\r\n    batch_size = params[\"batch_size\"]\r\n\r\n    # For training, we want a lot of parallel reading and shuffling.\r\n    # For eval, we want no shuffling and parallel reading doesn't matter.\r\n    d = tf.data.TFRecordDataset(input_file)\r\n    if is_training:\r\n      d = d.repeat()\r\n      d = d.shuffle(buffer_size=100)\r\n\r\n    d = d.apply(\r\n        tf.contrib.data.map_and_batch(\r\n            lambda record: _decode_record(record, name_to_features),\r\n            batch_size=batch_size,\r\n            drop_remainder=drop_remainder))\r\n\r\n    return d\r\n\r\n  return input_fn\r\n\r\n\r\nRawResult = collections.namedtuple(\"RawResult\",\r\n                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\r\n\r\n\r\ndef write_predictions(all_examples, all_features, all_results, n_best_size,\r\n                      max_answer_length, do_lower_case, output_prediction_file,\r\n                      output_nbest_file, output_null_log_odds_file):\r\n  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\r\n  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\r\n  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\r\n\r\n  example_index_to_features = collections.defaultdict(list)\r\n  for feature in all_features:\r\n    example_index_to_features[feature.example_index].append(feature)\r\n\r\n  unique_id_to_result = {}\r\n  for result in all_results:\r\n    unique_id_to_result[result.unique_id] = result\r\n\r\n  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\r\n      \"PrelimPrediction\",\r\n      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\r\n\r\n  all_predictions = collections.OrderedDict()\r\n  all_nbest_json = collections.OrderedDict()\r\n  scores_diff_json = collections.OrderedDict()\r\n\r\n  for (example_index, example) in enumerate(all_examples):\r\n    features = example_index_to_features[example_index]\r\n\r\n    prelim_predictions = []\r\n    # keep track of the minimum score of null start+end of position 0\r\n    score_null = 1000000  # large and positive\r\n    min_null_feature_index = 0  # the paragraph slice with min mull score\r\n    null_start_logit = 0  # the start logit at the slice with min null score\r\n    null_end_logit = 0  # the end logit at the slice with min null score\r\n    for (feature_index, feature) in enumerate(features):\r\n      result = unique_id_to_result[feature.unique_id]\r\n      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\r\n      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\r\n      # if we could have irrelevant answers, get the min score of irrelevant\r\n      if FLAGS.version_2_with_negative:\r\n        feature_null_score = result.start_logits[0] + result.end_logits[0]\r\n        if feature_null_score < score_null:\r\n          score_null = feature_null_score\r\n          min_null_feature_index = feature_index\r\n          null_start_logit = result.start_logits[0]\r\n          null_end_logit = result.end_logits[0]\r\n      for start_index in start_indexes:\r\n        for end_index in end_indexes:\r\n          # We could hypothetically create invalid predictions, e.g., predict\r\n          # that the start of the span is in the question. We throw out all\r\n          # invalid predictions.\r\n          if start_index >= len(feature.tokens):\r\n            continue\r\n          if end_index >= len(feature.tokens):\r\n            continue\r\n          if start_index not in feature.token_to_orig_map:\r\n            continue\r\n          if end_index not in feature.token_to_orig_map:\r\n            continue\r\n          if not feature.token_is_max_context.get(start_index, False):\r\n            continue\r\n          if end_index < start_index:\r\n            continue\r\n          length = end_index - start_index + 1\r\n          if length > max_answer_length:\r\n            continue\r\n          prelim_predictions.append(\r\n              _PrelimPrediction(\r\n                  feature_index=feature_index,\r\n                  start_index=start_index,\r\n                  end_index=end_index,\r\n                  start_logit=result.start_logits[start_index],\r\n                  end_logit=result.end_logits[end_index]))\r\n\r\n    if FLAGS.version_2_with_negative:\r\n      prelim_predictions.append(\r\n          _PrelimPrediction(\r\n              feature_index=min_null_feature_index,\r\n              start_index=0,\r\n              end_index=0,\r\n              start_logit=null_start_logit,\r\n              end_logit=null_end_logit))\r\n    prelim_predictions = sorted(\r\n        prelim_predictions,\r\n        key=lambda x: (x.start_logit + x.end_logit),\r\n        reverse=True)\r\n\r\n    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\r\n        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\r\n\r\n    seen_predictions = {}\r\n    nbest = []\r\n    for pred in prelim_predictions:\r\n      if len(nbest) >= n_best_size:\r\n        break\r\n      feature = features[pred.feature_index]\r\n      if pred.start_index > 0:  # this is a non-null prediction\r\n        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\r\n        orig_doc_start = feature.token_to_orig_map[pred.start_index]\r\n        orig_doc_end = feature.token_to_orig_map[pred.end_index]\r\n        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\r\n        tok_text = \" \".join(tok_tokens)\r\n\r\n        # De-tokenize WordPieces that have been split off.\r\n        tok_text = tok_text.replace(\" ##\", \"\")\r\n        tok_text = tok_text.replace(\"##\", \"\")\r\n\r\n        # Clean whitespace\r\n        tok_text = tok_text.strip()\r\n        tok_text = \" \".join(tok_text.split())\r\n        orig_text = \" \".join(orig_tokens)\r\n\r\n        final_text = get_final_text(tok_text, orig_text, do_lower_case)\r\n        if final_text in seen_predictions:\r\n          continue\r\n\r\n        seen_predictions[final_text] = True\r\n      else:\r\n        final_text = \"\"\r\n        seen_predictions[final_text] = True\r\n\r\n      nbest.append(\r\n          _NbestPrediction(\r\n              text=final_text,\r\n              start_logit=pred.start_logit,\r\n              end_logit=pred.end_logit))\r\n\r\n    # if we didn't inlude the empty option in the n-best, inlcude it\r\n    if FLAGS.version_2_with_negative:\r\n      if \"\" not in seen_predictions:\r\n        nbest.append(\r\n            _NbestPrediction(\r\n                text=\"\", start_logit=null_start_logit,\r\n                end_logit=null_end_logit))\r\n    # In very rare edge cases we could have no valid predictions. So we\r\n    # just create a nonce prediction in this case to avoid failure.\r\n    if not nbest:\r\n      nbest.append(\r\n          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\r\n\r\n    assert len(nbest) >= 1\r\n\r\n    total_scores = []\r\n    best_non_null_entry = None\r\n    for entry in nbest:\r\n      # tf.logging.info(\"entry %s\",entry)\r\n      total_scores.append(entry.start_logit + entry.end_logit)\r\n      if not best_non_null_entry:\r\n        if entry.text:\r\n          best_non_null_entry = entry\r\n\r\n    probs = _compute_softmax(total_scores)\r\n\r\n    nbest_json = []\r\n    for (i, entry) in enumerate(nbest):\r\n      output = collections.OrderedDict()\r\n      output[\"text\"] = entry.text\r\n      output[\"probability\"] = probs[i]\r\n      output[\"start_logit\"] = entry.start_logit\r\n      output[\"end_logit\"] = entry.end_logit\r\n      nbest_json.append(output)\r\n    # nbest_json=checkprob(nbest_json,tokenization.printable_text(example.question_text))\r\n    assert len(nbest_json) >= 1\r\n\r\n    if not FLAGS.version_2_with_negative:\r\n      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\r\n    else:\r\n      # predict \"\" iff the null score - the score of best non-null > threshold\r\n      # tf.logging.info(\"best non null %s\",best_non_null_entry)\r\n      if best_non_null_entry:\r\n        score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)\r\n      else:\r\n        tf.logging.info(\"null entry in ber not null\")\r\n        score_diff=score_null\r\n      scores_diff_json[example.qas_id] = score_diff\r\n      if score_diff > FLAGS.null_score_diff_threshold:\r\n        all_predictions[example.qas_id] = \"\"\r\n      else:\r\n        all_predictions[example.qas_id] = best_non_null_entry.text\r\n\r\n    all_nbest_json[example.qas_id] = nbest_json\r\n\r\n  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\r\n    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\r\n\r\n  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\r\n    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\r\n\r\n  if FLAGS.version_2_with_negative:\r\n    with tf.gfile.GFile(output_null_log_odds_file, \"w\") as writer:\r\n      writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\r\n\r\n\r\ndef get_final_text(pred_text, orig_text, do_lower_case):\r\n  \"\"\"Project the tokenized prediction back to the original text.\"\"\"\r\n\r\n  # When we created the data, we kept track of the alignment between original\r\n  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\r\n  # now `orig_text` contains the span of our original text corresponding to the\r\n  # span that we predicted.\r\n  #\r\n  # However, `orig_text` may contain extra characters that we don't want in\r\n  # our prediction.\r\n  #\r\n  # For example, let's say:\r\n  #   pred_text = steve smith\r\n  #   orig_text = Steve Smith's\r\n  #\r\n  # We don't want to return `orig_text` because it contains the extra \"'s\".\r\n  #\r\n  # We don't want to return `pred_text` because it's already been normalized\r\n  # (the SQuAD eval script also does punctuation stripping/lower casing but\r\n  # our tokenizer does additional normalization like stripping accent\r\n  # characters).\r\n  #\r\n  # What we really want to return is \"Steve Smith\".\r\n  #\r\n  # Therefore, we have to apply a semi-complicated alignment heruistic between\r\n  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\r\n  # can fail in certain cases in which case we just return `orig_text`.\r\n\r\n  def _strip_spaces(text):\r\n    ns_chars = []\r\n    ns_to_s_map = collections.OrderedDict()\r\n    for (i, c) in enumerate(text):\r\n      if c == \" \":\r\n        continue\r\n      ns_to_s_map[len(ns_chars)] = i\r\n      ns_chars.append(c)\r\n    ns_text = \"\".join(ns_chars)\r\n    return (ns_text, ns_to_s_map)\r\n\r\n  # We first tokenize `orig_text`, strip whitespace from the result\r\n  # and `pred_text`, and check if they are the same length. If they are\r\n  # NOT the same length, the heuristic has failed. If they are the same\r\n  # length, we assume the characters are one-to-one aligned.\r\n  tokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\r\n\r\n  tok_text = \" \".join(tokenizer.tokenize(orig_text))\r\n\r\n  start_position = tok_text.find(pred_text)\r\n  if start_position == -1:\r\n    if FLAGS.verbose_logging:\r\n      tf.logging.info(\r\n          \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\r\n    return orig_text\r\n  end_position = start_position + len(pred_text) - 1\r\n\r\n  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\r\n  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\r\n\r\n  if len(orig_ns_text) != len(tok_ns_text):\r\n    if FLAGS.verbose_logging:\r\n      tf.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\r\n                      orig_ns_text, tok_ns_text)\r\n    return orig_text\r\n\r\n  # We then project the characters in `pred_text` back to `orig_text` using\r\n  # the character-to-character alignment.\r\n  tok_s_to_ns_map = {}\r\n  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\r\n    tok_s_to_ns_map[tok_index] = i\r\n\r\n  orig_start_position = None\r\n  if start_position in tok_s_to_ns_map:\r\n    ns_start_position = tok_s_to_ns_map[start_position]\r\n    if ns_start_position in orig_ns_to_s_map:\r\n      orig_start_position = orig_ns_to_s_map[ns_start_position]\r\n\r\n  if orig_start_position is None:\r\n    if FLAGS.verbose_logging:\r\n      tf.logging.info(\"Couldn't map start position\")\r\n    return orig_text\r\n\r\n  orig_end_position = None\r\n  if end_position in tok_s_to_ns_map:\r\n    ns_end_position = tok_s_to_ns_map[end_position]\r\n    if ns_end_position in orig_ns_to_s_map:\r\n      orig_end_position = orig_ns_to_s_map[ns_end_position]\r\n\r\n  if orig_end_position is None:\r\n    if FLAGS.verbose_logging:\r\n      tf.logging.info(\"Couldn't map end position\")\r\n    return orig_text\r\n\r\n  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\r\n  return output_text\r\n\r\n\r\ndef _get_best_indexes(logits, n_best_size):\r\n  \"\"\"Get the n-best logits from a list.\"\"\"\r\n  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\r\n\r\n  best_indexes = []\r\n  for i in range(len(index_and_score)):\r\n    if i >= n_best_size:\r\n      break\r\n    best_indexes.append(index_and_score[i][0])\r\n  return best_indexes\r\n\r\n# dictofques={\"when\":[\"before\",\"after\",\"about\",\"around\",\"from\",\"during\"],\r\n# \"where\":[\"in\",\"at\",\"on\",\"behind\",\"from\",\"through\",\"between\",\"throughout\"],\"whose\":[\"s\"],\"which\":[\"the\",\"a\"]}\r\n\r\n# def checkprob(lists,question):\r\n#   # tf.logging.info(\"LOGGING QUESTION: '%s' \" % question)\r\n#   # qlist=re.findall(r'\\w+', question.lower)\r\n#   question=question.lower()\r\n#   # tf.logging.info(\"LOGGING QUESTION LOWER: '%s' \" % question)\r\n#   # qlist=split(question,\" -'\")\r\n#   # qlist=re.split('\\s|(?<!\\d)[,.]|[,.](?!\\d)', question)\r\n#   # qlist=question.split(\" .,:'\")\r\n#   qlist=re.findall(r'\\w+', question)\r\n#   # tf.logging.info(\"LOGGING QUESTION LOWER: '%s' \" % qlist)\r\n#   # tf.logging.info(\"LOGGING LISTS LOWER: '%s' \" % lists)\r\n#   for key, value in dictofques.items():\r\n#     if key in qlist:\r\n#       tf.logging.info(\"LOGGING QUESTION LOWER: '%s' \" % qlist)\r\n#       for a in value:\r\n#         for i,j in enumerate(lists):\r\n#           # tf.logging.info(\"LOGGING LISTS LOWER: '%s','%s' \" % (i,j))\r\n#           if a in re.findall(r'\\w+', j[\"text\"].lower()):\r\n#             tf.logging.info(\"LOGGING LISTS LOWER: '%s','%s' \" % (a,lists[i][\"probability\"]))\r\n#             lists[i][\"probability\"]+=0.2\r\n#   # return sorted(lists, key=\"probability\" k: k['name']) \r\n#   lists=sorted(lists, key=itemgetter('probability'), reverse=True)\r\n#   return lists\r\n\r\n\r\n\r\ndef _compute_softmax(scores):\r\n  \"\"\"Compute softmax probability over raw logits.\"\"\"\r\n  if not scores:\r\n    return []\r\n\r\n  max_score = None\r\n  for score in scores:\r\n    if max_score is None or score > max_score:\r\n      max_score = score\r\n\r\n  exp_scores = []\r\n  total_sum = 0.0\r\n  for score in scores:\r\n    x = math.exp(score - max_score)\r\n    exp_scores.append(x)\r\n    total_sum += x\r\n\r\n  probs = []\r\n  for score in exp_scores:\r\n    probs.append(score / total_sum)\r\n  return probs\r\n\r\n\r\nclass FeatureWriter(object):\r\n  \"\"\"Writes InputFeature to TF example file.\"\"\"\r\n\r\n  def __init__(self, filename, is_training):\r\n    self.filename = filename\r\n    self.is_training = is_training\r\n    self.num_features = 0\r\n    self._writer = tf.python_io.TFRecordWriter(filename)\r\n\r\n  def process_feature(self, feature):\r\n    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\r\n    self.num_features += 1\r\n\r\n    def create_int_feature(values):\r\n      feature = tf.train.Feature(\r\n          int64_list=tf.train.Int64List(value=list(values)))\r\n      return feature\r\n\r\n    features = collections.OrderedDict()\r\n    features[\"unique_ids\"] = create_int_feature([feature.unique_id])\r\n    features[\"input_ids\"] = create_int_feature(feature.input_ids)\r\n    features[\"input_mask\"] = create_int_feature(feature.input_mask)\r\n    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\r\n\r\n    if self.is_training:\r\n      features[\"start_positions\"] = create_int_feature([feature.start_position])\r\n      features[\"end_positions\"] = create_int_feature([feature.end_position])\r\n      impossible = 0\r\n      if feature.is_impossible:\r\n        impossible = 1\r\n      features[\"is_impossible\"] = create_int_feature([impossible])\r\n\r\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\r\n    self._writer.write(tf_example.SerializeToString())\r\n\r\n  def close(self):\r\n    self._writer.close()\r\n\r\n\r\ndef validate_flags_or_throw(bert_config):\r\n  \"\"\"Validate the input FLAGS or throw an exception.\"\"\"\r\n  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\r\n                                                FLAGS.init_checkpoint)\r\n\r\n  if not FLAGS.do_train and not FLAGS.do_predict:\r\n    raise ValueError(\"At least one of `do_train` or `do_predict` must be True.\")\r\n\r\n  if FLAGS.do_train:\r\n    if not FLAGS.train_file:\r\n      raise ValueError(\r\n          \"If `do_train` is True, then `train_file` must be specified.\")\r\n  if FLAGS.do_predict:\r\n    if not FLAGS.predict_file:\r\n      raise ValueError(\r\n          \"If `do_predict` is True, then `predict_file` must be specified.\")\r\n\r\n  if FLAGS.max_seq_length > bert_config.max_position_embeddings:\r\n    raise ValueError(\r\n        \"Cannot use sequence length %d because the BERT model \"\r\n        \"was only trained up to sequence length %d\" %\r\n        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\r\n\r\n  if FLAGS.max_seq_length <= FLAGS.max_query_length + 3:\r\n    raise ValueError(\r\n        \"The max_seq_length (%d) must be greater than max_query_length \"\r\n        \"(%d) + 3\" % (FLAGS.max_seq_length, FLAGS.max_query_length))\r\n\r\n\r\ndef main(_):\r\n  tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\r\n\r\n  validate_flags_or_throw(bert_config)\r\n\r\n  tf.gfile.MakeDirs(FLAGS.output_dir)\r\n\r\n  tokenizer = tokenization.FullTokenizer(\r\n      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\r\n\r\n  tpu_cluster_resolver = None\r\n  if FLAGS.use_tpu and FLAGS.tpu_name:\r\n    tf.logging.info(\"%s\",FLAGS.tpu_name)\r\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\r\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\r\n\r\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\r\n  run_config = tf.contrib.tpu.RunConfig(\r\n      cluster=tpu_cluster_resolver,\r\n      master=FLAGS.master,\r\n      model_dir=FLAGS.output_dir,\r\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n      tpu_config=tf.contrib.tpu.TPUConfig(\r\n          iterations_per_loop=FLAGS.iterations_per_loop,\r\n          num_shards=FLAGS.num_tpu_cores,\r\n          per_host_input_for_training=is_per_host))\r\n\r\n  train_examples = None\r\n  num_train_steps = None\r\n  num_warmup_steps = None\r\n  if FLAGS.do_train:\r\n    train_examples = read_squad_examples(\r\n        input_file=FLAGS.train_file, is_training=True) \r\n    num_train_steps = int(\r\n        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\r\n    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\r\n\r\n    # Pre-shuffle the input to avoid having to make a very large shuffle\r\n    # buffer in in the `input_fn`.\r\n    rng = random.Random(12345)\r\n    rng.shuffle(train_examples)\r\n\r\n  model_fn = model_fn_builder(\r\n      bert_config=bert_config,\r\n      init_checkpoint=FLAGS.init_checkpoint,\r\n      learning_rate=FLAGS.learning_rate,\r\n      num_train_steps=num_train_steps,\r\n      num_warmup_steps=num_warmup_steps,\r\n      use_tpu=FLAGS.use_tpu,\r\n      use_one_hot_embeddings=FLAGS.use_tpu)\r\n\r\n  # If TPU is not available, this will fall back to normal Estimator on CPU\r\n  # or GPU.\r\n  estimator = tf.contrib.tpu.TPUEstimator(\r\n      use_tpu=FLAGS.use_tpu,\r\n      model_fn=model_fn,\r\n      config=run_config,\r\n      train_batch_size=FLAGS.train_batch_size,\r\n      predict_batch_size=FLAGS.predict_batch_size)\r\n\r\n  if FLAGS.do_train:\r\n    # We write to a temporary file to avoid storing very large constant tensors\r\n    # in memory.\r\n    train_writer = FeatureWriter(\r\n        filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\r\n        is_training=True)\r\n    convert_examples_to_features(\r\n        examples=train_examples,\r\n        tokenizer=tokenizer,\r\n        max_seq_length=FLAGS.max_seq_length,\r\n        doc_stride=FLAGS.doc_stride,\r\n        max_query_length=FLAGS.max_query_length,\r\n        is_training=True,\r\n        output_fn=train_writer.process_feature)\r\n    train_writer.close()\r\n\r\n    tf.logging.info(\"***** Running training *****\")\r\n    tf.logging.info(\"  Num orig examples = %d\", len(train_examples))\r\n    tf.logging.info(\"  Num split examples = %d\", train_writer.num_features)\r\n    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\r\n    tf.logging.info(\"  Num steps = %d\", num_train_steps)\r\n    del train_examples\r\n\r\n    train_input_fn = input_fn_builder(\r\n        input_file=train_writer.filename,\r\n        seq_length=FLAGS.max_seq_length,\r\n        is_training=True,\r\n        drop_remainder=True)\r\n    print(\"HEEEERRRREE\")\r\n    print(train_input_fn,num_train_steps)\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n\r\n  if FLAGS.do_predict:\r\n    eval_examples = read_squad_examples(\r\n        input_file=FLAGS.predict_file, is_training=False)\r\n\r\n    eval_writer = FeatureWriter(\r\n        filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\r\n        is_training=False)\r\n    eval_features = []\r\n\r\n    def append_feature(feature):\r\n      eval_features.append(feature)\r\n      eval_writer.process_feature(feature)\r\n\r\n    convert_examples_to_features(\r\n        examples=eval_examples,\r\n        tokenizer=tokenizer,\r\n        max_seq_length=FLAGS.max_seq_length,\r\n        doc_stride=FLAGS.doc_stride,\r\n        max_query_length=FLAGS.max_query_length,\r\n        is_training=False,\r\n        output_fn=append_feature)\r\n    eval_writer.close()\r\n\r\n    tf.logging.info(\"***** Running predictions *****\")\r\n    tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\r\n    tf.logging.info(\"  Num split examples = %d\", len(eval_features))\r\n    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\r\n\r\n    all_results = []\r\n\r\n    predict_input_fn = input_fn_builder(\r\n        input_file=eval_writer.filename,\r\n        seq_length=FLAGS.max_seq_length,\r\n        is_training=False,\r\n        drop_remainder=False)\r\n\r\n    # If running eval on the TPU, you will need to specify the number of\r\n    # steps.\r\n    all_results = []\r\n    for result in estimator.predict(\r\n        predict_input_fn, yield_single_examples=True):\r\n      if len(all_results) % 1000 == 0:\r\n        tf.logging.info(\"Processing example: %d\" % (len(all_results)))\r\n      unique_id = int(result[\"unique_ids\"])\r\n      start_logits = [float(x) for x in result[\"start_logits\"].flat]\r\n      end_logits = [float(x) for x in result[\"end_logits\"].flat]\r\n      all_results.append(\r\n          RawResult(\r\n              unique_id=unique_id,\r\n              start_logits=start_logits,\r\n              end_logits=end_logits))\r\n\r\n    output_prediction_file = os.path.join(FLAGS.output_dir, \"predictions.json\")\r\n    output_nbest_file = os.path.join(FLAGS.output_dir, \"nbest_predictions.json\")\r\n    output_null_log_odds_file = os.path.join(FLAGS.output_dir, \"null_odds.json\")\r\n\r\n    write_predictions(eval_examples, eval_features, all_results,\r\n                      FLAGS.n_best_size, FLAGS.max_answer_length,\r\n                      FLAGS.do_lower_case, output_prediction_file,\r\n                      output_nbest_file, output_null_log_odds_file)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  flags.mark_flag_as_required(\"vocab_file\")\r\n  flags.mark_flag_as_required(\"bert_config_file\")\r\n  flags.mark_flag_as_required(\"output_dir\")\r\n  tf.app.run()\r\n\r\n```", "@rakshanda22 I tried reproducing the issue with above code but looks some of module are not found such as modeling, optimization,tokenization. Please help us to reproduce the issue to expedite the trouble-shooting process.Thanks!\r\n", "@gadagashwini  Thanks \r\nI don't need to implement that part now"]}, {"number": 29658, "title": "Error: Linking of rule '@protobuf_archive//:protoc' failed (Exit 1): clang failed: error executing command", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.23.0\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n\r\n\r\nI was trying to do real-time object detection in android using tensorflow. I already trained the model and have the frozen inference graph and other required data. Then I started following this website (https://www.skcript.com/svr/realtime-object-and-face-detection-in-android-using-tensorflow-object-detection-api/) to deploy it to android. Now I am not able to build a .so(c++ compiled) file from my existing tensorflow model. When I run the command, \r\n\r\n```\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \r\n  --crosstool_top=//external:android/crosstool\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n  --cpu=armeabi-v7a\r\n  --cxxopt=-std=c++17\r\n```\r\n\r\nI got this error:\r\n\r\n```\r\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection\\tensorflow>bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=-std=c++17\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/ProgramData/Anaconda3/envs/tensorflow1/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow1\\models\\research\\object_detection\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from c:\\tensorflow1\\models\\research\\object_detection\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/envs/tensorflow1/python.exe --action_env PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/envs/tensorflow1/lib/site-packages --python_path=C:/ProgramData/Anaconda3/envs/tensorflow1/python.exe --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow1\\models\\research\\object_detection\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: Rule 'io_bazel_rules_docker' modified arguments {\"shallow_since\": \"1556410077 -0400\"}\r\nINFO: Build option --cxxopt has changed, discarding analysis cache.\r\nWARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 20. The major revisions supported by Bazel are [10, 11, 12, 13, 14, 15, 16, 17, 18]. Bazel will attempt to treat the NDK as if it was r18. This may cause compilation and linkage problems. Please download a supported NDK version.\r\nWARNING: API level 14 specified by android_ndk_repository 'androidndk' is not available. Using latest known API level 28\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/internal:profiler_interface.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/internal:profiler_interface.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/internal:traceme_recorder.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/internal:traceme_recorder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/lib:BUILD' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/lib:profiler_session.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/lib:profiler_session.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/lib:traceme.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/profiler/lib:traceme.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:attr_builder.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:attr_builder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:context.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:context.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:copy_to_device_node.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:eager_executor.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:eager_executor.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:eager_operation.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:eager_operation.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:execute.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:execute.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:execute_node.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:kernel_and_device.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:kernel_and_device.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:tensor_handle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/common_runtime/eager:tensor_handle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:batch_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions-inl.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:c_api.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:c_api.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:c_api_function.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:c_api_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:checkpoint_reader.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:checkpoint_reader.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:env.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:env.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:kernels.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:kernels.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:ops.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_attrtype.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_datatype.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_datatype.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_status.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_status.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_status_helper.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_status_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_status_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_tensor.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c:tf_tensor.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc:framework/gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc:framework/ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc:framework/scope.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc:framework/scope_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc:ops/while_loop.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc/saved_model:loader.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/distributed_runtime:server_lib.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c/eager:c_api.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c/eager:c_api.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c/eager:c_api_debug.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c/eager:c_api_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/BUILD:1890:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/c/eager:tape.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: C:/tensorflow1/models/research/object_detection/tensorflow/tensorflow/core/kernels/BUILD:6410:12: in srcs attribute of cc_library rule //tensorflow/core/kernels:android_tensorflow_kernels: please do not import '//tensorflow/c/kernels:bitcast_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nINFO: Analysed target //tensorflow/contrib/android:libtensorflow_inference.so (0 packages loaded, 10655 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/devim/_bazel_devim/ktcla6nj/external/protobuf_archive/BUILD:388:1: Linking of rule '@protobuf_archive//:protoc' failed (Exit 1): clang failed: error executing command\r\n  cd C:/users/devim/_bazel_devim/ktcla6nj/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\ProgramData\\Anaconda3\\envs\\tensorflow1;C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\Library\\mingw-w64\\bin;C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\Library\\usr\\bin;C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\Library\\bin;C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\Scripts;C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\bin;C:\\ProgramData\\Anaconda3\\condabin;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\;C:\\Program Files\\MATLAB\\R2018a\\bin;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Tensorflow\\models\\research\\object_detection;C:\\Tensorflow\\object_detection;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\Users\\devim\\.dnx\\bin;C:\\Program Files\\Microsoft DNX\\Dnvm\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Users\\devim\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\devim\\AppData\\Local\\GitHubDesktop\\bin;c:\\msys64\\usr\\bin;C:\\tensorflow1\\models;C:\\tensorflow1\\models\\research;C:\\tensorflow1\\models\\research\\slim\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/envs/tensorflow1/python.exe\r\n    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/envs/tensorflow1/lib/site-packages\r\n    SET TF_CONFIGURE_IOS=0\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -o bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/protoc bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/_objs/protoc/main.o bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/libprotoc_lib.a bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/libprotobuf.a bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/libprotobuf_lite.a bazel-out/armeabi-v7a-opt/bin/external/zlib_archive/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libandroid_support.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++abi.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libunwind.a -target armv7-none-linux-androideabi -Wl,--fix-cortex-a8 -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64 -no-canonical-prefixes -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a -static-libgcc --sysroot=external/androidndk/ndk/platforms/android-28/arch-arm\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:834: error: undefined reference to 'ceilf'\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:834: error: undefined reference to 'ceilf'\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:834: error: undefined reference to 'ceilf'\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:834: error: undefined reference to 'ceilf'\r\nexternal/protobuf_archive/src/google/protobuf/stubs/common.cc:149: error: undefined reference to '__android_log_write'\r\nexternal/protobuf_archive/src/google/protobuf/stubs/common.cc:157: error: undefined reference to '__android_log_write'\r\n/usr/local/google/buildbot/src/android/ndk-release-r20/external/libcxx/../../external/libunwind_llvm/src/AddressSpace.hpp:481: error: undefined reference to 'dladdr'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 143.166s, Critical Path: 28.59s\r\nINFO: 297 processes: 297 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nProtobuf version is 3.6.1\r\n```\r\n\r\nPlease help me. I don't know what is causing this error. Is it because I am doing everything on Windows ??\r\n", "comments": ["Did you try to follow [Object Detection on android](https://www.tensorflow.org/lite/models/object_detection/overview). If not, please have a look and let us know if that helps. Thanks!\r\n", "@achandraa I tried this code already. For this, I needed a detect.tflite file from my tensorflow trained model. I followed [this](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) website to get the .tflite file from frozen_graph.pb using bazel. I was able to solve all the errors. But finally, I got a detect.tflite file with 0 kb.   Can you help me generate a detect.tflite file from my tensorflow model please ?", "Can you file a separate bug for the conversion issue you're facing? Thanks."]}, {"number": 29657, "title": "fix broken link", "body": "fixes #29591", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29657) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29657) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 29656, "title": "Bug on `gather_nd` with gradient.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tf2-gpu-beta\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nA simple test code\r\n```python\r\nv = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)\r\n\r\nwith tf.GradientTape() as tape:\r\n    l = tf.gather_nd(v, [[1, 1]])\r\n    l = tf.reduce_sum(l)\r\n    \r\ngrads = tape.gradient(l, v)\r\nprint(grads)\r\n````\r\ngives following error message\r\n```\r\n---------------------------------------------------------------------------\r\nLookupError                               Traceback (most recent call last)\r\n<ipython-input-12-28efd3aa3042> in <module>\r\n      5     l = tf.reduce_sum(l)\r\n      6 \r\n----> 7 grads = tape.gradient(l, v)\r\n      8 print(grads)\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1000         output_gradients=output_gradients,\r\n   1001         sources_raw=flat_sources_raw,\r\n-> 1002         unconnected_gradients=unconnected_gradients)\r\n   1003 \r\n   1004     if not self._persistent:\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     74       output_gradients,\r\n     75       sources_raw,\r\n---> 76       compat.as_str(unconnected_gradients.value))\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\r\n    131   \"\"\"\r\n    132   mock_op = _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\r\n--> 133   grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\r\n    134   if grad_fn is None:\r\n    135     return [None] * num_inputs\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/registry.py in lookup(self, name)\r\n     95     else:\r\n     96       raise LookupError(\r\n---> 97           \"%s registry has no entry for: %s\" % (self._name, name))\r\n\r\nLookupError: gradient registry has no entry for: ResourceGatherNd\r\n```\r\n**Describe the expected behavior**\r\nthe grads should be `[[0, 0], [0, 1]]` but error occurs. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I just found very weird behavior.\r\nIf I replace v to v+0 like below,\r\n```python\r\nv = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)\r\n\r\nwith tf.GradientTape() as tape:\r\n    l = tf.gather_nd(v+0, [[1, 1]])\r\n    l = tf.reduce_sum(l)\r\n    \r\ngrads = tape.gradient(l, v)\r\nprint(grads)\r\n```\r\nit gives expected result...\r\n```\r\ntf.Tensor(\r\n[[0. 0.]\r\n [0. 1.]], shape=(2, 2), dtype=float32)\r\n```\r\nIt seems the direct access to variables with `gather_nd` ruines something unexpected...", "@Sangwon91 I could able to reproduce the reported issue with Tensorflow 2.0.0.beta0. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29656\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29656\">No</a>\n"]}, {"number": 29655, "title": "formatted docstring", "body": "tf.Print was not rendered correctly in the website. So formatted docstring.", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` "]}, {"number": 29654, "title": "[TF 2.0 API Docs] tf.lite.OpHint.OpHintArgumentTracker", "body": "URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lite/OpHint/OpHintArgumentTracker\r\n\r\nDescription of issue (what needs changing):\r\nThe link to the TF 2.0 API Documentation is broken, it does not link to the documentation of the API. So, there is no documentation of TF 2.0 for this function.", "comments": ["@Denis-kisina Where did you find this link? We need to correct that page to route the target to correct location. Thanks!", "@jvishnuvardhan this is the documents link. https://docs.google.com/spreadsheets/d/1p3vqbocbKmcZQGrlxk9m2jHuleu8yr-XRbfKum0IDD8/edit#gid=0\r\n", "@Denis-kisina Currently there is no documentation for that op. [Here](https://www.tensorflow.org/api_docs/python/tf/lite/OpHint/OpHintArgumentTracker) is the link to TF1.13 version of that op. Thanks!", "@Denis-kisina,\r\nThe documentation of that API for 2.x version can be found here\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/OpHint/OpHintArgumentTracker\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29653, "title": "[TF 2.0 API Docs] tf.lite.OpHint", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lite/OpHint\r\n\r\n## Description of issue (what needs changing):\r\nThe link to the TF 2.0 API Documentation  is broken, it does not link to the documentation of the API. So, there is no documentation of TF 2.0 for this function.\r\n", "comments": ["@Denis-kisina In Tflite 2.0 version lite.OpHint has removed. Please have a look on [release](https://github.com/tensorflow/tensorflow/releases#TensorFlow%20Lite) note. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29652, "title": "[TF 2.0 API Docs] tf.data.experimental.Counter", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/Counter\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe description is not clear, not provided as in the DOCS\r\n\r\n### Raises listed and defined\r\nErrors are not defined at all\r\n\r\n### Request visuals, if applicable\r\nNo visuals are included\r\n\r\n### Submit a pull request?\r\nYes\r\n", "comments": ["Hi @TrevorNathan, late response here but can you take a look a[ look at the 2.4 API docs for Counter](https://www.tensorflow.org/api_docs/python/tf/data/experimental/Counter) and let me know if the description is now clear? ", "@TrevorNathan I think this was resolved in recently updated page https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/data/experimental/Counter .\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 29651, "title": "[TF 2.0 API Docs] tf.io.write_file", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/write_file\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe description does not recommend at all in any way when and when not to use this symbol.\r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\nThe returned objects are not well defined and therefore not correct, complete and appropriately formatted\r\n\r\n### Usage example\r\n\r\nYes\r\n\r\n### Request visuals, if applicable\r\n\r\nNo single visual included in the symbol\u2019s description. In some instances it will definitely clarify the content being presented.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@muwongeemmanuel \r\nPlease note the requested page has been updated, please refer to [this](https://www.tensorflow.org/api_docs) page and let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29650, "title": "[TF 2.0 API Docs] tf.data.experimental", "body": "## Existing URLs containing the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental\r\n\r\n## Description of the issue:\r\nCorrect Links: All links are correct and well defined\r\n\r\n## Clear Description\r\n\r\nThe description is not clear about when to use this symbol \r\n\r\n## Usage Example\r\nNo usage example is provided.\r\n\r\n## Parameters Defined\r\nParameters are poorly defined, and not formatted appropriately.\r\n\r\n## Returns Defined\r\nReturns are not defined.\r\n\r\n## Raises Listed and Defined\r\nErrors are not defined.\r\n\r\n## Visuals, if Applicable\r\nNo visuals are included.", "comments": ["@kabahima This `Experimental API` page lists overview of the methods available in this API.  so, it is not required to have any usage examples or returns defined for this API. \r\n\r\nFor example, `Counter` method under `Experimental API` clearly shows examples and arguments are shown.\r\n\r\nhttps://www.tensorflow.org/versions/r2.5/api_docs/python/tf/data/experimental/Counter\r\n\r\nIf you see anything is missing for those methods under `Experimental API`, then please raise a new issue. \r\n\r\nI am closing this issue. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 29649, "title": "[TF 2.0 API Docs] tf.io.FixedLenSequenceFeature", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/FixedLenSequenceFeature\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe description doesn\u2019t give recommendation on when to use and not use the symbol\r\n\r\n### Parameters defined\r\nNo parameters defined.\r\n\r\n### Returns defined\r\nNo returns defined\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nNo errors defined \r\n\r\n### Usage example\r\nNo usage example \r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\nYes", "comments": ["Please note the requested page has been updated, please refer to [this](https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/io/FixedLenSequenceFeature) page and let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29648, "title": "[TF 2.0 API Docs]  tf.io.read_file", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/read_file\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe description does not give recommendations of when and when not to use this symbol.\r\n\r\n### Correct links\r\nThe link to the source code file (python/ops/gen_io_ops.py) is dormant.\r\n\r\n### Raises listed and defined\r\nErrors are not defined.\r\n\r\n### Usage example\r\nNo usage example provided.\r\n\r\n### Submit a pull request?\r\nYes", "comments": ["@SimiCode,\r\nDetailed description, along with usage examples are present in the Nightly Version. Please refer the [Nightly Version of Documentation of tf.io.read_file](https://www.tensorflow.org/api_docs/python/tf/io/read_file?version=nightly). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29647, "title": "[TF 2.0 API Docs] tf.io.write_file", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@muwongeemmanuel Closing since it is a duplicate of [#29651](https://github.com/tensorflow/tensorflow/issues/29651). Feel free to reopen if the solution provided on that thread doesn't work Distributed for you. Thanks!"]}, {"number": 29646, "title": "[TF 2.0 API Docs] tf.io.FixedLenFeature", "body": "## Existing Url with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/FixedLenFeature\r\n\r\n## Description of issue (what needs changing):\r\nClear description with parameters, returns and usage example\r\n### Clear description\r\n\r\n### Correct links\r\nThe link to the python script where the function is define is active\u2028https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/FixedLenFeature\r\n\r\n### Parameters defined\r\nNo parameters defined\r\n\r\n### Returns defined\r\nNo returns defined\r\n\r\n### Raises listed and defined\r\nThe raises is defined further in this link\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/parsing_ops.py\r\n\r\n###Are the errors defined? For example,\r\nErrors are not defined\r\n\r\n### Usage example\r\nUsage example provided in this link https://www.tensorflow.org/beta/tutorials/load_data/tf_records", "comments": ["Created [a CL](https://critique-ng.corp.google.com/cl/373339875) to fix this issue. Thanks!", "@Emanuz,\r\n\r\nCan you take a look at this [link](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature) of the doc from latest version, which has the attribute values updated properly? Also it has a section of example notebooks on which `FixedLenFeature` is used. Let me know if its clear now. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29645, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The following PR broke the `--config=rocm` build when it was merged\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/27756\r\n\r\nThe above PR changed the API in the stream-executor dnn layer, made the corresponding updates in the CUDA implementation, but not in the ROCm implementation. This PR adds a bare-bones ROCm part to get the build working again.\r\n\r\n------------------------------------\r\n\r\n@tatianashp @whchung \r\n", "comments": ["/cc @thirupalanisamy "]}, {"number": 29644, "title": "Default build options creating larger(~3x) binary in linux ", "body": "**System information**\r\n- OS Platform and Distribution : ubuntu 16.04.10\r\n- TensorFlow installed from (source or binary): Not Installing. Building Binary\r\n- TensorFlow version: 1.13.1 and 1.14-rc0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): 4.9\r\n- CUDA/cuDNN version: NA. Not building with CUDA\r\n- GPU model and memory: NA\r\n\r\n**Describe the problem**\r\n\r\nThe size of TF binary we are building has increased from 54.3 MB(in TF 1.8) to 148.1 MB (int TF 1.14-rc0 and 1.13.1). \r\n\r\nWe have used all default settings in configure.py in 1.8. Building 1.14-rc0 with defaults is anyways giving a larger binary. Additionally we tried to disable many optional dependencies (like aws,kafka etc), but still the size was around 148 MB. \r\n\r\nIs there any new optional dependency added by default after 1.8 ? \r\n\r\n**Steps Followed**\r\n\r\nWe are building with default settings (except we say NO to XLA JIT support in 1.14.1-rc0 and 1.13.1). We are using following command to build : \r\n\r\n1) bazel build --config=opt //tensorflow:tensorflow\r\n2) bazel build --config=opt --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=nonccl  //tensorflow:tensorflow\r\n\r\n\r\n", "comments": ["@yifeif ", "Doesn't seem related to dynamic loading of cuda libraries, but I can take a closer look.", "> Doesn't seem related to dynamic loading of cuda libraries, but I can take a closer look.\r\n\r\nThanks for looking into this! I will be really helpful if you provide us a solution. \r\n\r\nI thought I will provide some information in the context of CUDA. During configuration step, we get a specific option to enable/disable CUDA support. And we select \"NO\" for CUDA support. \r\n\r\nIs there any new optional dependency added by default after 1.8 ? I ask this question because TF 1.8 binaries had lesser size (as mentioned in the issue information above)", "Looks like the majority of the size increase came in v 14.\r\nBut there were two 10 MB increases before.\r\n\r\nI cannot promnise a fix to this, but the best I can do is to maybe find an explanation for this increase.", "From what I can see, there are 30 MBs of additional python files due to \"tf.compat\" module, 15 MB size increase in libtensorflow_framework, which may be explained by starting to use MKL-DNN for contraction kernels, 8 MB increase in XLA, another 9 MB in contrib, and a few megabytes of new headers. I think due to the preparation to 2.0, we did a lot of things, which accumulated.\r\n\r\nI do not think we can make it as small as 1.8, unfortunately. Closing this issue as working as intended.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29644\">No</a>\n"]}, {"number": 29643, "title": "SetShapeFn gets nullptr for custom operator leading to crash", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0rc1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): gcc 7.4.0\r\n- CUDA/cuDNN version: NR\r\n- GPU model and memory: NR\r\n\r\n**Describe the current behavior**\r\n`SetShapeFn` doesn't work when called for `REGISTER_OP` for the custom operators. Whenever it is called by TF is gets `nullptr` instead valid value of the `InferenceContext` argument.\r\n\r\n**Describe the expected behavior**\r\n`SetShapeFn` should get meaningful ptr.\r\n\r\n**Code to reproduce the issue**\r\nIt doesn't work with https://www.tensorflow.org/guide/extend/op. [ Repro attached](https://github.com/tensorflow/tensorflow/files/3276625/tf_crash_repro.zip), just call `run.sh`.\r\n\r\n**Other info / logs**\r\nNA", "comments": ["@lamberta Can you find someone to verify this and find an owner?", "@yifeif has been working on the https://github.com/tensorflow/custom-op repo so maybe can take a look.\r\n\r\nTalking with @MarkDaoust and maybe https://www.tensorflow.org/guide/extend/op should just move over to that repo.", "It doesn't work with https://github.com/tensorflow/custom-op after calling `make zero_out_test`. But it is just a repro, the issue was reported because our custom operator doesn't work as well.", "@JanuszL , I guess the issue in the example is explicit addition of CXX abi to the compiler flags. You should not add this since TF already gives that flag and may conflict with what you specify. When I removed the explicit -D_GLIBCXX_USE_CXX11_ABI=0 from your example, it worked fine for me. Could you please check whether in your custom op you are not overriding it?", "@samikama - removing `-D_GLIBCXX_USE_CXX11_ABI=0` doesn't work either. Issuing:\r\n```\r\npython -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))'\r\n```\r\ngets me\r\n```\r\n-I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n```\r\nAnd  https://github.com/tensorflow/custom-op  doesn't work as well, while there is no ` -D_GLIBCXX_USE_CXX11_ABI` flag passed explicitly.", "@JanuszL you are right. Unless you use gcc 4.8 it bombs. @tfboyd @gunan This looks like an ABI issue. Was there a change in the way official pip package built for 1.14? I remember centos6 discussion but not sure if it made into 1.14. If it did, there seems to be a serious issue here.", "@yifeif for custom op instructions.\r\n@samikama our pip packages are still built with gcc 4.8.\r\nUnfortuanately, this is out of our control. To make anything that uses C++ APIs work, all custom ops need to be built with the same compiler (and the version) we use to build the pip packages. custom-op repository has a docker container with our toolchains deployed. Anything not built with that may break due to compilers generating ABIs differently.\r\n\r\n@yifeif should be able to help more on this.", "> custom-op repository has a docker container with our toolchains deployed. Anything not built with that may break due to compilers generating ABIs differently.\r\n\r\nThe custom-op repository references two docker images. The manylinux2010 image `tensorflow/tensorflow:custom-op-ubuntu16` has gcc 5.4 and indeed gives `nullptr` for custom op shape functions if you use tf 1.x (it works fine with tf 2.x). The image `tensorflow/tensorflow:custom-op-ubuntu14` has gcc 4.8 and works fine with custom ops and tf1.x.", "when to unify toolchains of  the pip build docker and released docker images ?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29643\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29643\">No</a>\n"]}, {"number": 29642, "title": "Customized loss can't get the shape of input data (BUG?)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta\r\n- Python version:\r\n- Bazel version (if compiling from source): 3.7.3\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.0 with cudnn 7.6\r\n- GPU model and memory: 12GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI write a custom loss function (dice coefficient) with following code:\r\n``` python\r\ndef dice_coef(y_true, y_pred, smooth=1e-7):\r\n    _, W, H, D, C = y_pred.get_shape()\r\n\r\n    y_pred = tf.reshape(y_pred, shape=[-1, W * H * D * C])\r\n    y_true = tf.reshape(y_true, shape=[-1, W * H * D * C])\r\n\r\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\r\n    y_true = tf.cast(y_true, dtype=tf.float32)\r\n\r\n    dice_numerator = 2.0 * tf.reduce_sum(y_true * y_pred, axis=1) + smooth\r\n    dice_denominator = tf.reduce_sum(y_pred + y_true, axis=1) + smooth\r\n\r\n    dice_coefficient = dice_numerator / dice_denominator\r\n    dice_coefficient = tf.reduce_mean(dice_coefficient)\r\n    return dice_coefficient\r\n```\r\nwhen i run code like this:\r\n``` python\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\n    with mirrored_strategy.scope():\r\n        model = unet3d(input_shape=input_shape, num_class=num_class, activation='sigmoid')\r\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\r\n                      loss=bce_dice,\r\n                      metrics=[\r\n                          tf.keras.metrics.BinaryAccuracy(),\r\n                          tf.keras.metrics.BinaryCrossentropy(),\r\n                      ])\r\n```\r\ni got the following error \r\n``` python\r\nTraceback (most recent call last):\r\n  File \"/tmp/pycharm_project_317/opt/train.py\", line 84, in <module>\r\n    epochs=50)\r\n  File \"/tmp/pycharm_project_317/opt/train.py\", line 21, in train\r\n    callbacks=callbacks)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 681, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 294, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py\", line 813, in execution_function\r\n    return [out.numpy() for out in distributed_function(input_fn)]\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 416, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 359, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1360, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1648, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1541, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 716, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 309, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 706, in wrapper\r\n    raise e.ag_error_metadata.to_exception(type(e))\r\nTypeError: in converted code:\r\n\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py:804 distributed_function  *\r\n        outputs = strategy.experimental_run_v2(\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:708 experimental_run_v2\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1710 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:708 _call_for_each_replica\r\n        fn, args, kwargs)\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:195 _call_for_each_replica\r\n        coord.join(threads)\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py:389 join\r\n        six.reraise(*self._exc_info_to_raise)\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/six.py:693 reraise\r\n        raise value\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\r\n        yield\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:926 run\r\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:908 train_on_batch\r\n        output_loss_metrics=self._output_loss_metrics)\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_eager.py:307 train_on_batch\r\n        output_loss_metrics=output_loss_metrics))\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_eager.py:237 _process_single_batch\r\n        training=training))\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_eager.py:152 _model_loss\r\n        per_sample_losses = loss_fn.call(targets[i], outs[i])\r\n    /home/tanjl/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:215 call\r\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    /tmp/pycharm_project_317/app/losses/binary.py:26 bce_dice\r\n        return -dice_coef(y_true, y_pred, smooth) + \\\r\n    /tmp/pycharm_project_317/app/losses/binary.py:7 dice_coef\r\n        y_pred = tf.reshape(y_pred, shape=[-1, W * H * D * C])\r\n\r\n    TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\r\n\r\nbash: \u884c 1:  7597 \u6bb5\u9519\u8bef               (\u6838\u5fc3\u5df2\u8f6c\u50a8) env \"PYCHARM_HOSTED\"=\"1\" \"PYTHONUNBUFFERED\"=\"1\" \"PYTHONIOENCODING\"=\"UTF-8\" \"PYCHARM_MATPLOTLIB_PORT\"=\"52949\" \"JETBRAINS_REMOTE_RUN\"=\"1\" \"PYTHONPATH\"=\"/home/tanjl/.pycharm_helpers/pycharm_matplotlib_backend:/tmp/pycharm_project_317\" /home/tanjl/anaconda3/envs/tensorflow2/bin/python3.7 -u /tmp/pycharm_project_317/opt/train.py\r\n```\r\n**Describe the expected behavior**\r\nit have no error when i run code with tf-2.0.0-alpha but the error happend with beta version tensorflow \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nit can't get the input shape in loss function, is that right? and i wish u could help me, thanks\r\n", "comments": ["Please provide us complete code to reproduce the issue.Thanks.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I'm having the same error. The [TF example VAE code](url) doesn't run with distribute strategy (see code below). I'm on tf2.0.0.0-beta1 (tried rc0 but it has too many issues at this stage to upgrade, I have to stick with beta). \r\n\r\nError: \r\n> per_sample_losses = loss_fn.call(targets[i], outs[i]) \r\n> IndexError: list index out of range\r\n\r\n(See full error stack in the bottom)\r\n\r\nAny workaround to this?\r\n\r\n```\r\n\r\nimport numpy as np\r\nimport os\r\nimport tensorflow_probability as tfp\r\ntfd = tfp.distributions\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nload_model = 1\r\n\r\nlatent_dim = 8\r\nlearning_rate = 1e-4\r\n\r\nBATCH_SIZE = 100\r\nTEST_BATCH_SIZE = 10\r\n\r\ncolor_channels = 1\r\n(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_images = train_images[:5000,::]\r\ntest_images = test_images[:1000,::]\r\nn_trainsamples = np.shape(train_images)[0]\r\nn_testsamples = np.shape(test_images)[0]\r\nimsize = np.shape(train_images)[1]\r\nnp.random.shuffle(train_images)\r\ntrain_images = train_images.reshape(-1, imsize, imsize, 1).astype('float32')\r\ntest_images = test_images.reshape(-1, imsize, imsize, 1).astype('float32')\r\ntrain_images /= 255.\r\ntest_images /= 255.\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(n_trainsamples).batch(BATCH_SIZE)\r\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images)).shuffle(n_testsamples).batch(TEST_BATCH_SIZE)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\n\r\nwith strategy.scope():\r\n\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n\r\n    class Sampling(tf.keras.layers.Layer):\r\n        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding an image.\"\"\"\r\n        def call(self, inputs):\r\n            z_mean, z_log_var = inputs\r\n            batch = tf.shape(z_mean)[0]\r\n            dim = tf.shape(z_mean)[1]\r\n            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n\r\n\r\n    original_dim = 784\r\n    intermediate_dim = 64\r\n    latent_dim = 32\r\n\r\n    # Define encoder model.\r\n    original_inputs = tf.keras.Input(shape=(imsize, imsize, color_channels), name='encoder_input')\r\n    x = layers.Dense(intermediate_dim, activation='relu')(original_inputs)\r\n    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\r\n    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\r\n    z = Sampling()((z_mean, z_log_var))\r\n    encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\r\n\r\n    # Define decoder model.\r\n    latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\r\n    x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\r\n    outputs = layers.Dense(original_dim, activation='sigmoid')(x)\r\n    outputs = tf.keras.layers.Reshape(target_shape=(imsize, imsize, color_channels))(x)\r\n    decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\r\n\r\n    # Define VAE model.\r\n    outputs = decoder(z)\r\n    vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')\r\n\r\n    # Add KL divergence regularization loss.\r\n    kl_loss = - 0.5 * tf.reduce_mean(\r\n        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\r\n    vae.add_loss(kl_loss)\r\n\r\n    # Train.\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n    vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\n    vae.fit(train_dataset, epochs=n_trainsamples//BATCH_SIZE)\r\n\r\n```\r\n\r\n\r\nLogs:\r\n\r\n> 2019-09-01 05:47:53.856027: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n> 2019-09-01 05:47:53.860170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\n> name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n> pciBusID: 7a50:00:00.0\r\n> 2019-09-01 05:47:53.860377: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n> 2019-09-01 05:47:53.861509: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n> 2019-09-01 05:47:53.862528: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n> 2019-09-01 05:47:53.862808: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n> 2019-09-01 05:47:53.864211: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n> 2019-09-01 05:47:53.865345: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n> 2019-09-01 05:47:53.868965: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-09-01 05:47:53.869575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n> 2019-09-01 05:47:53.869944: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2019-09-01 05:47:53.948861: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52c0430 executing computations on platform CUDA. Devices:\r\n> 2019-09-01 05:47:53.948905: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\r\n> 2019-09-01 05:47:53.951820: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\r\n> 2019-09-01 05:47:53.952412: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5330710 executing computations on platform Host. Devices:\r\n> 2019-09-01 05:47:53.952439: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n> 2019-09-01 05:47:53.952837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\n> name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n> pciBusID: 7a50:00:00.0\r\n> 2019-09-01 05:47:53.952893: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n> 2019-09-01 05:47:53.952914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n> 2019-09-01 05:47:53.952931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n> 2019-09-01 05:47:53.952948: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n> 2019-09-01 05:47:53.952965: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n> 2019-09-01 05:47:53.952981: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n> 2019-09-01 05:47:53.952999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-09-01 05:47:53.953570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n> 2019-09-01 05:47:53.953614: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n> 2019-09-01 05:47:53.955775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-09-01 05:47:53.955803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n> 2019-09-01 05:47:53.955816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n> 2019-09-01 05:47:53.956462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 224 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 7a50:00:00.0, compute capability: 3.7)\r\n> 2019-09-01 05:47:53.970613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\n> name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n> pciBusID: 7a50:00:00.0\r\n> 2019-09-01 05:47:53.970682: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n> 2019-09-01 05:47:53.970703: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n> 2019-09-01 05:47:53.970720: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n> 2019-09-01 05:47:53.970736: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n> 2019-09-01 05:47:53.970751: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n> 2019-09-01 05:47:53.970767: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n> 2019-09-01 05:47:53.970783: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-09-01 05:47:53.971338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n> 2019-09-01 05:47:53.971377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-09-01 05:47:53.971391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n> 2019-09-01 05:47:53.971401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n> 2019-09-01 05:47:53.971990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 224 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 7a50:00:00.0, compute capability: 3.7)\r\n> Train on 50 steps\r\n> Epoch 1/50\r\n> Traceback (most recent call last):\r\n>   File \"/home/kristofgiber/pycharm_project/VAE/save_issue_reproduction.py\", line 82, in <module>\r\n>     vae.fit(train_dataset, epochs=n_trainsamples//BATCH_SIZE)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 681, in fit\r\n>     steps_name='steps_per_epoch')\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 294, in model_iteration\r\n>     batch_outs = f(actual_inputs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py\", line 813, in execution_function\r\n>     return [out.numpy() for out in distributed_function(input_fn)]\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/def_function.py\", line 416, in __call__\r\n>     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/def_function.py\", line 359, in _initialize\r\n>     *args, **kwds))\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py\", line 1360, in _get_concrete_function_internal_garbage_collected\r\n>     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py\", line 1648, in _maybe_define_function\r\n>     graph_function = self._create_graph_function(args, kwargs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py\", line 1541, in _create_graph_function\r\n>     capture_by_value=self._capture_by_value),\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py\", line 716, in func_graph_from_py_func\r\n>     func_outputs = python_func(*func_args, **func_kwargs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/def_function.py\", line 309, in wrapped_fn\r\n>     return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py\", line 706, in wrapper\r\n>     raise e.ag_error_metadata.to_exception(type(e))\r\n> tensorflow.python.autograph.impl.api.StagingError: in converted code:\r\n>     relative to /usr:\r\n> \r\n>     local/lib/python3.5/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py:804 distributed_function  *\r\n>         outputs = strategy.experimental_run_v2(\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/distribute/distribute_lib.py:708 experimental_run_v2\r\n>         return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/distribute/distribute_lib.py:1710 call_for_each_replica\r\n>         return self._call_for_each_replica(fn, args, kwargs)\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:708 _call_for_each_replica\r\n>         fn, args, kwargs)\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:195 _call_for_each_replica\r\n>         coord.join(threads)\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py:389 join\r\n>         six.reraise(*self._exc_info_to_raise)\r\n>     lib/python3/dist-packages/six.py:686 reraise\r\n>         raise value\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\r\n>         yield\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:926 run\r\n>         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py:908 train_on_batch\r\n>         output_loss_metrics=self._output_loss_metrics)\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_eager.py:307 train_on_batch\r\n>         output_loss_metrics=output_loss_metrics))\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_eager.py:237 _process_single_batch\r\n>         training=training))\r\n>     local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_eager.py:152 _model_loss\r\n>         per_sample_losses = loss_fn.call(targets[i], outs[i])\r\n> \r\n>     IndexError: list index out of range\r\n> \r\n> \r\n> Process finished with exit code 1\r\n> \r\n\r\n", "I have same error"]}, {"number": 29641, "title": "minor change in output message in tensorflow/python/client/session.py", "body": "I got the following error and it doesn't look very nice in my opinion.\r\n\r\n![tensorflow_docs_error](https://user-images.githubusercontent.com/24234402/59270095-95c8a780-8c48-11e9-989b-7ca8ef3470f3.png)\r\n\r\nAlso, the message in the parentheses would be better if it said \"Cannot convert an int ...\"  but I can't find how to change it.\r\n", "comments": []}, {"number": 29639, "title": "quantized  tflite model result is different with CPU and NNAPI on my Android P devices", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Reno710\r\n- TensorFlow installed from (source or binary):conda  install\r\n- TensorFlow version (use command below):tf1.13.1\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ncustom quantized model run on android P with CPU get the right result.when I set use NNAPI  it get wrong result. So I had done some research :when I use tensorflow-lite-1.13.1.aar ,the two model get the same result.because my model have to use Op resizebiliear,but NNAPI before dose not include the Op ,I change it to tensorflow-nightly-0.0.0.aar,so I got the wrong result.Besides,I have build a aar with tensorflow master code,the result is still wrong.I have check the first Conv,and print the result.I found the result is still different.I don't know why.\r\n**Describe the expected behavior**\r\ngot the same result with quantized model between CPU model  and NNAPI\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n![CLz9KG2i](https://user-images.githubusercontent.com/30410113/59268453-46519e00-8c7f-11e9-9b36-adc9b0173baa.png)\r\nhere is my test model,just to verify the  output data\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29639\">No</a>\n"]}, {"number": 29638, "title": "Make the logic consistent with shape_inference.cc", "body": "", "comments": []}, {"number": 29637, "title": "Make the logic consistent with shape_inference.cc", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29637) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 29636, "title": "Can't call tf.contrib.layers.group_norm() due to supplying a DeferredTensor instead of (Eager)Tensor during eager execution.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: V8.0.61\r\n- GPU model and memory: Tesla P100-PCIE, 12193MiB\r\n\r\n**Describe the current behavior**\r\nI run the following snippet of code in eager execution:\r\n\r\n```\r\ntensor = tf.keras.layers.Conv2D(128, (3, 3), padding='same')(tensor)\r\ntensor = tf.contrib.layers.group_norm(tensor)\r\n```\r\n\r\nHowever, the call to `tf.contrib.layers.group_norm(tensor)` gives me the following error:\r\n\r\n```\r\nValueError: Attempt to convert a value (<DeferredTensor 'None' shape=(?, 16, 16, 128) dtype=float32>) with an unsupported type (<class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'>) to a Tensor.\r\n```\r\n\r\n\r\nAlso, I am not able to find any documentation on `DeferredTensor`s whatsoever. Can anyone link me to some?\r\n\r\n**Describe the expected behavior**\r\nA possible conversion from the `DeferredTensor` to a `Tensor` or `EagerTensor`, or alternatively to perform group normalization in another way.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport os\r\nimport cv2\r\nimport json\r\nfrom sklearn.metrics import confusion_matrix\r\n\r\nORIGINAL_WIDTH = 4608\r\nORIGINAL_HEIGHT = 4608\r\nSCALED_WIDTH = 512\r\nSCALED_HEIGHT = 512\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\r\n    filters1, filters2, filters3 = filters\r\n    conv_name_base = 'res' + str(stage) + block + '_branch'\r\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n\r\n    x = tf.keras.layers.Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\r\n    x = tf.keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2c')(x)\r\n\r\n    x = tf.keras.layers.add([x, input_tensor])\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n    return x\r\n\r\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2,2)):\r\n\r\n    filters1, filters2, filters3 = filters\r\n    conv_name_base = 'res' + str(stage) + block + '_branch'\r\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n    x = tf.keras.layers.Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + '2a')(input_tensor)\r\n    x = tf.keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2c')(x)\r\n\r\n    shortcut = tf.keras.layers.Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + '1')(input_tensor)\r\n    shortcut = tf.keras.layers.BatchNormalization(axis=3, name=bn_name_base + '1')(shortcut)\r\n\r\n    x = tf.keras.layers.add([x, shortcut])\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n    return x\r\n\r\ndef get_fpn(num_channels=3):\r\n\r\n    input = tf.keras.Input(shape=(SCALED_HEIGHT, SCALED_WIDTH, num_channels))\r\n    x = tf.keras.layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(input)\r\n    x = tf.keras.layers.BatchNormalization(axis=3, name='bn_conv1')(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n    x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\r\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\r\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\r\n    bottomup_xl = identity_block(x, 3, [64, 64, 256], stage=2, block='c') # (?, 127, 127, 256)\r\n\r\n    x = conv_block(bottomup_xl, 3, [128, 128, 512], stage=3, block='a')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\r\n    bottomup_l = identity_block(x, 3, [128, 128, 512], stage=3, block='d') # (?, 64, 64, 512)\r\n\r\n    x = conv_block(bottomup_l, 3, [256, 256, 1024], stage=4, block='a')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\r\n    bottomup_m = identity_block(x, 3, [256, 256, 1024], stage=4, block='f') # (?, 32, 32, 1024)\r\n\r\n    x = conv_block(bottomup_m, 3, [512, 512, 2048], stage=5, block='a')\r\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\r\n    bottomup_s = identity_block(x, 3, [512, 512, 2048], stage=5, block='c') # (?, 16, 16, 2048)\r\n\r\n    topdown_s = tf.keras.layers.Conv2D(1024, (1, 1), padding='same')(bottomup_s)\r\n    x = tf.keras.layers.UpSampling2D(size=(2,2))(topdown_s)\r\n    #x = tf.image.resize_images(topdown_s, (topdown_s.shape[1] * 2, topdown_s.shape[2] * 2), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\r\n    #x = tf.convert_to_tensor(cv2.resize(np.array(topdown_s), dsize=None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST))\r\n    topdown_m = tf.keras.layers.add([x, tf.keras.layers.Conv2D(1024, (1, 1), padding='same')(bottomup_m)])\r\n\r\n    x = tf.keras.layers.UpSampling2D(size=(2, 2))(topdown_m)\r\n    topdown_l = tf.keras.layers.add([x, tf.keras.layers.Conv2D(1024, (1, 1), padding='same')(bottomup_l)])\r\n    x = tf.keras.layers.UpSampling2D(size=(2, 2))(topdown_l)\r\n    topdown_xl = tf.keras.layers.add([x, tf.keras.layers.Conv2D(1024, (1, 1), padding='same')(bottomup_xl)])\r\n\r\n    pyramid_s = tf.keras.layers.Conv2D(256, (3, 3), padding='same')(topdown_s)\r\n    pyramid_m = tf.keras.layers.Conv2D(256, (3, 3), padding='same')(topdown_m)\r\n    pyramid_l = tf.keras.layers.Conv2D(256, (3, 3), padding='same')(topdown_l)\r\n    pyramid_xl = tf.keras.layers.Conv2D(256, (3, 3), padding='same')(topdown_xl)\r\n\r\n    # extra_channels = num_channels - 3\r\n    # paddings = tf.constant([[0, 0, ], [0, 0], [0, extra_channels], [0, 0]])\r\n    # weights[0] = tf.pad(weights[0], paddings, mode='REFLECT')\r\n    # model.set_weights(weights[:36])\r\n    return tf.keras.Model(input, [pyramid_s, pyramid_m, pyramid_l, pyramid_xl])\r\n\r\ndef get_semantic_fpn(num_channels=3):\r\n    def upsample(tensor, repetitions):\r\n        print(tf.executing_eagerly())\r\n        if repetitions == 0:\r\n            tensor = tf.keras.layers.Conv2D(128, (3, 3), padding='same')(tensor)\r\n            tensor = tf.contrib.layers.group_norm(tensor) # Problematic line\r\n            return tf.keras.layers.ReLU()(tensor)\r\n        for _ in range(repetitions):\r\n            tensor = tf.keras.layers.Conv2D(128, (3, 3), padding='same')(tensor)\r\n            tensor = tf.contrib.layers.group_norm(tensor) # Problematic line\r\n            tensor = tf.keras.layers.ReLU()(tensor)\r\n            tensor = tf.image.resize_images(tensor, (tensor.shape[1] * 2, tensor.shape[2] * 2))\r\n        return tensor\r\n\r\n    input = tf.keras.Input(shape=(SCALED_HEIGHT, SCALED_WIDTH, num_channels))\r\n    fpn = get_fpn(num_channels)\r\n    pyramid_s, pyramid_m, pyramid_l, pyramid_xl = fpn(input)\r\n    from_s = upsample(pyramid_s, 3)\r\n    from_m = upsample(pyramid_m, 2)\r\n    from_l = upsample(pyramid_l, 1)\r\n    from_xl = upsample(pyramid_xl, 0)\r\n\r\n    x = tf.keras.layers.add([from_s, from_m, from_l, from_xl])\r\n    x = tf.keras.layers.Conv2D(NUM_LABELS, (1, 1), padding='same')(x)\r\n    output = tf.image.resize_images(x, (x.shape[1] * 4, x.shape[2] * 4))\r\n    return tf.keras.Model(input, output)\r\n\r\nmodel = get_semantic_fpn()\r\n```", "comments": ["me too. It looks like that this op can run on CPU but not GPU for the recent version no matter in eager mode or not.", "@MaeThird What version are you running?", "@EmielBoss Please help us to reproduce the issue, we see some undefined entities like conv_block, identity_block. Thanks! ", "@EmielBoss tf-1.13.1 installed from binary with python-3.6", "@gadagashwini Apologies, I forgot about those and they've been added. I didn't think it would matter much, since I would assume `tf.keras.layers.Conv2D()`'s output would always be of the same type, but that's probably incorrect.", "```\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\n\r\ndef test():\r\n    x = tf.placeholder(dtype=tf.float32,shape=[None,112,112,3])\r\n    net = slim.conv2d(x,32,[3,3])\r\n    out = tf.contrib.layers.group_norm(net,groups=4)\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n\r\n    import numpy as np\r\n    inputs = np.random.rand(2,112,112,3)\r\n\r\n    print(sess.run(out,feed_dict={x:inputs}))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    test()\r\n```\r\nTry out this snippets with log out like this **TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 112, 112, 4, 8]. Consider casting elements to a supported type**.", "@EmielBoss I tried reproducing issue with above code but i got this NameError: name 'SCALED_HEIGHT' is not defined. Please help us to reproduce the issue. Thanks! ", "> ```\r\n> import tensorflow as tf\r\n> slim = tf.contrib.slim\r\n> \r\n> def test():\r\n>     x = tf.placeholder(dtype=tf.float32,shape=[None,112,112,3])\r\n>     net = slim.conv2d(x,32,[3,3])\r\n>     out = tf.contrib.layers.group_norm(net,groups=4)\r\n> \r\n>     sess = tf.Session()\r\n>     sess.run(tf.global_variables_initializer())\r\n>     sess.run(tf.local_variables_initializer())\r\n> \r\n>     import numpy as np\r\n>     inputs = np.random.rand(2,112,112,3)\r\n> \r\n>     print(sess.run(out,feed_dict={x:inputs}))\r\n> \r\n> \r\n> if __name__ == \"__main__\":\r\n>     test()\r\n> ```\r\n> \r\n> Try out this snippets with log out like this **TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 112, 112, 4, 8]. Consider casting elements to a supported type**.\r\n\r\n@MaeThird Can you please post a new issue by providing the information asked by the template?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "@gadagashwini [issues](29961)", "@gadagashwini Gee, that's embarrassing. I checked if I could run that code in isolation without any (other) issues, but of course I was still importing my own `utilities`... A thousand apologies! The code should now work out-of-the-box.", "@EmielBoss Now I am able to reproduce the issue on colab with Tf 1.12.0", "Apologies for the delay in response. This  executes successfully with TF 1.15. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29636\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29636\">No</a>\n"]}, {"number": 29634, "title": "Realime webcam error", "body": "(tfpose) C:\\Users\\Prabhavi\\Desktop\\ml\\tf-pose-estimation>python run_webcam.py --model=mobilenet_thin --resize=432x368 --camera=0\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_webcam.py\", line 8, in <module>\r\n    from tf_pose.estimator import TfPoseEstimator\r\n  File \"C:\\Users\\Prabhavi\\Desktop\\ml\\tf-pose-estimation\\tf_pose\\__init__.py\", line 5, in <module>\r\n    from tf_pose.runner import infer, Estimator, get_estimator\r\n  File \"C:\\Users\\Prabhavi\\Desktop\\ml\\tf-pose-estimation\\tf_pose\\runner.py\", line 7, in <module>\r\n    from tf_pose import common\r\n  File \"C:\\Users\\Prabhavi\\Desktop\\ml\\tf-pose-estimation\\tf_pose\\common.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Prabhavi\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "> \r\n> \r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n> \r\n> Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\nI am using a GTX920 on my laptop with the tensorflow version 1.10 \r\nI downloaded directly from the source.", "Did you try to follow steps from [TensorFlow website](https://www.tensorflow.org/install/source_windows) ? It is necessary to download proper Microsoft Visual C++ version. Also please verify the [CUDA compatibility matrix](https://www.tensorflow.org/install/source_windows#gpu) and let us know if that solves your problem. Thanks!", "> \r\n> \r\n> Did you try to follow steps from [TensorFlow website](https://www.tensorflow.org/install/source_windows) ? It is necessary to download proper Microsoft Visual C++ version. Also please verify the [CUDA compatibility matrix](https://www.tensorflow.org/install/source_windows#gpu) and let us know if that solves your problem. Thanks!\r\n\r\nYes I followed all the steps\r\nYes it necessary to download the proper c++ version and my CUDA compatibility is proper\r\n", "Please let us know which CUDA version you are using ? Also have a look on this [issue](https://github.com/tensorflow/tensorflow/issues/28848#issuecomment-495057676) and let us know if that helps. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29633, "title": "Make the logic consistent with shape_inference", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29633) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 29632, "title": "TF2.0 beta on RTX 2080 throws Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0-beta0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: built whl and installed with virtualenv\r\n- Bazel version (if compiling from source): 0.25.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0 / 7.5.0\r\n- GPU model and memory: RTX 2080, driver 418.40.04 \r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build the tf2.0 beta on my rtx system, and i can build successfully but i cannot run anything on i install the whl. i keep hitting \r\n```\r\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n```\r\nI also tried using the prebuilt binary it still throws the same error. Strangely 1.13.1 works prefectly only if i build it from source.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf                                                                                                                                                                             \r\n\r\nIn [2]: model = tf.keras.applications.ResNet50()                                                                                                                                                            \r\n2019-06-11 14:12:10.599679: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-06-11 14:12:10.654528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.655435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.83\r\npciBusID: 0000:02:00.0\r\n2019-06-11 14:12:10.655756: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-11 14:12:10.656708: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-11 14:12:10.672933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-11 14:12:10.673320: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-11 14:12:10.674796: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-11 14:12:10.675938: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-11 14:12:10.679002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-11 14:12:10.679234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.680150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.680897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-11 14:12:10.681515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.682282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.83\r\npciBusID: 0000:02:00.0\r\n2019-06-11 14:12:10.682345: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-11 14:12:10.682365: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-11 14:12:10.682391: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-11 14:12:10.682412: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-11 14:12:10.682432: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-11 14:12:10.682451: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-11 14:12:10.682470: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-11 14:12:10.682548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.683363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.684127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-11 14:12:10.722429: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-11 14:12:10.842536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-11 14:12:10.842661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-06-11 14:12:10.842716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-06-11 14:12:10.867592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.868473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.869242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-11 14:12:10.869958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7248 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:02:00.0, compute capability: 7.5)\r\n\r\nIn [3]: model(tf.random.normal([5, 224, 224, 3]))                                                                                                                                                           \r\n2019-06-11 14:13:21.027798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-11 14:13:22.315487: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-06-11 14:13:22.318659: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n````", "comments": ["@ymodak @jvishnuvardhan ", "Tried it on a fresh Ubuntu 18.04 installation and followed official scripts to install cuda and cudnn\r\nStill facing the same issue.\r\n```\r\n# Add NVIDIA package repositories\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install NVIDIA driver\r\nsudo apt-get install --no-install-recommends nvidia-driver-410\r\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-0 \\\r\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\r\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\r\n\r\n\r\n# Install TensorRT. Requires that libcudnn7 is installed above.\r\nsudo apt-get update && \\\r\n        sudo apt-get install nvinfer-runtime-trt-repo-ubuntu1804-5.0.2-ga-cuda10.0 \\\r\n        && sudo apt-get update \\\r\n        && sudo apt-get install -y --no-install-recommends libnvinfer-dev=5.0.2-1+cuda10.0\r\n\r\n```", "The only workaround I could find is this\r\n```\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\nBut is there any other way to fix this so that I wouldn't have to use this? This doesn't seem to be normal to me", "> The only workaround I could find is this\r\n> \r\n> ```\r\n> physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n> ```\r\n> \r\n> But is there any other way to fix this so that I wouldn't have to use this? This doesn't seem to be normal to me\r\n\r\nI have the same problem,and I have 3GPU.  By your way :\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\nand I got ValueError: Memory growth cannot differ between GPU devices\r\n\r\nand then:\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\ntf.config.experimental.set_memory_growth(physical_devices[1], True)\r\ntf.config.experimental.set_memory_growth(physical_devices[2], True)\r\n\r\nI sloved the problem,thanks a lot.\r\n", "@srihari-humbarwadi Sorry for the delay. Is this still an issue or was it resolved? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!\r\n", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29632\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29632\">No</a>\n", "I still experience the issue with an RTX 2080 and the latest tensorflow version with cuda 10.1 and cudnn 7.6", "I built TensorFlow 2.3 from source, using CUDA 10.2 and cuDNN 7.6, Arch Linux, and I get a similar error:\r\n\r\n```\r\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n```\r\n\r\nI've a GeForce 1660 Ti; the latest driver is installed, and PyTorch can use the libraries, no problem.", "> physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n> [tf.config.experimental.set_memory_growth(pd, True) for pd in physical_devices]"]}, {"number": 29631, "title": "fitting the model gives training data empty", "body": "in model.fit there is an error\r\nmodel.fit( train_dataset, \r\n    steps_per_epoch=len(image_list),\r\n    epochs=1, \r\n    validation_data=val_dataset, \r\n    validation_steps=len(val_image_list), \r\n    callbacks=callbacks)\r\n\r\n\r\n  File \"C:\\Users\\Student\\Anaconda2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 113, in finalize\r\n    raise ValueError('Empty training data.')\r\n\r\nValueError: Empty training data.\r\n\r\nusing tensorflow 2.0 for this and running on i7 CPU\r\n\r\n![1](https://user-images.githubusercontent.com/50418503/59259851-21960000-8c58-11e9-8576-95e26b7b0ead.JPG)\r\n\r\n", "comments": ["@ymodak @jvishnuvardhan "]}]