[{"number": 42563, "title": "TFLite Inference Runtime Error with Model' Conv2dLstm", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42563\">No</a>\n"]}, {"number": 42562, "title": "Set numpy version to 1.18.5", "body": "", "comments": []}, {"number": 42561, "title": "Bazel fixes for other toolchains should be global", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.15.3+nv20.07\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source):9.3.0\r\n- CUDA/cuDNN version:Cuda 11.0.207 cuDNN 8.0.1.13\r\n- GPU model and memory: Nvidia A100\r\n\r\n**Describe the current behavior**\r\n```\r\n(...)  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-py2-opt/bin/tensorflow/python/_tf_stack.so '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,--version-script bazel-out/k8-py2-opt/bin/tensorflow/python/_tf_stack-version-script.lds -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-py2-opt/bin/tensorflow/python/_tf_stack.so-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/usr/bin/ld.gold: --push-state: unknown option\r\n/usr/bin/ld.gold: use the --help option for usage information\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n**Describe the expected behavior**\r\nFixed ld.gold versions work\r\n\r\n**Code to reproduce the issue**\r\nCompile with standard Redhat's GCC\r\n\r\n**Other info / logs**\r\nYou describe the issue yourself here: https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L1192\r\n\r\nBut that does not help with other toolchains other than your own. So the solution is to use a properly patched libtool, and remove the hardcoded path to /usr/bin. The fine gentlemen of the EasyBuild project have a patch that does exactly this: https://github.com/easybuilders/easybuild-easyconfigs/blob/master/easybuild/easyconfigs/t/TensorFlow/TensorFlow-1.13.1_remove_usrbin_from_linker_bin_path_flag.patch", "comments": ["@surak,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the problem? Thanks!", "Did you read the other bug fix referenced above? \r\n\r\nI'm talking about this one: https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L1192\r\n\r\nYour fix is: either use whatever bazel gives, or, when it's clang, use this thing. But the problem appears in any system where the standard toolchain is not used.", "Can you send a PR please?", "This likely is a bazel issue anyway", "It seems that a patch exists, using host_compiler_prefix", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42561\">No</a>\n"]}, {"number": 42560, "title": "Pin numpy version to 1.19.0", "body": "", "comments": []}, {"number": 42559, "title": "Error while converting from .pb to .tflite ", "body": "The following error is being generated while I'm trying to convert my pb model file to tflite file.\r\n\r\n```\r\n2020-08-21 22:03:17.595865: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2020-08-21 22:03:17.638981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0\r\n2020-08-21 22:03:17.645082: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-08-21 22:03:17.650409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-08-21 22:03:17.656789: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-08-21 22:03:17.663393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0\r\n2020-08-21 22:03:17.668816: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-08-21 22:03:17.674059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-08-21 22:03:21.070834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-21 22:03:21.074328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0\r\n2020-08-21 22:03:21.076231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n2020-08-21 22:03:21.080389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2927 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-08-21 22:03:27.639983: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-08-21 22:03:27.644641: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2020-08-21 22:03:27.648974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0\r\n2020-08-21 22:03:27.656568: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-08-21 22:03:27.663018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-08-21 22:03:27.665528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-21 22:03:27.670558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0\r\n2020-08-21 22:03:27.672619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n2020-08-21 22:03:27.674983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2927 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-08-21 22:03:32.204885: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2020-08-21 22:03:32.208626: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7082 nodes (-490), 10726 edges (-491), time = 1643.91504ms.\r\n2020-08-21 22:03:32.212527: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7082 nodes (0), 10726 edges (0), time = 598.941ms.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\Scripts\\toco-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 898, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 404, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-08-21 22:04:44.796956: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FIFOQueueV2\r\n2020-08-21 22:04:44.814263: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"CPU\"') for unknown op: NoOp\r\n2020-08-21 22:04:44.814667: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"GPU\"') for unknown op: NoOp\r\n2020-08-21 22:04:44.815133: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostRecv\r\n2020-08-21 22:04:44.815574: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"CPU\"') for unknown op: _Send\r\n2020-08-21 22:04:44.815923: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"CPU\"') for unknown op: _HostRecv\r\n2020-08-21 22:04:44.816285: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"GPU\"') for unknown op: _Send\r\n2020-08-21 22:04:44.816618: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"CPU\"') for unknown op: _Recv\r\n2020-08-21 22:04:44.816993: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostSend\r\n2020-08-21 22:04:44.817412: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"GPU\"') for unknown op: _Recv\r\n2020-08-21 22:04:44.817697: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"CPU\"') for unknown op: _HostSend\r\n2020-08-21 22:04:44.817961: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\r\n2020-08-21 22:04:44.818266: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\r\n2020-08-21 22:04:44.818730: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\r\n2020-08-21 22:04:44.819075: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\r\n2020-08-21 22:04:44.820166: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: FIFOQueueV2\r\n2020-08-21 22:04:45.055786: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: QueueDequeueUpToV2\r\n2020-08-21 22:04:45.062161: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: QueueDequeueUpToV2\r\n2020-08-21 22:04:45.069979: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.070267: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.070687: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.071009: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.071499: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.071767: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.072164: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.072399: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.072714: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.072944: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.073255: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.073526: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.073978: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.074437: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.074677: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.075144: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.075363: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.075630: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.076161: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.076447: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.076664: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.076877: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.077287: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.077563: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.078077: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.078291: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.078502: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.078794: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.079051: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.079262: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.079752: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.080073: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.080325: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.080688: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.081058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.081355: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.081842: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.082050: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.082263: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.082747: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.082992: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.083223: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.083700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.083932: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.084143: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.084413: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.084793: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.085070: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.085578: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.085910: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.086128: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.086342: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.086559: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.086765: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.087274: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.087485: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.087699: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.087904: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.088286: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.088499: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.089035: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.089361: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.089583: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.089795: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.090012: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.090223: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.090720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.090931: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.091142: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.091416: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.091793: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.091999: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.093290: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.093599: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.093814: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.094021: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.094388: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.094594: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.094916: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.095183: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.095590: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.095799: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.096008: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.096213: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.096609: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.096825: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.097282: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.097495: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.098023: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.098249: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.098462: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.098668: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.099040: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.099255: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.099582: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.099787: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.100253: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.100544: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.100758: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.100966: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.101335: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.101541: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.101980: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.102187: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.102642: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.102850: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.103058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.103263: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.103630: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.103835: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.104161: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.104467: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.104934: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.105140: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.105351: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.105557: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.105927: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.106136: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.106472: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.106679: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.107204: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.107507: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.107782: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.107990: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.108378: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.108586: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.108919: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.109127: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.109592: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.109802: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.110012: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.110216: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.110593: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.110798: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.111180: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.111388: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.111853: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.112062: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.112275: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.112476: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.112851: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.113054: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.113390: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.113596: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.114093: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.114326: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.114539: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.114744: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.115146: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.115354: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.115692: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.115906: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.116497: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.116740: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.116969: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.117274: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.118154: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.118448: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.119075: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.119301: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.120202: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.120668: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.120890: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.121102: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.121318: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.121527: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.122257: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.122470: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.122679: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.122890: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.123110: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.123320: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.124627: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.124903: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.125548: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.125929: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.126250: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.126544: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.127186: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.127442: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.127857: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.128069: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.128641: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.128930: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.129144: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.129352: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.129738: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.129946: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.130291: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.130494: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.130995: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.131269: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.131483: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.131689: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.132131: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.132339: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.132739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.133007: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.133815: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.134120: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.134415: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.134699: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.135283: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.135572: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.136126: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.136414: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.137218: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.137537: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.137827: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.138114: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.138523: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.138732: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.139303: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.139513: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.140030: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.140245: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.140458: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.140889: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.141286: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.141602: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.141954: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.142212: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.142817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2020-08-21 22:04:45.143028: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal\r\n2020-08-21 22:04:45.961170: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 5663 operators, 8446 arrays (0 quantized)\r\n2020-08-21 22:04:46.802380: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4317 operators, 6315 arrays (0 quantized)\r\n2020-08-21 22:04:47.543171: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4317 operators, 6315 arrays (0 quantized)\r\n2020-08-21 22:04:48.477362: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 3554 operators, 5619 arrays (0 quantized)\r\n2020-08-21 22:04:49.187676: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 3554 operators, 5619 arrays (0 quantized)\r\n2020-08-21 22:04:49.715892: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 3554 operators, 5619 arrays (0 quantized)\r\n2020-08-21 22:04:50.394162: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2020-08-21 22:04:50.571496: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, FLOOR, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, MAX_POOL_2D, MEAN, MUL, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SQUARE, SQUARED_DIFFERENCE, SUB, SUM. Here is a list of operators for which you will need custom implementations: FIFOQueueV2, Merge, QueueDequeueUpToV2, RandomUniform, Reciprocal, Switch.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\Scripts\\toco_from_protos-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\ghosh\\anaconda3\\envs\\tf_gpu2\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, FLOOR, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, MAX_POOL_2D, MEAN, MUL, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SQUARE, SQUARED_DIFFERENCE, SUB, SUM. Here is a list of operators for which you will need custom implementations: FIFOQueueV2, Merge, QueueDequeueUpToV2, RandomUniform, Reciprocal, Switch.\r\n```", "comments": ["Since there is very limited information available, it would be great if you could provide the code from which the problem is originating.\r\nFrom the looks of the error, it seems like you are using FIFOQueueV2 and Reciprocal operators which are not yet implemented in TF Lite and thus require custom implementation. You can look [here](https://www.tensorflow.org/lite/guide/ops_custom) to get more info about how to add them.", "> Since there is very limited information available, it would be great if you could provide the code from which the problem is originating.\r\n> From the looks of the error, it seems like you are using FIFOQueueV2 and Reciprocal operators which are not yet implemented in TF Lite and thus require custom implementation. You can look [here](https://www.tensorflow.org/lite/guide/ops_custom) to get more info about how to add them.\r\n\r\nPlease find the below snippet for your reference. This the code being used to convert the pb file into a tffile.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrozen_model_filename = \"20170511-185253.pb\"\r\nINPUT_NODE = [\"batch_size\"]\r\nOUTPUT_NODE = [\"embeddings\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n    frozen_model_filename, INPUT_NODE, OUTPUT_NODE, input_shapes={\"batch_size\":[3,3,3,32]})\r\n\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```", "@Ayush-Ghosh Thanks for sharing this Ayush. However, the problem lies not in the conversion but the operations that were used for creating the model as some of those have not yet been implemented. You can add their custom implementation for now.\r\nDo you want to work on a PR?", "@NikhilBartwal Thanks Nikhil for the quick response and yes, I'm actually looking into it. Right now I'm going through the documentation to create a custom op for this. ", "@Ayush-Ghosh Happy to help :)", "@Ayush-Ghosh \r\nIs this still an issue, else could you please move this to closed status.", "@Saduf2019 Yes the issue still exists. The document turned out to be not so helpful for me. It would be great if some documentation can pe provided to add a custom operation using python. ", "Hey @Ayush-Ghosh, Could you tell us the operations that you are trying to use which have not been implemented by TFLite yet.\r\nAlso, it would be great if you show the code that you have written up till onw for the custom installation.", "@Ayush-Ghosh\r\nPlease provide simple indented stand alone code to build the model or to provide the pb file, or if possible share a colab gist with issue reported.", "@Saduf2019 You can find the code snippet above for the conversion of pb file to tflite. If you want I can share the pb file as well.", "@Ayush-Ghosh Can you please share the *.pb file? What is the TF version you are using? Thanks!\r\n\r\n[Here](https://github.com/jvishnuvardhan/TF_Lite/blob/master/Convert_pb_2_tflite.ipynb) I showed an example of how to covert *pb to *.tflite model. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42559\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42559\">No</a>\n"]}, {"number": 42558, "title": "gen_nccl_ops.nccl_all_reduce will influence optimizer!!!", "body": "My version is tf 1.13, I use multi-GPUs to  train a model, when i minimize the loss in 0-gpu, 1-gpu's is not minimized, there is someting strange.\r\nso this is my code \r\n\r\n<img width=\"701\" alt=\"20200821-224654(WeLinkPC)\" src=\"https://user-images.githubusercontent.com/34296901/90903315-58d97280-e400-11ea-9f58-a2a56ed87b76.png\">\r\n\r\n\r\nthen when i run the code , it will keep stopping at a point. After debuging, i find that when  i use nccl_all_reduce in syncBN, this will appear, like this\r\n\r\n<img width=\"588\" alt=\"20200821-224658(WeLinkPC)\" src=\"https://user-images.githubusercontent.com/34296901/90903324-5e36bd00-e400-11ea-87a9-bb44d21d66cd.png\">\r\n\r\n\r\nwhen i comment the two line, everything is ok.\r\nBy th way, i minimize all losses in all gpus using compute_gradient and apply_gradient, when using nccl_reduce_all,  the model could't converge, after commentting it everything  is ok.\r\n\r\nI think it's a bug.", "comments": ["@zihao-lu,\r\nCan you please paste the complete code (rather than sharing the screenshot) so that it would be easy for us to reproduce your issue. Thanks!", "@zihao-lu,\r\nCan you please respond to the comment. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42558\">No</a>\n"]}, {"number": 42557, "title": "how to use tensorflow2 to identify  captcha image?", "body": "the captcha image has  four characters, how to output?", "comments": ["@YouSmart2016,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n", "thank you!  I will colse the issue"]}, {"number": 42556, "title": "Can't install tensorflow - numpy incompatibility?", "body": "**System information**\r\n- Windows10 x64\r\n- Python version 3.8\r\n- Tryed to install in a virtualenv? using conda and pip\r\n- CUDA/cuDNN version: 9.2\r\n- GPU model and memory: NVIDIA GeForce GTX 950M\r\n\r\n\r\n\r\n**Problem**\r\nI created a virtual environment and there I started to install necessary packages. When I tried to run a script that used tensorflow, it could not imported it.\r\nAfter searching for this, I tried to import it in jupyter notebook, but it didn't work.\r\nThen, I tried to add this to my code but it didn't work either:\r\ntf.compat.v1.enable_eager_execution(\r\nconfig=None, device_policy=None, execution_mode=None\r\n)\r\nI tried to install tensorflow in that environment using first conda and then pip.\r\n\r\n\r\n**Sequence of commands / steps executed**\r\n-Runing the script:\r\n\r\nd:\\software\\anaconda_envs\\sweaver\\avgn_paper-vizmerge\\avgn\\utils\\json.py:64: ResourceWarning: unclosed file <_io.TextIOWrapper name='D:\\\\Software\\\\Anaconda_envs\\\\sweaver\\\\avgn_paper-vizmerge\\\\data\\\\processed\\\\sociable_weaver_damelio\\\\2020-08-21_09-35-13\\\\JSON\\\\2018-10-19_09-00-00-000001.JSON' mode='r' encoding='cp1252'>\r\n  return json.load(open(json_loc), object_pairs_hook=OrderedDict)\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nHBox(children=(FloatProgress(value=0.0, description='loading json', max=1.0, style=ProgressStyle(description_w\u2026\r\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\r\n\r\n[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.0s finished\r\nHBox(children=(FloatProgress(value=0.0, description='getting unique individuals', max=1.0, style=ProgressStyle\u2026\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-43-e50f361ab3a6> in <module>\r\n      1 # create a dataset object\r\n----> 2 dataset = DataSet(DATASET_ID, hparams = hparams)\r\n\r\nd:\\software\\anaconda_envs\\sweaver\\avgn_paper-vizmerge\\avgn\\dataset.py in __init__(self, DATASET_ID, hparams, default_rate, build_mel_matrix)\r\n     43 \r\n     44         if build_mel_matrix:\r\n---> 45             self.build_mel_matrix()\r\n     46 \r\n     47     def _get_wav_json_files(self):\r\n\r\nd:\\software\\anaconda_envs\\sweaver\\avgn_paper-vizmerge\\avgn\\dataset.py in build_mel_matrix(self, rate)\r\n     66         if rate is None:\r\n     67             rate = self.sample_json[\"samplerate_hz\"]\r\n---> 68         self.mel_matrix = prepare_mel_matrix(self.hparams, rate)\r\n     69 \r\n     70     def _get_unique_individuals(self):\r\n\r\nd:\\software\\anaconda_envs\\sweaver\\avgn_paper-vizmerge\\avgn\\signalprocessing\\filtering.py in prepare_mel_matrix(hparams, rate, return_numpy, GPU_backend)\r\n     71             os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\r\n     72             os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n---> 73         import tensorflow as tf\r\n     74 \r\n     75     tf.compat.v1.enable_eager_execution(\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n-Import it in jupyter notebook\r\nimport sys\r\n!conda install --yes --prefix {sys.prefix} tensorflow\r\n\r\nUnsatisfiableError: The following specifications were found to be incompatible with the existing python installation in your environment:\r\nSpecifications:\r\n- tensorflow -> python[version'3.5.*|3.6.*|3.7.*'\r\nYour python: python=3.8\r\n\r\nThe following specifications were found to be incompatible with your CUDA driver:\r\n- feature:/win-64::__cuda==9.2=0\r\nYour installed CUDA driver is: 9.2\r\n\r\n-When I tried to install it through conda:\r\n\r\n(D:\\Software\\Anaconda_envs\\sweaver) D:\\Software\\Anaconda_envs\\sweaver>conda install tensorflow\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: -\r\nFound conflicts! Looking for incompatible packages.\r\nThis can take several minutes.  Press CTRL-C to abort.\r\nExamining @/win-64::__cuda==9.2=0:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                | 2/3 [00:00<00:00, 18.17it/s]|Examining conflict for __cuda:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                 | 2/3 [00:00<00:00,  3.77it/s]|failed\r\n\r\nUnsatisfiableError: The following specifications were found\r\nto be incompatible with the existing python installation in your environment:\r\n\r\nSpecifications:\r\n\r\n  - tensorflow -> python[version='3.5.*|3.6.*|3.7.*']\r\n\r\nYour python: python=3.8\r\n\r\nIf python is on the left-most side of the chain, that's the version you've asked for.\r\nWhen python appears to the right, that indicates that the thing on the left is somehow\r\nnot available for the python version you are constrained to. Note that conda will not\r\nchange your python version to a different minor version unless you explicitly specify\r\nthat.\r\n\r\nThe following specifications were found to be incompatible with your system:\r\n\r\n  - feature:/win-64::__cuda==9.2=0\r\n  - feature:|@/win-64::__cuda==9.2=0\r\n\r\nYour installed version is: 9.2\r\n\r\n\r\n-When I tried to install it using pip:\r\n\r\n(D:\\Software\\Anaconda_envs\\sweaver) D:\\Software\\Anaconda_envs\\sweaver>pip install tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-2.3.0-cp38-cp38-win_amd64.whl (342.5 MB)\r\nRequirement already satisfied: wrapt>=1.11.1 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (1.11.2)\r\nCollecting keras-preprocessing<1.2,>=1.1.1\r\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\nCollecting tensorboard<3,>=2.3.0\r\n  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\r\nRequirement already satisfied: grpcio>=1.8.6 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (1.31.0)\r\nCollecting numpy<1.19.0,>=1.16.0\r\n  Using cached numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\r\nCollecting opt-einsum>=2.3.2\r\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\nRequirement already satisfied: absl-py>=0.7.0 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (0.10.0)\r\nRequirement already satisfied: wheel>=0.26 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (0.34.2)\r\nCollecting scipy==1.4.1\r\n  Using cached scipy-1.4.1-cp38-cp38-win_amd64.whl (31.0 MB)\r\nCollecting astunparse==1.6.3\r\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (2.10.0)\r\nCollecting protobuf>=3.9.2\r\n  Using cached protobuf-3.13.0-py2.py3-none-any.whl (438 kB)\r\nCollecting google-pasta>=0.1.8\r\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\nRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (2.3.0)\r\nRequirement already satisfied: gast==0.3.3 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (0.3.3)\r\nRequirement already satisfied: six>=1.12.0 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorflow) (1.15.0)\r\nProcessing c:\\users\\bf\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\r\nRequirement already satisfied: werkzeug>=0.11.15 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\r\nCollecting markdown>=2.6.8\r\n  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\r\nCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nRequirement already satisfied: setuptools>=41.0.0 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.6.0.post20200814)\r\nCollecting tensorboard-plugin-wit>=1.6.0\r\n  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\r\nRequirement already satisfied: requests<3,>=2.21.0 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.20.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\r\nRequirement already satisfied: idna<3,>=2.5 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\r\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.5)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\software\\anaconda_envs\\sweaver\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\r\nERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\r\nTraceback (most recent call last):\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3021, in _dep_map\r\n    return self.__dep_map\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\r\n    raise AttributeError(attr)\r\nAttributeError: _DistInfoDistribution__dep_map\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3012, in _parsed_pkg_info\r\n    return self._pkg_info\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\r\n    raise AttributeError(attr)\r\nAttributeError: _pkg_info\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 535, in _determine_conflicts\r\n    return check_install_conflicts(to_install)\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 108, in check_install_conflicts\r\n    package_set, _ = create_package_set_from_installed()\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 50, in create_package_set_from_installed\r\n    package_set[name] = PackageDetails(dist.version, dist.requires())\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2736, in requires\r\n    dm = self._dep_map\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3023, in _dep_map\r\n    self.__dep_map = self._compute_dependencies()\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3032, in _compute_dependencies\r\n    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3014, in _parsed_pkg_info\r\n    metadata = self.get_metadata(self.PKG_INFO)\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1420, in get_metadata\r\n    value = self._get(path)\r\n  File \"D:\\Software\\Anaconda_envs\\sweaver\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1616, in _get\r\n    with open(path, 'rb') as stream:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'd:\\\\software\\\\anaconda_envs\\\\sweaver\\\\lib\\\\site-packages\\\\numpy-1.19.1.dist-info\\\\METADATA'\r\nInstalling collected packages: numpy, keras-preprocessing, protobuf, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, opt-einsum, scipy, astunparse, google-pasta, termcolor, tensorflow\r\n  Attempting uninstall: numpy\r\n    Found existing installation: numpy 1.19.1\r\nERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'd:\\\\software\\\\anaconda_envs\\\\sweaver\\\\lib\\\\site-packages\\\\numpy-1.19.1.dist-info\\\\RECORD'\r\n\r\n\r\nMaybe it's a problem of compatibility between tensorflow and numpy? But the file 'd:\\\\software\\\\anaconda_envs\\\\sweaver\\\\lib\\\\site-packages\\\\numpy-1.19.1.dist-info\\\\RECORD' actually does not exist.\r\nI searched in my packages and there is a folder of numpy, numpy-1.18.5.dist-info and numpy-1.19.1.dist-info, but inside the last one, there are only two files: LICENSES_bundled.txt and REQUESTED.\r\n\r\nHope you can help me. I began using python just 2 weeks ago.\r\n", "comments": ["From the stack trace it looks like you are hitting windows path length limit.\n   * Try to disable path length limit on Windows 10.\n     * Refer [disable path length limit instructions guide.](https://mspoweruser.com/ntfs-260-character-windows-10/)\n\nPlease let us know if this helps.\n", "@brbfreitas \r\nPlease note the error \"ModuleNotFoundError: No module named 'tensorflow'\" is due to incompatibility.\r\nplease refer to : #42367\r\n\r\nalso verify: \r\nif you have 32 bit archutecture, Tensorflow does not support 32 bit architecture.Please, refer the issue #32315  and 342548\r\n\r\nFor anaconda installation issues:\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42556\">No</a>\n"]}, {"number": 42555, "title": "Make Grappler also ignore functions transitively called by XlaLaunch ops", "body": "The MemoryOptimizer Grappler pass can introduce TemporaryVariable ops if certain conditions are met:\r\n1. 2 or more tensors are added together with a total live memory size >204.8mb;\r\n2. they are added inside an XLA cluster;\r\n3. The XLA cluster is compiled by an XLA device that does not implement a kernel for TemporaryVariable.\r\n\r\nThe addition can be made explicitly by the user or implicitly by TF e.g. when summing an op's gradients w.r.t all its outputs in backpropagation.\r\n\r\nThis situation occurred in issue [#30580](https://github.com/tensorflow/tensorflow/issues/30580).\r\nThe fix ([471b73c238709](https://github.com/tensorflow/tensorflow/commit/471b73c238709fb796929eb412f1dab763b3f8cc)) simply turned Grappler off for such graphs by making it ignore all functions that are called by XlaLaunch ops.\r\nHowever this fix is insufficient if any op in the XlaLaunch function is functionalized out of it. For example, a While op in the main XlaLaunch function will be functionalized out before Grappler. Since it's now a separate function - and not _directly_ called by an XlaLaunch op - it's not ignored by Grappler.\r\n\r\nThis PR enhances the fix by also looking for transitively called functions in the XlaLaunch function.", "comments": []}, {"number": 42554, "title": "Regarding RISCV-V Vector ISA extension on CPU / RISC-V", "body": "As I understood is RISC-V Vector ISA extension is available with Tensorflow lite for mobile/IOT device. Can we also use the same (RISC-V Vector ISA enabled) for RISC-V machines or CPU by cross-compiling the same for RISC-V for neural network applications?   ", "comments": ["@soniab \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42553, "title": "StringLookup layer adds None to list of saved model variables which breaks hub.KerasLayer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-39890-gf74cc7a696 2.4.0-dev20200821\r\n- Python version: Colab default\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nWhen loading (tf.saved_model.load) saved model (tf.saved_model.save) with StringLookup layer, None appears in list of model variables.\r\nThis breaks loading such model in tensorflow_hub.KerasLayer\r\n\r\n**Describe the expected behavior**\r\nSaved model should not contain None in list of variables\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1sVGGeJZ2rq6PofMXJ31gknhnYomHFyX7?usp=sharing\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1d5b660004dbb02b4f5fb34089ccc1d2/42553-tf-nightly.ipynb). Thanks!", "TF 2.4.0: issue still here", "Hi @shkarupa-alex, can you try this with the Keras saving/loading APIs instead of the TF ones?\r\n\r\nmodel.save('model')\r\nrestored = tf.keras.models.load_model('model')\r\n\r\nshould get around this issue.", "@markomernick , your solution woks if used in custom code.\r\nBut originally this issue occurred when i tried to use exported model with TF-Hub. Just to make solving easier i dug deeper to find source of the issue.\r\n\r\nAt the moment issue with loading model with TF-Hub is present. I've added example at the end of https://colab.research.google.com/drive/1sVGGeJZ2rq6PofMXJ31gknhnYomHFyX7?usp=sharing#scrollTo=lPCNaXQn2wx7", "TF 2.5.0: issue still here", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/3f1c0e82b9bc4e771571205824a78158/untitled57.ipynb#scrollTo=qY3JNHEgRZ-l)..Thanks !", "@shkarupa-alex I tried to replicate the issue on colab using TF v2.8.0 and didn't see the error reported here .Please find the updated  [gist](https://colab.research.google.com/gist/sushreebarsa/d6f4d99afb7672fff2cc7baf556b139f/gist42553.ipynb) and let us know if it helps?Thanks!", "Seems that issue was resolved. At leas my test case in first message working well.", "@shkarupa-alex Thank you for the update!\r\nCould you please move this issue to closed status if it is resolved for you ?\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42553\">No</a>\n"]}, {"number": 42552, "title": "Add a tf.feature_column like sklearn.preprocessing.MultiLabelBinarizer", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No):\r\nIf I can ......\r\n\r\n**Describe the feature and the current behavior/state.**\r\n```tf.feature_column``` doesn't have any column for dealing with multi-hot feature like ```sklearn.preprocessing.MultiLabelBinarizer``` now.\r\n\r\n**Will this change the current api? How?**\r\nI guess not\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who has to deal with multi-hot feature \r\n\r\n**Any Other info.**\r\n", "comments": ["@DachuanZhao Tensorflow does have an `indicator_column` which you can see [here](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column) that is used for multi-hot representation of a categorical column.\r\nIs that the feature you were looking for?", "@DachuanZhao  I agree with @NikhilBartwal as he correct that it is being used for multi-hot represtation of the given categorical column!!\r\nI think it helped You!", "> @DachuanZhao Tensorflow does have an `indicator_column` which you can see [here](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column) that is used for multi-hot representation of a categorical column.\r\n> Is that the feature you were looking for?\r\n\r\nThere are two questions : \r\n1.  I should give ```indicator_column``` the ```vocabulary_list```  explicitly .\r\n\r\n```Python\r\n# this is a df column\r\ndf_column_list = [\"1,2\",\"3\"]\r\ndf_column_list = [v.split(\",\") for v in column_list]\r\n\r\n# sklearn doesn't raise error\r\nfrom sklearn.preprocessing import MultiLabelBinarizer\r\nmlb = MultiLabelBinarizer()\r\nmlb.fit_transform(df_column_list)\r\n\r\n#tf raise error \r\ntest_column = tf.feature_column.categorical_column_with_vocabulary_list(\"category\", df_column_list ,num_oov_buckets=0)\r\n```\r\n\r\n2. I should padding the ```df_column_list ``` first in tf .\r\n```Python\r\ntest_column = tf.feature_column.categorical_column_with_vocabulary_list(\"category\", [\"1\",\"2\",\"3\",\"4\"],num_oov_buckets=0)\r\n\r\n#This line raises error\r\n#tf.keras.layers.DenseFeatures(tf.feature_column.indicator_column(test_column))({\"category\":[[\"1\",\"2\"],[\"2\"],[\"3\"],[\"4\"],[\"3\"]]})\r\n\r\n#I should padding it first , then it doesn't raise error\r\ntf.keras.layers.DenseFeatures(tf.feature_column.indicator_column(test_column))({\"category\":[[\"1\",\"2\"],[\"2\",\"0\"],[\"3\",\"0\"],[\"4\",\"0\"],[\"3\",\"0\"]]})\r\n```\r\nBoth of them makes  ```indicator_column```  complicated to use ...", "> @DachuanZhao I agree with @NikhilBartwal as he correct that it is being used for multi-hot represtation of the given categorical column!!\r\n> I think it helped You!\r\n\r\n> @DachuanZhao Tensorflow does have an `indicator_column` which you can see [here](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column) that is used for multi-hot representation of a categorical column.\r\n> Is that the feature you were looking for?\r\n\r\nThere are two questions : \r\n1.  I should give ```indicator_column``` the ```vocabulary_list```  explicitly .\r\n\r\n```Python\r\n# this is a df column\r\ndf_column_list = [\"1,2\",\"3\"]\r\ndf_column_list = [v.split(\",\") for v in column_list]\r\n\r\n# sklearn doesn't raise error\r\nfrom sklearn.preprocessing import MultiLabelBinarizer\r\nmlb = MultiLabelBinarizer()\r\nmlb.fit_transform(df_column_list)\r\n\r\n#tf raise error \r\ntest_column = tf.feature_column.categorical_column_with_vocabulary_list(\"category\", df_column_list ,num_oov_buckets=0)\r\n```\r\n\r\n2. I should padding the ```df_column_list ``` first in tf .\r\n```Python\r\ntest_column = tf.feature_column.categorical_column_with_vocabulary_list(\"category\", [\"1\",\"2\",\"3\",\"4\"],num_oov_buckets=0)\r\n\r\n#This line raises error\r\n#tf.keras.layers.DenseFeatures(tf.feature_column.indicator_column(test_column))({\"category\":[[\"1\",\"2\"],[\"2\"],[\"3\"],[\"4\"],[\"3\"]]})\r\n\r\n#I should padding it first , then it doesn't raise error\r\ntf.keras.layers.DenseFeatures(tf.feature_column.indicator_column(test_column))({\"category\":[[\"1\",\"2\"],[\"2\",\"0\"],[\"3\",\"0\"],[\"4\",\"0\"],[\"3\",\"0\"]]})\r\n```\r\nBoth of them makes  ```indicator_column```  complicated to use ...", "We don't support feature column with Keras anymore (especially after 2.4) -- Please use `tf.keras.layers.experimental.preprocessing.CategoryEncoding` and its `adapt` method (which is equivalent to `fit_transform` here) and let us know if that fulfills your use case", "> We don't support feature column with Keras anymore (especially after 2.4) -- Please use `tf.keras.layers.experimental.preprocessing.CategoryEncoding` and its `adapt` method (which is equivalent to `fit_transform` here) and let us know if that fulfills your use case\r\n\r\nOK . It doesn't help me .\r\nMy problem is that I don't know how to deal with column like this :\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame( )\r\ndf[\"text_split_by_comma\"] = [\"1\",\"2,3\",\"4,5\",\"1,6,7,9\"]\r\n```\r\nThere isn't any function for dealing with it in ```tf.data``` and ```tf.feature_column``` simply .", "> We don't support feature column with Keras anymore (especially after 2.4) -- Please use `tf.keras.layers.experimental.preprocessing.CategoryEncoding` and its `adapt` method (which is equivalent to `fit_transform` here) and let us know if that fulfills your use case\r\n\r\nLike this [https://www.tensorflow.org/tutorials/structured_data/feature_columns#create_compile_and_train_the_model](https://www.tensorflow.org/tutorials/structured_data/feature_columns#create_compile_and_train_the_model)", "You should read this as string and perform split *after* you read from csv, not during read from csv", "> You should read this as string and perform split _after_ you read from csv, not during read from csv\r\n\r\nIt means I separate ```preprocess data```  from ```model``` if I do that . So I have to ```preprocess data```  once again when I call ```model.predict``` .  \r\n", "KPL can be part of your model:\r\n```\r\nclass MyModel(tf.keras.Model):\r\n  def __init__(self):\r\n    self.preprocess_layer = ...\r\n    ...\r\n  def call(self, inputs):\r\n    preprocessed_inputs = self.preprocess_layer(inputs)\r\n    output = ...\r\n    return output\r\n```", "> We don't support feature column with Keras anymore (especially after 2.4) -- Please use `tf.keras.layers.experimental.preprocessing.CategoryEncoding` and its `adapt` method (which is equivalent to `fit_transform` here) and let us know if that fulfills your use case\r\n\r\nYou should edit this [https://www.tensorflow.org/tutorials/structured_data/feature_columns](https://www.tensorflow.org/tutorials/structured_data/feature_columns) first if you don't  support feature column with Keras anymore ......", "> > We don't support feature column with Keras anymore (especially after 2.4) -- Please use `tf.keras.layers.experimental.preprocessing.CategoryEncoding` and its `adapt` method (which is equivalent to `fit_transform` here) and let us know if that fulfills your use case\r\n> \r\n> You should edit this https://www.tensorflow.org/tutorials/structured_data/feature_columns first if you don't support feature column with Keras anymore ......\r\n\r\nThat's not the way how this works. We need to move those experimental layers into core API before we can claim an alternative solution to be deprecated. For now, given they still serve a majority of use cases, they need to live.\r\n\r\nBut I do agree with the confusion, I think we can make this happen soon.", "> > > We don't support feature column with Keras anymore (especially after 2.4) -- Please use `tf.keras.layers.experimental.preprocessing.CategoryEncoding` and its `adapt` method (which is equivalent to `fit_transform` here) and let us know if that fulfills your use case\r\n> > \r\n> > \r\n> > You should edit this https://www.tensorflow.org/tutorials/structured_data/feature_columns first if you don't support feature column with Keras anymore ......\r\n> \r\n> That's not the way how this works. We need to move those experimental layers into core API before we can claim an alternative solution to be deprecated. For now, given they still serve a majority of use cases, they need to live.\r\n> \r\n> But I do agree with the confusion, I think we can make this happen soon.\r\n\r\nYou can refer to [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) for ```tf.keras.layers.experimental.preprocessing```  , especially ```MultiLabelBinarizer``` and ```LabelBinarizer``` . It's really convenient~~~", "> KPL can be part of your model:\r\n> \r\n> ```\r\n> class MyModel(tf.keras.Model):\r\n>   def __init__(self):\r\n>     self.preprocess_layer = ...\r\n>     ...\r\n>   def call(self, inputs):\r\n>     preprocessed_inputs = self.preprocess_layer(inputs)\r\n>     output = ...\r\n>     return output\r\n> ```\r\n\r\nI'm working on include my data processing code into the model, this is exactly what I'd like to use. But I just looked at functions in tf.keras.layers.experimental.preprocessing, it only has around 20 classes, seems very limited. My data processing code is written all in tf-functions, and I'm not able to convert all tf-functions in my data processing code to tf.keras.layers.experimental.preprocessing. Some of my tf functions in data processing code include: tf.strings.split, tf.lookup.StaticHashTable, tf.strings.to_number, tf.sparse.SparseTensor, tf.compat.v1.sparse_to_dense, tf.fill, tf.concat, tf.not_equal, tf.cast. Thanks.    @tanzhenyu @DachuanZhao \r\n", "> > > We don't support feature column with Keras anymore (especially after 2.4) -- Please use `tf.keras.layers.experimental.preprocessing.CategoryEncoding` and its `adapt` method (which is equivalent to `fit_transform` here) and let us know if that fulfills your use case\r\n> > \r\n> > \r\n> > You should edit this https://www.tensorflow.org/tutorials/structured_data/feature_columns first if you don't support feature column with Keras anymore ......\r\n> \r\n> That's not the way how this works. We need to move those experimental layers into core API before we can claim an alternative solution to be deprecated. For now, given they still serve a majority of use cases, they need to live.\r\n> \r\n> But I do agree with the confusion, I think we can make this happen soon.\r\n\r\nHi , I have read the newest document at [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) . It's very useful , but I have another question : \r\nHow to serve the model in that document by tf-serving ? Is there a tf-serving document ready for this new feature ?\r\nI add an issue at [https://github.com/tensorflow/tensorflow/issues/44002](https://github.com/tensorflow/tensorflow/issues/44002)  and you can answer it there .", "> KPL can be part of your model:\r\n> \r\n> ```\r\n> class MyModel(tf.keras.Model):\r\n>   def __init__(self):\r\n>     self.preprocess_layer = ...\r\n>     ...\r\n>   def call(self, inputs):\r\n>     preprocessed_inputs = self.preprocess_layer(inputs)\r\n>     output = ...\r\n>     return output\r\n> ```\r\n\r\nHi , there are two issues about KPL :\r\n[https://github.com/tensorflow/tensorflow/issues/44005](https://github.com/tensorflow/tensorflow/issues/44005)\r\n[https://github.com/tensorflow/tensorboard/issues/4530](https://github.com/tensorflow/tensorboard/issues/4530)\r\nDo you know anyone who may solve them ?  Could you please @  them in the issues ?"]}, {"number": 42551, "title": "[INTEL MKL] Enable Conv + (Bias+BN) + LeakyRelu Fusion with Eigen implementation in CPU (Resubmit)", "body": "This PR fixes the error in https://github.com/tensorflow/tensorflow/pull/42489\r\n\r\nThis error is caused by not sending activation parameter with conv + bn + leakyrelu pattern in `remapper.cc`\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/4d022d6e2cbc924fbff1ffa8c1d98383c4ecaeae is the squeeze commit for all  commits in https://github.com/tensorflow/tensorflow/pull/42489\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/6aabcb1923fce33dd3cd17ab6875f5d2ec9d49b1 updates the unit test to capture the error found in Google internal test.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/62db9a72f1d5d980ee6c8f8f9c69510b8220c5d5 fixes the error.", "comments": ["Hi Rajeshwar and Eugene, the UT failures are not related to this PR.  Please help to approve this PR."]}, {"number": 42550, "title": "Tensorflow Lite python interpreter for armv6", "body": "Very simple question:\r\n\r\nI'd like to install the Lite python interpreter ([https://www.tensorflow.org/lite/guide/python](https://www.tensorflow.org/lite/guide/python)) on an old RaspberryPi, that has an armv6.\r\n\r\nAt that page there is a list of downloadable Python wheels, but there is not the right one for my Pi, in fact, there are none for armv6. \r\n\r\nIdeally I would need: armv6, Py3.7, Tensorflow version 2.3 (actually I don't if version match between TF and TF Lite)\r\n\r\nWhat should I do?\r\n\r\nThanks, have a great day", "comments": ["I'd like to add that, more confusingly, there is a [pip package](https://pypi.org/project/tflite/) by @jackwish named 'tflite' which i'm not sure what really does and how it relates to the \"official\" TF Lite parser", "[Relevant SO question with no answer](https://stackoverflow.com/questions/62749168/modulenotfounderror-no-module-named-tflite-runtime)", "As it has been documented, tflite pip package is a *.tflite parsing\ninterface package, which doesn\u2019t support running tflite modules.\n\nLuca Olivieri <notifications@github.com>\u4e8e2020\u5e748\u670821\u65e5 \u5468\u4e94\u4e0b\u53484:17\u5199\u9053\uff1a\n\n> I'd like to add that, more confusingly, there is a pip package\n> <https://pypi.org/project/tflite/> by @jackwish\n> <https://github.com/jackwish> named 'tflite' which i'm not sure what\n> really does and how it relates to the \"official\" TF Lite parser\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42550#issuecomment-678109742>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABFVHDMP2YTWDPM57DN7ZT3SBYUNPANCNFSM4QG73UJA>\n> .\n>\n", "You may try building your own tf lite pip package using this [guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42550\">No</a>\n"]}, {"number": 42549, "title": "Fix GatherV2 shape inference", "body": "Fixes #42522.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8a002f2269310f513d326eaeff5c73f679c73f77/tensorflow/core/kernels/gather_op.cc#L92-L107", "comments": ["Sorry about the rollback, looks like it was totally spurious. I've rolled your PR forward, hopefully permanently this time (should show up in a few minutes)."]}, {"number": 42548, "title": "ImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002", "body": "gpu gtx660\r\nonly run pip install tensorflow-gpu\r\ntensorflow-gpu2.1\r\npycharm\r\npython3.7.6\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Program Files\\python\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Program Files\\python\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"D:/WorkSpace/py/tf/01.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"D:\\Program Files\\python\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\PyCharm\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\WorkSpace\\py\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Program Files\\python\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Program Files\\python\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@yixinNB,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42548\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42548\">No</a>\n"]}, {"number": 42547, "title": "TensorFlow nightly build 2.4.0.dev20200816 or newer breaks Horovod (0.19.1 and newer) installation.", "body": "I'm trying to install `Horovod==0.19.1` with OpenMPI support using TensorFlow cpu nightly wheel and I noticed I'm getting the error: (full error log is attached)\r\n\r\n```\r\n# pip install --no-cache-dir --ignore-installed horovod==0.19.1\r\nCollecting horovod==0.19.1\r\n  Downloading horovod-0.19.1.tar.gz (2.9 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9 MB 1.7 MB/s \r\nCollecting cloudpickle\r\n  Downloading cloudpickle-1.5.0-py3-none-any.whl (22 kB)\r\nCollecting psutil\r\n  Downloading psutil-5.7.2.tar.gz (460 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 460 kB 17.6 MB/s \r\nCollecting pyyaml\r\n  Downloading PyYAML-5.3.1.tar.gz (269 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 269 kB 15.8 MB/s \r\nCollecting six\r\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\r\nBuilding wheels for collected packages: horovod, psutil, pyyaml\r\n  Building wheel for horovod (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-db1tsv5m/horovod/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-db1tsv5m/horovod/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-w2tr7faj\r\n       cwd: /tmp/pip-install-db1tsv5m/horovod/\r\n  Complete output (1999 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  creating build/lib.linux-x86_64-3.6/horovod\r\n  copying horovod/__init__.py -> build/lib.linux-x86_64-3.6/horovod\r\n  creating build/lib.linux-x86_64-3.6/horovod/run\r\n  copying horovod/run/task_fn.py -> build/lib.linux-x86_64-3.6/horovod/run\r\n  copying horovod/run/mpi_run.py -> build/lib.linux-x86_64-3.6/horovod/run\r\n  copying horovod/run/run_task.py -> build/lib.linux-x86_64-3.6/horovod/run\r\n  copying horovod/run/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run\r\n  copying horovod/run/gloo_run.py -> build/lib.linux-x86_64-3.6/horovod/run\r\n  copying horovod/run/run.py -> build/lib.linux-x86_64-3.6/horovod/run\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark\r\n  copying horovod/spark/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark\r\n  creating build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/compression.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  creating build/lib.linux-x86_64-3.6/horovod/_keras\r\n  copying horovod/_keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/_keras\r\n  copying horovod/_keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/_keras\r\n  creating build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/basics.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/util.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  creating build/lib.linux-x86_64-3.6/horovod/keras\r\n  copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/keras\r\n  copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/keras\r\n  creating build/lib.linux-x86_64-3.6/horovod/mxnet\r\n  copying horovod/mxnet/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\r\n  copying horovod/mxnet/__init__.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\r\n  creating build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/compression.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/util.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  creating build/lib.linux-x86_64-3.6/horovod/run/driver\r\n  copying horovod/run/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\r\n  copying horovod/run/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\r\n  creating build/lib.linux-x86_64-3.6/horovod/run/http\r\n  copying horovod/run/http/http_client.py -> build/lib.linux-x86_64-3.6/horovod/run/http\r\n  copying horovod/run/http/http_server.py -> build/lib.linux-x86_64-3.6/horovod/run/http\r\n  copying horovod/run/http/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/http\r\n  creating build/lib.linux-x86_64-3.6/horovod/run/util\r\n  copying horovod/run/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/util\r\n  copying horovod/run/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/util\r\n  copying horovod/run/util/threads.py -> build/lib.linux-x86_64-3.6/horovod/run/util\r\n  copying horovod/run/util/cache.py -> build/lib.linux-x86_64-3.6/horovod/run/util\r\n  creating build/lib.linux-x86_64-3.6/horovod/run/task\r\n  copying horovod/run/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/task\r\n  copying horovod/run/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/task\r\n  creating build/lib.linux-x86_64-3.6/horovod/run/common\r\n  copying horovod/run/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common\r\n  creating build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/secret.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/host_hash.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/codec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/env.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/settings.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/timeout.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/safe_shell_exec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  copying horovod/run/common/util/config_parser.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\r\n  creating build/lib.linux-x86_64-3.6/horovod/run/common/service\r\n  copying horovod/run/common/service/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\r\n  copying horovod/run/common/service/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\r\n  copying horovod/run/common/service/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/mpirun_rsh.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/job_id.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/remote.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/task_info.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/mpirun_exec_fn.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/constants.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/_namedtuple_fix.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/params.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/serialization.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/store.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/backend.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/cache.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/bare.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/optimizer.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/remote.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/tensorflow.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\r\n  copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\r\n  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\r\n  copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\r\n  creating build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\r\n  copying horovod/tensorflow/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\r\n  copying horovod/tensorflow/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\r\n  running build_ext\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o\r\n  x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.so\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_link_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o\r\n  x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,--version-script=horovod.lds build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.so\r\n  INFO: Compilers /usr/bin/gcc and /usr/bin/g++ (version 8.4.0) selected for TensorFlow plugin build.\r\n  -- The CXX compiler identification is GNU 8.4.0\r\n  -- The C compiler identification is GNU 8.4.0\r\n  -- Check for working CXX compiler: /usr/bin/g++\r\n  -- Check for working CXX compiler: /usr/bin/g++ -- works\r\n  -- Detecting CXX compiler ABI info\r\n  -- Detecting CXX compiler ABI info - done\r\n  -- Detecting CXX compile features\r\n  -- Detecting CXX compile features - done\r\n  -- Check for working C compiler: /usr/bin/gcc\r\n  -- Check for working C compiler: /usr/bin/gcc -- works\r\n  -- Detecting C compiler ABI info\r\n  -- Detecting C compiler ABI info - done\r\n  -- Detecting C compile features\r\n  -- Detecting C compile features - done\r\n  -- Found MPI_C: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so (found version \"3.1\")\r\n  -- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\")\r\n  -- Found MPI: TRUE (found version \"3.1\")\r\n  -- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include\r\n  -- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\n  -- Configuring done\r\n  -- Generating done\r\n  -- Build files have been written to: /tmp/pip-install-db1tsv5m/horovod/build/temp.linux-x86_64-3.6/gloo/tf\r\n  Scanning dependencies of target gloo\r\n  [  3%] Building CXX object gloo/CMakeFiles/gloo.dir/allgather.cc.o\r\n  [  6%] Building CXX object gloo/CMakeFiles/gloo.dir/algorithm.cc.o\r\n  [  9%] Building CXX object gloo/CMakeFiles/gloo.dir/allreduce.cc.o\r\n  [ 12%] Building CXX object gloo/CMakeFiles/gloo.dir/allgatherv.cc.o\r\n  [ 15%] Building CXX object gloo/CMakeFiles/gloo.dir/allreduce_local.cc.o\r\n  [ 18%] Building CXX object gloo/CMakeFiles/gloo.dir/barrier.cc.o\r\n  [ 21%] Building CXX object gloo/CMakeFiles/gloo.dir/broadcast.cc.o\r\n  [ 24%] Building CXX object gloo/CMakeFiles/gloo.dir/context.cc.o\r\n  [ 27%] Building CXX object gloo/CMakeFiles/gloo.dir/gather.cc.o\r\n  [ 30%] Building CXX object gloo/CMakeFiles/gloo.dir/reduce.cc.o\r\n  [ 33%] Building CXX object gloo/CMakeFiles/gloo.dir/scatter.cc.o\r\n  [ 36%] Building CXX object gloo/CMakeFiles/gloo.dir/types.cc.o\r\n  [ 39%] Building CXX object gloo/CMakeFiles/gloo.dir/common/logging.cc.o\r\n  [ 42%] Building CXX object gloo/CMakeFiles/gloo.dir/common/linux.cc.o\r\n  [ 45%] Building CXX object gloo/CMakeFiles/gloo.dir/mpi/context.cc.o\r\n  [ 48%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/context.cc.o\r\n  [ 51%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/file_store.cc.o\r\n  In file included from /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc:16:\r\n  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc: In destructor \u2018gloo::mpi::MPIScope::~MPIScope()\u2019:\r\n  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:141:58: warning: throw will always call terminate() [-Wterminate]\r\n             r.get_message_and_free(MakeString(__VA_ARGS__))); \\\r\n                                                            ^\r\n  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:150:3: note: in expansion of macro \u2018GLOO_ENFORCE_THAT_IMPL\u2019\r\n     GLOO_ENFORCE_THAT_IMPL(Equals((x), (y)), #x \" == \" #y, __VA_ARGS__)\r\n     ^~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc:43:3: note: in expansion of macro \u2018GLOO_ENFORCE_EQ\u2019\r\n     GLOO_ENFORCE_EQ(rv, MPI_SUCCESS);\r\n     ^~~~~~~~~~~~~~~\r\n  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:141:58: note: in C++11 destructors default to noexcept\r\n             r.get_message_and_free(MakeString(__VA_ARGS__))); \\\r\n                                                            ^\r\n  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:150:3: note: in expansion of macro \u2018GLOO_ENFORCE_THAT_IMPL\u2019\r\n     GLOO_ENFORCE_THAT_IMPL(Equals((x), (y)), #x \" == \" #y, __VA_ARGS__)\r\n     ^~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc:43:3: note: in expansion of macro \u2018GLOO_ENFORCE_EQ\u2019\r\n     GLOO_ENFORCE_EQ(rv, MPI_SUCCESS);\r\n     ^~~~~~~~~~~~~~~\r\n  [ 54%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/hash_store.cc.o\r\n  [ 57%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/prefix_store.cc.o\r\n  [ 60%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/store.cc.o\r\n  [ 63%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/address.cc.o\r\n  [ 66%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/buffer.cc.o\r\n  [ 69%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/context.cc.o\r\n  [ 72%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/device.cc.o\r\n  [ 75%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/pair.cc.o\r\n  [ 78%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/unbound_buffer.cc.o\r\n  [ 81%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/address.cc.o\r\n  [ 84%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/buffer.cc.o\r\n  [ 87%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/context.cc.o\r\n  [ 90%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/device.cc.o\r\n  [ 93%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/pair.cc.o\r\n  [ 96%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/unbound_buffer.cc.o\r\n  [100%] Linking CXX static library /tmp/pip-install-db1tsv5m/horovod/build/temp.linux-x86_64-3.6/lib/tf/libgloo.a\r\n  [100%] Built target gloo\r\n  building 'horovod.tensorflow.mpi_lib' extension\r\n  creating build/temp.linux-x86_64-3.6/horovod\r\n  creating build/temp.linux-x86_64-3.6/horovod/common\r\n  creating build/temp.linux-x86_64-3.6/horovod/common/ops\r\n  creating build/temp.linux-x86_64-3.6/horovod/common/optim\r\n  creating build/temp.linux-x86_64-3.6/horovod/common/utils\r\n  creating build/temp.linux-x86_64-3.6/horovod/common/mpi\r\n  creating build/temp.linux-x86_64-3.6/horovod/common/ops/adasum\r\n  creating build/temp.linux-x86_64-3.6/horovod/common/gloo\r\n  creating build/temp.linux-x86_64-3.6/horovod/tensorflow\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/common.cc -o build/temp.linux-x86_64-3.6/horovod/common/common.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/controller.cc -o build/temp.linux-x86_64-3.6/horovod/common/controller.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/fusion_buffer_manager.cc -o build/temp.linux-x86_64-3.6/horovod/common/fusion_buffer_manager.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/logging.cc -o build/temp.linux-x86_64-3.6/horovod/common/logging.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/message.cc -o build/temp.linux-x86_64-3.6/horovod/common/message.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/mpi/mpi_context.h:25,\r\n                   from horovod/common/operations.cc:47:\r\n  horovod/common/mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/parameter_manager.cc -o build/temp.linux-x86_64-3.6/horovod/common/parameter_manager.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  horovod/common/parameter_manager.cc: In member function \u2018virtual bool horovod::common::ParameterManager::BayesianParameter::IsDoneTuning() const\u2019:\r\n  horovod/common/parameter_manager.cc:466:21: warning: comparison of integer expressions of different signedness: \u2018const uint32_t\u2019 {aka \u2018const unsigned int\u2019} and \u2018const int\u2019 [-Wsign-compare]\r\n     return iteration_ > max_samples_;\r\n            ~~~~~~~~~~~^~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/response_cache.cc -o build/temp.linux-x86_64-3.6/horovod/common/response_cache.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/stall_inspector.cc -o build/temp.linux-x86_64-3.6/horovod/common/stall_inspector.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/thread_pool.cc -o build/temp.linux-x86_64-3.6/horovod/common/thread_pool.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/timeline.cc -o build/temp.linux-x86_64-3.6/horovod/common/timeline.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/tensor_queue.cc -o build/temp.linux-x86_64-3.6/horovod/common/tensor_queue.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/collective_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/collective_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/operation_manager.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/operation_manager.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/optim/bayesian_optimization.cc -o build/temp.linux-x86_64-3.6/horovod/common/optim/bayesian_optimization.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/optim/gaussian_process.cc -o build/temp.linux-x86_64-3.6/horovod/common/optim/gaussian_process.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/utils/env_parser.cc -o build/temp.linux-x86_64-3.6/horovod/common/utils/env_parser.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/half.cc -o build/temp.linux-x86_64-3.6/horovod/common/half.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/half.cc:16:\r\n  horovod/common/half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/mpi/mpi_context.cc -o build/temp.linux-x86_64-3.6/horovod/common/mpi/mpi_context.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/mpi/mpi_context.h:25,\r\n                   from horovod/common/mpi/mpi_context.cc:17:\r\n  horovod/common/mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/mpi/mpi_controller.cc -o build/temp.linux-x86_64-3.6/horovod/common/mpi/mpi_controller.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/mpi/mpi_context.h:25,\r\n                   from horovod/common/mpi/mpi_controller.h:19,\r\n                   from horovod/common/mpi/mpi_controller.cc:16:\r\n  horovod/common/mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/mpi_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/mpi_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/ops/../mpi/mpi_context.h:25,\r\n                   from horovod/common/ops/mpi_operations.h:27,\r\n                   from horovod/common/ops/mpi_operations.cc:17:\r\n  horovod/common/ops/../mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/ops/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/adasum/adasum_mpi.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/adasum/adasum_mpi.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/ops/adasum/../../mpi/mpi_context.h:25,\r\n                   from horovod/common/ops/adasum/adasum_mpi.h:21,\r\n                   from horovod/common/ops/adasum/adasum_mpi.cc:16:\r\n  horovod/common/ops/adasum/../../mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/ops/adasum/../../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/adasum_mpi_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/adasum_mpi_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/ops/adasum/../../mpi/mpi_context.h:25,\r\n                   from horovod/common/ops/adasum/adasum_mpi.h:21,\r\n                   from horovod/common/ops/adasum_mpi_operations.h:22,\r\n                   from horovod/common/ops/adasum_mpi_operations.cc:16:\r\n  horovod/common/ops/adasum/../../mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/ops/adasum/../../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/gloo_context.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/gloo_context.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/gloo/../mpi/mpi_context.h:25,\r\n                   from horovod/common/gloo/gloo_context.h:25,\r\n                   from horovod/common/gloo/gloo_context.cc:16:\r\n  horovod/common/gloo/../mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/gloo/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/gloo_controller.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/gloo_controller.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/gloo/../mpi/mpi_context.h:25,\r\n                   from horovod/common/gloo/gloo_context.h:25,\r\n                   from horovod/common/gloo/gloo_controller.h:19,\r\n                   from horovod/common/gloo/gloo_controller.cc:16:\r\n  horovod/common/gloo/../mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/gloo/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/http_store.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/http_store.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/memory_store.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/memory_store.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/gloo_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/gloo_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from horovod/common/ops/../gloo/../mpi/mpi_context.h:25,\r\n                   from horovod/common/ops/../gloo/gloo_context.h:25,\r\n                   from horovod/common/ops/gloo_operations.h:20,\r\n                   from horovod/common/ops/gloo_operations.cc:16:\r\n  horovod/common/ops/../gloo/../mpi/../half.h: In function \u2018void horovod::common::HalfBits2Float(short unsigned int*, float*)\u2019:\r\n  horovod/common/ops/../gloo/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *res = *reinterpret_cast<float const*>(&f);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  In file included from horovod/common/ops/gloo_operations.cc:22:\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = bool; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = bool]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: \u2018int\u2019 and \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wsign-compare]\r\n     for (auto i = 0; i < n; i++) {\r\n                      ~~^~~\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = double; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = double]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: \u2018int\u2019 and \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wsign-compare]\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = float; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = float]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: \u2018int\u2019 and \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wsign-compare]\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = gloo::float16; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = gloo::float16]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: \u2018int\u2019 and \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wsign-compare]\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = long int; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = long int]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: \u2018int\u2019 and \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wsign-compare]\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = int; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = int]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: \u2018int\u2019 and \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wsign-compare]\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = short int; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = short int]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: \u2018int\u2019 and \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wsign-compare]\r\n  third_party/gloo/gloo/math.h: In instantiation of \u2018void gloo::sum(void*, const void*, const void*, size_t) [with T = short unsigned int; size_t = long unsigned int]\u2019:\r\n  horovod/common/ops/gloo_operations.cc:69:10:   required from \u2018void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = short unsigned int]\u2019\r\n  horovod/common/ops/gloo_operations.h:44:8:   required from here\r\n ...............\r\n...............\r\n...............\r\n[horovod_install_fail.log](https://github.com/tensorflow/tensorflow/files/5107149/horovod_install_fail.log)\r\n\r\n                         ^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:373:25: note: in definition of macro \u2018MATCH_TYPE_AND_ENUM\u2019\r\n       struct DataTypeToEnum<TYPE> {                         \\\r\n                             ^~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:373:29: error: template argument 1 is invalid\r\n       struct DataTypeToEnum<TYPE> {                         \\\r\n                                 ^\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:1: note: in expansion of macro \u2018MATCH_TYPE_AND_ENUM\u2019\r\n     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);\r\n     ^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:21: error: \u2018bfloat16\u2019 was not declared in this scope\r\n     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);\r\n                         ^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:379:26: note: in definition of macro \u2018MATCH_TYPE_AND_ENUM\u2019\r\n       struct IsValidDataType<TYPE> {                        \\\r\n                              ^~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:21: note: suggested alternative: \u2018float_t\u2019\r\n     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);\r\n                         ^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:379:26: note: in definition of macro \u2018MATCH_TYPE_AND_ENUM\u2019\r\n       struct IsValidDataType<TYPE> {                        \\\r\n                              ^~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:379:30: error: template argument 1 is invalid\r\n       struct IsValidDataType<TYPE> {                        \\\r\n                                  ^\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:1: note: in expansion of macro \u2018MATCH_TYPE_AND_ENUM\u2019\r\n     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);\r\n     ^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:21: error: \u2018bfloat16\u2019 does not name a type; did you mean \u2018float_t\u2019?\r\n     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);\r\n                         ^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:384:13: note: in definition of macro \u2018MATCH_TYPE_AND_ENUM\u2019\r\n         typedef TYPE Type;                                  \\\r\n                 ^~~~\r\n    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:27,\r\n                     from horovod/tensorflow/mpi_ops.cc:24:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:224:27: error: expected \u2018)\u2019 before \u2018scalar_value\u2019\r\n       explicit Tensor(bfloat16 scalar_value)\r\n                      ~        ^~~~~~~~~~~~~\r\n                               )\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:225:47: error: expected unqualified-id before \u2018)\u2019 token\r\n           : Tensor(scalar_value, host_scalar_tag{}) {}\r\n                                                   ^\r\n    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../SpecialFunctions:63,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/Tensor:18,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:24,\r\n                     from horovod/tensorflow/mpi_ops.cc:24:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_i0e<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:208:44:   required from \u2018static Scalar Eigen::internal::bessel_i0e_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1892:54:   required from \u2018typename Eigen::internal::bessel_i0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_i0e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_i0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:21:69:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:120:27: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         return pselect(pcmp_le(y, pset1<T>(8.0f)), y_le_eight, y_gt_eight);\r\n                        ~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:96:17: warning: unused variable \u2018A\u2019 [-Wunused-variable]\r\n         const float A[] = {-1.30002500998624804212E-8f, 6.04699502254191894932E-8f,\r\n                     ^\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_i1e<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:403:44:   required from \u2018static Scalar Eigen::internal::bessel_i1e_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1904:54:   required from \u2018typename Eigen::internal::bessel_i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_i1e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:29:69:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:313:24: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         y = pselect(pcmp_le(y, pset1<T>(8.0f)), y_le_eight, y_gt_eight);\r\n                     ~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:314:27: error: \u2018pcmp_lt\u2019 was not declared in this scope\r\n         return pselect(pcmp_lt(x, pset1<T>(0.0f)), pnegate(y), y);\r\n                        ~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_j0<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1283:43:   required from \u2018static Scalar Eigen::internal::bessel_j0_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1934:53:   required from \u2018typename Eigen::internal::bessel_j0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_j0(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_j0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:32:68:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1168:16: error: \u2018pcmp_lt\u2019 was not declared in this scope\r\n             pcmp_lt(y, pset1<T>(1.0e-3f)),\r\n             ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1177:27: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         return pselect(pcmp_le(y, pset1<T>(2.0)), y_le_two, y_gt_two);\r\n                        ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_j1<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1672:43:   required from \u2018static Scalar Eigen::internal::bessel_j1_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1946:53:   required from \u2018typename Eigen::internal::bessel_j1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_j1(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_j1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:36:68:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1575:16: error: \u2018pcmp_lt\u2019 was not declared in this scope\r\n             pcmp_lt(x, pset1<T>(0.0f)), pnegate(y_gt_two), y_gt_two);\r\n             ~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1576:27: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         return pselect(pcmp_le(y, pset1<T>(2.0f)), y_le_two, y_gt_two);\r\n                        ~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_y0<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1481:43:   required from \u2018static Scalar Eigen::internal::bessel_y0_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1940:53:   required from \u2018typename Eigen::internal::bessel_y0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_y0(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_y0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:40:68:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:31: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1380:27: error: \u2018pcmp_le\u2019 was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]\r\n         return pselect(pcmp_le(x, pset1<T>(2.0)), x_le_two, x_gt_two);\r\n                        ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:31: note: \u2018pcmp_le\u2019 declared here, later in the translation unit\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_y1<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1875:43:   required from \u2018static Scalar Eigen::internal::bessel_y1_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1952:53:   required from \u2018typename Eigen::internal::bessel_y1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_y1(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_y1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:44:68:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1766:31: error: \u2018pcmp_lt\u2019 was not declared in this scope\r\n         x_le_two = pselect(pcmp_lt(x, pset1<T>(0.0f)), NEG_MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1774:27: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         return pselect(pcmp_le(x, pset1<T>(2.0)), x_le_two, x_gt_two);\r\n                        ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_k0<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:761:43:   required from \u2018static Scalar Eigen::internal::bessel_k0_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1910:53:   required from \u2018typename Eigen::internal::bessel_k0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k0(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:47:68:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:31: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:675:27: error: \u2018pcmp_le\u2019 was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]\r\n         return pselect(pcmp_le(x, two), x_le_two, x_gt_two);\r\n                        ~~~~~~~^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:31: note: \u2018pcmp_le\u2019 declared here, later in the translation unit\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:651:17: warning: unused variable \u2018A\u2019 [-Wunused-variable]\r\n         const float A[] = {1.90451637722020886025E-9f, 2.53479107902614945675E-7f,\r\n                     ^\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_k0e<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:589:44:   required from \u2018static Scalar Eigen::internal::bessel_k0e_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1916:54:   required from \u2018typename Eigen::internal::bessel_k0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k0e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:51:69:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:502:16: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n             pcmp_le(x, pset1<T>(0.0)),\r\n             ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:504:24: error: \u2018pcmp_le\u2019 was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]\r\n             pselect(pcmp_le(x, two), x_le_two, x_gt_two));\r\n                     ~~~~~~~^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:502:16: note: \u2018pcmp_le\u2019 declared here, later in the translation unit\r\n             pcmp_le(x, pset1<T>(0.0)),\r\n             ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:479:17: warning: unused variable \u2018A\u2019 [-Wunused-variable]\r\n         const float A[] = {1.90451637722020886025E-9f, 2.53479107902614945675E-7f,\r\n                     ^\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_k1<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1083:43:   required from \u2018static Scalar Eigen::internal::bessel_k1_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1922:53:   required from \u2018typename Eigen::internal::bessel_k1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k1(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:55:68:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:31: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:999:27: error: \u2018pcmp_le\u2019 was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]\r\n         return pselect(pcmp_le(x, two), x_le_two, x_gt_two);\r\n                        ~~~~~~~^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:31: note: \u2018pcmp_le\u2019 declared here, later in the translation unit\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of \u2018static T Eigen::internal::generic_k1e<T, float>::run(const T&) [with T = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:917:44:   required from \u2018static Scalar Eigen::internal::bessel_k1e_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1928:54:   required from \u2018typename Eigen::internal::bessel_k1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k1e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:59:69:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:31: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:838:27: error: \u2018pcmp_le\u2019 was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]\r\n         return pselect(pcmp_le(x, two), x_le_two, x_gt_two);\r\n                        ~~~~~~~^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:31: note: \u2018pcmp_le\u2019 declared here, later in the translation unit\r\n         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);\r\n                            ~~~~~~~^~~~~~~~~~~~~~~~~~\r\n    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../SpecialFunctions:69,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/Tensor:18,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:24,\r\n                     from horovod/tensorflow/mpi_ops.cc:24:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of \u2018T Eigen::internal::generic_ndtri(const T&) [with T = float; ScalarType = float]\u2019:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:652:41:   required from \u2018static Scalar Eigen::internal::ndtri_impl<Scalar>::run(Scalar) [with Scalar = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1989:49:   required from \u2018typename Eigen::internal::ndtri_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::ndtri(const Scalar&) [with Scalar = float; typename Eigen::internal::ndtri_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]\u2019\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsHalf.h:34:64:   required from here\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:616:28: error: \u2018pcmp_le\u2019 was not declared in this scope\r\n       should_flipsign = pcmp_le(a, psub(one, exp_neg_two));\r\n                         ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:617:14: error: cannot convert \u2018const float\u2019 to \u2018fd_set*\u2019\r\n       b = pselect(should_flipsign, a, psub(one, a));\r\n           ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    In file included from /usr/include/x86_64-linux-gnu/sys/types.h:197,\r\n                     from /usr/include/stdlib.h:394,\r\n                     from /usr/include/c++/8/cstdlib:75,\r\n                     from /usr/include/c++/8/ext/string_conversions.h:41,\r\n                     from /usr/include/c++/8/bits/basic_string.h:6400,\r\n                     from /usr/include/c++/8/string:52,\r\n                     from /usr/include/c++/8/stdexcept:39,\r\n                     from /usr/include/c++/8/array:39,\r\n                     from /usr/include/c++/8/tuple:39,\r\n                     from /usr/include/c++/8/bits/unique_ptr.h:37,\r\n                     from /usr/include/c++/8/memory:80,\r\n                     from horovod/tensorflow/mpi_ops.cc:18:\r\n    /usr/include/x86_64-linux-gnu/sys/select.h:113:52: note:   initializing argument 2 of \u2018int pselect(int, fd_set*, fd_set*, fd_set*, const timespec*, const __sigset_t*)\u2019\r\n     extern int pselect (int __nfds, fd_set *__restrict __readfds,\r\n                                     ~~~~~~~~~~~~~~~~~~~^~~~~~~~~\r\n    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../SpecialFunctions:69,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/Tensor:18,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,\r\n                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:24,\r\n                     from horovod/tensorflow/mpi_ops.cc:24:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:620:14: error: \u2018pcmp_lt\u2019 was not declared in this scope\r\n           pcmp_lt(exp_neg_two, b),\r\n           ~~~~~~~^~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:625:14: error: \u2018pcmp_le\u2019 was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]\r\n           pcmp_le(a, zero), neg_maxnum,\r\n           ~~~~~~~^~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:616:28: note: \u2018pcmp_le\u2019 declared here, later in the translation unit\r\n       should_flipsign = pcmp_le(a, psub(one, exp_neg_two));\r\n                         ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:626:22: error: \u2018pcmp_le\u2019 was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]\r\n           pselect(pcmp_le(one, a), maxnum, ndtri));\r\n                   ~~~~~~~^~~~~~~~\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:616:28: note: \u2018pcmp_le\u2019 declared here, later in the translation unit\r\n       should_flipsign = pcmp_le(a, psub(one, exp_neg_two));\r\n                         ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    error: command '/usr/bin/gcc' failed with exit status 1\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-db1tsv5m/horovod/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-db1tsv5m/horovod/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-vpg897nc/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.6/horovod Check the logs for full command output.\r\n``` \r\n\r\nHere is how I prepared my environment:\r\nOn Ubuntu 18.04 Docker with Python3.6 and Pip3.6 after I install the nightly wheel, I run these commands:\r\n\r\n```\r\nexport HOROVOD_WITHOUT_PYTORCH=1\r\nexport HOROVOD_WITHOUT_MXNET=1\r\nexport HOROVOD_WITH_TENSORFLOW=1\r\n\r\napt-get update -y\r\napt-get install gcc-8 g++-8 cmake python-tk -y\r\nupdate-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 700 --slave /usr/bin/g++ g++ /usr/bin/g++-7\r\nupdate-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 800 --slave /usr/bin/g++ g++ /usr/bin/g++-8\r\napt-get install -y libsm6 libxext6\r\napt-get install google-perftools -y\r\napt-get install openmpi-bin openmpi-common openssh-client openssh-server libopenmpi-dev -y\r\npip install --no-cache-dir horovod==0.19.1\r\n```\r\nand that throws the above error.\r\n\r\nI have tried with all wheels from 08/16 all the way up to 08/20 and they all fail but 08/15 wheel seem to work.\r\n\r\nI've done a bit of research and seems like the regression was introduced somewhere between:\r\nhttps://github.com/tensorflow/tensorflow/commit/0ec146da5fee12617a944127f18998acd74414d8\r\nand\r\nhttps://github.com/tensorflow/tensorflow/commit/58de5563920eaf3d1454e924270c8f9b140301fe\r\n\r\nBut I'm not sure about that.", "comments": ["[horovod_install_fail.log](https://github.com/tensorflow/tensorflow/files/5111400/horovod_install_fail.log)\r\n", "@gunan any updates on this one?", "Looks like there is an eigen compilation issue?\r\nSorry, not an expert on this, thus removed my assignment. @jvishnuvardhan could you find another owner for this bug?\r\nSomeone who worked with horovod, custom op and eigen code?", "I submitted a similar issue in Horovod side and seems it has been fixed: https://github.com/horovod/horovod/issues/2232", "Thanks @Zantares \r\nLet me try that and will update here.", "Thanks @Zantares and @gunan \r\nI upgraded `Horovod` to [0.20.0](https://pypi.org/project/horovod/0.20.0/) and was able to successfully install it on top of `TF` nightly.\r\n\r\nClosing this now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42547\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42547\">No</a>\n"]}, {"number": 42546, "title": "Mixed precision training on TensorFlow 2.3 doesn't speed-up training progress as TensorFlow 2.2 does", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0/2.3.0\r\n- Python version: 3.6/3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda 10.1, cuDNN 7.6.5\r\n- GPU model and memory: 2080Ti/12Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nMixed precision training on tensorflow 2.3 doesn't speed-up training progress as Tensorflow 2.2 does \r\n\r\n**Describe the expected behavior**\r\n\r\nfaster or at least equal\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[nmt_with_attention.zip](https://github.com/tensorflow/tensorflow/files/5106684/nmt_with_attention.zip)\r\n\r\nI modify a bit based on tensorflow offcial tutorial nmt here (https://www.tensorflow.org/tutorials/text/nmt_with_attention). You can see the flag `IS_MIXED_PRECISION` in the head of the notebook, let change it to True/False to check by urself.\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5106690/log.txt)\r\n\r\nhere is my summary about the time consuming for tensorflow2.2/2.3 on both mixex precision and non-mixed precision.\r\n\r\nI have many model on my framework TensorFlowTTS (https://github.com/TensorSpeech/TensorFlowTTS). All of my model training on mixed precsion Tensorflow2.3 have this problem. The above notebook is just example. \r\n\r\nBTW, colab gpu haven't support mixed precision so to run this notebook require a local GPU to reproduce my issue. \r\n\r\n", "comments": ["@reedwm seem you are the person working on mixed precision :)). ", "Don't know if this is useful info for you but in Tensorflow 2.3.0 they removed the possibility to disable Tensor Cores in CUBLAS / GEMM functions (this used to be possible via environment variables). So it is currently always ON. ", "what's the latest on this issue?", "Could you please check the performance in Tensorflow 2.7 version and update if there is still performance issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42545, "title": "Tensorflow V2.2.0 boot fail on aarch64", "body": "When I run a testcase on my `aarch64` platform, it reports error as:\r\n```\r\ntensorflow/core/platform/profile_utils/cpu_utils.cc:106] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency\r\n```\r\n\r\nAnd I found `core/platform/profile_utils/cpu_utils.cc` not support `aarch64` yet as follows:\r\n```\r\n#if (defined(__powerpc__) || \\\r\n     defined(__ppc__) && (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__))\r\n    retval = sscanf(line.c_str(), \"clock              : %lfMHz\", &cpu_freq);\r\n    freq_factor = 1.0;\r\n#else\r\n    retval = sscanf(line.c_str(), \"bogomips : %lf\", &cpu_freq);\r\n#endif\r\n    if (retval > 0) {\r\n      const double freq_ghz = cpu_freq / 1000.0 / freq_factor;\r\n      if (retval != 1 || freq_ghz < 0.01) {\r\n        LOG(WARNING) << \"Failed to get CPU frequency: \" << freq_ghz << \" GHz\";\r\n        return INVALID_FREQUENCY;\r\n      }\r\n      const int64 freq_n =\r\n          static_cast<int64>(freq_ghz * 1000.0 * 1000.0 * 1000.0);\r\n      LOG(INFO) << \"CPU Frequency: \" << freq_n << \" Hz\";\r\n      return freq_n;\r\n    }\r\n  }\r\n  LOG(WARNING)\r\n      << \"Failed to find bogomips or clock in /proc/cpuinfo; cannot determine \"\r\n         \"CPU frequency\";\r\n  return INVALID_FREQUENCY;\r\n```\r\n\r\nBut my `proc/cpuinfo` is\r\n```\r\nBogoMIPS        : xxx.xx\r\n```\r\nwhich not match `bogomips : %lf`\r\n\r\nCould anyone help to fix this issue?", "comments": ["@darmac,\r\nIn order to expedite the trouble-shooting process, could you please fill in the below template and provide the exact sequence of commands / steps that you executed before running into the problem?\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nThanks!", "OS Platform and Distribution: `CentOS Linux 8`\r\nMobile device if the issue happens on mobile device: `N/A (TaiShan2280 V2 Server)`\r\nTensorFlow installed from (source or binary): `source`\r\nTensorFlow version: `V2.2.0`\r\nPython version: `3.6.3`\r\nInstalled using: `pip`\r\nBazel version (if compiling from source): `2.0.0`\r\nGCC/Compiler version (if compiling from source): `8.2.1`\r\nCUDA/cuDNN version: `N/A`\r\nGPU model and memory: `N/A`", "@darmac,\r\nA similar issue [#39185](https://github.com/tensorflow/tensorflow/issues/39185), was fixed in TensorFlow v2.3. Could you please update TensorFlow to v2.3 and check if you are facing the same issue?\r\n\r\nAlso, please provide the exact sequence of commands or the code that you executed before running into the error? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Got it, I have not test the latest version yet. Thanks.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Got it.", "> Got it, I have not test the latest version yet. Thanks.\r\n\r\n@darmac,\r\nIn this case, can we close the issue. Please feel free to re-open the issue when you have updates regarding it. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42545\">No</a>\n"]}, {"number": 42544, "title": "[Estimator] bazel test failures", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: r2.3\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2/7\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nEstimator bazel test failures\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel test //tensorflow_estimator/... --test_output=errors --verbose_failures=true\r\n  --keep_going --test_verbose_timeout_warnings\r\n\r\n**Any other info / logs**\r\n//tensorflow_estimator/python/estimator:rnn_test                         FAILED in 197.2s\r\n//tensorflow_estimator/python/estimator:distribute_strategy_estimator_training_test FAILED in 48 out of 48 in 13.4s\r\n//tensorflow_estimator/python/estimator:distribute_strategy_estimator_training_test_gpu FAILED in 48 out of 48 in 10.1s\r\n\r\n-------------------------------------------------------------------\r\nrnn_test failues:\r\n[  FAILED  ] RNNClassifierEvaluationTest.testMultiClassEvaluationMetrics\r\n[  FAILED  ] RNNClassifierPredictionTest.testBinaryClassPredictions\r\n[  FAILED  ] RNNClassifierPredictionTest.testBinaryClassPredictionsSequential\r\n[  FAILED  ] RNNClassifierPredictionTest.testMultiClassPredictions\r\n[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromCheckpoint\r\n[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromCheckpointSequential\r\n[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromCheckpointSequentialWithWeights\r\n[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromScratchWithDefaultOptimizer\r\n[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassWithExampleWeight\r\n[  FAILED  ] RNNClassifierTrainingTest.testDefaultGradientClipping\r\n[  FAILED  ] RNNClassifierTrainingTest.testFromScratchWithCustomRNNCellFn\r\n[  FAILED  ] RNNClassifierTrainingTest.testMultiClassFromCheckpoint\r\n[  FAILED  ] RNNClassifierTrainingTest.testMultiClassFromScratchWithDefaultOptimizer\r\n[  FAILED  ] RNNClassifierTrainingTest.testMultiClassWithExampleWeight\r\n[  FAILED  ] RNNLogitFnTest.testMultiDimLogitsSequential\r\n[  FAILED  ] RNNLogitFnTest.testMultiDimLogitsStatic\r\n[  FAILED  ] RNNLogitFnTest.testOneDimLogitsSequential\r\n[  FAILED  ] RNNLogitFnTest.testOneDimLogitsSequentialInfer\r\n[  FAILED  ] RNNLogitFnTest.testOneDimLogitsSequentialTrain\r\n[  FAILED  ] RNNLogitFnTest.testOneDimLogitsStatic\r\n[  FAILED  ] RNNLogitFnTest.testTrainingMode('train', True)\r\n[  FAILED  ] RNNLogitFnTest.testTrainingMode('eval', False)\r\n[  FAILED  ] RNNLogitFnTest.testTrainingMode('infer', False)\r\n\r\nMore details on testOneDimLogitsSequentialTrain failure\r\n======================================================================\r\nERROR: testOneDimLogitsSequentialTrain (__main__.RNNLogitFnTest)\r\ntestOneDimLogitsSequentialTrain (__main__.RNNLogitFnTest)\r\ntestOneDimLogitsSequentialTrain(return_sequences=True, expected_logits=[[[-1.4388], [-0.6033]]], training=True)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py\", line 1178, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/testing/parameterized.py\", line 263, in bound_param_test\r\n    test_method(self, **testcase_params)\r\n  File \"/root/.cache/bazel/_bazel_root/d226403c339549ea2db666353bf31144/sandbox/processwrapper-sandbox/902/execroot/org_tensorflow_estimator/bazel-out/k8-fastbuild/bin/tensorflow_estimator/python/estimator/rnn_test.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/canned/rnn_test.py\", line 325, in testOneDimLogits\r\n    training=training)\r\n  File \"/root/.cache/bazel/_bazel_root/d226403c339549ea2db666353bf31144/sandbox/processwrapper-sandbox/902/execroot/org_tensorflow_estimator/bazel-out/k8-fastbuild/bin/tensorflow_estimator/python/estimator/rnn_test.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/canned/rnn_test.py\", line 268, in _test_logits\r\n    logits = logit_layer(features_fn(), training=training)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 776, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 258, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    /root/.cache/bazel/_bazel_root/d226403c339549ea2db666353bf31144/sandbox/processwrapper-sandbox/902/execroot/org_tensorflow_estimator/bazel-out/k8-fastbuild/bin/tensorflow_estimator/python/estimator/rnn_test.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/canned/rnn.py:221 call  *\r\n        rnn_outputs = self._rnn_layer(\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py:663 __call__  **\r\n        return super(RNN, self).__call__(inputs, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:776 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py:1573 call\r\n        inputs, mask=mask, training=training, initial_state=initial_state)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py:807 call\r\n        zero_output_for_mask=self.zero_output_for_mask)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4327 rnn\r\n        **while_loop_kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:2696 while_loop\r\n        back_prop=back_prop)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/while_v2.py:229 while_loop\r\n        len_orig_loop_vars], expand_composites=True))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/while_v2.py:1151 _check_shapes_compat\r\n        \"specify a less-specific shape.\" % (input_t.name, shape, t.shape))\r\n\r\n    ValueError: Input tensor 'rnn_model/simple_rnn/zeros_like:0' enters the loop with shape (1, 2), but has shape (None, 2) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\r\n-------------------------------------------------------------------------\r\nThe distribute_strategy_estimator_training_test.* failed with the below error in the logs.\r\n[  FAILED  ] setUpClass (__main__.DistributeCoordinatorIntegrationTest)\r\nWe tried to use portpicker but ran into segmentation failures.\r\n-------------------------------------------------------------------\r\n", "comments": ["@satishpasumarthi \r\nCan you please use shape_invariants as per [this link](https://stackoverflow.com/questions/51395855/how-to-modify-a-tf-variable-using-while-loop-iterator/51398249) (also this [link](https://stackoverflow.com/questions/41604686/how-to-use-tf-while-loop-for-variable-length-inputs-in-tensorflow) )\r\n\r\nCan you please provide with the stand alone code that lead to this error, or share colab gist with the error.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "sure @Saduf2019 . let me get that info.", "@satishpasumarthi\r\nPlease update.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42543, "title": "[tflite] enable passing int32 elementray arith to NNAPI", "body": "NNAPI 1.3 (Android API level 30) [1] supports int32 add, sub, mul, and div. This patch enables passing +-*/ with int32 tensors to NNAPI. With that, we can do something like\r\n```\r\n./mul_test   --gtest_filter=IntegerMulOpTest.NoActivation --use_nnapi=1\r\n```\r\non Android R devices, such as Pixel 4 running Android R beta.\r\n\r\n[1] https://developer.android.com/ndk/reference/group/neural-networks", "comments": ["@freedomtan this changes are breaking this test https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/add_test.cc#L208 , can you please check ?", "@miaowang14 and @rthadur: I reverted DIV part and added activation function check, and ran add, sub, mul kernel tests with `--with_nnapi=1`"]}, {"number": 42542, "title": "WARNING:tensorflow:Gradients do not exist for variables  when minimizing the loss.", "body": "Hello \r\n\r\nI am trying to run a tutorial of ES-RNN code found in this link. [timeseries forcasting tutorial](https://github.com/Azure/DeepLearningForTimeSeriesForecasting/blob/master/4_ES_RNN.ipynb)\r\n\r\nWhen I run this in a conda environment where I installed required packages (tensorflow= 1.12.0, keras==2.2.4, etc), I was able to run model.fit function successfully.\r\n\r\nHowever I wanted to run this in recent tensorflow version (2.2) and so I created another conda environment. Also I made minor changes in the ES class. For example, instead of `from keras import backend as K` , I used `from tensorflow.keras import backend as K`\r\n\r\nAlso there is no `K.slice` function so I used `tf.slice` instead.\r\n\r\nHere is the original ES class code written in tensorflow=1.12.0 and keras=2.2.4. \r\n\r\n```\r\nfrom keras import backend as K\r\nfrom keras.layers import Layer\r\nfrom keras import initializers\r\n\r\nclass ES(Layer):\r\n\r\n    def __init__(self, horizon, m, batch_size, time_steps, **kwargs):\r\n        self.horizon = horizon\r\n        self.m = m\r\n        self.batch_size = batch_size\r\n        self.time_steps = time_steps\r\n        \r\n        super(ES, self).__init__(**kwargs)\r\n\r\n    # initialization of the learned parameters of exponential smoothing\r\n    def build(self, input_shape):\r\n        self.alpha = self.add_weight(name='alpha', shape=(1,),\r\n                                     initializer='uniform', trainable=True)\r\n        self.gamma = self.add_weight(name='gamma', shape=(1,),\r\n                                     initializer='uniform', trainable=True)\r\n        self.init_seasonality = self.add_weight(name='init_seasonality', shape=(self.m,),\r\n                                                initializer=initializers.Constant(value=0.8), trainable=True)\r\n        self.init_seasonality_list = [K.slice(self.init_seasonality,(i,),(1,)) for i in range(self.m)]\r\n        self.seasonality_queue = deque(self.init_seasonality_list, self.m)\r\n        self.level = self.add_weight(name='init_level', shape=(1,),\r\n                                     initializer=initializers.Constant(value=0.8), \r\n                                     trainable=True)\r\n        super(ES, self).build(input_shape)  \r\n\r\n    def call(self, x):\r\n\r\n        # extract time-series from feature vector\r\n        n_examples = K.int_shape(x)[0]\r\n        if n_examples is None:\r\n            n_examples = self.batch_size\r\n        x1 = K.slice(x,(0,0,0),(1,self.time_steps,1))\r\n        x1 = K.reshape(x1,(self.time_steps,))\r\n        x2 = K.slice(x,(1,self.time_steps-1,0),(n_examples-1,1,1))\r\n        x2 = K.reshape(x2,(n_examples-1,))\r\n        ts = K.concatenate([x1,x2])\r\n        \r\n        x_norm = []  # normalized values of time-series\r\n        ls = []      # coeffients for denormalization of forecasts\r\n        \r\n        l_t_minus_1 = self.level\r\n        \r\n        for i in range(n_examples+self.time_steps-1):\r\n        \r\n            # compute l_t\r\n            y_t = ts[i]\r\n            s_t = self.seasonality_queue.popleft()\r\n            l_t = self.alpha * y_t / s_t + (1 - self.alpha) * l_t_minus_1\r\n            \r\n            # compute s_{t+m}\r\n            s_t_plus_m = self.gamma * y_t / l_t + (1 - self.gamma) * s_t\r\n            \r\n            self.seasonality_queue.append(s_t_plus_m)\r\n            \r\n            # normalize y_t\r\n            x_norm.append(y_t / (s_t * l_t))\r\n\r\n            l_t_minus_1 = l_t\r\n\r\n            if i >= self.time_steps-1:\r\n                l = [l_t]*self.horizon\r\n                l = K.concatenate(l)\r\n                s = [self.seasonality_queue[i] for i in range(self.horizon)] # we assume here that horizon < m\r\n                s = K.concatenate(s)\r\n                ls_t = K.concatenate([K.expand_dims(l), K.expand_dims(s)])\r\n                ls.append(K.expand_dims(ls_t,axis=0))  \r\n       \r\n        self.level = l_t\r\n        x_norm = K.concatenate(x_norm)\r\n\r\n        # create x_out\r\n        x_out = []\r\n        for i in range(n_examples):\r\n            norm_features = K.slice(x_norm,(i,),(self.time_steps,))\r\n            norm_features = K.expand_dims(norm_features,axis=0)\r\n            x_out.append(norm_features)\r\n\r\n        x_out = K.concatenate(x_out, axis=0)\r\n        x_out = K.expand_dims(x_out)\r\n\r\n        # create tensor of denormalization coefficients \r\n        denorm_coeff = K.concatenate(ls, axis=0)\r\n        return [x_out, denorm_coeff]\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return [(input_shape[0], input_shape[1], input_shape[2]), (input_shape[0], self.horizon, 2)]\r\n    \r\nclass Denormalization(Layer):\r\n    \r\n    def __init__(self, **kwargs):\r\n        super(Denormalization, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        super(Denormalization, self).build(input_shape)  \r\n\r\n    def call(self, x):\r\n        return x[0] * x[1][:,:,0] * x[1][:,:,1]\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape[0]\r\n````\r\n\r\nHowever, the code failed to run at TF2.2 and the error message is below:\r\n\r\n```\r\nTrain on 23328 samples, validate on 1488 samples\r\nEpoch 1/10\r\nWARNING:tensorflow:Gradients do not exist for variables ['es/init_seasonality:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['es/init_seasonality:0'] when minimizing the loss.\r\n   48/23328 [..............................] - ETA: 4:05:37WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-25-a1542188ddf7> in <module>\r\n      6           validation_data=(valid_inputs['X'], valid_inputs['target']),\r\n      7           callbacks=[earlystop],\r\n----> 8           verbose=1)\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     74           \"Inputs to eager execution function cannot be Keras symbolic \"\r\n     75           \"tensors, but found {}\".format(keras_symbolic_tensors))\r\n---> 76     raise e\r\n     77   # pylint: enable=protected-access\r\n     78   return tensors\r\n\r\n~\\Miniconda3\\envs\\TF2p2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     59     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     60                                                op_name, inputs, attrs,\r\n---> 61                                                num_outputs)\r\n     62   except core._NotOkStatusException as e:\r\n     63     if name is not None:\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: model/es/add_59:0\r\n```\r\n\r\nCould you please let me know what part of the code should be modified to be compatible with tensorflow 2.X ? \r\n\r\nAlso model.summary shows the batch size (48) in es, gru and dense layer in TF2.X but it was \"None\" in the original tutorial. \r\n```\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, None, 1)]    0                                            \r\n__________________________________________________________________________________________________\r\nes (ES)                         [(48, 6, 1), (48, 3, 26          input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ngru (GRU)                       (48, 5)              120         es[0][0]                         \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (48, 3)              18          gru[0][0]                        \r\n__________________________________________________________________________________________________\r\ndenormalization (Denormalizatio (48, 3)              0           dense[0][0]                      \r\n                                                                 es[0][1]                         \r\n==================================================================================================\r\nTotal params: 164\r\nTrainable params: 164\r\nNon-trainable params: 0\r\n\r\n```\r\n\r\n\r\nThank you. ", "comments": ["@swsyoon \r\n\r\nCan you change the code as below and see. If the problem still persists can you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\n```\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as k\r\nfrom tensorflow.keras.layers import layer\r\nfrom tensorflow.python.keras import initializers\r\n```", "@ravikyram \r\n\r\nYes, that's what I tried but it didn't work. \r\n\r\nHere is the colab link for the jupyter file and data file used. \r\n\r\nhttps://drive.google.com/file/d/1nR0PiPPbYNNOp8EmpbXudcBmtumNY4jD/view?usp=sharing\r\n\r\n[GEFCom2014-E.xlsx](https://github.com/tensorflow/tensorflow/files/5109376/GEFCom2014-E.xlsx)\r\n\r\n", "Hi\r\nEven I am facing the same issue , while doing a model.fit. Gradients do not exist for variables in tf2.2.0\r\n ", "@swsyoon Can you please post it in Stackoverflow? GitHub is mainly for bug and performance related issues. There is a big community in Stackoverflow who support this kind of support questions. Thanks!", "@jvishnuvardhan , I have posted  this on the Stackoverflow . Please refer the link here https://stackoverflow.com/questions/64552543/tensorflow-2-2-0-warningtensorflowgradients-do-not-exist-for-variables-when.\r\nAlso could you please let me know , in this case why is it displayed as a warning , when ideally it should be an error because if gradients don't exists then weights will not update , the training will not happen.\r\nThanks", "Hi @jayaBalaR, this is a warning and not an error because depending on your use case having non trainable variables may be the desired outcome (eg fine tuning). You can [find the warning here](https://github.com/tensorflow/tensorflow/blob/1435e86cb651f4a8309f4a8ca23e1c23b25a3845/tensorflow/python/keras/optimizer_v2/utils.py#L81) in the source code.\r\n\r\nThe error message you're seeing `TypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. ` is a bit of a confusing one, but generally this means that you're trying to access a graph tensor in an eager context. In your case this is potentially caused by returning lists of tensors and then trying to access the tensors in that list. I don't know for sure, as there is a lot of custom code here.\r\n\r\nAs @jvishnuvardhan said Github is for bugs/performance issues. There does not seem to be a bug here and there is too much custom code for us to look through to find what is causing the error.", "Hi @nikitamaia , thank you very much for patiently reading my query and replying. basically I am trying to implement a custom loss function which is a complex loss sum of 3 individual losses, using the https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss example code.  After implementing loss using the class example detailed in this blog when I do a model.fit and not using auto differentiator available in tensorflow , I get the above error. maybe as per your analysis , there are few non trainable variables which not giving proper values. Do you have any example code reference in tensorflow which implements a custom loss function which could help me.\r\nthanks once again to you and @jvishnuvardhan.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@jayaBalaR [Here](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses) is one of the good  guide on custom losses. You could also search some medium articles that provide more examples of custom losses. Thanks!", "Thank you"]}, {"number": 42541, "title": "Split _cache_key", "body": "", "comments": ["Benchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 144 -> 147 us\r\ndefun_matmul_2_by_2_CPU_async: 80.5 -> 80.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 198 -> 194 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 163.5 -> 162 us\r\ndefun_matmul_2_by_2_CPU_async: 85 -> 85 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 207.5 -> 214 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 163.5-> 163 us\r\ndefun_matmul_2_by_2_CPU_async: 82.5 -> 86 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 206 -> 201 us\r\n\r\nInconclusive", "Updated:\r\nBenchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 144 -> 145.5 us\r\ndefun_matmul_2_by_2_CPU_async: 80.5 -> 80 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 198 -> 187 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 163.5 -> 153 us\r\ndefun_matmul_2_by_2_CPU_async: 85 -> 82.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 207.5 -> 195 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 163.5-> 151 us\r\ndefun_matmul_2_by_2_CPU_async: 82.5 -> 82.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 206 -> 194 us\r\n\r\nApparent improvement"]}, {"number": 42540, "title": "CallCounter C++ version", "body": "Feedback on C++ conventions and style particularly welcome", "comments": ["Benchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 145.5 -> 150.5 us\r\ndefun_matmul_2_by_2_CPU_async: 81 -> 81 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 194.5 -> 202 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 156.5 -> 166 us\r\ndefun_matmul_2_by_2_CPU_async: 84.5 -> 84 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 219 -> 221 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 155-> 167 us\r\ndefun_matmul_2_by_2_CPU_async: 84 -> 83 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 207.5 -> 217 us\r\n\r\nApparent slowdown", "Could you try benchmarking just `called_without_tracing()` calls, C++ vs Python?", "Ran ad hoc microbenchmarks for `called_without_tracing()` using timeit; not a significant difference between C++ and Python; if anything, C++ is faster by a hair\r\n\r\nMinimums (total over X iters): Python -> C++\r\n30000 iters: .0094 s -> .0085 s\r\n100000 iters: .033 s -> .0297 s", "@jonathanchu33 please feel free to leave as it is, You're not expected to work beyond your internship period. :)", "@kkimdev, @jonathanchu33  Any update on this PR? Please. Thanks!", "Status: This PR itself doesn't have much perf benefit unless `_FrequentTracingDetector` is also migrated to C++ together. So marking this P2 and we will revisit after we solve other tf.function overheads.", "@kkimdev  Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!", "@kkimdev Any update on this PR? Please. Thanks!"]}, {"number": 42539, "title": "Add the `size` API for TensorFlow NumPy operations.", "body": "The equivalent vanilla NumPy operation: https://numpy.org/doc/stable/reference/generated/numpy.ndarray.size.html", "comments": ["By the way, Peng @wangpengmit and Akshay @akshaym , this one I committed yesterday is ready for review!", "Hi Akshay @akshaym , I have revised the places as you suggested. I also tried to add tests for the `None` shape case. However, when I tried to define a `TensorSpec` as you suggested and converted using `asarray` wrapper, it always gives me the error `TypeError: data type not understood`.", "Hi @DarrenZhang01, `asarray` cannot wrap a TensorSpec.\r\n\r\nIf you want to test the code for partially defined shapes, you can try something like the following (copying the code I shared previously):\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\r\ndef f(x):\r\n  x = tnp.asarray(x)  # Wraps a graph tensor into a tnp.ndarray\r\n  return tnp.size(x)  # This should fail since the shape is \"None\"\r\n```\r\nNote that this code doesn't ever run `asarray` on a TensorSpec, it runs `asarray` on the input `x` (which is a graph tensor that has the shape `None`, and dtype `float32`).\r\n\r\nYou can now call the function `f` defined above with e.g. `f(tnp.ones([2, 3])`. If you try the above code exactly without your latest changes for supporting partially defined shapes, it should fail. \r\n", "> Hi @DarrenZhang01, `asarray` cannot wrap a TensorSpec.\r\n> \r\n> If you want to test the code for partially defined shapes, you can try something like the following (copying the code I shared previously):\r\n> \r\n> ```\r\n> @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\r\n> def f(x):\r\n>   x = tnp.asarray(x)  # Wraps a graph tensor into a tnp.ndarray\r\n>   return tnp.size(x)  # This should fail since the shape is \"None\"\r\n> ```\r\n> \r\n> Note that this code doesn't ever run `asarray` on a TensorSpec, it runs `asarray` on the input `x` (which is a graph tensor that has the shape `None`, and dtype `float32`).\r\n> \r\n> You can now call the function `f` defined above with e.g. `f(tnp.ones([2, 3])`. If you try the above code exactly without your latest changes for supporting partially defined shapes, it should fail.\r\n\r\nThanks very much, Akshay @akshaym ! I got what you said. Now the code is again ready for review.", "Thanks very much, @akshaym ! I learned a lot!", "Hey Zhibo @DarrenZhang01, Since you're adding a new API endpoint, you'll also need to regenerate the API goldens. \r\n\r\nYou'll have to run \r\n`bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True`", "> Hey Zhibo @DarrenZhang01, Since you're adding a new API endpoint, you'll also need to regenerate the API goldens.\r\n> \r\n> You'll have to run\r\n> `bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True`\r\n\r\nAkshay, where do I run this command?", "In the top-level directory of your tensorflow repo. ", "> In the top-level directory of your tensorflow repo.\r\n\r\n@akshaym A small question: Do I have to downgrade bazel to 3.1.0 (I have not found a proper way to do so on Mac)? If I build TensorFlow using bazel 3.5 (latest), the running process seems to be endless...not sure if it is caused by the version of bazel.\r\n\r\nUPDATE: Never mind, I have successfully installed bazel 3.1.0.", "Hi Akshay @akshaym , as a follow-up to the above message, even if I use bazel 3.1.0, the build process seems to be endless:\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/tools/api/tests:api_compatibility_test (366 packages loaded, 31781 targets configured).\r\nINFO: Found 1 target...\r\n[7,522 / 8,644] 4 actions running\r\n    Compiling tensorflow/core/kernels/relu_op.cc; 48s local\r\n    Compiling tensorflow/core/kernels/linalg/matrix_inverse_op.cc; 26s local\r\n    Compiling tensorflow/core/kernels/gather_nd_op.cc; 14s local\r\n    Compiling tensorflow/core/kernels/gather_nd_op_cpu_impl_0.cc; 7s local\r\n```\r\ni.e., the amount number right to the slash, 8644, is increasing all the time...\r\n\r\nUPDATE: The running process have been finished and the tests have been passed."]}, {"number": 42538, "title": "Pin numpy version to 1.19", "body": "Pin numpy version to 1.19.0", "comments": []}, {"number": 42537, "title": "Pin numpy version to 1.19", "body": "Pin numpy version to 1.19.0", "comments": []}, {"number": 42536, "title": "Pin numpy version to 1.19", "body": "Pin numpy version to 1.19.0", "comments": []}, {"number": 42535, "title": "TFLiteConverter Segmentation Fault during integer quantization representative_dataset --add_postprocessing_op=true", "body": "I'm using tensorflow==1.15.3 and I'm hitting a segmentation fault attempting [int8 post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only). The documentation for the 1.15 version of the TFLiteConverter can be found [here](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter). \r\n\r\nI found [a similar issue on github](https://github.com/tensorflow/tensorflow/issues/29829), but their solution to provide `--add_postprocessing_op=true` has not solved the segmentation fault.\r\n\r\nI've debugged it using PDB and found exactly where it crashes. It never reaches my `representative_dataset` function. It faults when running `CreateWrapperCPPFromBuffer(model_content)`\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.4 LTS\r\n- TensorFlow installed from (source or binary): source/pip tensorflow==1.15.3\r\n- TensorFlow version (or github SHA if from source): 1.15.3\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\npython -m pdb convert_model_to_tflite_int8.py --add_postprocessing_op=true\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-08-20 20:25:22.552188: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-08-20 20:25:22.573942: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz\r\n2020-08-20 20:25:22.574163: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x566ecb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-20 20:25:22.574183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n2020-08-20 20:25:22.574411: I tensorflow/core/common_runtime/direct_session.cc:359] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n2020-08-20 20:25:33.546206: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-08-20 20:25:33.546355: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n\r\n2020-08-20 20:25:37.496345: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-08-20 20:25:37.496382: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 433 nodes (-65), 484 edges (-65), time = 2221.83691ms.\r\n2020-08-20 20:25:37.496394: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 433 nodes (0), 484 edges (0), time = 935.13ms.\r\n> .../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py(51)__init__()\r\n-> .CreateWrapperCPPFromBuffer(model_content))\r\n(Pdb) s\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007ff40ee9f740 (most recent call first):\r\n  File \".../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 51 in __init__\r\n  File \".../python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 236 in _calibrate_quantize_model\r\n  File \".../python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 993 in convert\r\n  File \".../convert_model_to_tflite_int8.py\", line 97 in <module>\r\n  File \"<string>\", line 1 in <module>\r\n  File \"/usr/lib/python3.6/bdb.py\", line 434 in run\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1548 in _runscript\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1667 in main\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1694 in <module>\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85 in _run_code\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193 in _run_module_as_main\r\n[1]    17668 segmentation fault (core dumped)  python -m pdb convert_model_to_tflite_int8.py  --add_postprocessing_op=true\r\n```\r\n\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file=pb_model_path,\r\n  input_arrays=[\"device_0/input_node_name:1\"],\r\n  output_arrays=[\"device_0/output_node_name\"],\r\n  input_shapes={\"device_0/input_node_name:1\": [100, 16384]}\r\n)\r\nconverter.allow_custom_ops = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\ndef test():\r\n  pdb.set_trace()\r\n  print(' ! ! ! representative_dataset_gen ! ! ! ')\r\n  zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')\r\n  ds = tf.data.Dataset.from_tensor_slices((zeros)).batch(1)\r\n  for input_value in ds.take(1):\r\n    yield [input_value]\r\nconverter.representative_dataset = test\r\n\r\npdb.set_trace()\r\ntflite_model = converter.convert()\r\n\r\ntflite_model_size = open(model_name, 'wb').write(tflite_model)\r\nprint('TFLite Model is %d bytes' % tflite_model_size)\r\n```\r\n\r\n**Failure details**\r\n- int8 quantization with `representative_dataset`\r\n- Segmentation fault occurs when running `CreateWrapperCPPFromBuffer(model_content)`\r\n\r\n**Any other info / logs**\r\n- tf.float16 conversion works (without representative_dataset property)\r\n- asked on stack overflow [here](https://stackoverflow.com/questions/63514487/tfliteconverter-segmentation-fault-when-running-integer-quantization)\r\n", "comments": ["@CoreyCole,\r\nOn running the code, I am facing an error stating `NameError: name 'pb_model_path' is not defined`. \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the saved model file you are using or the code to build the model. Thanks!", "Also, is there any specific reason you are using TensorFlow v1.15? Please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "Upgrading my tf version to 2.3 solved the segmentation fault. My model code isn't compatible with tf==2.x yet, but luckily the conversion code is independent from that so the upgrade went smoothly."]}, {"number": 42534, "title": "Low performance of Object Detection Models in Coral Dev Board", "body": "Hi \r\n\r\nRecently my company bought a Coral Board Dev and  Coral USB Accelerator to detect some objects in outdoor environment.\r\n\r\nWe have tried to train MobileNet V1 and V2 using TPU on the Cloud, as well using GPU RT2070 and finally we have implemented other models such as Inception Lite and Tiny Yolo V3 but the results are no good according the accuracy during the performance. \r\n\r\n\r\nFor this reason, I would like to know what else could be done to improve this model in real Object detection in Coral edge device?", "comments": ["@IvanChavR \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]