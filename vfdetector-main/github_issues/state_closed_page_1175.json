[{"number": 17942, "title": "Many missing ops in iOS, output not consistent with linux", "body": "I have tried porting to iOS a fully working semantic segmentation nnet, which does run, but has a series of warnings and less than accurate output, although somewhat working, when compared to the same image input as our server. I am seeing a series of these errors which I suspect could be the problem. RightShift, LeftShift, BitwiseAnd, BitwiseOr, PopulationCount, LookupTableImportV2,MutableHashTable, etc. But I have set _ _ANDROID_TYPES_FULL_ _ non-ideally to try to enable as many operations as possible, and the errors listed much below are still showing up. \r\n\r\nI'm freezing graph and calling transform_graph with the following options:\r\n```\r\nadd_default_attributes\r\nstrip_unused_nodes(type=float, shape=\"$resolution,3\")\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms\r\nround_weights\r\nquantize_nodes\r\nstrip_unused_nodes\r\nsort_by_execution_order'\r\n```\r\nI've disabled optimize_for_inference because I understand that this could be a possible cause.\r\n\r\nI understand that @petewarden might have some experience here.\r\n\r\nHere's what I am seeing while executing in iOS:\r\n```\r\n2018-03-22 20:57:13.320436: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"PopulationCount\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: PopulationCount\r\n2018-03-22 20:57:13.320493: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"PopulationCount\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: PopulationCount\r\n2018-03-22 20:57:13.320774: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.320796: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.320814: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.320832: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.320849: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.320970: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.320994: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT32 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.321012: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RightShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } }') for unknown op: RightShift\r\n2018-03-22 20:57:13.321030: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321048: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321103: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321121: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321213: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321233: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321251: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT32 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321306: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } }') for unknown op: LeftShift\r\n2018-03-22 20:57:13.321324: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321341: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321406: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321458: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321476: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321493: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321510: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT32 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321527: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } }') for unknown op: BitwiseAnd\r\n2018-03-22 20:57:13.321576: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321640: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321659: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321676: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321693: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321802: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321846: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT32 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321875: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } }') for unknown op: BitwiseOr\r\n2018-03-22 20:57:13.321893: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322123: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322142: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322159: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322176: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322193: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322334: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT32 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322353: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } }') for unknown op: BitwiseXor\r\n2018-03-22 20:57:13.322420: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.322463: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.322500: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.323153: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.323176: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.323193: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.323210: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT32 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.323367: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } }') for unknown op: Invert\r\n2018-03-22 20:57:13.339078: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: MutableDenseHashTableV2\r\n2018-03-22 20:57:13.339132: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableDenseHashTableV2\r\n2018-03-22 20:57:13.339155: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_DOUBLE } } }') for unknown op: MutableDenseHashTableV2\r\n2018-03-22 20:57:13.339315: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableDenseHashTableV2\r\n2018-03-22 20:57:13.339339: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableDenseHashTableV2\r\n2018-03-22 20:57:13.339360: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableDenseHashTableV2\r\n2018-03-22 20:57:13.339388: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: MutableDenseHashTable\r\n2018-03-22 20:57:13.339668: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableDenseHashTable\r\n2018-03-22 20:57:13.339694: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_DOUBLE } } }') for unknown op: MutableDenseHashTable\r\n2018-03-22 20:57:13.339716: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableDenseHashTable\r\n2018-03-22 20:57:13.339774: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableDenseHashTable\r\n2018-03-22 20:57:13.339795: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableDenseHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableDenseHashTable\r\n2018-03-22 20:57:13.347418: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensorsV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableHashTableOfTensorsV2\r\n2018-03-22 20:57:13.347522: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensorsV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: MutableHashTableOfTensorsV2\r\n2018-03-22 20:57:13.347571: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensorsV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: MutableHashTableOfTensorsV2\r\n2018-03-22 20:57:13.347596: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensorsV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableHashTableOfTensorsV2\r\n2018-03-22 20:57:13.350735: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensors\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableHashTableOfTensors\r\n2018-03-22 20:57:13.350772: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensors\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: MutableHashTableOfTensors\r\n2018-03-22 20:57:13.350793: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensors\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: MutableHashTableOfTensors\r\n2018-03-22 20:57:13.353009: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableOfTensors\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableHashTableOfTensors\r\n2018-03-22 20:57:13.353049: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableHashTableV2\r\n2018-03-22 20:57:13.353070: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: MutableHashTableV2\r\n2018-03-22 20:57:13.354728: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: MutableHashTableV2\r\n2018-03-22 20:57:13.354846: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableHashTableV2\r\n2018-03-22 20:57:13.355137: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableHashTableV2\r\n2018-03-22 20:57:13.368009: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_DOUBLE } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.368077: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.368099: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT32 } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.369241: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.369279: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.369299: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.370155: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.370224: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.370289: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.370321: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT32 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT32 } } }') for unknown op: HashTableV2\r\n2018-03-22 20:57:13.372970: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_DOUBLE } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.373001: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.373021: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT32 } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.377387: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.377422: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.377890: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.381780: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.381821: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.381840: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.381860: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT32 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT32 } } }') for unknown op: HashTable\r\n2018-03-22 20:57:13.384408: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableImportV2\" device_type: \"CPU\"') for unknown op: LookupTableImportV2\r\n2018-03-22 20:57:13.384454: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableImport\" device_type: \"CPU\"') for unknown op: LookupTableImport\r\n2018-03-22 20:57:13.384488: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableExportV2\" device_type: \"CPU\"') for unknown op: LookupTableExportV2\r\n2018-03-22 20:57:13.384536: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableSize\" device_type: \"CPU\"') for unknown op: LookupTableSize\r\n2018-03-22 20:57:13.384571: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableInsertV2\" device_type: \"CPU\"') for unknown op: LookupTableInsertV2\r\n2018-03-22 20:57:13.384605: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableInsert\" device_type: \"CPU\"') for unknown op: LookupTableInsert\r\n2018-03-22 20:57:13.384709: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableFindV2\" device_type: \"CPU\"') for unknown op: LookupTableFindV2\r\n2018-03-22 20:57:13.384775: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableFind\" device_type: \"CPU\"') for unknown op: LookupTableFind\r\n2018-03-22 20:57:13.384815: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"InitializeTableFromTextFile\" device_type: \"CPU\"') for unknown op: InitializeTableFromTextFile\r\n2018-03-22 20:57:13.384852: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"InitializeTableV2\" device_type: \"CPU\"') for unknown op: InitializeTableV2\r\n2018-03-22 20:57:13.385205: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"InitializeTable\" device_type: \"CPU\"') for unknown op: InitializeTable\r\n2018-03-22 20:57:13.386101: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableExport\" device_type: \"CPU\"') for unknown op: LookupTableExport\r\n2018-03-22 20:57:13.386266: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"InitializeTableFromTextFileV2\" device_type: \"CPU\"') for unknown op: InitializeTableFromTextFileV2\r\n2018-03-22 20:57:13.386366: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableSizeV2\" device_type: \"CPU\"') for unknown op: LookupTableSizeV2\r\n2018-03-22 20:57:13.386434: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableHashTable\r\n2018-03-22 20:57:13.386500: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT64 } } }') for unknown op: MutableHashTable\r\n2018-03-22 20:57:13.386580: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_STRING } } }') for unknown op: MutableHashTable\r\n2018-03-22 20:57:13.386626: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_BOOL } } }') for unknown op: MutableHashTable\r\n2018-03-22 20:57:13.386665: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"MutableHashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: MutableHashTable\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17941, "title": "initial  large tf.variable error", "body": "### System information\r\n- **Have I written custom code: yes \r\n- **OS Platform and Distribution Ubuntu 16.04**:\r\n- **TensorFlow installed from source **:\r\n- **TensorFlow version 1.6**:\r\n- **Python version 3.5**: \r\n- **Bazel version 0.11.1**:\r\n- **GCC/Compiler version (5)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\nwhen i'm using same code, only the weight variable have different size\r\nand when weight array is small everything is fine, but when too big it show shape issue\r\n\r\n### Source code / logs\r\nsouce code:\r\nwith tf.device('/cpu:0'):\r\n\tW_fc1 = weight_variable([32768, 35000],\"W_fc1\")#32768\r\n\tb_fc1 = bias_variable([35000],\"b_fc1\")\r\n\th_pool5_flat = tf.reshape(h_pool5, [-1, 16*16*128])\r\n\th_fc1 = tf.nn.relu(tf.matmul(h_pool5_flat, W_fc1) + b_fc1,name=\"h_fc1\")\r\n\th_fc1_drop = tf.clip_by_value(tf.cast(tf.nn.dropout(tf.cast(h_fc1,tf.float32), keep_prob),tf.float16, name=\"h_fc1_drop\"),min_clip,max_clip, name=\"h_fc1_drop\")\r\n\th_fc1_drop_1 = tf.where(tf.is_nan(h_fc1_drop), tf.constant(min_clip,dtype=tf.float16, shape=h_fc1_drop.shape), h_fc1_drop)\r\n\r\n##down sampleing fc2 layer##\r\n#with tf.device('/cpu:0'):\r\n\tW_fc2 = weight_variable([35000,65536],\"W_fc2\")\r\n\tb_fc2 = bias_variable([65536],\"b_fc2\")\r\n\th_fc2 = tf.clip_by_value(tf.nn.relu(tf.matmul(h_fc1_drop_1, W_fc2) + b_fc2),min_clip,max_clip, name=\"h_fc2\")\r\n\th_fc2_1 = tf.where(tf.is_nan(h_fc2), tf.constant(min_clip,dtype=tf.float16, shape=h_fc2.shape), h_fc2)\r\n..........\r\nloss = tf.losses.mean_squared_error(y_reshape,prediction_1)\r\ntrain_step = tf.train.AdamOptimizer(lr).minimize(loss)\r\n\r\nwhen W_fc1 is as big as the above \r\nit show the error\r\n   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [35000,65536] rhs shape= [32768,35000]\r\n         [[Node: W_fc2/Adam_1/Assign = Assign[T=DT_HALF, _class=[\"loc:@W_fc2\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_fc2/Adam_1, W_fc2/Adam/Initializer/zeros)]]\r\n", "comments": ["The error suggests that the shapes of W_fc1 and W_fc2 need to be commensurate.  When you set W_fc1 to a smaller size, did W_fc2 also have a smaller size, or was it [35000,65536]?", "@poxvoculi yes i do , i'm sure all the setting are correct ,if not it'll crash at the begin , but the error comes out while the sess.run(init)", "I'm afraid I don't understand.  Can you post the full program that works correctly, and also the full program that fails?\r\n", "well the only different part of success and fail version are the size of the weight (6 line)\r\nand i'm sure i did it correct but i can still post my code here\r\nhttps://gist.github.com/B3N50N/aafd02d4c70cc7d24ce45c5fe472555f\r\nalso one more thing is that if the error come out at the begging of the program run than it's probably my problem\uff0cit seam's it'll check every line of code before init , but what happened to me is the error come out after 30 minuet the program has start it mean it has pass the first check\uff0cbut fail when it was initializing .\r\nand also in the error logs you can see the error comes out with Adam or something it mean it has already successful build the forward propagation \uff0cthe error bump out while building backward propagation. ", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm sorry but there's not enough information here for me to work with, in a time efficient fashion.  It sounds to me like some kind of usage problem.  Usage questions are best asked on stack overflow.  If you believe there is a TensorFlow bug, please try to post a very small example that demonstrates the problem."]}, {"number": 17940, "title": "[tflite] fix number of gemmlowp threads problem", "body": "Recent changes moved setting of the number of gemmlowp threads to\r\nInit() of conv operation so that invoking `Interpreter::SetNumThreads()`\r\nafter nodes created doesn't change of number of threads.\r\n\r\nNote that the number of Eigen threads is also problematic, but I don't have an easy fix yet. In current [`tflite::eigen_support::IncrementUsageCounter()` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/eigen_support.cc#L27), it assumes that `Eigen::setNbThreads()` changes the number of Eigen thread. But current [multithreaded conv](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/optimized/multithreaded_conv.h) actually has its own Eigen threadpool.", "comments": [" I'll close this, because the number of threads setting was reverted in d2604f8d. \r\n\r\n@aselle: However, the number of threads of Eigen is still problematic because `Eigen::setNbThreads()` just doesn't work."]}, {"number": 17939, "title": "Fail to convert TransposeConv into tflite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: latest\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: Build label: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**N/A:\r\n\r\n\r\nI try to convert my model into tflite, shows the below error:\r\n\r\nF tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:243] Check failed: weights_shape.dims(0) == 1 && weights_shape.dims(3) == 1 TransposeConv weights dimensions must begin and end with 1. Input weights \"Variable_39/read_transposed\" had shape [ 128, 3, 3, 64 ].\r\n\r\nDoes the TFLite TransposeConv weights only support [1, X, X, 1]  ?\r\n\r\nAnd I checked the commit ID: 58fe7d26afa435560e7a0d8ca6fc8d670d2477da : \"Support for transpose convolution. Includes striding, and a reference implementation.\"\r\nSeems that transpose convolution is not suppoertted yet, I can not find any Register_XXX in the ops files, only a function \"inline void TransposeConv(const float* input_data, const Dims<4>& input_dims,\r\n                          const float* filter_data, const Dims<4>& filter_dims,\r\n                          int stride_width, int stride_height, int pad_width,\r\n                          int pad_height, float* output_data,\r\n                          const Dims<4>& output_dims)\"\r\nand there is no place call this function \"TransposeConv\"", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary):source\r\nTensorFlow version (use command below): latest\r\nPython version: 2.7\r\nBazel version (if compiling from source): Build label: 0.7.0\r\nGCC/Compiler version (if compiling from source):N/A\r\nCUDA/cuDNN version:N/A\r\nGPU model and memory:N/A\r\nExact command to reproduceN/A:", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Did you strip the training (i.e. gradient and optimizer nodes) out of your graph. This can typically be done with things like freezing. A lot of times TransposeConv is present only because the gradient of Conv is TranposeConv. \r\n\r\n\r\nSee example 2  in https://www.tensorflow.org/mobile/tflite/devguide\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nvar = tf.get_variable(\"weights\", dtype=tf.float32, shape=(1,64,64,3))\r\nval = img + var\r\n\r\ndef canonical_name(x):\r\n  return x.name.split(\":\")[0]\r\n\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  out_tensors = [out]\r\n  frozen_graphdef = tf.graph_util.convert_variables_to_constants(\r\n      sess, sess.graph_def, map(canonical_name, out_tensors))\r\n  tflite_model = tf.contrib.lite.toco_convert(\r\n      frozen_graphdef, [img], out_tensors)\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@aselle I have the same problem with a frozen and optimized semantic segmentation network that has transposed convolution by design. Any advices?\r\n`F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:252] Check failed: weights_shape.dims(0) == 1 && weights_shape.dims(3) == 1 TransposeConv weights dimensions must begin and end with 1. Input weights \"score2_full_weight/read/_108__cf__108_transposed\" had shape [ 5, 16, 16, 5 ].`", "@huanyingjun  have this issue resolved or not . I have the similar issue on Tensorflow R1.8 GPU version on Ubuntu16.04:\r\n\r\n#2018-06-04 18:11:22.606761: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: SplitV\r\n#2018-06-04 18:11:22.606840: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Prod\r\n#2018-06-04 18:11:22.606859: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Prod\r\n#2018-06-04 18:11:22.617876: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 328 operators, 590 arrays (0 quantized)\r\n#2018-06-04 18:11:22.627740: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 326 operators, 586 arrays (0 quantized)\r\n#2018-06-04 18:11:22.650427: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 326 operators, 586 arrays (0 quantized)\r\n#2018-06-04 18:11:22.666719: F **tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:252] Check failed: weights_shape.dims(0) == 1 && weights_shape.dims(3) == 1 TransposeConv weights dimensions must begin and end with 1. Input weights \"import/transpose_23_transposed\" had shape [ 64, 4, 4, 32 ].\r\n#Aborted (core dumped)**\r\n"]}, {"number": 17938, "title": "Replace depreciated function variable_op_scope with variable_scope.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I think you may have to add keyword arguments, actually? So values= for the first one and name_or_scope= for the second one (glancing at the [definition of variable_op_scope](https://github.com/tensorflow/tensorflow/blob/1897cea1807c28d998b7efc3906de48d75d99a6d/tensorflow/python/ops/variable_scope.py#L2118), the argument order is swapped). Sorry for the \"just find-and-replace\" bait-and-switch, and thanks for doing this!"]}, {"number": 17937, "title": "Merge branch r1.7 into master", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Ugh, presubmits look real bad :)\r\nCould you take a  look?", "Is there a way to tell where the snapshot_op change comes from?", "a-ha that was the windows fix.\r\nAll good, merging."]}, {"number": 17936, "title": "Segmenter update for TFTRT", "body": "This PR cheery-picks #17912  into r1.7\r\n", "comments": ["Tagging @gunan and @aaroey."]}, {"number": 17935, "title": "Branch 190141732", "body": "", "comments": []}, {"number": 17934, "title": "MKL: Fixing Build Issue", "body": "Fixing build issue where MKL-DNN is getting built when not using --config=mkl.", "comments": []}, {"number": 17932, "title": "tf.contrib.data.bucket_by_sequence_length fails for nested Dataset element", "body": "Hello everyone,\r\n\r\nI just tried the new function to group variable length inputs for the dataset API, namely: `tf.contrib.data.bucket_by_sequence_length`, for a small Estimator-Model.\r\n\r\nI implemented the `input_fn` such that it returns a dataset, where each element is a tuple [(feature-dict, label)](https://www.tensorflow.org/get_started/premade_estimators#create_input_functions). However, when I run it, I get following exception:\r\n\r\n> Traceback (most recent call last):\r\n> ... \r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 960, in apply\r\n>     dataset = transformation_func(self)\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py\", line 198, in _apply_fn\r\n>     window_size_func=window_size_fn))\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 960, in apply\r\n>     dataset = transformation_func(self)\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py\", line 90, in _apply_fn\r\n>     window_size_func)\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py\", line 239, in __init__\r\n>     self._make_key_func(key_func, input_dataset)\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py\", line 289, in _make_key_func\r\n>     self._key_func.add_to_graph(ops.get_default_graph())\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 488, in add_to_graph\r\n>     self._create_definition_if_needed()\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 321, in _create_definition_if_needed\r\n>     self._create_definition_if_needed_impl()\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 338, in _create_definition_if_needed_impl\r\n>     outputs = self._func(*inputs)\r\n>   File \"/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py\", line 279, in tf_key_func\r\n>     ret = key_func(*nested_args)\r\n> TypeError: element_to_bucket_id() takes 1 positional argument but 2 were given\r\n\r\nHere is a [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/grouping.py#L143) to the function.\r\n\r\nHere is a code snipped to reproduce the error:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef input_fn():\r\n  def generator():\r\n    text = [[1, 2, 3],\r\n            [3, 4, 5, 6, 7],\r\n            [1, 2],\r\n            [8, 9, 0, 2, 3]]\r\n    label = [1, 2, 1, 2]\r\n\r\n    for x, y in zip(text, label):\r\n      yield (x, y)\r\n\r\n  dataset = tf.data.Dataset.from_generator(generator=generator,\r\n                                           output_shapes=(tf.TensorShape([None]), tf.TensorShape([])),\r\n                                           output_types=(tf.int32, tf.int32))\r\n\r\n  dataset = dataset.map(parse_example)\r\n  dataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=element_length_fn,\r\n                                                                    bucket_batch_sizes=[2, 2, 2],\r\n                                                                    bucket_boundaries=[0, 8],\r\n                                                                    pad_to_bucket_boundary=False))\r\n\r\n  return dataset\r\n\r\ndef parse_example(x, y):\r\n  return dict(\r\n    x=x\r\n  ), y\r\n\r\ndef element_length_fn(element):\r\n  features, label = element\r\n  return tf.shape(features[\"x\"])[0]\r\n\r\nif __name__ == '__main__':\r\n  with tf.Session() as sess:\r\n    dataset = input_fn()\r\n    iter = dataset.make_one_shot_iterator()\r\n\r\n    print(sess.run(iter.get_next()))\r\n```\r\n\r\nMy Env-Specs are logged in: [tf_env.txt](https://github.com/tensorflow/tensorflow/files/1838947/tf_env.txt)\r\n\r\nThanks in advance!", "comments": ["@rsepassi Can you take a look, please?\r\n\r\n/cc @ebrevdo FYI.", "Yes, looking into it.\n\nOn Thu, Mar 22, 2018 at 9:02 PM Derek Murray <notifications@github.com>\nwrote:\n\n> @rsepassi <https://github.com/rsepassi> Can you take a look, please?\n>\n> /cc @ebrevdo <https://github.com/ebrevdo> FYI.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17932#issuecomment-375505400>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABEGW-jtSsBu9OcEFUz8J3yEIi4AdEo5ks5thEmIgaJpZM4S3plo>\n> .\n>\n", "This is consistent with the behavior of all callables in `tf.data` with `tuple` inputs. The function [`_should_unpack_args`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L1769) is used to determine whether inputs to a user-supplied callable should be unpacked (i.e. called as `fn(*args)`) and is `True` if the inputs are a `tuple`. If the elements of a `Dataset` are tuples, as they are in the example in this thread, then callables must expect the arguments to be passed unpacked.\r\n\r\nFor example, here's a `Dataset` where the elements are tuples and a call to `map` fails when the `map_fn` expects only 1 element.\r\n\r\n```\r\n    def data_gen():\r\n      text = [[1, 2, 3], [3, 4, 5, 6, 7], [1, 2], [8, 9, 0, 2, 3]]\r\n      label = [1, 2, 1, 2]\r\n      for x, y in zip(text, label):\r\n        yield (x, y)\r\n\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator=data_gen,\r\n        output_shapes=(tensor_shape.TensorShape([None]), tensor_shape.TensorShape([])),\r\n        output_types=(dtypes.int32, dtypes.int32))\r\n\r\n    # Fails\r\n    dataset.map(lambda el: el)\r\n```\r\n\r\nWe may want to update this behavior but that's where it stands right now.\r\nFix right now would be to use a `dict` going in and apply a `map` to turn into a tuple afterwards.\r\n\r\n@mrry, should we look into updating the behavior of Dataset callables?", "@rsepassi Right, that example should fail, and we can't change this even if we wanted to. It works if you instead do `dataset.map(lambda x, y: (x, y))`.\r\n\r\nI think there might be a missing `_should_unpack_args()` in the `bucket_by_sequence_length` code, because making the equivalent change to @lhlmgr's program doesn't fix things, i.e.:\r\n\r\n```python\r\ndef element_length_fn(features, label):\r\n  return tf.shape(features[\"x\"])[0]\r\n```\r\n\r\n...still yields the same error: \r\n\r\n> TypeError: element_to_bucket_id() takes exactly 1 argument (2 given)\r\n", "Yup, found the issue. Have a fix out. `bucket_by_sequence_length` was not correctly handling tupleized elements."]}, {"number": 17931, "title": "[Intel MKL] Change inter op defaults when built with MKL ", "body": "Reduces inter_op parallelism when using MKL.", "comments": ["@rmlarsen Could you please take a look at this PR?", "@jbobba Thanks for the contribution!"]}, {"number": 17930, "title": "Memory Leak in SavedModelBundle.load() in the TensorFlow Java API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7.12\r\n- **Java version**: OpenJDK 1.8.0_151\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0\r\n- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0\r\n- **GPU model and memory**: name: TITAN Xp, compute capability: 6.1, total memory: 11.90GiB\r\n- **Exact command to reproduce**: SavedModelBundle.load() and Runner.run() from Java API\r\n\r\n### Describe the problem\r\n\r\nThere seems to be a memory leak when using the Tensorflow Java API when executing the load static method of SavedModelBundle class and the run method of the Runner class.  In the process of debugging, I've attached the VisualVM profiler and a remote Java debugger to step through code execution.  Additionally, I've been using `htop` to monitor the memory usage from active processes running on the server hosting the API.  \r\n\r\nFrom this, I've been able to observe\r\n- each time the inference is invoked, it increases my total memory consumption by about 0.18G - 0.19G, which is approximately the size of my saved model files being loaded.  \r\n- my Java heap memory is not permanently increasing; after the inference has finished all heap memory returns to the amount before the inference started.\r\n\r\nI've been sure to invoke the close method of each class that extends AutoCloseable that I've used.  I do not explicitly invoke the close method on the Session and Graph managed by the SavedModelBundle, as its close method invokes the close methods of its Session and Graph.  This is reflected in the code example provided below.\r\n\r\nThis memory leak occurs whether I'm building with GPU acceleration or not.\r\n\r\nTo build the native libraries and Java API from source with GPU acceleration, I'm using the following bazel command (from the 1.6.0 release tag):\r\n\r\n```\r\nbazel build --verbose_failures --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"  --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n```\r\n\r\nTo build the native libraries and Java API from source without GPU acceleration, I'm using the following bazel command (from the 1.6.0 release tag):\r\n\r\n```\r\nbazel build --verbose_failures --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"    //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n```\r\n\r\nIn an attempt to get more information about the execution of the native code, I've tried to enable debug mode while building the native libraries by repliacing `--config=opt` with `-c dbg --strip=never --compilation_mode=dbg`, however this causes the following exception:\r\n\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<const float, const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer> > > >; bool Vectorizable = true]: Assertion `cudaGetLastError() == cudaSuccess' failed.\r\n```\r\n\r\nI originally found the memory leak while using TensorFlow 1.4.0, CUDA 8.0, and cuDNN 6.0.  Before updating the versions, each inference was retaining about 1.4-1.5G of memory; by updating, I managed to reduce this significantly, however still need to eliminate the memory leak altogether.\r\n\r\nI've spent hours trying to figure out how to profile the native code execution, most recently by following these answers from Stack Overflow:\r\n1. [How to profile Native JNI library](https://stackoverflow.com/questions/14752459/how-to-profile-native-jni-library)\r\n2. [gprof : How to generate call graph for functions in shared library that is linked to main program](https://stackoverflow.com/questions/1838989/gprof-how-to-generate-call-graph-for-functions-in-shared-library-that-is-linke/4959168#4959168)\r\n\r\nI've been unable to successfully compile the native libraries with the debug flag (`-g`) to profile.  I've poked around the bazel build scripts a bit, as I don't believe that the debug mode indicated by the `--compilation_mode=dbg` flag actually compiles the libraries for debugging, however have been unable to locate where I'd add the `-g` compilation flag.\r\n\r\n\r\n### Source code / logs\r\n\r\nHere is my configuration output when configuring with GPU acceleration.\r\n```\r\nilats-admin$ ./configure\r\nYou have bazel 0.6.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: N\r\nNo jemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: N\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: N\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: N\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: N\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: N\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: N\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: \r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-9.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-9.0]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: N\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=tensorrt    \t# Build with TensorRT support.\r\nConfiguration finished\r\n```\r\n\r\nI've created a sample application below that is a minimal version of my custom code, and still causes the memory leak to occur.  I've used a simple while loop to perform an inference whenever a character is entered from `stdin`, to demonstrate the memory being used not released while the application is still running.\r\n\r\nThis code is used for inference against a trained version of Object Detection model architecture found [here](https://github.com/tensorflow/models/tree/master/research/object_detection).  I've prepared a custom dataset and performed training similar to the [Quick Start guide for Training a Pet Detector](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md).  Following this guide through the section titled **Exporting the Tensorflow Graph** should produce a saved model that can be used with this code sample.  You'll need to adjust the path to the saved model, the labels to those used for the guide, and the file path to the image to detect objects in, which are all captured as static final variables towards the top of the class.\r\n\r\n```\r\nimport java.io.*;\r\nimport org.tensorflow.Tensor;\r\nimport org.tensorflow.SavedModelBundle;\r\nimport org.tensorflow.types.UInt8;\r\nimport java.awt.image.BufferedImage;\r\nimport java.awt.image.DataBufferByte;\r\nimport javax.imageio.ImageIO;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.util.List;\r\n\r\npublic class Main {\r\n  private static final String[] LABELS = {\"label1\", \"label2\", \"label3\", \"label4\", \"label5\", \"label6\", \"label7\"};\r\n  private static final String SAVED_MODEL_PATH = \"/usr/repositories/resources/models/ilats-targets-4/saved_model\";\r\n  private static final String FILE_PATH = \"/tmp/inference_image.jpeg\";\r\n\r\n  // Definite input and output Tensors for detection_graph\r\n  private static final String IMAGE_TENSOR_NAME = \"image_tensor:0\";\r\n\r\n  // Each box represents a part of the image where a particular object was detected.\r\n  private static final String DETECTION_BOXES_NAME = \"detection_boxes:0\";\r\n\r\n  // Each score represent how level of confidence for each of the objects.\r\n  // Score is shown on the result image, together with the class label.\r\n  private static final String DETECTION_SCORES_NAME = \"detection_scores:0\";\r\n\r\n  private static final String DETECTION_CLASSES_NAME = \"detection_classes:0\";\r\n\r\n  public static void main(String[] args) {\r\n    int ch;\r\n    try {\r\n\r\n      while ((ch = System.in.read()) != -1) {\r\n        performInference();\r\n      }\r\n\r\n    } catch (Exception e) {\r\n    }\r\n  }\r\n\r\n  public static void performInference() {\r\n    SavedModelBundle model = null;\r\n    Tensor<UInt8> imageTensor = null;\r\n    List<Tensor<?>> outputs = null;\r\n\r\n    try {\r\n      model = SavedModelBundle.load(SAVED_MODEL_PATH, \"serve\");\r\n      imageTensor = makeImageTensor(FILE_PATH);\r\n      \r\n      outputs = model\r\n        .session()\r\n        .runner()\r\n        .feed(IMAGE_TENSOR_NAME, imageTensor)\r\n        .fetch(DETECTION_SCORES_NAME)\r\n        .fetch(DETECTION_CLASSES_NAME)\r\n        .fetch(DETECTION_BOXES_NAME)\r\n        .run();\r\n\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e.getMessage(), e);\r\n    } finally {\r\n      // this closes Session and Graph that belongs to model as well\r\n      if (model != null) {\r\n        model.close();\r\n      }\r\n\r\n      if (imageTensor != null) {\r\n        imageTensor.close();\r\n      }\r\n\r\n      if (outputs != null) {\r\n        for (Tensor output : outputs) {\r\n          if (output != null) {\r\n            output.close();\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \r\n\r\n  }\r\n\r\n  public static Tensor<UInt8> makeImageTensor(String filename) throws IOException {\r\n    BufferedImage img = ImageIO.read(new File(filename));\r\n    if (img.getType() != BufferedImage.TYPE_3BYTE_BGR) {\r\n      throw new IOException(\r\n        String.format(\"Expected 3-byte BGR encoding in BufferedImage, found %d (file: %s). This code could be made more robust\", img.getType(), filename)\r\n      );\r\n    }\r\n\r\n    byte[] data = ((DataBufferByte) img.getData().getDataBuffer()).getData();\r\n    // ImageIO.read seems to produce BGR-encoded images, but the model expects RGB.\r\n    bgr2rgb(data);\r\n    final long BATCH_SIZE = 1;\r\n    final long CHANNELS = 3;\r\n    long[] shape = new long[] {BATCH_SIZE, img.getHeight(), img.getWidth(), CHANNELS};\r\n    Tensor<UInt8> imageTensor = Tensor.create(UInt8.class, shape, ByteBuffer.wrap(data));\r\n    img.flush();\r\n    return imageTensor;\r\n  }\r\n\r\n  public static void bgr2rgb(byte[] data) {\r\n    for (int i = 0; i < data.length; i += 3) {\r\n      byte tmp = data[i];\r\n      data[i] = data[i + 2];\r\n      data[i + 2] = tmp;\r\n    }\r\n  }\r\n\r\n}\r\n```\r\n\r\nThe instructions to compile and run this are similar to those found in the [TensorFlow for Java Readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md).\r\n\r\n```\r\n$ GENFILE_DIR=/usr/repositories/tensorflow/bazel-bin/tensorflow/java\r\n$ javac -cp ./:$GENFILE_DIR/libtensorflow.jar Main.java\r\n$ java -cp ./:$GENFILE_DIR/libtensorflow.jar \\\r\n  -Djava.library.path=$GENFILE_DIR \\\r\n  Main\r\n```\r\n", "comments": ["Thanks for the report. Though, is it possible to come up with an easier to reproduce sample (that doesn't involve having to figure out a custom dataset etc.)? Also, I'm not sure I understand what precisely you meant by \"it increases my total memory consumption\". What are you measuring and how?\r\n\r\nIn an attempt to replicate the setup you described, I modified [the java sample code](https://github.com/tensorflow/models/tree/master/samples/languages/java/object_detection) so that the the contents of `main()` are called in an infinite loop. And then I looked at `top` as well as `/proc/<pid>/status` and saw pretty stable memory consumption.\r\n\r\nLooking forward to more information.", "Thank you for the quick reply.  I was able to reproduce the problem using the example you linked to as well, by making a similar modification to main.  To observe the memory usage continually increasing, I was using htop to see that the RAM used increases during inference and doesn't decrease after the inference finished.  I've attached some screen shots to show this.\r\n\r\n![screen shot 2018-03-23 at 10 45 37 am](https://user-images.githubusercontent.com/5429005/37836107-15ee3710-2e88-11e8-97ab-8472600b2594.png)\r\n![screen shot 2018-03-23 at 10 46 05 am](https://user-images.githubusercontent.com/5429005/37836108-15fcadd6-2e88-11e8-9678-ad69d4b6df65.png)\r\n![screen shot 2018-03-23 at 10 46 27 am](https://user-images.githubusercontent.com/5429005/37836109-160b5e58-2e88-11e8-9989-6b4c0def4165.png)\r\n![screen shot 2018-03-23 at 10 46 46 am](https://user-images.githubusercontent.com/5429005/37836110-16182b74-2e88-11e8-822e-5cd2166d0689.png)\r\n", "@chrisrsipes : Ah, sorry, I think I misunderstood the problem. If I understand correctly, you're _not_ claiming that there is a memory leak in that the total memory footprint keeps on increasing and eventually the process runs out of memory. Instead, you're pointing out that the virtual memory size and the resident set size of the process does not decrease. Correct?\r\n\r\nIf so, that is intended behavior. This happens because the TensorFlow runtime's memory allocator (e.g., [jemalloc](http://jemalloc.net) on some platforms) may manage memory allocations itself. In other words, it can allocate memory from the system and instead of returning it, can keep it around for future allocations. Which is why, even though the VM size and RSS don't go down after you call `close()`, it doesn't go up on the next iteration either (since the allocator is re-using that memory).\r\n\r\nLet me know if I understood correctly.\r\nThanks.", "My memory footprint actually does continually increase and the process would eventually run out of memory.  My guess is that perhaps on each iteration, instead of reusing that memory that was allocated and left for future use, it is allocating indeed allocating new memory that never gets freed.\r\n\r\nThe screenshots from above are successive invocations of performing the inference.  No other processes (as least not with significant memory footprints) were started in between invocations.  I now realize that the process shown using htop isn't the same process that was doing the invocation, which was unintentional.  \r\n\r\nStill, after performing the first inference, the RAM usage is at 3.73G.  after the second inference, the RAM usage is at 4.14G.  After the third inference, the RAM usage is at 4.66G.  Each time the inference runs, it uses a significant amount of RAM that is not released after the inference finishes.  If I was to kill the program, all the memory allocated is free.  However, I am wrapping the capability of performing the inference in an API, and as such can not shut down the application each time an inference is performed.\r\n\r\nIf you have any tips of how I can debug / profile the native libraries or see where the memory blocks are being allocated, I would be happy to try it out.", "@chrisrsipes : To make sure I understand correctly, the `htop` screenshots you had attached earlier do not show the problem you're observing, right? (Because, in those I see the same  879MB of RSS and ~12GB of VIRT across all three screenshots).\r\n\r\nIf so, I'm still confused about the problem you're observing. For example:\r\n- How are you measuring \"RAM usage\"? As in, what number are you looking at?\r\n- Do you see the same issue with the example mentioned in https://github.com/tensorflow/tensorflow/issues/17930#issuecomment-375463817 ?\r\n- Can you provide a minimal example to reproduce the problem?\r\n\r\nI'm still not quite sure I understand the problem, and can't reproduce it, so I'm having trouble being helpful :).", "@asimshankar That's partially correct.  With the screenshots I posted, the overall RAM consumption (measured by the MEM visualization that appears in the bottom left of the top section) does indeed illuminate the issue, however the processes that are in view are was not the sample application that was causing the memory leak.  \r\n\r\nTo clarify this, I've gathered new screenshots, with the process running the sample application in view.  The RES/MEM% columns show the steadily increasing RAM allocated to the process that is never freed.  For your questions:\r\n\r\n* I'm measuring the RAM usage by both the overall MEM consumption shown towards the bottom left of the top section (that is near the CPU usage of each core), as well as the RES/MEM% columns that are per process.  The new screenshots will show RES/MEM% increasing with each inference.\r\n* I did indeed still experience the issue with using the example mentioned in your comment.  \r\n* To provide a minimal example, I've taken the example and simplified it further to just load the model architecture and execute a run, and have that code available in this repo: [https://github.com/chrisrsipes/tf-leak](https://github.com/chrisrsipes/tf-leak). It does not require any labels or a file.  I've also provided the saved model protocol buffer file of the frozen model architecture I'm using, with a link to download it in the README.  I've tested and verified that I do not need to provide labels, nor feed/fetch any values to observe the memory leak.\r\n\r\n![screen shot 2018-04-03 at 11 30 44 am](https://user-images.githubusercontent.com/5429005/38266588-1059fa24-3747-11e8-916f-5e245b9b8b0f.png)\r\n![screen shot 2018-04-03 at 11 30 54 am](https://user-images.githubusercontent.com/5429005/38266589-1065d506-3747-11e8-88e4-1a1cffd42593.png)\r\n![screen shot 2018-04-03 at 11 31 03 am](https://user-images.githubusercontent.com/5429005/38266590-10737378-3747-11e8-8fb4-b167e74576a8.png)\r\n![screen shot 2018-04-03 at 11 31 12 am](https://user-images.githubusercontent.com/5429005/38266591-1085fd68-3747-11e8-908f-b9cdad5dcba3.png)\r\n![screen shot 2018-04-03 at 11 31 22 am](https://user-images.githubusercontent.com/5429005/38266592-10955cb8-3747-11e8-8eaa-2ee57b1e657c.png)\r\n![screen shot 2018-04-03 at 11 31 45 am](https://user-images.githubusercontent.com/5429005/38266593-10a3de50-3747-11e8-8c86-5f266ec51ffd.png)\r\n![screen shot 2018-04-03 at 11 31 57 am](https://user-images.githubusercontent.com/5429005/38266595-10ce618e-3747-11e8-8a5d-25048e232769.png)\r\n![screen shot 2018-04-03 at 11 32 04 am](https://user-images.githubusercontent.com/5429005/38266596-111da6e0-3747-11e8-8e28-628cde889953.png)\r\n![screen shot 2018-04-03 at 11 32 12 am](https://user-images.githubusercontent.com/5429005/38266597-112cfbb8-3747-11e8-95b0-9bc0f09a2340.png)\r\n![screen shot 2018-04-03 at 11 32 20 am](https://user-images.githubusercontent.com/5429005/38266598-11396880-3747-11e8-979d-8ad2062c2fd7.png)\r\n![screen shot 2018-04-03 at 11 32 28 am](https://user-images.githubusercontent.com/5429005/38266599-114aa55a-3747-11e8-94c9-0ebba9acfe08.png)\r\n![screen shot 2018-04-03 at 11 32 35 am](https://user-images.githubusercontent.com/5429005/38266600-1156fb7a-3747-11e8-930c-28332244aec5.png)\r\n![screen shot 2018-04-03 at 11 32 40 am](https://user-images.githubusercontent.com/5429005/38266601-11692980-3747-11e8-8981-3f62e46ae57b.png)\r\n![screen shot 2018-04-03 at 11 32 52 am](https://user-images.githubusercontent.com/5429005/38266603-1177c94a-3747-11e8-9a33-b9b9f5895c6a.png)\r\n", "Apologies for the delay, I am able to reproduce. Will look into it.", "Hi everybody.\r\n\r\nI have the same problem using the Tensorflow Java API when executing the load static method of SavedModelBundle class and the run method of the Runner class.\r\nWhat my application  does is to capture pictures of the IP camera and classify it.\r\nThis app runs in Maven project, where includes Tensorflow 1.8.0 and proto 1.8.0\r\n\r\n**OS Platform: Linux Ubuntu 16.4**\r\n**I'm not using GPU.**\r\nI've been using nmon to monitor the memory usage.\r\n\r\nWhen the application starts. The free percent memory is **39.5%**\r\nAfter 3 hours the free percent goes to **13.8%**\r\n\r\n![smart 95 - 2018_05_02_14_54_58](https://user-images.githubusercontent.com/38957144/39591885-82608ca4-4edb-11e8-815c-74b159646131.png)\r\n\r\n\r\nAfter 3 hours\r\n![2018_05_03_14_11_28_root](https://user-images.githubusercontent.com/38957144/39592065-167ce798-4edc-11e8-99e6-6c3144526a32.png)\r\n\r\n@chrisrsipes \r\nHave you discovered something to avoid this memory occupation?\r\n\r\n@tensorflowbutler \r\nDoes Google have any new information about this problem?", "@danielroson : Apologies, but no new information yet. To make sure what you're seeing is the same as @chrisrsipes  - you see this when repeatedly invoking `SavedModelBundle.load()`, correct? Because, AFAIK, there is no issue if you load the model once and repeatedly invoke it via `SavedModelBundle.session().runner().run()`.", "\r\n@asimshankar \r\n\r\nThanks friends. I load the model one time... and the \"classify(BufferedImage image)\" is called every frame with motion captured on IP camera.\r\n\r\nsee the example:\r\n\r\n```\r\npublic class TensorflowImageTypeClassifier {\r\n\t\r\n\tpublic static boolean\t\t\tINITIALIZED \t\t= false;\r\n\tprivate static float\t\t\tMIN_SCORE\t\t= 0.8f;\r\n\tprivate static String[]\t\t\tLABELS\t\t\t= null;\r\n\tprivate static\t\t\tSavedModelBundle\tMODEL\t= null;\r\n\r\n\tstatic {\r\n\t\ttry {\r\n\t\t\t\r\n\t\t\tLABELS = loadLabels();\r\n\t\t\tMODEL = loadModel();\r\n\t\t\t\r\n\t\t\tif(LABELS != null && MODEL != null) INITIALIZED = true;\r\n\t\t\t\r\n\t\t} catch (Exception e) {\r\n\t\t\tConsoleLogging.logError(log, \"[!] Error while loading models, labels and configurations\", e);\r\n\t\t}\r\n\t}\r\n\r\n\tpublic List<TensorflowImageTypeModel> classify(BufferedImage image) {\r\n\t\ttry {\r\n\t\t\tif (!INITIALIZED) {\r\n\t\t\t\tConsoleLogging.logError(log, \"[!] Classification process cannot be initialized\", null);\r\n\t\t\t\treturn null;\r\n\t\t\t}\r\n\t\t\t\r\n\t\t\tList<TensorflowImageTypeModel> modelsList = new ArrayList<TensorflowImageTypeModel>(); \r\n\r\n\t\t\tint width = image.getWidth();\r\n\t\t\tint height = image.getHeight();\r\n\r\n\t\t\tList<Tensor<?>> outputs = null;\r\n\r\n\t\t\ttry (Tensor<UInt8> input = makeImageTensor(image, width, height)) {\r\n\t\t\t\toutputs = MODEL\r\n\t\t\t\t\t\t.session()\r\n\t\t\t\t\t\t.runner()\r\n\t\t\t\t\t\t.feed(\"image_tensor\", input)\r\n\t\t\t\t\t\t.fetch(\"detection_scores\")\r\n\t\t\t\t\t\t.fetch(\"detection_classes\")\r\n\t\t\t\t\t\t.fetch(\"detection_boxes\")\r\n\t\t\t\t\t\t.run();\r\n\t\t\t}catch (Exception e) {\r\n\t\t\t\tConsoleLogging.logError(LogManager.getLogger(TensorflowImageTypeClassifier.class), \"Error making image tensor. Check file path\", e);\r\n\t\t\t}\r\n\r\n\t\t\t\r\n\r\n```", "@danielroson : Your issue seems to be different then, since this thread is mostly about repeated calls to `SavedModelBundle.load()` seemingly leaking memory. So, it may be better for you to file a separate issue, ideally with a [minimal, complete, and verifiable](https://stackoverflow.com/help/mcve) example to reproduce the problem.\r\n\r\nThat said, the code snippet you did provide seems a bit incomplete. Are you invoking [`Tensor.close()`](https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/Tensor) on the elements in `outputs`? If not, that would explain a leak. I'd expect something like:\r\n\r\n```java\r\nfor (Tensor<?> t : outputs) {\r\n  t.close();\r\n}\r\n```\r\n\r\nHope that helps.", "@asimshankar  Helped me a lot\r\n\r\nSorry, the code snippet is incomplete. I invoke only outputs.clear().\r\nI'll test this invocation solution **Tensor.close()** on the elements in the **outputs**.\r\n\r\nThank you so much. I apologize for the inconvenience of posting a problem other than what was presented in this post.\r\n\r\nthank you all\r\n", "Same Issue here, I have a Java application that should load at most 1000 models and if it reaches that limit it should close already loaded networks to limit RAM usage. Unfortunately it's not working and the process is consuming infinite RAM until the machine has no RAM left. \r\nAre there any updates regarding this problem or maybe a workaround ?", "Got the same problem. Java Tensorflow Version 1.8, but also testet older versions, all with the same bug - consuming infinity ram when loading and closing sessions.", "@SPLNatho I've been able to temporarily work around the problem by invoking an inference by executing bash commands in Java (Using DefaultExecutor and CommandLine)  to kick off the inference instead of using the TensorFlow API, and parsing the output.  It's hacky, but it at least side steps the problem for now.", "we are also facing this issue on tf java v1.3 whenever SavedModelBundle.load() is called.\r\nOur Environment\r\nOS - Ubuntu 14.04\r\nJava - 1.7\r\nJetty - 8\r\n\r\n@asimshankar any updates here?", "My apologies, no updates yet. The last time I dug into it, I had observed some curious behavior but didn't get to the bottom of it. Haven't had the change to dig into it further for a while.", "We are facing the same problem here.\r\nIs there any updates?", "> @asimshankar Helped me a lot\r\n> \r\n> Sorry, the code snippet is incomplete. I invoke only outputs.clear().\r\n> I'll test this invocation solution **Tensor.close()** on the elements in the **outputs**.\r\n> \r\n> Thank you so much. I apologize for the inconvenience of posting a problem other than what was presented in this post.\r\n> \r\n> thank you all\r\n\r\nhi, `Tensor.close()`has any help for memory leak problem?", "me too", "Hi @chrisrsipes! \r\nIt seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17929, "title": "Branch 190078959", "body": "", "comments": []}, {"number": 17928, "title": "Fix the mirror link to replace bazel-mirror by mirror.bazel", "body": "This PR is to fix the mirror link pattern in the download_dependencies make file.\r\nAs we can see, mirror links in [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl) is marked as mirror.bazel instead of bazel-mirror.", "comments": []}, {"number": 17927, "title": "Update tensorflow/contrib/lite/BUILD", "body": "exports_files([\"LICENSE\"]) gives error while building on Mac and Ubuntu", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@andrehentz can you update label for merging ? ", "@andrehentz can you please merge this pull request?"]}, {"number": 17926, "title": "Fix minor markdown typo in lite models.md", "body": "This PR is to fix a minor markdown typo in the lite [models.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md).\r\n\r\nBasically add a space after # to let it become a highlighted title.", "comments": []}, {"number": 17925, "title": "MaskedAutoregressiveFlow example (tf.contrib.distributions) raises ValueError", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab (also tried on a Windows 10 machine with TF 1.6)\r\n- **TensorFlow installed from (source or binary)**: defaults from Colab\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6.3 (default, Oct  3 2017, 21:45:48)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see attached file\r\n\r\n### Describe the problem\r\nI'm new to tf.contrib.distributions. I've just copied the example for MaskedAutoregressiveFlow from [https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow](https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow). Running the example fails with a ValueError at `maf.sample()`. See the attached file and error log below.  Running `tf.global_variables_initializer()` in the session doesn't solve it either. It looks like `masked_autoregressive_default_template` expects a tensor with `ndim>1` but `MaskedAutoregressiveFlow.forward()` passes a tensor with `ndim=1`. \r\n\r\n[masked_autoregressive_issue.txt](https://github.com/tensorflow/tensorflow/files/1837741/masked_autoregressive_issue.txt)\r\n\r\n\r\n### Source code / logs\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-646c58f4e818> in <module>()\r\n     12 sess.run(tf.global_variables_initializer())\r\n     13 \r\n---> 14 x = maf.sample()  # Expensive; uses `tf.while_loop`, no Bijector caching.\r\n     15 maf.log_prob(x)   # Almost free; uses Bijector caching.\r\n     16 maf.log_prob(0.)  # Cheap; no `tf.while_loop` despite no Bijector caching.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/distribution.py in sample(self, sample_shape, seed, name)\r\n    687       samples: a `Tensor` with prepended dimensions `sample_shape`.\r\n    688     \"\"\"\r\n--> 689     return self._call_sample_n(sample_shape, seed, name)\r\n    690 \r\n    691   def _log_prob(self, value):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/transformed_distribution.py in _call_sample_n(self, sample_shape, seed, name, **kwargs)\r\n    411       # work, it is imperative that this is the last modification to the\r\n    412       # returned result.\r\n--> 413       y = self.bijector.forward(x, **kwargs)\r\n    414       y = self._set_sample_static_shape(y, sample_shape)\r\n    415 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bijector_impl.py in forward(self, x, name)\r\n    618       NotImplementedError: if `_forward` is not implemented.\r\n    619     \"\"\"\r\n--> 620     return self._call_forward(x, name)\r\n    621 \r\n    622   def _inverse(self, y):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bijector_impl.py in _call_forward(self, x, name, **kwargs)\r\n    599       if mapping.y is not None:\r\n    600         return mapping.y\r\n--> 601       mapping = mapping.merge(y=self._forward(x, **kwargs))\r\n    602       self._cache(mapping)\r\n    603       return mapping.y\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py in _forward(self, x)\r\n    245     y0 = array_ops.zeros_like(x, name=\"y0\")\r\n    246     # call the template once to ensure creation\r\n--> 247     _ = self._shift_and_log_scale_fn(y0)\r\n    248     def _loop_body(index, y0):\r\n    249       \"\"\"While-loop body for autoregression calculation.\"\"\"\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/template.py in __call__(self, *args, **kwargs)\r\n    358           custom_getter=self._custom_getter) as vs:\r\n    359         self._variable_scope = vs\r\n--> 360         result = self._call_func(args, kwargs)\r\n    361         return result\r\n    362 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/template.py in _call_func(self, args, kwargs)\r\n    300       trainable_at_start = len(\r\n    301           ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES))\r\n--> 302       result = self._func(*args, **kwargs)\r\n    303 \r\n    304       if self._variables_created:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py in _fn(x)\r\n    478             activation=activation,\r\n    479             *args,\r\n--> 480             **kwargs)\r\n    481       x = masked_dense(\r\n    482           inputs=x,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py in masked_dense(inputs, units, num_blocks, exclusive, kernel_initializer, reuse, name, *args, **kwargs)\r\n    386         *args,\r\n    387         **kwargs)\r\n--> 388     return layer.apply(inputs)\r\n    389 \r\n    390 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in apply(self, inputs, *args, **kwargs)\r\n    807       Output tensor(s).\r\n    808     \"\"\"\r\n--> 809     return self.__call__(inputs, *args, **kwargs)\r\n    810 \r\n    811   def _add_inbound_node(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    671 \r\n    672           # Check input assumptions set before layer building, e.g. input rank.\r\n--> 673           self._assert_input_compatibility(inputs)\r\n    674           if input_list and self._dtype is None:\r\n    675             try:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in _assert_input_compatibility(self, inputs)\r\n   1195                            ', found ndim=' + str(ndim) +\r\n   1196                            '. Full shape received: ' +\r\n-> 1197                            str(x.get_shape().as_list()))\r\n   1198       # Check dtype.\r\n   1199       if spec.dtype is not None:\r\n\r\nValueError: Input 0 of layer dense_1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [5]\r\n\r\noriginally defined at:\r\n  File \"<ipython-input-3-646c58f4e818>\", line 10, in <module>\r\n    hidden_layers=[512,512])),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py\", line 499, in masked_autoregressive_default_template\r\n    \"masked_autoregressive_default_template\", _fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/template.py\", line 152, in make_template\r\n    **kwargs)\r\n```\r\nedit: fixed link to TF doc", "comments": ["It looks like changing the base distribution to have shape [1,1] eliminates the exception:\r\n`distribution=tfd.Normal(loc=[[0.]], scale=[[1.]])`.\r\n\r\nI feel silly realising this only now but it seems the general semantics of tf.layers expects the first dimension of inputs to be `batch_size`. If this is the case, may I suggest that this could be made clearer in the tf.layers documentation and/or reflected in the tf.contrib.distributions examples?", "@jvdillon @langmore @ebrevdo : Mind taking a look?", "Thanks for reporting! Ill take a look ASAP.", "Nagging Assignees @jvdillon, @ebrevdo: It has been 253 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed by https://github.com/tensorflow/probability/commit/50f38f3b0184c1a441921fb7826d9c76672ca44f. Switch over to the bijector in that repository for the fix."]}, {"number": 17924, "title": "[tensorflow] Android studio doesn't fetch setUseNNAPI", "body": "in build.gradle below dependency will not download\r\nlatest file of ./tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java\r\n\r\ndependencies {\r\ncompile 'org.tensorflow:tensorflow-lite:+'\r\n}\r\n\r\nHence we are getting unresolved symbol for new API setUseNNAPI\r\n\r\nas gradle unable to download new API as below\r\n\r\n  /** Turns on/off Android NNAPI for hardware acceleration when it is available. */\r\n  public void setUseNNAPI(boolean useNNAPI) {\r\n    if (wrapper != null) {\r\n      wrapper.setUseNNAPI(useNNAPI);\r\n    } else {\r\n      throw new IllegalStateException(\"NativeInterpreterWrapper has already been closed.\");\r\n    }\r\n  }\r\n", "comments": ["Could someone reply please", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @dnpawate Is this a new project integrating TF?\r\n\r\nAs long as you configure repositories like below, it should work:\r\nrepositories {\r\n    maven {\r\n        url 'https://google.bintray.com/tensorflow'\r\n    }\r\n}\r\n\r\nIf you still cannot make it work, would you mind sharing your (2) build.gradles? You can delete your other dependencies / app names etc. ", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This should be fixed now. Please reopen if necessary."]}, {"number": 17923, "title": "Does Tensorflow Lite supports LSTM?", "body": "Hi, \r\n\r\nTensorflow Lite does not supports tf.tanh according to [compatibility guide].(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md). \r\nHowever tf.tanh is usually part of LSTM. \r\nThen, Does TF-Lite support LSTM or not? \r\nDoes TF-Lite have compatible operator of BasicLSTMCell?\r\n\r\nThanks.", "comments": ["Turns out the 'tanh' function and the sigmoid function are related by tanh(a) = 2*sigmoid(2*a) - 1. tf.sigmoid is supported by Tensorflow Lite, so if there is no LSTM implementation we could replace tf.tanh with tf.sigmoid as needed and have one created.", "How do I replace this in my code? Is there an official release of TFLite that supports RNNs already?", "@op21beyond @Ala-Me-Da @RoboEvangelist were any of you able to successfully port an LSTM model to TF Lite? \r\nIf yes, could you please share the source code?"]}, {"number": 17922, "title": "'cannot import name string_int_label_map_pb2' ", "body": "Hello there;\r\n\r\nI am using tensorflow 1.6 cpu in windows 8.1 anaconda jupyter notebook\r\n\r\nI want to use tensorflow object detection api and I follow the installation instruction provided in the  models tensorflow github repository\r\n\r\naccording to that instructions \r\n\r\n1. First I installed tensorflow and the other dependencies \r\n2. I did the protobuf compilation\r\n3. The pythonpath is also being set\r\nand the setup.py file is being run\r\nbut when I try to use the object_detection_tutorial.ipynb it still shows the \"import error cannot import name string_int_label_map_pb2\" \r\nplease help me if I am doing something wrong, I know this issue is old but still, it causes a lot of problems. help me \r\n\r\nThanking you in anticipation\r\n", "comments": ["Hey, I myself am getting error with \r\n`py train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config`\r\ncommand in cmd. I am using windows 10 OS. I am getting the error\r\n\r\n`ImportError: cannot import name 'preprocessor_pb2'`\r\nEven though I have compiled all the proto files and preprocessor_pb2.py file exists in object-detection/protos directory.\r\nLet me too know if you find any solution to it!", "I have checked the proto files  from the \"models/research/object_detefction/proto \" , and  i found that there is proto file named 'string_int_label_map.proto',  I changed the import 'string_int_label_map_pb2'in label.map.util to 'string_int_label_map' but it still shows the error. ", "Hey I found the solution I follow the steps given in this link in case you are not able to understand I will explain it to you\r\n[https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10](link)", "Nice to see you figured it out!", "I am getting same error and ling provided by @yash486gadhavi  is broken", "https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\r\n\r\nThis is the link I checked it again it works\r\n \r\nIn case if you are not able to find out that GitHub page just write \"Tensorflow object detection windows 10 \" the first link we get you to the git hub page\r\n\r\n", "> Hey I found the solution I follow the steps given in this link in case you are not able to understand I will explain it to you\r\n> [https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10](link)\r\n\r\nI am facing this error when I am running the jupyter notebook:\r\n\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-aa270cd948af> in <module>()\r\n----> 1 from utils import label_map_util\r\n      2 \r\n      3 from utils import visualization_utils as vis_util\r\n\r\n/home/icts/models/research/object_detection/utils/label_map_util.py in <module>()\r\n     19 import tensorflow as tf\r\n     20 from google.protobuf import text_format\r\n---> 21 from object_detection.protos import string_int_label_map_pb2\r\n     22 \r\n     23 \r\n\r\nImportError: cannot import name string_int_label_map_pb2\r\n\r\nWhen I run:\r\nprotoc object_detection/protos/*.proto --python_out=\r\nI am getting the following error:\r\n\r\nobject_detection/protos/ssd.proto:104:3: Expected \"required\", \"optional\", or \"repeated\".\r\nobject_detection/protos/ssd.proto:104:12: Expected field name.\r\nobject_detection/protos/model.proto: Import \"object_detection/protos/ssd.proto\" was not found or had errors.\r\nobject_detection/protos/model.proto:12:5: \"Ssd\" is not defined.\r\n\r\nI am working on machine with ubuntu version 16.04", "> \r\n> \r\n> I am getting same error and ling provided by @yash486gadhavi is broken\r\n\r\nIf your issue is the following error\r\n\r\n> ImportError                               Traceback (most recent call last)\r\n> <ipython-input-3-aa270cd948af> in <module>()\r\n> ----> 1 from utils import label_map_util\r\n>       2 \r\n>       3 from utils import visualization_utils as vis_util\r\n> \r\n> ~\\models\\research\\object_detection\\utils\\label_map_util.py in <module>()\r\n>      19 import tensorflow as tf\r\n>      20 from google.protobuf import text_format\r\n> ---> 21 from object_detection.protos import string_int_label_map_pb2\r\n>      22 \r\n>      23 \r\n> \r\n> ImportError: cannot import name 'string_int_label_map_pb2'   \r\n\r\nand if your protoc file is in the models/research/  folder, \r\n\r\n1. go to the utils folder in the object_detection folder\r\n2. open the label_map_util.py file in an editor and edit the line 'from object_detection.protos import string_int_label_map_pb2' to 'from protos import string_int_label_map_pb2", "> \r\n> \r\n> > Hey I found the solution I follow the steps given in this link in case you are not able to understand I will explain it to you\r\n> > [https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10](link)\r\n> \r\n> I am facing this error when I am running the jupyter notebook:\r\n> \r\n> ImportError Traceback (most recent call last)\r\n> in ()\r\n> ----> 1 from utils import label_map_util\r\n> 2\r\n> 3 from utils import visualization_utils as vis_util\r\n> \r\n> /home/icts/models/research/object_detection/utils/label_map_util.py in ()\r\n> 19 import tensorflow as tf\r\n> 20 from google.protobuf import text_format\r\n> ---> 21 from object_detection.protos import string_int_label_map_pb2\r\n> 22\r\n> 23\r\n> \r\n> ImportError: cannot import name string_int_label_map_pb2\r\n> \r\n> When I run:\r\n> protoc object_detection/protos/*.proto --python_out=\r\n> I am getting the following error:\r\n> \r\n> object_detection/protos/ssd.proto:104:3: Expected \"required\", \"optional\", or \"repeated\".\r\n> object_detection/protos/ssd.proto:104:12: Expected field name.\r\n> object_detection/protos/model.proto: Import \"object_detection/protos/ssd.proto\" was not found or had errors.\r\n> object_detection/protos/model.proto:12:5: \"Ssd\" is not defined.\r\n> \r\n> I am working on machine with ubuntu version 16.04\r\n\r\nThe new protoc version does not allow you to do **protoc object_detection/protos/*.proto --python_out=**. I hope you understand that *.proto  executes the above command for all files with extension .proto in the object_detection/protos/ folder. In the new version, you will need to compile each file in protos folder individually as, for example,  **protoc object_detection/protos/anchor_generator.proto --python_out=**.\r\n\r\nAs for the import error, I have mentioned a few steps above, that you can follow to solve it.", "Simply, go to **protos** dirctory in object detection folder in your pc and put this python file into the it:\r\n[https://github.com/datitran/object_detector_app/blob/master/object_detection/protos/string_int_label_map_pb2.py](url)", "this is the fix in @pranka02's solution above:\r\n\r\n> and if your protoc file is in the models/research/ folder,\r\n\r\n> go to the utils folder in the object_detection folder\r\n> open the label_map_util.py file in an editor and edit the line 'from object_detection.protos import string_int_label_map_pb2' to > 'from protos import string_int_label_map_pb2"]}, {"number": 17921, "title": "Using Android studio, Interpreter.java is not getting picked for compiling", "body": "I am using android studio and have made changes in Imageclassifier calling setUseNNAPI. But it is not getting picked from Interpreter.java which enable developers turn on & off NNAPI. \r\n\r\n\r\nOS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow installed from: Git cloned\r\nTensorFlow version: N/A\r\nBazel version: Build label: 0.11.1\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: x86 8GB RAM\r\nExact command to reproduce: Using Android studio as a existing project", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "OS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow installed from: Git cloned\r\nTensorFlow version: N/A\r\nBazel version: Build label: 0.11.1\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: x86 8GB RAM\r\nExact command to reproduce: Using Android studio as a existing project", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17920, "title": "Fix incosistencies in the shape of the input Tensor in Layers Doc", "body": "This pull request is to fix the inconsistencies in the Tensorflow Layers doc in #17892 and adds to #17893 which only fixes a single instance.\r\n\r\n### Problem\r\nThe Tensorflow Layers Guide at https://www.tensorflow.org/tutorials/layers specifies:\r\n\r\n> The methods in the layers module for creating convolutional and pooling layers for two-dimensional image data expect input tensors to have a shape of [batch_size, image_width, image_height, channels]\r\n\r\nWhile, the inline documentation specified for conv2D (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L444) mentions the following valid data_formats:\r\n\r\n> data_format: A string, one of `channels_last` (default) or `channels_first`.\r\n      The ordering of the dimensions in the inputs.\r\n      `channels_last` corresponds to inputs with shape\r\n      `(batch, height, width, channels)` while `channels_first` corresponds to\r\n      inputs with shape `(batch, channels, height, width)`\r\n\r\nThere are several instances where height and width are not ordered correctly.\r\n\r\nMoreover, for data_format=channels_first the shape should be NCHW while it is specified as CNWH.\r\n\r\nThis PR fixes these issues.", "comments": ["Hi @benoitsteiner ,\r\n\r\nI see that the code review for this PR is complete but it is still labelled \"awaiting review\". Are there any more reviews required?\r\n\r\nThanks!", "Switched from \"awaiting review\" to \"awaiting testing (then merge).\""]}, {"number": 17919, "title": "Tensorflow Install Problem", "body": "I installed Tensorflow 1.6 CPU only on Linux Ubuntu 17.10 use Virtualenv approach last night, follow the installing instructions 1-5, every step are success. after system response install  success, I want to validate the installation. the steps as below:\r\n1. type the commond \"source ~/tensorflow/bin/activate\" the os system response:\r\n(tensorflow) bruce@bruce:~$ \r\n2.input python response:\r\n(tensorflow) bruce@bruce:~$ python\r\nPython 3.6.3 (default, Oct  3 2017, 21:45:48) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n3.input \"import tensorflow as tf\". the system crashed:\r\n>>> import tensorflow as tf\r\n\u975e\u6cd5\u6307\u4ee4 (\u6838\u5fc3\u5df2\u8f6c\u50a8)\r\n(tensorflow) bruce@bruce:~$\r\n\r\nI try many times the result same as above descript.Even i restart my laptop the problem still here. i use the gdb open the core file, the information as below, i don't know how to handle  this problem . for i couldn't register account on stack overflow, so i couldn't ask question at that site. Who can help me solve this issue, please help me!\r\n\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nCore was generated by `python'.\r\nProgram terminated with signal SIGILL, Illegal instruction.\r\n#0  0x00007fb0828de880 in std::pair<std::__detail::_Node_iterator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, false, true>, bool> std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, std::allocator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> > >(std::integral_constant<bool, true>, std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> >&&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n[Current thread is 1 (Thread 0x7fb0916f0740 (LWP 2908))]\r\n(gdb) \r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom binary\r\n- **TensorFlow version (use command below)**:\r\n1.6\r\n- **Python version**: \r\n3.6.3\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n\r\n", "comments": ["Can you `cat /proc/cpuinfo` and tell me what type of CPU you have?", "Thank you for your response! I change the system to win10 the CUP information is : Intel Celeron CPU 3865U@1.8GHz \r\n\r\nBTW: I install the tensorflow in another laptop which CPU is Intel i5, the tensorflow works.", "I believe this is happening the tensorflow binaries in PIP were built with AVX instructions, which are not supported on older or low-end CPUs.\r\n\r\nI personally have an old model \"Intel(R) Core(TM) i7 CPU         860  @ 2.80GHz\" (Lynnfield) and am experiencing the exact same problem with tensorflow 1.6.  Installing 1.5 is an effective workaround for the time being.\r\n\r\nThis is discussed more in #17411, specifically [this comment](https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-373938846).", "thank you nacl, i think you are right  and i believe tensor 1.6 can't installed in the celeron CPU. close the issue. "]}, {"number": 17918, "title": "swapped const and variables in documentation", "body": "The freeze_graph.py is converting Variables to Consts, not the other way around :)", "comments": []}, {"number": 17917, "title": "Bazel build FAILED on Ubuntu16.04 caused by extension file to detect MSVC for Windows", "body": "When using BAZEL to build TF r.17 from source on Ubuntu16.04. It failed caused by the bazel extension file located at /tensorflow/tools/def_file_filter/def_file_filter_configure.bzl, which is used to export symbols from TF DLL file for Windows. The same problem occurs for building TF-Lite also.\r\n\r\nCommend:\r\n> bazel build --config=opt  //tensorflow/tools/pip_package:build_pip_package\r\n> bazel build --config=opt   //tensorflow/contrib/lite/toco:toco\r\n\r\nBazel output:\r\n**ERROR:** ~/tensorflow-r1.7/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl:22:1: file '@bazel_tools//tools/cpp:windows_cc_configure.bzl' does not contain symbol 'find_vc_path' (did you mean '_find_vc_path'?).\r\n**ERROR:** ~/tensorflow-r1.7/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl:23:1: file '@bazel_tools//tools/cpp:windows_cc_configure.bzl' does not contain symbol 'find_msvc_tool' (did you mean '_find_msvc_tool'?).\r\n**ERROR:** error loading package '': Extension file 'tensorflow/tools/def_file_filter/def_file_filter_configure.bzl' has errors.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@yifeif are you able to comment?", "@HwMohanLiu which bazel version are you using?", "According to the problem that bazel can not fetch the sources behind a proxy, I used a modified bazel from https://github.com/ixuexi/bazel, which using wget to download sources instead. However, the problem is caused by the bazel version. I updated bazel to version 0.11.0. The proxy problem of bazel have been solved by installing bazel without JDK, setting cntlm proxy and adding certificate to system JDK. And then, it works now."]}, {"number": 17916, "title": " ImportError: libcublas.so.9.0: cannot", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Can you fill template and the steps which you are trying to execute?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17915, "title": "change denominator so that the normalized values to [-1, +1] range", "body": "the origin code just normalized the values to [-0.5, 0.5], we need normalized the values to [-1, +1] range.", "comments": ["This was somewhat intentional. There is no 'magic' to the 1.0 value, all that matters is that it's a number small enough that the dynamic range of the activations is sane. I also don't want to break backward compatibility since it's meant to be referred to in educational material."]}, {"number": 17914, "title": "TF 1.7.0-rc1 unable to build with TensorRT support", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0-rc1\r\n- **Python version**: Python 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0-6ubuntu1~16.04.9\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5\r\n- **GPU model and memory**: GTX 1070, 8Gb\r\n- **Exact command to reproduce**: `bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\n### Describe the problem\r\nTry to build TF with TensorRT support using the command above and following [this](https://github.com/tensorflow/tensorflow/tree/v1.7.0-rc1/tensorflow/contrib/tensorrt) instruction, i.e. there is no TensorRT installed from repo, only downloaded binaries for 14.04 + `echo \"<install_dir>/TensorRT-3.0.4/lib\" | sudo tee /etc/ld.so.conf.d/tensorrt304.conf && sudo ldconfig`, as suggested. \r\nThe `libcudnn.so*` available in TensorRT `lib` dir, `/usr/local/lib` and `/usr/lib/x86_64-linux-gnu`. `LD_LIBRARY_PATH` contains all relevant paths. \r\nWhile `configure`, I pointed to TensorRT binaries dir when it asked.\r\n`/usr/local/cuda` points to `9.0` version and this path was specified while `configure`.\r\n\r\n### Source code / logs\r\n```\r\nWARNING: /home/alexandr/.cache/bazel/_bazel_alexandr/6c93df3ac6ca06598f74e718588ae6cc/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/alexandr/.cache/bazel/_bazel_alexandr/6c93df3ac6ca06598f74e718588ae6cc/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: /home/alexandr/distr/tensorflow_tensorrt/tensorflow/core/BUILD:1955:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/alexandr/distr/tensorflow_tensorrt/tensorflow/tensorflow.bzl:1179:30\r\nWARNING: /home/alexandr/distr/tensorflow_tensorrt/tensorflow/core/BUILD:1955:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/alexandr/distr/tensorflow_tensorrt/tensorflow/tensorflow.bzl:1179:30\r\nWARNING: /home/alexandr/.cache/bazel/_bazel_alexandr/6c93df3ac6ca06598f74e718588ae6cc/external/grpc/WORKSPACE:1: Workspace name in /home/alexandr/.cache/bazel/_bazel_alexandr/6c93df3ac6ca06598f74e718588ae6cc/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions\r\nWARNING: /home/alexandr/distr/tensorflow_tensorrt/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/alexandr/distr/tensorflow_tensorrt/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/alexandr/distr/tensorflow_tensorrt/tensorflow/contrib/tensorrt/BUILD:122:1: Linking of rule '//tensorflow/contrib/tensorrt:gen_trt_engine_op_py_wrappers_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/alexandr/.cache/bazel/_bazel_alexandr/6c93df3ac6ca06598f74e718588ae6cc/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64 \\\r\n    PATH=/home/alexandr/bin:/home/alexandr/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/contrib/tensorrt/gen_trt_engine_op_py_wrappers_cc '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U_S_Stensorflow_Scontrib_Stensorrt_Cgen_Utrt_Uengine_Uop_Upy_Uwrappers_Ucc___Utensorflow' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensorrt_Cgen_Utrt_Uengine_Uop_Upy_Uwrappers_Ucc___Utensorflow -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..' -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -Wl,-no-as-needed -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/contrib/tensorrt/gen_trt_engine_op_py_wrappers_cc-2.params)\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreate@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreatePoolingDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateConvolutionDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyConvolutionDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetReductionWorkspaceSize@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetPooling2dDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyFilterDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetReduceTensorDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnLRNCrossChannelForward@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateRNNDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyRNNDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnConvolutionBackwardData@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateFilterDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetTensor4dDescriptorEx@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyLRNDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroy@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDropoutGetStatesSize@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetConvolutionForwardWorkspaceSize@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetActivationDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetRNNWorkspaceSize@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetConvolutionGroupCount@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetTensorNdDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnAddTensor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyActivationDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetConvolutionBackwardDataAlgorithmMaxCount@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetFilterNdDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetLRNDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyTensorDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyDropoutDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateTensorDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetStream@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyOpTensorDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateOpTensorDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateDropoutDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetFilter4dDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyPoolingDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateActivationDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnRNNForwardInference@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnActivationForward@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetRNNLinLayerMatrixParams@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetConvolutionBackwardDataWorkspaceSize@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnReduceTensor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetRNNDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetDropoutDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSoftmaxForward@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnConvolutionForward@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetRNNLinLayerBiasParams@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetConvolution2dDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetFilterNdDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnGetRNNParamsSize@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetTensor4dDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnSetOpTensorDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnDestroyReduceTensorDescriptor@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnFindConvolutionBackwardDataAlgorithmEx@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnPoolingForward@libcudnn.so.7'\r\nbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib/libnvinfer.so.4: undefined reference to `cudnnCreateLRNDescriptor@libcudnn.so.7'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 0.642s, Critical Path: 0.22s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n---\r\nThe extended system info:\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux power-linux 4.12.0-041200-generic #201707022031 SMP Mon Jul 3 00:32:52 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux power-linux 4.12.0-041200-generic #201707022031 SMP Mon Jul 3 00:32:52 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmsgpack-numpy (0.4.1)\r\nnumpy (1.14.2)\r\nprotobuf (3.5.2.post1)\r\ntensorflow (1.6.0)\r\ntensorflow-tensorboard (0.4.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.1\r\ntf.GIT_VERSION = v1.4.1-0-g438604f\r\ntf.COMPILER_VERSION = v1.4.1-0-g438604f\r\nSanity check: array([1], dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH :/usr/local/cuda/lib64:/home/alexandr/distr/tensorrt/lib:/usr/local/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Mar 22 10:33:28 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   57C    P0    41W /  N/A |    372MiB /  8112MiB |     39%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1166      G   /usr/lib/xorg/Xorg                           211MiB |\r\n|    0      3086      G   compiz                                       150MiB |\r\n|    0      3136      G   ...-token=69155177EA9D6BB8687BA31223F7A104     7MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.7\r\n```", "comments": ["Check that you have the current libcudnn installed and your build env finds it (seem that libcudnn.so.7 is wanted).", "@svarjo what you mean by \r\n> build env\r\n\r\n?\r\npay an attention that\r\n> The libcudnn.so* available in TensorRT lib dir, /usr/local/lib and /usr/lib/x86_64-linux-gnu. LD_LIBRARY_PATH contains all relevant paths.", "And BTW, so many steps before the fail `bazel` is able to link `cudnn`. I suppose `TensorRT` step is quite close to the end of the build, maybe about 60-70%. Unbelievable that `cudnn` linking wasn't needed until `TensorRT` step.", "Hi @arassadin, did you set the cuDNN installation path to the tensorrt's `lib` dir? I.e. what did you set for this prompt:\r\n`Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:`", "Hi @aaroey ,\r\n\r\nI set it to `/usr/local` where it really located.", "This is weird, would you please paste your `.tf_configure.bazelrc` file content here? Also @samikama.", "I though in your internal builds, LD_LIBRARY_PATH was ignored due to sandboxing. @aaroey mentioned had to do some fixes to workaround it. Also `bazel build <your other config flags> -s //tensorflow/contrib/tensorrt:gen_trt_engine_op_py_wrappers_cc` might help with identifying the issue. Another point, notice that cudnn is not added to the linker command line. Check whether it is in linker scipt. if not you are missing a dependency in your repositories.\r\n", "Hi @aaroey , @samikama \r\n\r\nMy `.tf_configure.bazelrc`:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=false\r\nbuild:hdfs --define with_hdfs_support=false\r\nbuild:s3 --define with_s3_support=false\r\nbuild:kafka --define with_kafka_support=false\r\nbuild --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=false\r\nbuild:verbs --define with_verbs_support=false\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/local/lib\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TENSORRT_INSTALL_PATH=\"/home/alexandr/distr/tensorrt/targets/x86_64-linux-gnu/lib\"\r\nbuild --action_env TF_TENSORRT_VERSION=\"4.0.4\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/lib:/usr/local/tensorrt/lib:/usr/local/cuda/lib64\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n```\r\nand it still failes on `v1.7.0` with the same error (`/usr/local/tensorrt/lib` is a link to `/home/alexandr/distr/tensorrt/targets/x86_64-linux-gnu/lib`).\r\n\r\nI tried `bazel build LD_LIBRARY_PATH=\"/usr/local/lib:/usr/local/tensorrt/lib:/usr/local/cuda/lib64\" -s //tensorflow/contrib/tensorrt:gen_trt_engine_op_py_wrappers_cc` but result is the same.\r\n\r\nI not really understand\r\n> Another point, notice that cudnn is not added to the linker command line. Check whether it is in linker scipt. if not you are missing a dependency in your repositories.\r\n\r\nIt in all standart (system-wide) linker paths\r\n```\r\nsudo find /usr/ -name libcudnn.so*\r\n/usr/lib/libcudnn.so\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n/usr/lib/libcudnn.so.7\r\n/usr/lib/libcudnn.so.7.0.5\r\n/usr/local/lib/libcudnn.so\r\n/usr/local/lib/libcudnn.so.7\r\n/usr/local/lib/libcudnn.so.7.0.5\r\n```\r\nand in TensorRT `lib` dir. I can compile anything with `-lcudnn` flag and exec it. Can you elaborate more on this?..", "@arassadin , Could you please post the contents of linker script `gen_trt_engine_op_py_wrappers_cc-2.params`. If you are using normal compiler commands from command-line, -lcudnn would work. But bazel uses sandboxed compilation and may (will) try to avoid libraries from system. Also notice that -lcudnn is *not* in your command line.\r\n", "I was able to found two such files:\r\n* `org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/tensorrt/gen_trt_engine_op_py_wrappers_cc-2.params`:\r\n```\r\n-ltensorflow_framework\r\nbazel-out/k8-py3-opt/bin/tensorflow/python/libpython_op_gen_main.a\r\nbazel-out/k8-py3-opt/bin/tensorflow/python/eager/libpython_eager_op_gen.a\r\n-whole-archive\r\nbazel-out/k8-py3-opt/bin/tensorflow/python/libpython_op_gen.lo\r\n-no-whole-archive\r\nbazel-out/k8-py3-opt/bin/tensorflow/core/libop_gen_lib.a\r\n-whole-archive\r\nbazel-out/k8-py3-opt/bin/tensorflow/contrib/tensorrt/libtrt_calib_op_op_lib.lo\r\n-no-whole-archive\r\n-whole-archive\r\nbazel-out/k8-py3-opt/bin/tensorflow/contrib/tensorrt/libtrt_engine_op_op_lib.lo\r\n-no-whole-archive\r\nbazel-out/k8-py3-opt/bin/external/com_google_absl/absl/base/libbase.a\r\nbazel-out/k8-py3-opt/bin/external/com_google_absl/absl/base/libspinlock_wait.a\r\nbazel-out/k8-py3-opt/bin/external/com_google_absl/absl/base/libdynamic_annotations.a\r\nbazel-out/k8-py3-opt/bin/external/gif_archive/libgif.a\r\nbazel-out/k8-py3-opt/bin/external/jpeg/libjpeg.a\r\nbazel-out/k8-py3-opt/bin/external/jpeg/libsimd_x86_64.a\r\nbazel-out/k8-py3-opt/bin/external/com_googlesource_code_re2/libre2.a\r\nbazel-out/k8-py3-opt/bin/external/farmhash_archive/libfarmhash.a\r\nbazel-out/k8-py3-opt/bin/external/fft2d/libfft2d.a\r\nbazel-out/k8-py3-opt/bin/external/highwayhash/libsip_hash.a\r\nbazel-out/k8-py3-opt/bin/external/highwayhash/libarch_specific.a\r\nbazel-out/k8-py3-opt/bin/external/png_archive/libpng.a\r\nbazel-out/k8-py3-opt/bin/external/zlib_archive/libzlib.a\r\nbazel-out/k8-py3-opt/bin/tensorflow/contrib/tensorrt/libtrt_shape_function.a\r\nbazel-out/k8-py3-opt/bin/tensorflow/contrib/tensorrt/libtrt_logging.a\r\nbazel-out/k8-py3-opt/bin/tensorflow/core/liblib_proto_parsing.a\r\n-l:libnvinfer.so.4\r\n-l:libcublas.so.9.0\r\n-l:libcudnn.so.7\r\n-l:libcufft.so.9.0\r\n-l:libcurand.so.9.0\r\n-l:libcudart.so.9.0\r\n-lm\r\n-ldl\r\n-lpthread\r\n-lm\r\n-lm\r\n-lstdc++\r\n```\r\nand\r\n* `org_tensorflow/bazel-out/host/bin/tensorflow/contrib/tensorrt/gen_trt_engine_op_py_wrappers_cc-2.params`:\r\n```\r\n-ltensorflow_framework\r\nbazel-out/host/bin/tensorflow/python/libpython_op_gen_main.a\r\nbazel-out/host/bin/tensorflow/python/eager/libpython_eager_op_gen.a\r\n-whole-archive\r\nbazel-out/host/bin/tensorflow/python/libpython_op_gen.lo\r\n-no-whole-archive\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a\r\n-whole-archive\r\nbazel-out/host/bin/tensorflow/contrib/tensorrt/libtrt_calib_op_op_lib.lo\r\n-no-whole-archive\r\n-whole-archive\r\nbazel-out/host/bin/tensorflow/contrib/tensorrt/libtrt_engine_op_op_lib.lo\r\n-no-whole-archive\r\nbazel-out/host/bin/external/com_google_absl/absl/base/libbase.a\r\nbazel-out/host/bin/external/com_google_absl/absl/base/libspinlock_wait.a\r\nbazel-out/host/bin/external/com_google_absl/absl/base/libdynamic_annotations.a\r\nbazel-out/host/bin/external/gif_archive/libgif.a\r\nbazel-out/host/bin/external/jpeg/libjpeg.a\r\nbazel-out/host/bin/external/jpeg/libsimd_x86_64.a\r\nbazel-out/host/bin/external/com_googlesource_code_re2/libre2.a\r\nbazel-out/host/bin/external/farmhash_archive/libfarmhash.a\r\nbazel-out/host/bin/external/fft2d/libfft2d.a\r\nbazel-out/host/bin/external/highwayhash/libsip_hash.a\r\nbazel-out/host/bin/external/highwayhash/libarch_specific.a\r\nbazel-out/host/bin/external/png_archive/libpng.a\r\nbazel-out/host/bin/external/zlib_archive/libzlib.a\r\nbazel-out/host/bin/tensorflow/contrib/tensorrt/libtrt_shape_function.a\r\nbazel-out/host/bin/tensorflow/contrib/tensorrt/libtrt_logging.a\r\nbazel-out/host/bin/tensorflow/core/liblib_proto_parsing.a\r\n-l:libnvinfer.so.4\r\n-l:libcublas.so.9.0\r\n-l:libcudnn.so.7\r\n-l:libcufft.so.9.0\r\n-l:libcurand.so.9.0\r\n-l:libcudart.so.9.0\r\n-lm\r\n-ldl\r\n-lpthread\r\n-lm\r\n-lm\r\n-lstdc++\r\n```\r\n\r\nHope this will help...", "Hi @arassadin, linker file contains libcudnn. Could you please also check if cudnn is in one of the directories below and not a stub.\r\n\r\nThanks,\r\nSami\r\n\r\n`\r\n-Lbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensorrt_Cgen_Utrt_Uengine_Uop_Upy_Uwrappers_Ucc___Utensorflow\r\n-Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Utensorrt_S_S_Cnv_Uinfer___Uexternal_Slocal_Uconfig_Utensorrt_Stensorrt_Slib\r\n-Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib\r\n-Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib\r\n-Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib\r\n-Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib\r\n-Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib\r\n`", "Yes, `libcudnn.so.7` in `_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib`. It points to the corresponding `libcudnn.so.7` in `~/.cache/(...)` which points to the existing `/usr/local/lib/libcudnn.so.7.0.5`", "Hi @arassadin, I should have read the issue earlier. For building from sources, please use your native TensorRT distribution. 14.04 binaries are needed only for using the pip package without building anything. Let me know if it is fixed when you install native Deb package. Also please remove old TensorRT installation from your ld.config i.e \r\n`sudo rm /etc/ld.so.conf.d/tensorrt304.conf && sudo ldconfig`\r\n\r\nLet me know if it works.", "Hi @samikama ,\r\n\r\nFollowing the Installation Guide, I install TensorRT from the repo and purged the previous binaries (also updated linker paths). Unfortunately, result is the same.", "Whether I understand right that I can obtain pre-built wheel with TensorRT support?", "@arassadin , Yes that is correct.  Starting with TensorFlow 1.7, TensorRT support is enabled by default with tensorflow-gpu package. So you can just do pip install tensorflow-gpu and will be able to use it, provided you have the Ubuntu 14.04 version of TensorRT 3.0.4 binaries installed in your machine. If you want to build locally, then you can use your platform native TensorRT installation.\r\nBut from our discussion above, your system seems to have some problems with the installations of libraries. ", "I cannot reproduce. Marking as community support because we cannot reproduce internally.\r\n\r\nYou can try asking on StackOverflow as well.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "Closing the issue. It seems it is specific to user and looks like resolved since user reported issue #18777", "@arassadin Have you solved this problem? And I find this problem too. If you have fixed this problem, could you please share your approaches?", "@ZhenYangIACAS, still not solved"]}, {"number": 17913, "title": "Tensorflow in Android: You must feed a value for placeholder tensor 'Placeholder' with dtype float issue", "body": " have a problem with importing Tensorflow model into Android Studio application. I have built a model, froze the model and optimized that frozen model in Python, and now I'm trying to use it in the Android app, but it constantly returns me that same error that I must feed a value for a placeholder. This is happening in the Android app when I run inferenceInterface.run(OUTPUT_NODES); function.\r\n\r\nI don't know if the error is it in the model itself, but I'm assuming that python would throw me an error and won't build the model, what he did.\r\n\r\nThis is the example of rows of data I'm sending to Tensorflow (in csv file):\r\n\r\n1,26,2091,5,2,0,0,0,0,0,85,105,6,4,0,1\r\n1,26,47,9,4,0,0,0,0,0,85,0,7,4,1,0\r\n\r\n\r\nThis is the creating model in Python:\r\n*************************************************************************************************\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport seaborn as sns\r\nfrom math import floor, ceil\r\nfrom pylab import rcParams\r\n\r\n\r\ncolumns = [\"Gender\", \"Age\", \"StepsNum\", \"Still\", \"ContinuousStill\",\r\n           \"Running\", \"Driving\", \"Cycling\", \"Weather\", \"TargetWeight\",\r\n           \"Calories\", \"DayOfTheWeek\", \"PartOfTheDay\",\r\n           \"NotificationType\", \"UserInput\"]\r\n\r\nuserinput = ['0','1']\r\n\r\n\r\nactivites_df = pd.read_csv(\"E:\\\\MASTER\\\\PythonPrograms\\\\useractivityInt.csv\", header = None, names=columns)\r\n\r\n\r\n#encode all strings\r\ndef encode(series): \r\n    #print(pd.get_dummies(series.astype(str)))\r\n    return pd.get_dummies(series.astype(str))\r\n\r\n\r\ntrain_x = pd.DataFrame(activites_df, columns = columns, dtype=float)\r\n# train_x = activites_df\r\ntrain_y = encode(activites_df.UserInput)\r\nprint(train_y)\r\n# train_y = activites_df.iloc[:,-1]\r\n# print(train_y)\r\n# train_y = pd.DataFrame(userinput, dtype=float)\r\n\r\n\r\ntrain_size = 0.9\r\n\r\ntrain_cnt = floor(train_x.shape[0] * train_size)\r\n#iloc[0] - first row, iloc[:0] - first column of data frame, iloc[0:n] - first n rows\r\nx_train = train_x.iloc[0:train_cnt].values\r\ny_train = train_y.iloc[0:train_cnt].values\r\n\r\nx_test = train_x.iloc[train_cnt:].values\r\ny_test = train_y.iloc[train_cnt:].values\r\n\r\n\r\ndef multilayer_perceptron(x, weights, biases, keep_prob):\r\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\r\n    layer_1 = tf.nn.relu(layer_1)\r\n    layer_1 = tf.nn.dropout(layer_1, keep_prob)\r\n    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\r\n    return out_layer\r\n\r\n\r\n#shape[0] - Gives the number of rows in matrix.. shape[1] - numbers of columns \r\nn_hidden_1 = 38\r\nn_input = train_x.shape[1]\r\nn_classes = train_y.shape[1]\r\n# n_classes = 2\r\n\r\nweights = {\r\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]),tf.float32),\r\n    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]),tf.float32)\r\n}\r\n\r\nbiases = {\r\n    'b1': tf.Variable(tf.random_normal([n_hidden_1]),tf.float32),\r\n    'out': tf.Variable(tf.random_normal([n_classes]),tf.float32)\r\n}\r\n\r\n#keep_prob: A scalar Tensor with the same type as x. The probability that each element is kept.\r\n# keep_prob = tf.placeholder(\"float\")\r\nkeep_prob = tf.placeholder(tf.float32)\r\n\r\ntraining_epochs = 5000\r\ndisplay_step = 1000\r\nbatch_size = 32\r\n\r\nx = tf.placeholder(tf.float32, [None, n_input], name='input')\r\ny = tf.placeholder(tf.float32, [None, n_classes])\r\n\r\npredictions = multilayer_perceptron(x, weights, biases, keep_prob)\r\n\r\n#_y is name for output node\r\n#If we take an input of [1, 2, 3, 4, 1, 2, 3], the softmax of that is [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. \r\n# The output has most of its weight where the '4' was in the original input. \r\n# This is what the function is normally used for: to highlight the largest values and suppress values which are significantly below the maximum value.\r\n\r\npred_softmax = tf.nn.softmax(predictions, name=\"y_\")\r\n\r\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\r\n\r\nLEARNING_RATE = 0.0025\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    for epoch in range(training_epochs):\r\n        avg_cost = 0.0\r\n        total_batch = int(len(x_train) / batch_size)\r\n        x_batches = np.array_split(x_train, total_batch)\r\n        y_batches = np.array_split(y_train, total_batch)\r\n        for i in range(total_batch):\r\n            batch_x, batch_y = x_batches[i], y_batches[i]\r\n            _, c = sess.run([optimizer, cost], \r\n                            feed_dict={\r\n                                x: batch_x, \r\n                                y: batch_y, \r\n                                keep_prob: 0.8\r\n                            })\r\n            avg_cost += c / total_batch\r\n        if epoch % display_step == 0:\r\n            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \\\r\n                \"{:.9f}\".format(avg_cost))\r\n    print(\"Optimization Finished!\")\r\n    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\r\n\r\n    saver = tf.train.Saver()\r\n    tf.train.write_graph(sess.graph_def, '.', 'E:\\\\MASTER\\\\PythonPrograms\\\\har.pbtxt')  \r\n    saver.save(sess, save_path = \"E:\\\\MASTER\\\\PythonPrograms\\\\har.ckpt\")\r\n```\r\n\r\n****************************************************************************************************\r\nThe main parts of the code in Android Studio are those:\r\n\r\nDefinition of variables:\r\n************************************************************************************************\r\n```\r\nprivate static final String MODEL_FILE = \"file:///android_asset/optimized_frozen_har.pb\";\r\n\r\n    String INPUT_NODE = \"input\";\r\n    String[] OUTPUT_NODES = {\"y_\"};\r\n    String OUTPUT_NODE = \"y_\";\r\n    //I don't know what is input size\r\n    long[] INPUT_SIZE = {1, 15};\r\n    int OUTPUT_SIZE = 2;\r\n\r\n    private TensorFlowInferenceInterface inferenceInterface;\r\n```\r\n************************************************************************************************\r\nInitialization and calling function in OnCreate:\r\n************************************************************************************************\r\n```\r\ninferenceInterface = new TensorFlowInferenceInterface(appContext.getAssets(), MODEL_FILE);\r\n\r\n\r\n float[] data = {(float)1.0, (float)26.0, (float)1000.0, (float)3.0, (float)1.0,(float)0.0, (float) 0.0, (float)0.0, (float)0.0, (float)0.0, (float)85.0, (float)48.0, (float)7.0, (float)4.0, (float)2.0};\r\n        float[] out = predictProbabilitiesFloat(data);\r\n```\r\n*************************************************************************************************\r\nFunction for returning result from TensorFlow:\r\n*************************************************************************************************\r\n```\r\npublic float[] predictProbabilitiesFloat(float[] data) {\r\n        float[] result = new float[OUTPUT_SIZE];\r\n        inferenceInterface.feed(INPUT_NODE, data, INPUT_SIZE);\r\n        inferenceInterface.run(OUTPUT_NODES);\r\n        inferenceInterface.fetch(OUTPUT_NODE, result);\r\n\r\n        //for us it should be 0 or 1\r\n        return result;\r\n    }\r\n```\r\n************************************************************************************************\r\n\r\nIf somebody knows how to fix this problem, please help me, I have few more days to finish this as one part of my master thesis.\r\n\r\nThank you in advance!``", "comments": ["When fetching an output, all the placeholders required in order to compute the output need to be fetched. Looking at your model, it seems likely that in order to compute `OUTPUT_NODES` (i.e., the tensor named `y_`), you need to feed the input and the `keep_prob` placeholder, defined in your Python model in the line:\r\n\r\n```python\r\nkeep_prob = tf.placeholder(tf.float32)\r\n```\r\n\r\nLong story short, either change your Python model so that `keep_prob` isn't a placeholder but rather a constant. Or feed a value for it using `inferenceInterface.feed()`.\r\n\r\nHope that helps.\r\nI'm going to close this issue since this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) as it is not a  bug or feature request."]}, {"number": 17912, "title": "Update TF-TensorRT segmenter", "body": "This PR fixes some issues in TF-TensorRT integration segmenter code for graphs where there are heavy control flow connections. This PR also improves error handling and error messages.", "comments": ["Tagging @aaroey.", "No need for my review for the PR  to master."]}]