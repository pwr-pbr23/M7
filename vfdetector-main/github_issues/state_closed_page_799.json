[{"number": 29569, "title": "when i tried ''import tensorflow as tf' this came up, Can anyone help me with this, I am a newbie  ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 29568, "title": " about bottleneck_input = tf.placeholder_with_default ? ", "body": "In the original retrain.py:\r\nbottleneck_input = tf.placeholder_with_default(\r\nbottleneck_tensor, shape=[batch_size, bottleneck_tensor_size],\r\nname='BottleneckInputPlaceholder')\r\nChanged to:\r\nbottleneck_input = tf.placeholder(\r\ntf.float32, shape=[batch_size, bottleneck_tensor_size], name='BottleneckInputPlaceholder')\r\n\r\nAfter training, check the parameters in retrained_graph.pb.\r\nUtilize:\r\nimport tensorflow as tf\r\nimport os\r\n\r\nmodel_name = 'retrained_graph.pb'\r\ndef create_graph():\r\nwith tf.gfile.FastGFile(os.path.join(model_name), 'rb') as f:\r\ngraph_def = tf.GraphDef()\r\ngraph_def.ParseFromString(f.read())\r\ncreate_graph()\r\n\r\ntensor_name_list = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]\r\nfor tensor_name in tensor_name_list:\r\nprint(tensor_name,'\\n')\r\n\r\nBefore BOTTLENECK_TENSOR_NAME = 'pool_3/_reshape:0' was discovered, all the parameters were missing. Why?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact code to reproduce the issue included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29567, "title": "Fix random_normal with shape issue", "body": "This fix tries to address the issue raised in #29434 where error was encountered where shape is a list of Dimension:\r\n```\r\nTypeError: Failed to convert object of type <class 'tuple'> \\\r\n    to Tensor. Contents: (1, 1, Dimension(256), 16). \\\r\n    Consider casting elements to a supported type.\r\n```\r\n\r\nThis fix fixes the issue.\r\n\r\nThis fix fixes #29434.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Could you please check reviewer comments and keep us posted. Thanks!", "@alextp @gbaned The PR has been updated. Please take a look and let me know if there are other issues.", "@yongtang Can you please check build failures and resolve the merge conflicts? Thanks!", "@gbaned The PR has been rebased with merge conflict resolved. Please take a look.", "@alextp Thanks for the help in this PR. Since this PR has been covered by another commit c4f40ae so the issue has been fixed already.\r\n\r\nI will close this PR, but thanks for the help along the way."]}, {"number": 29566, "title": "The `Load CSV with tf.data` tutorial creates confusion about categorical data", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/csv#categorical_data\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe new `Load CSV with tf.data` tutorial is very nice. The tutorial shows users how to load a csv file into a tf.data Dataset. However, there are a couple of issues in the tutorial. First, the tutorial shows a very inconsistent and un-scalable way to encode categorical data using Regex expressions. A simpler way would be to use the already developed Feature Columns API, which is more consistent with the existing Tensorflow API. Second, the name of the tutorial is improper English. The tutorial is about loading tf.data with a CSV, not loading a CSV file with tf.data. So that should be fixed. \r\n\r\n### Clear description\r\n1. Overly complicated and unscalable explanation of how to encode categorical features.\r\n\r\nThe tutorial takes the user through loading a CSV file into a tf.data Dataset using the experimental `make_csv_dataset` function in TF-2.0.0-beta. That is all very well done. But the problem arises in the \"Data Preprocessing\" section. \r\n\r\nThe section on categorical data says that data must be converted from text to numerical encodings before passing the data to the model. However, the method described looks like this:\r\n\r\n```\r\nCATEGORIES = {\r\n    'sex': ['male', 'female'],\r\n    'class' : ['First', 'Second', 'Third'],\r\n    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\r\n    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\r\n    'alone' : ['y', 'n']\r\n}\r\n``` \r\nThen using this dictionary, the user is asked to:\r\n\r\n```\r\ndef process_categorical_data(data, categories):\r\n  \"\"\"Returns a one-hot encoded tensor representing categorical values.\"\"\"\r\n  \r\n  # Remove leading ' '.\r\n  data = tf.strings.regex_replace(data, '^ ', '')\r\n  # Remove trailing '.'.\r\n  data = tf.strings.regex_replace(data, r'\\.$', '')\r\n  \r\n  # ONE HOT ENCODE\r\n  # Reshape data from 1d (a list) to a 2d (a list of one-element lists)\r\n  data = tf.reshape(data, [-1, 1])\r\n  # For each element, create a new list of boolean values the length of categories,\r\n  # where the truth value is element == category label\r\n  data = tf.equal(categories, data)\r\n  # Cast booleans to floats.\r\n  data = tf.cast(data, tf.float32)\r\n  \r\n  # The entire encoding can fit on one line:\r\n  # data = tf.cast(tf.equal(categories, tf.reshape(data, [-1, 1])), tf.float32)\r\n  return data\r\n```\r\n\r\nNow this approach will work, but there are a couple of really big problems. First, Tensorflow has a Feature Column API already developed to handle this type of conversion. If the user simply did something like creating a feature column with vocabulary list, and then wrapping that column in an indicator column, this same code could be resolved in 2 lines instead of 14 lines. This change would also make the code easier to read, and provide more insight into how to use the Feature API.  \r\n```\r\ncol_sex = tf.feature_column.categorical_column_with_vocabulary_list(key=\"Sex\", vocabulary_list=[\"male\", \"female\"])\r\n\r\nfc_sex =  tf.feature_column.indicator_column(col_sex)\r\n```\r\n2. Correct the name of the tutorial\r\n\r\nThe name of the tutorial is \"Load CSV with tf.data.\" This name is actually improper English and a bit confusing. The current name makes it sound like you are loaded a tf.data dataset into a CSV, which is the opposite of the intent. The tutorial is about taking a CSV file and loading the data into a tf.data Dataset. So the proper name of the tutorial should be \"Load a CSV file into a tf.data Dataset,\" or \"Populate a tf.data Dataset with CSV file.\" This change might help reduce confusion about what the tutorial is trying to demonstrate. \r\n\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\nYes\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\nYes\r\n### Returns defined\r\n\r\nAre return values defined?\r\nYes\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\nErrors are not clearly defined. \r\n### Usage example\r\n\r\nIs there a usage example?\r\nThere is a usage example, but the usage example is very poorly designed. Hence I submitted the issue.\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\nVisuals are okay. \r\n### Submit a pull request?\r\nI am happy to submit a pull request. I guess let me see the response to this issue. If the development team agrees, then I can work on a pull request to update the documentation. \r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@MarkDaoust @yashk2810 \r\nWhat is the state of the feature column API? Is the change requested above advisable?", "@adammichaelwood Thanks for checking into this. Yeah, this issue bring up one of the most confusing and problematic aspects of all the tensorflow tutorials. Having tutorials is really helpful, but each tutorial shows a completely different way of doing the exact same thing. Like no two tutorials match, haha. It would be like if I was trying to show you how to drive to from my house to my office, and each time I took a completely different route. So anything you can do to add some consistency would be helpful. Just to be brutally honest, at this point I tell anyone trying to learn Tensorflow NOT to look at the tutorials because they just add confusion. In a similar situation, I think the introductory tutorials all showed loading MNIST data from `keras` even after the `Tensorflow Datasets` API was released. So half the tutorials shows loading from `tfds` but the intro ones showed `keras`. I mean any new user is going to get confused trying to figure out what is the difference between the two loading methods--and there are differences in how you load and configure the batches, etc. So again, for newer users consistency in the interface and approach matters. Thanks again. \r\n", "One thing that isn't clear, which maybe we should/could make more clear is that often the tutorials include, by necessity, things that aren't *the point* of the tutorial. So, for example, this tutorial is really just supposed to focus on loading CSV data in to TF datasets. It isn't really about pre-processing. This happens in other places, too, and we end up with the \"secondary\" content of a tutorial (for example - building the model, in a tutorial about loading data) that is somewhat inconsistent. \r\n\r\nA related problem is that there really *is* more than one legitimate way to do many things, and best practices haven't always been established. So we somewhat deliberately show multiple ways of doing things.\r\n\r\nAnd, of course, as best practices settle and new APIs come along, we generally prioritize new content and updating primary content over fixing something that is secondary in a tutorial. \r\n\r\nAnyway - we're trying to make all this a better experience for the community, and we very much appreciate this feedback. So thank you!\r\n\r\nAs for this tutorial in particular - I've gotten some guidance internally about how this should look. I'm working on updating it to use the feature column API as you suggest, and also making some other improvements to the data processing portion. \r\n\r\n\r\nThanks again.", "@adammichaelwood Thanks so much for your very thoughtful comments. Yes, I always intend to be constructive and not negative. This was just a pain point for me when I first started using Tensorflow, so I guess some of that still colors my views. \r\n\r\nYou are totally correct that that there is more than one way to do things. I guess I can give you my own experiences and hopefully that helps you to understand the user perspective better. The case of the feature columns and tensorflow datasets is a really good example. So when tensorflow datasets came out, it was really a big improvement. The demo video from the Tensorflow dev-con seemed impressive. But there was really hardly any documentation on how the Tensorflow datasets were setup or accessed or sliced. There was a guide, but it was really really incomplete. Like it took me forever to figure out how to add simple \"headings\" to a dataset. Like if you have a CSV file and want to preserve the \"headings\" there was no guide--or you had to read 100 different guides to find what you needed. The current method of loading a CSV is the `tf.io.decode_csv()` function which lets you import the data, but not the headings. Since there are no code examples in any of the Tensorflow api documentation, the API documentation was useless. Actually, the current and best way to import a CSV with headings is still technically \"experimental\" which is the `make_csv_dataset()` function in `tf.data`, but again there is no guide to tell you this juicy tidbit.\r\n\r\nNow you might ask why do headings matter in a CSV dataset? Well, because the Feature API requires headings to reference columns or features for transformation. So if I want to convert a dataset from some categorical value to a one-hot encoded format, I could either use a very cryptic function like the one in the Tutorial referenced in this issue, or I could just use a very simple call to the Tensorflow Features API. Again, the only reason I know about this is because I read Aurelien Geron's new book on Tensorflow 2.0. But none of this stuff is referenced anywhere in the Tensorflow documentation.\r\n\r\nSo I understand that showing different ways to do stuff can be useful in a tutorial. But that presupposes that there is some documentation that lays out the details of how say `tf.datasets` or something is implemented. Then I could say to myself, here is one implementation in tutorial 1, and another implementation in tutorial 2, and let me look at the docs to figure out how these differ. But in reality, there are NO docs on this. So in that case, the tutorials become the only docs and they are not consistent and so add to the confusion. \r\n\r\nI mean there are so many omissions like this in the Tensorflow docs that I really tell people to avoid them. The Tensorflow 2.0 docs are better, but even then I tell people to avoid them. It is like almost impossible to convey to you the levels of frustration that so many users I talk to feel. I remember even when Tensorflow Datasets first came out, I was really excited about how to use it. They had a tutorial to show how to download and load a dataset, but they did not include in their tutorial how to run a model on a `tfds` Tensorflow Dataset. If you look, a `Tensorflow Datasets` dataset is not a standard `tf.Dataset`, it is a variant. So you have to do some stuff before you can pass it directly to the `tf.keras` api. I actually just watched the video from Tensorflow Dev-Con to see how to run a model. For some reason, the people in the videos will show a complete workflow from loading data to running a model. But the documentation or tutorials ignore or omit some of those parts. \r\n\r\nI appreciate that you guys are trying to improve and make things better. And I wish you the best of luck. But speaking as honestly as I can, I am basically losing my faith in Google to correct something that seems like it should be pretty simple. I mean Tensorflow still doesn't have code examples in any of the API documentation after what, 15 versions of the software--from TF 0.1 to TF 1.14 to 2.0. So if Google can't do something so simple as this, then why would I believe they can do anything more complicated. I use Tensorflow today, but I am moving more and more of my workflow off of Tensorflow and over to Flux in Julia and other places where there is not such a huge gulf between developers and users. \r\n\r\nAgain, hopefully this information is helpful for you in understanding user experiences. Thanks again for all of your efforts to make things better. ", "Thank you for all of your feedback here. \r\nSometimes people are too nice to say this stuff, and sometimes people don't say it until they are too angry to say anything actually helpful. So I really appreciate your thoughtful, and kind, criticism.\r\n\r\nIf you have more thoughts and ideas about how we can improve TensorFlow docs, please share them. You might know all this already, but for anyone else reading along:\r\n\r\n - For very specific (single document corrections, specific missing information, factual errors), please file a new issue like this one.\r\n - For higher level conversation about how the docs can be improved as a whole, we have a [TensorFlow Docs Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs). The tech writers and other docs-focused people read all emails to that group, and we can have extended discussion. (There's also been talk of a Docs SIG, but last I heard we weren't quite there yet.)\r\n\r\nAs for this tutorial in particular, I'll have a PR soon and I hope you'll have the time to review it.\r\n\r\n\r\nThanks again.", "@adammichaelwood Certain, I am happy to help in any way I can. I will keep monitoring this issue and when the PR is ready I can add my 2 cents :). Thanks for the tip about the Tensorflow Docs group. I will look into that. Keep up the good work. Even I will keep looking through the tutorials to see if there are more places where we can improve stuff :). ", "updated: \r\nPR here: https://github.com/tensorflow/docs/pull/786", "Fixed via https://github.com/tensorflow/docs/pull/786\r\n\r\n@00krishna Please submit a new issue (or a PR!) if you have additional suggestions on this.\r\nThanks!", "I had a problem with this tutorial in particular. I am using TF 1.14 and though I have enable eager execution, I still could not run the last bit of code when training the model. Could anyone show me how to run this with TF 1.14? Thank you!\r\n\r\nWhen I run:\r\nmodel.fit(train_data, epochs=20)\r\n\r\nThe error message was:\r\nFailedPreconditionError: Table already initialized.\r\n\t [[{{node sequential_2/dense_features_9/sex_indicator/sex_lookup/hash_table/table_init/LookupTableImportV2}}]] [Op:__inference_keras_scratch_graph_6365]"]}, {"number": 29565, "title": "Update tf.keras.backend.cast_to_floatx example", "body": "Redoing [this PR](https://github.com/tensorflow/tensorflow/pull/29284) against master instead of 2.0.", "comments": []}, {"number": 29564, "title": "Adding docstring to tf.keras.activations.deserialize", "body": "Redoing [this PR](https://github.com/tensorflow/tensorflow/pull/29282) against master instead of 2.0.", "comments": ["@eito-fis Could you please address the reviewer comments. Thanks!", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29563, "title": "Add an example to keras backend categorical crossentropy documentation.", "body": "Remade [this PR](https://github.com/tensorflow/tensorflow/pull/29300) against master instead of 2.0", "comments": []}, {"number": 29562, "title": "Custom Layer in tensorflow v2.0.0beta throws object has no attribute '_expects_mask_arg' error ", "body": "I am trying to reconsturct an image based on three inputs of previous layers normal (None,128,128,3),albedo(None,128,128,3) and lighting(27) . But here the code still says object has no attribute '_expects_mask_arg' error .I have presented my code here in which I have implemented a custom layer using Tensorflow v2 beta using the high level API.\r\nThe code begins here .\r\n\r\n\r\nclass Reconstruction_Layer(tf.keras.layers.Layer):\r\n\r\n\r\n        def __init__(self,input_shape):\r\n       # super(Reconstruction_Layer, self).__init__()\r\n        #self.num_outputs = num_outputs\r\n        #self.pixel=np.zeros((9),dtype=int)\r\n        \r\n        self.sphar=np.zeros((9),dtype=float)\r\n        self.y=np.zeros((9),dtype=float)\r\n        self.reconstructed_img=np.zeros((128,128,3),dtype=float)\r\n        #self.y=tf.zeros([128,128,9])\r\n        self.normal_light=np.zeros((128,128,9),dtype=float)\r\n        self.y_temp=np.zeros((9),dtype=float)\r\n        w_init = tf.random_normal_initializer()\r\n        self.r_img = tf.Variable(initial_value=w_init(shape=input_shape),dtype='float32',trainable=True)\r\n    \r\n       \r\n\r\n    def build(self, input_shape):\r\n        print(input_shape)\r\n        super(Reconstruction_Layer, self).build(input_shape)\r\n            \r\n    \r\n    \r\n    def call(self,input_layer,*args,**kwargs):\r\n        self.normal,self.albedo = input_layer\r\n        super().__call__(self,*args,**kwargs)\r\n            \r\n        \r\n        \"\"\"\r\n        self.normal=np.array(self.normal)\r\n        self.albedo=np.array(self.albedo)\r\n        self.light=np.array(self.light)\r\n        \"\"\"\r\n        for i in range(128):\r\n            for j in range(128):\r\n                #self.y=spherical_harmonic_calc(self.normal(i,j))\r\n                \r\n                self.pixel=self.normal[i,j,:]\r\n                \r\n                np.reshape(self.pixel,(3))\r\n                \r\n                #self.normal_light(i,j)= self.y\r\n                self.sphar[0]=(1/((4*math.pi)**0.5))\r\n                self.sphar[1]=((3/(4*math.pi))**0.5)*self.pixel[2]\r\n                self.sphar[3]=(((3/(4*math.pi))**0.5)*self.pixel[1])\r\n                self.sphar[4]=((1/2)*((5/(4*math.pi))**0.5)*(3*(self.pixel[2]**2) - 1))\r\n                self.sphar[5]=(3*((5/(12*math.pi))**0.5)*self.pixel[2]*self.pixel[0])\r\n                self.sphar[6]=(3*((5/(12*math.pi))**0.5)*self.pixel[2]*self.pixel[1])\r\n                self.sphar[7]=((3/2)*((5/(12*math.pi))**0.5)*((self.pixel[0]**2)-(self.pixel[1]**2)))\r\n                self.sphar[8]=(3*((5/(12*math.pi))**0.5)*self.pixel[0]*self.pixel[1])\r\n                \r\n                self.normal_light[i,j,:]=self.sphar\r\n        \r\n        for j in range(128):\r\n            for k in range(128):\r\n                for i in range(3):\r\n                    #self.y_temp=self.normal_light(j,k)\r\n                    \"\"\"\r\n                    print('self.normal_light[j,k]',self.normal_light[j,k])\r\n                    print('self.normal_light[j,k].shape',self.normal_light[j,k].shape)\r\n                    print('self.light[i*9:(i+1)*9 - 1]',self.light[i*9:(i+1)*9 - 1])\r\n                    print('self.light[i*9:(i+1)*9 - 1]',self.light[i*9:(i+1)*9 - 1].shape)\r\n                    \"\"\"\r\n                    self.reconstructed_img[j,k,i]=self.albedo[j,k,i]* tf.tensordot(self.normal_light[j,k],self.light[i*9:(i+1)*9 ],axes=1)\r\n        \r\n        self.reconstructed_img=tf.convert_to_tensor(self.reconstructed_img)\r\n        self.r_img=self.reconstructed_img\r\n        \r\n        def compute_output_shape(self):\r\n            return shape(self.r_img)\r\n     \r\n        return self.r_img\r\n\r\n![Screenshot from 2019-06-08 23-50-31](https://user-images.githubusercontent.com/20143249/59150910-4a47ab00-8a48-11e9-9dbf-5f9fa24153dc.png)\r\n", "comments": ["I have tried to execute the code snippet on Colab with TF 2.0 Beta but was not able to get the mentioned error. Please help us to reproduce the issue or if possible provide us the full snippet that can replicate the issue. Also provide information as platform (operating system and architecture you are using). Thanks! ", "here is the code which reproduces the error:----\r\n\r\n`\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers import BatchNormalization,Dense,Input\r\nfrom tensorflow.keras.models import Model\r\n#from keras.layers.advanced_activations import LeakyReLU\r\nfrom tensorflow.keras.layers import  Conv2DTranspose\r\nfrom tensorflow.keras.layers import LeakyReLU ,ReLU\r\nfrom tensorflow.keras.datasets import mnist,cifar10\r\nimport numpy as np\r\ninput_tensor=Input(shape=(128,128,3))\r\nx=layers.Conv2D(64,(7,7),strides=1,activation='relu',padding='same')(input_tensor)\r\nx=BatchNormalization()(x)\r\nx=layers.Conv2D(128,(3,3),strides=1,activation='relu',padding='same')(x)\r\nx=BatchNormalization()(x)\r\nx=layers.Conv2D(128,(3,3),strides=2,activation='relu',padding='same')(x)\r\nnormal_1=BatchNormalization()(x)\r\nnormal_1=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(normal_1)\r\nnormal_1=BatchNormalization()(normal_1)\r\nnormal_1=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(normal_1)\r\noutput_res_1 = layers.add([normal_1,x])\r\n#Normal Residual Block\r\n#Residual Block 1\r\nnormal_1=BatchNormalization()(x)\r\nnormal_1=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(normal_1)\r\nnormal_1=BatchNormalization()(normal_1)\r\nnormal_1=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(normal_1)\r\noutput_res_1 = layers.add([normal_1,x])\r\noutput_normal=output_res_1\r\n\r\noutput_normal=BatchNormalization()(output_normal)\r\noutput_normal=tf.nn.relu(output_normal)\r\nalbedo_1=BatchNormalization()(x)\r\nalbedo_1=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(albedo_1)\r\nalbedo_1=BatchNormalization()(albedo_1)\r\nalbedo_1=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(albedo_1)\r\noutput_res_11 = layers.add([albedo_1,x])\r\noutput_albedo=output_res_11\r\n\r\n\r\noutput_albedo=BatchNormalization()(output_albedo)\r\noutput_albedo=tf.nn.relu(output_albedo)\r\n\r\noutput_albedo_conv=tf.image.resize(output_albedo,size=[128,128])\r\noutput_normal_conv=tf.image.resize(output_normal,size=[128,128])\r\noutput_normal_conv=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(output_normal_conv)\r\noutput_normal_conv=BatchNormalization()(output_normal_conv)\r\noutput_normal_conv=tf.nn.relu(output_normal_conv)\r\n\r\noutput_normal_conv=layers.Conv2D(64,(3,3),strides=1,activation='relu',padding='same')(output_normal_conv)\r\noutput_normal_conv=BatchNormalization()(output_normal_conv)\r\noutput_normal_conv=tf.nn.relu(output_normal_conv)\r\n\r\noutput_normal_conv=layers.Conv2D(3,(1,1),strides=1,padding='same')(output_normal_conv)\r\noutput_albedo_conv=layers.Conv2D(128,(1,1),strides=1,activation='relu',padding='same')(output_albedo)\r\noutput_albedo_conv=BatchNormalization()(output_albedo_conv)\r\noutput_albedo_conv=tf.nn.relu(output_albedo_conv)\r\n\r\noutput_albedo_conv=layers.Conv2D(64,(3,3),strides=1,activation='relu',padding='same')(output_albedo_conv)\r\noutput_albedo_conv=BatchNormalization()(output_albedo_conv)\r\noutput_albedo_conv=tf.nn.relu(output_albedo_conv)\r\n\r\noutput_albedo_conv=layers.Conv2D(3,(1,1),strides=1,padding='same')(output_albedo_conv)\r\n#light Estimator\r\nlighting_vector=layers.Concatenate()([x,output_albedo,output_normal])\r\nlighting_vector=layers.Conv2D(128,(1,1),padding='same')(lighting_vector)\r\nlighting_vector=tf.nn.relu(lighting_vector)\r\nlighting_vector=BatchNormalization()(lighting_vector)\r\nlighting_vector=layers.AveragePooling2D(pool_size=(2, 2), padding='same')(lighting_vector)\r\nlighting_vector=layers.Flatten()(lighting_vector)\r\nlighting_vector=layers.Dense(27,activation='relu')(lighting_vector)\r\nimport math\r\nclass Reconstruction_Layer(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self,input_shape):\r\n       # super(Reconstruction_Layer, self).__init__()\r\n        #self.num_outputs = num_outputs\r\n        #self.pixel=np.zeros((9),dtype=int)\r\n        \r\n        self.sphar=np.zeros((9),dtype=float)\r\n        self.y=np.zeros((9),dtype=float)\r\n        self.reconstructed_img=np.zeros((128,128,3),dtype=float)\r\n        #self.y=tf.zeros([128,128,9])\r\n        self.normal_light=np.zeros((128,128,9),dtype=float)\r\n        self.y_temp=np.zeros((9),dtype=float)\r\n        w_init = tf.random_normal_initializer()\r\n        self.r_img = tf.Variable(initial_value=w_init(shape=input_shape),dtype='float32',trainable=True)\r\n    \r\n       \r\n\r\n    def build(self, input_shape):\r\n        print(input_shape)\r\n        super(Reconstruction_Layer, self).build(input_shape)\r\n            \r\n    \r\n    \r\n    def call(self,input_layer,*args,**kwargs):\r\n        self.normal,self.albedo = input_layer\r\n        super().__call__(self,*args,**kwargs)\r\n            \r\n        \r\n        \"\"\"\r\n        self.normal=np.array(self.normal)\r\n        self.albedo=np.array(self.albedo)\r\n        self.light=np.array(self.light)\r\n        \"\"\"\r\n        for i in range(128):\r\n            for j in range(128):\r\n                #self.y=spherical_harmonic_calc(self.normal(i,j))\r\n                \r\n                self.pixel=self.normal[i,j,:]\r\n                \r\n                np.reshape(self.pixel,(3))\r\n                \r\n                #self.normal_light(i,j)= self.y\r\n                self.sphar[0]=(1/((4*math.pi)**0.5))\r\n                self.sphar[1]=((3/(4*math.pi))**0.5)*self.pixel[2]\r\n                self.sphar[3]=(((3/(4*math.pi))**0.5)*self.pixel[1])\r\n                self.sphar[4]=((1/2)*((5/(4*math.pi))**0.5)*(3*(self.pixel[2]**2) - 1))\r\n                self.sphar[5]=(3*((5/(12*math.pi))**0.5)*self.pixel[2]*self.pixel[0])\r\n                self.sphar[6]=(3*((5/(12*math.pi))**0.5)*self.pixel[2]*self.pixel[1])\r\n                self.sphar[7]=((3/2)*((5/(12*math.pi))**0.5)*((self.pixel[0]**2)-(self.pixel[1]**2)))\r\n                self.sphar[8]=(3*((5/(12*math.pi))**0.5)*self.pixel[0]*self.pixel[1])\r\n                \r\n                self.normal_light[i,j,:]=self.sphar\r\n        \r\n        for j in range(128):\r\n            for k in range(128):\r\n                for i in range(3):\r\n                    #self.y_temp=self.normal_light(j,k)\r\n                    \"\"\"\r\n                    print('self.normal_light[j,k]',self.normal_light[j,k])\r\n                    print('self.normal_light[j,k].shape',self.normal_light[j,k].shape)\r\n                    print('self.light[i*9:(i+1)*9 - 1]',self.light[i*9:(i+1)*9 - 1])\r\n                    print('self.light[i*9:(i+1)*9 - 1]',self.light[i*9:(i+1)*9 - 1].shape)\r\n                    \"\"\"\r\n                    self.reconstructed_img[j,k,i]=self.albedo[j,k,i]* tf.tensordot(self.normal_light[j,k],self.light[i*9:(i+1)*9 ],axes=1)\r\n        \r\n        self.reconstructed_img=tf.convert_to_tensor(self.reconstructed_img)\r\n        self.r_img=self.reconstructed_img\r\n        \r\n        def compute_output_shape(self):\r\n            return shape(self.r_img)\r\n     \r\n        return self.r_img\r\nd=[output_normal_conv,output_albedo_conv,lighting_vector]\r\ny=Reconstruction_Layer((128,128,3))(d)\r\n`", "I am using Ubuntu 18.04 in 64 bit architecture", "Tried to execute the full code snippet provided on Colab with TF 2.0.0-beta but still unable to get the error. Could you please help and let us know which line or part of code is giving the error. Thanks!", "Can you please provide the link of the notebook where you tried to run it??\r\nI tried running the above code snippet in google colab and got the follwing error \r\nAttributeError: 'Reconstruction_Layer' object has no attribute '_trainable'\r\n\r\nThe following is the colab link . Anyone with the lin can also edit .\r\nThanks!\r\nhttps://colab.research.google.com/drive/1ZSwmN1BCh6nm68bxC9TOcLrzshIesTWg", "Have tried on Colab with TF 2.0beta and was now able to get the mentioned error. Thanks!", "Apologies for the delay in response. Its difficult to debug extensive models like these, We will greatly appreciate if you can construct a minimal concise code snippet which can reproduce the bug reported rather than whole model. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29562\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29562\">No</a>\n", "You must call the super class `__init__` function. This is the source of this particular error, given that _expects_mask_arg is defined in `tf.keras.layers.Layer::__init__` as can be seen [here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/base_layer.py#L216)"]}, {"number": 29561, "title": "Fixes include path broken by virtual pip packaging", "body": "Ping @mihaimaruseac for review. Ping @lark as this is a work-related contribution.\r\n\r\nThis fixes https://github.com/horovod/horovod/issues/1129 in our in-house nightly build.", "comments": ["Oh this should not be merged...ping @perfinion to help sort out this patch in a proper way."]}, {"number": 29560, "title": "[INTEL MKL] Fix proxy info during docker builds", "body": "This was causing `swift` package downloads to fail because for some reason `HTTP_PROXY` and `HTTPS_PROXY` values were empty.", "comments": ["Hi @penpornk any feedback on this?", "Thanks @penpornk \r\nComments added to the `Dockerfile`."]}, {"number": 29559, "title": "TF2 Nightly-20190608 unable to link to core framework", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04+\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf_nightly_2.0_preview==2.0.0.dev20190608\r\n- Python version: Python2 / 3\r\n- Bazel version (if compiling from source): 0.24.1\r\n\r\n**Describe the current behavior**\r\nTF Addons is failing to link to the tensorflow core framework for our python 2 build:\r\n```\r\ntensorflow_addons/custom_ops/text/cc/kernels/skip_gram_kernels.cc:20:49: fatal error: tensorflow/core/framework/op_kernel.h: No such file or directory\r\n #include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\ntensorflow_addons/custom_ops/text/cc/ops/skip_gram_ops.cc:16:42: fatal error: tensorflow/core/framework/op.h: No such file or directory\r\n #include \"tensorflow/core/framework/op.h\"\r\n\r\n./tensorflow_addons/custom_ops/image/cc/kernels/euclidean_distance_transform_op.h:23:52: fatal error: tensorflow/core/framework/tensor_types.h: No such file or directory\r\n #include \"tensorflow/core/framework/tensor_types.h\"\r\n\r\n....\r\n```\r\n\r\nStrangely this is only affecting our python2 builds. See the build logs in https://github.com/tensorflow/addons/pull/273\r\n\r\n**Describe the expected behavior**\r\nAble to correctly link to the framework.\r\n\r\n**Other info / logs**\r\nAssuming this is related to the virtual pip package. Should these be looking in `tensorflow_core/...`?\r\n\r\ncc @mihaimaruseac @perfinion \r\n", "comments": ["Sorry for the breakage. I think #29561 should fix this, but I'll take another look at this tomorrow", "According to the nightly RFC, it is possible to have same day nightly packages for different platforms at different commits. Ping @gunan to double check.", "Correct. If the tree is healthy, nightly is automatically pushed. If the tree is unhealthy, we may skip pushing nightlies for a particular configuration for days.", "Sounds good. I don't think it disrupted any work on this but FWIW the reason Python3 didn't fail was because there was no [py34 nightly published for 20190608](https://pypi.org/project/tf-nightly-2.0-preview/2.0.0.dev20190608/#files). ", "I think that these should be fixed now", "@seanpmorgan : let me know if we can close this issue since it looks to be fixed. Thanks!", "Can confirm this is good to close. Thanks!"]}, {"number": 29558, "title": "model.fit() does not reshuffle the dataset between epochs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION='2.0.0-dev20190606'\r\nGIT_VERSION='v1.12.1-3447-g5a0f1bbfb7'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen calling `model.fit(dataset, epochs=2)` with a finite shuffled dataset, the model is trained on the same dataset order at each epoch.\r\n\r\n**Describe the expected behavior**\r\nI expect the dataset to be reshuffled after each epoch. Right now, it's not, even when I use `reshuffle_each_iteration=True` in the dataset's `shuffle()` method. This argument seems to only shuffle between iterations within one epoch.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nX = np.arange(6).astype(np.float32).reshape(-1, 1)\r\ny = X**2\r\ndataset = tf.data.Dataset.from_tensor_slices((X,y))\r\ndataset = dataset.shuffle(100, reshuffle_each_iteration=True)\r\ndataset = dataset.repeat(2)\r\ndataset = dataset.batch(2)\r\n\r\n@tf.function\r\ndef log_inputs(inputs):\r\n    tf.print(inputs)\r\n    return inputs\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Lambda(log_inputs),\r\n    keras.layers.Dense(1)\r\n])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nmodel.fit(dataset, epochs=2, verbose=0)\r\n```\r\n\r\n**Other info / logs**\r\nThe output is as follows (I've added comments):\r\n\r\n```python\r\n[[5]   # first epoch, first iteration, first batch\r\n [2]]\r\n[[3]      # second batch\r\n [1]]\r\n[[0]      # third batch\r\n [4]]\r\n[[0]   # first epoch, second iteration, first batch\r\n [3]]\r\n[[1]      # second batch\r\n [5]]\r\n[[4]      # third batch\r\n [2]]\r\n[[5]   # second epoch, first iteration, first batch\r\n [2]]\r\n[[3]\r\n [1]]\r\n[[0]\r\n [4]]\r\n[[0]   # second epoch, second iteration, first batch\r\n [3]]\r\n[[1]\r\n [5]]\r\n[[4]\r\n [2]]\r\n```\r\n\r\nAs you can see the order of the data is perfectly identical during the 1st and 2nd epochs. It is only reshuffled at each iteration within the same epoch.\r\n\r\nSo the only way to ensure that the data will be reshuffled at each epoch is to use `dataset.repeat(n_epochs)`, then `model.fit(dataset, steps_per_epoch=..., epochs=n_epochs)`. It feels like unnecessary complexity.", "comments": ["@jsimsa -- can you comment on the best way to get dataset re-shuffling here? It seems like the reshuffling arg should apply when we move to the next epoch?", "This is a known issue that we are working on fixing by RC0. The gist is that `reshuffle_each_iteration` was introduced when iterations were achieved by `repeat` (and that's what the flag controls).\r\n\r\nI am not sure what Keras does internally to perform epochs, but in 2.0 it should create a new iterator for each epoch and reshuffling the order between different iterators will be controlled by a `tf.data.Option` that will be available by RC0.", "Is this issue still open? If so I would like to help, otherwise it should be closed", "@joshz123 I believe that this issue is fixed in TF 2.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29558\">No</a>\n"]}, {"number": 29557, "title": "Reopen #26098", "body": "Please see https://github.com/tensorflow/tensorflow/issues/26098#issuecomment-489175950. That issue needs to be reopened since it is **not** fixed. It's been over a month since I pointed that out so I'm creating this new issue to bring the necessary attention to it. When https://github.com/tensorflow/tensorflow/issues/26098 is reopened you may close this. Thanks.", "comments": []}, {"number": 29556, "title": "Tensorflow Keras: all Keras optimizers throw an error when training in eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a model with SGD optimizer in eager mode and setting its `run_eagerly` attribute to true when I fit the model using some tf.Dataset (created using [tf.data.experimental.CsvDataset](https://www.tensorflow.org/api_docs/python/tf/contrib/data/CsvDataset) if that matters) as an input, I get the following error:\r\n\r\n```\r\nAttributeError: 'SGD' object has no attribute 'apply_gradients'\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI'd like to set Keras model's run_eagerly property to true so that I'd be able to step into custom-defined loss functions when being in eager mode when using SGD as an optimiser.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nsgd = tf.keras.optimizers.SGD()\r\n\r\ninputs = tf.keras.Input(shape=(3,))\r\n\r\n# First layer\r\nx = tf.keras.layers.Dense(5, kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\r\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\r\n\r\n# Second layer\r\nx = tf.keras.layers.Dense(5, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\r\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\r\n\r\n# Output\r\noutputs = tf.keras.layers.Dense(\r\n\t1,\r\n\tkernel_regularizer=tf.keras.regularizers.l2(0.01),\r\n\tactivation='sigmoid'\r\n)(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.compile(optimizer=sgd, loss='mean_squared_error')\r\n\r\nmodel.run_eagerly = True\r\n\r\n# Define some dummy dataset\r\nx = [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]\r\ny = [0, 1, 0]\r\n\r\nmodel.fit(x, y)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nFull traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm\\helpers\\pydev\\pydevd.py\", line 1758, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm\\helpers\\pydev\\pydevd.py\", line 1752, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm\\helpers\\pydev\\pydevd.py\", line 1147, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/QMUL/user-state-recognition/src/train_MLP.py\", line 331, in <module>\r\n    main()\r\n  File \"C:/QMUL/user-state-recognition/src/train_MLP.py\", line 171, in main\r\n    validation_steps=steps_per_epoch_val, callbacks=[tb_callback], verbose=1)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 851, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 191, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1179, in train_on_batch\r\n    self, x, y, sample_weights=sample_weights)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_eager.py\", line 260, in train_on_batch\r\n    model, inputs, targets, sample_weights=sample_weights, training=True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_eager.py\", line 225, in _process_single_batch\r\n    model.optimizer.apply_gradients(zip(grads,\r\nAttributeError: 'SGD' object has no attribute 'apply_gradients'\r\n```\r\n\r\nRegarding `run_eagerly` attribute: I need to step into my custom loss and metric functions. The code takes the decision in [Model.train_on_batch() function](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/python/keras/engine/training.py#L1123) in [this](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/python/keras/engine/training.py#L1177) line. If run_eagerly parameter will not be set explicitly to `True` before calling model.fit function, the modell will take [this](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/python/keras/engine/training.py#L1180) path instead, which will make it impossible to step into custom loss function nor custom metric function. When `run_eagerly` is set to true the code will eventually get to [this](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/python/keras/engine/training_eager.py#L225) line and that's where the code fails, as Keras's optimizer interface doesn't have `apply_gradients` method defined.", "comments": ["As it turns out the error appears for any Keras optimiser taken from here: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers I'm going to change the title of the issue.", "I'm also pretty sure the docs are wrong. For now, I'm just using an optimizer from `tf.train`.", "@mjarosie : Thank you for reaching out  to us. Will it be possible to provide the full minimal code snippet to reproduce the error caught by you. It will help us to proceed faster. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi @achandraa I've modified the original code, it reproduces the exact issue now.", "Hi, too face the same issue when running under eager execution. please provide a solution for this @achandraa "]}, {"number": 29555, "title": "[TF 2.0 alpha] Erratic metric behaviour with MAE and MSE", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below):2.0.0a0\r\n- Python version:3.6.6\r\n- Bazel version (if compiling from source):-\r\n- GCC/Compiler version (if compiling from source):-\r\n- CUDA/cuDNN version: Using CPU\r\n- GPU model and memory: Using CPU\r\n\r\n**Describe the current behavior**\r\nThe MAE and the MSE should be correlated but the MSE value used in the metric is not correlated [see this minimal log for instance](https://gist.github.com/roya0045/5e4c2950da48092af266c945dc69e0a1)\r\n\r\n**Describe the expected behavior**\r\nI assume that the MSE should be equivalent to the MAE squared but I may be wrong.\r\n\r\n**Code to reproduce the issue**\r\nBuild a small LSTM with keras for sequence prediction.\r\n\r\nLoss: 'mae'\r\nmetrics: ['mse','acc']\r\noptimizer: kr.optimizers.Adam\r\n\r\n**Other info / logs**\r\nSee link in the described behaviour.", "comments": ["Hi, looking at the source code you can see that they are correlated in some way. However, due to their differences, you can find data, such that they do not behave the same way.\r\nMaybe you can have a look at [this post](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d). They also show occasions, where both metrics show non correlating events.", "@lufol If you look more closely at the end of the 2nd epoch and the start of the 3rd epoch ( line 650 up to around line 828) you can see that the behaviour changes between epochs, which is highly unusual.\r\n\r\nI forgot to add that my data are rescaled between 0 and 1. I also something get random spikes in MSA error (in the thousands) in half an epoch and the next pass everything goes back to normal.", "@lufol But you may be right and it might just be me overreacting. I'll close this in the meantime, thanks!", "Yes @roya0045 , I get definitely your point. If you would be able to provide a small demo of the behaivour it would be much easier to serach for the cause.", "@lufol What would that small demo entail?", "I guess by opening an issue here, you suspected that there is a bug somewhere in the code. By providing an easy to follow example that reproduces this bug, it would be much easier to find the cause for that.\r\n\r\nBest regards", "@lufol My setup is quite simple, I have time series data in input, I have 4 lstm layers and the last one does not output a sequence but all others do. The output are the next 5 dates for x elements. Those 5 days are squashed. So if I want the output of 3 elements, I will have  15 output.\r\n\r\nThe compilation is as mentioned in the main post.\r\n\r\nThe layers are aroud 270 units wide.\r\n\r\nThe layers are not stateful (could this cause the behaviour change between epochs?)"]}, {"number": 29554, "title": "def call() should be def __call__()", "body": "[https://www.tensorflow.org/beta/tutorials/quickstart/advanced](url)\r\n[https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/advanced.ipynb](url)\r\n\r\nCorrect your mistakes, please.\r\n\r\n```\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.conv1 = Conv2D(32, 3, activation='relu')\r\n    self.flatten = Flatten()\r\n    self.d1 = Dense(128, activation='relu')\r\n    self.d2 = Dense(10, activation='softmax')\r\n\r\n  def call(self, x):  #!!! should be  __call__  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    x = self.conv1(x)\r\n    x = self.flatten(x)\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n```", "comments": []}, {"number": 29553, "title": "ModuleNotFoundError: No module named 'tensorflow.python.ops.cond_v2'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): tensorflow-gpu 2.0.0-beta and any tf 2.0 nightly build\r\n- Python version: Python 3.6.7\r\n- GPU model and memory: Tesla K80 (12GB RAM)\r\n\r\n**Describe the current behavior**\r\nInitilalizing a Keras model raises following error: ModuleNotFoundError: No module named 'tensorflow.python.ops.cond_v2' when using Google Colab\r\n\r\nCode was working fine until yesterday's build. The error doesn't go away in other tf 2.0 versions.\r\n\r\n**Code to reproduce the issue**\r\nbase_model = tf.keras.applications.MobileNetV2(input_shape=(128,128,3),\r\n                                                   include_top=False,\r\n                                                   weights='imagenet')\r\nor\r\n\r\nbase_model = tf.keras.applications.MobileNetV2(input_shape=(128,128,3))\r\n", "comments": ["I see you closed the issue. Does that mean you found a fix?", "It turned out to be a Colab issue for me. I just restarted the environment and installed the dev20190607 nightly build then it started working fine!", "Perfect. Thank you.\r\n\r\nFor full disclosure: I made a change which broke a few things in the nightlies since 6th of June until today so I was gathering to see if there were unaccounted for failures"]}, {"number": 29552, "title": "Python: Use `PyLong_AsVoidPtr`, `PyLong_FromVoidPtr` for conversion", "body": "Use `PyLong_AsVoidPtr`, `PyLong_FromVoidPtr` to convert the pointer.  Use of `PyInt_AsLong` returns a `long` which will truncate the pointer on a LLP64 target (i.e. Windows).  Using `PyLong_AsVoidPtr` will return a `void *` for the Python integer or long integer.  Define the corresponding \"out\" map which specifies the conversion to wrap the return value of a function returning a `TfLiteDelegate *`.  This ensures that the conversion is always valid as the value is constructed fro the `PyLong_FromVoidPtr`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29552) for more info**.\n\n<!-- need_sender_cla -->", "@compnerd  Please sign CLA in order to proceed with next steps. Thank you!", "I singed it!\n\nOn Saturday, June 8, 2019, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29552>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/29552?email_source=notifications&email_token=AGFYVZN2ZBNMBIBUGQT4LBLPZNKIDA5CNFSM4HWEZDW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXHPEVA#issuecomment-500101716>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGFYVZIPOD2QHD3W3IDUS2LPZNKIDANCNFSM4HWEZDWQ>\n> .\n>\n", "@gbaned - seems that its the author email address that is the issue", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29552) for more info**.\n\n<!-- ok -->", "@saeta - mind kicking off the testing for this?  I think that this should help repair the Windows build\r\n\r\n> `bazel-out/x64_windows-opt/bin/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.cc(3981): error C2338: TFLiteDelegate must be representable as a long.`\r\n", "I signed it!", "@jdduke - I think that this would be better - it makes it explicit that we want the conversion to the pointer and provides a bijective mapping for the `TfLiteDelegate *` to the `PyLong`.  Thoughts?", "Okay, seems that @jdduke applied this in CL 252753868.  The bijection is a nice thing for the mental model, but, shouldn't be absolutely required.  Going to close this off.", "Ah, didn't see this! Feel free to keep the bijection counterpart in your PR and I can approve?", "Sounds good, I'll upload a new version of the change to this PR."]}, {"number": 29551, "title": "Fix minor typos", "body": "partion -> partition\r\npartioned -> partitioned", "comments": []}, {"number": 29550, "title": "TF 2.0 Beta TPUStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRunning the following code to setup TPUStrategy results in NotFoundError: No registered 'ConfigureDistributedTPU' OpKernel for TPU_SYSTEM devices compatible with node {{node ConfigureDistributedTPU}}\r\n\r\n`import tensorflow as tf\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_host(cluster_resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)`\r\n\r\n**Describe the expected behavior**\r\nTPUStrategy setup completes without error.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://colab.research.google.com/drive/1trrM2E-x912TiP0CeHtAi8jxZaA5y_SL\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nWith reference to the [Distributed Training docs](https://www.tensorflow.org/beta/guide/distribute_strategy), I understand that TPUStrategy does not support all APIs in Tensorflow 2.0 Beta:\r\n![image](https://user-images.githubusercontent.com/29782952/59141572-00c47500-89e2-11e9-82a4-9e045187db00.png)\r\nHowever, given that some APIs are supported, it would follow that the TPUStrategy initialization in Tensorflow 2.0 Beta is expected to be functional. Hence, this bug report. Thank you!\r\n\r\n", "comments": ["Hello, this error should be a basic issue that your Cloud TPU version is too old that does not contain the particular kernel you need. Could you try to contact Cloud TPU account team or raise an issue in https://github.com/tensorflow/tpu? I think you will need a tf 1.14 TPU at least (or tf-nightly), as it is close to tf 2.0 beta.", "Hi Chia, \r\nas noted in the doc section referenced by you, the TPU support is planned for a 2.0RC release. ", "Yep, it'll work in the RC, or if you use the nightlies."]}, {"number": 29549, "title": "Tensorflow 2.0 alpha/beta performance", "body": "**System information**\r\nGoogle Colab system (GPU mode) with TensorFlow 2.0 and Python 3.\r\n\r\n**Describe the current behavior**\r\nUsing TensorFlow 2.0 beta (pip install) on a CNN-RNN (two bidirecional lstm), is taking about 25 minutes to complete a single epoch (training).\r\n\r\n**Describe the expected behavior**\r\nThe curious thing is that when using the same code, same file/platform (google colab) but with the Alpha version of the TF 2.0 (pip install), the same epoch takes around only 70 seconds.\r\n\r\n**Other info / logs**\r\nEverything is the same in both scenarios and also the identification of the GPU:\r\n> Found GPU at: / device: GPU: 0\r\n\r\nAnother thing I noticed is that the alpha warning disappeared:\r\n\r\n> W0608 02:52:12.500632 140219739858816 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f86d4231da0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\r\n\r\nMaybe it has something related?", "comments": ["@arthurflor23  Refer to #29506 ", "Sorry for the performance regression, we are aware of this issue, and trying to address it right now. Please monitor on https://github.com/tensorflow/tensorflow/issues/29506 which is the same issue."]}, {"number": 29548, "title": "Copy cl/250062993", "body": "", "comments": []}, {"number": 29547, "title": "Update the version to 2.0.0 in tensorflow.bzl", "body": "Fix https://github.com/tensorflow/tensorflow/issues/29540", "comments": ["Thanks for the quick fix Yifei. Will merge it when we are ready to start taking additional cherrypicks to this branch.", "@yifeif @bananabowl Should we have a cherry-pick to r1.14 as well? ", "I don't think it's needed. The major version number is used as the number appended at the end of `libtensorflow_framework.so.1` (`1` on 1.13, 1.14, etc.)"]}, {"number": 29546, "title": "Bump up shard count for keras:normalization_test from 3 to 4.", "body": "Same change made on head in cl/252111328", "comments": []}, {"number": 29545, "title": "Subclassing tf.keras.models.Model save_model saves h5py but not h5/hdf5 file types", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): 2.0.0b\r\n- Python version: 3.6.7\r\n- GPU model and memory: Tesla T4\r\n\r\n**Describe the current behavior**\r\n-Using tf 2.0.0b-gpu on google colab.\r\n\r\nWhile using the subclassing API for a subclassed layer and model, I was unable to use the model.save_model() function for h5 or hdf5 file types, but I could successfully save and load the model if it was saved as h5py file type. In the toy example being used it worked correctly, although this may not be the case. Note that the get_config method was implemented in the custom layer and custom model.\r\n\r\n**Describe the expected behavior**\r\nEither the save_model should always work (I believe this is a feature goal) and the documentation should reflect this, or if the save is likely to produce incorrect results it should raise an error and the documentation should continue to suggest that custom models can only be saved with the save_weights feature. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass resblock(keras.layers.Layer):\r\n    def __init__(self, n_layers, n_neurons, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.hidden = [keras.layers.Dense(n_neurons,\r\n                                          activation='elu',\r\n                                          kernel_initializer='he_normal')\r\n                       for _ in range(n_layers)]\r\n        \r\n    def call(self, inputs):\r\n        z = inputs\r\n        for layer in self.hidden:\r\n            z = layer(z)\r\n        return inputs + z\r\n    \r\n    def get_config(self):\r\n        base_config = super().get_config()\r\n        return {**base_config}\r\n\r\nclass res_mod(keras.models.Model):\r\n    def __init__(self, output_dim, activation=None, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.f1 = keras.layers.Flatten()\r\n        self.hidden1 = keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal')\r\n        self.b1 = resblock(2, 100)\r\n        self.b2 = resblock(2, 100)\r\n        self.output1 = keras.layers.Dense(output_dim, activation=keras.activations.get(activation))\r\n        \r\n    def call(self, inputs):\r\n        z = self.f1(inputs)\r\n        z = self.hidden1(z)\r\n        for _ in range(4):\r\n            z = self.b1(z)\r\n        z = self.b2(z)\r\n        return self.output1(z)\r\n    \r\n    def get_config(self):\r\n        base_config = super().get_config()\r\n        return{**base_config, \"output_dim\" : output_dim, \"activation\": activation}\r\n    \r\n\r\nmodel = res_mod(10, activation='softmax')\r\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\r\n\r\nmodel.fit(train, epochs=25, validation_data=test)\r\n```\r\n```\r\n# This is able to save and works correctly, returning the trained model\r\nmodel.save('custom_model.h5py')\r\ndel model\r\nmodel = keras.models.load_model('custom_model.h5py', custom_objects={'resblock': resblock})\r\n```\r\n\r\n\r\n**Other info / logs**\r\nThis will raise an error that only sequential or functional models can be saved\r\n```model.save('custom_model.h5')```\r\n", "comments": ["@Ryandry1st I tried reproducing the issue but looks code snippet is incomplete. Please provide minimal code snippet to reproduce issue reported here. Also provide the error message that you have got. Thanks!", "The only code not provided was the actual data for the train and test data objects. It is possible to reproduce with the MNIST/ fashion MNIST or really any other dataset, simply change the *10* from the line \r\n`model = res_mod(10, activation='softmax')`\r\n to whatever you are using it for. \r\nAssuming you have loaded train and test data then the code executes up to the save_model function, the error is produced by attempting to save the model as either h5 or hdf5 (while h5py works fine) and produces the following error:\r\n\r\nNotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\r\n\r\nThe entire traceback is attached in the text file.\r\n\r\n[Custom_Model_Save_Error.txt](https://github.com/tensorflow/tensorflow/files/3276997/Custom_Model_Save_Error.txt)\r\n", "The point of this issue is that if it cannot be safely serialized then saving as an h5py either is handling it improperly, and should raise an error similar to the above, _OR_ it works as it should with h5py (based on only this one test case and limited testing besides checking weights and accuracy) and the documentation should include this information, and probably adopt the method/recommend it.", "Any luck on this issue? One option for the data is of course just randomly generating x and y values. It does not matter what data is fed into the network, only that it is a trained subclassed network that is attempted to save", "The code snippet you have provided looks incomplete. Can you please check with latest tf-2.0--nightly version?\r\n```pip install tf-2.0-nightly-preview```", "Here is a more basic code snippet that throws the error. I do want to stress though that the only difference is that I am providing some randomly generated x and y data, and removing layers from the custom model for simplicity. There is no real new code or any ideas and it was no stretch to reach the warning using the previous code, and using random data as was suggested a month ago.\r\n```\r\nx = tf.random.uniform((100,))\r\ny = tf.random.uniform((100,))\r\n\r\n\r\nclass test_model(keras.models.Model):\r\n    def __init__(self, output_dim, activation=None, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.f1 = keras.layers.Flatten()\r\n        self.hidden1 = keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', name=\"h1\")\r\n        self.output1 = keras.layers.Dense(output_dim, activation=keras.activations.get(activation))\r\n        \r\n    def call(self, inputs):\r\n        z = self.f1(inputs)\r\n        z = self.hidden1(z)\r\n        return self.output1(z)\r\n    \r\n    def get_config(self):\r\n        base_config = super().get_config()\r\n        return{**base_config, \"output_dim\" : output_dim, \"activation\": activation}\r\n    \r\nmodel = test_model(1)\r\nmodel.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\nmodel.fit(x, y, epochs=5)\r\nprint(model.weights[0])\r\nmodel.save('custom_model.hdf5')\r\ndel model\r\nmodel = keras.models.load_model('custom_model.hdf5')\r\nprint(model.weights[0])\r\n```\r\n\r\nYou will see that this throws the error, but the following would not.\r\n```\r\nx = tf.random.uniform((100,))\r\ny = tf.random.uniform((100,))\r\n\r\n\r\nclass test_model(keras.models.Model):\r\n    def __init__(self, output_dim, activation=None, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.f1 = keras.layers.Flatten()\r\n        self.hidden1 = keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', name=\"h1\")\r\n        self.output1 = keras.layers.Dense(output_dim, activation=keras.activations.get(activation))\r\n        \r\n    def call(self, inputs):\r\n        z = self.f1(inputs)\r\n        z = self.hidden1(z)\r\n        return self.output1(z)\r\n    \r\n    def get_config(self):\r\n        base_config = super().get_config()\r\n        return{**base_config, \"output_dim\" : output_dim, \"activation\": activation}\r\n    \r\nmodel = test_model(1)\r\nmodel.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\nmodel.fit(x, y, epochs=5)\r\nprint(model.weights[0])\r\nmodel.save('custom_model.h5py')\r\ndel model\r\nmodel = keras.models.load_model('custom_model.h5py')\r\nprint(model.weights[0])\r\n```\r\nSo somewhere along the way it should either be adopted, the error removed, or it should throw a warning using this file format as well.", "I also tried using the preview with the more complicated code and found that it throws a new error, which appears to be due to improperly recalling custom objects. This may be some of the premise behind the warning, but it did work when using the beta version and no longer works with the nightly preview. \r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _convert_inputs_to_signature(inputs, input_signature, flat_input_signature)\r\n   1701       flatten_inputs)):\r\n   1702     raise ValueError(\"Python inputs incompatible with input_signature:\\n%s\" %\r\n-> 1703                      format_error_message(inputs, input_signature))\r\n   1704 \r\n   1705   if need_packing:\r\n\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    Tensor(\"res_mod_1_2/Cast:0\", shape=(None, 16), dtype=float32))\r\n  input_signature: (\r\n    TensorSpec(shape=(None, 16), dtype=tf.float64, name='input_1'))\r\n```", "In TensorFlow 2.0, you should save the model with `save_format=\"h5\"` extension, otherwise it will default to saving to the SavedModel format. Another way to save to H5 is to set the extension to \".h5\" or \".hdf5\" or \".keras\". The H5 format does not support saving Subclassed models, so the initial error stating that only Sequential and Functional models may be saved is intended.\r\n\r\n(The documentation describes the argument on this page: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#save)\r\n\r\n---- \r\nI have an internal change that should fix the ValueError (`Python inputs incompatible with input_signature`), but I can't be sure because the examples above do not raise the error. I'll update this post when the change is copied to github.", "I agree with the save format, my question comes down to the use of h5py, which seems to allow the model to be saved, and it is able to load the model correctly then too. So the question is, should it not allow h5py save format with subclassed models, or if it does work correctly, then shouldn't this be a standard?", "Calling `model.save('custom_model.h5py')` still saves out in the SavedModel format, even though the extension says `.h5py` (so it's not actually using `h5py`).", "That is fine, however, that does not solve the actual issue. The issue is that there is no error or warning if this method is used to save the model, even though it explicitly states that you should not be able to save a subclassed models, yet it clearly allows it if you change the saved model extension to be \".h5py\". So regardless of what is occurring in the background, there is a problem that subclassed models are being saved, unless this can be adopted for all instances.", "This problem has not been solved in rc0, and should be reopened.", "I would like to get a better understanding of the issue is. \r\n\r\nPerhaps docstring for the save_format is unclear? Currently, it says:\r\n\r\n```\r\nsave_format: Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None. Otherwise None defaults to 'tf'.\r\n```", "The NotImplementedError error from an earlier post above states that HDF5 cannot save out subclassed models. The SavedModel format is able to handle these models, so you should not get an error.\r\n\r\n", "I see, so the SavedModel format is able to handle these models. I believe this should be documented then in the warning, as the warning seems to suggest that there is not a clear method for saving these models outside of saving the weights independently from the model, which is obviously more complicated and prone to error", "Specifying that the SavedModel format is able to handle custom models sounds reasonable and would remove the ambiguity. I'll add that to the warning message. Thanks for the suggestion!", "Thank you for clearing that up, appreciate it!", "Now what is the best way to save model created in subclassed ? Were there some examples for how to save and load models ? thank you!", "I understand that so long as you use the save model format, which is done by calling\r\n`model.save(\"NameOfYourModel\", save_format='tf')`\r\nThis should be the most clear method of successfully saving a subclassed model, while also being clear of the format being used. If you specify the file format, e.g. .h5, .keras, then it will not work for a subclassed model. If you specify anything else, as I was experiencing with .h5py, it will default to using save_format=tf", "Closing this issue since I understand it to be resolved by updating the warning, but please let me know if I'm mistaken.\r\n\r\nThe current warning is clearly mentions what to do to save subclass model. Please check the gist [here](https://colab.research.google.com/gist/jvishnuvardhan/7fed70726f8e6def50d408e13a3416d7/untitled783.ipynb).\r\n\r\nCurrent Warning message is as follows\r\n`NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using save_weights.`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29545\">No</a>\n", "Hi @Ryandry1st, may I have help on this similar issue please, using tf==2.2.0\r\nhttps://github.com/tensorflow/tensorflow/issues/41543\r\n\r\nI have tried\r\n```\r\nmodel.save(\"NameOfModel\", save_format='tf')\r\n```\r\nand loaded back with\r\n```\r\nloaded_tfkmodel = tf.keras.models.load_model('./NameOfModel')\r\n```\r\nstill i get\r\n```\r\n    ValueError: Python inputs incompatible with input_signature:\r\n      inputs: (\r\n        Tensor(\"IteratorGetNext:0\", shape=(None, 2), dtype=int32))\r\n      input_signature: (\r\n        TensorSpec(shape=(None, 2), dtype=tf.int64, name='input_1'))\r\n```\r\nand I tried with tf=2.0.0 version, getting same error, I need of help please\r\nThanks", "Found it, Thanks\r\nActually recreating the model with\r\n```\r\nkeras.models.load_model('path_to_my_model')\r\n```\r\ndidn't work for me\r\n\r\nFirst we have to save_weights from the built model\r\n```\r\nmodel.save_weights('model_weights', save_format='tf')\r\n```\r\nThen\r\nwe have to initiate a new instance for the subclass Model then compile and train_on_batch with one record and load_weights of built model\r\n```\r\nloaded_model = ClassifierModel(parameter)\r\nloaded_model.compile(parameters)\r\nloaded_model.train_on_batch(x_train[:1], y_train[:1])\r\nloaded_model.load_weights('model_weights')\r\n```\r\nThis work perfectly in TensorFlow==2.2.0", "Glad you got that figured out @hanzigs, the way you are doing it is one that I use as well when I have custom classes/layers which require the subclassing method. It seems like with the definition of base_config it should work to save the entire model as a SavedModel format, but this has not been my experience. As you said, I just save the weights and then instantiate a new model and load the weights. ", "@Ryandry1st Similar issue, I'm trying to save a subclass model and convert it to TensorFlowJS layers model format. This only works if I'm able to get the model to be saved as h5 but I get the error about not being able to save a non-sequential or functional model. The insistence for me to have the layers model format is so I can retrain using TFJS. Any suggestions?", "I believe you cannot save the model in its entirety as an h5 format because this ignores the information of the control flow/ops that are in the subclasses model. Instead, you need to save the weights in h5 format then make a new model the same way, train_on_batch as shown above, and then load the weights. \r\n\r\nI have not had to deploy a subclasses model in TF JS so I could not say if this is usable or not.", "Hi,\r\n\r\nAdding to @hanzigs , When I train the model and the save the weights using,\r\n\r\nmodel.save_weights(path)\r\n\r\nAnd load it in another session using,\r\n\r\nmodel1 = DCN( )   \r\nmodel1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\r\nmodel1.load_weights(path)\r\n\r\n\r\nThe prediction got in the first session from model and the prediction got from the second session model1 is completely different. How do I solve this ??\r\n\r\n\r\nNOTE: If I save the model using,\r\nmodel.save(os.path.join(model_path,'my_model'),save_format='tf')\r\n\r\nThen I get the below warning. \r\n\r\nWARNING:absl:Found untraced functions such as ranking_layer_call_fn, ranking_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, ranking_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\r\n\r\nThis warning cannot be ignored because when I load the model and try to evaluate it with the same test set , I get the following\r\nerror\r\nValueError: Exception encountered when calling layer \"dcn\" (type DCN).\r\n Could not find matching concrete function to call loaded from the SavedModel."]}, {"number": 29544, "title": "[TF-TRT] Change BiasAdd converter to use Elementwise layer", "body": "Changes the BiasAdd converter to use TRT's Elementwise layer instead of Scale.", "comments": ["I've rebased onto master, but I'm now getting some unit test failures. Looking into those now.", "This broke some tests internally. I am rolling this change back for now.\r\n@aaroey could you help resolve the issues?"]}, {"number": 29543, "title": "Docs should describe how to add a MetaGraphDef to an existing graph", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/saved_model#save_and_restore_models\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description and Usage Example\r\n\r\nI've already created several models, trained over several days each, that we're ready to move from local testing to a serving environment.\r\n\r\nThe models were saved using the function\r\n\r\n```python\r\ndef save_graph_to_file(sess, graph, graph_file_name):\r\n    \"\"\"Saves an graph to file, creating a valid quantized one if necessary.\"\"\"\r\n    output_graph_def = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [final_tensor_name])\r\n    with gfile.FastGFile(graph_file_name, 'wb') as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n```\r\n\r\nvia the [image retraining sample script](https://github.com/tensorflow/tensorflow/blob/v1.6.0/tensorflow/examples/image_retraining/retrain.py#L853-L859).\r\n\r\nNow, I'm ready to move this to a serving environment (via Sagemaker, but that just implements `tensorflow.serving`).\r\n\r\nThe error is clear enough:\r\n\r\n```\r\n2019-06-04 22:38:53.794056: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2019-06-04 22:38:53.798096: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: fail. Took 83297 microseconds.\r\n2019-06-04 22:38:53.798132: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: model version: 1} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\r\n```\r\n\r\nLoading up the graph by [adapting the loader from the retrain script](https://github.com/tensorflow/tensorflow/blob/v1.6.0/tensorflow/examples/image_retraining/retrain.py#L270-L293), I try to just append the serving tag to the graph\r\n\r\n```python\r\ndef load_graph(model_file):\r\n    \"\"\"\r\n    Code from v1.6.0 of Tensorflow's label_image.py example\r\n    \"\"\"\r\n    graph = tf.Graph()\r\n    graph_def = tf.GraphDef()\r\n    with open(model_file, \"rb\") as f:\r\n        graph_def.ParseFromString(f.read())\r\n    with graph.as_default():\r\n        tf.import_graph_def(graph_def)\r\n    return graph\r\n# Load the graph\r\ngraph = load_graph(modelPath)\r\nimport shutil\r\nif os.path.exists(exportDir):\r\n    shutil.rmtree(exportDir)\r\n# Add the serving metagraph tag\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(exportDir)\r\nfrom tensorflow.saved_model import tag_constants\r\nwith tf.Session(graph= graph) as sess:\r\n    builder.add_meta_graph_and_variables(sess, [tag_constants.SERVING, tag_constants.GPU], strip_default_attrs= True)\r\nbuilder.save()\r\nprint(\"Built a SavedModel\")\r\n```\r\n\r\nwhich doesn't work (still has the same error). I'm not giving up on this, but given that the code was written out with a Tensorflow example, this seems like an obvious use case to cover.\r\n\r\n\r\n### Submit a pull request?\r\n\r\nIf I'm able to solve this in a timely way, I'll submit a PR for the docs; at the moment it's a few steps away from me being there, however.\r\n", "comments": ["Solved [the overall issue](https://stackoverflow.com/a/56602995/1877527):\r\n\r\n```python\r\n#!python3\r\n\"\"\"\r\nAssumes we've defined:\r\n\r\n- A directory for our working files to live in, CONTAINER_DIR\r\n- an arbitrary integer VERSION_INT\r\n- We have established local and S3 paths for our model and their labels as variables\r\n\"\"\"\r\n\r\n# Create a versioned path for the models to live in\r\n# See https://stackoverflow.com/a/54014480/1877527\r\nmodelVersionPath = os.path.join(CONTAINER_DIR, VERSION_INT)\r\nif os.path.exists(modelVersionPath):\r\n    shutil.rmtree(modelVersionPath)\r\nos.mkdir(modelVersionPath)\r\nimport tensorflow as tf\r\ndef load_graph(model_file, returnElements= None):\r\n    \"\"\"\r\n    Code from v1.6.0 of Tensorflow's label_image.py example\r\n    \"\"\"\r\n    graph = tf.Graph()\r\n    graph_def = tf.GraphDef()\r\n    with open(model_file, \"rb\") as f:\r\n        graph_def.ParseFromString(f.read())\r\n    returns = None\r\n    with graph.as_default():\r\n        returns = tf.import_graph_def(graph_def, return_elements= returnElements)\r\n    if returnElements is None:\r\n        return graph\r\n    return graph, returns\r\n# Set up dirs\r\nexportDir = modelVersionPath\r\n# Clean up any existing one\r\nimport shutil\r\nif os.path.exists(exportDir):\r\n    shutil.rmtree(exportDir)\r\n# Add the serving metagraph tag\r\n# We need the inputLayerName; in Inception we're feeding the resized tensor\r\n# corresponding to resized_input_tensor_name\r\n# May be able to get away with auto-determining this if not using Inception,\r\n# but for Inception this is the 11th layer\r\ninputLayerName = \"Mul:0\"\r\n# Load the graph\r\nif inputLayerName is None:\r\n    graph = load_graph(modelPath)\r\n    inputTensor = None\r\nelse:\r\n    graph, returns = load_graph(modelPath, returnElements= [inputLayerName])\r\n    inputTensor = returns[0]\r\nwith tf.Session(graph= graph) as sess:\r\n    # Read the layers\r\n    try:\r\n        from tensorflow.compat.v1.saved_model import simple_save\r\n    except (ModuleNotFoundError, ImportError):\r\n        from tensorflow.saved_model import simple_save\r\n    with graph.as_default():\r\n        layers = [n.name for n in graph.as_graph_def().node]\r\n        outName = layers.pop() + \":0\"\r\n        if inputLayerName is None:\r\n            inputLayerName = layers.pop(0) + \":0\"\r\n    print(\"Checking outlayer\", outName)\r\n    outLayer = tf.get_default_graph().get_tensor_by_name(outName)\r\n    if inputTensor is None:\r\n        print(\"Checking inlayer\", inputLayerName)\r\n        inputTensor = tf.get_default_graph().get_tensor_by_name(inputLayerName)\r\n    inputs = {\r\n        inputLayerName: inputTensor\r\n    }\r\n    outputs = {\r\n        outName: outLayer\r\n    }\r\n    simple_save(sess, exportDir, inputs, outputs)\r\nprint(\"Built a SavedModel\")\r\n```\r\n\r\nThis is a good baseline for the docs, I'll see if I can write up a PR around it in the next few days\r\n", "@tigerhawkvok Please provide a link to the PR when you raise so that we will close this when you merge your PR. Thanks!", "@tigerhawkvok,\r\nI'm closing the issue as TensorFlow 1.6 is not actively supported anymore. \r\n\r\nPlease feel free to re-open this issue if you are facing the same error in TensorFlow 2.x. Thanks!"]}, {"number": 29542, "title": "Loss of shape information when using dilation_rate != 1 in Conv layers", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0, 7.5\r\n- GPU model and memory: 11gb, GTX1080Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI was using the code below with 1.12, 1.13.1 and tf2.0 alpha. But it fails to run in the latest tf2.0 beta. As you can see there is nothing fancy going on in the code. This is the block of code that produces this error\r\n```\r\ndef ASPP(tensor):\r\n    '''atrous spatial pyramid pooling'''\r\n    dims = K.int_shape(tensor)\r\n\r\n    y_pool = AveragePooling2D(pool_size=(\r\n        dims[1], dims[2]), name='average_pooling')(tensor)\r\n    y_pool = Conv2D(filters=256, kernel_size=1, padding='same',\r\n                    kernel_initializer='he_normal', name='pool_1x1conv2d', use_bias=False)(y_pool)\r\n    y_pool = BatchNormalization(name=f'bn_1')(y_pool)\r\n    y_pool = Activation('relu', name=f'relu_1')(y_pool)\r\n\r\n    y_pool = Upsample(tensor=y_pool, size=[dims[1], dims[2]])\r\n\r\n    y_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',\r\n                 kernel_initializer='he_normal', name='ASPP_conv2d_d1', use_bias=False)(tensor)\r\n    y_1 = BatchNormalization(name=f'bn_2')(y_1)\r\n    y_1 = Activation('relu', name=f'relu_2')(y_1)\r\n\r\n    y_6 = Conv2D(filters=256, kernel_size=3, dilation_rate=6, padding='same',\r\n                 kernel_initializer='he_normal', name='ASPP_conv2d_d6', use_bias=False)(tensor)\r\n    y_6 = BatchNormalization(name=f'bn_3')(y_6)\r\n    y_6 = Activation('relu', name=f'relu_3')(y_6)\r\n\r\n    y_12 = Conv2D(filters=256, kernel_size=3, dilation_rate=12, padding='same',\r\n                  kernel_initializer='he_normal', name='ASPP_conv2d_d12', use_bias=False)(tensor)\r\n    y_12 = BatchNormalization(name=f'bn_4')(y_12)\r\n    y_12 = Activation('relu', name=f'relu_4')(y_12)\r\n\r\n    y_18 = Conv2D(filters=256, kernel_size=3, dilation_rate=18, padding='same',\r\n                  kernel_initializer='he_normal', name='ASPP_conv2d_d18', use_bias=False)(tensor)\r\n    y_18 = BatchNormalization(name=f'bn_5')(y_18)\r\n    y_18 = Activation('relu', name=f'relu_5')(y_18)\r\n\r\n    y = concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')\r\n\r\n    y = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',\r\n               kernel_initializer='he_normal', name='ASPP_conv2d_final', use_bias=False)(y)\r\n    y = BatchNormalization(name=f'bn_final')(y)\r\n    y = Activation('relu', name=f'relu_final')(y)\r\n    return y\r\n```\r\nStrangely shapes of  y_pool, y_1 are correctly inferred , complete code to reproduce the issue available below\r\n``` \r\ny = concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')\r\n```\r\n**Describe the expected behavior**\r\nThe code should work as it did in the earlier release ie tf2.0 alpha\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import AveragePooling2D, Lambda, Conv2D, Conv2DTranspose, Activation, Reshape, concatenate, Concatenate, BatchNormalization, ZeroPadding2D\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50\r\n\r\n\r\ndef Upsample(tensor, size):\r\n    '''bilinear upsampling'''\r\n    name = tensor.name.split('/')[0] + '_upsample'\r\n\r\n    def bilinear_upsample(x, size):\r\n        resized = tf.image.resize(\r\n            images=x, size=size)\r\n        return resized\r\n    y = Lambda(lambda x: bilinear_upsample(x, size),\r\n               output_shape=size, name=name)(tensor)\r\n    return y\r\n\r\n\r\ndef ASPP(tensor):\r\n    '''atrous spatial pyramid pooling'''\r\n    dims = K.int_shape(tensor)\r\n\r\n    y_pool = AveragePooling2D(pool_size=(\r\n        dims[1], dims[2]), name='average_pooling')(tensor)\r\n    y_pool = Conv2D(filters=256, kernel_size=1, padding='same',\r\n                    kernel_initializer='he_normal', name='pool_1x1conv2d', use_bias=False)(y_pool)\r\n    y_pool = BatchNormalization(name=f'bn_1')(y_pool)\r\n    y_pool = Activation('relu', name=f'relu_1')(y_pool)\r\n\r\n    y_pool = Upsample(tensor=y_pool, size=[dims[1], dims[2]])\r\n\r\n    y_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',\r\n                 kernel_initializer='he_normal', name='ASPP_conv2d_d1', use_bias=False)(tensor)\r\n    y_1 = BatchNormalization(name=f'bn_2')(y_1)\r\n    y_1 = Activation('relu', name=f'relu_2')(y_1)\r\n\r\n    y_6 = Conv2D(filters=256, kernel_size=3, dilation_rate=6, padding='same',\r\n                 kernel_initializer='he_normal', name='ASPP_conv2d_d6', use_bias=False)(tensor)\r\n    y_6 = BatchNormalization(name=f'bn_3')(y_6)\r\n    y_6 = Activation('relu', name=f'relu_3')(y_6)\r\n\r\n    y_12 = Conv2D(filters=256, kernel_size=3, dilation_rate=12, padding='same',\r\n                  kernel_initializer='he_normal', name='ASPP_conv2d_d12', use_bias=False)(tensor)\r\n    y_12 = BatchNormalization(name=f'bn_4')(y_12)\r\n    y_12 = Activation('relu', name=f'relu_4')(y_12)\r\n\r\n    y_18 = Conv2D(filters=256, kernel_size=3, dilation_rate=18, padding='same',\r\n                  kernel_initializer='he_normal', name='ASPP_conv2d_d18', use_bias=False)(tensor)\r\n    y_18 = BatchNormalization(name=f'bn_5')(y_18)\r\n    y_18 = Activation('relu', name=f'relu_5')(y_18)\r\n\r\n    y = concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')\r\n\r\n    y = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',\r\n               kernel_initializer='he_normal', name='ASPP_conv2d_final', use_bias=False)(y)\r\n    y = BatchNormalization(name=f'bn_final')(y)\r\n    y = Activation('relu', name=f'relu_final')(y)\r\n    return y\r\n\r\n\r\ndef DeepLabV3Plus(img_height, img_width):\r\n    base_model = ResNet50(input_shape=(\r\n        img_height, img_width, 3), weights='imagenet', include_top=False)\r\n\r\n    image_features = base_model.get_layer('activation_39').output\r\n    x_a = ASPP(image_features)\r\n    x_a = Upsample(tensor=x_a, size=[img_height // 4, img_width // 4])\r\n\r\n    x_b = base_model.get_layer('activation_9').output\r\n    x_b = Conv2D(filters=48, kernel_size=1, padding='same',\r\n                 kernel_initializer='he_normal', name='low_level_projection', use_bias=False)(x_b)\r\n    x_b = BatchNormalization(name=f'bn_low_level_projection')(x_b)\r\n    x_b = Activation('relu', name='low_level_activation')(x_b)\r\n\r\n    x = concatenate([x_a, x_b], name='decoder_concat')\r\n\r\n    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu',\r\n               kernel_initializer='he_normal', name='decoder_conv2d_1', use_bias=False)(x)\r\n    x = BatchNormalization(name=f'bn_decoder_1')(x)\r\n    x = Activation('relu', name='activation_decoder_1')(x)\r\n\r\n    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu',\r\n               kernel_initializer='he_normal', name='decoder_conv2d_2', use_bias=False)(x)\r\n    x = BatchNormalization(name=f'bn_decoder_2')(x)\r\n    x = Activation('relu', name='activation_decoder_2')(x)\r\n    x = Upsample(x, [img_height, img_width])\r\n\r\n    x = Conv2D(1, (1, 1), name='output_layer')(x)\r\n    x = Activation('sigmoid')(x)\r\n    model = Model(inputs=base_model.input, outputs=x, name='DeepLabV3_Plus')\r\n    return model\r\n```\r\n**Other info / logs**\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-20-0876af914f24> in <module>()\r\n----> 1 DeepLabV3Plus(512, 512)\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py in build(self, input_shape)\r\n    389                        'inputs with matching shapes '\r\n    390                        'except for the concat axis. '\r\n--> 391                        'Got inputs shapes: %s' % (input_shape))\r\n    392 \r\n    393   def _merge_function(self, inputs):\r\n\r\nValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 32, 32, 256), (None, 32, 32, 256), (None, None, None, 256), (None, None, None, 256), (None, None, None, 256)]\r\n```\r\n", "comments": ["@gadagashwini @jvishnuvardhan @dynamicwebpaige ", "Interesting thing I found out was, this happens to all Conv2D layers with dilation_rate != 1 \r\nExplicitly setting shapes to the tensors seems to be a dirty workaround as of now\r\n```\r\ny_6.set_shape([None, 32, 32, 256])\r\ny_6.shape\r\n```\r\nOutput\r\n```\r\nTensorShape([None, 32, 32, 256])\r\n```", "Did you use tf.function decorator?", "here is colab notebook showing the problem\r\nhttps://colab.research.google.com/drive/1UtkZRNyBBRJqDgDo3VMMP2otxSJXcv88", "@llan-ml no I did not use it", "@fchollet \r\n\r\nProblem in compute_output_shape or should the set_shape be inside call?", "Duplicate of issue #28400\r\nThe regression happened somewhere between tf-nightly-gpu-2.0-preview==2.0.0.dev20190410  and tf-nightly-gpu-2.0-preview==2.0.0.dev20190504. \r\nIt also affects 1.14 RC0 when Eager mode is enabled.\r\nDilations worked fine in 2.0-Alpha", "#29843 has some debugging information.", "I will close the other two as duplicates.", "Perhaps related to https://github.com/tensorflow/tensorflow/issues/26797, where the shape in dilated conv is not just lost but wrong.", "are there some news/updates ?", "@k-w-w is working on a fix.", "Thanks for finding this bug! The fix has been submitted (waiting for the changes to be copied in). \r\n\r\nFor now, to get around the shape issue, run the code below after the Conv layer has just been created/before calling the layer:\r\n\r\n```\r\nfrom tensorflow.python.keras import backend\r\nwith backend.get_graph().as_default():\r\n  layer.build(tensor.shape) \r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29542\">No</a>\n", "how/where can i see your merge request to know when it gets merged and a nightly is usable?", "Here's the commit: https://github.com/tensorflow/tensorflow/commit/0b4c19a3c1d1d3e7c016760aeea099fecb5fa544", "thanks"]}, {"number": 29541, "title": "tf.train.Saver fails on AutoTrackable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): public colab instance\r\n- TensorFlow installed from (source or binary): tf-nightly-1.14.1.dev20190607\r\n- TensorFlow version (use command below): 1.14.1.dev20190607\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nManually specifying an instance of AutoTrackable (e.g., tf.Module) causes `tf.train.Saver` to fail with a somewhat cryptic error:\r\n\r\n`ValueError: Attr 'dtypes' of 'SaveV2' Op passed list of length 0 less than minimum 1.`\r\n\r\nThe issue seems to come from a call to `Trackable._gather_saveables_for_checkpoint` which is not overridden by `tf.Module` or `AutoTrackable`. \r\n\r\n**Describe the expected behavior**\r\nI'm not sure if this is intended to work (so maybe this should be a feature request?) but at the least I think the issue should be caught earlier and return a more informative error message. \r\n\r\nThe docs for `tf.train.Saver` call for a list/dict of `SaveableObject`, which `AutoTrackable` is not, but it seems odd that it isn't. It is possible that I am misunderstanding the `SaveableObject`/`Saver` API, but I do feel like `AutoTrackable` should be compatible `tf.train.Saver`.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.Module):\r\n  \r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self._var = tf.Variable(1.)\r\n    \r\nmodel = MyModel()\r\nsaver = tf.train.Saver({\"model\": model})\r\n```\r\n\r\n**Other info / logs**\r\n@tomhennigan \r\n", "comments": ["Please use `tf.train.Checkpoint` rather than `tf.train.Saver` to save objects: https://www.tensorflow.org/beta/guide/checkpoints (the guide is for 2.x, but the APIs are in 1.x as well).\r\n\r\nThe error message could be improved though; the reason it gets that far is that you can pass some Trackable objects to Saver (e.g. MirroredVariables).", "Thanks for the response! I had just assumed that `Saver` would be present in 2.x. Since it's not, this is probably not worth much manpower. It might be worth marking `tf.train.Saver` as deprecated since the issue arises using features present in 1.x. Alternatively, if deprecating is not desirable, a warning in the `tf.train.Saver` docs would probably help people like me who accidentally mix the old and new TF features.", "@gehring : Thanks for your support to the community. Can we close the issue since it looks to be resolved.  Let us know.", "@achandraa The issue is resolved on my end but I do think a note in the docs would help prevent others from running into the same confusion. I've made a small PR for this (#29609). Feel free modify/merge/close the PR however you think is best and close this issue.", "@gehring : Thanks for raising the PR. We will close the issue once PR gets merged.", "@achandraa PR merged, closing this issue. Feel free to re-open if you feel like there is still something to address.", "@gehring : Thanks for the update. Please feel free to come back in case of any other issue.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29541)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29541)\r\n"]}, {"number": 29540, "title": "TF2-beta has mismatch between libtensorflow_framework and link flag", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0.b0\r\n\r\n**Describe the current behavior**\r\nThere is a mismatch in the name of `libtensorflow_framework` and the `tf.sysconfig.get_link_flag` name. As an [example TF-Addons uses this to link](https://github.com/tensorflow/addons/blob/master/configure.sh#L46) with tensorflow core. \r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n('v1.12.1-3259-gf59745a381', '2.0.0-beta0')\r\n\r\n$ python -c \"import tensorflow as tf; print(tf.sysconfig.get_link_flags())\"\r\n['-L/usr/local/lib/python2.7/dist-packages/tensorflow', '-l:libtensorflow_framework.so.2\r\n\r\n$ ll /usr/local/lib/python2.7/dist-packages/tensorflow\r\ntotal 34128\r\ndrwxr-sr-x 10 root staff     4096 Jun  7 17:24 ./\r\ndrwxrwsr-x  1 root staff     4096 Jun  7 17:24 ../\r\n...\r\ndrwxr-sr-x  9 root staff     4096 Jun  7 17:24 include/\r\n-rwxr-xr-x  1 root staff 34858656 Jun  7 17:24 libtensorflow_framework.so.1*\r\ndrwxr-sr-x  5 root staff     4096 Jun  7 17:24 lite/\r\ndrwxr-sr-x 29 root staff     4096 Jun  7 17:24 python/\r\ndrwxr-sr-x  6 root staff     4096 Jun  7 17:24 tools/\r\n\r\n\r\n```\r\n**Describe the expected behavior**\r\nThe framework should be named `libtensorflow_framework.so.2`\r\n\r\n**Code to reproduce the issue**\r\n```\r\npip install tensorflow==2.0.0.b0\r\n```\r\n\r\ncc @yifeif @perfinion ", "comments": ["Seems like it might be due to VERSION is set to 1.13.1 here https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/tensorflow.bzl#L60", "This issue is now fixed on beta1. Closing and thanks for the help!"]}]