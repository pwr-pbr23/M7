[{"number": 32830, "title": "TfLite ObjectDetection Demo crashed on Android with armeabi", "body": "java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:71)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:52)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)\r\n        at com.tfdetection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:123)", "comments": ["@WestbrookZero,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "@WestbrookZero, Please elaborate the issue with context. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32830\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32830\">No</a>\n"]}, {"number": 32829, "title": "fix .so file name error on linux and macos", "body": "I have reported an error when using tensorflow1.14 on linux. The program prompts that libtensorflow_framework.so.1 cannot be found. It is found that only libtensorflow_framework.so is decompressed under /tmp, so I modified the NativeLibrary class and corrected the name when releasing the resource.\r\n\r\nrun on linux error\uff1a\r\njava.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1562914806051-0/libtensorflow_jni.so: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory\r\n\r\nrun on macos error\uff1a\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: /private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib: dlopen(/private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib, 1): Library not loaded: @rpath/libtensorflow_framework.1.dylib\r\n  Referenced from: /private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib\r\n  Reason: image not found", "comments": ["I will try to check the method and resolve this problem, thanks", "> Can you modify the getVersionedLibraryName method instead of fixing its incorrect output afterwords?\r\n> \r\n> And do you know what the issue is there? Could be something like getMajorVersionNumber returning 2 but the shared object filenames haven't been updated yet. In which case we should update the shared objects, or just fix the version number at 1 for now since the Java API hasn't changed for TF 2.x.\r\n\r\nI found the problem, getMajorVersionNumber() uses the package's getImplementVersion(), which returns the implementation version of the MANIFEST.MF in the jar package, but tensorflow does not declare the pointer when the jar is called, resulting in an empty return, so Make the suffix of the .so file incorrect.\r\n\r\nI found that even if MANIFEST.MF contains the Implementation-Version field, getImplementVersion() does not return the version number correctly. I am at https://stackoverflow.com/questions/33553570/how-to-read-meta-data-from-manifest-file found a way to correctly read the Implementation-Version.\r\n\r\nSo I think we can temporarily fix the version number to 1, and then tf2.x declares the Implementation-Version.", "666"]}, {"number": 32828, "title": "[r1.15-CherryPick]:Update TRT6 headerfiles", "body": "- This enables users to compile TF2.0 with TensorRT6.0.\r\n- Without this PR, users would need to apply the patch before compiling TF2.0 with TensorRT6.0 which would not be a good experience.\r\n- TensorRT6.0 comes with optimizations that accelerate 3D image segmentation among others.\r\n- Google can still build and release TF2.0 with TensorRT5.1. So this PR doesn't affect that build/release process.", "comments": []}, {"number": 32827, "title": "[Intel Mkl] Upgrading MKL public CI to py3", "body": "", "comments": []}, {"number": 32826, "title": "[r2.0-CherryPick]: Fix deprecation warnings in feature_column_v2.", "body": "This PR is to fix deprecation warnings in feature_column_v2.", "comments": []}, {"number": 32825, "title": "r2.0 cherry-pick request: Fix deprecation warnings in feature_column_v2.", "body": "This PR is to fix deprecation warnings in feature_column_v2.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32825) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 32824, "title": "[r2.0-CherryPick]: Update TF version to 2.0.0.", "body": "", "comments": []}, {"number": 32823, "title": "[r2.0-CherryPick]:Update TRT6 headerfiles", "body": "- This enables users to compile TF2.0 with TensorRT6.0.\r\n- Without this PR, users would need to apply the patch before compiling TF2.0 with TensorRT6.0 which would not be a good experience.\r\n- TensorRT6.0 comes with optimizations that accelerate 3D image segmentation among others.\r\n- Google can still build and release TF2.0 with TensorRT5.1. So this PR doesn't affect that build/release process.\r\n\r\n", "comments": []}, {"number": 32822, "title": "[r2.0-CherryPick]: Add missing 'continue' in code intended to skip creating TemporaryVar\u2026", "body": "\u2026iable nodes of invalid types on GPUs.\r\n\r\nPiperOrigin-RevId: 267179945", "comments": []}, {"number": 32821, "title": "Optimizer Fails to Run in Compatibility Mode (tested for Adam and Gradient Descent)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.12.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0-rc1\r\n- Python version:3.7.1\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A (CPU)\r\n- GPU model and memory:N/A (CPU)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nOptimizer increments the global step by 1 and stops without changing the network.\r\n**Describe the expected behavior**\r\nOptimizer runs metric to convergence\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\nimport numpy as np\r\ntf.disable_v2_behavior()\r\ntf.disable_eager_execution()\r\ninput = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name=\"input\")\r\nreshape = tf.reshape(input, [tf.shape(input)[0],784])\r\nw1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name=\"w1\")\r\nb1 = tf.Variable(tf.random_normal([128], dtype=tf.float32), name=\"b1\")\r\nlayer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))\r\nlayer_one_biased = tf.add(tf.transpose(layer_one_unbiased),b1)\r\nactivated_layer_one = tf.nn.relu(layer_one_biased)\r\nw2 = tf.Variable(tf.random_normal([10,128]), dtype=tf.float32, name=\"w2\")\r\nb2 = tf.Variable(tf.random_normal([10], dtype=tf.float32, name=\"b2\"))\r\nlayer_two_unbiased = tf.matmul(w2,tf.transpose(activated_layer_one))\r\nlayer_two_biased = tf.add(tf.transpose(layer_two_unbiased), b2)\r\nlabels= tf.placeholder(dtype=tf.int32, shape=(None))\r\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased)\r\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\r\n#optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\r\noptimizer = tf.train.GradientDescentOptimizer(0.1)\r\ntrain = optimizer.minimize(loss, global_step=global_step, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\r\n\r\npredictions = tf.nn.softmax(layer_two_biased, name=\"final\")\r\n#label_acc = tf.one_hot(labels,10)\r\n#accuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2\r\nacc, acc_op = tf.metrics.accuracy(tf.argmax(predictions,1), labels)\r\n#accuracy_averaged = tf.math.reduce_mean(accuracy)\r\n\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\ntrain_images = train_images/255.0\r\ntest_images = test_images/255.0\r\nprint(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\nfeed_dict = {input: train_images, labels: train_labels}\r\nprint(sess.run([loss,train,acc_op, global_step],feed_dict=feed_dict))\r\n\r\nfeed_dict2 = {input: test_images, labels: test_labels}\r\nprint(\"Test accuracy\", sess.run(acc_op, feed_dict = feed_dict2))\r\nprint(\"Training accuracy\",sess.run(acc_op, feed_dict = feed_dict))\r\nsaver =tf.train.Saver()\r\nsave_path = saver.save(sess, \"model.ckpt\")\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nCode successfully runs so no error occurs but output of global step value is 1. Note that this network matches the fashion mnist network but is just using tf instead of tf.keras.layers to encode the layers.", "comments": ["Note eager execution is disabled since under eager execution the optimizer is supposed to take a function that calculates loss and not a tensor.  With eager execution disabled according to the docs a tensor is acceptable input. Also tried without global step and with using var_list as just the defaults.", "@TimCapes ,\r\nWhen tried executing the given code the following output was received, take a look at the [gist](https://colab.sandbox.google.com/gist/oanush/c3b49526e45c415e33d707960caef47b/untitled10.ipynb) of colab.\r\nCan you confirm if the same output is received?Thanks!", "@oanush Output looks to be the same as what I'm getting.  You'll notice it doesn't include any training of the network and that the accuracy is roughly random and the global_steps variable has value 1.", "I am now using the following revised code which works better for TFLite conversion. Still have the optimizer issue.\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\nimport numpy as np\r\ntf.disable_v2_behavior()\r\ntf.disable_eager_execution()\r\ninput = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name=\"input\")\r\nreshape = tf.reshape(input, [tf.shape(input)[0],784])\r\nw1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name=\"w1\")\r\nb1 = tf.Variable(tf.random_normal([128,1], dtype=tf.float32), name=\"b1\")\r\nlayer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))\r\nlayer_one_biased = tf.add(layer_one_unbiased,b1)\r\nactivated_layer_one = tf.nn.relu(layer_one_biased)\r\nw2 = tf.Variable(tf.random_normal([10,128]), dtype=tf.float32, name=\"w2\")\r\nb2 = tf.Variable(tf.random_normal([10,1], dtype=tf.float32, name=\"b2\"))\r\nlayer_two_unbiased = tf.matmul(w2,activated_layer_one)\r\nlayer_two_biased = tf.add(layer_two_unbiased, b2)\r\nlabels= tf.placeholder(dtype=tf.int32, shape=(None))\r\nlayer_two_biased_transpose = tf.transpose(layer_two_biased)\r\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased_transpose)\r\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\r\n#optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\r\noptimizer = tf.train.GradientDescentOptimizer(0.1)\r\ntrain = optimizer.minimize(loss, global_step=global_step, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\r\n\r\npredictions = tf.nn.softmax(layer_two_biased_transpose, name=\"final\")\r\n#label_acc = tf.one_hot(labels,10)\r\n#accuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2\r\nacc, acc_op = tf.metrics.accuracy(tf.argmax(predictions,1), labels)\r\n#accuracy_averaged = tf.math.reduce_mean(accuracy)\r\n\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\ntrain_images = train_images/255.0\r\ntest_images = test_images/255.0\r\nprint(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\nfeed_dict = {input: train_images, labels: train_labels}\r\nprint(sess.run([loss,train,acc_op, global_step],feed_dict=feed_dict))\r\n```", "Apologies for the delay in response, Is this still an issue? \r\nCan you please test with latest version of TensorFlow? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32821\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32821\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32820, "title": "[r2.0-CherryPick]:remove array_ops.where deprecation message.", "body": "PiperOrigin-RevId: 271168043", "comments": []}, {"number": 32819, "title": "Shared and mutated state between training and validation callbacks", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Datalab\r\n- Mobile device: NA\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: 1.15.0rc1\r\n- Python version: 3.5.6\r\n- Bazel version: NA\r\n- GCC/Compiler version: NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nWhen invoking `keras.Model.fit` with two `data.Dataset`s, one is for training, and one is validation, so that the training one is infinite with `epochs` and `steps_per_epoch` provided to `fit`, and the validation one is finite without any additional parameters to `fit`, the progress bar shows incorrect number of steps after the first validation. In particular, the number of steps per epoch gets set to the number of steps in the validation set.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe prescribed `steps_per_epoch` remains the same in the progress bar for all epochs.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee below.\r\n\r\n**Other info / logs**\r\n\r\nI believe the problem is due to this modification when the input is exhausted:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L136\r\n\r\nThe number of steps gets overwritten. However, `self.params` is shared across both the training and validation callbacks, which essentially misleads the progress bar for training. The state share is potentially due to the following line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L347\r\n\r\nIn other words, `validation_callbacks` is based on `training_callbacks`. `configure_callbacks` should probably take `callbacks` instead.", "comments": ["@IvanUkhov \r\nIn order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "@ravikyram, it is good that you asked for an example. It turned out it was only about those datasets whose size could not be known in advance. I had to use a generator to reproduce.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef generator():\r\n    for _ in range(100):\r\n        yield [1, 1], 1\r\n\r\ntraining = tf.data.Dataset \\\r\n    .from_generator(\r\n        generator=generator,\r\n        output_types=(tf.float64, tf.float64),\r\n        output_shapes=(tf.TensorShape([2]), tf.TensorShape([])),\r\n    ) \\\r\n    .batch(2) \\\r\n    .repeat()\r\n\r\nvalidation = tf.data.Dataset \\\r\n    .from_generator(\r\n        generator=generator,\r\n        output_types=(tf.float64, tf.float64),\r\n        output_shapes=(tf.TensorShape([2]), tf.TensorShape([])),\r\n    ) \\\r\n    .batch(2)\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(2),\r\n    tf.keras.layers.Dense(1),\r\n])\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\nmodel.fit(\r\n    x=training,\r\n    validation_data=validation,\r\n    epochs=10,\r\n    steps_per_epoch=20,\r\n)\r\n```\r\n\r\n```\r\nTrain for 20 steps\r\nEpoch 1/10\r\n20/20 [==============================] - 0s 24ms/step - loss: 1.1096 - val_loss: 1.0443\r\nEpoch 2/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.9881 - val_loss: 0.0000e+00Epoch 3/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.8761 - val_loss: 0.0000e+00Epoch 4/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.7738 - val_loss: 0.0000e+00Epoch 5/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.6808 - val_loss: 0.0000e+00Epoch 6/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.5967 - val_loss: 0.0000e+00Epoch 7/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.5209 - val_loss: 0.0000e+00Epoch 8/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.4528 - val_loss: 0.0000e+00Epoch 9/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.3920 - val_loss: 0.0000e+00Epoch 10/10\r\n20/50 [===========>..................] - ETA: 0s - loss: 0.3378 - val_loss: 0.0000e+00\r\n<tensorflow.python.keras.callbacks.History at 0x7f01119bdb38>\r\n```", "I have tried on colab with TF version 1.15.0-rc0,1.15.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/608e6fcff91f3b80d8025f7f9c4876f6/untitled231.ipynb). Thanks!", "I have an idea of how it could be fixed, and I will try to explore it in #32847.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32819\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32819\">No</a>\n"]}, {"number": 32818, "title": "Add function to gather custom objects - easy pickling for models", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): idk\r\n\r\n**Describe the feature and the current behavior/state.**\r\nKeras models should be able to iterate through their config and find any layers/functions that would be considered `custom_objects`. Currently, you need to handle them manually.\r\n\r\nMy goal is to be able to do:\r\n```python\r\nwith open('custom_objects.pkl', 'wb') as f:\r\n    pickle.dump(gather_custom_objects(model), f)\r\n\r\nwith open('model_spec.pkl', 'wb') as f:\r\n    pickle.dump(model.get_config(), f)\r\n\r\n...\r\n\r\nwith open('custom_objects.pkl', 'rb') as f:\r\n    custom_objects = pickle.load(f)\r\n\r\nwith open('model_spec.pkl', 'rb') as f:\r\n    model = model_from_config(pickle.load(f), custom_objects=custom_objects)\r\n```\r\n\r\n**Will this change the current api? How?**\r\nIt would add a utility function `gather_custom_objects(model)`\r\n\r\n**Who will benefit with this feature?**\r\nPeople who want to not have to think about custom objects in their models.\r\n\r\n**Any Other info.**\r\nI realize it might not be trivial to do in all cases (non-standard layer configs, for example) which is probably why it's not implemented yet.", "comments": ["Hello,\r\n\r\nIt is unsafe to pickle a Keras model. Rather than doing this, consider using `model.save(path, format='tf')`, which will use the SavedModel format and which will also include the custom objects.", "Automatically closing this out since I understand it to be resolved by @fchollet comment, but please let me know if I'm mistaken.Thanks!"]}, {"number": 32817, "title": "Re-emerged Issue #31509 - BaseCollectiveExecutor::StartAbort Out of range:", "body": "The previous issue described in #31509 was fixed, but I am now experiencing exactly the same issue  with all the same setup using the latest nightly build of TF2.0 when using tf.keras.optimizers.Adam", "comments": ["@oracle3001 \r\nI am not seeing any issue with tf.keras.optimizers.Adam in latest TF 2.0.0-rc2 version.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/fbae46031175e81f1c11f7e76a1e3ed0/untitled224.ipynb). Thanks!", "I am having the exact same problem using this mock model. I am using tf 2.0.0 release.\r\nOn windows\r\n\r\n```python\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    x = tf.random.normal((14000, 30, 1))\r\n    y = tf.ones_like(x)\r\n\r\n    discriminator = tf.keras.models.Sequential([\r\n        tf.keras.layers.LSTM(100, input_shape=(30, 1), return_sequences=True),\r\n        tf.keras.layers.LSTM(100, recurrent_dropout=0.4,\r\n                             dropout=0.4, return_sequences=True)\r\n    ])\r\n\r\n    discriminator.compile(loss='binary_crossentropy',\r\n                          optimizer=tf.keras.optimizers.Adam(lr=0.001))\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    dataset = dataset.batch(64)\r\n\r\n    discriminator.fit(dataset, epochs=2)\r\n``", "@duysPES \r\nI am able to execute the code successfully in colab using TF 2.0.0-rc2 .Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/422f9b591f99f38e5c7f9f9b7211fea1/untitled248.ipynb).Thanks! ", "I am also having this message feeding a dataset into a 1D Convnet. Happens on my Mac with tf version 2.0.0-rc2. Not reproducible on Colab.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ndef create_timeseries_element():\r\n    # returns a random time series of 100 intervals, each with 3 features,\r\n    # and a random one-hot array of 5 entries\r\n    data = np.random.rand(100,3)\r\n    label = np.eye(5, dtype='int')[np.random.choice(5)]\r\n    return data, label\r\n\r\ndef data_generator():\r\n    d, l = create_timeseries_element()\r\n    yield (d, l)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Conv1D(128, 9, activation='relu', input_shape=(100, 3)),\r\n    tf.keras.layers.Conv1D(128, 9, activation='relu'),\r\n    tf.keras.layers.MaxPooling1D(2),\r\n    tf.keras.layers.Conv1D(256, 5, activation='relu'),\r\n    tf.keras.layers.Conv1D(256, 5, activation='relu'),\r\n    tf.keras.layers.GlobalAveragePooling1D(),\r\n    tf.keras.layers.Dropout(0.5),\r\n    tf.keras.layers.Dense(5, activation='softmax')])\r\nmodel.compile(optimizer='adam',\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nds = tf.data.Dataset.from_generator(data_generator, output_types=(tf.float32, tf.int32),\r\n                                      output_shapes=(tf.TensorShape([100, 3]), tf.TensorShape([5])))\r\nmodel.fit(ds.batch(32))\r\n```", "I am having an issue similar to this and tried to run in Collab just to get an un-ending runtime. I asked my question in full on SO [here](https://stackoverflow.com/questions/58245855/numpy-arrays-used-in-training-in-tf1-keras-have-much-lower-accuracy-in-tf2?noredirect=1#comment102869129_58245855).\r\n\r\nI had some numpy arrays that were trained in keras in the previous version of tf and now have to rewrite my model. Got way worse accuracy so I am thinking I need to switch to tf.data.Dataset.\r\n\r\nSo I did:\r\n`\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_deleted_nans, y_train_no_nans))\r\ntrain_dataset = train_dataset.shuffle(SHUFFLE_CONST).batch(BATCH_SIZE)\r\n`\r\n\r\n`model.summary()` gave me:\r\n\r\n```\r\nBatchDataset shapes: ((None, 2756), (None,)), types: (tf.float64, tf.int64)\r\nModel: sequential\r\n\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 1379)              3801903   \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1379)              1903020   \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1379)              1903020   \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 1379)              1903020   \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_4 (Dense)              (None, 1)                 1380      \r\n=================================================================\r\nTotal params: 9,512,343\r\nTrainable params: 9,512,343\r\nNon-trainable params: 0\r\n\r\n```\r\n```\r\nmodel.compile(optimizer=adam, loss=bce, metrics=['accuracy'])\r\nmodel.fit(train_dataset, epochs=1000, verbose=0)\r\n```\r\nOnce the training starts I get this warning error:\r\n\r\n```\r\n2019-10-04 23:47:56.691434: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n     [[{{node IteratorGetNext}}]]\r\n\r\n```", "I am having the same issue as above on TF 2.0 release. Is this a bug with tensorflow or is there an issue with the code?", "It seems everybody who is having this issue is using Windows. I presume that must have something to do with it?", "I am having the issue on a Mac with the latest version of MacOS", "I am having the same problem after porting my code from 1.14 to 2.0.\r\n\r\nI am running on UBUNTU 18.04 (not only a windows problem).  It occurs for me during both training and predict.  (so not linked to the optimiser).  I do NOT get the problem if i hide the GPU.   I do get the problem if I expose the GPU.\r\n\r\n\r\nEdit - Note:  Everything seems to run properly, I just get the warnings\r\n\r\nEdit - In another case - I get the problem whether I use GPU or not.", "I think I may have found why it is complaining. However, I have no idea how to fix it. While training, we all get the IteratorGetNext Error: Sequence out of range. \r\n\r\nI noticed that let\u2019s say I have a dataset size of 60,000 with a batch size of 64 that would require floor(60000/64)= 937 to iterate through the entire dataset for one epoch. However, when training using .fit(verbose=1) I noticed that it attempts to iterate through the dataset 938 (most likely a rounding error because 60000/64=937.5) and thus I get this error. Can someone please confirm this is the case for you as well? Thanks\r\n\r\nEdit:\r\n\r\nSo I found a way around this when building the tf.data.Dataset, make sure to add the .repeat() method because the program will complain that you ran out of data, and when using .fit() ~add the following~:\r\n\r\nHere is a full example that got it working.\r\n\r\nThis will cause the error:\r\n\r\n```python\r\ndata = tf.random.normal((60000,30,4))\r\nground_truth = tf.ones((60000,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).batch(64)\r\n\r\n#predefined model here: input: [?, 30,4] output: [?,1]\r\nmodel.fit(dataset, epochs=5)\r\n\r\n'''\r\n    938/Unknown - 16s 17ms/step - loss: 0.02172019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Shape/_2]]\r\n2019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n938/938 [==============================] - 16s 17ms/step - loss: 0.0217\r\nEpoch 2/5\r\n935/938 [============================>.] - ETA: 0s - loss: 2.2229e-062019-10-07 14:49:59.722216: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n2019-10-07 14:49:59.722218: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Shape/_2]]\r\n'''\r\n```\r\nThis is the work around.\r\n\r\n```python\r\nbatch_size = 64\r\ndata = tf.random.normal((60000,30,4))\r\nground_truth = tf.ones((60000,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).batch(batch_size).repeat()\r\n\r\n#predefined model here: input: [?, 30,4] output: [?,1]\r\nmodel.fit(dataset, epochs=5, steps_per_epoch=data.shape[0]//batch_size)\r\n\r\n'''\r\n937/937 [==============================] - 15s 16ms/step - loss: 0.0135\r\nEpoch 2/5\r\n937/937 [==============================] - 10s 10ms/step - loss: 1.4460e-05\r\nEpoch 3/5\r\n937/937 [==============================] - 10s 11ms/step - loss: 4.3097e-06\r\nEpoch 4/5\r\n937/937 [==============================] - 10s 10ms/step - loss: 1.8212e-06\r\nEpoch 5/5\r\n'''\r\n\r\n``", "Good idea.   I think you can use the take statement on the dataset to limit\nyourself to the first 64*937 records avoiding any need to round at the end\n\nOn Mon, 7 Oct 2019, 23:02 duysqubix <notifications@github.com> wrote:\n\n> I think I may have found why it is complaining. However, I have no idea\n> how to fix it. While training, we all get the IteratorGetNext Error:\n> Sequence out of range.\n>\n> I noticed that let\u2019s say I have a dataset size of 60,000 with a batch size\n> of 64 that would require floor(60000/64)= 937 to iterate through entire\n> dataset for one epoch. However, when training using .fit(verbose=1) i\n> botice that it attempts to iterate through the dataset 938 (most likely a\n> rounding error because 60000/64=937.5) and thus I get this error. Can\n> someone please confirm this is the case for you as well? Thanks\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32817?email_source=notifications&email_token=ABW6MMFS2UIXVTALDOPPVSLQNOPYDA5CNFSM4I2PHNA2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEARYYMI#issuecomment-539200561>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABW6MMANBQ3NCFAKJLMWHH3QNOPYDANCNFSM4I2PHNAQ>\n> .\n>\n", "I'm experiencing a similar problem to @duysqubix with my code in that I have a number of samples that doesn't neatly divide by the batch size. @duysqubix code works for me and the error disappears if I repeat the dataset and specify `steps_per_epoch`.\r\n\r\n- I'm seeing this on Ubuntu 18.04, so definitely not a Windows only problem. \r\n- I see this issue with both the tensorflow 2 release and the tensorflow 2 RC2.\r\n\r\nTrying @mtpgva's advice above and using a `take` a number of samples that are divisible by the batch size I find that I still get the same message, even when using the simplified example provided by @duysqubix:\r\n\r\n```python\r\nimport tensorflow as tf\r\ndata = tf.random.normal((60000,30,4))\r\nground_truth = tf.ones((60000,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).take(512).batch(64)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Dense(1, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n#predefined model here: input: [?, 30,4] output: [?,1]\r\nmodel.fit(dataset, epochs=5)\r\n```\r\n\r\n```\r\nEpoch 1/5\r\n2019-10-08 09:01:00.212603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n      8/Unknown - 1s 84ms/step - loss: 102.0359 - accuracy: 1.00002019-10-08 09:01:00.443158: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_16]]\r\n2019-10-08 09:01:00.443241: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n8/8 [==============================] - 1s 85ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 2/5\r\n1/8 [==>...........................] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:01:00.502043: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Shape/_4]]\r\n2019-10-08 09:01:00.502100: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n8/8 [==============================] - 0s 7ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 3/5\r\n1/8 [==>...........................] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:01:00.544339: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_16]]\r\n2019-10-08 09:01:00.544373: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n8/8 [==============================] - 0s 5ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 4/5\r\n1/8 [==>...........................] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:01:00.587002: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_16]]\r\n2019-10-08 09:01:00.587044: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n8/8 [==============================] - 0s 5ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 5/5\r\n1/8 [==>...........................] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:01:00.631688: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Shape/_4]]\r\n2019-10-08 09:01:00.631740: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n8/8 [==============================] - 0s 6ms/step - loss: 102.0359 - accuracy: 1.0000\r\n```\r\nI also tried using the `drop_remainder=True` argument on `.batch` but still get the error message:\r\n\r\n```python\r\nimport tensorflow as tf\r\ndata = tf.random.normal((60000,30,4))\r\nground_truth = tf.ones((60000,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).batch(64, drop_remainder=True)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Dense(1, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n#predefined model here: input: [?, 30,4] output: [?,1]\r\nmodel.fit(dataset, epochs=5)\r\n\r\n```\r\n\r\n```\r\nEpoch 1/5\r\n2019-10-08 09:03:47.431058: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n    937/Unknown - 3s 3ms/step - loss: 102.0359 - accuracy: 1.00002019-10-08 09:03:50.275433: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_2]]\r\n2019-10-08 09:03:50.275587: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n937/937 [==============================] - 3s 3ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 2/5\r\n919/937 [============================>.] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:03:52.891814: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_2]]\r\n2019-10-08 09:03:52.891940: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n937/937 [==============================] - 3s 3ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 3/5\r\n931/937 [============================>.] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:03:55.506978: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_2]]\r\n2019-10-08 09:03:55.507100: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n937/937 [==============================] - 3s 3ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 4/5\r\n918/937 [============================>.] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:03:58.045499: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_2]]\r\n2019-10-08 09:03:58.045610: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n937/937 [==============================] - 3s 3ms/step - loss: 102.0359 - accuracy: 1.0000\r\nEpoch 5/5\r\n932/937 [============================>.] - ETA: 0s - loss: 102.0359 - accuracy: 1.00002019-10-08 09:04:00.654601: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_2]]\r\n2019-10-08 09:04:00.654715: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n937/937 [==============================] - 3s 3ms/step - loss: 102.0359 - accuracy: 1.0000\r\n```", "I think we may need to summon @fchollet", "@duysqubix  Your suggestion fixed my issue!", "@oracle3001 \r\n\r\nCan you please let us know if the issue still persists?.Please close the issue if it was resolved already. Thanks!", "Any update?", "> @oracle3001\r\n> \r\n> Can you please let us know if the issue still persists?.Please close the issue if it was resolved already. Thanks!\r\n\r\nThis fixes, my issue, however, surely the final batch size being reduced should not create an issue? \r\n\r\nRepeating the part of the first batch of data should not be the solution, surely?", "Hey @ravikyram,\r\n\r\nthe solution that @duysqubix postet is a **workaround**, the underlying problem **still exists**.\r\n\r\nThe problem is that the number of iterations performed on the dataset is greater than the number of batches in the dataset. I'm not actually sure that this is a bug, considering that python iteration uses the  `StopIteration` exception to mark endings of iterables as well. But if that's the case **the warning should not be displayed**.\r\n\r\nThe work around \"fixes\" this by giving an explicitly calculated number of iterations to the `model.fit` method. This should not be necessary and might not even be possible in all cases. For example when using bucketing, the exact number of batches cannot be easily extracted from the dataset (except by performing a full dataset iteration before training, which would be a workaround as well).\r\n\r\nSo either the behavior is correct and the warning should be hidden, or the internally calculated number of iterations is faulty and should be changed.", "> @oracle3001\r\n> \r\n> Can you please let us know if the issue still persists?.Please close the issue if it was resolved already. Thanks!\r\n\r\nYes, it still persists...see all the other posts with the same issue.", "Hi, I agree with @BeWe11, the issue is still there.\r\nMoreover if you are using a data.from_generator() function the number of actual steps must be computed on the first epoch.", "have the same issue on a centos7 (tf-2.0)\r\n\r\ndemo source:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__ == '__main__':\r\n    x = tf.random.normal((14000, 30, 1))\r\n    y = tf.ones_like(x)\r\n\r\n    discriminator = tf.keras.models.Sequential([\r\n        tf.keras.layers.LSTM(100, input_shape=(30, 1), return_sequences=True),\r\n        tf.keras.layers.LSTM(100, recurrent_dropout=0.4,\r\n                             dropout=0.4, return_sequences=True)\r\n    ])\r\n\r\n    discriminator.compile(loss='binary_crossentropy',\r\n                          optimizer=tf.keras.optimizers.Adam(lr=0.001))\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    dataset = dataset.batch(64)\r\n\r\n    discriminator.fit(dataset, epochs=2)\r\n```\r\n\r\nconsole:\r\n```\r\n2019-11-15 18:56:52.780799: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-15 18:56:52.787215: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494130000 Hz\r\n2019-11-15 18:56:52.788336: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ebe390 executing computations on platform Host. Devices:\r\n2019-11-15 18:56:52.788386: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nEpoch 1/2\r\n2019-11-15 18:56:55.992738: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_4693_5178' and '__inference___backward_standard_lstm_4693_5178_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_5271' both implement 'lstm_d3e9f06d-9768-4d2d-a545-12ffe5c3e735' but their signatures do not match.\r\n    219/Unknown - 20s 93ms/step - loss: 1.37292019-11-15 18:57:13.959804: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n219/219 [==============================] - 20s 93ms/step - loss: 1.3729\r\nEpoch 2/2\r\n218/219 [============================>.] - ETA: 0s - loss: 0.46322019-11-15 18:57:31.342946: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n219/219 [==============================] - 17s 79ms/step - loss: 0.4632\r\n```", "Same issue with a trivial example : \r\n\r\nTf 2.0.0\r\npython 3.6-64 bit\r\nwindows 10\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.array([1, 5, 8, 9 ,10, 15,13, 3,-2],np.float32)\r\ny = np.array([-2,-5, -7, -12 ,-15, -5, -12,-10,-5],np.float32)\r\ntaille_lot = 2\r\nnum_epochs = 3\r\ndataset = tf.data.Dataset.from_tensor_slices(( x , y ))\r\ndataset = dataset.shuffle(5000).batch(taille_lot)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='linear',input_shape=(1,)))\r\nmodel.layers[0].set_weights([np.array([[7.3]],np.float32),np.array([5.5],np.float32)])\r\nmodel.compile(optimizer='sgd', loss='mse')\r\nmodel.fit(dataset, epochs=num_epochs, verbose=0)\r\nprint(model.layers[0].get_weights())\r\n```\r\n\r\nResults : \r\n\r\n```\r\n2019-11-19 22:12:24.480601: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n2019-11-19 22:12:24.517314: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n2019-11-19 22:12:24.551855: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n[array([[-0.9485246]], dtype=float32), array([4.253337], dtype=float32)]\r\n>>>\r\n```", "I think An example is missing in Doc :\r\n\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef prepara_data(x, y, num_epochs, batch_size):\r\n    dataset = tf.data.Dataset.from_tensor_slices(( x , y ))\r\n    dataset = dataset.batch(batch_size, drop_remainder=True).repeat(num_epochs)\r\n    return dataset\r\n    \r\nbatch_size  = 2\r\nnum_epochs = 3\r\nx = np.array([1, 2, 3, 4 ,5, 6, 7, 8, 9],np.float32)\r\ny = 2 * x + 1\r\ndataset = prepara_data(x, y, num_epochs, batch_size)\r\niterateur =  dataset.__iter__()\r\n\r\nfor i in range(0,1000):\r\n    try:\r\n        x_batch , y_batch = iterateur.get_next()\r\n        print(\"iter \", i,'->',x_batch)\r\n    except:\r\n        print (\"Exception Iteration \", i,\" max = \",num_epochs * tf.data.experimental.cardinality(dataset).numpy() // batch_size )\r\n        break\r\ndataset = prepara_data(x, y, num_epochs, batch_size)\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='linear',input_shape=(1,)))\r\nmodel.layers[0].set_weights([np.array([[7.3]],np.float32),np.array([5.5],np.float32)])\r\nmodel.layers[0](np.array([[3]]))\r\nmodel.compile(optimizer='sgd', loss='mse')\r\nmodel.fit(dataset, epochs=num_epochs ,steps_per_epoch=x.shape[0] // batch_size, verbose=0)\r\nprint(model.layers[0].get_weights())\r\n```\r\n\r\nyou must set drop_remainder to True in batch method argument and set steps_per_epoch=x.shape[0] // batch_size in fit method\r\n\r\n@sharkdtu \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__ == '__main__':\r\n    x = tf.random.normal((14000, 30, 1))\r\n    y = tf.ones_like(x)\r\n\r\n    discriminator = tf.keras.models.Sequential([\r\n        tf.keras.layers.LSTM(100, input_shape=(30, 1), return_sequences=True),\r\n        tf.keras.layers.LSTM(100, recurrent_dropout=0.4,\r\n                             dropout=0.4, return_sequences=True)\r\n    ])\r\n\r\n    discriminator.compile(loss='binary_crossentropy',\r\n                          optimizer=tf.keras.optimizers.Adam(lr=0.001))\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    batch_size = 64\r\n    num_epoch = 2\r\n    dataset = dataset.batch(batch_size, drop_remainder=True).repeat(num_epoch)\r\n\r\n    discriminator.fit(dataset, epochs=num_epoch, steps_per_epoch=tf.data.experimental.cardinality(dataset).numpy() // batch_size)\r\n```", "@LaurentBerger thx\uff0c but i don't think it is a good idea for forcing users to set the `steps_per_epoch`.", "@LaurentBerger sometimes we even don't know the full size of the data.", "@sharkdtu I changed code and use [tf.data.experimental.cardinality](https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality)[ issue here](https://github.com/tensorflow/tensorflow/issues/26966)\r\n\r\n@npuichigo Why? (of course  I must shuffle data before fit but it's only an example)\r\n\r\n", "\r\n```\r\nkeras\u7684fit\u51fd\u6570\u7684\u90e8\u5206\u53c2\u6570\u5982\u4e0b\uff1a\r\nmodel.fit(self, x=None, y=None,epochs=1,steps_per_epoch=None)\r\n```\r\n- [epoch:\u8fed\u4ee3\u6b21\u6570\uff0c\u4e00\u6b21\u8fed\u4ee3\u53ef\u4ee5\u7c97\u7cd9\u7684\u7406\u89e3\u6210\u4f7f\u7528\u4e00\u4e2abatch\u7684\u8bad\u7ec3\u6570\u636e\u5bf9model\u8fdb\u884c\u8bad\u7ec3\u3002\r\nThe number of iterations, one iteration can be roughly understood as using a batch of training data to train the model.]\r\n- [steps_per_epoch\uff1a\u6bcf\u6b21\u8fed\u4ee3\u88abmodel\u6d88\u8d39\u7684batches\uff0c\u53ef\u4ee5\u7c97\u7cd9\u7684\u7406\u89e3\u4e3a\u5c06\u56fa\u5b9a\u6570\u91cf\u7684batch\u5408\u5e76\u6210\u4e00\u4e2a\u66f4\u5927\u7684bigger_batch\uff0c\u7136\u540e\u7528bigger_batch\u5bf9model\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u7ed3\u675f\u5373\u4e3a\u5b8c\u6210\u4e00\u4e2aepoch\u3002\r\nEach iteration of the batches consumed by the model. It can be roughly seen as a combination of a fixed number of batches into a bigger batch named bigger_batch, and then the model is trained with the bigger_batch. The end of the training is the completion of an epoch.] \r\n\r\n- \u4e2a\u4eba\u770b\u6cd5\uff1a\u8fd9\u662fmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06model\u7684\u8bad\u7ec3\u8fc7\u7a0b\u7406\u89e3\u4e3a\u751f\u4ea7\u8005\u4e0e\u6d88\u8d39\u8005\u7684\u5173\u7cfb\u3002tensorflow2.0\u7684dataset\u96c6\u6210\u4e86generator\u7684\u529f\u80fd\uff0c\u53ef\u76f4\u63a5\u4f5c\u4e3a\u5410\u51fa\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u5668.\r\ndataset\u4e0d\u65ad\u63d0\u4f9b\u6570\u636e\uff0cmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u65ad\u6d88\u8d39\u6570\u636e\uff0c\u4e00\u65e6dataset\u6ca1\u6709\u6570\u636e\u63d0\u4f9b\u5e76\u4e14model\u7684\u8bad\u7ec3\u8fc7\u7a0b\u8fd8\u6ca1\u7ed3\u675f\uff0c\u5c31\u4f1a\u62a5\u9519\uff0c\u6240\u4ee5\u9700\u8981\u786e\u4fddepochs*steps_per_epoch <= dataset\u6240\u80fd\u63d0\u4f9b\u7684batches\u3002\r\n\u4f60\u53ef\u4ee5\u6839\u636e\u7ecf\u9a8c\u786e\u5b9abatch_size\u548csteps_per_epoch\uff0c\u7136\u540e\u5bf9\u5168\u91cf\u6570\u636e\u96c6\u4f7f\u7528repeat()\u6765\u907f\u514dmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5982\u679c\u89c9\u5f97\u6ca1\u6709\u5fc5\u8981\u5bf9batch\u518d\u505a\u5904\u7406\uff0c\u53ef\u4ee4steps_per_epoch=1\u3002\r\n- My View:This is the problem of insufficient training data in the model training process, which can be seen as the relationship between producers and consumers.\r\nTensorflow2.0 Dataset integrates the function of generator and can be used to spit out training data directly.Dataset provides data continuously, and model training process consumes data continuously. Once dataset does not provide data and model training process is not finished, an error will be reported. Therefore, it is necessary to ensure that epochs*steps_per_epoch is less than the size of batches provided by dataset.You can determine batch_size and steps_per_epoch based on experience, and then use repeat() for Dataset to avoid data shortage during model training.If you don't think it's necessary to deal with the batch again, you can make steps_per_epoch = 1.\r\n", "> \r\n> \r\n> \u5c06\u5168\u91cf\u6570\u636e\u96c6\u5207\u5206\u6210\u591a\u4e2abatch\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u5206\u6279\u8fed\u4ee3\u8bad\u7ec3\uff0c\u662f\u5e38\u89c4\u505a\u6cd5\uff0c\u5148\u7406\u89e3\u4e00\u4e0b\u4e24\u4e2a\u6982\u5ff5\uff1a\r\n> \r\n>     * [batch_size\uff1a\u6bcf\u4e2abatch\u7684\u6570\u636e\u91cf\u5927\u5c0f]\r\n> \r\n>     * [batches\uff1a\u6574\u4e2a\u6570\u636e\u96c6\u6309\u7167batch_size\u5207\u5206\u540e\u7684batch\u6570\u91cf]\r\n> \r\n> \r\n> ```\r\n> keras\u7684fit\u51fd\u6570\u7684\u90e8\u5206\u53c2\u6570\u5982\u4e0b\uff1a\r\n> model.fit(self, x=None, y=None,epochs=1,steps_per_epoch=None)\r\n> ```\r\n> \r\n>     * [epoch:\u8fed\u4ee3\u6b21\u6570\uff0c\u4e00\u6b21\u8fed\u4ee3\u53ef\u4ee5\u7c97\u7cd9\u7684\u7406\u89e3\u6210\u4f7f\u7528\u4e00\u4e2abatch\u7684\u8bad\u7ec3\u6570\u636e\u5bf9model\u8fdb\u884c\u8bad\u7ec3\u3002\r\n>       The number of iterations, one iteration can be roughly understood as using a batch of training data to train the model.]\r\n> \r\n>     * [steps_per_epoch\uff1a\u6bcf\u6b21\u8fed\u4ee3\u88abmodel\u6d88\u8d39\u7684batches\uff0c\u53ef\u4ee5\u7c97\u7cd9\u7684\u7406\u89e3\u4e3a\u5c06\u56fa\u5b9a\u6570\u91cf\u7684batch\u5408\u5e76\u6210\u4e00\u4e2a\u66f4\u5927\u7684bigger_batch\uff0c\u7136\u540e\u7528bigger_batch\u5bf9model\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u7ed3\u675f\u5373\u4e3a\u5b8c\u6210\u4e00\u4e2aepoch\u3002\r\n>       Each iteration of the batches consumed by the model. It can be roughly seen as a combination of a fixed number of batches into a bigger batch named bigger_batch, and then the model is trained with the bigger_batch. The end of the training is the completion of an epoch.]\r\n> \r\n>     * \u4e2a\u4eba\u770b\u6cd5\uff1a\u8fd9\u662fmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06model\u7684\u8bad\u7ec3\u8fc7\u7a0b\u7406\u89e3\u4e3a\u751f\u4ea7\u8005\u4e0e\u6d88\u8d39\u8005\u7684\u5173\u7cfb\u3002tensorflow2.0\u7684dataset\u96c6\u6210\u4e86generator\u7684\u529f\u80fd\uff0c\u53ef\u76f4\u63a5\u4f5c\u4e3a\u5410\u51fa\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u5668.\r\n>       dataset\u4e0d\u65ad\u63d0\u4f9b\u6570\u636e\uff0cmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u65ad\u6d88\u8d39\u6570\u636e\uff0c\u4e00\u65e6dataset\u6ca1\u6709\u6570\u636e\u63d0\u4f9b\u5e76\u4e14model\u7684\u8bad\u7ec3\u8fc7\u7a0b\u8fd8\u6ca1\u7ed3\u675f\uff0c\u5c31\u4f1a\u62a5\u9519\uff0c\u6240\u4ee5\u9700\u8981\u786e\u4fddepochs*steps_per_epoch <= dataset\u6240\u80fd\u63d0\u4f9b\u7684batches\u3002\r\n>       \u4f60\u53ef\u4ee5\u6839\u636e\u7ecf\u9a8c\u786e\u5b9abatch_size\u548csteps_per_epoch\uff0c\u7136\u540e\u5bf9\u5168\u91cf\u6570\u636e\u96c6\u4f7f\u7528repeat()\u6765\u907f\u514dmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5982\u679c\u89c9\u5f97\u6ca1\u6709\u5fc5\u8981\u5bf9batch\u518d\u505a\u5904\u7406\uff0c\u53ef\u4ee4steps_per_epoch=1\u3002\r\n> \r\n>     * My View:This is the problem of insufficient training data in the model training process, which can be seen as the relationship between producers and consumers.\r\n>       Tensorflow2.0 Dataset integrates the function of generator and can be used to spit out training data directly.Dataset provides data continuously, and model training process consumes data continuously. Once dataset does not provide data and model training process is not finished, an error will be reported. Therefore, it is necessary to ensure that epochs*steps_per_epoch is less than the size of batches provided by dataset.You can determine batch_size and steps_per_epoch based on experience, and then use repeat() for Dataset to avoid data shortage during model training.If you don't think it's necessary to deal with the batch again, you can make steps_per_epoch = 1.\r\n> \r\n> \r\n> \u9a8c\u8bc1\u8fc7\u7a0b\uff1a\r\n> \r\n> ```\r\n> train_data = tf.random.normal((5,4))#5\u4e2a4\u7ef4\u7279\u5f81\u5411\u91cf\r\n> label = tf.ones((5,1))#5\u4e2a\u7c7b\u522b\u6807\u7b7e\r\n> dataset = tf.data.Dataset.from_tensor_slices((data, label))\r\n> ```\r\n> \r\n> ```\r\n> dataset\r\n> <TensorSliceDataset shapes: ((4,), (1,)), types: (tf.float32, tf.float32)>\r\n> ```\r\n> \r\n> \u5168\u91cf\u6570\u636e\u96c6dataset\u6309\u7167batch_size\u8fdb\u884c\u5207\u5206\uff0c\u5982\u679c\u6700\u540e\u4e00\u4e2abatch\u7684\u6570\u91cf\u4e0d\u8db3batch_size\uff0c\u6839\u636edrop_remainder\u5224\u65ad\u662f\u5426\u5c06\u5176\u4e22\u5f03\u3002\u5728\u4e0a\u8ff0\u4f8b\u5b50\u4e2d\uff0ctrain_data\u548clabel\u6784\u6210\u7684Dataset\u5305\u542b5\u4e2a\u7528\u6765\u8bad\u7ec3model\u7684\u5f20\u91cf\uff0c\u6682\u4e14\u79f0\u4e4b\u4e3atrain\u5f20\u91cf\uff0ctrain\u5f20\u91cf\u53c8\u5305\u542b2\u4e2a\u5f20\u91cf\uff1a1\u4e2a4\u7ef4\u7279\u5f81\u5411\u91cf\u548c1\u4e2alabel\u3002\r\n> \r\n> `dataset = dataset.batch(batch_size, drop_remainder=True).repeat(2)`\r\n> \r\n> ```\r\n> dataset\r\n> <RepeatDataset shapes: ((2, 4), (2, 1)), types: (tf.float32, tf.float32)>\r\n> ```\r\n> \r\n> \u8c03\u7528batch()\u8fdb\u884c\u5207\u5206\uff0cbatch_size=2\uff0cdrop_remainder=True\uff0c\u53ef\u77e5batches==2\uff0c\u6bcf\u4e2abatch\u5305\u542b2\u4e2atrain\u5f20\u91cf\uff0c\u6700\u540e\u4e00\u4e2abatch\u7684\u5927\u5c0f\u4e3a1\uff0c\u4e22\u5f03\uff1brepeat(2)\u540ebatches==4\u3002\r\n> \r\n> ```\r\n> model.fit(dataset, epochs=4, steps_per_epoch=1)\r\n> #fit\u51fd\u6570\u4e2d\u7684x\u548cy\u53c2\u6570\u4ee3\u8868\u7279\u5f81\u5411\u91cf\u548c\u7c7b\u522b\uff0c\u53ef\u76f4\u63a5\u7528Dataset\u7c7b\u578b\u7684\u53d8\u91cf\u8d4b\u503c\r\n> ```\r\n> \r\n> dataset\u67094\u4e2abatch\uff0cbatch_size == 2,\u6bcf\u6b21\u4f7f\u75281\u4e2abatch\u7684\u6570\u636e\u8bad\u7ec3model(bigger_batch_size == batch_size x 1 == 2)\uff0c\u53ef\u4ee5\u8fed\u4ee34\u6b21\r\n> \r\n> ```\r\n> model.fit(dataset, epochs=1, steps_per_epoch=4)\r\n> ```\r\n> \r\n> dataset\u67094\u4e2abatch\uff0cbatch_size == 2,\u6bcf\u6b21\u4f7f\u75284\u4e2abatch\u7684\u6570\u636e\u8bad\u7ec3model(bigger_batch_size == batch_size x 4 == 6)\uff0c\u53ef\u4ee5\u8fed\u4ee31\u6b21\r\n> \r\n> ```\r\n> \u5b8c\u6574\u7684\u9a8c\u8bc1\u4ee3\u7801\u5982\u4e0b\uff1a\r\n> import tensorflow as tf\r\n> tf.__version__\r\n> \r\n> def build_model():\r\n>     model = tf.keras.Sequential([\r\n>         tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  \r\n>         tf.keras.layers.Dense(10, activation=tf.nn.relu),\r\n>         tf.keras.layers.Dense(3, activation='softmax')])\r\n>     model.compile(\r\n>         optimizer='Adam',\r\n>         loss='categorical_crossentropy',\r\n>         metrics=['accuracy']\r\n>     )\r\n>     return model\r\n> \r\n> def check_data_batch_size(dataset):\r\n>     #iterator = iter(dataset)\r\n>     iterator = dataset.__iter__()\r\n>     i=0\r\n>     try:\r\n>         while i<100:\r\n>             #data = next(iterator)\r\n>             data = iterator.get_next()\r\n>             i += 1\r\n>             print('id:',i)\r\n>             print('data:',data)\r\n>     except Exception as e:\r\n>         print(repr(e))\r\n>     return i\r\n> \r\n> batch_size =  2\r\n> data = tf.random.normal((5,4))\r\n> label = tf.ones((5,1))\r\n> dataset = tf.data.Dataset.from_tensor_slices((data, label))\r\n> dataset = dataset.batch(2, drop_remainder=True).repeat(2)\r\n> batches = check_data_batch_size(dataset)\r\n> print('batches:',batches)\r\n> model = build_model()\r\n> model.fit(dataset, epochs=2, steps_per_epoch=2)\r\n> ```\r\n\r\nIs this reply related to the original question?", "I just mentioned my understanding of the two parameters of epochs and steps_per_epoch in keras model, why they cause errors, and my handling method. I'm sorry to confuse you.\r\n\r\n \r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: \"Zhu, Lingchen\"<notifications@github.com&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2019\u5e7412\u670812\u65e5(\u661f\u671f\u56db) \u665a\u4e0a10:52\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;; \r\n\u6284\u9001: \"326437990\"<326437990@qq.com&gt;; \"Comment\"<comment@noreply.github.com&gt;; \r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Re-emerged Issue #31509 - BaseCollectiveExecutor::StartAbort Out of range: (#32817)\r\n\r\n\r\n\r\n \r\n\u5c06\u5168\u91cf\u6570\u636e\u96c6\u5207\u5206\u6210\u591a\u4e2abatch\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u5206\u6279\u8fed\u4ee3\u8bad\u7ec3\uff0c\u662f\u5e38\u89c4\u505a\u6cd5\uff0c\u5148\u7406\u89e3\u4e00\u4e0b\u4e24\u4e2a\u6982\u5ff5\uff1a\r\n * [batch_size\uff1a\u6bcf\u4e2abatch\u7684\u6570\u636e\u91cf\u5927\u5c0f] * [batches\uff1a\u6574\u4e2a\u6570\u636e\u96c6\u6309\u7167batch_size\u5207\u5206\u540e\u7684batch\u6570\u91cf]  keras\u7684fit\u51fd\u6570\u7684\u90e8\u5206\u53c2\u6570\u5982\u4e0b\uff1a model.fit(self, x=None, y=None,epochs=1,steps_per_epoch=None)  * [epoch:\u8fed\u4ee3\u6b21\u6570\uff0c\u4e00\u6b21\u8fed\u4ee3\u53ef\u4ee5\u7c97\u7cd9\u7684\u7406\u89e3\u6210\u4f7f\u7528\u4e00\u4e2abatch\u7684\u8bad\u7ec3\u6570\u636e\u5bf9model\u8fdb\u884c\u8bad\u7ec3\u3002   The number of iterations, one iteration can be roughly understood as using a batch of training data to train the model.] * [steps_per_epoch\uff1a\u6bcf\u6b21\u8fed\u4ee3\u88abmodel\u6d88\u8d39\u7684batches\uff0c\u53ef\u4ee5\u7c97\u7cd9\u7684\u7406\u89e3\u4e3a\u5c06\u56fa\u5b9a\u6570\u91cf\u7684batch\u5408\u5e76\u6210\u4e00\u4e2a\u66f4\u5927\u7684bigger_batch\uff0c\u7136\u540e\u7528bigger_batch\u5bf9model\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u7ed3\u675f\u5373\u4e3a\u5b8c\u6210\u4e00\u4e2aepoch\u3002   Each iteration of the batches consumed by the model. It can be roughly seen as a combination of a fixed number of batches into a bigger batch named bigger_batch, and then the model is trained with the bigger_batch. The end of the training is the completion of an epoch.] * \u4e2a\u4eba\u770b\u6cd5\uff1a\u8fd9\u662fmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06model\u7684\u8bad\u7ec3\u8fc7\u7a0b\u7406\u89e3\u4e3a\u751f\u4ea7\u8005\u4e0e\u6d88\u8d39\u8005\u7684\u5173\u7cfb\u3002tensorflow2.0\u7684dataset\u96c6\u6210\u4e86generator\u7684\u529f\u80fd\uff0c\u53ef\u76f4\u63a5\u4f5c\u4e3a\u5410\u51fa\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u5668.   dataset\u4e0d\u65ad\u63d0\u4f9b\u6570\u636e\uff0cmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u65ad\u6d88\u8d39\u6570\u636e\uff0c\u4e00\u65e6dataset\u6ca1\u6709\u6570\u636e\u63d0\u4f9b\u5e76\u4e14model\u7684\u8bad\u7ec3\u8fc7\u7a0b\u8fd8\u6ca1\u7ed3\u675f\uff0c\u5c31\u4f1a\u62a5\u9519\uff0c\u6240\u4ee5\u9700\u8981\u786e\u4fddepochs*steps_per_epoch <= dataset\u6240\u80fd\u63d0\u4f9b\u7684batches\u3002   \u4f60\u53ef\u4ee5\u6839\u636e\u7ecf\u9a8c\u786e\u5b9abatch_size\u548csteps_per_epoch\uff0c\u7136\u540e\u5bf9\u5168\u91cf\u6570\u636e\u96c6\u4f7f\u7528repeat()\u6765\u907f\u514dmodel\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5982\u679c\u89c9\u5f97\u6ca1\u6709\u5fc5\u8981\u5bf9batch\u518d\u505a\u5904\u7406\uff0c\u53ef\u4ee4steps_per_epoch=1\u3002 * My View:This is the problem of insufficient training data in the model training process, which can be seen as the relationship between producers and consumers.   Tensorflow2.0 Dataset integrates the function of generator and can be used to spit out training data directly.Dataset provides data continuously, and model training process consumes data continuously. Once dataset does not provide data and model training process is not finished, an error will be reported. Therefore, it is necessary to ensure that epochs*steps_per_epoch is less than the size of batches provided by dataset.You can determine batch_size and steps_per_epoch based on experience, and then use repeat() for Dataset to avoid data shortage during model training.If you don't think it's necessary to deal with the batch again, you can make steps_per_epoch = 1.  \r\n\u9a8c\u8bc1\u8fc7\u7a0b\uff1a\r\n train_data = tf.random.normal((5,4))#5\u4e2a4\u7ef4\u7279\u5f81\u5411\u91cf label = tf.ones((5,1))#5\u4e2a\u7c7b\u522b\u6807\u7b7e dataset = tf.data.Dataset.from_tensor_slices((data, label))  dataset <TensorSliceDataset shapes: ((4,), (1,)), types: (tf.float32, tf.float32)&gt;  \r\n\u5168\u91cf\u6570\u636e\u96c6dataset\u6309\u7167batch_size\u8fdb\u884c\u5207\u5206\uff0c\u5982\u679c\u6700\u540e\u4e00\u4e2abatch\u7684\u6570\u91cf\u4e0d\u8db3batch_size\uff0c\u6839\u636edrop_remainder\u5224\u65ad\u662f\u5426\u5c06\u5176\u4e22\u5f03\u3002\u5728\u4e0a\u8ff0\u4f8b\u5b50\u4e2d\uff0ctrain_data\u548clabel\u6784\u6210\u7684Dataset\u5305\u542b5\u4e2a\u7528\u6765\u8bad\u7ec3model\u7684\u5f20\u91cf\uff0c\u6682\u4e14\u79f0\u4e4b\u4e3atrain\u5f20\u91cf\uff0ctrain\u5f20\u91cf\u53c8\u5305\u542b2\u4e2a\u5f20\u91cf\uff1a1\u4e2a4\u7ef4\u7279\u5f81\u5411\u91cf\u548c1\u4e2alabel\u3002\r\n \r\ndataset = dataset.batch(batch_size, drop_remainder=True).repeat(2)\r\n dataset <RepeatDataset shapes: ((2, 4), (2, 1)), types: (tf.float32, tf.float32)&gt;  \r\n\u8c03\u7528batch()\u8fdb\u884c\u5207\u5206\uff0cbatch_size=2\uff0cdrop_remainder=True\uff0c\u53ef\u77e5batches==2\uff0c\u6bcf\u4e2abatch\u5305\u542b2\u4e2atrain\u5f20\u91cf\uff0c\u6700\u540e\u4e00\u4e2abatch\u7684\u5927\u5c0f\u4e3a1\uff0c\u4e22\u5f03\uff1brepeat(2)\u540ebatches==4\u3002\r\n model.fit(dataset, epochs=4, steps_per_epoch=1) #fit\u51fd\u6570\u4e2d\u7684x\u548cy\u53c2\u6570\u4ee3\u8868\u7279\u5f81\u5411\u91cf\u548c\u7c7b\u522b\uff0c\u53ef\u76f4\u63a5\u7528Dataset\u7c7b\u578b\u7684\u53d8\u91cf\u8d4b\u503c  \r\ndataset\u67094\u4e2abatch\uff0cbatch_size == 2,\u6bcf\u6b21\u4f7f\u75281\u4e2abatch\u7684\u6570\u636e\u8bad\u7ec3model(bigger_batch_size == batch_size x 1 == 2)\uff0c\u53ef\u4ee5\u8fed\u4ee34\u6b21\r\n model.fit(dataset, epochs=1, steps_per_epoch=4)  \r\ndataset\u67094\u4e2abatch\uff0cbatch_size == 2,\u6bcf\u6b21\u4f7f\u75284\u4e2abatch\u7684\u6570\u636e\u8bad\u7ec3model(bigger_batch_size == batch_size x 4 == 6)\uff0c\u53ef\u4ee5\u8fed\u4ee31\u6b21\r\n \u5b8c\u6574\u7684\u9a8c\u8bc1\u4ee3\u7801\u5982\u4e0b\uff1a import tensorflow as tf tf.__version__ def build_model():     model = tf.keras.Sequential([         tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),           tf.keras.layers.Dense(10, activation=tf.nn.relu),         tf.keras.layers.Dense(3, activation='softmax')])     model.compile(         optimizer='Adam',         loss='categorical_crossentropy',         metrics=['accuracy']     )     return model def check_data_batch_size(dataset):     #iterator = iter(dataset)     iterator = dataset.__iter__()     i=0     try:         while i<100:             #data = next(iterator)             data = iterator.get_next()             i += 1             print('id:',i)             print('data:',data)     except Exception as e:         print(repr(e))     return i batch_size =  2 data = tf.random.normal((5,4)) label = tf.ones((5,1)) dataset = tf.data.Dataset.from_tensor_slices((data, label)) dataset = dataset.batch(2, drop_remainder=True).repeat(2) batches = check_data_batch_size(dataset) print('batches:',batches) model = build_model() model.fit(dataset, epochs=2, steps_per_epoch=2)   \r\nIs this reply related to the original question?\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@duysqubix Brilliant, your suggestion fixed my issue.\r\n\r\nAnd for other people facing this issue, just for the record:\r\n```python\r\n# loss, acc = net.evaluate(tst_set)  # do not use this when using a Repeating dataset\r\nloss, acc = net.evaluate(tst_set, steps=3)  # e.g., 3\r\n```", "I got the same problems in the Tensflow 2-gpu in the centos. Has anyone know how to fix this problem?", "This issue should be fixed by the pull request for issue https://github.com/tensorflow/tensorflow/issues/35314 . The warning was actually propagated up from C++ and so python was passing it forward. But there is really no problem here, no issues with training or anything, according to the issue. \r\n\r\nThe solution was that Google lowered the logging level to ignore these warnings. The change is in TF 2.0 nightly, and will be widely available in the next release. But you can use TF nightly to get the benefit now. \r\n\r\nSo this issue can probably be closed. ", "Tensorflow 2.1(stable) released, has anyone known if this warning fixed in the new version ?", "I have the same problem when practicing the code from official tutorial.\r\nI am using Catalina 10.15 python 3.76 TF 2.1.0\r\n\r\n```\r\nmbedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\r\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\r\n                           dtype=tf.string, trainable=True)\r\nhub_layer(train_examples_batch[:3])\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(hub_layer)\r\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_data.shuffle(10000).batch(512),\r\n                    epochs=20,\r\n                    validation_data=validation_data.batch(512),\r\n                    verbose=1)\r\n\r\nresults = model.evaluate(test_data.batch(512), verbose=2)\r\nfor name, value in zip(model.metrics_names, results):\r\n  print(\"%s: %.3f\" % (name, value))\r\n```\r\n\r\n```\r\n29/30 [============================>.] - ETA: 0s - loss: 0.2012 - accuracy: 0.92892020-01-13 13:53:00.393082: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n```\r\n", "@Xiaohui-Z I can also confirm that the issue is not solved. Using the example code from the TF docs still produces the issue:\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\n\r\n\r\ndef make_datasets_unbatched():\r\n    # Scaling MNIST data from (0, 255] to (0., 1.]\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n        return image, label\r\n\r\n    datasets, info = tfds.load(name='mnist',\r\n                               with_info=True,\r\n                               as_supervised=True)\r\n\r\n    return datasets['train'].map(scale).cache().shuffle(10000)\r\n\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32,\r\n                               3,\r\n                               activation='relu',\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\n\r\ntrain_datasets = make_datasets_unbatched().batch(64)\r\nmodel = build_and_compile_cnn_model()\r\n\r\nmodel.fit(x=train_datasets, epochs=2)\r\n```\r\n\r\nI noted that this only happens during the first iteration where the total count seems to be unknown. This is odd too because the `numExamples` in the `statistics` key of the dataset_info.json is set correctly.", "Can also confirm that the error (warning) is still being raised on 2.1 (my docker base image is cuda:10.1-cudnn7-devel-ubuntu18.04).\r\n\r\n```python\r\n# Triggers the warning.\r\ndataset = raw_dataset.map(_parse_proto).take(32).batch(8)  \r\nmodel.evaluate(dataset)\r\n\r\n# No warning\r\ndataset = raw_dataset.map(_parse_proto).take(32).batch(8)  \r\nmodel.evaluate(dataset, steps=4)\r\n```", "I also have this warning in TF2.1.0. `model.predict(ds.batch(1))` works but gives this warning :\r\n```\r\n2020-03-11 17:04:24.760612: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n```", "I have a similar error but can't seem to find anywhere else where anyone else is experiencing here it, and here is the traceback of my error:\r\n\r\n\r\nTrain on 2737611 samples, validate on 2737612 samples\r\nEpoch 1/123\r\nEpoch 2/123\r\nEpoch 3/123\r\nEpoch 4/123\r\nEpoch 5/123\r\nEpoch 6/123\r\n2020-08-20 22:56:33.810266: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_10/Sigmoid:0) = ] [[nan][nan][nan]...] [y (metrics/tp/Cast_2/x:0) = ] [0]\r\n         [[{{node metrics/tp/assert_greater_equal/Assert/AssertGuard/else/_1/Assert}}]]\r\n         [[metrics/recall/assert_greater_equal/Assert/AssertGuard/pivot_f/_143/_157]]\r\n2020-08-20 22:56:33.824745: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_10/Sigmoid:0) = ] [[nan][nan][nan]...] [y (metrics/tp/Cast_2/x:0) = ] [0]\r\n         [[{{node metrics/tp/assert_greater_equal/Assert/AssertGuard/else/_1/Assert}}]]\r\nWARNING:tensorflow:Can save best model only with val_precision available, skipping.\r\nTraceback (most recent call last):\r\n  File \"tf_working.py\", line 399, in <module>\r\n    keras_auto_tuner(training_df, '1week_target_class')\r\n  File \"tf_working.py\", line 382, in keras_auto_tuner\r\n    validation_data=(val_features, y_val))\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\", line 130, in search\r\n    self.run_trial(trial, *fit_args, **fit_kwargs)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\", line 96, in run_trial\r\n    history = model.fit(*fit_args, **copied_fit_kwargs)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\evoot\\anaconda3\\envs\\tf_sh\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_10/Sigmoid:0) = ] [[nan][nan][nan]...] [y (metrics/tp/Cast_2/x:0) = ] [0]\r\n         [[{{node metrics/tp/assert_greater_equal/Assert/AssertGuard/else/_1/Assert}}]]\r\n         [[metrics/recall/assert_greater_equal/Assert/AssertGuard/pivot_f/_143/_157]]\r\n  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_10/Sigmoid:0) = ] [[nan][nan][nan]...] [y (metrics/tp/Cast_2/x:0) = ] [0]\r\n         [[{{node metrics/tp/assert_greater_equal/Assert/AssertGuard/else/_1/Assert}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_222355]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function", "Can anyone confirm what the result of this behavior is? I'm confused whether its a logging error, or whether the final batch does not get train/evaluated. For example, imagine I had 100 samples with a batch size of 52. Would I be training on a batch of 50 and 48 (expected behavior), or would I train on 50 and then just fail to fill the next batch and move to the next epoch? This is especially scary in a validation batch and I would be terrified to find that I have a variable validation set (especially if you shuffle!). There is alot of discussion in many spots, but no clear indication of the significance of this error. Some would have you believe it is just a warning. I am on tensorflow==2.1.0.", "> \u6211\u60f3\u6211\u53ef\u80fd\u5df2\u7ecf\u627e\u5230\u4e86\u4e3a\u4ec0\u4e48\u8981\u62b1\u6028\u3002\u4f46\u662f\uff0c\u6211\u4e0d\u77e5\u9053\u5982\u4f55\u89e3\u51b3\u5b83\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u90fd\u4f1a\u6536\u5230IteratorGetNext\u9519\u8bef\uff1a\u5e8f\u5217\u8d85\u51fa\u8303\u56f4\u3002\r\n> \r\n> \u6211\u6ce8\u610f\u5230\uff0c\u5047\u8bbe\u6211\u7684\u6570\u636e\u96c6\u5927\u5c0f\u4e3a60,000\uff0c\u800c\u6279\u5904\u7406\u5927\u5c0f\u4e3a64\uff0c\u5219\u9700\u8981floor\uff0860000/64\uff09= 937\u624d\u80fd\u904d\u5386\u6574\u4e2a\u6570\u636e\u96c6\u4e00\u4e2a\u65f6\u671f\u3002\u4f46\u662f\uff0c\u5f53\u4f7f\u7528.fit\uff08verbose = 1\uff09\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u6211\u6ce8\u610f\u5230\u5b83\u5c1d\u8bd5\u904d\u5386\u6570\u636e\u96c6938\uff08\u5f88\u53ef\u80fd\u662f\u820d\u5165\u9519\u8bef\uff0c\u56e0\u4e3a60000/64 = 937.5\uff09\uff0c\u56e0\u6b64\u6211\u5f97\u5230\u4e86\u8fd9\u4e2a\u9519\u8bef\u3002\u6709\u4eba\u53ef\u4ee5\u8bf7\u60a8\u786e\u8ba4\u8fd9\u79cd\u60c5\u51b5\u5417\uff1f\u8c22\u8c22\r\n> \r\n> \u7f16\u8f91\uff1a\r\n> \r\n> \u56e0\u6b64\uff0c\u5728\u6784\u5efatf.data.Dataset\u65f6\uff0c\u6211\u627e\u5230\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6cd5\uff0c\u8bf7\u786e\u4fdd\u6dfb\u52a0.repeat\uff08\uff09\u65b9\u6cd5\uff0c\u56e0\u4e3a\u7a0b\u5e8f\u4f1a\u62b1\u6028\u60a8\u7528\u5b8c\u4e86\u6570\u636e\uff0c\u5e76\u4e14\u5728\u4f7f\u7528.fit\uff08\uff09\u65f6 ~\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9~\uff1a\r\n> \r\n> \u8fd9\u662f\u4e00\u4e2a\u5b8c\u6574\u7684\u793a\u4f8b\uff0c\u53ef\u4ee5\u6b63\u5e38\u5de5\u4f5c\u3002\r\n> \r\n> \u8fd9\u5c06\u5bfc\u81f4\u9519\u8bef\uff1a\r\n> \r\n> ```python\r\n> \u6570\u636e =  TF\u3002\u968f\u673a\u7684\u3002\u6b63\u5e38\uff08\uff0860000\uff0c30\uff0c4\uff09\uff09\r\n>  ground_truth  =  TF\u3002\u90a3\u4e9b\uff08\uff0860000\uff0c1\uff09\uff09\r\n> \u7684\u6570\u636e\u96c6 =  TF\u3002\u6570\u636e\u3002\u6570\u636e\u96c6\u3002from_tensor_slices\uff08\uff08data\uff0cground_truth\uff09\uff09\u3002\u6279\uff0864\uff09\r\n> \r\n> #predefined\u6a21\u578b\u5728\u8fd9\u91cc\uff1a\u8f93\u5165\uff1a[\uff1f\uff0c30,4]\u8f93\u51fa\uff1a[\uff1f\uff0c1]\r\n> \u6a21\u578b\u3002\u9002\u5408\uff08\u8d44\u6599\u96c6\uff0c\u5386\u5143= 5\uff09\r\n> \r\n> ''' \r\n>     938 /\u672a\u77e5-16\u79d217\u6beb\u79d2/\u6b65-\u4e22\u5931\uff1a0.02172019-10-07 14\uff1a49\uff1a49.928619\uff1aW tensorflow / core / common_runtime / base_collective_executor.cc\uff1a216] BaseCollectiveExecutor :: StartAbort\u8d85\u51fa\u8303\u56f4\uff1a\u5e8f\u5217\u7ed3\u675f\r\n>          [ [{{node IteratorGetNext}}]] \r\n>          [[Shape / _2]] \r\n> 2019-10-07 14\uff1a49\uff1a49.928619\uff1aW tensorflow / core / common_runtime / base_collective_executor.cc\uff1a216] BaseCollectiveExecutor :: StartAbort\u8d85\u51fa\u8303\u56f4\uff1a\u7ed3\u675f\u4e8e\u5e8f\u5217\r\n>          [[{{node IteratorGetNext}}]] \r\n> 938/938 [=============================]-16s 17ms /\u6b65\u8fdb-\u4e8f\u635f\uff1a0.0217\r\n> \u65f6\u4ee32/5\r\n> 935/938 [===========================>\u3002]-ETA\uff1a0\u79d2-\u635f\u5931\uff1a2.2229e-062019-10-07 14\uff1a49\uff1a59.722216\uff1aW tensorflow / core / common_runtime / base_collective_executor.cc\uff1a216] BaseCollectiveExecutor :: StartAbort\u8d85\u51fa\u8303\u56f4\uff1a\u5e8f\u5217\u7ed3\u675f\r\n>          [[{{node IteratorGetNext}}]] \r\n> 2019-10-07 14\uff1a49\uff1a59.722218 \uff1aW tensorflow / core / common_runtime / base_collective_executor.cc\uff1a216] BaseCollectiveExecutor :: StartAbort\u8d85\u51fa\u8303\u56f4\uff1a\u5e8f\u5217\u7ed3\u675f\r\n>          [[{{node IteratorGetNext}}] \r\n>          [[Shape / _2]] \r\n> '''\r\n> ```\r\n> \r\n> \u8fd9\u662f\u53d8\u901a\u529e\u6cd5\u3002\r\n> \r\n> ```python\r\n> batch_size  =  64\r\n> \u6570\u636e =  tf\u3002\u968f\u673a\u7684\u3002\u6b63\u5e38\uff08\uff0860000\uff0c30\uff0c4\uff09\uff09\r\n>  ground_truth  =  TF\u3002\u90a3\u4e9b\uff08\uff0860000\uff0c1\uff09\uff09\r\n> \u7684\u6570\u636e\u96c6 =  TF\u3002\u6570\u636e\u3002\u6570\u636e\u96c6\u3002from_tensor_slices\uff08\uff08data\uff0cground_truth\uff09\uff09\u3002\u6279\u5904\u7406\uff08batch_size\uff09\u3002\u91cd\u590d\uff08\uff09\r\n> \r\n> #predefined\u6a21\u578b\u5728\u8fd9\u91cc\uff1a\u8f93\u5165\uff1a[\uff1f\uff0c30,4]\u8f93\u51fa\uff1a[\uff1f\uff0c1]\r\n> \u6a21\u578b\u3002\u914d\u5408\uff08\u6570\u636e\u96c6\uff0c\u5386\u5143= 5\uff0csteps_per_epoch =\u6570\u636e\u3002\u5f62\u72b6[ 0 ] //\u7684batch_size\uff09\r\n> \r\n> ''' \r\n> 937/937 [=============================]-15s 16ms / step-\u635f\u5931\uff1a0.0135 \r\n> Epoch 2 / 5 \r\n> 937/937 [==============================]-10s 10ms / step-\u635f\u8017\uff1a1.4460e-05\r\n> \u65f6\u4ee33 /\r\n> 937/937 [=============================]-10s 11ms / step-\u635f\u5931\uff1a4.3097e-06\r\n> \u65f6\u671f4/5 \r\n> 937/937 [=============================]-10s 10ms / step-\u635f\u8017\uff1a1.8212e-06\r\n> \u65f6\u4ee35/5 \r\n> '''\r\n> \r\n> ``\r\n> ```\r\n\r\n\r\n\r\n> I think I may have found why it is complaining. However, I have no idea how to fix it. While training, we all get the IteratorGetNext Error: Sequence out of range.\r\n> \r\n> I noticed that let\u2019s say I have a dataset size of 60,000 with a batch size of 64 that would require floor(60000/64)= 937 to iterate through the entire dataset for one epoch. However, when training using .fit(verbose=1) I noticed that it attempts to iterate through the dataset 938 (most likely a rounding error because 60000/64=937.5) and thus I get this error. Can someone please confirm this is the case for you as well? Thanks\r\n> \r\n> Edit:\r\n> \r\n> So I found a way around this when building the tf.data.Dataset, make sure to add the .repeat() method because the program will complain that you ran out of data, and when using .fit() ~add the following~:\r\n> \r\n> Here is a full example that got it working.\r\n> \r\n> This will cause the error:\r\n> \r\n> ```python\r\n> data = tf.random.normal((60000,30,4))\r\n> ground_truth = tf.ones((60000,1))\r\n> dataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).batch(64)\r\n> \r\n> #predefined model here: input: [?, 30,4] output: [?,1]\r\n> model.fit(dataset, epochs=5)\r\n> \r\n> '''\r\n>     938/Unknown - 16s 17ms/step - loss: 0.02172019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n>          [[{{node IteratorGetNext}}]]\r\n>          [[Shape/_2]]\r\n> 2019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n>          [[{{node IteratorGetNext}}]]\r\n> 938/938 [==============================] - 16s 17ms/step - loss: 0.0217\r\n> Epoch 2/5\r\n> 935/938 [============================>.] - ETA: 0s - loss: 2.2229e-062019-10-07 14:49:59.722216: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n>          [[{{node IteratorGetNext}}]]\r\n> 2019-10-07 14:49:59.722218: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n>          [[{{node IteratorGetNext}}]]\r\n>          [[Shape/_2]]\r\n> '''\r\n> ```\r\n> \r\n> This is the work around.\r\n> \r\n> ```python\r\n> batch_size = 64\r\n> data = tf.random.normal((60000,30,4))\r\n> ground_truth = tf.ones((60000,1))\r\n> dataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).batch(batch_size).repeat()\r\n> \r\n> #predefined model here: input: [?, 30,4] output: [?,1]\r\n> model.fit(dataset, epochs=5, steps_per_epoch=data.shape[0]//batch_size)\r\n> \r\n> '''\r\n> 937/937 [==============================] - 15s 16ms/step - loss: 0.0135\r\n> Epoch 2/5\r\n> 937/937 [==============================] - 10s 10ms/step - loss: 1.4460e-05\r\n> Epoch 3/5\r\n> 937/937 [==============================] - 10s 11ms/step - loss: 4.3097e-06\r\n> Epoch 4/5\r\n> 937/937 [==============================] - 10s 10ms/step - loss: 1.8212e-06\r\n> Epoch 5/5\r\n> '''\r\n> \r\n> ``\r\n> ```\r\n\r\nhi\uff0cI am a freshman in DL from CHina ,I meet the same error like you.Through search the interent,I  found the answer:you need add the 'repeat()',but rember that don't input function parameter,then you need to add \"step_per_epoch\" in fit(),and it's value is \"x_train//batchszie''.it works in my project,I hope it can help you to solve your problem .my English is poor,don't mind!", "I tried to run on Colab with TF v2.5 and faced  a different error,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/116fd9d5f99e22cf0fb927b0bc2ce936/untitled307.ipynb#scrollTo=M9SHFU8JyT61)..Thanks!", "@oracle3001,\r\n\r\nI've tried reproducing the issue in `TF 2.6.0` and its working fine now. Please take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/fd0785c3eebe142319b89ba4b52b111f/32817ipynb.ipynb). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32817\">No</a>\n"]}, {"number": 32816, "title": "huge runtime increase for keras converted tflite on some android devices", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Honor Play (COR-AL00) and Poco F1\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Tested on various versions(1.14, 1.15rc1, 2.0rc1)\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NIL\r\n- GCC/Compiler version (if compiling from source): NIL\r\n- CUDA/cuDNN version: NIL\r\n- GPU model and memory: NIL\r\n\r\n**Describe the current behavior**\r\nHonor Play and POCO F1 phones give nearly same performance for TFLite models converted using TensorFlow (.pb) to TFLite whereas TFLite model converted using Keras (h5) to TFLite is behaving strange i.e. runtime on HonorPlay(COR-AL00) is 3 times higher than POCOF1.\r\n \r\nExample:  \r\nMobilenet-Unet (converted from keras(h5) to tflite)\r\nAverage model runtime of mobilenet-unet(Poco): 60ms\r\nAverage model runtime of mobilenet-unet(Honor): 180ms\r\n\r\nSimple - Unet (converted from tensorflow(pb) to tflite)\r\nAverage model runtime of unet(Poco): 50ms\r\nAverage model runtime of unet(Honor): 60ms\r\n\r\n**Describe the expected behavior**\r\nSame runtime for models with same architecture, no matter whether it is keras(h5) converted to tflite, or pb converted to tflite.\r\n\r\n**Code to reproduce the issue**\r\nOfficial Benchmark tool test and android integration test (Benchmark version- 1.14, and Android(tflite gpu delegate-nightly)\r\n\r\n**Other info / logs**\r\nThe base keras(h5) model has been created using pure keras, not tf-keras.\r\n\r\n**### Benchmark tool results for sample model (google drive link attached):**\r\n\r\n**### HONOR PLAY RESULTS**\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nUse legacy nnapi : [0]\r\nUse gpu : [1]\r\nAllow fp16 : [0]\r\nEnable op profiling: [0]\r\nLoaded model /data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Next operations are not supported by GPU delegate:\r\nCAST: Operation is not supported.\r\nDIV: Expected 2 input tensor(s), but node has 1 runtime input(s).\r\nRESHAPE: \r\nFirst 93 operations will run on the GPU, and the remaining 3 on the CPU.\r\nApplied GPU delegate.\r\nInitialized session in 894.605ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=3 first=227955 curr=174845 min=174845 max=227955 avg=204373 std=22086\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=148625 curr=101260 min=97334 max=148625 avg=103508 std=7440\r\n\r\nAverage inference timings in us: Warmup: 204373, Init: 894605, no stats: 103508\r\n\r\n**### POCO F1 RESULTS** \r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nUse legacy nnapi : [0]\r\nUse gpu : [1]\r\nAllow fp16 : [0]\r\nEnable op profiling: [0]\r\nLoaded model /data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Next operations are not supported by GPU delegate:\r\nCAST: Operation is not supported.\r\nDIV: Expected 2 input tensor(s), but node has 1 runtime input(s).\r\nRESHAPE: \r\nFirst 93 operations will run on the GPU, and the remaining 3 on the CPU.\r\nApplied GPU delegate.\r\nInitialized session in 612.655ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=15 first=75811 curr=30928 min=30909 max=75811 avg=34068.6 std=11156\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=31179 curr=31509 min=30836 max=32212 avg=31148.3 std=261\r\n\r\nAverage inference timings in us: Warmup: 34068.6, Init: 612655, no stats: 31148.3\r\n\r\n**### SAMPLE MODEL**\r\n[https://drive.google.com/file/d/1PvvoIzfrzLY8VNw5wTdpA6hxOFw_Ri9B/view?usp=sharing](https://drive.google.com/file/d/1PvvoIzfrzLY8VNw5wTdpA6hxOFw_Ri9B/view?usp=sharing)", "comments": ["Can you attach both the .tflite model converted from Keras and that converted from the frozen graph (.pb)? As well as the frozen graph itself? Thanks.", "Attaching with this, the mobilenet-unet keras model and tflite for 2 class segmentation..\r\n\r\nAn update to the issue, this issue does not serve the same for all keras models. Some other models are performing nearly the same speed. For reference, that file is also attached with this issue.. \r\n\r\nBoth models are keras trained. They have been named according to their behavior on the mobiles. The same speed giving model is a simple unet encoder-decoder model. The model giving speed issue is a keras trained mobilenet-encoder and unet-decoder model.\r\n\r\nModels ( check out the google drive link below)\r\n\r\n[https://drive.google.com/file/d/16yNeKx94Oh3JxTQaLmK2q_rARAMjrWfm/view?usp=sharing](https://drive.google.com/file/d/16yNeKx94Oh3JxTQaLmK2q_rARAMjrWfm/view?usp=sharing)\r\n\r\nThe speed details are listed below. \r\n**unet_mobilenet_not_same_speed.tflite**\r\nHonor Play : 154ms (Average)\r\nPoco F1: 47ms (Average)\r\n\r\n**working_nearly_same_speed.tflite**\r\nHonor Play: 18.9 ms (Average)\r\nPoco F1: 24ms ( Average)\r\n\r\nAndroid Benchmark Tool Logs:\r\n\r\nPoco F1:\r\n[PocoF1_same_speed_model.txt](https://github.com/tensorflow/tensorflow/files/3674658/PocoF1_same_speed_model.txt)\r\n[PocoF1_unet_mobilenet_not_same_speed_model.txt](https://github.com/tensorflow/tensorflow/files/3674659/PocoF1_unet_mobilenet_not_same_speed_model.txt)\r\n\r\nHonor Play:\r\n[HonorPlay_same_speed_model.txt](https://github.com/tensorflow/tensorflow/files/3674660/HonorPlay_same_speed_model.txt)\r\n[HonorPlay_unet_mobilenet_not_same_speed_model.txt](https://github.com/tensorflow/tensorflow/files/3674661/HonorPlay_unet_mobilenet_not_same_speed_model.txt)\r\n", "@SanthoshRajendiran \r\nplease let us know if the issue still persist", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32816\">No</a>\n"]}, {"number": 32815, "title": "tf.io.gfile.copy and tf.gfile.Copy input and output same path with overwrite removes all contents", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.15\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`cat label.pbtxt`\r\n> Some txt in the file\r\n\r\n`tf.io.gfile.Copy('label.pbtxt', 'label.pbtxt', overwrite=True)`\r\n\r\n`cat label.pbtxt`\r\n> <Empty text file>\r\n\r\n**Describe the expected behavior**\r\n`cat label.pbtxt`\r\n> Some txt in the file\r\n\r\n`tf.io.gfile.Copy('label.pbtxt', 'label.pbtxt', overwrite=True)`\r\n\r\n`cat label.pbtxt`\r\n> Some txt in the file\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`import tensorflow as tf `\r\n`tf.io.gfile.Copy('label.pbtxt', 'label.pbtxt', overwrite=True)`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Issue replicating with TF-1.14, also in TF-2.0rc2. kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/5a040c1b1f4af41b5e3f1d51f3f3d9e4/untitled8.ipynb#scrollTo=CMr3psVURw-5) of colab.Thanks!", "This is kind of working as intended. The same behavior happens on the operating system level:\r\n\r\n```console\r\nmihaimaruseac@ankh:/tmp$ cat x\r\n1\r\n2\r\n3\r\nmihaimaruseac@ankh:/tmp$ cat x > x\r\nmihaimaruseac@ankh:/tmp$ cat x\r\nmihaimaruseac@ankh:/tmp$\r\n```\r\n\r\nIn the end, you are opening the file twice, once for reading and once for writing on it from scratch (thus truncating it to size 0). Then the copy reads a chunk from the beginning and writes it on the output. However, as the file on disk has been cleared, the first chunk is empty so nothing gets written.\r\n\r\nEdit: note the behavior when not overwritting but appending:\r\n\r\n```bash\r\nmihaimaruseac@ankh:/tmp$ cat x >> x\r\ncat: x: input file is output file\r\n```\r\n\r\nAgain, this makes sense: if error was not provided then you could create an infinite (to the limit of hardware) file.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32815\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32815\">No</a>\n"]}, {"number": 32814, "title": "gpu_options doesn't work", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04, Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **pip install way**\r\n- TensorFlow version (use command below): \r\n        **pip install tensorflow-gpu==1.12.\\* and \r\n        pip install tensorflow-gpu==1.14.\\***\r\n- Python version: **python 3.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **tf1.12 with cuda9, tf1.14 with cuda10**\r\n- GPU model and memory: **ubuntu 16.04 with 12G, ubuntu 18.04 with 24G**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI used the codes in AdaNet like\r\n```python\r\nGPU_OPTIONS = tf.GPUOptions(allow_growth=True)\r\nCONFIG = tf.ConfigProto(gpu_options=GPU_OPTIONS)\r\nsess = tf.Session(config = CONFIG)\r\n```\r\nBut it still occupied the whole gpu memory when I run the AdaNet. For example: \r\n(1) If there is only 4G memory left, it will occupied the remaining 6G memory\r\n(2) If there is no other process occupying the GPU memory (i.e., there is 10G memory left), it will occupied the whole 10G memory when I run the AdaNet. \r\nSo I think it doesn't need 10G to run but it takes that much any way.\r\n\r\n**Describe the expected behavior**\r\nI expect it only takes the memory it needs, instead of taking all of them. Therefore, I could make use of the 10G GPU memory better. \r\nBut now, I have no idea how to fix this issue. Could anybody please give me some suggestions? Thanks a lot.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nI used the codes there [AdaNet](https://github.com/tensorflow/adanet) and change all \"tf.Session()\" to \"tf.Session(config=CONFIG)\" with codes as I mentioned before.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You need to put the gpu configuration on top of your code.\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\nsess = tf.Session(config=config)\r\n# Your model\r\n  . . .\r\n  . . .\r\n```\r\nCan you please confirm if this was the case? Also can you please point me to example you tried?Thanks!", "> You need to put the gpu configuration on top of your code.\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth=True\r\n> sess = tf.Session(config=config)\r\n> # Your model\r\n>   . . .\r\n>   . . .\r\n> ```\r\n> \r\n> Can you please confirm if this was the case? Also can you please point me to example you tried?Thanks!\r\n\r\nYes, that's exactly what I did. Didn't work", "The codes I used is here [AdaNet](https://github.com/tensorflow/adanet), to be specific, the codes I got by \"pip install adanet==0.5.0\". Since AdaNet has been updated, AdaNet 0.5.0 will have some differences, compared with [AdaNet](https://github.com/tensorflow/adanet). \r\n\r\nBesides, I changed all the original \"sess = tf.Session()\" inside to \"config, ... , sess=tf.Session()\". The files involving \"tf.Session()\" are \"core/ensemble.py\", \"core/estimator.py\", and \"subnetwork/generator.py\". \r\n\r\nI put codes as follows in the beginning of each file, like:\r\n```python \r\nfrom __future__ import ...\r\nimport ...\r\n\r\nGPU_OPTIONS = tf.GPUOptions(allow_growth=True)\r\nCONFIG = tf.ConfigProto(gpu_options=GPU_OPTIONS)\r\n\r\ndef ....\r\nclass ...\r\n\r\n    with tf.Session(config=CONFIG) as sess:\r\n        # codes\r\n```\r\n\r\nAnother way is \r\n```python\r\nfrom __future__ import ...\r\nimport ...\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\n...\r\nwith tf.Session(config=config) as sess:\r\n    ...\r\n```\r\n\r\nI tried both and none of them worked. If there is still ambiguity, please tell me to be specific. Thank you so much for help!", "As for the examples I tried, it refers to [A Simple Example using AdaNet](https://github.com/tensorflow/adanet/blob/master/adanet/examples/tutorials/customizing_adanet.ipynb)  \r\n\r\nPlease modify some of them as follows:\r\n```python\r\nfrom core_estimator import Estimator as MyEstimator  # I did some modifications based on AdaNet\r\nfrom core_evaluator import Evaluator as MyEvaluator   # I did some modifications based on AdaNet\r\nimport examples_simple_dnn as mine_simple_dnn     # I did some modifications based on AdaNet\r\n...\r\nADANET_ITERATIONS = 7  ##@param {type: \"integer\"}\r\nmax_iteration_steps = TRAIN_STEPS // ADANET_ITERATIONS\r\n\r\nestimator = MyEstimator(\r\n            head=head, \r\n            subnetwork_generator=my_simple_dnn.Generator(\r\n                feature_columns=feature_columns, \r\n                optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE), \r\n                seed=RANDOM_SEED), \r\n            max_iteration_steps=temp_stp, \r\n            evaluator=MyEvaluator(input_fn=eval_input_fn, steps=None), \r\n            config=make_config(this_experiment))\r\n\r\n```", "It doesn't work for me either, have you found the solution?", "> It doesn't work for me either, have you found the solution?\r\n\r\nNope. If I did, I wouldn't open this issue", "I cannot reproduce this issue with 1.15.0rc2. Here is what I did:\r\n```\r\n$ pip install tensorflow-gpu==1.15.0rc2\r\n$ python\r\n>>> import tensorflow as tf\r\n>>> GPU_OPTIONS = tf.GPUOptions(allow_growth=True)\r\n>>> CONFIG = tf.ConfigProto(gpu_options=GPU_OPTIONS)\r\n>>> sess = tf.Session(config = CONFIG)\r\n```\r\n\r\nI'm using a Titan-V GPU. After running the above command, only 319MiB out of 12066MiB is used based on nvidia-smi.\r\n\r\nCould you try with the above commands?", "**Did you use the AdaNet codes that I mentioned before? This issue happens when I use AdaNet models**, involving some modifications of course, but I think all you need to modify is those related to \"sess = tf.Session()\", by replacing them with \"sess = tf.Session(config=config)\". I will explain that in detail in the following. \r\n\r\nTo be clear, I get the AdaNet codes by \"**pip install adanet==0.5.0**\". Then I **changed all the original \"sess = tf.Session()\" inside to \"config, ..., sess=tf.Session()\"**. The files **involving \"tf.Session()\"** are **\"core/ensemble.py\", \"core/estimator.py\", and \"subnetwork/generator.py\"**.   \r\nI put codes as follows at the beginning of each file, like:\r\n```python\r\nfrom __future__ import ...\r\nimport ...\r\n\r\n# the first method\r\nGPU_OPTIONS = tf.GPUOptions(allow_growth=True)\r\nCONFIG = tf.ConfigProto(gpu_options=GPU_OPTIONS)\r\n# the second method\r\n# config = tf.ConfigProto()\r\n# config.gpu_options.allow_growth = True\r\n\r\ndef ....\r\nclass ...\r\n\r\n    with tf.Session(config=CONFIG) as sess:\r\n        # codes\r\n```\r\n\r\nFor your convenience, after modifying those \"sess = tf.Session(config=config)\", **this issue could be reproduced by using examples, which could refer to** [A Simple Example using AdaNet](https://github.com/tensorflow/adanet/blob/master/adanet/examples/tutorials/customizing_adanet.ipynb). **Please note that the \"MyEstimator, MyEvaluator\" should be imported from the codes that you just modified**, instead of \r\n```python\r\nfrom adanet import Estimator\r\nfrom adanet import Evaluator\r\n```\r\n. Please modify some of them as follows:\r\n```python\r\n# Please import MyEstimator, MyEvaluator from the codes you just modified\r\n# It may differ from the following three lines that I wrote\r\nfrom core_estimator import Estimator as MyEstimator  # I did some modifications based on AdaNet\r\nfrom core_evaluator import Evaluator as MyEvaluator   # I did some modifications based on AdaNet\r\nimport examples_simple_dnn as mine_simple_dnn     # I did some modifications based on AdaNet\r\n...\r\nADANET_ITERATIONS = 7  ##@param {type: \"integer\"}\r\nmax_iteration_steps = TRAIN_STEPS // ADANET_ITERATIONS\r\n\r\nestimator = MyEstimator(\r\n            head=head, \r\n            subnetwork_generator=my_simple_dnn.Generator(\r\n                feature_columns=feature_columns, \r\n                optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE), \r\n                seed=RANDOM_SEED), \r\n            max_iteration_steps=temp_stp, \r\n            evaluator=MyEvaluator(input_fn=eval_input_fn, steps=None), \r\n            config=make_config(this_experiment))\r\n```\r\n\r\nBesides, I'm using a TITAN RTX on Ubuntu 18.04 and a GTX 1080 Ti on Ubuntu 16.04. Other details could refer to the beginning of this issue. Therefore, I don't think \"TITAN RTX / GTX 1080 Ti / Titan-V GPU\" is the problem. \r\n\r\nThanks for helping me. \r\n\r\n", "It's strange,  I found that only the \"allow_growth\" option doesn't work, others work well.\r\nIf I add use this config in my code:\r\n\r\n`tf_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\r\ntf_config.gpu_options.allow_growth = True\r\ntf_config.gpu_options.per_process_gpu_memory_fraction = 0.05`\r\n\r\nThe program will only use 5% gpu memory, and report this message:\r\n`Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.`\r\n\r\nAnd the program couldn't run if I allocate less memory.\r\nSo the program can't automatic allocate memory as the option \"allow_growth\" doesn't work.", "> It's strange, I found that only the \"allow_growth\" option doesn't work, others work well.\r\n> If I add use this config in my code:\r\n> \r\n> `tf_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)) tf_config.gpu_options.allow_growth = True tf_config.gpu_options.per_process_gpu_memory_fraction = 0.05`\r\n> \r\n> The program will only use 5% gpu memory, and report this message:\r\n> `Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.`\r\n> \r\n> And the program couldn't run if I allocate less memory.\r\n> So the program can't automatic allocate memory as the option \"allow_growth\" doesn't work.\r\n\r\nI remembered I tried \r\n```python\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\r\n```\r\nSame situation as \"allow_growth=True\". \r\n\r\nMaybe it is a problem of AdaNet? It always wants to take as much memory as possible, even though it doesn't need that much? ", "I'm not familiar with adanet, but as another quick test, can you try running these\r\n```\r\nimport tensorflow as tf\r\nGPU_OPTIONS = tf.GPUOptions(allow_growth=True)\r\nCONFIG = tf.ConfigProto(gpu_options=GPU_OPTIONS)\r\nsess = tf.Session(config = CONFIG)\r\n# <-- sleep and measure the mem usage here\r\n```\r\nbefore importing anything from adanet?", "> I'm not familiar with adanet, but as another quick test, can you try running these\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> GPU_OPTIONS = tf.GPUOptions(allow_growth=True)\r\n> CONFIG = tf.ConfigProto(gpu_options=GPU_OPTIONS)\r\n> sess = tf.Session(config = CONFIG)\r\n> # <-- sleep and measure the mem usage here\r\n> ```\r\n> \r\n> before importing anything from adanet?\r\n\r\nI already did that. Please refer to my previous comments", "Well, it seems like this issue goes away even though I'm not sure whether that I'm guessing is the right reason.  \r\nPreviously, in the middle of models, I initialize some variables manually instead of using tf.global_variables_initializer. \r\nNow, I only initialize those variables that are not initialized at the very time, and suddenly, it works, now the model wouldn't take the whole GPU memory, at least it works for now.\r\n\r\n@aaroey @ymodak You may close this issue now. Thank you very much for helping me!", "Glad to know that it worked.Closing the issue since its resolved. Thanks!", "I had the same code and had debugged for hours but still it did not work -- it just used all the GPU memory it had.\r\n\r\nIn the end, **setting GPU before loading Keras** solved my problem of limiting GPU memory usage!\r\n\r\nhttps://github.com/keras-team/keras/issues/1538#issuecomment-342034737"]}, {"number": 32813, "title": "Windows chief can not establish session with unix worker", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.2\r\n- Python version: 3.6.5 (on Windows), 3.6.3 (on CentOS)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GTX 1080 Ti, RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nI run a simple code to report on the system devices in a 2-worker cluster. When Unix system is the chief (task_id=0), it can communicate and establish session with Windows worker and display cluster devices. However, when the Windows system becomes chief it can not establish session with Unix worker (hangs on displaying ' CreateSession still waiting for response from worker: /job:worker/replica:0/task:1'). Both system can reach each other in both cases via ping.\r\n\r\n**Code to reproduce the issue**\r\n\r\nCode for chief:\r\n```\r\nimport tensorflow as tf\r\ninit = tf.global_variables_initializer()\r\ncluster_spec = tf.train.ClusterSpec({'worker' : [(IP_ADDRESS1:PORT1), (IP_ADDRESS2:PORT2)]})\r\ntask_idx=0\r\nserver = tf.train.Server(cluster_spec, job_name='worker', task_index=task_idx)\r\nwith tf.Session(server.target) as sess:\r\n    sess.run(init)\r\n    print(sess.list_devices())\r\n```\r\n\r\nCode for Worker:\r\n```\r\nimport tensorflow as tf\r\ninit = tf.global_variables_initializer()\r\ncluster_spec = tf.train.ClusterSpec({'worker' : [(IP_ADDRESS1:PORT1), (IP_ADDRESS2:PORT2)]})\r\ntask_idx=1\r\nserver = tf.train.Server(cluster_spec, job_name='worker', task_index=task_idx)\r\nserver.join()\r\n```\r\n\r\nIP_ADDRESS1 is always the address of the chief system and IP_ADDRESS2 is the adddress of worker, and it is swapped when swapping Windows and Unix systems rules.\r\n\r\n", "comments": ["Is there a firewall active by default on either machine? Can you connect manually to the port?\r\n\r\nIt could be that the Unix worker doesn't accept connections by default (but can initiate them). ", "@martinwicke \r\nThey can ping each other. How can I check the port connection manually? ", "I've always used `telnet` for that sort of diagnostic, though I think recent *nixes don't include it. ", "Yes it looks like there is a port blocking issue. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32813\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32813\">No</a>\n"]}, {"number": 32812, "title": "deleted", "body": "", "comments": ["sorry I issued the wrong place..."]}, {"number": 32811, "title": "fix .so file name error on linux and macos", "body": "run on linux error\uff1a\r\njava.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1562914806051-0/libtensorflow_jni.so: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory\r\n\r\nrun on macos error\uff1a\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: /private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib: dlopen(/private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib, 1): Library not loaded: @rpath/libtensorflow_framework.1.dylib\r\n  Referenced from: /private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib\r\n  Reason: image not found", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32811) for more info**.\n\n<!-- need_sender_cla -->", "> \u611f\u8c22\u60a8\u7684\u8bf7\u6c42\u3002\u770b\u6765\u8fd9\u53ef\u80fd\u662f\u60a8\u5bf9Google\u5f00\u6e90\u9879\u76ee\u7684\u7b2c\u4e00\u4e2a\u8d21\u732e\uff08\u5982\u679c\u6ca1\u6709\uff0c\u8bf7\u67e5\u770b\u4e0b\u9762\u7684\u5e2e\u52a9\uff09\u3002\u5728\u6211\u4eec\u67e5\u770b\u60a8\u7684\u8bf7\u6c42\u8bf7\u6c42\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u7b7e\u7f72\u8d21\u732e\u8005\u8bb8\u53ef\u534f\u8bae\uff08CLA\uff09\u3002\r\n> \r\n>  **\u8bf7\u8bbf\u95eehttps://cla.developers.google.com/\u8fdb\u884c\u7b7e\u540d\u3002**\r\n> \r\n> \u7b7e\u7f72\uff08\u6216\u89e3\u51b3\u4efb\u4f55\u95ee\u9898\uff09\u540e\uff0c\u8bf7\u5728\u6b64\u5904\u56de\u590d`@googlebot I signed it!`\uff0c\u6211\u4eec\u5c06\u5bf9\u5176\u8fdb\u884c\u9a8c\u8bc1\u3002\r\n> \r\n> #### \u5982\u679c\u60a8\u5df2\u7ecf\u7b7e\u7f72\u4e86CLA\uff0c\u8be5\u600e\u4e48\u529e\r\n> ##### \u4e2a\u4eba\u7b7e\u540d\u4eba\r\n> * \u53ef\u80fd\u6211\u4eec\u6ca1\u6709\u60a8\u7684GitHub\u7528\u6237\u540d\uff0c\u6216\u8005\u60a8\u5728\u63d0\u4ea4\u65f6\u4f7f\u7528\u4e86\u5176\u4ed6\u7535\u5b50\u90ae\u4ef6\u5730\u5740\u3002\u68c0\u67e5[\u60a8\u73b0\u6709\u7684CLA\u6570\u636e\uff0c](https://cla.developers.google.com/clas)\u5e76\u9a8c\u8bc1\u662f\u5426[\u5728git commit\u4e0a\u8bbe\u7f6e\u4e86\u7535\u5b50\u90ae\u4ef6](https://help.github.com/articles/setting-your-email-in-git/)\u3002\r\n> \r\n> ##### \u516c\u53f8\u7b7e\u540d\u4eba\r\n> * \u60a8\u7684\u516c\u53f8\u6709\u4e00\u4e2a\u8054\u7edc\u70b9\uff0c\u53ef\u4ee5\u51b3\u5b9a\u6388\u6743\u54ea\u4e9b\u5458\u5de5\u53c2\u52a0\u3002\u8981\u6c42\u5c06\u60a8\u7684POC\u6dfb\u52a0\u5230\u6388\u6743\u8d21\u732e\u8005\u7ec4\u4e2d\u3002\u5982\u679c\u60a8\u4e0d\u77e5\u9053\u8054\u7edc\u4eba\u662f\u8c01\uff0c\u8bf7\u6307\u793aGoogle\u9879\u76ee\u7ef4\u62a4\u8005\u8fdb\u884c[go / cla\uff03troubleshoot](http://go/cla#troubleshoot)\uff08[\u516c\u5f00\u7248\u672c](https://opensource.google.com/docs/cla/#troubleshoot)\uff09\u3002\r\n> * \u7528\u4e8e\u5c06\u60a8\u6ce8\u518c\u4e3a\u6388\u6743\u8d21\u732e\u8005\u7684\u7535\u5b50\u90ae\u4ef6\u5fc5\u987b\u662f\u7528\u4e8eGit\u63d0\u4ea4\u7684\u7535\u5b50\u90ae\u4ef6\u3002\u68c0\u67e5[\u60a8\u73b0\u6709\u7684CLA\u6570\u636e\uff0c](https://cla.developers.google.com/clas)\u5e76\u9a8c\u8bc1\u662f\u5426[\u5728git commit\u4e0a\u8bbe\u7f6e\u4e86\u7535\u5b50\u90ae\u4ef6](https://help.github.com/articles/setting-your-email-in-git/)\u3002\r\n> * \u7528\u4e8e\u5c06\u60a8\u6ce8\u518c\u4e3a\u6388\u6743\u8d21\u732e\u8005\u7684\u7535\u5b50\u90ae\u4ef6\u8fd8\u5fc5\u987b[\u9644\u52a0\u5230\u60a8\u7684GitHub\u5e10\u6237\u4e2d](https://github.com/settings/emails)\u3002\r\n> \r\n>  **Google\u5458\u5de5\uff1a[\u5230\u8fd9\u91cc](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32811)\u83b7\u53d6\u66f4\u591a\u4fe1\u606f**\u3002\r\n\r\n\r\n\r\n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n>  **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n>  **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32811) for more info**.\r\n\r\n@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32811) for more info**.\n\n<!-- ok -->", "I have reported an error when using tensorflow1.14 on linux. The program prompts that libtensorflow_framework.so.1 cannot be found. It is found that only libtensorflow_framework.so is decompressed under tmp, so I modified the NativeLibrary class and corrected the name when releasing the resource.", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac", "Please make the PR agains master so it can be picked up on the next releases."]}, {"number": 32810, "title": "Send/Recv of collective_ops hangs in a distributed environment ", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux CentOS 7.6.1810\r\n- Mobile device if the issue happens on mobile device: No\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: v1.13.1-0-g6612da8951\r\n- Python version: 3.6.8\r\n- Bazel version: None\r\n- GCC/Compiler version: None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nThe monitored session hangs in there fetching the send/recv tensors of `collective_ops`. The same code works well for fetching `all_reduce` tensors.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe send/recv tensors `tensors` gives proper answer on all workers.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\"\"\"Illustrate send/recv in collective_ops\"\"\"\r\n\r\nMP_METHOD = 'fork'  # 'fork' (UNIX), 'spawn' (WINDOWS);\r\nNUM_PROCESSES = 2\r\nJOB = 'worker'\r\n\r\ndef process_fn(hosts, task_index):\r\n    \"\"\"process fn\"\"\"\r\n    import time\r\n    import tensorflow as tf\r\n    from tensorflow.python.ops import collective_ops\r\n\r\n    cluster_spec = tf.train.ClusterSpec({JOB: hosts})\r\n    host_devices = list()\r\n    for task, _ in enumerate(hosts):\r\n        host_devices.append(tf.DeviceSpec(\r\n            job=JOB, replica=0, task=task, device_type='cpu', device_index=0))\r\n    chief_host_device = host_devices[0]\r\n\r\n    # unconfigured collective_group_leader make each worker the leader\r\n    # '/replica:0' is necessary in the configuration.\r\n    collective_group_leader, _, _ = \\\r\n        chief_host_device.to_string().partition('/device')\r\n    config = tf.ConfigProto()\r\n    config.experimental.collective_group_leader = collective_group_leader\r\n    server = tf.train.Server(cluster_spec, config=config,\r\n                             job_name=JOB, task_index=task_index)\r\n    run_options = tf.RunOptions()\r\n    run_options.experimental.collective_graph_key = 1\r\n    with tf.Graph().as_default():\r\n        weights = list()\r\n        tensors = list()\r\n        instance_key = 1\r\n        for task, device in enumerate(host_devices):\r\n            with tf.device(device), tf.variable_scope('{}{}'.format(JOB, task)):\r\n                weight = tf.get_variable('weight', shape=[])\r\n                weights.append(weight)\r\n\r\n                # send/recv\r\n                if task == task_index:\r\n                    tensor = collective_ops.broadcast_send(\r\n                        weight, weight.shape, weight.dtype,\r\n                        len(hosts), 1, instance_key)\r\n                else:\r\n                    tensor = collective_ops.broadcast_recv(\r\n                        weight.shape, weight.dtype,\r\n                        len(hosts), 1, instance_key)\r\n                tensors.append(tensor)\r\n                instance_key += 1\r\n\r\n#                # allreduce\r\n#                if task == task_index:\r\n#                    tensor = collective_ops.all_reduce(\r\n#                        weight, len(hosts), 0, instance_key, 'Add', 'Div')\r\n#                    tensors.append(tensor)\r\n#                    instance_key += 1\r\n\r\n        if task_index == 0:\r\n            session_creator = tf.train.ChiefSessionCreator(\r\n                master=server.target)\r\n        else:\r\n            session_creator = tf.train.WorkerSessionCreator(\r\n                master=server.target)\r\n        with tf.train.MonitoredSession(session_creator=session_creator) \\\r\n                as mon_sess:\r\n            print('task {} running.'.format(task_index))\r\n            result_weights = mon_sess.run(weights, options=run_options)\r\n            print('task {} sense {}'.format(task_index, result_weights))\r\n            result_tensors = mon_sess.run(tensors, options=run_options)\r\n            print('task {} broadcast {}'.format(task_index, result_tensors))\r\n            time.sleep(10)\r\n\r\ndef start_process():\r\n    \"\"\"start process\"\"\"\r\n    import time\r\n    import multiprocessing as mp\r\n\r\n    port = 60000\r\n    host_fmt = 'localhost:{}'\r\n    hosts = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        hosts.append(host_fmt.format(port + process_index))\r\n    mp_ctx = mp.get_context(MP_METHOD)\r\n    processes = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        process = mp_ctx.Process(target=process_fn,\r\n                                 args=(hosts, process_index,))\r\n        processes.append(process)\r\n        process.start()\r\n        time.sleep(0.1)\r\n    for process in processes:\r\n        process.join()\r\n\r\nif __name__ == '__main__':\r\n    start_process()\r\n```\r\n\r\n**Other info / logs**\r\n```console\r\n(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_distribute_collective_ops_sendrecv.py \r\n2019-09-25 17:46:29.612863: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-25 17:46:29.625630: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-09-25 17:46:29.625962: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2c2e220 executing computations on platform Host. Devices:\r\n2019-09-25 17:46:29.625984: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-25 17:46:29.627731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-09-25 17:46:29.628640: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60000\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-09-25 17:46:29.696555: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-25 17:46:29.709593: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-09-25 17:46:29.709906: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2c2e410 executing computations on platform Host. Devices:\r\n2019-09-25 17:46:29.709932: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-25 17:46:29.711328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-09-25 17:46:29.712186: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60001\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-09-25 17:46:29.741447: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 22dc988b8d402f7f with config: experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\ntask 0 running.\r\ntask 0 sense [-0.30021727, -0.797495]\r\n2019-09-25 17:46:29.798887: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session bedba16e0ccc4c27 with config: experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\ntask 1 running.\r\n2019-09-25 17:46:29.820772: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.\r\n2019-09-25 17:46:29.820874: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.\r\n2019-09-25 17:46:29.821341: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Aborted: Cleanup 70896605979375878\r\n\t [[{{node worker1/CollectiveBcastRecv}}]]\r\ntask 1 sense [-0.30021727, -0.797495]\r\n```\r\nThere is a related issue #31913, which can be solved by specifying the `experimental.collective_group_leader` in [tf.ConfigProto](https://www.tensorflow.org/api_docs/python/tf/ConfigProto/Experimental#collective_group_leader).\r\nAn info about `rendezvous re-initialization` and a warning about`BaseCollectiveExecutor` are raised from the `send/recv`. Nevertheless, the script works fine for the commented `allreduce`. ", "comments": ["Could you please take a look at this issue and the related #31913? @dubey", "In this case it looks like you're actually creating 2 instances of broadcast collectives, but assigning both of them the same instance key.  A group defines a set of devices participating in a collective op, an instance is a specific collective, like a single all-reduce or a single broadcast.  In this case there is an instance in which task 0 sends and task 1 receives, and there is another instance in which task 0 receives and task 1 sends.  They should have separate instance keys.", "Thank you very much for the fast reply! @dubey I improve the code and come across another problem. The script is simplified to keep only one variable named 'weight' on each worker, and every worker creates its own chief session to initialize the variable. The code of `send/recv` is improved by creating **a pair** of tensors of the sending and the receiving with **the same** `instance_key`. When worker0 sends and worker1 receives, it exits successfully without warnings.\r\n\r\nUnfortunately and _ridiculously_, it fails with worker0 as the receiver and worker1 as the sender, with the error `Instance 0 found no source for broadcast`. It seems the `send_tensor` on worker1 is unregistered in the send/recv pair. If `collective_group_leader` is set to worker1, the process raises the same error when `if task_index == 0`, but hangs when `if task_index == 1`.\r\n\r\nscript snippet:\r\n```python\r\n        # send/recv\r\n        instance_key = 0\r\n        with tf.device(host_devices[task_index]), \\\r\n                tf.variable_scope('{}{}'.format(JOB, task_index)):\r\n            weight = tf.get_variable('weight', shape=[])\r\n            weights.append(weight)\r\n            send_tensor = collective_ops.broadcast_send(\r\n                weight, weight.shape, weight.dtype,\r\n                len(hosts), 0, instance_key)\r\n            recv_tensor = collective_ops.broadcast_recv(\r\n                weight.shape, weight.dtype,\r\n                len(hosts), 0, instance_key)\r\n            instance_key += 1\r\n            if task_index == 0:  # fails when 'task_index == 1'\r\n                tensors.append(send_tensor)\r\n            else:\r\n                tensors.append(recv_tensor)\r\n```\r\n\r\nsuccess log when `if task_index == 0:`:\r\n```console\r\ntask 1 running.\r\ntask 1 sense [0.825487]\r\ntask 0 running.\r\ntask 0 sense [-1.5433102]\r\ntask 0 broadcast [-1.5433102]\r\ntask 1 broadcast [-1.5433102]\r\n```\r\n\r\nfailure log when `if task_index == 1:`:\r\n```console\r\ntask 1 running.\r\ntask 1 sense [1.0812124]\r\ntask 0 running.\r\ntask 0 sense [0.3877238]\r\n2019-09-26 10:51:21.474402: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Internal: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[{{node worker0/CollectiveBcastRecv}}]]\r\n2019-09-26 10:51:21.474438: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Internal: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[{{node worker1/CollectiveBcastSend}}]]\r\nProcess ForkProcess-1:\r\nProcess ForkProcess-2:\r\n```\r\nThe full script and logs of success and failure are attached below.\r\n```python\r\n\"\"\"Illustrate send/recv in collective_ops\"\"\"\r\n\r\nMP_METHOD = 'fork'  # 'fork' (UNIX), 'spawn' (WINDOWS);\r\nNUM_PROCESSES = 2\r\nJOB = 'worker'\r\n\r\ndef process_fn(hosts, task_index):\r\n    \"\"\"process fn\"\"\"\r\n    import time\r\n    import tensorflow as tf\r\n    from tensorflow.python.ops import collective_ops\r\n\r\n    cluster_spec = tf.train.ClusterSpec({JOB: hosts})\r\n    host_devices = list()\r\n    for task, _ in enumerate(hosts):\r\n        host_devices.append(tf.DeviceSpec(\r\n            job=JOB, replica=0, task=task, device_type='cpu', device_index=0))\r\n    chief_host_device = host_devices[0]\r\n\r\n    # unconfigured collective_group_leader make each worker the leader\r\n    # '/replica:0' is necessary in the configuration.\r\n    collective_group_leader, _, _ = \\\r\n        chief_host_device.to_string().partition('/device')\r\n    config = tf.ConfigProto()\r\n    config.experimental.collective_group_leader = collective_group_leader\r\n    server = tf.train.Server(cluster_spec, config=config,\r\n                             job_name=JOB, task_index=task_index)\r\n    run_options = tf.RunOptions()\r\n    run_options.experimental.collective_graph_key = 1\r\n    with tf.Graph().as_default():\r\n        weights = list()\r\n        tensors = list()\r\n        \r\n        # send/recv\r\n        instance_key = 0\r\n        with tf.device(host_devices[task_index]), \\\r\n                tf.variable_scope('{}{}'.format(JOB, task_index)):\r\n            weight = tf.get_variable('weight', shape=[])\r\n            weights.append(weight)\r\n            send_tensor = collective_ops.broadcast_send(\r\n                weight, weight.shape, weight.dtype,\r\n                len(hosts), 0, instance_key)\r\n            recv_tensor = collective_ops.broadcast_recv(\r\n                weight.shape, weight.dtype,\r\n                len(hosts), 0, instance_key)\r\n            instance_key += 1\r\n            if task_index == 1:\r\n                tensors.append(send_tensor)\r\n            else:\r\n                tensors.append(recv_tensor)\r\n\r\n#        if task_index == 0:\r\n#            session_creator = tf.train.ChiefSessionCreator(\r\n#                master=server.target)\r\n#        else:\r\n#            session_creator = tf.train.WorkerSessionCreator(\r\n#                master=server.target)\r\n        session_creator = tf.train.ChiefSessionCreator(\r\n            master=server.target)\r\n        with tf.train.MonitoredSession(session_creator=session_creator) \\\r\n                as mon_sess:\r\n            print('task {} running.'.format(task_index))\r\n            result_weights = mon_sess.run(weights, options=run_options)\r\n            print('task {} sense {}'.format(task_index, result_weights))\r\n            result_tensors = mon_sess.run(tensors, options=run_options)\r\n            print('task {} broadcast {}'.format(task_index, result_tensors))\r\n            time.sleep(1)\r\n\r\ndef start_process():\r\n    \"\"\"start process\"\"\"\r\n    import time\r\n    import multiprocessing as mp\r\n\r\n    port = 60000\r\n    host_fmt = 'localhost:{}'\r\n    hosts = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        hosts.append(host_fmt.format(port + process_index))\r\n    mp_ctx = mp.get_context(MP_METHOD)\r\n    processes = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        process = mp_ctx.Process(target=process_fn,\r\n                                 args=(hosts, process_index,))\r\n        processes.append(process)\r\n        process.start()\r\n        time.sleep(0.1)\r\n    for process in processes:\r\n        process.join()\r\n\r\nif __name__ == '__main__':\r\n    start_process()\r\n```\r\nfull success log:\r\n```console\r\n(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_distribute_collective_ops_sendrecv.py \r\n2019-09-26 10:32:41.825361: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-26 10:32:41.838175: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-09-26 10:32:41.838505: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2da1270 executing computations on platform Host. Devices:\r\n2019-09-26 10:32:41.838533: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-26 10:32:41.840395: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-09-26 10:32:41.841263: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60000\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-09-26 10:32:41.913407: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-26 10:32:41.925525: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-09-26 10:32:41.925829: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2da1200 executing computations on platform Host. Devices:\r\n2019-09-26 10:32:41.925857: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-26 10:32:41.927326: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-09-26 10:32:41.928167: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60001\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-09-26 10:32:41.998983: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session e57a61876767fe8a with config: experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\n2019-09-26 10:32:42.918306: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session d36eb78a8ccde9b9 with config: experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\ntask 1 running.\r\ntask 1 sense [0.825487]\r\ntask 0 running.\r\ntask 0 sense [-1.5433102]\r\ntask 0 broadcast [-1.5433102]\r\ntask 1 broadcast [-1.5433102]\r\n```\r\nfull failure log:\r\n```console\r\n(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_distribute_collective_ops_sendrecv.py \r\n2019-09-26 10:51:20.319482: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-26 10:51:20.333906: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-09-26 10:51:20.334383: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x31fed80 executing computations on platform Host. Devices:\r\n2019-09-26 10:51:20.334424: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-26 10:51:20.336501: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-09-26 10:51:20.337508: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60000\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-09-26 10:51:20.404411: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-26 10:51:20.417182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-09-26 10:51:20.417500: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x31ff210 executing computations on platform Host. Devices:\r\n2019-09-26 10:51:20.417536: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-26 10:51:20.419285: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-09-26 10:51:20.420258: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60001\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-09-26 10:51:20.490274: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 568c3548db581180 with config: experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\n2019-09-26 10:51:21.416032: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session ff0f7c9a1e31db0d with config: experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\ntask 1 running.\r\ntask 1 sense [1.0812124]\r\ntask 0 running.\r\ntask 0 sense [0.3877238]\r\n2019-09-26 10:51:21.474402: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Internal: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[{{node worker0/CollectiveBcastRecv}}]]\r\n2019-09-26 10:51:21.474438: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Internal: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[{{node worker1/CollectiveBcastSend}}]]\r\nProcess ForkProcess-1:\r\nProcess ForkProcess-2:\r\nTraceback (most recent call last):\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[{{node worker0/CollectiveBcastRecv}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 129, in process_fn\r\n    result_tensors = mon_sess.run(tensors, options=run_options)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 676, in run\r\n    run_metadata=run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[{{node worker1/CollectiveBcastSend}}]]\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1171, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1270, in run\r\n    raise six.reraise(*original_exc_info)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\nTraceback (most recent call last):\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1327, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1091, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\ntensorflow.python.framework.errors_impl.InternalError: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[node worker0/CollectiveBcastRecv (defined at tf_distribute_collective_ops_sendrecv.py:45) ]]\r\n\r\nCaused by op 'worker0/CollectiveBcastRecv', defined at:\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 155, in <module>\r\n    start_process()\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 149, in start_process\r\n    process.start()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/usr/lib64/python3.6/multiprocessing/context.py\", line 277, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/usr/lib64/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/usr/lib64/python3.6/multiprocessing/popen_fork.py\", line 73, in _launch\r\n    code = process_obj._bootstrap()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 45, in process_fn\r\n    len(hosts), 0, instance_key)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/ops/collective_ops.py\", line 133, in broadcast_recv\r\n    instance_key=instance_key)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_collective_ops.py\", line 74, in collective_bcast_recv\r\n    shape=shape, name=name)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[node worker0/CollectiveBcastRecv (defined at tf_distribute_collective_ops_sendrecv.py:45) ]]\r\n\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 129, in process_fn\r\n    result_tensors = mon_sess.run(tensors, options=run_options)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 676, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1171, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1270, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1327, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1091, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[node worker1/CollectiveBcastSend (defined at tf_distribute_collective_ops_sendrecv.py:42) ]]\r\n\r\nCaused by op 'worker1/CollectiveBcastSend', defined at:\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 155, in <module>\r\n    start_process()\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 149, in start_process\r\n    process.start()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/usr/lib64/python3.6/multiprocessing/context.py\", line 277, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/usr/lib64/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/usr/lib64/python3.6/multiprocessing/popen_fork.py\", line 73, in _launch\r\n    code = process_obj._bootstrap()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"tf_distribute_collective_ops_sendrecv.py\", line 42, in process_fn\r\n    len(hosts), 0, instance_key)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/ops/collective_ops.py\", line 105, in broadcast_send\r\n    instance_key=instance_key)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_collective_ops.py\", line 152, in collective_bcast_send\r\n    shape=shape, name=name)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): Instance 0 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.\r\n\t [[node worker1/CollectiveBcastSend (defined at tf_distribute_collective_ops_sendrecv.py:42) ]]\r\n```", "It seems only the `collective_group_leader` is able to trigger the `broadcast_send`. And it is unnecessary to create a pair of send/recv op of the same `instance_key` on the same device.", "Thanks for the report.  I'll look into this a bit more and get back to you.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32810\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32810\">No</a>\n"]}, {"number": 32808, "title": "Why the communication of distributed training not hidden when using XLA?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): nvcr.io/nvidia/tensorflow:19.03-py3 ; \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n\r\n\r\n- TensorFlow version (use command below):  tf 1.13.1 ; \r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n Recently, we have designed a tool to monitor the communication of distributed training process in microsecond. As we all know that the commnuication is hidden in the back-propagation progress; and that's true in my monitoring, like the yellow line in the figure below, that is with XLA disabled. But when we enable the XLA, the traffic between GPU is suspend until the finish of the back-propagation in my figure (blue line). So, is there anyone can help me to figure out the principle behind this phenomenon. Since the traffic can not be hidden in the back-propagation progress, the requirement of the bandwidth for the hardware, like nv-link in the server or infiniband network between servers goes up a lot. \r\n\r\n![\u4e0b\u8f7d (3)](https://user-images.githubusercontent.com/5318606/65588391-0ad2a980-dfba-11e9-8c70-1815edc17bce.png)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is probably because of bad clustering decisions that is decreasing concurrency / overlapped execution.  Can you give us instructions on how to reproduce this?\r\n\r\nCC @trentlo @cheshire @tpopp ", "\r\nLooks like a similar issue as the description about the HorovodAllReduce nodes in this [report](https://docs.google.com/document/d/1q1UPN2CRRNBoUXM0zORT-cTG9m7ctQRB2xfUPhaK1Ek/edit?usp=sharing).\r\n\r\nWondering which model you are running? Is it with Horovod?\r\n", "hi\uff0ctrentlo\uff1b\r\n1. I find this phenomenon using horovod and also in tensorflow distributed strategy.  \r\n2. I find this phenomenon training resnet50 and also bert-large squad. \r\nSo, I think this is a XLA internal issue.", "> This is probably because of bad clustering decisions that is decreasing concurrency / overlapped execution. Can you give us instructions on how to reproduce this?\r\n> \r\n> CC @trentlo @cheshire @tpopp\r\n\r\nHi, trentlo\r\n\r\nusing nvcr.io/nvidia/tensorflow:19.03-py3  + tf_benchmark 1.13 + resnet50 or other cnn models + xla \r\n", "hi\uff0c guys\uff1b any update\uff1f", "@zhuhong realistically, I won't have time for this anytime soon.  Can you try using the [TensorBoard profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) on your model to see something obvious shows up?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32808\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32808\">No</a>\n"]}, {"number": 32807, "title": "AttributeError: 'Tensor' object has no attribute '_lazy_read'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI run this following code\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\n#tf.enable_eager_execution()\r\na = tf.constant([[[i] * 2 for i in range(7)]]*3, dtype=tf.float32)\r\nindice = tf.constant([[1,3,6,0],[1,4,6,0],[2,4,6,0]])\r\ndef fn(args):\r\n    n = args[3]\r\n    temp = args[0]\r\n    index_first = args[1]\r\n    index_second = args[2]\r\n    time = tf.constant(0)\r\n    #out_temp = args[4]\r\n    #out_temp = tf.TensorArray(tf.float32, n,\r\n    #                           tensor_array_name=\"output_array\")\r\n    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\r\n        out_temp = tf.get_variable(name='temp',shape=[3,2],dtype=tf.float32, initializer=tf.constant_initializer(0))\r\n        #zero = lambda: tf.zeros([3,2], dtype=tf.int32)\r\n        #out_temp = tf.Variable(zero)\r\n    out_list = []\r\n    def loop_fn(t, x, ind_1, ind_2, n, out_temp):\r\n        #print(ind_1[t], ind_2[t])\r\n        #print(x[ind_1[t]:ind_2[t]])\r\n        vec = tf.reduce_mean(tf.cast(x[ind_1[t]:ind_2[t]], tf.float32), axis=0)\r\n        #print(vec)\r\n        #vec = tf.Print(vec, [vec], message='vec: ')\r\n        #print_op = tf.print('vec:', vec, output_stream=sys.stdout)\r\n        #out_temp = tf.assign(out_temp[t], vec)\r\n        #out_temp = out_temp.write(t, vec)\r\n        #op = tf.assign(out_temp[t], vec)\r\n        with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\r\n            #out_temp = tf.get_variable(name='temp',shape=[3,2],dtype=tf.float32)\r\n            out_temp = tf.scatter_update(out_temp, t, vec)\r\n        #print(out_temp)\r\n        with tf.control_dependencies([out_temp]):\r\n            return t+1, x, ind_1, ind_2, n, out_temp\r\n    out = tf.while_loop(lambda t, *_: t < n, loop_fn, (time, temp, index_first, index_second, n, out_temp))\r\n    #out_temp = tf.get_variable(name='temp',shape=[3,2],dtype=tf.float32)\r\n    out_temp = out[-1]\r\n    print(out_temp)\r\n    print_op = tf.print('temp:', out_temp, output_stream=sys.stdout)\r\n    #out_temp = tf.Print(out_temp, [out_temp], message='temp')\r\n    with tf.control_dependencies([out_temp]):\r\n        return [out_temp, args[1], args[2], args[3]]\r\n\r\nn = [tf.shape(indice)[1] - 1]\r\nn = tf.tile(n,[tf.shape(indice)[0]])\r\n#out_tas = tf.tile(out_ta, [tf.shape(indice)[0]])\r\n#a_temp = tf.zeros([tf.shape(a)[0], tf.shape(indice)[1]-1, 5], dtype=tf.float32)\r\nout = tf.map_fn(fn, [a, indice[:,:-1], indice[:,1:], n])\r\nprint(out[0])\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    print(sess.run(out))\r\n```\r\nIt print the good result in eager mode, but it return AttributeError: 'Tensor' object has no attribute '_lazy_read' when I close the eager mode. I think I do feed a tf.Variable to scatter_update, but it still return the error message.\r\n**Describe the expected behavior**\r\nReturn the value like eager mode.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 32806, "title": "keras.Model.fit does not work with custom layers in Tensorflow 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below):\u00a02.0.0-rc1\r\n- Python version: 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\r\n\r\n**Describe the current behavior**\r\nhttps://colab.research.google.com/drive/1dUGcOfRIXkP6ZaI-9xKRXAntf8ZzS6bp\r\n\r\n```python\r\nclass DualLayer(keras.layers.Layer):\r\n\u00a0 def __init__(self, units):\r\n\u00a0 \u00a0 super().__init__(self)\r\n\u00a0 \u00a0 self.first_layer = keras.layers.Dense(units, 'relu')\r\n\u00a0 \u00a0 self.second_layer = keras.layers.Dense(units, 'relu')\r\n\u00a0 \r\n\u00a0 def call(self, input):\r\n\u00a0 \u00a0 return self.second_layer(self.first_layer(input))\r\n\r\nmodel = keras.Sequential([keras.layers.Flatten(), DualLayer(128), keras.layers.Dense(10, activation='softmax')])\r\nmodel.compile(optimizer='adam',\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 loss='sparse_categorical_crossentropy',\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metrics=['accuracy'])\r\nhistory = model.fit(train_images, train_labels, batch_size=32, epochs=5)\r\n```\r\n\r\nThis code dies at model.compile with `RecursionError: maximum recursion depth exceeded while calling a Python object`.According to the stack trace, it seems like it falls into an infinite loop with `hasattr` and `layer.trainable = value`.\r\n```\u00a0hasattr(self.__class__, name)):\r\n\u00a0 \u00a02255 \u00a0 \u00a0 \u00a0 try:\r\n-> 2256 \u00a0 \u00a0 \u00a0 \u00a0 super(tracking.AutoTrackable, self).__setattr__(name, value)\r\n\u00a0 \u00a02257 \u00a0 \u00a0 \u00a0 except AttributeError:\r\n\u00a0 \u00a02258 \u00a0 \u00a0 \u00a0 \u00a0 raise AttributeError(\r\n\r\n/tensorflow-2.0.0-rc1/python3.6/tensorflow_core/python/keras/engine/base_layer.py in trainable(self, value)\r\n\u00a0 \u00a0 915 \u00a0 \u00a0 self._trainable = value\r\n\u00a0 \u00a0 916 \u00a0 \u00a0 for layer in getattr(self, '_layers', []):\r\n--> 917 \u00a0 \u00a0 \u00a0 layer.trainable = value\r\n\u00a0 \u00a0 918 \r\n\u00a0 \u00a0 919 \u00a0 @property\r\n\r\n/tensorflow-2.0.0-rc1/python3.6/tensorflow_core/python/keras/engine/base_layer.py in __setattr__(self, name, value)\r\n\u00a0 \u00a02254 \u00a0 \u00a0 \u00a0 \u00a0 hasattr(self.__class__, name)):\r\n\u00a0 \u00a02255 \u00a0 \u00a0 \u00a0 try:\r\n-> 2256 \u00a0 \u00a0 \u00a0 \u00a0 super(tracking.AutoTrackable, self).__setattr__(name, value)\r\n\u00a0 \u00a02257 \u00a0 \u00a0 \u00a0 except AttributeError:\r\n\u00a0 \u00a02258 \u00a0 \u00a0 \u00a0 \u00a0 raise AttributeError(\r\n```\r\n**Describe the expected behavior**\r\nTensorflow 2.x should work with a custom nested layer as Tensorflow 1.x can.\r\n\r\n**Code to reproduce the issue**\r\n\r\n2.0.0-rc1:https://colab.research.google.com/drive/1dUGcOfRIXkP6ZaI-9xKRXAntf8ZzS6bp\r\n1.14.0:\r\nhttps://colab.research.google.com/drive/1DeOxKVDM8xEJmU_8GZo-h0iJkWGA4De8\r\n", "comments": ["1. Use gast==0.2.2 for now. gast 0.3.x will give Attribute Missing Error. This hasn't been fixed as of rc2.\r\n\r\n2. You need to use \r\n`super().__init__()`\r\nor \r\n`super(DualLayer, self).__init__()`\r\n\r\nThe code runs otherwise.", "> 1. Use gast==0.2.2 for now. gast 0.3.x will give Attribute Missing Error. This hasn't been fixed as of rc2.\r\n\r\nDo you mean `gast 0.3.x` will improve the error message and I will be able to notice such a silly mistake more easily?\r\n\r\n> 2. You need to use\r\n>    `super().__init__()`\r\n>    or\r\n>    `super(DualLayer, self).__init__()`\r\n> \r\n\r\nOops... I assigned `self` as `trainable` of `__init__` of `keras.layers.Layer`. I'm very ashamed. Thank you for the suggestion.", "@yunabe \r\nPlease,let me know if we can close this issue since it looks to be fixed. Thanks!", "Sure, it was due to my silly typo, which did not cause any problem in tf1.\r\nIt would be great I could get a more readable error message.", "@yunabe Gast 0.3.x won't help error message. I recommend to use gast 0.2.2 for now, atleast until the stable version is released. gast 0.3.x will give attribute missing warning. (See this Issue: #32319)"]}, {"number": 32805, "title": "model_pruning: Why 50%  and 90% zeros of the stripped models are the same size?", "body": "1. I'm trying tensorflow/contrib/model_pruning/examples/cifar10.  I use strip_pruning_vars to  remove pruning ops from the trained graph, strip_pruning_vars  shows 50% and 90% zeros respectively, but their final model are the same size.\r\n\r\n2.  As described in the doc\uff0c\u201cFor now, it is assumed that the underlying hardware platform will provide mechanisms for compressing the sparse tensors and/or accelerating the sparse tensor computations\u201c\r\n   Does it mean that model pruning requires hardware or inference engine support in sparse matrices? \r\n   In the current version, can I implement inference acceleration on x86 or GPU with the model pruning?\r\n\r\n", "comments": ["Apologies for the delay in response.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n"]}, {"number": 32804, "title": "Remove redundant `self.built=true` from keras/layers/convolutional.py", "body": "In `_maybe_build()`, `self.built` will be set `true` automatically. So all `self.built=true` of `Conv` will be redundant. This PR removes them. https://github.com/tensorflow/tensorflow/blob/75a91fac16493b6f68c0740b214ff18f5d88e5a8/tensorflow/python/keras/engine/base_layer.py#L2144", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32804) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32804) for more info**.\n\n<!-- ok -->", "@yd8534976 Can you please resolve conflicts? Thanks!", "@gbaned Done. Thanks!", "@tanzhenyu Can you please take a look on this PR? Thanks!", "@yd8534976 Can you please check build failures? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 32803, "title": "ResizeNearestNeighbor support bfloat16", "body": "Fix #32801", "comments": ["@fsx950223 Could you please check reviewer comments and keep us posted. Thanks!", "Yes, I'm still looking for the commit which implemented bfloat16 kernel alone. \n\n_\u4f7f\u7528 [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github) \u4ece\u6211\u7684 Pixel \u53d1\u9001_", "I may reopen it when I find the solution"]}, {"number": 32802, "title": "Problem to transform an custom efficient-net with an unofficial API to tf-lite version", "body": "**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 18.04):\r\n- TensorFlow installed from source:\r\n- TensorFlow version (1.12):\r\n- Keras version (2.2.4)\r\n\r\n\r\nHello, I use this Keras-repository to deploy my efficient-net-model for specific application.\r\nhttps://github.com/titu1994/keras-efficientnets\r\n\r\nI want to transform the h5-file to tflite and met some problems.\r\n\r\nMy code is:\r\n```\r\nimport tensorflow as tf\r\nimport keras\r\nimport h5py\r\nimport keras_efficientnets\r\nfrom custom_objects import EfficientNetConvInitializer\r\nfrom custom_objects import EfficientNetDenseInitializer\r\nfrom custom_objects import Swish, DropConnect\r\nif __name__ == \"__main__\":\r\n    debugger = EfficientNetConvInitializer()\r\n    converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(\"efficient_net_v1_190925.h5\")\r\n    converter.optimizations = [tf.contrib.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    tflite_model = converter.convert()\r\n    open(\"efficient_net_wrap_finger.tflite\", \"wb\").write(tflite_model)\r\n```\r\nMy problem is:\r\n\r\n> Using TensorFlow backend.\r\n> Traceback (most recent call last):\r\n>   File \"model_convert.py\", line 10, in <module>\r\n>     converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(\"efficient_net_v1_190925.h5\")\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 368, in from_keras_model_file\r\n>     keras_model = _keras.models.load_model(model_file)\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\", line 230, in load_model\r\n>     model = model_from_config(model_config, custom_objects=custom_objects)\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\", line 310, in model_from_config\r\n>     return deserialize(config, custom_objects=custom_objects)\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n>     printable_module_name='layer')\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 173, in deserialize_keras_object\r\n>     list(custom_objects.items())))\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1292, in from_config\r\n>     process_layer(layer_data)\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1278, in process_layer\r\n>     layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n>     printable_module_name='layer')\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 175, in deserialize_keras_object\r\n>     return cls.from_config(config['config'])\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1606, in from_config\r\n>     return cls(**config)\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 488, in __init__\r\n>     kernel_initializer=initializers.get(kernel_initializer),\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py\", line 155, in get\r\n>     return deserialize(identifier)\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py\", line 147, in deserialize\r\n>     printable_module_name='initializer')\r\n>   File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 163, in deserialize_keras_object\r\n>     raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n> ValueError: Unknown initializer: EfficientNetConvInitializer\r\n\r\nWhat is the root-cause for this problem?\r\n\r\nCan I clarify that keras-efficient-net API is not compatible with official tensorflow lite support?\r\nIs there any work-around that I can build an efficent-net and depoly on my mobile device?\r\n\r\nThanks & Regards!", "comments": ["Over to @xunkai55 who has experience with EfficientNet.", "Same problem when I refer to some Keras MobileNetv3 customed model for tf-lite transorfmation:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"model_converter.py\", line 14, in <module>\r\n>     converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 747, in from_keras_model_file\r\n>     keras_model = _keras.models.load_model(model_file, custom_objects)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 146, in load_model\r\n>     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\", line 212, in load_model_from_hdf5\r\n>     custom_objects=custom_objects)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 55, in model_from_config\r\n>     return deserialize(config, custom_objects=custom_objects)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 89, in deserialize\r\n>     printable_module_name='layer')\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 192, in deserialize_keras_object\r\n>     list(custom_objects.items())))\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1121, in from_config\r\n>     process_layer(layer_data)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1105, in process_layer\r\n>     layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 89, in deserialize\r\n>     printable_module_name='layer')\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 194, in deserialize_keras_object\r\n>     return cls.from_config(cls_config)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 446, in from_config\r\n>     return cls(**config)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 357, in __init__\r\n>     self.activation = activations.get(activation)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 306, in get\r\n>     return deserialize(identifier)\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 297, in deserialize\r\n>     printable_module_name='activation function')\r\n>   File \"E:\\Users\\Admin\\Anaconda3\\envs\\converter_from_cloud_to_cpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 211, in deserialize_keras_object\r\n>     raise ValueError('Unknown ' + printable_module_name + ':' + object_name)\r\n> ValueError: Unknown activation function:_hard_swish\r\n\r\nThe code is defined in this repository:\r\nhttps://github.com/xiaochus/MobileNetV3\r\n\r\n```\r\n    def _hard_swish(self, x):\r\n        \"\"\"Hard swish\r\n        \"\"\"\r\n        return x * K.relu(x + 3.0, max_value=6.0) / 6.0\r\n```\r\n\r\nI try to import the header file, e.g.\r\n`from mobilenetv3_model.mobilenet_base import MobileNetBase`\r\n\r\nFailure to tf-lite transformation.\r\n\r\nIt looks same with the problem of efficientnet model, tf-lite authorities do not support the customed Keras activation function or kernel function.\r\n\r\nHow to resolve the problem?\r\n\r\nThanks & Regards\uff01\r\n", "Hi Momo1986, sorry I've been in traveling and vacation for a while. I'll take a look and get back to you soon.", "it seems it hasn't come down to tflite yet, I think you need to make your function serializable by keras", "> it seems it hasn't come down to tflite yet, I think you need to make your function serializable by keras\r\n\r\nHello\uff0c@renjie-liu. I use keras.models.Sequential(model) for saving to realize the goal of serialization.\r\nProblem still have.\r\n\r\nThanks & Regards!", "Hi Momo,\r\n\r\nI mean your function needs to be serializable by Keras.\r\n\r\nAlso added Tiezhen who is the expert in Keras model.", "Hello, @renjie-liu .\r\nIt looks like questionable modules are defined by developer themselves rather than official Keras team.\r\nEfficientNetConvInitializer is such a customed object.\r\nhttps://github.com/titu1994/keras-efficientnets/blob/master/keras_efficientnets/custom_objects.py\r\n```\r\nget_custom_objects().update({\r\n    'EfficientNetConvInitializer': EfficientNetConvInitializer,\r\n    'EfficientNetDenseInitializer': EfficientNetDenseInitializer,\r\n    'DropConnect': DropConnect,\r\n    'Swish': Swish,\r\n})\r\n```\r\n\r\nCustomed API is called by EfficientNet and EfficientNet will be trained on specific dataset. It is OK in server to be saved with hdf5 file. However, it cannot be converted to tf-lite.\r\n\r\nThanks & Regards!", "Looking at the traceback, it did hit tflite converter codepath.\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(\"efficient_net_v1_190925.h5\")\r\nFile \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 368, in from_keras_model_file\r\n\r\nAssigning the bug to Haoliang to help identify if it's a problem with the converter.\r\n", "Can you try with the new TF Lite converter?\r\n\r\nInstall latest tf-nightly, and then try:\r\nconverter.experimental_new_converter = True.\r\n\r\n(The tensorflow version you use is a bit old, please try with tf-nightly to see if your issue persists).", "Hello, I found a workaround.\r\n\r\n1. Using tensorflow2.0\r\n2. Using custom object corresponding APIs.\r\n\r\nHere is a guide in Chines to convert efficient_net to tensorflow-lite.\r\nhttps://zhuanlan.zhihu.com/p/89599048\r\nRegards.\r\nJun Yan", "Translation & code snippet from https://zhuanlan.zhihu.com/p/89599048 for friends who do not read Chinese:\r\n\r\nRequirements:\r\n- tensorflow 2.x\r\n- if model is trained in native Keras, do `import tf.keras as keras`\r\n\r\nCode snippet:\r\n```\r\nimport functools\r\nimport keras\r\nimport os\r\nfrom efficientnet import model\r\nfrom tensorflow.python.keras.utils import CustomObjectScope, get_custom_objects\r\n\r\ndef inject_keras_modules(func):\r\n    @functools.wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        kwargs['backend'] = keras.backend\r\n        kwargs['layers'] = keras.layers\r\n        kwargs['models'] = keras.models\r\n        kwargs['utils'] = keras.utils\r\n        return func(*args, **kwargs)\r\n\r\n    return wrapper\r\ndef init_keras_custom_objects():\r\n    custom_objects = {\r\n        'swish': inject_keras_modules(model.get_swish)(),\r\n        'FixedDropout': inject_keras_modules(model.get_dropout)()\r\n    }\r\n\r\n    get_custom_objects().update(custom_objects)\r\n\r\ninit_keras_custom_objects()\r\nkeras_model_path = './some_model.h5'\r\nsave_model = tf.keras.models.load_model(keras_model_path)\r\nexport_dir='save'\r\ntf.saved_model.save(save_model, export_dir)\r\nnew_model = tf.saved_model.load(export_dir)\r\n\r\nIMAGE_WIDTH = 64 # example\r\nwith CustomObjectScope({'swish': inject_keras_modules(model.get_swish)(),\r\n                        'FixedDropout': inject_keras_modules(model.get_dropout)()}):\r\n    concrete_func = new_model.signatures[\r\n        tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n    concrete_func.inputs[0].set_shape([1, IMAGE_WIDTH, IMAGE_WIDTH, 3])\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n\r\nconcrete_func = new_model.signatures[\r\n        tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, IMAGE_WIDTH, IMAGE_WIDTH, 3])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n\r\nMODEL_OUTPUT_PATH = \"efficient_net_b0.tflite\"\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\nopen(MODEL_OUTPUT_PATH, \"wb\").write(tflite_model)\r\n\r\n\r\n```\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32802\">No</a>\n"]}, {"number": 32801, "title": "UpSampling2D doesn't support bfloat16", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): nightly\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTypeError: Value passed to parameter 'images' has DataType bfloat16 not in list of allowed values: int8, uint8, int16, uint16, int32, int64, float16, float32, float64\r\n**Describe the expected behavior**\r\nsupport bfloat16\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n``` python\r\nimport tensorflow as tf\r\ninput = tf.keras.Input(shape=(28, 28, 1), name='img',dtype=tf.bfloat16)\r\nx = tf.keras.layers.UpSampling2D(3)(input)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@fsx950223 I could reproduce the issue with `TF1.15.0`. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/371cabae8a54797e13b887103170f6c6/untitled629.ipynb) with `TF1.15.0`.\r\n\r\nHowever, your code works without an issue using `TF2.0`. Please take a look at the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/701fd556c91f231b7cc2316fe53d62d4/untitled628.ipynb). \r\n\r\nI think there may not be any updates to `TF1.15.0` unless the issue is related to security. Are you willing to upgrade to `TF2.0`? Thanks!", "> @fsx950223 I could reproduce the issue with `TF1.15.0`. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/371cabae8a54797e13b887103170f6c6/untitled629.ipynb) with `TF1.15.0`.\r\n> \r\n> However, your code works without an issue using `TF2.0`. Please take a look at the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/701fd556c91f231b7cc2316fe53d62d4/untitled628.ipynb).\r\n> \r\n> I think there may not be any updates to `TF1.15.0` unless the issue is related to security. Are you willing to upgrade to `TF2.0`? Thanks!\r\n\r\nYes", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32801\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32801\">No</a>\n", "I'm getting this same issue in TF2.1:\r\n```import tensorflow as tf\r\n\r\ntf.keras.mixed_precision.experimental.set_policy('mixed_bfloat16')\r\n\r\noptimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)\r\n\r\ninput=tf.keras.layers.Input(shape=(256, 256, 3))\r\n\r\nx=tf.keras.layers.Conv2D(32,(3,3))(input)\r\nx=tf.keras.layers.UpSampling2D()(x)\r\n\r\nx=tf.keras.layers.Conv2D(32,(3,3))(x)\r\nout=tf.keras.layers.Activation('sigmoid', dtype='float32')(x)\r\n\r\nmy_model = tf.keras.models.Model(inputs=input, outputs=out)\r\n\r\noptimizer = tf.keras.optimizers.RMSprop()\r\n```\r\n\r\n>  line 10, in <module>\r\n>     x=tf.keras.layers.UpSampling2D()(x)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 773, in __call__\r\n>     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional.py\", line 2004, in call\r\n>     interpolation=self.interpolation)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 2782, in resize_images\r\n>     x, new_shape, method=image_ops.ResizeMethod.NEAREST_NEIGHBOR)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py\", line 1357, in resize_images_v2\r\n>     skip_resize_if_same=False)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py\", line 1133, in _resize_images_common\r\n>     images = resizer_fn(images, size)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py\", line 1337, in resize_fn\r\n>     images_t, new_size, half_pixel_centers=True)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_image_ops.py\", line 3419, in resize_nearest_neighbor\r\n>     name=name)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 576, in _apply_op_helper\r\n>     param_name=input_name)\r\n>   File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 61, in _SatisfiesTypeConstraint\r\n>     \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n> TypeError: Value passed to parameter 'images' has DataType bfloat16 not in list of allowed values: int8, uint8, int16, uint16, int32, int64, float16, float32, float64", "@LambertMark Please create a new issue with details and a simple standalone code to reproduce the issue. Thanks!"]}, {"number": 32800, "title": "\"Autograph capabilities and limitations\" link in AutoGraph doc is outdated", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/autograph\r\n\r\n## Description of issue (what needs changing):\r\n\"Autograph capabilities and limitations.\" link points an out dated doc at:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md\r\n\r\n### Clear description\r\n\r\n- There was a similar bug in https://github.com/tensorflow/tensorflow/issues/22280 in 2018.\r\n- This issue was reported [in the closed bug](https://github.com/tensorflow/tensorflow/issues/22280#issuecomment-527699503)) but this comment did not get a reply. Thus, I'm filing a new bug.\r\n\r\n### Submit a pull request?\r\n\r\nActually, this issue was fixed on Aug 6th, 1.5+ month ago.\r\nhttps://github.com/tensorflow/docs/commit/c29c6fa8202fcd8da4ab8d8072c5f7dacf7c160a#diff-039832f2dbb662a37df6e0fa64ebe35e", "comments": ["It seems like the link was fixed on Aug 6th, 1.5+ month ago, on GitHub.\r\nhttps://github.com/tensorflow/docs/commit/c29c6fa8202fcd8da4ab8d8072c5f7dacf7c160a#diff-039832f2dbb662a37df6e0fa64ebe35e\r\n\r\nI'm wondering why the live document is not fixed yet.\r\nIsn't `r2.0rc` branch the version we use to create the site? Or we haven't pushed any document fixes at least since Aug 6th?", "@yunabe TF team is working on updating the website for future 2.0 release. Once, TF2.0 is released, all the links will work as expected. Thanks!\r\n\r\n[Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md) is the missing link.\r\n\r\n[This `autograph.ipynb`](https://github.com/tensorflow/docs/blob/r2.0rc/site/en/guide/autograph.ipynb) has correct links. Thanks!  ", "Do you mean we won't fix problems of exiting docs until tf2 is launched?", "@yunabe As you mentioned and I also checked source doc which was fixed already. But, TF team is working on updating the TF website. It will take some time to update all the links. Thanks!", "I see. I'm not very happy about the fact that the website is frozen for months. But I understand the current status. Thank you for the advice, Vishnuvardhan.\r\n\r\nBTW, when TF2 will be released? https://www.tensorflow.org/community/roadmap says *The official 2.0 release target is Q2 2019*.\r\n\r\nAnyway, I'm closing this bug because there is no solution until TF2 is launched.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32800\">No</a>\n"]}]